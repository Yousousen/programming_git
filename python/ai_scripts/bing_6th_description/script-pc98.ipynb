{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary modules\n",
    "import torch # For creating tensors and neural network\n",
    "import torch.nn as nn # For creating layers and activation functions\n",
    "import torch.optim as optim # For creating optimizer\n",
    "import numpy as np # For creating arrays and sampling data\n",
    "import matplotlib.pyplot as plt # For plotting results\n",
    "\n",
    "# Setting the random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Checking if GPU is available and setting the device accordingly\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Defining the constants for the neural network\n",
    "NUM_EPOCHS = 400 # Number of epochs for training\n",
    "NUM_INPUTS = 3 # Number of inputs: D, S_x, tau\n",
    "NUM_OUTPUTS = 1 # Number of outputs: p\n",
    "NUM_HIDDEN1 = 600 # Number of neurons in the first hidden layer\n",
    "NUM_HIDDEN2 = 200 # Number of neurons in the second hidden layer\n",
    "BATCH_SIZE = 32 # Size of mini-batches for gradient descent\n",
    "INIT_LR = 6e-4 # Initial learning rate for training\n",
    "LR_FACTOR = 0.5 # Factor to multiply learning rate when adapting\n",
    "LR_PATIENCE = 5 # Number of epochs to wait before adapting learning rate\n",
    "LR_THRESHOLD = 0.0005 # Threshold to check improvement in loss before adapting learning rate\n",
    "LR_COOLDOWN = 10 # Number of epochs to wait after adapting learning rate\n",
    "\n",
    "# Defining the constants for the data generation\n",
    "TRAIN_SIZE = 80000 # Number of samples in the training dataset\n",
    "TEST_SIZE = 10000 # Number of samples in the test dataset\n",
    "RHO_MIN = 0.0 # Minimum value of rho (density) for sampling\n",
    "RHO_MAX = 10.1 # Maximum value of rho (density) for sampling\n",
    "EPS_MIN = 0.0 # Minimum value of eps (specific internal energy) for sampling\n",
    "EPS_MAX = 2.02 # Maximum value of eps (specific internal energy) for sampling\n",
    "VX_MIN = 0.0 # Minimum value of v_x (velocity in x direction) for sampling\n",
    "VX_MAX = 0.721 # Maximum value of v_x (velocity in x direction) for sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a function to generate data from primitive variables to conservative variables\n",
    "def generate_data(size):\n",
    "    \"\"\"\n",
    "    Generates data from primitive variables to conservative variables.\n",
    "\n",
    "    Parameters:\n",
    "        size (int): The number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        X (torch.Tensor): The tensor of shape (size, NUM_INPUTS) containing the conservative variables D, S_x, tau.\n",
    "        y (torch.Tensor): The tensor of shape (size, NUM_OUTPUTS) containing the primitive variable p.\n",
    "    \"\"\"\n",
    "    # Sampling primitive variables from uniform distributions over given intervals\n",
    "    rho = np.random.uniform(RHO_MIN, RHO_MAX, size) # Density\n",
    "    eps = np.random.uniform(EPS_MIN, EPS_MAX, size) # Specific internal energy\n",
    "    vx = np.random.uniform(VX_MIN, VX_MAX, size) # Velocity in x direction\n",
    "\n",
    "    # Calculating primitive variable p (pressure) from equation of state (EOS)\n",
    "    gamma = 4/3 # Adiabatic index for ultra-relativistic gas EOS\n",
    "    p = rho * eps * (gamma - 1) # Pressure\n",
    "\n",
    "    # Calculating conservative variables from primitive variables using equations (2) from Dieseldorst et al.\n",
    "    W = 1 / np.sqrt(1 - vx**2) # Lorentz factor\n",
    "    h = 1 + eps + p / rho # Specific enthalpy\n",
    "    D = rho * W # Conserved density\n",
    "    Sx = rho * h * W**2 * vx # Conserved momentum in x direction\n",
    "    tau = rho * h * W**2 - p - D # Conserved energy density\n",
    "\n",
    "    # Converting numpy arrays to torch tensors and moving them to device\n",
    "    X = torch.tensor(np.stack([D, Sx, tau], axis=1), dtype=torch.float32).to(device) # Conservative variables\n",
    "    y = torch.tensor(p, dtype=torch.float32).unsqueeze(1).to(device) # Primitive variable p\n",
    "\n",
    "    # Returning X and y\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generating training and test datasets using the generate_data function\n",
    "X_train, y_train = generate_data(TRAIN_SIZE)\n",
    "X_test, y_test = generate_data(TEST_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a class for the neural network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A class for the neural network model that performs conservative-to-primitive inversion.\n",
    "\n",
    "    Attributes:\n",
    "        fc1 (nn.Linear): The first fully connected layer with NUM_INPUTS inputs and NUM_HIDDEN1 outputs.\n",
    "        fc2 (nn.Linear): The second fully connected layer with NUM_HIDDEN1 inputs and NUM_HIDDEN2 outputs.\n",
    "        fc3 (nn.Linear): The third fully connected layer with NUM_HIDDEN2 inputs and NUM_OUTPUTS outputs.\n",
    "        sigmoid (nn.Sigmoid): The sigmoid activation function for the hidden layers.\n",
    "        relu (nn.ReLU): The ReLU activation function for the output layer.\n",
    "\n",
    "    Methods:\n",
    "        forward(x): Performs the forward pass of the neural network on the input x and returns the output y.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the neural network model with the attributes defined above.\n",
    "        \"\"\"\n",
    "        super(NeuralNetwork, self).__init__() # Calling the superclass constructor\n",
    "        self.fc1 = nn.Linear(NUM_INPUTS, NUM_HIDDEN1) # First fully connected layer\n",
    "        self.fc2 = nn.Linear(NUM_HIDDEN1, NUM_HIDDEN2) # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(NUM_HIDDEN2, NUM_OUTPUTS) # Third fully connected layer\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid activation function\n",
    "        self.relu = nn.ReLU() # ReLU activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the neural network on the input x and returns the output y.\n",
    "\n",
    "        Parameters:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, NUM_INPUTS) containing the conservative variables.\n",
    "\n",
    "        Returns:\n",
    "            y (torch.Tensor): The output tensor of shape (batch_size, NUM_OUTPUTS) containing the primitive variable p.\n",
    "        \"\"\"\n",
    "        x = self.fc1(x) # Passing x through the first fully connected layer\n",
    "        x = self.sigmoid(x) # Applying sigmoid activation function\n",
    "        x = self.fc2(x) # Passing x through the second fully connected layer\n",
    "        x = self.sigmoid(x) # Applying sigmoid activation function\n",
    "        y = self.fc3(x) # Passing x through the third fully connected layer\n",
    "        y = self.relu(y) # Applying ReLU activation function\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating an instance of the neural network model and moving it to device\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "# Defining the loss function as mean squared error\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Defining the optimizer as Adam with initial learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=INIT_LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a function to calculate other primitive variables from p using equations (A2), (A3), (A4), (A5) from Dieseldorst et al.\n",
    "def calculate_primitives(X, p):\n",
    "    \"\"\"\n",
    "    Calculates other primitive variables from p using equations (A2), (A3), (A4), (A5) from Dieseldorst et al.\n",
    "\n",
    "    Parameters:\n",
    "        X (torch.Tensor): The tensor of shape (batch_size, NUM_INPUTS) containing the conservative variables D, S_x, tau.\n",
    "        p (torch.Tensor): The tensor of shape (batch_size, NUM_OUTPUTS) containing the primitive variable p.\n",
    "\n",
    "    Returns:\n",
    "        rho (torch.Tensor): The tensor of shape (batch_size, 1) containing the primitive variable rho (density).\n",
    "        eps (torch.Tensor): The tensor of shape (batch_size, 1) containing the primitive variable eps (specific internal energy).\n",
    "        vx (torch.Tensor): The tensor of shape (batch_size, 1) containing the primitive variable v_x (velocity in x direction).\n",
    "    \"\"\"\n",
    "    # Extracting conservative variables from X\n",
    "    D = X[:, 0].unsqueeze(1) # Conserved density\n",
    "    Sx = X[:, 1].unsqueeze(1) # Conserved momentum in x direction\n",
    "    tau = X[:, 2].unsqueeze(1) # Conserved energy density\n",
    "\n",
    "    # Calculating primitive variable v_x\n",
    "    vx = Sx / (tau + D + p) # Velocity in x direction\n",
    "\n",
    "    # Calculating primitive variable W (Lorentz factor)\n",
    "    W = 1 / torch.sqrt(1 - vx**2) # Lorentz factor\n",
    "\n",
    "    # Calculating primitive variable eps (specific internal energy)\n",
    "    eps = (tau + D * (1 - W) + p * (1 - W**2)) / (D * W) # Specific internal energy\n",
    "\n",
    "    # Calculating primitive variable rho (density)\n",
    "    rho = D / W # Density\n",
    "\n",
    "    # Returning rho, eps, vx\n",
    "    return rho, eps, vx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a function to calculate the L1 and L_inf norms of the error between two tensors\n",
    "def calculate_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the L1 and L_inf norms of the error between two tensors.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (torch.Tensor): The tensor of shape (batch_size, 1) containing the true values.\n",
    "        y_pred (torch.Tensor): The tensor of shape (batch_size, 1) containing the predicted values.\n",
    "\n",
    "    Returns:\n",
    "        L1 (float): The L1 norm of the error.\n",
    "        Linf (float): The L_inf norm of the error.\n",
    "    \"\"\"\n",
    "    # Calculating the absolute error\n",
    "    error = torch.abs(y_true - y_pred)\n",
    "\n",
    "    # Calculating the L1 norm by taking the mean of the error\n",
    "    L1 = torch.mean(error).item()\n",
    "\n",
    "    # Calculating the L_inf norm by taking the maximum of the error\n",
    "    Linf = torch.max(error).item()\n",
    "\n",
    "    # Returning L1 and Linf\n",
    "    return L1, Linf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating lists to store the training and test losses and errors for each epoch\n",
    "train_losses = [] # List to store the training losses\n",
    "test_losses = [] # List to store the test losses\n",
    "train_errors_L1 = [] # List to store the training errors in L1 norm\n",
    "test_errors_L1 = [] # List to store the test errors in L1 norm\n",
    "train_errors_Linf = [] # List to store the training errors in L_inf norm\n",
    "test_errors_Linf = [] # List to store the test errors in L_inf norm\n",
    "\n",
    "# Creating a scheduler to adapt the learning rate based on the training loss\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=LR_FACTOR, patience=LR_PATIENCE, threshold=LR_THRESHOLD, cooldown=LR_COOLDOWN)\n",
    "\n",
    "# Creating a loop for training the neural network for a given number of epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    # Printing the current epoch and learning rate\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "    # Setting the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Initializing the running loss for training data\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Creating a loop for processing each mini-batch of training data\n",
    "    for i in range(0, TRAIN_SIZE, BATCH_SIZE):\n",
    "\n",
    "        # Getting the mini-batch of training data and labels\n",
    "        X_batch = X_train[i:i+BATCH_SIZE] # Mini-batch of conservative variables\n",
    "        y_batch = y_train[i:i+BATCH_SIZE] # Mini-batch of primitive variable p\n",
    "\n",
    "        # Zeroing the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Performing a forward pass through the neural network and getting the output\n",
    "        y_pred = model(X_batch) # Predicted primitive variable p\n",
    "\n",
    "        # Calculating the loss using the criterion\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "\n",
    "        # Performing a backward pass through the neural network and calculating the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating the parameters using the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # Updating the running loss for training data\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculating and printing the average loss for training data\n",
    "    train_loss = running_loss / (TRAIN_SIZE / BATCH_SIZE)\n",
    "    print(f\"Training loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Appending the training loss to the list\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Setting the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Performing a forward pass through the neural network on the test data and getting the output\n",
    "    y_test_pred = model(X_test) # Predicted primitive variable p for test data\n",
    "\n",
    "    # Calculating the loss for test data using the criterion\n",
    "    test_loss = criterion(y_test_pred, y_test).item()\n",
    "\n",
    "    # Printing the loss for test data\n",
    "    print(f\"Test loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Appending the test loss to the list\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    # Calculating other primitive variables for test data using the calculate_primitives function\n",
    "    rho_test_pred, eps_test_pred, vx_test_pred = calculate_primitives(X_test, y_test_pred)\n",
    "\n",
    "    # Calculating other primitive variables for test labels using the calculate_primitives function\n",
    "    rho_test_true, eps_test_true, vx_test_true = calculate_primitives(X_test, y_test)\n",
    "\n",
    "    # Calculating and printing the L1 and L_inf errors for test data for each primitive variable\n",
    "    rho_error_L1, rho_error_Linf = calculate_error(rho_test_true, rho_test_pred)\n",
    "    print(f\"Test error for rho: L1 = {rho_error_L1:.4f}, Linf = {rho_error_Linf:.4f}\")\n",
    "    eps_error_L1, eps_error_Linf = calculate_error(eps_test_true, eps_test_pred)\n",
    "    print(f\"Test error for eps: L1 = {eps_error_L1:.4f}, Linf = {eps_error_Linf:.4f}\")\n",
    "    vx_error_L1, vx_error_Linf = calculate_error(vx_test_true, vx_test_pred)\n",
    "    print(f\"Test error for vx: L1 = {vx_error_L1:.4f}, Linf = {vx_error_Linf:.4f}\")\n",
    "    p_error_L1, p_error_Linf = calculate_error(y_test, y_test_pred)\n",
    "    print(f\"Test error for p: L1 = {p_error_L1:.4f}, Linf = {p_error_Linf:.4f}\")\n",
    "\n",
    "    # Appending the L1 and L_inf errors for test data to the lists\n",
    "    test_errors_L1.append([rho_error_L1, eps_error_L1, vx_error_L1, p_error_L1])\n",
    "    test_errors_Linf.append([rho_error_Linf, eps_error_Linf, vx_error_Linf, p_error_Linf])\n",
    "\n",
    "    # Calculating other primitive variables for training data using the calculate_primitives function\n",
    "    y_train_pred = model(X_train) # Predicted primitive variable p for training data\n",
    "    rho_train_pred, eps_train_pred, vx_train_pred = calculate_primitives(X_train, y_train_pred)\n",
    "\n",
    "    # Calculating other primitive variables for training labels using the calculate_primitives function\n",
    "    rho_train_true, eps_train_true, vx_train_true = calculate_primitives(X_train, y_train)\n",
    "\n",
    "    # Calculating and printing the L1 and L_inf errors for training data for each primitive variable\n",
    "    rho_error_L1, rho_error_Linf = calculate_error(rho_train_true, rho_train_pred)\n",
    "    print(f\"Training error for rho: L1 = {rho_error_L1:.4f}, Linf = {rho_error_Linf:.4f}\")\n",
    "    eps_error_L1, eps_error_Linf = calculate_error(eps_train_true, eps_train_pred)\n",
    "    print(f\"Training error for eps: L1 = {eps_error_L1:.4f}, Linf = {eps_error_Linf:.4f}\")\n",
    "    vx_error_L1, vx_error_Linf = calculate_error(vx_train_true, vx_train_pred)\n",
    "    print(f\"Training error for vx: L1 = {vx_error_L1:.4f}, Linf = {vx_error_Linf:.4f}\")\n",
    "    p_error_L1, p_error_Linf = calculate_error(y_train, y_train_pred)\n",
    "    print(f\"Training error for p: L1 = {p_error_L1:.4f}, Linf = {p_error_Linf:.4f}\")\n",
    "\n",
    "    # Appending the L1 and L_inf errors for training data to the lists\n",
    "    train_errors_L1.append([rho_error_L1, eps_error_L1, vx_error_L1, p_error_L1])\n",
    "    train_errors_Linf.append([rho_error_Linf, eps_error_Linf, vx_error_Linf, p_error_Linf])\n",
    "\n",
    "    # Adapting the learning rate based on the training loss using the scheduler\n",
    "    scheduler.step(train_loss)\n",
    "\n",
    "# Converting the lists of errors to numpy arrays for easier indexing\n",
    "train_errors_L1 = np.array(train_errors_L1)\n",
    "test_errors_L1 = np.array(test_errors_L1)\n",
    "train_errors_Linf = np.array(train_errors_Linf)\n",
    "test_errors_Linf = np.array(test_errors_Linf)\n",
    "\n",
    "# Plotting the training and test losses against epochs\n",
    "plt.figure()\n",
    "plt.plot(range(1, NUM_EPOCHS+1), train_losses, label='Training loss')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), test_losses, label='Test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs Epoch')\n",
    "plt.legend()\n",
    "plt.savefig('loss.png')\n",
    "\n",
    "# Plotting the training and test errors in L1 norm against epochs for each primitive variable\n",
    "plt.figure()\n",
    "plt.plot(range(1, NUM_EPOCHS+1), train_errors_L1[:, 0], label='Training error for rho')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), test_errors_L1[:, 0], label='Test error for rho')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), train_errors_L1[:, 1], label='Training error for eps')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), test_errors_L1[:, 1], label='Test error for eps')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), train_errors_L1[:, 2], label='Training error for vx')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), test_errors_L1[:, 2], label='Test error for vx')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), train_errors_L1[:, 3], label='Training error for p')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), test_errors_L1[:, 3], label='Test error for p')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error (L1 norm)')\n",
    "plt.title('Error (L1 norm) vs Epoch')\n",
    "plt.legend()\n",
    "plt.savefig('error_L1.png')\n",
    "\n",
    "# Plotting the training and test errors in L_inf norm against epochs for each primitive variable\n",
    "plt.figure()\n",
    "plt.plot(range(1, NUM_EPOCHS+1), train_errors_Linf[:, 0], label='Training error for rho')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), test_errors_Linf[:, 0], label='Test error for rho')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), train_errors_Linf[:, 1], label='Training error for eps')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), test_errors_Linf[:, 1], label='Test error for eps')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), train_errors_Linf[:, 2], label='Training error for vx')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), test_errors_Linf[:, 2], label='Test error for vx')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), train_errors_Linf[:, 3], label='Training error for p')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), test_errors_Linf[:, 3], label='Test error for p')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error (L_inf norm)')\n",
    "plt.title('Error (L_inf norm) vs Epoch')\n",
    "plt.legend()\n",
    "plt.savefig('error_Linf.png')\n",
    "\n",
    "# Saving the model parameters to a file\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "# Printing a message to indicate the end of the code\n",
    "print(\"The code has finished running.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

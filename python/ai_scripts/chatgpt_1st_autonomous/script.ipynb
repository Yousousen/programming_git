{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT - 1st Code generation fully autonomous interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Here's a PyTorch code snippet that implements the neural network described in the \"Machine Learning for Conservative-to-Primitive in Relativistic Hydrodynamics\" paper for the conservative-to-primitive (C2P) conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class C2P_Network(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(C2P_Network, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer3 = nn.Linear(hidden_size, output_size)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.layer1(x))\n",
    "        x = self.activation(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This code defines a PyTorch module called `C2P_Network`, which is a neural network with three fully connected layers. The input size, hidden size, and output size of the network are specified as arguments to the constructor.\n",
    "\n",
    "The `__init__` method initializes the layers of the network, which consist of two hidden layers and one output layer. The activation function used between each layer is the leaky rectified linear unit (LeakyReLU) function.\n",
    "\n",
    "The `forward` method defines how the input is passed through the layers of the network during the forward pass. The input tensor `x` is passed through each layer in turn, with the activation function applied after each hidden layer. The output of the final layer is returned as the output of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the \"Machine Learning for Conservative-to-Primitive in Relativistic Hydrodynamics\" paper, the authors describe the architecture of their neural network as consisting of three fully connected layers with a leaky rectified linear unit (LeakyReLU) activation function. The input to the network is a tensor with shape (batch_size, num_variables), where `num_variables` is the number of fluid variables being considered (e.g., energy density, momentum density, pressure, etc.). The output of the network is a tensor with shape (batch_size, num_variables).\n",
    "\n",
    "Here's an example of how you could use the `C2P_Network` module in your PyTorch code to perform C2P conversion:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My additions\n",
    "num_variables = 5\n",
    "hidden_size = 2\n",
    "num_variables = 1\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create an instance of the C2P_Network module with the appropriate input, hidden, and output sizes\n",
    "network = C2P_Network(num_variables, hidden_size, num_variables)\n",
    "\n",
    "# Define an input tensor with shape (batch_size, num_variables)\n",
    "input_tensor = torch.randn(batch_size, num_variables)\n",
    "\n",
    "# Pass the input tensor through the network to obtain the output tensor\n",
    "output_tensor = network(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this example, `num_variables` is the number of fluid variables being considered, `hidden_size` is the number of neurons in the hidden layers of the network, `batch_size` is the number of samples being processed simultaneously, and `torch.randn` generates a random tensor with the specified shape. The output tensor `output_tensor` contains the converted primitive variables (e.g., pressure, velocity, etc.) for each sample in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors of the paper also describe the training procedure for their neural network, which involves minimizing a loss function that measures the error between the predicted primitive variables and the ground-truth primitive variables obtained from a conservative-to-primitive solver. They use a mean-squared error (MSE) loss function, which is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Here's an example of what a ground truth tensor might look like for the C2P conversion problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example ground truth tensor with shape (batch_size, num_cells, num_vars)\n",
    "ground_truth_tensor = torch.tensor([\n",
    "    [\n",
    "        [1.0, 0.0, 0.0],  # Density\n",
    "        [0.0, 2.0, 0.0],  # x-momentum\n",
    "        #[0.0, 0.0, 3.0],  # y-momentum\n",
    "        #[0.0, 0.0, 0.0],  # z-momentum\n",
    "        [4.0, 0.0, 0.0]   # Energy\n",
    "    ],\n",
    "    [\n",
    "        [2.0, 0.0, 0.0],  # Density\n",
    "        [0.0, 4.0, 0.0],  # x-momentum\n",
    "        #[0.0, 0.0, 6.0],  # y-momentum\n",
    "        #[0.0, 0.0, 0.0],  # z-momentum\n",
    "        [8.0, 0.0, 0.0]   # Energy\n",
    "    ]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this example, the ground truth tensor has a shape of `(batch_size, num_cells, num_vars)`, where `batch_size` is the number of data samples in the batch, `num_cells` is the number of cells in each data sample, and `num_vars` is the number of variables to be predicted by the C2PNet module.\n",
    "\n",
    "Each element in the tensor corresponds to a single variable at a single cell in a single data sample. For example, the first element `[1.0, 0.0, 0.0]` corresponds to the density at the first cell in the first data sample, while the second element `[0.0, 2.0, 0.0]` corresponds to the x-momentum at the second cell in the first data sample.\n",
    "\n",
    "Note that the ground truth tensor should be constructed using the true values of the variables, which are known in the training data. The goal of the C2PNet module is to predict these values for new data samples where the true values are not known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bvptr\\anaconda3\\envs\\BScPhysics\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([2, 3, 3])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m mse_loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[1;32m----> 2\u001b[0m loss \u001b[39m=\u001b[39m mse_loss(output_tensor, ground_truth_tensor)\n",
      "File \u001b[1;32mc:\\Users\\bvptr\\anaconda3\\envs\\BScPhysics\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\bvptr\\anaconda3\\envs\\BScPhysics\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 536\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmse_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\bvptr\\anaconda3\\envs\\BScPhysics\\lib\\site-packages\\torch\\nn\\functional.py:3294\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3291\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3292\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3294\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbroadcast_tensors(\u001b[39minput\u001b[39;49m, target)\n\u001b[0;32m   3295\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32mc:\\Users\\bvptr\\anaconda3\\envs\\BScPhysics\\lib\\site-packages\\torch\\functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[1;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mbroadcast_tensors(tensors)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "mse_loss = nn.MSELoss()\n",
    "loss = mse_loss(output_tensor, ground_truth_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code snippet, `output_tensor` is the tensor containing the predicted primitive variables for a batch of input samples, and `ground_truth_tensor` is a tensor containing the ground-truth primitive variables obtained from the conservative-to-primitive solver.\n",
    "\n",
    "To train the network, the authors use stochastic gradient descent (SGD) with a learning rate schedule, which reduces the learning rate over time to allow the network to converge more smoothly. Here's an example of how you could implement SGD with a learning rate schedule in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the optimizer and the learning rate schedule\n",
    "optimizer = optim.SGD(network.parameters(), lr=initial_lr)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "# Loop over the training data for multiple epochs\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in data_loader:\n",
    "        # Zero out the gradients of the network parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute the output of the network for the current batch\n",
    "        input_batch, ground_truth_batch = batch\n",
    "        output_batch = network(input_batch)\n",
    "\n",
    "        # Compute the loss and backpropagate\n",
    "        loss = mse_loss(output_batch, ground_truth_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Update the learning rate based on the validation loss\n",
    "    with torch.no_grad():\n",
    "        val_loss = evaluate(validation_data, network)\n",
    "        lr_scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this code snippet, `initial_lr` is the initial learning rate for the optimizer, `data_loader` is a PyTorch data loader that iterates over the training data in batches, `num_epochs` is the number of times to loop over the training data, and `evaluate` is a function that evaluates the network on a validation set and returns the validation loss. The `ReduceLROnPlateau` scheduler reduces the learning rate by a factor of 0.5 when the validation loss does not improve for `patience` epochs.\n",
    "\n",
    "Note that this is just an example of how you could implement the training procedure for the neural network based on the \"Machine Learning for Conservative-to-Primitive in Relativistic Hydrodynamics\" paper. The actual implementation may differ depending on the specific requirements of your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the training procedure, the paper also discusses the importance of data preprocessing and normalization for effective C2P conversion. The authors suggest using a log transformation for the input data to reduce the dynamic range of the variables, and normalizing the input and output data to have zero mean and unit variance. Here's an example of how you could perform data preprocessing and normalization for C2P conversion in PyTorch:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Here's an example of what `input_data` and `ground_truth_data` tensors might look like for the C2P conversion problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example input_data tensor with shape (batch_size, num_cells, num_vars)\n",
    "input_data = torch.tensor([\n",
    "    [\n",
    "        [1.0, 2.0, 3.0, 4.0, 5.0],  # Conserved variables at first cell\n",
    "        [6.0, 7.0, 8.0, 9.0, 10.0] # Conserved variables at second cell\n",
    "    ],\n",
    "    [\n",
    "        [11.0, 12.0, 13.0, 14.0, 15.0],  # Conserved variables at first cell\n",
    "        [16.0, 17.0, 18.0, 19.0, 20.0]  # Conserved variables at second cell\n",
    "    ]\n",
    "])\n",
    "\n",
    "# Example ground_truth_data tensor with shape (batch_size, num_cells, num_vars)\n",
    "ground_truth_data = torch.tensor([\n",
    "    [\n",
    "        [1.0, 0.0, 0.0],  # Density\n",
    "        [0.0, 2.0, 0.0],  # x-momentum\n",
    "        [0.0, 0.0, 3.0],  # y-momentum\n",
    "        [0.0, 0.0, 0.0],  # z-momentum\n",
    "        [4.0, 0.0, 0.0]   # Energy\n",
    "    ],\n",
    "    [\n",
    "        [2.0, 0.0, 0.0],  # Density\n",
    "        [0.0, 4.0, 0.0],  # x-momentum\n",
    "        [0.0, 0.0, 6.0],  # y-momentum\n",
    "        [0.0, 0.0, 0.0],  # z-momentum\n",
    "        [8.0, 0.0, 0.0]   # Energy\n",
    "    ]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this example, the `input_data` tensor and `ground_truth_data` tensor have the same shape of `(batch_size, num_cells, num_vars)` as the ground truth tensor in the previous example.\n",
    "\n",
    "The `input_data` tensor contains the conserved variables at each cell for each data sample in the batch. For example, the first element `[1.0, 2.0, 3.0, 4.0, 5.0]` corresponds to the conserved variables at the first cell in the first data sample, while the second element `[6.0, 7.0, 8.0, 9.0, 10.0]` corresponds to the conserved variables at the second cell in the first data sample.\n",
    "\n",
    "The `ground_truth_data` tensor contains the true values of the primitive variables at each cell for each data sample in the batch. For example, the first element `[1.0, 0.0, 0.0]` corresponds to the density at the first cell in the first data sample, while the second element `[0.0, 2.0, 0.0]` corresponds to the x-momentum at the second cell in the first data sample.\n",
    "\n",
    "Note that the input data and ground truth data should be constructed using the known conserved variables and true values of the primitive variables, respectively, for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'input_data.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "\u001b[0;32m      3\u001b[0m \u001b[39m# Load the data into NumPy arrays\u001b[39;00m\n",
      "\u001b[1;32m----> 4\u001b[0m input_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39minput_data.npy\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[0;32m      5\u001b[0m ground_truth_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mground_truth_data.npy\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;32m      7\u001b[0m \u001b[39m# Apply a log transformation to the input data\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\bvptr\\anaconda3\\envs\\BScPhysics\\lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n",
      "\u001b[0;32m    403\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;32m    404\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m--> 405\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "\u001b[0;32m    406\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;32m    408\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'input_data.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the data into NumPy arrays\n",
    "input_data = np.load('input_data.npy')\n",
    "ground_truth_data = np.load('ground_truth_data.npy')\n",
    "\n",
    "# Apply a log transformation to the input data\n",
    "input_data = np.log(input_data + 1e-10)\n",
    "\n",
    "# Compute the mean and standard deviation of the input and output data\n",
    "input_mean = np.mean(input_data, axis=0)\n",
    "input_std = np.std(input_data, axis=0)\n",
    "ground_truth_mean = np.mean(ground_truth_data, axis=0)\n",
    "ground_truth_std = np.std(ground_truth_data, axis=0)\n",
    "\n",
    "# Normalize the input and output data\n",
    "input_data = (input_data - input_mean) / input_std\n",
    "ground_truth_data = (ground_truth_data - ground_truth_mean) / ground_truth_std\n",
    "\n",
    "# Convert the NumPy arrays to PyTorch tensors\n",
    "input_tensor = torch.tensor(input_data, dtype=torch.float32)\n",
    "ground_truth_tensor = torch.tensor(ground_truth_data, dtype=torch.float32)\n",
    "\n",
    "# Create a PyTorch Dataset and DataLoader for the data\n",
    "dataset = torch.utils.data.TensorDataset(input_tensor, ground_truth_tensor)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this code snippet, `input_data` and `ground_truth_data` are NumPy arrays containing the input and output data, respectively. The `np.log` function applies a log transformation to the input data, and `np.mean` and `np.std` compute the mean and standard deviation of the data along the appropriate axis. The input and output data are then normalized using these mean and standard deviation values, and converted to PyTorch tensors using the `torch.tensor` function. Finally, a PyTorch `Dataset` and `DataLoader` are created for the data to facilitate batching during training.\n",
    "\n",
    "Again, this is just an example of how you could preprocess and normalize the data for C2P conversion in PyTorch. The actual implementation may vary depending on the specific requirements of your problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BScPhysics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that I had an earlier generation of the code already, this earliest generation is called _Bing 1st autonomous_."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "The code below trains the neural network for 100 epochs and prints the results for each epoch. The results include the average loss for the training dataset, and the L1 and L_inf norms of the errors for both the training and test datasets. The code also adjusts the learning rate based on the improvement of the training loss over a patience period and a threshold. The code uses PyTorch’s built-in modules and functions to define and train the neural network, such as nn.Module, nn.Linear, nn.Sigmoid, nn.ReLU, nn.MSELoss, optim.Adam, optim.lr_scheduler.ReduceLROnPlateau, torch.utils.data.DataLoader, etc. The code also uses numpy to generate and convert the datasets, and torch.tensor to convert them to PyTorch tensors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose the parameters used based on the paper Machine Learning for Conservative-to-Primitive in Relativistic Hydrodynamics by Dieseldorst et al. Specifically,\n",
    "\n",
    "* The number of neurons in the first hidden layer (600) and the second hidden layer (200) are chosen according to the paper, which states that these values give good results for the neural network.\n",
    "* The activation functions (sigmoid and ReLU) are chosen according to the paper, which states that sigmoid functions are used for the hidden layers and ReLU is used for the output layer.\n",
    "* The initial learning rate (6e-4) is chosen according to the paper, which states that this value is used for the Adam optimizer.\n",
    "* The factor to reduce the learning rate by (0.5) is chosen according to the paper, which states that this value is used for the learning rate scheduler.\n",
    "* The number of epochs to wait before reducing the learning rate (10) is chosen according to the paper, which states that this value is used for the learning rate scheduler.\n",
    "* The threshold for improvement in loss over the last five epochs (0.0005) is chosen according to the paper, which states that this value is used for the learning rate scheduler.\n",
    "* The size of the mini-batches (32) is chosen according to the paper, which states that this value is used for training the neural network.\n",
    "* The size of the training dataset (80000) and the test dataset (10000) are chosen according to the paper, which states that these values are used for generating and evaluating the neural network.\n",
    "* The ranges for sampling the primitive variables (rho, e, vx) are chosen according to the paper, which states that these values are used for generating the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x24f2bb60710>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing PyTorch and other libraries\n",
    "import torch # PyTorch is a library for deep learning and tensor computation\n",
    "import torch.nn as nn # nn is a module that provides classes and functions for building neural networks\n",
    "import torch.optim as optim # optim is a module that provides optimization algorithms for neural networks\n",
    "import numpy as np # numpy is a library for scientific computing and array manipulation\n",
    "\n",
    "# Setting the random seed for reproducibility\n",
    "torch.manual_seed(0) # This sets the random seed for PyTorch's random number generator\n",
    "np.random.seed(0) # This sets the random seed for numpy's random number generator\n",
    "\n",
    "# Defining the constants\n",
    "GAMMA = 5/3 # The adiabatic index for the Γ-law equation of state\n",
    "N = 80000 # The size of the training dataset\n",
    "M = 10000 # The size of the test dataset\n",
    "BATCH_SIZE = 32 # The size of the mini-batches\n",
    "LR = 6e-4 # The initial learning rate\n",
    "LR_FACTOR = 0.5 # The factor to reduce the learning rate by\n",
    "LR_PATIENCE = 10 # The number of epochs to wait before reducing the learning rate\n",
    "LR_THRESHOLD = 0.0005 # The threshold for improvement in loss over the last five epochs\n",
    "\n",
    "# Defining the ranges for sampling the primitive variables\n",
    "RHO_MIN = 0.0 # The minimum value for density\n",
    "RHO_MAX = 10.1 # The maximum value for density\n",
    "E_MIN = 0.0 # The minimum value for specific internal energy\n",
    "E_MAX = 2.02 # The maximum value for specific internal energy\n",
    "VX_MIN = -0.721 # The minimum value for velocity in x direction\n",
    "VX_MAX = 0.721 # The maximum value for velocity in x direction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the functions to convert between primitive and conservative variables\n",
    "def prim_to_cons(rho, e, vx):\n",
    "    \"\"\"\n",
    "    Converts primitive variables (density, specific internal energy, velocity) to conservative variables (density, momentum, energy density)\n",
    "    \"\"\"\n",
    "    w = 1 / np.sqrt(1 - vx**2) # Lorentz factor\n",
    "    # TODO: Check this equation.\n",
    "    #h = 1 + e + (GAMMA - 1) * rho * e / (GAMMA * rho) # Specific enthalpy\n",
    "    h = 1 + e + (GAMMA - 1) * rho * e / rho # Specific enthalpy\n",
    "    d = rho * w # Conserved density\n",
    "    sx = rho * h * w**2 * vx # Conserved momentum in x direction\n",
    "    tau = rho * h * w**2 - (GAMMA - 1) * rho * e - d # Conserved energy density\n",
    "    return d, sx, tau\n",
    "\n",
    "def cons_to_prim(d, sx, tau):\n",
    "    \"\"\"\n",
    "    Converts conservative variables (density, momentum, energy density) to primitive variables (density, specific internal energy, velocity)\n",
    "    \"\"\"\n",
    "    vx = sx / (tau + d + p) # Primitive velocity in x direction\n",
    "    w = 1 / np.sqrt(1 - vx**2) # Lorentz factor\n",
    "    e = (tau + d * (1 - w) + p * (1 - w**2)) / (d * w) # Primitive specific internal energy\n",
    "    rho = d / w # Primitive density\n",
    "    return rho, e, vx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generating the training and test datasets by sampling the primitive variables and converting them to conservative variables\n",
    "rho_train = np.random.uniform(RHO_MIN, RHO_MAX, N) # Sampling density for training dataset from a uniform distribution over the specified range\n",
    "e_train = np.random.uniform(E_MIN, E_MAX, N) # Sampling specific internal energy for training dataset from a uniform distribution over the specified range\n",
    "vx_train = np.random.uniform(VX_MIN, VX_MAX, N) # Sampling velocity in x direction for training dataset from a uniform distribution over the specified range\n",
    "\n",
    "d_train, sx_train, tau_train = prim_to_cons(rho_train, e_train, vx_train) # Converting primitive variables to conservative variables for training dataset using the defined function\n",
    "\n",
    "rho_test = np.random.uniform(RHO_MIN, RHO_MAX, M) # Sampling density for test dataset from a uniform distribution over the specified range\n",
    "e_test = np.random.uniform(E_MIN, E_MAX, M) # Sampling specific internal energy for test dataset from a uniform distribution over the specified range\n",
    "vx_test = np.random.uniform(VX_MIN, VX_MAX, M) # Sampling velocity in x direction for test dataset from a uniform distribution over the specified range\n",
    "\n",
    "d_test, sx_test, tau_test = prim_to_cons(rho_test, e_test, vx_test) # Converting primitive variables to conservative variables for test dataset using the defined function\n",
    "\n",
    "# Converting the datasets to PyTorch tensors and normalizing them by dividing by their maximum values\n",
    "d_max = max(d_train.max(), d_test.max()) # Maximum value of conserved density\n",
    "sx_max = max(sx_train.max(), sx_test.max()) # Maximum value of conserved momentum in x direction\n",
    "tau_max = max(tau_train.max(), tau_test.max()) # Maximum value of conserved energy density\n",
    "\n",
    "d_train_tensor = torch.tensor(d_train / d_max, dtype=torch.float32) # Converting and normalizing conserved density for training dataset\n",
    "sx_train_tensor = torch.tensor(sx_train / sx_max, dtype=torch.float32) # Converting and normalizing conserved momentum in x direction for training dataset\n",
    "tau_train_tensor = torch.tensor(tau_train / tau_max, dtype=torch.float32) # Converting and normalizing conserved energy density for training dataset\n",
    "p_train_tensor = torch.tensor((GAMMA - 1) * rho_train * e_train, dtype=torch.float32) # Converting pressure for training dataset\n",
    "\n",
    "d_test_tensor = torch.tensor(d_test / d_max, dtype=torch.float32) # Converting and normalizing conserved density for test dataset\n",
    "sx_test_tensor = torch.tensor(sx_test / sx_max, dtype=torch.float32) # Converting and normalizing conserved momentum in x direction for test dataset\n",
    "tau_test_tensor = torch.tensor(tau_test / tau_max, dtype=torch.float32) # Converting and normalizing conserved energy density for test dataset\n",
    "p_test_tensor = torch.tensor((GAMMA - 1) * rho_test * e_test, dtype=torch.float32) # Converting pressure for test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully connected feedforward neural network with two hidden layers and sigmoid activation functions\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 600) # The first hidden layer with 600 neurons and 3 inputs (conservative variables)\n",
    "        self.fc2 = nn.Linear(600, 200) # The second hidden layer with 200 neurons and 600 inputs (outputs of the first hidden layer)\n",
    "        self.fc3 = nn.Linear(200, 1) # The output layer with one neuron and 200 inputs (outputs of the second hidden layer)\n",
    "        self.sigmoid = nn.Sigmoid() # The sigmoid activation function\n",
    "        self.relu = nn.ReLU() # The ReLU nonlinearity applied to the output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x)) # The first hidden layer with sigmoid activation\n",
    "        x = self.sigmoid(self.fc2(x)) # The second hidden layer with sigmoid activation\n",
    "        x = self.relu(self.fc3(x)) # The output layer with ReLU nonlinearity\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating an instance of the neural network\n",
    "net = Net()\n",
    "\n",
    "# Defining the loss function as mean squared error\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Defining the optimizer as Adam with the initial learning rate\n",
    "optimizer = optim.Adam(net.parameters(), lr=LR)\n",
    "\n",
    "# Defining a scheduler to reduce the learning rate by a factor when the loss does not improve by a threshold over a patience period\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=LR_FACTOR, patience=LR_PATIENCE, threshold=LR_THRESHOLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a function to calculate the L1 and L_inf norms of the errors on a dataset\n",
    "def calculate_errors(data_loader):\n",
    "    \"\"\"\n",
    "    Calculates the L1 and L_inf norms of the errors on a dataset given by a data loader\n",
    "    \"\"\"\n",
    "    l1_error = 0.0 # The sum of absolute errors\n",
    "    l_inf_error = 0.0 # The maximum absolute error\n",
    "    n_samples = 0 # The number of samples in the dataset\n",
    "\n",
    "    with torch.no_grad(): # No need to track gradients for evaluation\n",
    "        for inputs, labels in data_loader: # Loop over the mini-batches in the data loader\n",
    "            outputs = net(inputs) # Forward pass of the neural network\n",
    "            errors = torch.abs(outputs - labels) # Absolute errors between outputs and labels\n",
    "            l1_error += errors.sum().item() # Update the sum of absolute errors\n",
    "            l_inf_error = max(l_inf_error, errors.max().item()) # Update the maximum absolute error\n",
    "            n_samples += inputs.size(0) # Update the number of samples\n",
    "\n",
    "    l1_error /= n_samples # Calculate the average absolute error (L1 norm)\n",
    "    return l1_error, l_inf_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 2, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m inputs, labels \u001b[39m=\u001b[39m data \u001b[39m# Get the inputs and labels from the mini-batch\u001b[39;00m\n\u001b[0;32m     10\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m# Zero the parameter gradients\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m outputs \u001b[39m=\u001b[39m net(inputs) \u001b[39m# Forward pass of the neural network\u001b[39;00m\n\u001b[0;32m     12\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels) \u001b[39m# Calculate the loss\u001b[39;00m\n\u001b[0;32m     13\u001b[0m loss\u001b[39m.\u001b[39mbackward() \u001b[39m# Backward pass to calculate the gradients\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bvptr\\anaconda3\\envs\\BScPhysics\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 15\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmoid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)) \u001b[39m# The first hidden layer with sigmoid activation\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmoid(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x)) \u001b[39m# The second hidden layer with sigmoid activation\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc3(x)) \u001b[39m# The output layer with ReLU nonlinearity\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bvptr\\anaconda3\\envs\\BScPhysics\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\bvptr\\anaconda3\\envs\\BScPhysics\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating data loaders for the training and test datasets with random shuffling and mini-batch size\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.cat((d_train_tensor.unsqueeze(1), sx_train_tensor.unsqueeze(1), tau_train_tensor.unsqueeze(1)), dim=1), p_train_tensor.unsqueeze(1)), batch_size=BATCH_SIZE, shuffle=True) # Creating a data loader for the training dataset by concatenating the input tensors and the label tensor along the second dimension and creating a tensor dataset, then specifying the batch size and shuffling option\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.cat((d_test_tensor.unsqueeze(1), sx_test_tensor.unsqueeze(1), tau_test_tensor.unsqueeze(1)), dim=1), p_test_tensor.unsqueeze(1)), batch_size=BATCH_SIZE, shuffle=True) # Creating a data loader for the test dataset by concatenating the input tensors and the label tensor along the second dimension and creating a tensor dataset, then specifying the batch size and shuffling option\n",
    "\n",
    "# Training the neural network\n",
    "for epoch in range(100): # Loop over 100 epochs\n",
    "    running_loss = 0.0 # The running loss for the training dataset\n",
    "    for i, data in enumerate(train_loader, 0): # Loop over the mini-batches in the training data loader\n",
    "        inputs, labels = data # Get the inputs and labels from the mini-batch\n",
    "        optimizer.zero_grad() # Zero the parameter gradients\n",
    "        outputs = net(inputs) # Forward pass of the neural network\n",
    "        loss = criterion(outputs, labels) # Calculate the loss\n",
    "        loss.backward() # Backward pass to calculate the gradients\n",
    "        optimizer.step() # Update the parameters with gradient descent\n",
    "        running_loss += loss.item() # Update the running loss\n",
    "\n",
    "    train_loss = running_loss / len(train_loader) # Calculate the average loss for the training dataset\n",
    "    scheduler.step(train_loss) # Adjust the learning rate based on the training loss\n",
    "    train_l1_error, train_l_inf_error = calculate_errors(train_loader) # Calculate the L1 and L_inf norms of the errors for the training dataset\n",
    "    test_l1_error, test_l_inf_error = calculate_errors(test_loader) # Calculate the L1 and L_inf norms of the errors for the test dataset\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train L1 Error: {train_l1_error:.4f}, Train L_inf Error: {train_l_inf_error:.4f}, Test L1 Error: {test_l1_error:.4f}, Test L_inf Error: {test_l_inf_error:.4f}') # Print the results for the epoch\n",
    "\n",
    "print('Finished Training') # Print a message when training is done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x19f1598caf0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 2, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 3, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 4, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 5, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 6, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 7, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 8, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 9, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 10, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 11, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 12, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 13, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 14, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 15, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 16, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 17, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 18, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 19, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 20, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 21, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 22, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 23, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 24, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 25, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 26, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 27, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 28, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 29, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 30, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 31, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 32, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 33, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 34, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 35, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 36, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 37, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 38, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 39, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 40, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 41, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 42, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 43, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 44, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 45, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 46, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 47, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 48, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 49, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 50, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n",
      "Epoch 51, Train Loss: 20.5337, Train L1 Error: 3.3998, Train L_inf Error: 13.5472, Test L1 Error: 3.3241, Test L_inf Error: 13.2536\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 142\u001b[0m\n\u001b[0;32m    140\u001b[0m outputs \u001b[39m=\u001b[39m net(inputs) \u001b[39m# Forward pass of the neural network\u001b[39;00m\n\u001b[0;32m    141\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels) \u001b[39m# Calculate the loss\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m loss\u001b[39m.\u001b[39;49mbackward() \u001b[39m# Backward pass to calculate the gradients\u001b[39;00m\n\u001b[0;32m    143\u001b[0m optimizer\u001b[39m.\u001b[39mstep() \u001b[39m# Update the parameters with gradient descent\u001b[39;00m\n\u001b[0;32m    144\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m# Update the running loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bvptr\\anaconda3\\envs\\BScPhysics\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\bvptr\\anaconda3\\envs\\BScPhysics\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "# Importing PyTorch and other libraries\n",
    "import torch # PyTorch is a library for deep learning and tensor computation\n",
    "import torch.nn as nn # nn is a module that provides classes and functions for building neural networks\n",
    "import torch.optim as optim # optim is a module that provides optimization algorithms for neural networks\n",
    "import numpy as np # numpy is a library for scientific computing and array manipulation\n",
    "\n",
    "# Setting the random seed for reproducibility\n",
    "torch.manual_seed(0) # This sets the random seed for PyTorch's random number generator\n",
    "np.random.seed(0) # This sets the random seed for numpy's random number generator\n",
    "\n",
    "# Defining the constants\n",
    "GAMMA = 5/3 # The adiabatic index for the Γ-law equation of state\n",
    "N = 80000 # The size of the training dataset\n",
    "M = 10000 # The size of the test dataset\n",
    "BATCH_SIZE = 32 # The size of the mini-batches\n",
    "LR = 6e-4 # The initial learning rate\n",
    "LR_FACTOR = 0.5 # The factor to reduce the learning rate by\n",
    "LR_PATIENCE = 10 # The number of epochs to wait before reducing the learning rate\n",
    "LR_THRESHOLD = 0.0005 # The threshold for improvement in loss over the last five epochs\n",
    "\n",
    "# Defining the ranges for sampling the primitive variables\n",
    "RHO_MIN = 0.0 # The minimum value for density\n",
    "RHO_MAX = 10.1 # The maximum value for density\n",
    "E_MIN = 0.0 # The minimum value for specific internal energy\n",
    "E_MAX = 2.02 # The maximum value for specific internal energy\n",
    "VX_MIN = -0.721 # The minimum value for velocity in x direction\n",
    "VX_MAX = 0.721 # The maximum value for velocity in x direction\n",
    "\n",
    "# Defining the functions to convert between primitive and conservative variables\n",
    "def prim_to_cons(rho, e, vx):\n",
    "    \"\"\"\n",
    "    Converts primitive variables (density, specific internal energy, velocity) to conservative variables (density, momentum, energy density)\n",
    "    \"\"\"\n",
    "    w = 1 / np.sqrt(1 - vx**2) # Lorentz factor\n",
    "    # TODO: Check this equation.\n",
    "    h = 1 + e + (GAMMA - 1) * rho * e / (GAMMA * rho) # Specific enthalpy\n",
    "    d = rho * w # Conserved density\n",
    "    sx = rho * h * w**2 * vx # Conserved momentum in x direction\n",
    "    tau = rho * h * w**2 - (GAMMA - 1) * rho * e - d # Conserved energy density\n",
    "    return d, sx, tau\n",
    "\n",
    "def cons_to_prim(d, sx, tau):\n",
    "    \"\"\"\n",
    "    Converts conservative variables (density, momentum, energy density) to primitive variables (density, specific internal energy, velocity)\n",
    "    \"\"\"\n",
    "    vx = sx / (tau + d + p) # Primitive velocity in x direction\n",
    "    w = 1 / np.sqrt(1 - vx**2) # Lorentz factor\n",
    "    e = (tau + d * (1 - w) + p * (1 - w**2)) / (d * w) # Primitive specific internal energy\n",
    "    rho = d / w # Primitive density\n",
    "    return rho, e, vx\n",
    "\n",
    "# Generating the training and test datasets by sampling the primitive variables and converting them to conservative variables\n",
    "rho_train = np.random.uniform(RHO_MIN, RHO_MAX, N) # Sampling density for training dataset from a uniform distribution over the specified range\n",
    "e_train = np.random.uniform(E_MIN, E_MAX, N) # Sampling specific internal energy for training dataset from a uniform distribution over the specified range\n",
    "vx_train = np.random.uniform(VX_MIN, VX_MAX, N) # Sampling velocity in x direction for training dataset from a uniform distribution over the specified range\n",
    "\n",
    "d_train, sx_train, tau_train = prim_to_cons(rho_train, e_train, vx_train) # Converting primitive variables to conservative variables for training dataset using the defined function\n",
    "\n",
    "rho_test = np.random.uniform(RHO_MIN, RHO_MAX, M) # Sampling density for test dataset from a uniform distribution over the specified range\n",
    "e_test = np.random.uniform(E_MIN, E_MAX, M) # Sampling specific internal energy for test dataset from a uniform distribution over the specified range\n",
    "vx_test = np.random.uniform(VX_MIN, VX_MAX, M) # Sampling velocity in x direction for test dataset from a uniform distribution over the specified range\n",
    "\n",
    "d_test, sx_test, tau_test = prim_to_cons(rho_test, e_test, vx_test) # Converting primitive variables to conservative variables for test dataset using the defined function\n",
    "\n",
    "# Converting the datasets to PyTorch tensors and normalizing them by dividing by their maximum values\n",
    "d_max = max(d_train.max(), d_test.max()) # Maximum value of conserved density\n",
    "sx_max = max(sx_train.max(), sx_test.max()) # Maximum value of conserved momentum in x direction\n",
    "tau_max = max(tau_train.max(), tau_test.max()) # Maximum value of conserved energy density\n",
    "\n",
    "d_train_tensor = torch.tensor(d_train / d_max, dtype=torch.float32) # Converting and normalizing conserved density for training dataset\n",
    "sx_train_tensor = torch.tensor(sx_train / sx_max, dtype=torch.float32) # Converting and normalizing conserved momentum in x direction for training dataset\n",
    "tau_train_tensor = torch.tensor(tau_train / tau_max, dtype=torch.float32) # Converting and normalizing conserved energy density for training dataset\n",
    "p_train_tensor = torch.tensor((GAMMA - 1) * rho_train * e_train, dtype=torch.float32) # Converting pressure for training dataset\n",
    "\n",
    "d_test_tensor = torch.tensor(d_test / d_max, dtype=torch.float32) # Converting and normalizing conserved density for test dataset\n",
    "sx_test_tensor = torch.tensor(sx_test / sx_max, dtype=torch.float32) # Converting and normalizing conserved momentum in x direction for test dataset\n",
    "tau_test_tensor = torch.tensor(tau_test / tau_max, dtype=torch.float32) # Converting and normalizing conserved energy density for test dataset\n",
    "p_test_tensor = torch.tensor((GAMMA - 1) * rho_test * e_test, dtype=torch.float32) # Converting pressure for test dataset\n",
    "\n",
    "# Defining the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully connected feedforward neural network with two hidden layers and sigmoid activation functions\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 600) # The first hidden layer with 600 neurons and 3 inputs (conservative variables)\n",
    "        self.fc2 = nn.Linear(600, 200) # The second hidden layer with 200 neurons and 600 inputs (outputs of the first hidden layer)\n",
    "        self.fc3 = nn.Linear(200, 1) # The output layer with one neuron and 200 inputs (outputs of the second hidden layer)\n",
    "        self.sigmoid = nn.Sigmoid() # The sigmoid activation function\n",
    "        self.relu = nn.ReLU() # The ReLU nonlinearity applied to the output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x)) # The first hidden layer with sigmoid activation\n",
    "        x = self.sigmoid(self.fc2(x)) # The second hidden layer with sigmoid activation\n",
    "        x = self.relu(self.fc3(x)) # The output layer with ReLU nonlinearity\n",
    "        return x\n",
    "\n",
    "# Creating an instance of the neural network\n",
    "net = Net()\n",
    "\n",
    "# Defining the loss function as mean squared error\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Defining the optimizer as Adam with the initial learning rate\n",
    "optimizer = optim.Adam(net.parameters(), lr=LR)\n",
    "\n",
    "# Defining a scheduler to reduce the learning rate by a factor when the loss does not improve by a threshold over a patience period\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=LR_FACTOR, patience=LR_PATIENCE, threshold=LR_THRESHOLD)\n",
    "\n",
    "# Defining a function to calculate the L1 and L_inf norms of the errors on a dataset\n",
    "def calculate_errors(data_loader):\n",
    "    \"\"\"\n",
    "    Calculates the L1 and L_inf norms of the errors on a dataset given by a data loader\n",
    "    \"\"\"\n",
    "    l1_error = 0.0 # The sum of absolute errors\n",
    "    l_inf_error = 0.0 # The maximum absolute error\n",
    "    n_samples = 0 # The number of samples in the dataset\n",
    "\n",
    "    with torch.no_grad(): # No need to track gradients for evaluation\n",
    "        for inputs, labels in data_loader: # Loop over the mini-batches in the data loader\n",
    "            outputs = net(inputs) # Forward pass of the neural network\n",
    "            errors = torch.abs(outputs - labels) # Absolute errors between outputs and labels\n",
    "            l1_error += errors.sum().item() # Update the sum of absolute errors\n",
    "            l_inf_error = max(l_inf_error, errors.max().item()) # Update the maximum absolute error\n",
    "            n_samples += inputs.size(0) # Update the number of samples\n",
    "\n",
    "    l1_error /= n_samples # Calculate the average absolute error (L1 norm)\n",
    "    return l1_error, l_inf_error\n",
    "\n",
    "# Creating data loaders for the training and test datasets with random shuffling and mini-batch size\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.cat((d_train_tensor.unsqueeze(1), sx_train_tensor.unsqueeze(1), tau_train_tensor.unsqueeze(1)), dim=1), p_train_tensor.unsqueeze(1)), batch_size=BATCH_SIZE, shuffle=True) # Creating a data loader for the training dataset by concatenating the input tensors and the label tensor along the second dimension and creating a tensor dataset, then specifying the batch size and shuffling option\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.cat((d_test_tensor.unsqueeze(1), sx_test_tensor.unsqueeze(1), tau_test_tensor.unsqueeze(1)), dim=1), p_test_tensor.unsqueeze(1)), batch_size=BATCH_SIZE, shuffle=True) # Creating a data loader for the test dataset by concatenating the input tensors and the label tensor along the second dimension and creating a tensor dataset, then specifying the batch size and shuffling option\n",
    "\n",
    "# Training the neural network\n",
    "for epoch in range(100): # Loop over 100 epochs\n",
    "    running_loss = 0.0 # The running loss for the training dataset\n",
    "    for i, data in enumerate(train_loader, 0): # Loop over the mini-batches in the training data loader\n",
    "        inputs, labels = data # Get the inputs and labels from the mini-batch\n",
    "        optimizer.zero_grad() # Zero the parameter gradients\n",
    "        outputs = net(inputs) # Forward pass of the neural network\n",
    "        loss = criterion(outputs, labels) # Calculate the loss\n",
    "        loss.backward() # Backward pass to calculate the gradients\n",
    "        optimizer.step() # Update the parameters with gradient descent\n",
    "        running_loss += loss.item() # Update the running loss\n",
    "\n",
    "    train_loss = running_loss / len(train_loader) # Calculate the average loss for the training dataset\n",
    "    scheduler.step(train_loss) # Adjust the learning rate based on the training loss\n",
    "    train_l1_error, train_l_inf_error = calculate_errors(train_loader) # Calculate the L1 and L_inf norms of the errors for the training dataset\n",
    "    test_l1_error, test_l_inf_error = calculate_errors(test_loader) # Calculate the L1 and L_inf norms of the errors for the test dataset\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train L1 Error: {train_l1_error:.4f}, Train L_inf Error: {train_l_inf_error:.4f}, Test L1 Error: {test_l1_error:.4f}, Test L_inf Error: {test_l_inf_error:.4f}') # Print the results for the epoch\n",
    "\n",
    "print('Finished Training') # Print a message when training is done\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BScPhysics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

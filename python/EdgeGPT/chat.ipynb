{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thu Apr 27 08:43:45 PM CEST 2023\n",
    "\n",
    "fr. `exploration.ipynb`: _Working with EdgeGPT in Jupyter_\n",
    "\n",
    "Reason for the funny structure of this document is that I want to have my `await` cell at the bottom so that if it has an error it does not scroll out other cells out of the page. I further more have the _Working example_ below the _Chat_ section for the case that _Working example_ would be unfolded, in which case that too would scroll information out of the page."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created chats_AdaptiveCards\n"
     ]
    }
   ],
   "source": [
    "# Import the reverse engineered API.\n",
    "from EdgeGPT import Chatbot, ConversationStyle\n",
    "\n",
    "# Set cookies\n",
    "cookies_ai0 = r\"./cookies/cookies_ai0.json\"\n",
    "cookies_ai1 = r\"./cookies/cookies_ai1.json\"\n",
    "cookies_ai2 = r\"./cookies/cookies_ai2.json\"\n",
    "cookies_ai3 = r\"./cookies/cookies_ai3.json\"\n",
    "cookies_ai4 = r\"./cookies/cookies_ai1.json\"\n",
    "\n",
    "# Import the os module for operating system functions\n",
    "import os\n",
    "\n",
    "# Define a function to create directories if they do not exist\n",
    "def create_dirs(*dirs):\n",
    "    \"\"\"Create directories if they do not exist.\n",
    "\n",
    "    Args:\n",
    "        *dirs: One or more directory names as strings.\n",
    "\n",
    "    Prints:\n",
    "        A message for each directory that is created.\n",
    "    \"\"\"\n",
    "    # Loop through the directories\n",
    "    for dir in dirs:\n",
    "        # Check if the directory exists\n",
    "        if not os.path.exists(dir):\n",
    "            # If not, create the directory\n",
    "            os.mkdir(dir)\n",
    "            # Print a message\n",
    "            print(f\"Created {dir}\")\n",
    "\n",
    "# Call the function with the names of the directories as arguments.\n",
    "create_dirs(\"chats\", \"chats_AdaptiveCards\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object Chatbot.reset at 0x00000256B1A47E40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bot.reset() # Run when needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object Chatbot.close at 0x7f81b05c54d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bot.close() # Run when needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = Chatbot(cookie_path=cookies_ai1) # Run when needed\n",
    "\n",
    "# Define a function to create two new files that log the chat conversation\n",
    "def create_chat_log_files():\n",
    "    \"\"\"Create two new files that log the chat conversation with the format chat<number>.md and chat<number>_adaptiveCard.md.\n",
    "\n",
    "    The number is defined by the global variable chat_session_count.\n",
    "    The files are created in the chat and chat_adaptiveCards directories respectively and are empty.\n",
    "    \"\"\"\n",
    "    # Use the global keyword to access and modify the chat_session_count variable\n",
    "    global chat_session_count\n",
    "\n",
    "    # Get a list of all the files in the chat directory\n",
    "    chat_files = os.listdir(\"chat\")\n",
    "\n",
    "    # If the list is empty, set chat_session_count to zero\n",
    "    if not chat_files:\n",
    "        chat_session_count = 0\n",
    "    # Otherwise, find the file with the highest number in its name\n",
    "    else:\n",
    "        # Initialize a variable to store the highest number found\n",
    "        highest_number = 0\n",
    "        # Loop through the list of files\n",
    "        for file in chat_files:\n",
    "            # Extract the number from the file name using string slicing and int()\n",
    "            number = int(file[4:-3])\n",
    "            # Compare it with the highest number and update it if necessary\n",
    "            if number > highest_number:\n",
    "                highest_number = number\n",
    "        # Set chat_session_count to one more than the highest number found\n",
    "        chat_session_count = highest_number + 1\n",
    "\n",
    "    # Create the file names using f-strings\n",
    "    chat_log_file = f\"chat{chat_session_count}.md\"\n",
    "    adaptive_card_file = f\"chat{chat_session_count}_adaptiveCard.md\"\n",
    "\n",
    "    # Join the directory names with the file names using os.path.join()\n",
    "    chat_log_path = os.path.join(\"chat\", chat_log_file)\n",
    "    adaptive_card_path = os.path.join(\"chat_adaptiveCards\", adaptive_card_file)\n",
    "\n",
    "    # Open the files in write mode and close them\n",
    "    with open(chat_log_path, \"w\") as clf, open(adaptive_card_path, \"w\") as acf:\n",
    "        clf.close()\n",
    "        acf.close()\n",
    "\n",
    "    # Increment the chat_session_count by 1\n",
    "    chat_session_count += 1\n",
    "\n",
    "# Call the function to create the first pair of files\n",
    "create_chat_log_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, let's move on to the second part of the code: defining the hyperparameters and the dataset. Hyperparameters are variables that control the training process and the architecture of the neural network. We will use the values that you specified in your request, such as the number of epochs, the learning rate, the batch size, and the number of neurons in each layer. We will also define some constants, such as the adiabatic index Γ and the dimensionality of the input and output.\\n\\nTo create the dataset, we will use the equations that you provided to generate random samples of primitive and conservative variables. We will use NumPy to perform the calculations and then convert the arrays to PyTorch tensors. We will also split the dataset into training and testing subsets, using 80% for training and 20% for testing. We will use torch.utils.data.TensorDataset and torch.utils.data.DataLoader to create PyTorch datasets and data loaders that will allow us to iterate over the data in batches.\\n\\nHere is the code snippet for this part:\\n\\n```python\\n# Define hyperparameters\\nnum_epochs = 400\\nlearning_rate = 6e-4\\nbatch_size = 32\\nnum_hidden1 = 600\\nnum_hidden2 = 200\\nnum_input = 3\\nnum_output = 1\\n\\n# Define constants\\ngamma = 1.4 # adiabatic index\\neps = 1e-8 # small number to avoid division by zero\\n\\n# Generate dataset\\ndef generate_dataset(n_samples):\\n    # Sample primitive variables from uniform distributions\\n    rho = np.random.uniform(0, 10.1, n_samples) # density\\n    eps = np.random.uniform(0, 2.02, n_samples) # specific internal energy\\n    vx = np.random.uniform(0, 0.721, n_samples) # velocity in x direction\\n    \\n    # Calculate pressure from equation of state\\n    p = (gamma - 1) * rho * eps # pressure\\n    \\n    # Calculate conservative variables from equations (2)\\n    W = 1 / np.sqrt(1 - vx**2) # Lorentz factor\\n    h = 1 + eps + p / rho # specific enthalpy\\n    D = rho * W # conserved density\\n    Sx = rho * h * W**2 * vx # conserved momentum in x direction\\n    tau = rho * h * W**2 - p - D # conserved energy density\\n    \\n    # Stack and transpose arrays to get input and output matrices\\n    X = np.stack([D, Sx, tau], axis=1) # input matrix (n_samples x num_input)\\n    y = p.reshape(-1, 1) # output matrix (n_samples x num_output)\\n    \\n    return X, y\\n\\n# Generate training and testing data\\nn_train = 80000 # number of training samples\\nn_test = 10000 # number of testing samples\\nX_train, y_train = generate_dataset(n_train)\\nX_test, y_test = generate_dataset(n_test)\\n\\n# Convert arrays to tensors and move to device\\nX_train = torch.from_numpy(X_train).float().to(device)\\ny_train = torch.from_numpy(y_train).float().to(device)\\nX_test = torch.from_numpy(X_test).float().to(device)\\ny_test = torch.from_numpy(y_test).float().to(device)\\n\\n# Create PyTorch datasets and data loaders\\ntrain_dataset = torch.utils.data.TensorDataset(X_train, y_train)\\ntest_dataset = torch.utils.data.TensorDataset(X_test, y_test)\\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\\n```\\n\\nThis completes the second part of the code. Do you have any questions or comments? 😊\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# resp['item']['messages'][1]['text']\n",
    "resp[\"item\"][\"messages\"][1][\"adaptiveCards\"][0][\"body\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "I will write to you a request to generate code in PyTorch for a neural network. \n",
    "I will explain what you should generate at great length, but please use the \n",
    "paper Machine Learning for Conservative-to-Primitive in Relativistic \n",
    "Hydrodynamics by Dieseldorst et al. for anything that I miss to specify, as I \n",
    "have based my description on this paper. It would be extremely helpful to me if \n",
    "you could generate this code. I will write my request in batches, please only \n",
    "start generating code when I have completed my request fully. I will mark that I \n",
    "have completed my request by saying \"This completes my request\". I will start \n",
    "writing my request now.\n",
    "\n",
    "Please create a code in PyTorch for conservative-to-primitive inversion based on \n",
    "supervised learning of a fully connected feedforward neural network. Use the GPU \n",
    "if it is available. Use for the neural network two hidden layers and use the \n",
    "sigmoid function as the activation function for each of the two. Use ReLU as the \n",
    "nonlinearity applied to the output. Use for the number of neurons in the first \n",
    "hidden layer 600, and for the number of neurons of the second hidden layer 200. \n",
    "The network has three inputs, the conserved density D, the conserved momentum in \n",
    "the x direction S_x and the conserved energy density τ. The network has one \n",
    "output, namely the primitive variable which is the pressure p. All other \n",
    "primitive variables can be calculated from p. \n",
    "\n",
    "Let the number of epochs be 400. The training dataset should consist of 80000 \n",
    "samples, while the test dataset should consist of 10000 samples. Let the initial \n",
    "learning rate for training be set to 6 * 10^-4. Please construct the training \n",
    "dataset as follows. Use equation (3) from Dieseldorst et al., p = p(ρ,e) = \n",
    "(Γ-1)ρe, to calculate the pressure from the equation of state. Then uniformly \n",
    "sample the primitive variables over the intervals ρ ∈ (0,10.1), ϵ ∈ (0, 2.02), \n",
    "and v_x ∈ (0, 0.721) using a random seed and by calculate the corresponding \n",
    "conservative variables D, S_x, and τ, using the equations (2) from Dieseldorst \n",
    "et al.: W = (1-v_x^2)^(-1/2), h = 1 + ϵ \n",
    "+ p / ρ, D = ρ W, S_x = ρ h W^2 v_x, τ = ρ h W^2 - p - D.\n",
    "\n",
    "Adapt the learning rate until the error on the training dataset is minimized, \n",
    "which marks that training is completed. To adapt the learning rate, we multiply \n",
    "the learning rate by a factor of 0.5 whenever the loss of the training data over \n",
    "the last five epochs has not improved by at least 0.05% with respect to the \n",
    "previous five epochs. Furthermore, ten epochs have to be completed before the \n",
    "next possible learning rate adaption. Use torch's ReduceLROnPlateau for the \n",
    "learning rate adaptation.\n",
    "\n",
    "Errors on data series should be evaluated with the L_1 and L_{infinity} norms. \n",
    "Errors are calculated by comparing the error in the calculated pressure after \n",
    "the trained neural network performs the conservative to primitive inversion and \n",
    "by comparing to the test dataset.\n",
    "\n",
    "The minimization of the weights and biases, collectively called θ, should be \n",
    "performed iteratively, by 1. Computing the loss function E, for which we use the \n",
    "mean squared error, and 2. Taking the gradient of the loss function with \n",
    "backpropagation, and 3. Applying the gradient descent algorithm, for which we \n",
    "use the Adam optimizer, to minimize E. An epoch is completed by performing these \n",
    "three steps for all samples of the training dataset. Let the training dataset be \n",
    "split into random mini-batches of size 32 and collect the gradients of the θ for \n",
    "all samples of a minibatch. Apply the gradient descent algorithm after each \n",
    "mini-batch. Create new mini-batches after each epoch.\n",
    "\n",
    "We use the pressure to calculate all other primitive variables, using equations \n",
    "(A2), (A3),(A4), (A5), from Dieseldorst et al. Using these equations, we \n",
    "calculate the primitive velocity in the x-direction to be v_x(p) =  S_x / (τ + D \n",
    "+ p), we calculate the Lorentz factor to be W(p) = 1 / (1- v^2(p))^(1/2), we \n",
    "calculate the primitive variable specific internal energy to be ϵ(p) = (τ + D(1- \n",
    "W(p) + p(1- W^2(p)) / (D W(p)) and we calculate the primitive variable density \n",
    "to be ρ(p) = D / (W(p)).\n",
    "\n",
    "The code should print the progress and should plot all the relevant results. \n",
    "Make sure that in plotting no errors are thrown due to mixing of numpy arrays \n",
    "and torch tensors, and to that end convert all numpy arrays to torch tensors. \n",
    "Create, among other plots, the mean squared error against epochs for the \n",
    "training data, the testing data and the learning adaptation, both as separate \n",
    "plots as well as all in one. Furthermore, create a plot of the learning rate \n",
    "against epoch. Use different color palettes for the plots to make them more \n",
    "appealing and distinguishable. Add labels, legends, titles, and annotations to \n",
    "the plots to make them more informative and clear. Adjust the figure size, \n",
    "aspect ratio, margins, and spacing of the plots to make them more readable. Use \n",
    "different kinds of plots or combine multiple plots to show different aspects of \n",
    "the data or highlight interesting patterns. \n",
    "\n",
    "The code should save all results and plots. Please explain in great detail the \n",
    "code step-by-step in the code comments. Make the code readable by creating many \n",
    "functions to perform the tasks that I have described.\n",
    "\n",
    "This completes my request.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'I am more than ready!'\n",
    "prompt = 'No questions or comments, please continue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, let's move on to the second part of the code: defining the hyperparameters and the dataset. Hyperparameters are variables that control the training process and the architecture of the neural network. We will use the values that you specified in your request, such as the number of epochs, the learning rate, the batch size, and the number of neurons in each layer. We will also define some constants, such as the adiabatic index Γ and the dimensionality of the input and output.\\n\\nTo create the dataset, we will use the equations that you provided to generate random samples of primitive and conservative variables. We will use NumPy to perform the calculations and then convert the arrays to PyTorch tensors. We will also split the dataset into training and testing subsets, using 80% for training and 20% for testing. We will use torch.utils.data.TensorDataset and torch.utils.data.DataLoader to create PyTorch datasets and data loaders that will allow us to iterate over the data in batches.\\n\\nHere is the code snippet for this part:\\n\\n```python\\n# Define hyperparameters\\nnum_epochs = 400\\nlearning_rate = 6e-4\\nbatch_size = 32\\nnum_hidden1 = 600\\nnum_hidden2 = 200\\nnum_input = 3\\nnum_output = 1\\n\\n# Define constants\\ngamma = 1.4 # adiabatic index\\neps = 1e-8 # small number to avoid division by zero\\n\\n# Generate dataset\\ndef generate_dataset(n_samples):\\n    # Sample primitive variables from uniform distributions\\n    rho = np.random.uniform(0, 10.1, n_samples) # density\\n    eps = np.random.uniform(0, 2.02, n_samples) # specific internal energy\\n    vx = np.random.uniform(0, 0.721, n_samples) # velocity in x direction\\n    \\n    # Calculate pressure from equation of state\\n    p = (gamma - 1) * rho * eps # pressure\\n    \\n    # Calculate conservative variables from equations (2)\\n    W = 1 / np.sqrt(1 - vx**2) # Lorentz factor\\n    h = 1 + eps + p / rho # specific enthalpy\\n    D = rho * W # conserved density\\n    Sx = rho * h * W**2 * vx # conserved momentum in x direction\\n    tau = rho * h * W**2 - p - D # conserved energy density\\n    \\n    # Stack and transpose arrays to get input and output matrices\\n    X = np.stack([D, Sx, tau], axis=1) # input matrix (n_samples x num_input)\\n    y = p.reshape(-1, 1) # output matrix (n_samples x num_output)\\n    \\n    return X, y\\n\\n# Generate training and testing data\\nn_train = 80000 # number of training samples\\nn_test = 10000 # number of testing samples\\nX_train, y_train = generate_dataset(n_train)\\nX_test, y_test = generate_dataset(n_test)\\n\\n# Convert arrays to tensors and move to device\\nX_train = torch.from_numpy(X_train).float().to(device)\\ny_train = torch.from_numpy(y_train).float().to(device)\\nX_test = torch.from_numpy(X_test).float().to(device)\\ny_test = torch.from_numpy(y_test).float().to(device)\\n\\n# Create PyTorch datasets and data loaders\\ntrain_dataset = torch.utils.data.TensorDataset(X_train, y_train)\\ntest_dataset = torch.utils.data.TensorDataset(X_test, y_test)\\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\\n```\\n\\nThis completes the second part of the code. Do you have any questions or comments? 😊\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# resp['item']['messages'][1]['text']\n",
    "resp[\"item\"][\"messages\"][1][\"adaptiveCards\"][0][\"body\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Alright, let's proceed to the third part of the code: defining the neural network model and the loss function. We will use PyTorch's nn.Module class to create a custom model that consists of three fully connected layers with ReLU activation functions. The first layer will take the input vector of size num_input and output a hidden vector of size num_hidden1. The second layer will take the hidden vector of size num_hidden1 and output another hidden vector of size num_hidden2. The third layer will take the hidden vector of size num_hidden2 and output a prediction vector of size num_output. We will also use nn.init.kaiming_uniform_ to initialize the weights of the layers according to the He initialization method.\\n\\nTo define the loss function, we will use PyTorch's nn.MSELoss class to create a mean squared error (MSE) criterion that measures the difference between the predicted pressure and the true pressure. We will also use torch.sqrt to calculate the root mean squared error (RMSE) for evaluation purposes.\\n\\nHere is the code snippet for this part:\\n\\n```python\\n# Define neural network model\\nclass NeuralNet(nn.Module):\\n    def __init__(self, num_input, num_hidden1, num_hidden2, num_output):\\n        super(NeuralNet, self).__init__()\\n        # Define layers\\n        self.fc1 = nn.Linear(num_input, num_hidden1)\\n        self.fc2 = nn.Linear(num_hidden1, num_hidden2)\\n        self.fc3 = nn.Linear(num_hidden2, num_output)\\n        # Initialize weights\\n        nn.init.kaiming_uniform_(self.fc1.weight)\\n        nn.init.kaiming_uniform_(self.fc2.weight)\\n        nn.init.kaiming_uniform_(self.fc3.weight)\\n\\n    def forward(self, x):\\n        # Define forward pass\\n        x = torch.relu(self.fc1(x))\\n        x = torch.relu(self.fc2(x))\\n        x = self.fc3(x)\\n        return x\\n\\n# Create model and move to device\\nmodel = NeuralNet(num_input, num_hidden1, num_hidden2, num_output).to(device)\\n\\n# Define loss function\\ncriterion = nn.MSELoss()\\n```\\n\\nThis completes the third part of the code. Do you have any questions or comments? 😊\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resp = await bot.ask(prompt=prompt, conversation_style=ConversationStyle.creative, wss_link=\"wss://sydney.bing.com/sydney/ChatHub\")\n",
    "# resp[\"item\"][\"messages\"][1][\"adaptiveCards\"][0][\"body\"][0][\"text\"]\n",
    "resp['item']['messages'][1]['text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working example with Bing description 7 prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EdgeGPT import Chatbot, ConversationStyle\n",
    "cookies_ai0 = r\"./cookies/cookies_ai0.json\"\n",
    "cookies_ai1 = r\"./cookies/cookies_ai1.json\"\n",
    "cookies_ai2 = r\"./cookies/cookies_ai2.json\"\n",
    "cookies_ai3 = r\"./cookies/cookies_ai3.json\"\n",
    "cookies_ai4 = r\"./cookies/cookies_ai1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object Chatbot.reset at 0x00000256B1A47E40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bot.reset() # Run when needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object Chatbot.close at 0x7f81b05c54d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bot.close() # Run when needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = Chatbot(cookie_path=cookies_ai1) # Run when needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, let's move on to the second part of the code: defining the hyperparameters and the dataset. Hyperparameters are variables that control the training process and the architecture of the neural network. We will use the values that you specified in your request, such as the number of epochs, the learning rate, the batch size, and the number of neurons in each layer. We will also define some constants, such as the adiabatic index Γ and the dimensionality of the input and output.\\n\\nTo create the dataset, we will use the equations that you provided to generate random samples of primitive and conservative variables. We will use NumPy to perform the calculations and then convert the arrays to PyTorch tensors. We will also split the dataset into training and testing subsets, using 80% for training and 20% for testing. We will use torch.utils.data.TensorDataset and torch.utils.data.DataLoader to create PyTorch datasets and data loaders that will allow us to iterate over the data in batches.\\n\\nHere is the code snippet for this part:\\n\\n```python\\n# Define hyperparameters\\nnum_epochs = 400\\nlearning_rate = 6e-4\\nbatch_size = 32\\nnum_hidden1 = 600\\nnum_hidden2 = 200\\nnum_input = 3\\nnum_output = 1\\n\\n# Define constants\\ngamma = 1.4 # adiabatic index\\neps = 1e-8 # small number to avoid division by zero\\n\\n# Generate dataset\\ndef generate_dataset(n_samples):\\n    # Sample primitive variables from uniform distributions\\n    rho = np.random.uniform(0, 10.1, n_samples) # density\\n    eps = np.random.uniform(0, 2.02, n_samples) # specific internal energy\\n    vx = np.random.uniform(0, 0.721, n_samples) # velocity in x direction\\n    \\n    # Calculate pressure from equation of state\\n    p = (gamma - 1) * rho * eps # pressure\\n    \\n    # Calculate conservative variables from equations (2)\\n    W = 1 / np.sqrt(1 - vx**2) # Lorentz factor\\n    h = 1 + eps + p / rho # specific enthalpy\\n    D = rho * W # conserved density\\n    Sx = rho * h * W**2 * vx # conserved momentum in x direction\\n    tau = rho * h * W**2 - p - D # conserved energy density\\n    \\n    # Stack and transpose arrays to get input and output matrices\\n    X = np.stack([D, Sx, tau], axis=1) # input matrix (n_samples x num_input)\\n    y = p.reshape(-1, 1) # output matrix (n_samples x num_output)\\n    \\n    return X, y\\n\\n# Generate training and testing data\\nn_train = 80000 # number of training samples\\nn_test = 10000 # number of testing samples\\nX_train, y_train = generate_dataset(n_train)\\nX_test, y_test = generate_dataset(n_test)\\n\\n# Convert arrays to tensors and move to device\\nX_train = torch.from_numpy(X_train).float().to(device)\\ny_train = torch.from_numpy(y_train).float().to(device)\\nX_test = torch.from_numpy(X_test).float().to(device)\\ny_test = torch.from_numpy(y_test).float().to(device)\\n\\n# Create PyTorch datasets and data loaders\\ntrain_dataset = torch.utils.data.TensorDataset(X_train, y_train)\\ntest_dataset = torch.utils.data.TensorDataset(X_test, y_test)\\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\\n```\\n\\nThis completes the second part of the code. Do you have any questions or comments? 😊\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# resp['item']['messages'][1]['text']\n",
    "resp[\"item\"][\"messages\"][1][\"adaptiveCards\"][0][\"body\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "I will write to you a request to generate code in PyTorch for a neural network. \n",
    "I will explain what you should generate at great length, but please use the \n",
    "paper Machine Learning for Conservative-to-Primitive in Relativistic \n",
    "Hydrodynamics by Dieseldorst et al. for anything that I miss to specify, as I \n",
    "have based my description on this paper. It would be extremely helpful to me if \n",
    "you could generate this code. I will write my request in batches, please only \n",
    "start generating code when I have completed my request fully. I will mark that I \n",
    "have completed my request by saying \"This completes my request\". I will start \n",
    "writing my request now.\n",
    "\n",
    "Please create a code in PyTorch for conservative-to-primitive inversion based on \n",
    "supervised learning of a fully connected feedforward neural network. Use the GPU \n",
    "if it is available. Use for the neural network two hidden layers and use the \n",
    "sigmoid function as the activation function for each of the two. Use ReLU as the \n",
    "nonlinearity applied to the output. Use for the number of neurons in the first \n",
    "hidden layer 600, and for the number of neurons of the second hidden layer 200. \n",
    "The network has three inputs, the conserved density D, the conserved momentum in \n",
    "the x direction S_x and the conserved energy density τ. The network has one \n",
    "output, namely the primitive variable which is the pressure p. All other \n",
    "primitive variables can be calculated from p. \n",
    "\n",
    "Let the number of epochs be 400. The training dataset should consist of 80000 \n",
    "samples, while the test dataset should consist of 10000 samples. Let the initial \n",
    "learning rate for training be set to 6 * 10^-4. Please construct the training \n",
    "dataset as follows. Use equation (3) from Dieseldorst et al., p = p(ρ,e) = \n",
    "(Γ-1)ρe, to calculate the pressure from the equation of state. Then uniformly \n",
    "sample the primitive variables over the intervals ρ ∈ (0,10.1), ϵ ∈ (0, 2.02), \n",
    "and v_x ∈ (0, 0.721) using a random seed and by calculate the corresponding \n",
    "conservative variables D, S_x, and τ, using the equations (2) from Dieseldorst \n",
    "et al.: W = (1-v_x^2)^(-1/2), h = 1 + ϵ \n",
    "+ p / ρ, D = ρ W, S_x = ρ h W^2 v_x, τ = ρ h W^2 - p - D.\n",
    "\n",
    "Adapt the learning rate until the error on the training dataset is minimized, \n",
    "which marks that training is completed. To adapt the learning rate, we multiply \n",
    "the learning rate by a factor of 0.5 whenever the loss of the training data over \n",
    "the last five epochs has not improved by at least 0.05% with respect to the \n",
    "previous five epochs. Furthermore, ten epochs have to be completed before the \n",
    "next possible learning rate adaption. Use torch's ReduceLROnPlateau for the \n",
    "learning rate adaptation.\n",
    "\n",
    "Errors on data series should be evaluated with the L_1 and L_{infinity} norms. \n",
    "Errors are calculated by comparing the error in the calculated pressure after \n",
    "the trained neural network performs the conservative to primitive inversion and \n",
    "by comparing to the test dataset.\n",
    "\n",
    "The minimization of the weights and biases, collectively called θ, should be \n",
    "performed iteratively, by 1. Computing the loss function E, for which we use the \n",
    "mean squared error, and 2. Taking the gradient of the loss function with \n",
    "backpropagation, and 3. Applying the gradient descent algorithm, for which we \n",
    "use the Adam optimizer, to minimize E. An epoch is completed by performing these \n",
    "three steps for all samples of the training dataset. Let the training dataset be \n",
    "split into random mini-batches of size 32 and collect the gradients of the θ for \n",
    "all samples of a minibatch. Apply the gradient descent algorithm after each \n",
    "mini-batch. Create new mini-batches after each epoch.\n",
    "\n",
    "We use the pressure to calculate all other primitive variables, using equations \n",
    "(A2), (A3),(A4), (A5), from Dieseldorst et al. Using these equations, we \n",
    "calculate the primitive velocity in the x-direction to be v_x(p) =  S_x / (τ + D \n",
    "+ p), we calculate the Lorentz factor to be W(p) = 1 / (1- v^2(p))^(1/2), we \n",
    "calculate the primitive variable specific internal energy to be ϵ(p) = (τ + D(1- \n",
    "W(p) + p(1- W^2(p)) / (D W(p)) and we calculate the primitive variable density \n",
    "to be ρ(p) = D / (W(p)).\n",
    "\n",
    "The code should print the progress and should plot all the relevant results. \n",
    "Make sure that in plotting no errors are thrown due to mixing of numpy arrays \n",
    "and torch tensors, and to that end convert all numpy arrays to torch tensors. \n",
    "Create, among other plots, the mean squared error against epochs for the \n",
    "training data, the testing data and the learning adaptation, both as separate \n",
    "plots as well as all in one. Furthermore, create a plot of the learning rate \n",
    "against epoch. Use different color palettes for the plots to make them more \n",
    "appealing and distinguishable. Add labels, legends, titles, and annotations to \n",
    "the plots to make them more informative and clear. Adjust the figure size, \n",
    "aspect ratio, margins, and spacing of the plots to make them more readable. Use \n",
    "different kinds of plots or combine multiple plots to show different aspects of \n",
    "the data or highlight interesting patterns. \n",
    "\n",
    "The code should save all results and plots. Please explain in great detail the \n",
    "code step-by-step in the code comments. Make the code readable by creating many \n",
    "functions to perform the tasks that I have described.\n",
    "\n",
    "This completes my request.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "prompt = 'I am more than ready!'\n",
    "prompt = 'No questions or comments, please continue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, let's move on to the second part of the code: defining the hyperparameters and the dataset. Hyperparameters are variables that control the training process and the architecture of the neural network. We will use the values that you specified in your request, such as the number of epochs, the learning rate, the batch size, and the number of neurons in each layer. We will also define some constants, such as the adiabatic index Γ and the dimensionality of the input and output.\\n\\nTo create the dataset, we will use the equations that you provided to generate random samples of primitive and conservative variables. We will use NumPy to perform the calculations and then convert the arrays to PyTorch tensors. We will also split the dataset into training and testing subsets, using 80% for training and 20% for testing. We will use torch.utils.data.TensorDataset and torch.utils.data.DataLoader to create PyTorch datasets and data loaders that will allow us to iterate over the data in batches.\\n\\nHere is the code snippet for this part:\\n\\n```python\\n# Define hyperparameters\\nnum_epochs = 400\\nlearning_rate = 6e-4\\nbatch_size = 32\\nnum_hidden1 = 600\\nnum_hidden2 = 200\\nnum_input = 3\\nnum_output = 1\\n\\n# Define constants\\ngamma = 1.4 # adiabatic index\\neps = 1e-8 # small number to avoid division by zero\\n\\n# Generate dataset\\ndef generate_dataset(n_samples):\\n    # Sample primitive variables from uniform distributions\\n    rho = np.random.uniform(0, 10.1, n_samples) # density\\n    eps = np.random.uniform(0, 2.02, n_samples) # specific internal energy\\n    vx = np.random.uniform(0, 0.721, n_samples) # velocity in x direction\\n    \\n    # Calculate pressure from equation of state\\n    p = (gamma - 1) * rho * eps # pressure\\n    \\n    # Calculate conservative variables from equations (2)\\n    W = 1 / np.sqrt(1 - vx**2) # Lorentz factor\\n    h = 1 + eps + p / rho # specific enthalpy\\n    D = rho * W # conserved density\\n    Sx = rho * h * W**2 * vx # conserved momentum in x direction\\n    tau = rho * h * W**2 - p - D # conserved energy density\\n    \\n    # Stack and transpose arrays to get input and output matrices\\n    X = np.stack([D, Sx, tau], axis=1) # input matrix (n_samples x num_input)\\n    y = p.reshape(-1, 1) # output matrix (n_samples x num_output)\\n    \\n    return X, y\\n\\n# Generate training and testing data\\nn_train = 80000 # number of training samples\\nn_test = 10000 # number of testing samples\\nX_train, y_train = generate_dataset(n_train)\\nX_test, y_test = generate_dataset(n_test)\\n\\n# Convert arrays to tensors and move to device\\nX_train = torch.from_numpy(X_train).float().to(device)\\ny_train = torch.from_numpy(y_train).float().to(device)\\nX_test = torch.from_numpy(X_test).float().to(device)\\ny_test = torch.from_numpy(y_test).float().to(device)\\n\\n# Create PyTorch datasets and data loaders\\ntrain_dataset = torch.utils.data.TensorDataset(X_train, y_train)\\ntest_dataset = torch.utils.data.TensorDataset(X_test, y_test)\\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\\n```\\n\\nThis completes the second part of the code. Do you have any questions or comments? 😊\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# resp['item']['messages'][1]['text']\n",
    "resp[\"item\"][\"messages\"][1][\"adaptiveCards\"][0][\"body\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Alright, let's proceed to the third part of the code: defining the neural network model and the loss function. We will use PyTorch's nn.Module class to create a custom model that consists of three fully connected layers with ReLU activation functions. The first layer will take the input vector of size num_input and output a hidden vector of size num_hidden1. The second layer will take the hidden vector of size num_hidden1 and output another hidden vector of size num_hidden2. The third layer will take the hidden vector of size num_hidden2 and output a prediction vector of size num_output. We will also use nn.init.kaiming_uniform_ to initialize the weights of the layers according to the He initialization method.\\n\\nTo define the loss function, we will use PyTorch's nn.MSELoss class to create a mean squared error (MSE) criterion that measures the difference between the predicted pressure and the true pressure. We will also use torch.sqrt to calculate the root mean squared error (RMSE) for evaluation purposes.\\n\\nHere is the code snippet for this part:\\n\\n```python\\n# Define neural network model\\nclass NeuralNet(nn.Module):\\n    def __init__(self, num_input, num_hidden1, num_hidden2, num_output):\\n        super(NeuralNet, self).__init__()\\n        # Define layers\\n        self.fc1 = nn.Linear(num_input, num_hidden1)\\n        self.fc2 = nn.Linear(num_hidden1, num_hidden2)\\n        self.fc3 = nn.Linear(num_hidden2, num_output)\\n        # Initialize weights\\n        nn.init.kaiming_uniform_(self.fc1.weight)\\n        nn.init.kaiming_uniform_(self.fc2.weight)\\n        nn.init.kaiming_uniform_(self.fc3.weight)\\n\\n    def forward(self, x):\\n        # Define forward pass\\n        x = torch.relu(self.fc1(x))\\n        x = torch.relu(self.fc2(x))\\n        x = self.fc3(x)\\n        return x\\n\\n# Create model and move to device\\nmodel = NeuralNet(num_input, num_hidden1, num_hidden2, num_output).to(device)\\n\\n# Define loss function\\ncriterion = nn.MSELoss()\\n```\\n\\nThis completes the third part of the code. Do you have any questions or comments? 😊\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resp = await bot.ask(prompt=prompt, conversation_style=ConversationStyle.creative, wss_link=\"wss://sydney.bing.com/sydney/ChatHub\")\n",
    "# resp[\"item\"][\"messages\"][1][\"adaptiveCards\"][0][\"body\"][0][\"text\"]\n",
    "resp['item']['messages'][1]['text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

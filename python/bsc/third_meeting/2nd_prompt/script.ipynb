{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Setting the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Defining the functions to be approximated\n",
    "def f(x1, x2, x3):\n",
    "    return x1 + x2 + x3\n",
    "\n",
    "def g(x1, x2, x3):\n",
    "    return x1**2 + x2**3 + 0.5 * x3\n",
    "\n",
    "def h(x1, x2, x3):\n",
    "    return x3 * x1**(x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generating the data by sampling from the given ranges\n",
    "x1 = np.arange(0, 10, 0.5)\n",
    "x2 = np.arange(0, 10, 0.5)\n",
    "x3 = np.arange(0, 10, 0.5)\n",
    "X = np.array(np.meshgrid(x1, x2, x3)).T.reshape(-1, 3) # shape: (8000, 3)\n",
    "y_f = f(X[:, 0], X[:, 1], X[:, 2]) # shape: (8000,)\n",
    "y_g = g(X[:, 0], X[:, 1], X[:, 2]) # shape: (8000,)\n",
    "y_h = h(X[:, 0], X[:, 1], X[:, 2]) # shape: (8000,)\n",
    "Y = np.stack([y_f, y_g, y_h], axis=1) # shape: (8000, 3)\n",
    "\n",
    "# Converting the data to torch tensors\n",
    "X = torch.from_numpy(X).float().to(device)\n",
    "Y = torch.from_numpy(Y).float().to(device)\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "train_size = int(0.8 * len(X))\n",
    "test_size = len(X) - train_size\n",
    "X_train, X_test = torch.split(X, [train_size, test_size])\n",
    "Y_train, Y_test = torch.split(Y, [train_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining the hyperparameters to be tested\n",
    "num_layers_list = [2, 3, 4] # number of hidden layers\n",
    "num_units_list = [16, 32, 64] # number of units per hidden layer\n",
    "activation_list = [nn.ReLU(), nn.Sigmoid(), nn.Tanh()] # activation functions for hidden layers\n",
    "loss_list = [F.mse_loss, F.smooth_l1_loss, F.huber_loss] # loss functions for output layer\n",
    "optimizer_list = [optim.SGD, optim.Adam] # optimizer algorithms for updating parameters\n",
    "learning_rate_list = [0.01, 0.001] # learning rates for optimizer\n",
    "batch_size_list = [32, 64] # batch sizes for data loader\n",
    "num_epochs_list = [50] # number of epochs for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a function to create a neural network model with the given hyperparameters\n",
    "def create_model(num_layers, num_units, activation):\n",
    "    layers = []\n",
    "    layers.append(nn.Linear(3, num_units)) # input layer\n",
    "    layers.append(activation) # activation for input layer\n",
    "    for i in range(num_layers - 1): # hidden layers\n",
    "        layers.append(nn.Linear(num_units, num_units))\n",
    "        layers.append(activation)\n",
    "    layers.append(nn.Linear(num_units, 3)) # output layer\n",
    "    model = nn.Sequential(*layers).to(device) # creating the model from the layers list\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a function to train a neural network model with the given hyperparameters and data\n",
    "def train_model(model, loss_fn, optimizer, batch_size, num_epochs):\n",
    "    train_loader = DataLoader(list(zip(X_train, Y_train)), batch_size=batch_size) # creating a data loader for training data\n",
    "    train_losses = [] # list to store the training losses per epoch\n",
    "    for epoch in range(num_epochs): # looping over the epochs\n",
    "        epoch_loss = 0 # variable to store the epoch loss\n",
    "        for x_batch, y_batch in train_loader: # looping over the batches\n",
    "            optimizer.zero_grad() # zeroing the gradients of the parameters\n",
    "            y_pred = model(x_batch) # forward pass\n",
    "            loss = loss_fn(y_pred, y_batch) # calculating the loss\n",
    "            loss.backward() # backward pass\n",
    "            optimizer.step() # updating the parameters\n",
    "            epoch_loss += loss.item() # adding the batch loss to the epoch loss\n",
    "        train_losses.append(epoch_loss / len(train_loader)) # appending the average epoch loss to the list\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {train_losses[-1]:.4f}\") # printing the epoch loss\n",
    "    return train_losses # returning the list of training losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a function to test a neural network model with the given data\n",
    "def test_model(model, loss_fn):\n",
    "    test_loader = DataLoader(list(zip(X_test, Y_test)), batch_size=len(X_test)) # creating a data loader for testing data\n",
    "    with torch.no_grad(): # disabling gradient computation\n",
    "        for x_batch, y_batch in test_loader: # looping over the batches\n",
    "            y_pred = model(x_batch) # forward pass\n",
    "            loss = loss_fn(y_pred, y_batch) # calculating the loss\n",
    "            print(f\"Test Loss: {loss:.4f}\") # printing the test loss\n",
    "            return y_pred, loss # returning the predictions and the loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a function to plot the results of a neural network model with the given data and hyperparameters\n",
    "def plot_results(model, y_pred, train_losses, num_layers, num_units, activation, loss_fn, optimizer, learning_rate):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10)) # creating a figure with 2x2 subplots\n",
    "    fig.suptitle(f\"Results for model with {num_layers} layers, {num_units} units, {activation}, {loss_fn}, {optimizer}, {learning_rate}\") # setting the figure title\n",
    "\n",
    "    # Plotting the training losses\n",
    "    axs[0, 0].plot(train_losses) # plotting the losses per epoch\n",
    "    axs[0, 0].set_xlabel(\"Epoch\") # setting the x-axis label\n",
    "    axs[0, 0].set_ylabel(\"Loss\") # setting the y-axis label\n",
    "    axs[0, 0].set_title(\"Training Loss\") # setting the subplot title\n",
    "\n",
    "    # Plotting the true vs predicted values for f function\n",
    "    axs[0, 1].scatter(Y_test[:, 0].cpu(), y_pred[:, 0].cpu(), c=\"blue\", alpha=0.5) # plotting the true and predicted values as scatter points\n",
    "    axs[0, 1].plot([Y_test[:, 0].min().cpu(), Y_test[:, 0].max().cpu()], [Y_test[:, 0].min().cpu(), Y_test[:, 0].max().cpu()], c=\"red\", linestyle=\"--\") # plotting the identity line as reference\n",
    "    axs[0, 1].set_xlabel(\"True\") # setting the x-axis label\n",
    "    axs[0, 1].set_ylabel(\"Predicted\") # setting the y-axis label\n",
    "    axs[0, 1].set_title(\"f function\") # setting the subplot title\n",
    "\n",
    "    # Plotting the true vs predicted values for g function\n",
    "    axs[1, 0].scatter(Y_test[:, 1].cpu(), y_pred[:, 1].cpu(), c=\"green\", alpha=0.5) # plotting the true and predicted values as scatter points\n",
    "    axs[1, 0].plot([Y_test[:, 1].min().cpu(), Y_test[:, 1].max().cpu()], [Y_test[:, 1].min().cpu(), Y_test[:, 1].max().cpu()], c=\"red\", linestyle=\"--\") # plotting the identity line as reference\n",
    "    axs[1, 0].set_xlabel(\"True\") # setting the x-axis label\n",
    "    axs[1, 0].set_ylabel(\"Predicted\") # setting the y-axis label\n",
    "    axs[1, 0].set_title(\"g function\") # setting the subplot title\n",
    "\n",
    "    # Plotting the true vs predicted values for h function\n",
    "    axs[1, 1].scatter(Y_test[:, 2].cpu(), y_pred[:, 2].cpu(), c=\"orange\", alpha=0.5) # plotting the true and predicted values as scatter points\n",
    "    axs[1, 1].plot([Y_test[:, 2].min().cpu(), Y_test[:, 2].max().cpu()], [Y_test[:, 2].min().cpu(), Y_test[:, 2].max().cpu()], c=\"red\", linestyle=\"--\") # plotting the identity line as reference\n",
    "    axs[1, 1].set_xlabel(\"True\") # setting the x-axis label\n",
    "    axs[1, 1].set_ylabel(\"Predicted\") # setting the y-axis label\n",
    "    axs[1, 1].set_title(\"h function\") # setting the subplot title\n",
    "\n",
    "    fig.tight_layout() # adjusting the spacing between subplots\n",
    "    plt.show() # showing the figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a list to store the best results for each function\n",
    "best_results = [{\"loss\": float(\"inf\"), \"model\": None, \"y_pred\": None, \"train_losses\": None, \"hyperparams\": None} for _ in range(3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layers: 2, num_units: 16, activation: ReLU(), loss_fn: <function mse_loss at 0x7f4fe412dcf0>, optimizer: <class 'torch.optim.sgd.SGD'>, learning_rate: 0.01, batch_size: 32, num_epochs: 50\n",
      "Epoch 1, Loss: nan\n",
      "Epoch 2, Loss: nan\n",
      "Epoch 3, Loss: nan\n",
      "Epoch 4, Loss: nan\n",
      "Epoch 5, Loss: nan\n",
      "Epoch 6, Loss: nan\n",
      "Epoch 7, Loss: nan\n",
      "Epoch 8, Loss: nan\n",
      "Epoch 9, Loss: nan\n",
      "Epoch 10, Loss: nan\n",
      "Epoch 11, Loss: nan\n",
      "Epoch 12, Loss: nan\n",
      "Epoch 13, Loss: nan\n",
      "Epoch 14, Loss: nan\n",
      "Epoch 15, Loss: nan\n",
      "Epoch 16, Loss: nan\n",
      "Epoch 17, Loss: nan\n",
      "Epoch 18, Loss: nan\n",
      "Epoch 19, Loss: nan\n",
      "Epoch 20, Loss: nan\n",
      "Epoch 21, Loss: nan\n",
      "Epoch 22, Loss: nan\n",
      "Epoch 23, Loss: nan\n",
      "Epoch 24, Loss: nan\n",
      "Epoch 25, Loss: nan\n",
      "Epoch 26, Loss: nan\n",
      "Epoch 27, Loss: nan\n",
      "Epoch 28, Loss: nan\n",
      "Epoch 29, Loss: nan\n",
      "Epoch 30, Loss: nan\n",
      "Epoch 31, Loss: nan\n",
      "Epoch 32, Loss: nan\n",
      "Epoch 33, Loss: nan\n",
      "Epoch 34, Loss: nan\n",
      "Epoch 35, Loss: nan\n",
      "Epoch 36, Loss: nan\n",
      "Epoch 37, Loss: nan\n",
      "Epoch 38, Loss: nan\n",
      "Epoch 39, Loss: nan\n",
      "Epoch 40, Loss: nan\n",
      "Epoch 41, Loss: nan\n",
      "Epoch 42, Loss: nan\n",
      "Epoch 43, Loss: nan\n",
      "Epoch 44, Loss: nan\n",
      "Epoch 45, Loss: nan\n",
      "Epoch 46, Loss: nan\n",
      "Epoch 47, Loss: nan\n",
      "Epoch 48, Loss: nan\n",
      "Epoch 49, Loss: nan\n",
      "Epoch 50, Loss: nan\n",
      "Test Loss: nan\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39m# Checking if the current results are better than the previous best results for each function\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m):\n\u001b[0;32m---> 28\u001b[0m     \u001b[39mif\u001b[39;00m test_loss[i]\u001b[39m.\u001b[39mitem() \u001b[39m<\u001b[39m best_results[i][\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     29\u001b[0m         \u001b[39m# Updating the best results for the current function\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         best_results[i][\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m test_loss[i]\n\u001b[1;32m     31\u001b[0m         best_results[i][\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
     ]
    }
   ],
   "source": [
    "\n",
    "# Looping over all possible combinations of hyperparameters\n",
    "for num_layers in num_layers_list:\n",
    "    for num_units in num_units_list:\n",
    "        for activation in activation_list:\n",
    "            for loss_fn in loss_list:\n",
    "                for optimizer_class in optimizer_list:\n",
    "                    for learning_rate in learning_rate_list:\n",
    "                        for batch_size in batch_size_list:\n",
    "                            for num_epochs in num_epochs_list:\n",
    "\n",
    "                                # Printing the current hyperparameters\n",
    "                                print(f\"num_layers: {num_layers}, num_units: {num_units}, activation: {activation}, loss_fn: {loss_fn}, optimizer: {optimizer_class}, learning_rate: {learning_rate}, batch_size: {batch_size}, num_epochs: {num_epochs}\")\n",
    "\n",
    "                                # Creating a model with the current hyperparameters\n",
    "                                model = create_model(num_layers, num_units, activation)\n",
    "\n",
    "                                # Creating an optimizer with the current hyperparameters\n",
    "                                optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "\n",
    "                                # Training the model with the current hyperparameters and data\n",
    "                                train_losses = train_model(model, loss_fn, optimizer, batch_size, num_epochs)\n",
    "\n",
    "                                # Testing the model with the current hyperparameters and data\n",
    "                                y_pred, test_loss = test_model(model, loss_fn)\n",
    "\n",
    "                                # Checking if the current results are better than the previous best results for each function\n",
    "                                for i in range(3):\n",
    "                                    if test_loss[i].item() < best_results[i][\"loss\"]:\n",
    "                                        # Updating the best results for the current function\n",
    "                                        best_results[i][\"loss\"] = test_loss[i]\n",
    "                                        best_results[i][\"model\"] = model\n",
    "                                        best_results[i][\"y_pred\"] = y_pred\n",
    "                                        best_results[i][\"train_losses\"] = train_losses\n",
    "                                        best_results[i][\"hyperparams\"] = (num_layers, num_units, activation, loss_fn, optimizer_class, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Printing and plotting the best results for each function\n",
    "for i in range(3):\n",
    "    print(f\"Best results for function {i + 1}:\")\n",
    "    print(f\"Loss: {best_results[i]['loss']:.4f}\")\n",
    "    print(f\"Hyperparameters: {best_results[i]['hyperparams']}\")\n",
    "    plot_results(best_results[i][\"model\"], best_results[i][\"y_pred\"][:, i], best_results[i][\"train_losses\"], *best_results[i][\"hyperparams\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Saving the models and plots\n",
    "torch.save(best_results[0][\"model\"], \"f_model.pth\")\n",
    "torch.save(best_results[1][\"model\"], \"g_model.pth\")\n",
    "torch.save(best_results[2][\"model\"], \"h_model.pth\")\n",
    "fig.savefig(\"results.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

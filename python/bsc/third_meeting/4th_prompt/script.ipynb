{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximating h\n",
    "\n",
    "We use Optuna to do a type of Bayesian optimization of the hyperparameters of the model. We then train the model using these hyperparameters to approximate the function $h = x_3 \\cdot x_1^{x_2}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this first cell to convert the notebook to a python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert script.ipynb --TagRemovePreprocessor.enabled=True --TagRemovePreprocessor.remove_cell_tags='{\"remove_cell\"}' --to script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import optuna\n",
    "import tensorboardX as tbx\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining the function to approximate\n",
    "def h(x1, x2, x3):\n",
    "    \"\"\"Returns the value of the function h(x1, x2, x3) = x3 * x1 ** x2.\n",
    "\n",
    "    Args:\n",
    "        x1 (float or array-like): The first input.\n",
    "        x2 (float or array-like): The second input.\n",
    "        x3 (float or array-like): The third input.\n",
    "\n",
    "    Returns:\n",
    "        float or array-like: The value of the function h at the given inputs.\n",
    "    \"\"\"\n",
    "    return x3 * x1 ** x2\n",
    "\n",
    "# Generating the data samples\n",
    "np.random.seed(0)\n",
    "x1 = np.random.uniform(0, 10, size=10**6)\n",
    "x2 = np.random.uniform(0, 10, size=10**6)\n",
    "x3 = np.random.uniform(0, 10, size=10**6)\n",
    "y = h(x1, x2, x3)\n",
    "\n",
    "# Converting the data to tensors\n",
    "x1 = torch.from_numpy(x1).float()\n",
    "x2 = torch.from_numpy(x2).float()\n",
    "x3 = torch.from_numpy(x3).float()\n",
    "y = torch.from_numpy(y).float()\n",
    "\n",
    "# Stacking the inputs into a single tensor\n",
    "x = torch.stack([x1, x2, x3], dim=1)\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "train_size = int(0.8 * len(x))\n",
    "test_size = len(x) - train_size\n",
    "x_train, x_test = torch.split(x, [train_size, test_size])\n",
    "y_train, y_test = torch.split(y, [train_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a custom neural network class\n",
    "class Net(nn.Module):\n",
    "    \"\"\"A custom neural network class that approximates the function h.\n",
    "\n",
    "    Args:\n",
    "        n_layers (int): The number of hidden layers in the network.\n",
    "        n_units (int): The number of hidden units in each layer.\n",
    "        hidden_activation (torch.nn.Module): The activation function for the hidden layers.\n",
    "        output_activation (torch.nn.Module): The activation function for the output layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers, n_units, hidden_activation, output_activation):\n",
    "        super(Net, self).__init__()\n",
    "        # Creating a list of linear layers with n_units each\n",
    "        self.layers = nn.ModuleList([nn.Linear(3, n_units)])\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.layers.append(nn.Linear(n_units, n_units))\n",
    "        # Adding the final output layer with one unit\n",
    "        self.layers.append(nn.Linear(n_units, 1))\n",
    "        # Storing the activation functions\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.output_activation = output_activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Performs a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, 3).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape (batch_size, 1).\n",
    "        \"\"\"\n",
    "        # Passing the input through the hidden layers with the hidden activation function\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.hidden_activation(layer(x))\n",
    "        # Passing the output through the final layer with the output activation function\n",
    "        x = self.output_activation(self.layers[-1](x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a function to create a trial network and optimizer\n",
    "def create_model(trial):\n",
    "    \"\"\"Creates a trial network and optimizer based on the sampled hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): The trial object that contains the hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of (net, loss_fn, optimizer, batch_size, n_epochs,\n",
    "            scheduler), where net is the trial network,\n",
    "            loss_fn is the loss function,\n",
    "            optimizer is the optimizer,\n",
    "            batch_size is the batch size,\n",
    "            n_epochs is the number of epochs,\n",
    "            scheduler is the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Sampling the hyperparameters from the search space\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    n_units = trial.suggest_int(\"n_units\", 16, 256)\n",
    "    hidden_activation_name = trial.suggest_categorical(\"hidden_activation\", [\"ReLU\", \"Tanh\", \"Sigmoid\"])\n",
    "    output_activation_name = trial.suggest_categorical(\"output_activation\", [\"ReLU\", \"Tanh\", \"Sigmoid\"])\n",
    "    loss_name = trial.suggest_categorical(\"loss\", [\"MSE\", \"MAE\", \"Huber\"])\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"SGD\", \"Adam\", \"RMSprop\"])\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 32, 512)\n",
    "    n_epochs = trial.suggest_int(\"n_epochs\", 10, 100)\n",
    "    scheduler_name = trial.suggest_categorical(\"scheduler\", [\"None\", \"StepLR\", \"ExponentialLR\"])\n",
    "\n",
    "    # Creating the activation functions from their names\n",
    "    if hidden_activation_name == \"ReLU\":\n",
    "        hidden_activation = nn.ReLU()\n",
    "    elif hidden_activation_name == \"Tanh\":\n",
    "        hidden_activation = nn.Tanh()\n",
    "    else:\n",
    "        hidden_activation = nn.Sigmoid()\n",
    "\n",
    "    if output_activation_name == \"ReLU\":\n",
    "        output_activation = nn.ReLU()\n",
    "    elif output_activation_name == \"Tanh\":\n",
    "        output_activation = nn.Tanh()\n",
    "    else:\n",
    "        output_activation = nn.Sigmoid()\n",
    "\n",
    "    # Creating the loss function from its name\n",
    "    if loss_name == \"MSE\":\n",
    "        loss_fn = nn.MSELoss()\n",
    "    elif loss_name == \"MAE\":\n",
    "        loss_fn = nn.L1Loss()\n",
    "    else:\n",
    "        loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "    # Creating the network with the sampled hyperparameters\n",
    "    net = Net(n_layers, n_units, hidden_activation, output_activation)\n",
    "\n",
    "    # Creating the optimizer from its name\n",
    "    if optimizer_name == \"SGD\":\n",
    "        optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr=lr)\n",
    "\n",
    "    # Creating the learning rate scheduler from its name\n",
    "    if scheduler_name == \"StepLR\":\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif scheduler_name == \"ExponentialLR\":\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    # Returning the network, the loss function, the optimizer, the batch size, the number of epochs and the scheduler\n",
    "    return net, loss_fn, optimizer, batch_size, n_epochs, scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a function to train and evaluate a network\n",
    "def train_and_eval(net, loss_fn, optimizer, batch_size, n_epochs, scheduler):\n",
    "    \"\"\"Trains and evaluates a network on the train and test sets.\n",
    "\n",
    "    Args:\n",
    "        net (Net): The network to train and evaluate.\n",
    "        loss_fn (torch.nn.Module): The loss function to use.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer to use.\n",
    "        batch_size (int): The batch size to use.\n",
    "        n_epochs (int): The number of epochs to train for.\n",
    "        scheduler (torch.optim.lr_scheduler._LRScheduler or None): The learning rate scheduler to use.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of (train_losses, test_losses,\n",
    "            train_accuracies, test_accuracies), where train_losses and test_losses are lists of average losses per epoch for the train and test sets,\n",
    "            train_accuracies and test_accuracies are lists of average accuracies per epoch for the train and test sets.\n",
    "    \"\"\"\n",
    "    # Creating data loaders for train and test sets\n",
    "    train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test, y_test), batch_size=batch_size)\n",
    "\n",
    "    # Initializing lists to store the losses and accuracies for each epoch\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    # Looping over the epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # Setting the network to training mode\n",
    "        net.train()\n",
    "        # Initializing variables to store the total loss and correct predictions for the train set\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # Looping over the batches in the train set\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            # Zeroing the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            y_pred = net(x_batch)\n",
    "            # Reshaping the target tensor to match the input tensor\n",
    "            y_batch = y_batch.view(-1, 1)\n",
    "            # Computing the loss\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Updating the total loss and correct predictions\n",
    "            train_loss += loss.item() * x_batch.size(0)\n",
    "            train_correct += torch.sum((y_pred - y_batch).abs() < 1e-3)\n",
    "\n",
    "        # Updating the learning rate scheduler if applicable\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Setting the network to evaluation mode\n",
    "        net.eval()\n",
    "        # Initializing variables to store the total loss and correct predictions for the test set\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        # Looping over the batches in the test set\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            # Forward pass\n",
    "            y_pred = net(x_batch)\n",
    "            # Computing the loss\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # Updating the total loss and correct predictions\n",
    "            test_loss += loss.item() * x_batch.size(0)\n",
    "            test_correct += torch.sum((y_pred - y_batch).abs() < 1e-3)\n",
    "\n",
    "        # Computing the average losses and accuracies for the train and test sets\n",
    "        train_loss /= len(x_train)\n",
    "        test_loss /= len(x_test)\n",
    "        train_accuracy = train_correct / len(x_train)\n",
    "        test_accuracy = test_correct / len(x_test)\n",
    "\n",
    "        # Appending the losses and accuracies to the lists\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # Printing the epoch summary\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Returning the lists of losses and accuracies\n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a function to compute the objective value for a trial\n",
    "def objective(trial):\n",
    "    \"\"\"Computes the objective value (final test loss) for a trial.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): The trial object that contains the hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        float: The objective value (final test loss) for the trial.\n",
    "    \"\"\"\n",
    "    # Creating a trial network and optimizer with the create_model function\n",
    "    net, loss_fn, optimizer, batch_size, n_epochs, scheduler = create_model(trial)\n",
    "\n",
    "    # Training and evaluating the network with the train_and_eval function\n",
    "    train_losses, test_losses, train_accuracies, test_accuracies = train_and_eval(net, loss_fn, optimizer, batch_size, n_epochs, scheduler)\n",
    "\n",
    "    # Returning the final test loss as the objective value\n",
    "    return test_losses[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best hyperparameters with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 18:22:34,728]\u001b[0m A new study created in memory with name: no-name-d6aada26-f2b2-483f-a9c3-571ada3c43a9\u001b[0m\n",
      "/tmp/ipykernel_32604/253581879.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
      "/home/codespace/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:928: UserWarning: Using a target size (torch.Size([378])) that is different to the input size (torch.Size([378, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n",
      "/home/codespace/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:928: UserWarning: Using a target size (torch.Size([38])) that is different to the input size (torch.Size([38, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 203394538.9512, Train Accuracy: 0.0012, Test Loss: 208727337.6375, Test Accuracy: 0.0136\n",
      "Epoch 2: Train Loss: 203394536.3485, Train Accuracy: 0.0000, Test Loss: 208727334.4019, Test Accuracy: 0.0085\n",
      "Epoch 3: Train Loss: 203394532.6924, Train Accuracy: 0.0000, Test Loss: 208727330.8381, Test Accuracy: 0.0058\n",
      "Epoch 4: Train Loss: 203394529.0325, Train Accuracy: 0.0000, Test Loss: 208727327.1382, Test Accuracy: 0.0031\n",
      "Epoch 5: Train Loss: 203394526.0599, Train Accuracy: 0.0000, Test Loss: 208727325.2028, Test Accuracy: 0.0024\n",
      "Epoch 6: Train Loss: 203394523.8068, Train Accuracy: 0.0000, Test Loss: 208727323.2297, Test Accuracy: 0.0020\n",
      "Epoch 7: Train Loss: 203394521.7837, Train Accuracy: 0.0000, Test Loss: 208727321.8764, Test Accuracy: 0.0016\n",
      "Epoch 8: Train Loss: 203394520.3641, Train Accuracy: 0.0000, Test Loss: 208727319.5857, Test Accuracy: 0.0015\n",
      "Epoch 9: Train Loss: 203394518.6461, Train Accuracy: 0.0000, Test Loss: 208727318.8108, Test Accuracy: 0.0016\n",
      "Epoch 10: Train Loss: 203394517.4315, Train Accuracy: 0.0000, Test Loss: 208727317.6353, Test Accuracy: 0.0013\n",
      "Epoch 11: Train Loss: 203394516.6547, Train Accuracy: 0.0000, Test Loss: 208727316.7356, Test Accuracy: 0.0011\n",
      "Epoch 12: Train Loss: 203394515.1254, Train Accuracy: 0.0000, Test Loss: 208727315.7226, Test Accuracy: 0.0011\n",
      "Epoch 13: Train Loss: 203394514.1886, Train Accuracy: 0.0000, Test Loss: 208727314.9666, Test Accuracy: 0.0011\n",
      "Epoch 14: Train Loss: 203394514.0564, Train Accuracy: 0.0017, Test Loss: 208727314.4449, Test Accuracy: 0.0548\n",
      "Epoch 15: Train Loss: 203394512.8882, Train Accuracy: 0.0044, Test Loss: 208727314.2106, Test Accuracy: 0.0975\n",
      "Epoch 16: Train Loss: 203394512.5907, Train Accuracy: 0.0062, Test Loss: 208727314.0518, Test Accuracy: 0.1431\n",
      "Epoch 17: Train Loss: 203394510.0809, Train Accuracy: 0.0106, Test Loss: 208727316.0855, Test Accuracy: 0.3734\n",
      "Epoch 18: Train Loss: 203394507.9733, Train Accuracy: 0.0196, Test Loss: 208727314.9817, Test Accuracy: 0.4762\n",
      "Epoch 19: Train Loss: 203394506.6222, Train Accuracy: 0.0197, Test Loss: 208727314.4223, Test Accuracy: 0.5394\n",
      "Epoch 20: Train Loss: 203394506.1347, Train Accuracy: 0.0197, Test Loss: 208727314.2484, Test Accuracy: 0.5748\n",
      "Epoch 21: Train Loss: 203394505.5119, Train Accuracy: 0.0197, Test Loss: 208727313.8779, Test Accuracy: 0.6074\n",
      "Epoch 22: Train Loss: 203394505.4276, Train Accuracy: 0.0197, Test Loss: 208727313.8250, Test Accuracy: 0.6313\n",
      "Epoch 23: Train Loss: 203394505.0043, Train Accuracy: 0.0197, Test Loss: 208727313.4999, Test Accuracy: 0.6507\n",
      "Epoch 24: Train Loss: 203394504.1201, Train Accuracy: 0.0197, Test Loss: 208727313.3487, Test Accuracy: 0.6687\n",
      "Epoch 25: Train Loss: 203394503.8142, Train Accuracy: 0.0197, Test Loss: 208727313.2202, Test Accuracy: 0.6775\n",
      "Epoch 26: Train Loss: 203394503.4781, Train Accuracy: 0.0197, Test Loss: 208727313.2278, Test Accuracy: 0.6929\n",
      "Epoch 27: Train Loss: 203394502.9531, Train Accuracy: 0.0197, Test Loss: 208727313.1295, Test Accuracy: 0.7061\n",
      "Epoch 28: Train Loss: 203394502.5671, Train Accuracy: 0.0197, Test Loss: 208727313.0388, Test Accuracy: 0.7143\n",
      "Epoch 29: Train Loss: 203394502.6429, Train Accuracy: 0.0197, Test Loss: 208727312.9103, Test Accuracy: 0.7250\n",
      "Epoch 30: Train Loss: 203394502.5670, Train Accuracy: 0.0197, Test Loss: 208727312.8951, Test Accuracy: 0.7304\n",
      "Epoch 31: Train Loss: 203394501.9539, Train Accuracy: 0.0197, Test Loss: 208727312.6948, Test Accuracy: 0.7387\n",
      "Epoch 32: Train Loss: 203394502.4955, Train Accuracy: 0.0197, Test Loss: 208727312.6646, Test Accuracy: 0.7431\n",
      "Epoch 33: Train Loss: 203394501.7854, Train Accuracy: 0.0197, Test Loss: 208727312.5663, Test Accuracy: 0.7480\n",
      "Epoch 34: Train Loss: 203394501.9544, Train Accuracy: 0.0197, Test Loss: 208727312.5360, Test Accuracy: 0.7543\n",
      "Epoch 35: Train Loss: 203394501.5044, Train Accuracy: 0.0197, Test Loss: 208727312.5927, Test Accuracy: 0.7565\n",
      "Epoch 36: Train Loss: 203394501.6388, Train Accuracy: 0.0197, Test Loss: 208727312.6079, Test Accuracy: 0.7625\n",
      "Epoch 37: Train Loss: 203394501.4139, Train Accuracy: 0.0197, Test Loss: 208727312.5776, Test Accuracy: 0.7663\n",
      "Epoch 38: Train Loss: 203394501.0029, Train Accuracy: 0.0197, Test Loss: 208727312.5323, Test Accuracy: 0.7683\n",
      "Epoch 39: Train Loss: 203394501.1097, Train Accuracy: 0.0197, Test Loss: 208727312.4189, Test Accuracy: 0.7722\n",
      "Epoch 40: Train Loss: 203394500.4808, Train Accuracy: 0.0197, Test Loss: 208727312.3433, Test Accuracy: 0.7737\n",
      "Epoch 41: Train Loss: 203394500.6117, Train Accuracy: 0.0197, Test Loss: 208727312.3357, Test Accuracy: 0.7767\n",
      "Epoch 42: Train Loss: 203394500.6936, Train Accuracy: 0.0197, Test Loss: 208727312.3281, Test Accuracy: 0.7777\n",
      "Epoch 43: Train Loss: 203394500.7625, Train Accuracy: 0.0197, Test Loss: 208727312.2677, Test Accuracy: 0.7791\n",
      "Epoch 44: Train Loss: 203394500.6874, Train Accuracy: 0.0197, Test Loss: 208727312.2979, Test Accuracy: 0.7811\n",
      "Epoch 45: Train Loss: 203394500.7688, Train Accuracy: 0.0197, Test Loss: 208727312.2374, Test Accuracy: 0.7819\n",
      "Epoch 46: Train Loss: 203394500.2548, Train Accuracy: 0.0197, Test Loss: 208727312.2405, Test Accuracy: 0.7846\n",
      "Epoch 47: Train Loss: 203394500.4357, Train Accuracy: 0.0197, Test Loss: 208727312.2102, Test Accuracy: 0.7852\n",
      "Epoch 48: Train Loss: 203394500.3673, Train Accuracy: 0.0197, Test Loss: 208727312.2102, Test Accuracy: 0.7866\n",
      "Epoch 49: Train Loss: 203394500.7447, Train Accuracy: 0.0197, Test Loss: 208727312.1951, Test Accuracy: 0.7870\n",
      "Epoch 50: Train Loss: 203394500.4029, Train Accuracy: 0.0197, Test Loss: 208727312.1800, Test Accuracy: 0.7885\n",
      "Epoch 51: Train Loss: 203394500.4071, Train Accuracy: 0.0197, Test Loss: 208727312.1951, Test Accuracy: 0.7890\n",
      "Epoch 52: Train Loss: 203394500.4797, Train Accuracy: 0.0197, Test Loss: 208727312.1800, Test Accuracy: 0.7895\n",
      "Epoch 53: Train Loss: 203394500.2001, Train Accuracy: 0.0197, Test Loss: 208727312.1800, Test Accuracy: 0.7902\n",
      "Epoch 54: Train Loss: 203394500.8641, Train Accuracy: 0.0197, Test Loss: 208727312.1800, Test Accuracy: 0.7911\n",
      "Epoch 55: Train Loss: 203394500.4397, Train Accuracy: 0.0197, Test Loss: 208727312.2102, Test Accuracy: 0.7915\n",
      "Epoch 56: Train Loss: 203394500.1846, Train Accuracy: 0.0197, Test Loss: 208727312.1800, Test Accuracy: 0.7921\n",
      "Epoch 57: Train Loss: 203394500.1727, Train Accuracy: 0.0197, Test Loss: 208727312.1800, Test Accuracy: 0.7926\n",
      "Epoch 58: Train Loss: 203394500.9500, Train Accuracy: 0.0197, Test Loss: 208727312.1800, Test Accuracy: 0.7928\n",
      "Epoch 59: Train Loss: 203394500.2250, Train Accuracy: 0.0197, Test Loss: 208727312.1800, Test Accuracy: 0.7932\n",
      "Epoch 60: Train Loss: 203394499.9329, Train Accuracy: 0.0197, Test Loss: 208727312.1800, Test Accuracy: 0.7936\n",
      "Epoch 61: Train Loss: 203394500.3890, Train Accuracy: 0.0197, Test Loss: 208727312.1800, Test Accuracy: 0.7939\n",
      "Epoch 62: Train Loss: 203394500.1771, Train Accuracy: 0.0197, Test Loss: 208727312.1497, Test Accuracy: 0.7938\n",
      "Epoch 63: Train Loss: 203394500.7637, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7940\n",
      "Epoch 64: Train Loss: 203394500.3294, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7942\n",
      "Epoch 65: Train Loss: 203394500.4525, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7942\n",
      "Epoch 66: Train Loss: 203394500.4037, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7947\n",
      "Epoch 67: Train Loss: 203394500.1608, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7947\n",
      "Epoch 68: Train Loss: 203394500.4657, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7948\n",
      "Epoch 69: Train Loss: 203394499.8416, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7949\n",
      "Epoch 70: Train Loss: 203394499.8561, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7948\n",
      "Epoch 71: Train Loss: 203394500.8326, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7948\n",
      "Epoch 72: Train Loss: 203394499.8003, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7949\n",
      "Epoch 73: Train Loss: 203394500.1964, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7949\n",
      "Epoch 74: Train Loss: 203394500.1703, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7950\n",
      "Epoch 75: Train Loss: 203394500.2036, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7951\n",
      "Epoch 76: Train Loss: 203394500.1503, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7951\n",
      "Epoch 77: Train Loss: 203394500.1656, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7951\n",
      "Epoch 78: Train Loss: 203394500.5025, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7952\n",
      "Epoch 79: Train Loss: 203394500.2999, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7952\n",
      "Epoch 80: Train Loss: 203394499.7636, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7952\n",
      "Epoch 81: Train Loss: 203394500.3972, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7952\n",
      "Epoch 82: Train Loss: 203394500.1798, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7951\n",
      "Epoch 83: Train Loss: 203394500.4293, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7951\n",
      "Epoch 84: Train Loss: 203394500.4788, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7951\n",
      "Epoch 85: Train Loss: 203394500.1034, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7951\n",
      "Epoch 86: Train Loss: 203394500.2239, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7950\n",
      "Epoch 87: Train Loss: 203394500.1389, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7950\n",
      "Epoch 88: Train Loss: 203394499.8500, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7950\n",
      "Epoch 89: Train Loss: 203394500.3723, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7950\n",
      "Epoch 90: Train Loss: 203394500.1760, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7950\n",
      "Epoch 91: Train Loss: 203394500.6694, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7950\n",
      "Epoch 92: Train Loss: 203394500.3815, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7950\n",
      "Epoch 93: Train Loss: 203394500.2301, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 18:38:27,420]\u001b[0m Trial 0 finished with value: 208727312.13462 and parameters: {'n_layers': 4, 'n_units': 54, 'hidden_activation': 'Tanh', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'SGD', 'lr': 0.00013122385625362353, 'batch_size': 378, 'n_epochs': 94, 'scheduler': 'ExponentialLR'}. Best is trial 0 with value: 208727312.13462.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94: Train Loss: 203394500.4477, Train Accuracy: 0.0197, Test Loss: 208727312.1346, Test Accuracy: 0.7949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([243])) that is different to the input size (torch.Size([243, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/codespace/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 3407524570262444544.0000, Train Accuracy: 0.0000, Test Loss: 3642164249656742400.0000, Test Accuracy: 0.0109\n",
      "Epoch 2: Train Loss: 3407524572757956096.0000, Train Accuracy: 0.0000, Test Loss: 3642164249656742400.0000, Test Accuracy: 0.0109\n",
      "Epoch 3: Train Loss: 3407524554504217088.0000, Train Accuracy: 0.0000, Test Loss: 3642164249656742400.0000, Test Accuracy: 0.0109\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating a study object with Optuna\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "# Running the optimization for 100 trials\n",
    "study.optimize(objective, n_trials=100)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Printing the best parameters and the best value\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "print(\"Best value:\", study.best_value)\n",
    "\n",
    "# Creating a network with the best parameters\n",
    "best_net, best_loss_fn, best_optimizer, best_batch_size, best_n_epochs, best_scheduler = create_model(study.best_trial)\n",
    "\n",
    "# Training and evaluating the network with the best parameters\n",
    "best_train_losses, best_test_losses, best_train_accuracies, best_test_accuracies = train_and_eval(best_net, best_loss_fn, best_optimizer, best_batch_size, best_n_epochs, best_scheduler)\n",
    "\n",
    "# Saving the network with the best parameters\n",
    "torch.save(best_net.state_dict(), \"best_net.pth\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating a tensorboard writer\n",
    "writer = tbx.SummaryWriter()\n",
    "\n",
    "# Logging the losses and accuracies for each epoch\n",
    "for epoch in range(best_n_epochs):\n",
    "    writer.add_scalars(\"Loss\", {\"Train\": best_train_losses[epoch], \"Test\": best_test_losses[epoch]}, epoch + 1)\n",
    "    writer.add_scalars(\"Accuracy\", {\"Train\": best_train_accuracies[epoch], \"Test\": best_test_accuracies[epoch]}, epoch + 1)\n",
    "\n",
    "# Closing the writer\n",
    "writer.close()\n",
    "\n",
    "# Plotting the predicted values and true values for a random sample of 1000 test points\n",
    "indices = np.random.choice(len(x_test), size=1000)\n",
    "x_sample = x_test[indices]\n",
    "y_sample = y_test[indices]\n",
    "y_pred = best_net(x_sample).detach().numpy()\n",
    "plt.scatter(y_sample, y_pred)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Predicted Values vs True Values\")\n",
    "plt.savefig(\"pred_vs_true.png\")\n",
    "\n",
    "# Plotting the bias (difference between predicted values and true values) for a random sample of 1000 test points\n",
    "bias = y_pred - y_sample.numpy()\n",
    "plt.hist(bias)\n",
    "plt.xlabel(\"Bias\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Bias Distribution\")\n",
    "plt.savefig(\"bias_dist.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

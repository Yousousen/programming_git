{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network to learn a function f\n",
    "\n",
    "We use Optuna to do a type of Bayesian optimization of the hyperparameters of the model. We then train the model using these hyperparameters to approximate the function $f$ that we define in this notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this first cell to convert the notebook to a python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook script.ipynb to script\n",
      "[NbConvertApp] Writing 20039 bytes to script.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert script.ipynb --TagRemovePreprocessor.enabled=True --TagRemovePreprocessor.remove_cell_tags='{\"remove_cell\"}' --to script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import tensorboardX as tbx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "# Defining the function to approximate\n",
    "def f(x1, x2, x3):\n",
    "    return x1 + x2 + x3\n",
    "\n",
    "# Generating the data samples\n",
    "nsamples = 10**5\n",
    "\n",
    "x1min = x2min = x3min = -10\n",
    "x1max = x2max = x3max = 10\n",
    "\n",
    "np.random.seed(0)\n",
    "x1 = np.random.uniform(x1min, x1max, size=nsamples)\n",
    "x2 = np.random.uniform(x2min, x2max, size=nsamples)\n",
    "x3 = np.random.uniform(x3min, x3max, size=nsamples)\n",
    "y = f(x1, x2, x3)\n",
    "\n",
    "# Converting the data to tensors\n",
    "x1 = torch.from_numpy(x1).float()\n",
    "x2 = torch.from_numpy(x2).float()\n",
    "x3 = torch.from_numpy(x3).float()\n",
    "y = torch.from_numpy(y).float()\n",
    "\n",
    "# Stacking the inputs into a single tensor\n",
    "x = torch.stack([x1, x2, x3], dim=1)\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "train_size = int(0.8 * len(x))\n",
    "test_size = len(x) - train_size\n",
    "x_train, x_test = torch.split(x, [train_size, test_size])\n",
    "y_train, y_test = torch.split(y, [train_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if GPU is available and setting the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generating the data\n",
    "np.random.seed(0)\n",
    "x = np.random.uniform(-10, 10, size=(10000, 3))\n",
    "y = 2 * x[:, 0] - 3 * x[:, 1] + 5 * x[:, 2] + np.random.normal(0, 1, size=10000)\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "x_train = torch.tensor(x[:8000], dtype=torch.float32).to(device) # Added moving the tensor to the device\n",
    "y_train = torch.tensor(y[:8000], dtype=torch.float32).to(device) # Added moving the tensor to the device\n",
    "x_test = torch.tensor(x[8000:], dtype=torch.float32).to(device) # Added moving the tensor to the device\n",
    "y_test = torch.tensor(y[8000:], dtype=torch.float32).to(device) # Added moving the tensor to the device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a class for the network\n",
    "class Net(nn.Module):\n",
    "    \"\"\"A class for creating a network with a variable number of hidden layers and units.\n",
    "\n",
    "    Attributes:\n",
    "        n_layers (int): The number of hidden layers in the network.\n",
    "        n_units (list): A list of integers representing the number of units in each hidden layer.\n",
    "        hidden_activation (torch.nn.Module): The activation function for the hidden layers.\n",
    "        output_activation (torch.nn.Module): The activation function for the output layer.\n",
    "        layers (torch.nn.ModuleList): A list of linear layers in the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_layers, n_units, hidden_activation, output_activation):\n",
    "        \"\"\"Initializes the network with the given hyperparameters.\n",
    "\n",
    "        Args:\n",
    "            n_layers (int): The number of hidden layers in the network.\n",
    "            n_units (list): A list of integers representing the number of units in each hidden layer.\n",
    "            hidden_activation (torch.nn.Module): The activation function for the hidden layers.\n",
    "            output_activation (torch.nn.Module): The activation function for the output layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_units = n_units\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.output_activation = output_activation\n",
    "\n",
    "        # Creating a list of linear layers with different numbers of units for each layer\n",
    "        self.layers = nn.ModuleList([nn.Linear(3, n_units[0])])\n",
    "        for i in range(1, n_layers):\n",
    "            self.layers.append(nn.Linear(n_units[i - 1], n_units[i]))\n",
    "        self.layers.append(nn.Linear(n_units[-1], 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Performs a forward pass on the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, 3).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape (batch_size, 1).\n",
    "        \"\"\"\n",
    "        # Looping over the hidden layers and applying the linear transformation and the activation function\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.hidden_activation(layer(x))\n",
    "\n",
    "        # Applying the linear transformation and the activation function on the output layer\n",
    "        x = self.output_activation(self.layers[-1](x))\n",
    "\n",
    "        # Returning the output tensor\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to create a trial network and optimizer\n",
    "def create_model(trial):\n",
    "    \"\"\"Creates a trial network and optimizer based on the sampled hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): The trial object that contains the hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of (net, loss_fn, optimizer, batch_size, n_epochs,\n",
    "            scheduler), where net is the trial network,\n",
    "            loss_fn is the loss function,\n",
    "            optimizer is the optimizer,\n",
    "            batch_size is the batch size,\n",
    "            n_epochs is the number of epochs,\n",
    "            scheduler is the learning rate scheduler.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sampling the hyperparameters from the search space\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    n_units = [trial.suggest_int(f\"n_units_{i}\", 1, 256) for i in range(n_layers)]\n",
    "    hidden_activation_name = trial.suggest_categorical(\"hidden_activation\", [\"ReLU\", \"Tanh\", \"Sigmoid\"])\n",
    "    output_activation_name = trial.suggest_categorical(\"output_activation\", [\"ReLU\", \"Tanh\", \"Sigmoid\"])\n",
    "    # Added another common loss function: CrossEntropyLoss\n",
    "    loss_name = trial.suggest_categorical(\"loss\", [\"MSE\", \"MAE\", \"Huber\", \"CrossEntropy\"])\n",
    "    # Added another common optimizer: Adagrad\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"SGD\", \"Adam\", \"RMSprop\", \"Adagrad\"])\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 1, 512)\n",
    "    # Increased the maximum number of epochs that are tried\n",
    "    n_epochs = trial.suggest_int(\"n_epochs\", 10, 200)\n",
    "    # Added the possibility of having different schedulers: CosineAnnealingLR and ReduceLROnPlateau\n",
    "    scheduler_name = trial.suggest_categorical(\"scheduler\", [\"None\", \"StepLR\", \"ExponentialLR\", \"CosineAnnealingLR\", \"ReduceLROnPlateau\"])\n",
    "\n",
    "    # Creating the activation functions from their names\n",
    "    if hidden_activation_name == \"ReLU\":\n",
    "        hidden_activation = nn.ReLU()\n",
    "    elif hidden_activation_name == \"Tanh\":\n",
    "        hidden_activation = nn.Tanh()\n",
    "    else:\n",
    "        hidden_activation = nn.Sigmoid()\n",
    "\n",
    "    if output_activation_name == \"ReLU\":\n",
    "        output_activation = nn.ReLU()\n",
    "    elif output_activation_name == \"Tanh\":\n",
    "        output_activation = nn.Tanh()\n",
    "    else:\n",
    "        output_activation = nn.Sigmoid()\n",
    "\n",
    "    # Creating the loss function from its name\n",
    "    if loss_name == \"MSE\":\n",
    "        loss_fn = nn.MSELoss()\n",
    "    elif loss_name == \"MAE\":\n",
    "        loss_fn = nn.L1Loss()\n",
    "    elif loss_name == \"Huber\":\n",
    "        loss_fn = nn.SmoothL1Loss()\n",
    "    else:\n",
    "        # Added creating the CrossEntropyLoss function\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Creating the network with the sampled hyperparameters\n",
    "    net = Net(n_layers, n_units, hidden_activation, output_activation).to(device) # Added moving the network to the device\n",
    "\n",
    "    # Creating the optimizer from its name\n",
    "    if optimizer_name == \"SGD\":\n",
    "        optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr=lr)\n",
    "    else:\n",
    "        # Added creating the Adagrad optimizer\n",
    "        optimizer = optim.Adagrad(net.parameters(), lr=lr)\n",
    "\n",
    "    # Creating the learning rate scheduler from its name\n",
    "    if scheduler_name == \"StepLR\":\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif scheduler_name == \"ExponentialLR\":\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    elif scheduler_name == \"CosineAnnealingLR\":\n",
    "        # Added creating the CosineAnnealingLR scheduler\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "    elif scheduler_name == \"ReduceLROnPlateau\":\n",
    "        # Added creating the ReduceLROnPlateau scheduler\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=10)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    # Returning the network, the loss function, the optimizer, the batch size, the number of epochs and the scheduler\n",
    "    return net, loss_fn, optimizer, batch_size, n_epochs, scheduler\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The train and eval loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to train and evaluate a network on the train and test sets\n",
    "def train_and_eval(net, loss_fn, optimizer, batch_size, n_epochs, scheduler):\n",
    "    \"\"\"Trains and evaluates a network on the train and test sets.\n",
    "\n",
    "    Args:\n",
    "        net (Net): The network to train and evaluate.\n",
    "        loss_fn (torch.nn.Module): The loss function to use.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer to use.\n",
    "        batch_size (int): The batch size to use.\n",
    "        n_epochs (int): The number of epochs to train for.\n",
    "        scheduler (torch.optim.lr_scheduler._LRScheduler or None): The learning rate scheduler to use.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of (train_losses, test_losses,\n",
    "            train_accuracies, test_accuracies), where train_losses and test_losses are lists of average losses per epoch for the train and test sets,\n",
    "            train_accuracies and test_accuracies are lists of average accuracies per epoch for the train and test sets.\n",
    "    \"\"\"\n",
    "    # Creating data loaders for train and test sets\n",
    "    train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test, y_test), batch_size=batch_size)\n",
    "\n",
    "    # Initializing lists to store the losses and accuracies for each epoch\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    # Looping over the epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # Setting the network to training mode\n",
    "        net.train()\n",
    "        # Initializing variables to store the total loss and correct predictions for the train set\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # Looping over the batches in the train set\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            # Zeroing the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            y_pred = net(x_batch)\n",
    "            # Reshaping the target tensor to match the input tensor\n",
    "            y_batch = y_batch.view(-1, 1)\n",
    "            # Computing the loss\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Updating the total loss and correct predictions\n",
    "            train_loss += loss.item() * x_batch.size(0)\n",
    "            train_correct += torch.sum((y_pred - y_batch).abs() < 1e-3)\n",
    "\n",
    "        # Setting the network to evaluation mode\n",
    "        net.eval()\n",
    "        # Initializing variables to store the total loss and correct predictions for the test set\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        # Looping over the batches in the test set\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            # Forward pass\n",
    "            y_pred = net(x_batch)\n",
    "            # Reshaping the target tensor to match the input tensor\n",
    "            y_batch = y_batch.view(-1, 1)\n",
    "            # Computing the loss\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # Updating the total loss and correct predictions\n",
    "            test_loss += loss.item() * x_batch.size(0)\n",
    "            test_correct += torch.sum((y_pred - y_batch).abs() < 1e-3)\n",
    "\n",
    "        # Computing the average losses and accuracies for the train and test sets\n",
    "        train_loss /= len(x_train)\n",
    "        test_loss /= len(x_test)\n",
    "        train_accuracy = train_correct / len(x_train)\n",
    "        test_accuracy = test_correct / len(x_test)\n",
    "\n",
    "        # Appending the losses and accuracies to the lists\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # Updating the learning rate scheduler with validation loss if applicable\n",
    "        if scheduler is not None: # Added checking if scheduler is not None\n",
    "            if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(test_loss) # Added passing validation loss as metric value\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        # Printing the epoch summary\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Returning the lists of losses and accuracies\n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to compute the objective value for a trial\n",
    "def objective(trial):\n",
    "    \"\"\"Computes the objective value (final test loss) for a trial.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): The trial object that contains the hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        float: The objective value (final test loss) for the trial.\n",
    "    \"\"\"\n",
    "    # Creating a trial network and optimizer with the create_model function\n",
    "    net, loss_fn, optimizer, batch_size, n_epochs, scheduler = create_model(trial)\n",
    "\n",
    "    # Training and evaluating the network with the train_and_eval function\n",
    "    train_losses, test_losses, train_accuracies, test_accuracies = train_and_eval(net, loss_fn, optimizer, batch_size, n_epochs, scheduler)\n",
    "\n",
    "    # Returning the final test loss as the objective value\n",
    "    return test_losses[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best hyperparameters with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-04 19:10:46,328]\u001b[0m A new study created in memory with name: no-name-bf198c00-97a8-4244-b55e-c2b08b70769c\u001b[0m\n",
      "/tmp/ipykernel_27400/1147161505.py:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 2: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 3: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 4: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 5: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 6: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 7: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 8: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 9: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 10: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 11: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 12: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 13: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 14: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 15: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 16: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 17: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 18: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 19: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 20: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 21: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 22: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 23: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 24: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 25: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 26: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 27: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 28: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 29: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 30: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 31: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 32: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 33: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 34: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 35: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 36: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 37: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 38: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 39: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 40: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 41: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 42: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 43: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 44: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 45: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 46: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 47: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 48: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 49: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 50: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 51: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 52: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 53: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 54: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 55: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 56: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 57: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 58: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 59: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 60: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 61: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 62: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 63: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 64: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 65: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 66: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 67: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 68: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 69: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 70: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 71: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 72: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 73: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 74: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 75: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 76: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 77: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 78: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 79: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 80: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 81: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 82: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 83: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 84: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 85: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 86: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 87: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 88: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 89: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 90: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 91: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 92: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 93: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 94: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 95: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 96: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 97: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 98: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 99: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 100: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 101: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 102: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 103: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 104: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 105: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 106: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 107: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 108: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 109: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 110: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 111: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 112: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 113: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 114: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 115: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 116: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 117: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 118: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 119: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 120: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 121: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 122: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 123: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 124: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 125: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 126: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 127: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 128: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 129: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 130: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 131: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 132: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 133: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 134: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 135: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 136: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 137: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-04 19:11:18,067]\u001b[0m Trial 0 finished with value: 0.0 and parameters: {'n_layers': 1, 'n_units_0': 163, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'CrossEntropy', 'optimizer': 'Adagrad', 'lr': 0.0011655728647735535, 'batch_size': 36, 'n_epochs': 138, 'scheduler': 'None'}. Best is trial 0 with value: 0.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138: Train Loss: 0.0000, Train Accuracy: 0.0000, Test Loss: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch 1: Train Loss: 28.6625, Train Accuracy: 0.0000, Test Loss: 27.7361, Test Accuracy: 0.0005\n",
      "Epoch 2: Train Loss: 28.5568, Train Accuracy: 0.0001, Test Loss: 27.7326, Test Accuracy: 0.0000\n",
      "Epoch 3: Train Loss: 28.5458, Train Accuracy: 0.0000, Test Loss: 27.7309, Test Accuracy: 0.0000\n",
      "Epoch 4: Train Loss: 28.5421, Train Accuracy: 0.0000, Test Loss: 27.7182, Test Accuracy: 0.0000\n",
      "Epoch 5: Train Loss: 28.5373, Train Accuracy: 0.0000, Test Loss: 27.7138, Test Accuracy: 0.0000\n",
      "Epoch 6: Train Loss: 28.5375, Train Accuracy: 0.0000, Test Loss: 27.7124, Test Accuracy: 0.0000\n",
      "Epoch 7: Train Loss: 28.5354, Train Accuracy: 0.0000, Test Loss: 27.7244, Test Accuracy: 0.0000\n",
      "Epoch 8: Train Loss: 28.5362, Train Accuracy: 0.0000, Test Loss: 27.7149, Test Accuracy: 0.0000\n",
      "Epoch 9: Train Loss: 28.5348, Train Accuracy: 0.0000, Test Loss: 27.7137, Test Accuracy: 0.0000\n",
      "Epoch 10: Train Loss: 28.5342, Train Accuracy: 0.0000, Test Loss: 27.7137, Test Accuracy: 0.0000\n",
      "Epoch 11: Train Loss: 28.5340, Train Accuracy: 0.0000, Test Loss: 27.7137, Test Accuracy: 0.0000\n",
      "Epoch 12: Train Loss: 28.5341, Train Accuracy: 0.0001, Test Loss: 27.7139, Test Accuracy: 0.0000\n",
      "Epoch 13: Train Loss: 28.5345, Train Accuracy: 0.0003, Test Loss: 27.7141, Test Accuracy: 0.0000\n",
      "Epoch 14: Train Loss: 28.5351, Train Accuracy: 0.0000, Test Loss: 27.7143, Test Accuracy: 0.0000\n",
      "Epoch 15: Train Loss: 28.5342, Train Accuracy: 0.0000, Test Loss: 27.7112, Test Accuracy: 0.0000\n",
      "Epoch 16: Train Loss: 28.5364, Train Accuracy: 0.0000, Test Loss: 27.7128, Test Accuracy: 0.0000\n",
      "Epoch 17: Train Loss: 28.5369, Train Accuracy: 0.0000, Test Loss: 27.7114, Test Accuracy: 0.0000\n",
      "Epoch 18: Train Loss: 28.5370, Train Accuracy: 0.0000, Test Loss: 27.7207, Test Accuracy: 0.0000\n",
      "Epoch 19: Train Loss: 28.5378, Train Accuracy: 0.0000, Test Loss: 27.7144, Test Accuracy: 0.0000\n",
      "Epoch 20: Train Loss: 28.5368, Train Accuracy: 0.0000, Test Loss: 27.7126, Test Accuracy: 0.0000\n",
      "Epoch 21: Train Loss: 28.5374, Train Accuracy: 0.0000, Test Loss: 27.7182, Test Accuracy: 0.0000\n",
      "Epoch 22: Train Loss: 28.5372, Train Accuracy: 0.0000, Test Loss: 27.7146, Test Accuracy: 0.0000\n",
      "Epoch 23: Train Loss: 28.5376, Train Accuracy: 0.0000, Test Loss: 27.7245, Test Accuracy: 0.0000\n",
      "Epoch 24: Train Loss: 28.5360, Train Accuracy: 0.0000, Test Loss: 27.7128, Test Accuracy: 0.0000\n",
      "Epoch 25: Train Loss: 28.5357, Train Accuracy: 0.0000, Test Loss: 27.7171, Test Accuracy: 0.0000\n",
      "Epoch 26: Train Loss: 28.5346, Train Accuracy: 0.0000, Test Loss: 27.7180, Test Accuracy: 0.0000\n",
      "Epoch 27: Train Loss: 28.5352, Train Accuracy: 0.0000, Test Loss: 27.7106, Test Accuracy: 0.0000\n",
      "Epoch 28: Train Loss: 28.5343, Train Accuracy: 0.0001, Test Loss: 27.7123, Test Accuracy: 0.0000\n",
      "Epoch 29: Train Loss: 28.5337, Train Accuracy: 0.0000, Test Loss: 27.7122, Test Accuracy: 0.0000\n",
      "Epoch 30: Train Loss: 28.5335, Train Accuracy: 0.0000, Test Loss: 27.7122, Test Accuracy: 0.0000\n",
      "Epoch 31: Train Loss: 28.5334, Train Accuracy: 0.0001, Test Loss: 27.7122, Test Accuracy: 0.0000\n",
      "Epoch 32: Train Loss: 28.5335, Train Accuracy: 0.0000, Test Loss: 27.7121, Test Accuracy: 0.0000\n",
      "Epoch 33: Train Loss: 28.5336, Train Accuracy: 0.0000, Test Loss: 27.7124, Test Accuracy: 0.0000\n",
      "Epoch 34: Train Loss: 28.5340, Train Accuracy: 0.0000, Test Loss: 27.7124, Test Accuracy: 0.0000\n",
      "Epoch 35: Train Loss: 28.5340, Train Accuracy: 0.0001, Test Loss: 27.7128, Test Accuracy: 0.0000\n",
      "Epoch 36: Train Loss: 28.5350, Train Accuracy: 0.0000, Test Loss: 27.7142, Test Accuracy: 0.0000\n",
      "Epoch 37: Train Loss: 28.5351, Train Accuracy: 0.0000, Test Loss: 27.7163, Test Accuracy: 0.0000\n",
      "Epoch 38: Train Loss: 28.5365, Train Accuracy: 0.0001, Test Loss: 27.7169, Test Accuracy: 0.0000\n",
      "Epoch 39: Train Loss: 28.5353, Train Accuracy: 0.0000, Test Loss: 27.7183, Test Accuracy: 0.0000\n",
      "Epoch 40: Train Loss: 28.5349, Train Accuracy: 0.0000, Test Loss: 27.7095, Test Accuracy: 0.0000\n",
      "Epoch 41: Train Loss: 28.5358, Train Accuracy: 0.0000, Test Loss: 27.7129, Test Accuracy: 0.0000\n",
      "Epoch 42: Train Loss: 28.5370, Train Accuracy: 0.0000, Test Loss: 27.7174, Test Accuracy: 0.0000\n",
      "Epoch 43: Train Loss: 28.5353, Train Accuracy: 0.0000, Test Loss: 27.7137, Test Accuracy: 0.0000\n",
      "Epoch 44: Train Loss: 28.5350, Train Accuracy: 0.0000, Test Loss: 27.7123, Test Accuracy: 0.0000\n",
      "Epoch 45: Train Loss: 28.5346, Train Accuracy: 0.0000, Test Loss: 27.7130, Test Accuracy: 0.0005\n",
      "Epoch 46: Train Loss: 28.5341, Train Accuracy: 0.0000, Test Loss: 27.7120, Test Accuracy: 0.0000\n",
      "Epoch 47: Train Loss: 28.5343, Train Accuracy: 0.0000, Test Loss: 27.7129, Test Accuracy: 0.0000\n",
      "Epoch 48: Train Loss: 28.5340, Train Accuracy: 0.0000, Test Loss: 27.7117, Test Accuracy: 0.0000\n",
      "Epoch 49: Train Loss: 28.5334, Train Accuracy: 0.0000, Test Loss: 27.7116, Test Accuracy: 0.0000\n",
      "Epoch 50: Train Loss: 28.5331, Train Accuracy: 0.0000, Test Loss: 27.7116, Test Accuracy: 0.0000\n",
      "Epoch 51: Train Loss: 28.5330, Train Accuracy: 0.0000, Test Loss: 27.7116, Test Accuracy: 0.0000\n",
      "Epoch 52: Train Loss: 28.5331, Train Accuracy: 0.0000, Test Loss: 27.7117, Test Accuracy: 0.0000\n",
      "Epoch 53: Train Loss: 28.5332, Train Accuracy: 0.0000, Test Loss: 27.7115, Test Accuracy: 0.0000\n",
      "Epoch 54: Train Loss: 28.5335, Train Accuracy: 0.0000, Test Loss: 27.7118, Test Accuracy: 0.0005\n",
      "Epoch 55: Train Loss: 28.5334, Train Accuracy: 0.0000, Test Loss: 27.7115, Test Accuracy: 0.0000\n",
      "Epoch 56: Train Loss: 28.5338, Train Accuracy: 0.0000, Test Loss: 27.7127, Test Accuracy: 0.0000\n",
      "Epoch 57: Train Loss: 28.5342, Train Accuracy: 0.0000, Test Loss: 27.7192, Test Accuracy: 0.0000\n",
      "Epoch 58: Train Loss: 28.5346, Train Accuracy: 0.0000, Test Loss: 27.7120, Test Accuracy: 0.0000\n",
      "Epoch 59: Train Loss: 28.5345, Train Accuracy: 0.0000, Test Loss: 27.7215, Test Accuracy: 0.0000\n",
      "Epoch 60: Train Loss: 28.5364, Train Accuracy: 0.0000, Test Loss: 27.7109, Test Accuracy: 0.0000\n",
      "Epoch 61: Train Loss: 28.5353, Train Accuracy: 0.0000, Test Loss: 27.7161, Test Accuracy: 0.0000\n",
      "Epoch 62: Train Loss: 28.5350, Train Accuracy: 0.0000, Test Loss: 27.7141, Test Accuracy: 0.0000\n",
      "Epoch 63: Train Loss: 28.5351, Train Accuracy: 0.0000, Test Loss: 27.7131, Test Accuracy: 0.0000\n",
      "Epoch 64: Train Loss: 28.5350, Train Accuracy: 0.0000, Test Loss: 27.7144, Test Accuracy: 0.0000\n",
      "Epoch 65: Train Loss: 28.5341, Train Accuracy: 0.0003, Test Loss: 27.7130, Test Accuracy: 0.0000\n",
      "Epoch 66: Train Loss: 28.5332, Train Accuracy: 0.0000, Test Loss: 27.7115, Test Accuracy: 0.0000\n",
      "Epoch 67: Train Loss: 28.5338, Train Accuracy: 0.0000, Test Loss: 27.7118, Test Accuracy: 0.0000\n",
      "Epoch 68: Train Loss: 28.5333, Train Accuracy: 0.0000, Test Loss: 27.7113, Test Accuracy: 0.0000\n",
      "Epoch 69: Train Loss: 28.5329, Train Accuracy: 0.0000, Test Loss: 27.7117, Test Accuracy: 0.0000\n",
      "Epoch 70: Train Loss: 28.5327, Train Accuracy: 0.0000, Test Loss: 27.7117, Test Accuracy: 0.0000\n",
      "Epoch 71: Train Loss: 28.5327, Train Accuracy: 0.0000, Test Loss: 27.7117, Test Accuracy: 0.0000\n",
      "Epoch 72: Train Loss: 28.5327, Train Accuracy: 0.0000, Test Loss: 27.7118, Test Accuracy: 0.0000\n",
      "Epoch 73: Train Loss: 28.5329, Train Accuracy: 0.0000, Test Loss: 27.7118, Test Accuracy: 0.0000\n",
      "Epoch 74: Train Loss: 28.5330, Train Accuracy: 0.0000, Test Loss: 27.7114, Test Accuracy: 0.0000\n",
      "Epoch 75: Train Loss: 28.5335, Train Accuracy: 0.0000, Test Loss: 27.7117, Test Accuracy: 0.0000\n",
      "Epoch 76: Train Loss: 28.5338, Train Accuracy: 0.0000, Test Loss: 27.7114, Test Accuracy: 0.0000\n",
      "Epoch 77: Train Loss: 28.5334, Train Accuracy: 0.0000, Test Loss: 27.7107, Test Accuracy: 0.0000\n",
      "Epoch 78: Train Loss: 28.5339, Train Accuracy: 0.0000, Test Loss: 27.7227, Test Accuracy: 0.0000\n",
      "Epoch 79: Train Loss: 28.5354, Train Accuracy: 0.0000, Test Loss: 27.7107, Test Accuracy: 0.0000\n",
      "Epoch 80: Train Loss: 28.5346, Train Accuracy: 0.0000, Test Loss: 27.7168, Test Accuracy: 0.0000\n",
      "Epoch 81: Train Loss: 28.5345, Train Accuracy: 0.0000, Test Loss: 27.7164, Test Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-04 19:11:27,045]\u001b[0m Trial 1 finished with value: 27.712963032722474 and parameters: {'n_layers': 3, 'n_units_0': 44, 'n_units_1': 77, 'n_units_2': 14, 'hidden_activation': 'ReLU', 'output_activation': 'Tanh', 'loss': 'MAE', 'optimizer': 'Adagrad', 'lr': 0.043225063648159846, 'batch_size': 313, 'n_epochs': 83, 'scheduler': 'CosineAnnealingLR'}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "\u001b[33m[W 2023-05-04 19:11:27,128]\u001b[0m Trial 2 failed with parameters: {'n_layers': 1, 'n_units_0': 188, 'hidden_activation': 'Tanh', 'output_activation': 'Sigmoid', 'loss': 'MAE', 'optimizer': 'Adagrad', 'lr': 0.0004411365914680884, 'batch_size': 251, 'n_epochs': 86, 'scheduler': 'ReduceLROnPlateau'} because of the following error: UnboundLocalError(\"local variable 'test_loss' referenced before assignment\").\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_27400/851963682.py\", line 15, in objective\n",
      "    train_losses, test_losses, train_accuracies, test_accuracies = train_and_eval(net, loss_fn, optimizer, batch_size, n_epochs, scheduler)\n",
      "  File \"/tmp/ipykernel_27400/201918727.py\", line 55, in train_and_eval\n",
      "    scheduler.step(test_loss) # Added passing validation loss as metric value\n",
      "UnboundLocalError: local variable 'test_loss' referenced before assignment\n",
      "\u001b[33m[W 2023-05-04 19:11:27,128]\u001b[0m Trial 2 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82: Train Loss: 28.5349, Train Accuracy: 0.0000, Test Loss: 27.7230, Test Accuracy: 0.0000\n",
      "Epoch 83: Train Loss: 28.5357, Train Accuracy: 0.0000, Test Loss: 27.7130, Test Accuracy: 0.0005\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'test_loss' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Running the optimization for 100 trials\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/optuna/study/study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     _optimize(\n\u001b[1;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    435\u001b[0m     )\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     12\u001b[0m net, loss_fn, optimizer, batch_size, n_epochs, scheduler \u001b[39m=\u001b[39m create_model(trial)\n\u001b[1;32m     14\u001b[0m \u001b[39m# Training and evaluating the network with the train_and_eval function\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m train_losses, test_losses, train_accuracies, test_accuracies \u001b[39m=\u001b[39m train_and_eval(net, loss_fn, optimizer, batch_size, n_epochs, scheduler)\n\u001b[1;32m     17\u001b[0m \u001b[39m# Returning the final test loss as the objective value\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mreturn\u001b[39;00m test_losses[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[23], line 55\u001b[0m, in \u001b[0;36mtrain_and_eval\u001b[0;34m(net, loss_fn, optimizer, batch_size, n_epochs, scheduler)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m scheduler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39m# Added checking if scheduler is not None\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(scheduler, optim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mReduceLROnPlateau):\n\u001b[0;32m---> 55\u001b[0m         scheduler\u001b[39m.\u001b[39mstep(test_loss) \u001b[39m# Added passing validation loss as metric value\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m         scheduler\u001b[39m.\u001b[39mstep()\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'test_loss' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Creating a study object with Optuna\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "# Running the optimization for 100 trials\n",
    "study.optimize(objective, n_trials=100)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No trials are completed yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Printing the best parameters and the best value\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest parameters:\u001b[39m\u001b[39m\"\u001b[39m, study\u001b[39m.\u001b[39;49mbest_params)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest value:\u001b[39m\u001b[39m\"\u001b[39m, study\u001b[39m.\u001b[39mbest_value)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Creating a network with the best parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/optuna/study/study.py:116\u001b[0m, in \u001b[0;36mStudy.best_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbest_params\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m    106\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return parameters of the best trial in the study.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[39m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \n\u001b[1;32m    114\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbest_trial\u001b[39m.\u001b[39mparams\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/optuna/study/study.py:159\u001b[0m, in \u001b[0;36mStudy.best_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_multi_objective():\n\u001b[1;32m    154\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mA single best trial cannot be retrieved from a multi-objective study. Consider \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39musing Study.best_trials to retrieve a list containing the best trials.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[0;32m--> 159\u001b[0m \u001b[39mreturn\u001b[39;00m copy\u001b[39m.\u001b[39mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_storage\u001b[39m.\u001b[39;49mget_best_trial(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_study_id))\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/optuna/storages/_in_memory.py:250\u001b[0m, in \u001b[0;36mInMemoryStorage.get_best_trial\u001b[0;34m(self, study_id)\u001b[0m\n\u001b[1;32m    247\u001b[0m best_trial_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_studies[study_id]\u001b[39m.\u001b[39mbest_trial_id\n\u001b[1;32m    249\u001b[0m \u001b[39mif\u001b[39;00m best_trial_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo trials are completed yet.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    251\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_studies[study_id]\u001b[39m.\u001b[39mdirections) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    252\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    253\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBest trial can be obtained only for single-objective optimization.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: No trials are completed yet."
     ]
    }
   ],
   "source": [
    "\n",
    "# Printing the best parameters and the best value\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "print(\"Best value:\", study.best_value)\n",
    "\n",
    "# Creating a network with the best parameters\n",
    "best_net, best_loss_fn, best_optimizer, best_batch_size, best_n_epochs, best_scheduler = create_model(study.best_trial)\n",
    "\n",
    "# Training and evaluating the network with the best parameters\n",
    "best_train_losses, best_test_losses, best_train_accuracies, best_test_accuracies = train_and_eval(best_net, best_loss_fn, best_optimizer, best_batch_size, best_n_epochs, best_scheduler)\n",
    "\n",
    "# Saving the network with the best parameters\n",
    "torch.save(best_net.state_dict(), \"best_net.pth\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_n_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m writer \u001b[39m=\u001b[39m tbx\u001b[39m.\u001b[39mSummaryWriter()\n\u001b[1;32m      4\u001b[0m \u001b[39m# Logging the losses and accuracies for each epoch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(best_n_epochs):\n\u001b[1;32m      6\u001b[0m     writer\u001b[39m.\u001b[39madd_scalars(\u001b[39m\"\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m\"\u001b[39m, {\u001b[39m\"\u001b[39m\u001b[39mTrain\u001b[39m\u001b[39m\"\u001b[39m: best_train_losses[epoch], \u001b[39m\"\u001b[39m\u001b[39mTest\u001b[39m\u001b[39m\"\u001b[39m: best_test_losses[epoch]}, epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m     writer\u001b[39m.\u001b[39madd_scalars(\u001b[39m\"\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m\"\u001b[39m, {\u001b[39m\"\u001b[39m\u001b[39mTrain\u001b[39m\u001b[39m\"\u001b[39m: best_train_accuracies[epoch], \u001b[39m\"\u001b[39m\u001b[39mTest\u001b[39m\u001b[39m\"\u001b[39m: best_test_accuracies[epoch]}, epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_n_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating a tensorboard writer\n",
    "writer = tbx.SummaryWriter()\n",
    "\n",
    "# Logging the losses and accuracies for each epoch\n",
    "for epoch in range(best_n_epochs):\n",
    "    writer.add_scalars(\"Loss\", {\"Train\": best_train_losses[epoch], \"Test\": best_test_losses[epoch]}, epoch + 1)\n",
    "    writer.add_scalars(\"Accuracy\", {\"Train\": best_train_accuracies[epoch], \"Test\": best_test_accuracies[epoch]}, epoch + 1)\n",
    "\n",
    "# Closing the writer\n",
    "writer.close()\n",
    "\n",
    "# Plotting the predicted values and true values for a random sample of 1000 test points\n",
    "indices = np.random.choice(len(x_test), size=1000)\n",
    "x_sample = x_test[indices]\n",
    "y_sample = y_test[indices]\n",
    "y_pred = best_net(x_sample).detach().numpy()\n",
    "plt.scatter(y_sample, y_pred, alpha=0.1)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Predicted Values vs True Values\")\n",
    "plt.savefig(\"pred_vs_true.png\")\n",
    "plt.figure()\n",
    "\n",
    "# Plotting the bias (difference between predicted values and true values) for a random sample of 1000 test points\n",
    "bias = y_pred - y_sample.numpy()\n",
    "plt.hist(bias)\n",
    "plt.xlabel(\"Bias\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Bias Distribution\")\n",
    "plt.savefig(\"bias_dist.png\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Save the objects to a file\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mbest_objects.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> 5\u001b[0m     pickle\u001b[39m.\u001b[39mdump(best_net, f)\n\u001b[1;32m      6\u001b[0m     pickle\u001b[39m.\u001b[39mdump(best_loss_fn, f)\n\u001b[1;32m      7\u001b[0m     pickle\u001b[39m.\u001b[39mdump(best_optimizer, f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_net' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the objects to a file\n",
    "with open(\"best_objects.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_net, f)\n",
    "    pickle.dump(best_loss_fn, f)\n",
    "    pickle.dump(best_optimizer, f)\n",
    "    pickle.dump(best_batch_size, f)\n",
    "    pickle.dump(best_n_epochs, f)\n",
    "    pickle.dump(best_scheduler, f)\n",
    "    pickle.dump(best_train_losses, f)\n",
    "    pickle.dump(best_test_losses, f)\n",
    "    pickle.dump(best_train_accuracies, f)\n",
    "    pickle.dump(best_test_accuracies, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is not suitable for saving complex objects like PyTorch models, loss functions, optimizers, or schedulers. Pandas is mainly used for saving tabular data, such as lists of numbers or strings. If you want to save these objects for later analysis, you may want to use pickle or torch.save instead. However, if you only want to save the hyperparameters and the metrics, you can use pandas.DataFrame to create a data frame with one row and multiple columns. You can then use pandas.to_csv to write the data frame to a CSV file. For example, you could use the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_loss_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Get the names of the loss function, optimizer, and scheduler\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m loss_name \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(best_loss_fn)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m      5\u001b[0m optimizer_name \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(best_optimizer)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m      6\u001b[0m scheduler_name \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(best_scheduler)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mif\u001b[39;00m best_scheduler \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mNone\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_loss_fn' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get the names of the loss function, optimizer, and scheduler\n",
    "loss_name = type(best_loss_fn).__name__\n",
    "optimizer_name = type(best_optimizer).__name__\n",
    "scheduler_name = type(best_scheduler).__name__ if best_scheduler else \"None\"\n",
    "\n",
    "# Create a data frame with one row and multiple columns\n",
    "data = pd.DataFrame({\n",
    "    \"hidden_activation\": [best_net.hidden_activation.__class__.__name__],\n",
    "    \"output_activation\": [best_net.output_activation.__class__.__name__],\n",
    "    \"n_units\": [str(study.best_params[\"n_units\"])],\n",
    "    \"loss\": [loss_name],\n",
    "    \"optimizer\": [optimizer_name],\n",
    "    \"lr\": [best_optimizer.param_groups[0][\"lr\"]],\n",
    "    \"batch_size\": [best_batch_size],\n",
    "    \"n_epochs\": [best_n_epochs],\n",
    "    \"scheduler\": [scheduler_name],\n",
    "    \"train_loss\": [best_train_losses[-1]],\n",
    "    \"test_loss\": [best_test_losses[-1]],\n",
    "    \"train_accuracy\": [best_train_accuracies[-1]],\n",
    "    \"test_accuracy\": [best_test_accuracies[-1]]\n",
    "})\n",
    "\n",
    "# Save the data frame to a CSV file\n",
    "data.to_csv(\"best_data.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON to store study values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the values to a file\n",
    "with open(\"best_values.json\", \"w\") as f:\n",
    "    json.dump({\"params\": study.best_params, \"value\": study.best_value}, f)\n",
    "\n",
    "# Load the values from the file\n",
    "with open(\"best_values.json\", \"r\") as f:\n",
    "    best_values = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the objects from the file\n",
    "with open(\"best_objects.pkl\", \"rb\") as f:\n",
    "    best_net = pickle.load(f)\n",
    "    best_loss_fn = pickle.load(f)\n",
    "    best_optimizer = pickle.load(f)\n",
    "    best_batch_size = pickle.load(f)\n",
    "    best_n_epochs = pickle.load(f)\n",
    "    best_scheduler = pickle.load(f)\n",
    "    best_train_losses = pickle.load(f)\n",
    "    best_test_losses = pickle.load(f)\n",
    "    best_train_accuracies = pickle.load(f)\n",
    "    best_test_accuracies = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model\n",
    "\n",
    "To load your saved best_net.pth, you need to create an instance of the same model class first, and then load the state dictionary using the torch.load and load_state_dict methods. For example, you could use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 1\n",
      "Number of units: 6\n",
      "Hidden activation: ReLU\n",
      "Output activation: ReLU\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the values from the file\n",
    "with open(\"best_values.json\", \"r\") as f:\n",
    "    best_values = json.load(f)\n",
    "\n",
    "# Get the number of layers, the number of units, the hidden activation and the output activation\n",
    "n_layers = best_values[\"params\"][\"n_layers\"]\n",
    "n_units = best_values[\"params\"][\"n_units\"]\n",
    "hidden_activation = best_values[\"params\"][\"hidden_activation\"]\n",
    "output_activation = best_values[\"params\"][\"output_activation\"]\n",
    "\n",
    "# Print the values\n",
    "print(\"Number of layers:\", n_layers)\n",
    "print(\"Number of units:\", n_units)\n",
    "print(\"Hidden activation:\", hidden_activation)\n",
    "print(\"Output activation:\", output_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the same model class\n",
    "model = Net(n_layers, n_units, hidden_activation, output_activation)\n",
    "\n",
    "# Load the state dictionary from the file\n",
    "model.load_state_dict(torch.load(\"best_net.pth\"))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

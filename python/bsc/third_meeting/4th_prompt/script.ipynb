{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximating h\n",
    "\n",
    "We use Optuna to do a type of Bayesian optimization of the hyperparameters of the model. We then train the model using these hyperparameters to approximate the function $h = x_3 \\cdot x_1^{x_2}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this first cell to convert the notebook to a python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook script.ipynb to script\n",
      "[NbConvertApp] Writing 13447 bytes to script.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert script.ipynb --TagRemovePreprocessor.enabled=True --TagRemovePreprocessor.remove_cell_tags='{\"remove_cell\"}' --to script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import optuna\n",
    "import tensorboardX as tbx\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining the function to approximate\n",
    "def h(x1, x2, x3):\n",
    "    \"\"\"Returns the value of the function h(x1, x2, x3) = x3 * x1 ** x2.\n",
    "\n",
    "    Args:\n",
    "        x1 (float or array-like): The first input.\n",
    "        x2 (float or array-like): The second input.\n",
    "        x3 (float or array-like): The third input.\n",
    "\n",
    "    Returns:\n",
    "        float or array-like: The value of the function h at the given inputs.\n",
    "    \"\"\"\n",
    "    return x3 * x1 ** x2\n",
    "\n",
    "# Generating the data samples\n",
    "np.random.seed(0)\n",
    "x1 = np.random.uniform(0, 10, size=10**6)\n",
    "x2 = np.random.uniform(0, 10, size=10**6)\n",
    "x3 = np.random.uniform(0, 10, size=10**6)\n",
    "y = h(x1, x2, x3)\n",
    "\n",
    "# Converting the data to tensors\n",
    "x1 = torch.from_numpy(x1).float()\n",
    "x2 = torch.from_numpy(x2).float()\n",
    "x3 = torch.from_numpy(x3).float()\n",
    "y = torch.from_numpy(y).float()\n",
    "\n",
    "# Stacking the inputs into a single tensor\n",
    "x = torch.stack([x1, x2, x3], dim=1)\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "train_size = int(0.8 * len(x))\n",
    "test_size = len(x) - train_size\n",
    "x_train, x_test = torch.split(x, [train_size, test_size])\n",
    "y_train, y_test = torch.split(y, [train_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a custom neural network class\n",
    "class Net(nn.Module):\n",
    "    \"\"\"A custom neural network class that approximates the function h.\n",
    "\n",
    "    Args:\n",
    "        n_layers (int): The number of hidden layers in the network.\n",
    "        n_units (int): The number of hidden units in each layer.\n",
    "        hidden_activation (torch.nn.Module): The activation function for the hidden layers.\n",
    "        output_activation (torch.nn.Module): The activation function for the output layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers, n_units, hidden_activation, output_activation):\n",
    "        super(Net, self).__init__()\n",
    "        # Creating a list of linear layers with n_units each\n",
    "        self.layers = nn.ModuleList([nn.Linear(3, n_units)])\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.layers.append(nn.Linear(n_units, n_units))\n",
    "        # Adding the final output layer with one unit\n",
    "        self.layers.append(nn.Linear(n_units, 1))\n",
    "        # Storing the activation functions\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.output_activation = output_activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Performs a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, 3).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape (batch_size, 1).\n",
    "        \"\"\"\n",
    "        # Passing the input through the hidden layers with the hidden activation function\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.hidden_activation(layer(x))\n",
    "        # Passing the output through the final layer with the output activation function\n",
    "        x = self.output_activation(self.layers[-1](x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a function to create a trial network and optimizer\n",
    "def create_model(trial):\n",
    "    \"\"\"Creates a trial network and optimizer based on the sampled hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): The trial object that contains the hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of (net, loss_fn, optimizer, batch_size, n_epochs,\n",
    "            scheduler), where net is the trial network,\n",
    "            loss_fn is the loss function,\n",
    "            optimizer is the optimizer,\n",
    "            batch_size is the batch size,\n",
    "            n_epochs is the number of epochs,\n",
    "            scheduler is the learning rate scheduler.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sampling the hyperparameters from the search space\n",
    "    n_layers = 2 #trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    n_units = 32 #trial.suggest_int(\"n_units\", 32, 128)\n",
    "    hidden_activation_name = \"ReLU\" # trial.suggest_categorical(\"hidden_activation\", [\"ReLU\", \"Tanh\", \"Sigmoid\"])\n",
    "    output_activation_name = \"ReLU\" # trial.suggest_categorical(\"output_activation\", [\"ReLU\", \"Tanh\", \"Sigmoid\"])\n",
    "    loss_name = trial.suggest_categorical(\"loss\", [\"MSE\"])#, \"MAE\", \"Huber\"])\n",
    "    optimizer_name = \"SGD\" # trial.suggest_categorical(\"optimizer\", [\"SGD\", \"Adam\", \"RMSprop\"])\n",
    "    lr = 1e-3 #trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
    "    batch_size = 256 #trial.suggest_int(\"batch_size\", 32, 512)\n",
    "    n_epochs = trial.suggest_int(\"n_epochs\", 10, 100)\n",
    "    scheduler_name = None # trial.suggest_categorical(\"scheduler\", [\"None\", \"StepLR\", \"ExponentialLR\"])\n",
    "\n",
    "    # Creating the activation functions from their names\n",
    "    if hidden_activation_name == \"ReLU\":\n",
    "        hidden_activation = nn.ReLU()\n",
    "    elif hidden_activation_name == \"Tanh\":\n",
    "        hidden_activation = nn.Tanh()\n",
    "    else:\n",
    "        hidden_activation = nn.Sigmoid()\n",
    "\n",
    "    if output_activation_name == \"ReLU\":\n",
    "        output_activation = nn.ReLU()\n",
    "    elif output_activation_name == \"Tanh\":\n",
    "        output_activation = nn.Tanh()\n",
    "    else:\n",
    "        output_activation = nn.Sigmoid()\n",
    "\n",
    "    # Creating the loss function from its name\n",
    "    if loss_name == \"MSE\":\n",
    "        loss_fn = nn.MSELoss()\n",
    "    elif loss_name == \"MAE\":\n",
    "        loss_fn = nn.L1Loss()\n",
    "    else:\n",
    "        loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "    # Creating the network with the sampled hyperparameters\n",
    "    net = Net(n_layers, n_units, hidden_activation, output_activation)\n",
    "\n",
    "    # Creating the optimizer from its name\n",
    "    if optimizer_name == \"SGD\":\n",
    "        optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr=lr)\n",
    "\n",
    "    # Creating the learning rate scheduler from its name\n",
    "    if scheduler_name == \"StepLR\":\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif scheduler_name == \"ExponentialLR\":\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    # Returning the network, the loss function, the optimizer, the batch size, the number of epochs and the scheduler\n",
    "    return net, loss_fn, optimizer, batch_size, n_epochs, scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a function to train and evaluate a network\n",
    "def train_and_eval(net, loss_fn, optimizer, batch_size, n_epochs, scheduler):\n",
    "    \"\"\"Trains and evaluates a network on the train and test sets.\n",
    "\n",
    "    Args:\n",
    "        net (Net): The network to train and evaluate.\n",
    "        loss_fn (torch.nn.Module): The loss function to use.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer to use.\n",
    "        batch_size (int): The batch size to use.\n",
    "        n_epochs (int): The number of epochs to train for.\n",
    "        scheduler (torch.optim.lr_scheduler._LRScheduler or None): The learning rate scheduler to use.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of (train_losses, test_losses,\n",
    "            train_accuracies, test_accuracies), where train_losses and test_losses are lists of average losses per epoch for the train and test sets,\n",
    "            train_accuracies and test_accuracies are lists of average accuracies per epoch for the train and test sets.\n",
    "    \"\"\"\n",
    "    # Creating data loaders for train and test sets\n",
    "    train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test, y_test), batch_size=batch_size)\n",
    "\n",
    "    # Initializing lists to store the losses and accuracies for each epoch\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    # Looping over the epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # Setting the network to training mode\n",
    "        net.train()\n",
    "        # Initializing variables to store the total loss and correct predictions for the train set\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        # Looping over the batches in the train set\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            # Zeroing the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            y_pred = net(x_batch)\n",
    "            # Reshaping the target tensor to match the input tensor\n",
    "            y_batch = y_batch.view(-1, 1)\n",
    "            # Computing the loss\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Updating the total loss and correct predictions\n",
    "            train_loss += loss.item() * x_batch.size(0)\n",
    "            train_correct += torch.sum((y_pred - y_batch).abs() < 1e-3)\n",
    "\n",
    "        # Updating the learning rate scheduler if applicable\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Setting the network to evaluation mode\n",
    "        net.eval()\n",
    "        # Initializing variables to store the total loss and correct predictions for the test set\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        # Looping over the batches in the test set\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            # Forward pass\n",
    "            y_pred = net(x_batch)\n",
    "            # Reshaping the target tensor to match the input tensor\n",
    "            y_batch = y_batch.view(-1, 1)\n",
    "            # Computing the loss\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # Updating the total loss and correct predictions\n",
    "            test_loss += loss.item() * x_batch.size(0)\n",
    "            test_correct += torch.sum((y_pred - y_batch).abs() < 1e-3)\n",
    "\n",
    "        # Computing the average losses and accuracies for the train and test sets\n",
    "        train_loss /= len(x_train)\n",
    "        test_loss /= len(x_test)\n",
    "        train_accuracy = train_correct / len(x_train)\n",
    "        test_accuracy = test_correct / len(x_test)\n",
    "\n",
    "        # Appending the losses and accuracies to the lists\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # Printing the epoch summary\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Returning the lists of losses and accuracies\n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a function to compute the objective value for a trial\n",
    "def objective(trial):\n",
    "    \"\"\"Computes the objective value (final test loss) for a trial.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): The trial object that contains the hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        float: The objective value (final test loss) for the trial.\n",
    "    \"\"\"\n",
    "    # Creating a trial network and optimizer with the create_model function\n",
    "    net, loss_fn, optimizer, batch_size, n_epochs, scheduler = create_model(trial)\n",
    "\n",
    "    # Training and evaluating the network with the train_and_eval function\n",
    "    train_losses, test_losses, train_accuracies, test_accuracies = train_and_eval(net, loss_fn, optimizer, batch_size, n_epochs, scheduler)\n",
    "\n",
    "    # Returning the final test loss as the objective value\n",
    "    return test_losses[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best hyperparameters with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 18:59:28,168]\u001b[0m A new study created in memory with name: no-name-2dfb985d-e615-45fa-8b89-a2b0d82262f4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: nan, Train Accuracy: 0.0000, Test Loss: nan, Test Accuracy: 0.0000\n",
      "Epoch 2: Train Loss: nan, Train Accuracy: 0.0000, Test Loss: nan, Test Accuracy: 0.0000\n",
      "Epoch 3: Train Loss: nan, Train Accuracy: 0.0000, Test Loss: nan, Test Accuracy: 0.0000\n",
      "Epoch 4: Train Loss: nan, Train Accuracy: 0.0000, Test Loss: nan, Test Accuracy: 0.0000\n",
      "Epoch 5: Train Loss: nan, Train Accuracy: 0.0000, Test Loss: nan, Test Accuracy: 0.0000\n",
      "Epoch 6: Train Loss: nan, Train Accuracy: 0.0000, Test Loss: nan, Test Accuracy: 0.0000\n",
      "Epoch 7: Train Loss: nan, Train Accuracy: 0.0000, Test Loss: nan, Test Accuracy: 0.0000\n",
      "Epoch 8: Train Loss: nan, Train Accuracy: 0.0000, Test Loss: nan, Test Accuracy: 0.0000\n",
      "Epoch 9: Train Loss: nan, Train Accuracy: 0.0000, Test Loss: nan, Test Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-05-02 19:01:04,815]\u001b[0m Trial 0 failed with parameters: {'loss': 'MSE'} because of the following error: The value nan is not acceptable..\u001b[0m\n",
      "\u001b[33m[W 2023-05-02 19:01:04,816]\u001b[0m Trial 0 failed with value nan.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: nan, Train Accuracy: 0.0000, Test Loss: nan, Test Accuracy: 0.0000\n",
      "Epoch 1: Train Loss: nan, Train Accuracy: 0.0000, Test Loss: nan, Test Accuracy: 0.0000\n",
      "Epoch 2: Train Loss: nan, Train Accuracy: 0.0000, Test Loss: nan, Test Accuracy: 0.0000\n",
      "Epoch 3: Train Loss: nan, Train Accuracy: 0.0000, Test Loss: nan, Test Accuracy: 0.0000\n",
      "Epoch 4: Train Loss: nan, Train Accuracy: 0.0000, Test Loss: nan, Test Accuracy: 0.0000\n",
      "Epoch 5: Train Loss: nan, Train Accuracy: 0.0000, Test Loss: nan, Test Accuracy: 0.0000\n",
      "Epoch 6: Train Loss: nan, Train Accuracy: 0.0000, Test Loss: nan, Test Accuracy: 0.0000\n",
      "Epoch 7: Train Loss: nan, Train Accuracy: 0.0000, Test Loss: nan, Test Accuracy: 0.0000\n",
      "Epoch 8: Train Loss: nan, Train Accuracy: 0.0000, Test Loss: nan, Test Accuracy: 0.0000\n",
      "Epoch 9: Train Loss: nan, Train Accuracy: 0.0000, Test Loss: nan, Test Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-05-02 19:02:41,599]\u001b[0m Trial 1 failed with parameters: {'loss': 'MSE'} because of the following error: The value nan is not acceptable..\u001b[0m\n",
      "\u001b[33m[W 2023-05-02 19:02:41,600]\u001b[0m Trial 1 failed with value nan.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: nan, Train Accuracy: 0.0000, Test Loss: nan, Test Accuracy: 0.0000\n",
      "Epoch 1: Train Loss: 3407524570986831872.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 2: Train Loss: 3407524574134183936.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 3: Train Loss: 3407524579072880640.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 4: Train Loss: 3407524562179400192.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 5: Train Loss: 3407524571647397888.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 6: Train Loss: 3407524566994745344.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 7: Train Loss: 3407524568397997056.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 8: Train Loss: 3407524574575878144.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 9: Train Loss: 3407524573647821824.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 19:04:18,392]\u001b[0m Trial 2 finished with value: 3.6421642718371026e+18 and parameters: {'loss': 'MSE'}. Best is trial 2 with value: 3.6421642718371026e+18.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 3407524559837440000.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 1: Train Loss: 8732546294802695489092958814208.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 2: Train Loss: 3407524569952947200.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 3: Train Loss: 3407524567837590016.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 4: Train Loss: 3407524563385533952.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 5: Train Loss: 3407524566251115008.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 6: Train Loss: 3407524582600594944.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 7: Train Loss: 3407524567649126912.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 8: Train Loss: 3407524574770096640.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 9: Train Loss: 3407524573810858496.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 19:05:54,209]\u001b[0m Trial 3 finished with value: 3.6421642718371026e+18 and parameters: {'loss': 'MSE'}. Best is trial 2 with value: 3.6421642718371026e+18.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 3407524575267195904.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 1: Train Loss: 368751368430118296610668544.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 2: Train Loss: 3407524555804466176.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 3: Train Loss: 3407524575412537856.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 4: Train Loss: 3407524574723281408.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 5: Train Loss: 3407524559344291840.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 6: Train Loss: 3407524571947701760.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 7: Train Loss: 3407524559492124672.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 8: Train Loss: 3407524569489606144.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n",
      "Epoch 9: Train Loss: 3407524561557660672.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 19:07:29,799]\u001b[0m Trial 4 finished with value: 3.6421642718371026e+18 and parameters: {'loss': 'MSE'}. Best is trial 2 with value: 3.6421642718371026e+18.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 3407524566995432960.0000, Train Accuracy: 0.0197, Test Loss: 3642164271837102592.0000, Test Accuracy: 0.0201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-05-02 19:07:33,367]\u001b[0m Trial 5 failed with parameters: {'loss': 'MSE'} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_18399/840654580.py\", line 15, in objective\n",
      "    train_losses, test_losses, train_accuracies, test_accuracies = train_and_eval(net, loss_fn, optimizer, batch_size, n_epochs, scheduler)\n",
      "  File \"/tmp/ipykernel_18399/59979330.py\", line 36, in train_and_eval\n",
      "    for x_batch, y_batch in train_loader:\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 634, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 677, in _next_data\n",
      "    index = self._next_index()  # may raise StopIteration\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 624, in _next_index\n",
      "    return next(self._sampler_iter)  # may raise StopIteration\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/sampler.py\", line 257, in __iter__\n",
      "    if idx_in_batch == self.batch_size:\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2023-05-02 19:07:33,368]\u001b[0m Trial 5 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Running the optimization for 100 trials\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/optuna/study/study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     _optimize(\n\u001b[1;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    435\u001b[0m     )\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     12\u001b[0m net, loss_fn, optimizer, batch_size, n_epochs, scheduler \u001b[39m=\u001b[39m create_model(trial)\n\u001b[1;32m     14\u001b[0m \u001b[39m# Training and evaluating the network with the train_and_eval function\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m train_losses, test_losses, train_accuracies, test_accuracies \u001b[39m=\u001b[39m train_and_eval(net, loss_fn, optimizer, batch_size, n_epochs, scheduler)\n\u001b[1;32m     17\u001b[0m \u001b[39m# Returning the final test loss as the objective value\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mreturn\u001b[39;00m test_losses[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[14], line 36\u001b[0m, in \u001b[0;36mtrain_and_eval\u001b[0;34m(net, loss_fn, optimizer, batch_size, n_epochs, scheduler)\u001b[0m\n\u001b[1;32m     34\u001b[0m train_correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[39m# Looping over the batches in the train set\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[39mfor\u001b[39;00m x_batch, y_batch \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     37\u001b[0m     \u001b[39m# Zeroing the gradients\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     39\u001b[0m     \u001b[39m# Forward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_index\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 624\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sampler_iter)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/sampler.py:257\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m batch[idx_in_batch] \u001b[39m=\u001b[39m idx\n\u001b[1;32m    256\u001b[0m idx_in_batch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 257\u001b[0m \u001b[39mif\u001b[39;00m idx_in_batch \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size:\n\u001b[1;32m    258\u001b[0m     \u001b[39myield\u001b[39;00m batch\n\u001b[1;32m    259\u001b[0m     idx_in_batch \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating a study object with Optuna\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "# Running the optimization for 100 trials\n",
    "study.optimize(objective, n_trials=100)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Printing the best parameters and the best value\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "print(\"Best value:\", study.best_value)\n",
    "\n",
    "# Creating a network with the best parameters\n",
    "best_net, best_loss_fn, best_optimizer, best_batch_size, best_n_epochs, best_scheduler = create_model(study.best_trial)\n",
    "\n",
    "# Training and evaluating the network with the best parameters\n",
    "best_train_losses, best_test_losses, best_train_accuracies, best_test_accuracies = train_and_eval(best_net, best_loss_fn, best_optimizer, best_batch_size, best_n_epochs, best_scheduler)\n",
    "\n",
    "# Saving the network with the best parameters\n",
    "torch.save(best_net.state_dict(), \"best_net.pth\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating a tensorboard writer\n",
    "writer = tbx.SummaryWriter()\n",
    "\n",
    "# Logging the losses and accuracies for each epoch\n",
    "for epoch in range(best_n_epochs):\n",
    "    writer.add_scalars(\"Loss\", {\"Train\": best_train_losses[epoch], \"Test\": best_test_losses[epoch]}, epoch + 1)\n",
    "    writer.add_scalars(\"Accuracy\", {\"Train\": best_train_accuracies[epoch], \"Test\": best_test_accuracies[epoch]}, epoch + 1)\n",
    "\n",
    "# Closing the writer\n",
    "writer.close()\n",
    "\n",
    "# Plotting the predicted values and true values for a random sample of 1000 test points\n",
    "indices = np.random.choice(len(x_test), size=1000)\n",
    "x_sample = x_test[indices]\n",
    "y_sample = y_test[indices]\n",
    "y_pred = best_net(x_sample).detach().numpy()\n",
    "plt.scatter(y_sample, y_pred)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Predicted Values vs True Values\")\n",
    "plt.savefig(\"pred_vs_true.png\")\n",
    "\n",
    "# Plotting the bias (difference between predicted values and true values) for a random sample of 1000 test points\n",
    "bias = y_pred - y_sample.numpy()\n",
    "plt.hist(bias)\n",
    "plt.xlabel(\"Bias\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Bias Distribution\")\n",
    "plt.savefig(\"bias_dist.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

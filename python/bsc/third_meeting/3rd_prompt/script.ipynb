{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting to optimize for h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a new class that inherits from nn.Module\n",
    "class VariableNetwork(nn.Module):\n",
    "    # Define the constructor that takes the model as an argument\n",
    "    def __init__(self, model):\n",
    "        # Call the parent constructor\n",
    "        super().__init__()\n",
    "        # Assign the model to an attribute\n",
    "        self.model = model\n",
    "    \n",
    "    # Override the forward function\n",
    "    def forward(self, x):\n",
    "        # Loop over the layers in the ModuleList\n",
    "        for layer in self.model:\n",
    "            # Apply the layer to the input\n",
    "            x = layer(x)\n",
    "        # Return the final output\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the functions to be approximated\n",
    "def f(x1, x2, x3):\n",
    "    return x1 + x2 + x3\n",
    "\n",
    "def g(x1, x2, x3):\n",
    "    return x1**2 + x2**3 + 0.5 * x3\n",
    "\n",
    "def h(x1, x2, x3):\n",
    "    return x3 * x1**(x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range and step size for the input variables\n",
    "x1_range = (0, 10)\n",
    "x2_range = (0, 10)\n",
    "x3_range = (0, 10)\n",
    "dx = 0.5\n",
    "\n",
    "# Generate the input data by sampling uniformly from the ranges\n",
    "x1 = np.arange(*x1_range, dx)\n",
    "x2 = np.arange(*x2_range, dx)\n",
    "x3 = np.arange(*x3_range, dx)\n",
    "X1, X2, X3 = np.meshgrid(x1, x2, x3)\n",
    "X = np.stack([X1.flatten(), X2.flatten(), X3.flatten()], axis=1)\n",
    "\n",
    "# Compute the output data by applying the functions\n",
    "Y_f = f(X[:, 0], X[:, 1], X[:, 2])\n",
    "Y_g = g(X[:, 0], X[:, 1], X[:, 2])\n",
    "Y_h = h(X[:, 0], X[:, 1], X[:, 2])\n",
    "\n",
    "# Convert the input and output data to torch tensors\n",
    "X = torch.from_numpy(X).float()\n",
    "Y_f = torch.from_numpy(Y_f).float().unsqueeze(1)\n",
    "Y_g = torch.from_numpy(Y_g).float().unsqueeze(1)\n",
    "Y_h = torch.from_numpy(Y_h).float().unsqueeze(1)\n",
    "\n",
    "# Split the data into train and test sets (80% train, 20% test)\n",
    "train_size = int(0.8 * len(X))\n",
    "test_size = len(X) - train_size\n",
    "X_train, X_test = torch.utils.data.random_split(X, [train_size, test_size])\n",
    "Y_f_train, Y_f_test = torch.utils.data.random_split(Y_f, [train_size, test_size])\n",
    "Y_g_train, Y_g_test = torch.utils.data.random_split(Y_g, [train_size, test_size])\n",
    "Y_h_train, Y_h_test = torch.utils.data.random_split(Y_h, [train_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us have a variable number of hidden layers.\n",
    "# Define a function to create a neural network with given hyperparameters\n",
    "def create_network(input_size, output_size, hidden_sizes, activations, output_activation=None):\n",
    "    # Create a ModuleList to hold the layers\n",
    "    model = nn.ModuleList()\n",
    "    # Loop over the hidden sizes and activations\n",
    "    for hidden_size, activation in zip(hidden_sizes, activations):\n",
    "        # Add a linear layer with the input size and hidden size\n",
    "        model.append(nn.Linear(input_size, hidden_size))\n",
    "        # Use a batch normalization layer between linear and activation layers to improve training stability\n",
    "        #model.append(nn.BatchNorm1d(hidden_size))\n",
    "        # Add an activation layer with the given activation function\n",
    "        model.append(activation())\n",
    "        # Update the input size for the next layer\n",
    "        input_size = hidden_size\n",
    "    # Add the final output layer with the output size\n",
    "    model.append(nn.Linear(input_size, output_size))\n",
    "    # If an output activation function is specified, add it to the model\n",
    "    if output_activation:\n",
    "        model.append(output_activation())\n",
    "    # Return the model\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train a neural network with given hyperparameters and data\n",
    "def train_network(model, optimizer, loss_fn, batch_size, epochs,\n",
    "                  X_train, Y_train, X_test=None, Y_test=None):\n",
    "    # Create a data loader for the training data\n",
    "    train_loader = DataLoader(\n",
    "        dataset=torch.utils.data.TensorDataset(X_train, Y_train),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    # Initialize a list to store the training losses\n",
    "    train_losses = []\n",
    "    # Initialize a list to store the test losses if test data is given\n",
    "    if X_test is not None and Y_test is not None:\n",
    "        test_losses = []\n",
    "    # Loop over the number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Initialize a variable to store the running loss for this epoch\n",
    "        running_loss = 0.0\n",
    "        # Loop over the batches of training data\n",
    "        for inputs, targets in train_loader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass: compute the outputs from the inputs\n",
    "            outputs = model(inputs)\n",
    "            # Compute the loss from the outputs and targets\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            # Backward pass: compute the gradients from the loss\n",
    "            loss.backward()\n",
    "            # Update the parameters using the optimizer\n",
    "            optimizer.step()\n",
    "            # Accumulate the running loss\n",
    "            running_loss += loss.item()\n",
    "        # Compute and append the average training loss for this epoch\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        # Print the progress\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}\")\n",
    "        # If test data is given, compute and append the test loss for this epoch\n",
    "        if X_test is not None and Y_test is not None:\n",
    "            # Compute the outputs from the test inputs\n",
    "            outputs = model(X_test)\n",
    "            # Compute the loss from the outputs and test targets\n",
    "            loss = loss_fn(outputs, Y_test)\n",
    "            # Append the test loss\n",
    "            test_loss = loss.item()\n",
    "            test_losses.append(test_loss)\n",
    "            # Print the progress\n",
    "            print(f\"Epoch {epoch+1}, Test Loss: {test_loss:.4f}\")\n",
    "    # Return the train and test losses if test data is given, otherwise return only train losses\n",
    "    if X_test is not None and Y_test is not None:\n",
    "        return train_losses, test_losses\n",
    "    else:\n",
    "        return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the losses during training\n",
    "def plot_losses(train_losses, test_losses=None, function_name=None, hyperparameters=\"\"):\n",
    "    # Create a figure and an axis\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    # Plot the train losses\n",
    "    ax.plot(train_losses, label=\"Train Loss\")\n",
    "    # If test losses are given, plot them as well\n",
    "    if test_losses is not None:\n",
    "        ax.plot(test_losses, label=\"Test Loss\")\n",
    "    # Set the title, labels, and legend\n",
    "    ax.set_title(f\"Losses during Training ({hyperparameters})\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend()\n",
    "    # Save and show the plot\n",
    "    # Use format method to insert hyperparameters into file name\n",
    "    plt.savefig(f\"losses_{function_name}_{hyperparameters}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the predictions versus the true values\n",
    "def plot_predictions(model, X, Y_true, function_name, hyperparameters=\"\"):\n",
    "    # Create a figure and an axis\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    # Compute the predictions from the inputs\n",
    "    Y_pred = model(X).detach().numpy()\n",
    "    # Plot the predictions and the true values as scatter plots\n",
    "    ax.scatter(Y_true, Y_pred, label=\"Predictions\", s=2, alpha=0.3)\n",
    "    ax.scatter(Y_true, Y_true, label=\"True Values\", s=2, alpha=0.3)\n",
    "    # Set the title, labels, and legend\n",
    "    ax.set_title(f\"Predictions versus True Values ({hyperparameters})\")\n",
    "    ax.set_xlabel(\"True Value\")\n",
    "    ax.set_ylabel(\"Predicted Value\")\n",
    "    ax.legend()\n",
    "    # Save and show the plot\n",
    "    # Use format method to insert hyperparameters into file name\n",
    "    plt.savefig(f\"predictions_{function_name}_{hyperparameters}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of functions to be approximated\n",
    "functions = [f, g, h]\n",
    "# Define a list of function names for printing and plotting purposes\n",
    "function_names = [\"f\", \"g\", \"h\"]\n",
    "# Define a list of output tensors for each function\n",
    "outputs = [Y_f, Y_g, Y_h]\n",
    "# Define a list of output tensors for each function for train and test sets\n",
    "outputs_train = [Y_f_train, Y_g_train, Y_h_train]\n",
    "outputs_test = [Y_f_test, Y_g_test, Y_h_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "# Loop over each function to be approximated\n",
    "for i in range(len(functions)):\n",
    "    # Print the function name\n",
    "    print(f\"Approximating function {function_names[i]}\")\n",
    "    # Create a neural network with given hyperparameters\n",
    "    input_size = 3 # The number of input variables (x1, x2, x3)\n",
    "    output_size = 1 # The number of output variables (y)\n",
    "    # Create a network with 3 hidden layers and ReLU activations, and an optional output activation\n",
    "    hidden_sizes = [64, 128, 256, 512]\n",
    "    activations = [nn.ELU, nn.ELU, nn.ELU, nn.ELU]\n",
    "\n",
    "\n",
    "    output_activation = None\n",
    "    model = create_network(input_size, output_size,\n",
    "                        hidden_sizes, activations, output_activation=output_activation)\n",
    "\n",
    "    # Create an instance of VariableNetwork by passing the model\n",
    "    network = VariableNetwork(model)\n",
    "\n",
    "    # Create an optimizer with given hyperparameters\n",
    "    optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "    # Create a loss function with given hyperparameters\n",
    "    loss_fn = nn.MSELoss()\n",
    "    # Train the network with given hyperparameters and data\n",
    "    batch_size = 64 # The number of samples in each batch\n",
    "    epochs = 100 # The number of times to loop over the whole dataset\n",
    "    # Create a string representation of the hyperparameters\n",
    "    hyperparameters_str = f\"hidden_sizes_{hidden_sizes}_activations_{[act.__name__ for act in activations]}_optimizer_{optimizer.__class__.__name__}_lr_{optimizer.param_groups[0]['lr']}_batch_size_{batch_size}_epochs_{epochs}\"\n",
    "    if output_activation:\n",
    "        hyperparameters_str += f\"_output_activation_{output_activation.__name__}\"\n",
    "\n",
    "    if output_activation:\n",
    "        hyperparameters_str += f\"_output_activation_{output_activation.__name__}\"\n",
    "\n",
    "    train_losses, test_losses = train_network(network, optimizer, loss_fn,\n",
    "                                            batch_size, epochs,\n",
    "                                            X_train.dataset, outputs_train[i].dataset,\n",
    "                                            X_test.dataset, outputs_test[i].dataset)\n",
    "    plot_losses(train_losses, test_losses, function_names[i], hyperparameters=hyperparameters_str)\n",
    "    plot_predictions(network, X, outputs[i], function_names[i], hyperparameters=hyperparameters_str)\n",
    "\n",
    "    # Save the network with hyperparameters in the file name\n",
    "    torch.save(network, f\"network_{function_names[i]}_{hyperparameters_str}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook script.ipynb to script\n",
      "[NbConvertApp] Writing 15362 bytes to script.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert script.ipynb --TagRemovePreprocessor.enabled=True --TagRemovePreprocessor.remove_cell_tags='{\"remove_cell\"}' --to script\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Tune"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now implement Ray Tune to find good parameters for our network.\n",
    "I had asked C too to implement more different activation functions, upon which he modified the `create_network` function too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs: 8\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "num_cpus = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPUs: {num_cpus}\")\n",
    "\n",
    "input_size = 3  # The number of input variables (x1, x2, x3)\n",
    "output_size = 1  # The number of output variables (y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.suggest.skopt import SkOptSearch\n",
    "\n",
    "# Create a function to create a neural network with given hyperparameters\n",
    "def create_network(input_size, output_size, hidden_sizes, activation_classes, output_activation_class=None):\n",
    "    # Create a ModuleList to hold the layers\n",
    "    model = nn.ModuleList()\n",
    "    # Loop over the hidden sizes\n",
    "    for hidden_size, activation_class in zip(hidden_sizes, activation_classes):\n",
    "        # Add a linear layer with the input size and hidden size\n",
    "        model.append(nn.Linear(input_size, hidden_size))\n",
    "        # Add an activation layer with the given activation function\n",
    "        model.append(activation_class())\n",
    "        # Update the input size for the next layer\n",
    "        input_size = hidden_size\n",
    "    # Add the final output layer with the output size\n",
    "    model.append(nn.Linear(input_size, output_size))\n",
    "    # If an output activation function is specified, add it to the model\n",
    "    if output_activation_class:\n",
    "        model.append(output_activation_class())\n",
    "    # Return the model\n",
    "    return model\n",
    "\n",
    "def tune_network(config):\n",
    "    activation_classes = [getattr(nn, act_class_name) for act_class_name in config[\"activation_classes\"]]\n",
    "    hidden_sizes = config[\"hidden_sizes\"]\n",
    "    output_activation_class = getattr(nn, config[\"output_activation_class\"]) if config[\"output_activation_class\"] else None\n",
    "\n",
    "    model = create_network(input_size, output_size, hidden_sizes, activation_classes, output_activation_class=output_activation_class)\n",
    "    network = VariableNetwork(model)\n",
    "    optimizer = optim.Adam(network.parameters(), lr=config[\"lr\"])\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    train_losses, test_losses = train_network(network, optimizer, loss_fn,\n",
    "                                              config[\"batch_size\"], config[\"epochs\"],\n",
    "                                              X_train.dataset, Y_f_train.dataset,\n",
    "                                              X_test.dataset, Y_f_test.dataset)\n",
    "\n",
    "    tune.report(test_loss=test_losses[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "# Define the search space for SkOpt\n",
    "search_space = {\n",
    "    \"hidden_sizes\": Integer(32, 1024),\n",
    "    \"activation_classes\": Categorical([\"ReLU\", \"ELU\", \"LeakyReLU\", \"Tanh\", \"Sigmoid\"]),\n",
    "    \"output_activation_class\": Categorical([None, \"ReLU\", \"ELU\", \"LeakyReLU\", \"Tanh\", \"Sigmoid\"]),\n",
    "    \"lr\": Real(1e-4, 1e-2, \"log-uniform\"),\n",
    "    \"batch_size\": Integer(32, 256),\n",
    "    \"epochs\": Integer(10, 200),\n",
    "}\n",
    "\n",
    "# Initialize SkOpt search algorithm\n",
    "skopt_search = SkOptSearch(space=search_space, metric=\"test_loss\", mode=\"min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximating function f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 09:42:15,628\tINFO worker.py:1625 -- Started a local Ray instance.\n",
      "2023-05-02 09:42:17,703\tINFO tune.py:218 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mApproximating function \u001b[39m\u001b[39m{\u001b[39;00mfunction_names[i]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[39m# Start the tuning process\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m analysis \u001b[39m=\u001b[39m tune\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m     18\u001b[0m     tune_network,\n\u001b[1;32m     19\u001b[0m     search_alg\u001b[39m=\u001b[39;49mskopt_search,\n\u001b[1;32m     20\u001b[0m     scheduler\u001b[39m=\u001b[39;49masha_scheduler,\n\u001b[1;32m     21\u001b[0m     num_samples\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m     22\u001b[0m     resources_per_trial\u001b[39m=\u001b[39;49mresources_per_trial,\n\u001b[1;32m     23\u001b[0m     config\u001b[39m=\u001b[39;49msearch_space,\n\u001b[1;32m     24\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtune_network_\u001b[39;49m\u001b[39m{\u001b[39;49;00mfunction_names[i]\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[39m# Get the best set of hyperparameters\u001b[39;00m\n\u001b[1;32m     28\u001b[0m best_trial \u001b[39m=\u001b[39m analysis\u001b[39m.\u001b[39mget_best_trial(\u001b[39m\"\u001b[39m\u001b[39mtest_loss\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlast\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/tune/tune.py:904\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, chdir_to_trial_dir, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, trial_executor, _experiment_checkpoint_dir, _remote, _remote_string_queue, _tuner_api)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[39mwhile\u001b[39;00m (\n\u001b[1;32m    902\u001b[0m         \u001b[39mnot\u001b[39;00m runner\u001b[39m.\u001b[39mis_finished() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m experiment_interrupted_event\u001b[39m.\u001b[39mis_set()\n\u001b[1;32m    903\u001b[0m     ):\n\u001b[0;32m--> 904\u001b[0m         runner\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    905\u001b[0m         \u001b[39mif\u001b[39;00m has_verbosity(Verbosity\u001b[39m.\u001b[39mV1_EXPERIMENT):\n\u001b[1;32m    906\u001b[0m             _report_progress(runner, progress_reporter)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py:1338\u001b[0m, in \u001b[0;36mTrialRunner.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1333\u001b[0m \u001b[39mwith\u001b[39;00m warn_if_slow(\u001b[39m\"\u001b[39m\u001b[39mcallbacks.on_step_begin\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1334\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callbacks\u001b[39m.\u001b[39mon_step_begin(\n\u001b[1;32m   1335\u001b[0m         iteration\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iteration, trials\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trials\n\u001b[1;32m   1336\u001b[0m     )\n\u001b[0;32m-> 1338\u001b[0m next_trial \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_trial_queue_and_get_next_trial()\n\u001b[1;32m   1339\u001b[0m \u001b[39mif\u001b[39;00m next_trial:\n\u001b[1;32m   1340\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot new trial to run: \u001b[39m\u001b[39m{\u001b[39;00mnext_trial\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py:1315\u001b[0m, in \u001b[0;36mTrialRunner._update_trial_queue_and_get_next_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_search_alg\u001b[39m.\u001b[39mis_finished():\n\u001b[1;32m   1313\u001b[0m     \u001b[39m# Create pending trials until it fails.\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m     \u001b[39mwhile\u001b[39;00m num_pending_trials \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_pending_trials:\n\u001b[0;32m-> 1315\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_trial_queue(blocking\u001b[39m=\u001b[39;49mwait_for_trial):\n\u001b[1;32m   1316\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   1317\u001b[0m         wait_for_trial \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# wait at most one trial\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py:1054\u001b[0m, in \u001b[0;36m_TuneControllerBase._update_trial_queue\u001b[0;34m(self, blocking, timeout)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_trial_queue\u001b[39m(\u001b[39mself\u001b[39m, blocking: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, timeout: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m600\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m   1042\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Adds next trials to queue if possible.\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \n\u001b[1;32m   1044\u001b[0m \u001b[39m    Note that the timeout is currently unexposed to the user.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[39m        Boolean indicating if a new trial was created or not.\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1054\u001b[0m     trial \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_search_alg\u001b[39m.\u001b[39;49mnext_trial()\n\u001b[1;32m   1055\u001b[0m     \u001b[39mif\u001b[39;00m blocking \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m trial:\n\u001b[1;32m   1056\u001b[0m         start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/tune/search/search_generator.py:100\u001b[0m, in \u001b[0;36mSearchGenerator.next_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Provides one Trial object to be queued into the TrialRunner.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m \u001b[39mReturns:\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39m    Trial: Returns a single trial.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_finished():\n\u001b[0;32m--> 100\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_trial_if_possible(\n\u001b[1;32m    101\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_experiment\u001b[39m.\u001b[39;49mspec, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_experiment\u001b[39m.\u001b[39;49mdir_name\n\u001b[1;32m    102\u001b[0m     )\n\u001b[1;32m    103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/tune/search/search_generator.py:110\u001b[0m, in \u001b[0;36mSearchGenerator.create_trial_if_possible\u001b[0;34m(self, experiment_spec, output_path)\u001b[0m\n\u001b[1;32m    108\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mcreating trial\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m trial_id \u001b[39m=\u001b[39m Trial\u001b[39m.\u001b[39mgenerate_id()\n\u001b[0;32m--> 110\u001b[0m suggested_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msearcher\u001b[39m.\u001b[39;49msuggest(trial_id)\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m suggested_config \u001b[39m==\u001b[39m Searcher\u001b[39m.\u001b[39mFINISHED:\n\u001b[1;32m    112\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_finished \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/tune/search/skopt/skopt_search.py:304\u001b[0m, in \u001b[0;36mSkOptSearch.suggest\u001b[0;34m(self, trial_id)\u001b[0m\n\u001b[1;32m    302\u001b[0m     skopt_config \u001b[39m=\u001b[39m [suggested_config[par] \u001b[39mfor\u001b[39;00m par \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parameters]\n\u001b[1;32m    303\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     skopt_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_skopt_opt\u001b[39m.\u001b[39;49mask()\n\u001b[1;32m    305\u001b[0m     suggested_config \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parameters, skopt_config))\n\u001b[1;32m    306\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_live_trial_mapping[trial_id] \u001b[39m=\u001b[39m skopt_config\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/skopt/optimizer/optimizer.py:367\u001b[0m, in \u001b[0;36mOptimizer.ask\u001b[0;34m(self, n_points, strategy)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Query point or multiple points at which objective should be evaluated.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \n\u001b[1;32m    338\u001b[0m \u001b[39mn_points : int or None, default: None\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    364\u001b[0m \n\u001b[1;32m    365\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m n_points \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ask()\n\u001b[1;32m    369\u001b[0m supported_strategies \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mcl_min\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcl_mean\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcl_max\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    371\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(n_points, \u001b[39mint\u001b[39m) \u001b[39mand\u001b[39;00m n_points \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/skopt/optimizer/optimizer.py:434\u001b[0m, in \u001b[0;36mOptimizer._ask\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_initial_points \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_estimator_ \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    431\u001b[0m     \u001b[39m# this will not make a copy of `self.rng` and hence keep advancing\u001b[39;00m\n\u001b[1;32m    432\u001b[0m     \u001b[39m# our random state.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_samples \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 434\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspace\u001b[39m.\u001b[39;49mrvs(random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrng)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    435\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m         \u001b[39m# The samples are evaluated starting form initial_samples[0]\u001b[39;00m\n\u001b[1;32m    437\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_samples[\n\u001b[1;32m    438\u001b[0m             \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_samples) \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_initial_points]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/skopt/space/space.py:900\u001b[0m, in \u001b[0;36mSpace.rvs\u001b[0;34m(self, n_samples, random_state)\u001b[0m\n\u001b[1;32m    897\u001b[0m columns \u001b[39m=\u001b[39m []\n\u001b[1;32m    899\u001b[0m \u001b[39mfor\u001b[39;00m dim \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdimensions:\n\u001b[0;32m--> 900\u001b[0m     columns\u001b[39m.\u001b[39mappend(dim\u001b[39m.\u001b[39;49mrvs(n_samples\u001b[39m=\u001b[39;49mn_samples, random_state\u001b[39m=\u001b[39;49mrng))\n\u001b[1;32m    902\u001b[0m \u001b[39m# Transpose\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[39mreturn\u001b[39;00m _transpose_list_array(columns)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/skopt/space/space.py:158\u001b[0m, in \u001b[0;36mDimension.rvs\u001b[0;34m(self, n_samples, random_state)\u001b[0m\n\u001b[1;32m    156\u001b[0m rng \u001b[39m=\u001b[39m check_random_state(random_state)\n\u001b[1;32m    157\u001b[0m samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rvs\u001b[39m.\u001b[39mrvs(size\u001b[39m=\u001b[39mn_samples, random_state\u001b[39m=\u001b[39mrng)\n\u001b[0;32m--> 158\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minverse_transform(samples)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/skopt/space/space.py:528\u001b[0m, in \u001b[0;36mInteger.inverse_transform\u001b[0;34m(self, Xt)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Inverse transform samples from the warped space back into the\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[39m   original space.\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    526\u001b[0m \u001b[39m# The concatenation of all transformed dimensions makes Xt to be\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[39m# of type float, hence the required cast back to int.\u001b[39;00m\n\u001b[0;32m--> 528\u001b[0m inv_transform \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(Integer, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49minverse_transform(Xt)\n\u001b[1;32m    529\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inv_transform, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    530\u001b[0m     inv_transform \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(inv_transform)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/skopt/space/space.py:168\u001b[0m, in \u001b[0;36mDimension.inverse_transform\u001b[0;34m(self, Xt)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minverse_transform\u001b[39m(\u001b[39mself\u001b[39m, Xt):\n\u001b[1;32m    165\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Inverse transform samples from the warped space back into the\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39m       original space.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer\u001b[39m.\u001b[39;49minverse_transform(Xt)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/skopt/space/transformers.py:309\u001b[0m, in \u001b[0;36mPipeline.inverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minverse_transform\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m    308\u001b[0m     \u001b[39mfor\u001b[39;00m transformer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformers[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> 309\u001b[0m         X \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49minverse_transform(X)\n\u001b[1;32m    310\u001b[0m     \u001b[39mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/skopt/space/transformers.py:275\u001b[0m, in \u001b[0;36mNormalize.inverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    273\u001b[0m X_orig \u001b[39m=\u001b[39m X \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhigh \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow\n\u001b[1;32m    274\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_int:\n\u001b[0;32m--> 275\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mround(X_orig)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39;49mint)\n\u001b[1;32m    276\u001b[0m \u001b[39mreturn\u001b[39;00m X_orig\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/numpy/__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    300\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    301\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIn the future `np.\u001b[39m\u001b[39m{\u001b[39;00mattr\u001b[39m}\u001b[39;00m\u001b[39m` will be defined as the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcorresponding NumPy scalar.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 305\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    307\u001b[0m \u001b[39m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[39m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[39m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtesting\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "# Set up the scheduler, searcher, and resources\n",
    "asha_scheduler = ASHAScheduler(\n",
    "    metric=\"test_loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=200,\n",
    "    grace_period=50,\n",
    "    reduction_factor=2\n",
    ")\n",
    "\n",
    "resources_per_trial = {\"cpu\": num_cpus, \"gpu\": 0}\n",
    "\n",
    "for i in range(len(functions)):\n",
    "    # Print the function name\n",
    "    print(f\"Approximating function {function_names[i]}\")\n",
    "\n",
    "    # Start the tuning process\n",
    "    analysis = tune.run(\n",
    "        tune_network,\n",
    "        search_alg=skopt_search,\n",
    "        scheduler=asha_scheduler,\n",
    "        num_samples=50,\n",
    "        resources_per_trial=resources_per_trial,\n",
    "        config=search_space,\n",
    "        name=f\"tune_network_{function_names[i]}\"\n",
    "    )\n",
    "\n",
    "    # Get the best set of hyperparameters\n",
    "    best_trial = analysis.get_best_trial(\"test_loss\", \"min\", \"last\")\n",
    "    best_config = best_trial.config\n",
    "    print(f\"Best configuration: {best_config}\")\n",
    "\n",
    "    # Train the network with the best hyperparameters\n",
    "    best_activation_classes = [getattr(nn, act_class_name) for act_class_name in best_config[\"activation_classes\"]]\n",
    "    best_hidden_sizes = best_config[\"hidden_sizes\"]\n",
    "    best_output_activation_class = getattr(nn, best_config[\"output_activation_class\"]) if best_config[\"output_activation_class\"] else None\n",
    "    best_model = create_network(input_size, output_size, best_hidden_sizes, best_activation_classes, output_activation_class=best_output_activation_class)\n",
    "    best_network = VariableNetwork(best_model)\n",
    "    best_optimizer = optim.Adam(best_network.parameters(), lr=best_config[\"lr\"])\n",
    "    best_loss_fn = nn.MSELoss()\n",
    "\n",
    "    best_train_losses, best_test_losses = train_network(best_network, best_optimizer, best_loss_fn,\n",
    "                                                         best_config[\"batch_size\"], best_config[\"epochs\"],\n",
    "                                                         X_train.dataset, outputs_train[i].dataset,\n",
    "                                                         X_test.dataset, outputs_test[i].dataset)\n",
    "\n",
    "    # Print the test loss for the best model\n",
    "    print(f\"Test loss for the best model: {best_test_losses[-1]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3",
   "to": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting to optimize for h\n",
    "\n",
    "Note, section _Optuna_, _Two hidden layers_ is self-contained, and the code can be run from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a new class that inherits from nn.Module\n",
    "class VariableNetwork(nn.Module):\n",
    "    # Define the constructor that takes the model as an argument\n",
    "    def __init__(self, model):\n",
    "        # Call the parent constructor\n",
    "        super().__init__()\n",
    "        # Assign the model to an attribute\n",
    "        self.model = model\n",
    "    \n",
    "    # Override the forward function\n",
    "    def forward(self, x):\n",
    "        # Loop over the layers in the ModuleList\n",
    "        for layer in self.model:\n",
    "            # Apply the layer to the input\n",
    "            x = layer(x)\n",
    "        # Return the final output\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the functions to be approximated\n",
    "def f(x1, x2, x3):\n",
    "    return x1 + x2 + x3\n",
    "\n",
    "def g(x1, x2, x3):\n",
    "    return x1**2 + x2**3 + 0.5 * x3\n",
    "\n",
    "def h(x1, x2, x3):\n",
    "    return x3 * x1**(x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range and step size for the input variables\n",
    "x1_range = (0, 10)\n",
    "x2_range = (0, 10)\n",
    "x3_range = (0, 10)\n",
    "dx = 0.5\n",
    "\n",
    "# Generate the input data by sampling uniformly from the ranges\n",
    "x1 = np.arange(*x1_range, dx)\n",
    "x2 = np.arange(*x2_range, dx)\n",
    "x3 = np.arange(*x3_range, dx)\n",
    "X1, X2, X3 = np.meshgrid(x1, x2, x3)\n",
    "X = np.stack([X1.flatten(), X2.flatten(), X3.flatten()], axis=1)\n",
    "\n",
    "# Compute the output data by applying the functions\n",
    "Y_f = f(X[:, 0], X[:, 1], X[:, 2])\n",
    "Y_g = g(X[:, 0], X[:, 1], X[:, 2])\n",
    "Y_h = h(X[:, 0], X[:, 1], X[:, 2])\n",
    "\n",
    "# Convert the input and output data to torch tensors\n",
    "X = torch.from_numpy(X).float()\n",
    "Y_f = torch.from_numpy(Y_f).float().unsqueeze(1)\n",
    "Y_g = torch.from_numpy(Y_g).float().unsqueeze(1)\n",
    "Y_h = torch.from_numpy(Y_h).float().unsqueeze(1)\n",
    "\n",
    "# Split the data into train and test sets (80% train, 20% test)\n",
    "train_size = int(0.8 * len(X))\n",
    "test_size = len(X) - train_size\n",
    "X_train, X_test = torch.utils.data.random_split(X, [train_size, test_size])\n",
    "Y_f_train, Y_f_test = torch.utils.data.random_split(Y_f, [train_size, test_size])\n",
    "Y_g_train, Y_g_test = torch.utils.data.random_split(Y_g, [train_size, test_size])\n",
    "Y_h_train, Y_h_test = torch.utils.data.random_split(Y_h, [train_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us have a variable number of hidden layers.\n",
    "# Define a function to create a neural network with given hyperparameters\n",
    "def create_network(input_size, output_size, hidden_sizes, activations, output_activation=None):\n",
    "    # Create a ModuleList to hold the layers\n",
    "    model = nn.ModuleList()\n",
    "    # Loop over the hidden sizes and activations\n",
    "    for hidden_size, activation in zip(hidden_sizes, activations):\n",
    "        # Add a linear layer with the input size and hidden size\n",
    "        model.append(nn.Linear(input_size, hidden_size))\n",
    "        # Use a batch normalization layer between linear and activation layers to improve training stability\n",
    "        #model.append(nn.BatchNorm1d(hidden_size))\n",
    "        # Add an activation layer with the given activation function\n",
    "        model.append(activation())\n",
    "        # Update the input size for the next layer\n",
    "        input_size = hidden_size\n",
    "    # Add the final output layer with the output size\n",
    "    model.append(nn.Linear(input_size, output_size))\n",
    "    # If an output activation function is specified, add it to the model\n",
    "    if output_activation:\n",
    "        model.append(output_activation())\n",
    "    # Return the model\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train a neural network with given hyperparameters and data\n",
    "def train_network(model, optimizer, loss_fn, batch_size, epochs,\n",
    "                  X_train, Y_train, X_test=None, Y_test=None):\n",
    "    # Create a data loader for the training data\n",
    "    train_loader = DataLoader(\n",
    "        dataset=torch.utils.data.TensorDataset(X_train, Y_train),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    # Initialize a list to store the training losses\n",
    "    train_losses = []\n",
    "    # Initialize a list to store the test losses if test data is given\n",
    "    if X_test is not None and Y_test is not None:\n",
    "        test_losses = []\n",
    "    # Loop over the number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Initialize a variable to store the running loss for this epoch\n",
    "        running_loss = 0.0\n",
    "        # Loop over the batches of training data\n",
    "        for inputs, targets in train_loader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass: compute the outputs from the inputs\n",
    "            outputs = model(inputs)\n",
    "            # Compute the loss from the outputs and targets\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            # Backward pass: compute the gradients from the loss\n",
    "            loss.backward()\n",
    "            # Update the parameters using the optimizer\n",
    "            optimizer.step()\n",
    "            # Accumulate the running loss\n",
    "            running_loss += loss.item()\n",
    "        # Compute and append the average training loss for this epoch\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        # Print the progress\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}\")\n",
    "        # If test data is given, compute and append the test loss for this epoch\n",
    "        if X_test is not None and Y_test is not None:\n",
    "            # Compute the outputs from the test inputs\n",
    "            outputs = model(X_test)\n",
    "            # Compute the loss from the outputs and test targets\n",
    "            loss = loss_fn(outputs, Y_test)\n",
    "            # Append the test loss\n",
    "            test_loss = loss.item()\n",
    "            test_losses.append(test_loss)\n",
    "            # Print the progress\n",
    "            print(f\"Epoch {epoch+1}, Test Loss: {test_loss:.4f}\")\n",
    "    # Return the train and test losses if test data is given, otherwise return only train losses\n",
    "    if X_test is not None and Y_test is not None:\n",
    "        return train_losses, test_losses\n",
    "    else:\n",
    "        return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the losses during training\n",
    "def plot_losses(train_losses, test_losses=None, function_name=None, hyperparameters=\"\"):\n",
    "    # Create a figure and an axis\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    # Plot the train losses\n",
    "    ax.plot(train_losses, label=\"Train Loss\")\n",
    "    # If test losses are given, plot them as well\n",
    "    if test_losses is not None:\n",
    "        ax.plot(test_losses, label=\"Test Loss\")\n",
    "    # Set the title, labels, and legend\n",
    "    ax.set_title(f\"Losses during Training ({hyperparameters})\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend()\n",
    "    # Save and show the plot\n",
    "    # Use format method to insert hyperparameters into file name\n",
    "    plt.savefig(f\"losses_{function_name}_{hyperparameters}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the predictions versus the true values\n",
    "def plot_predictions(model, X, Y_true, function_name, hyperparameters=\"\"):\n",
    "    # Create a figure and an axis\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    # Compute the predictions from the inputs\n",
    "    Y_pred = model(X).detach().numpy()\n",
    "    # Plot the predictions and the true values as scatter plots\n",
    "    ax.scatter(Y_true, Y_pred, label=\"Predictions\", s=2, alpha=0.3)\n",
    "    ax.scatter(Y_true, Y_true, label=\"True Values\", s=2, alpha=0.3)\n",
    "    # Set the title, labels, and legend\n",
    "    ax.set_title(f\"Predictions versus True Values ({hyperparameters})\")\n",
    "    ax.set_xlabel(\"True Value\")\n",
    "    ax.set_ylabel(\"Predicted Value\")\n",
    "    ax.legend()\n",
    "    # Save and show the plot\n",
    "    # Use format method to insert hyperparameters into file name\n",
    "    plt.savefig(f\"predictions_{function_name}_{hyperparameters}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of functions to be approximated\n",
    "functions = [f, g, h]\n",
    "# Define a list of function names for printing and plotting purposes\n",
    "function_names = [\"f\", \"g\", \"h\"]\n",
    "# Define a list of output tensors for each function\n",
    "outputs = [Y_f, Y_g, Y_h]\n",
    "# Define a list of output tensors for each function for train and test sets\n",
    "outputs_train = [Y_f_train, Y_g_train, Y_h_train]\n",
    "outputs_test = [Y_f_test, Y_g_test, Y_h_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "# Loop over each function to be approximated\n",
    "for i in range(len(functions)):\n",
    "    # Print the function name\n",
    "    print(f\"Approximating function {function_names[i]}\")\n",
    "    # Create a neural network with given hyperparameters\n",
    "    input_size = 3 # The number of input variables (x1, x2, x3)\n",
    "    output_size = 1 # The number of output variables (y)\n",
    "    # Create a network with 3 hidden layers and ReLU activations, and an optional output activation\n",
    "    hidden_sizes = [64, 128, 256, 512]\n",
    "    activations = [nn.ELU, nn.ELU, nn.ELU, nn.ELU]\n",
    "\n",
    "\n",
    "    output_activation = None\n",
    "    model = create_network(input_size, output_size,\n",
    "                        hidden_sizes, activations, output_activation=output_activation)\n",
    "\n",
    "    # Create an instance of VariableNetwork by passing the model\n",
    "    network = VariableNetwork(model)\n",
    "\n",
    "    # Create an optimizer with given hyperparameters\n",
    "    optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "    # Create a loss function with given hyperparameters\n",
    "    loss_fn = nn.MSELoss()\n",
    "    # Train the network with given hyperparameters and data\n",
    "    batch_size = 64 # The number of samples in each batch\n",
    "    epochs = 100 # The number of times to loop over the whole dataset\n",
    "    # Create a string representation of the hyperparameters\n",
    "    hyperparameters_str = f\"hidden_sizes_{hidden_sizes}_activations_{[act.__name__ for act in activations]}_optimizer_{optimizer.__class__.__name__}_lr_{optimizer.param_groups[0]['lr']}_batch_size_{batch_size}_epochs_{epochs}\"\n",
    "    if output_activation:\n",
    "        hyperparameters_str += f\"_output_activation_{output_activation.__name__}\"\n",
    "\n",
    "    if output_activation:\n",
    "        hyperparameters_str += f\"_output_activation_{output_activation.__name__}\"\n",
    "\n",
    "    train_losses, test_losses = train_network(network, optimizer, loss_fn,\n",
    "                                            batch_size, epochs,\n",
    "                                            X_train.dataset, outputs_train[i].dataset,\n",
    "                                            X_test.dataset, outputs_test[i].dataset)\n",
    "    plot_losses(train_losses, test_losses, function_names[i], hyperparameters=hyperparameters_str)\n",
    "    plot_predictions(network, X, outputs[i], function_names[i], hyperparameters=hyperparameters_str)\n",
    "\n",
    "    # Save the network with hyperparameters in the file name\n",
    "    torch.save(network, f\"network_{function_names[i]}_{hyperparameters_str}.pt\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert notebook to python script\n",
    "\n",
    "Running the cell converts this whole notebook to a python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook script.ipynb to script\n",
      "[NbConvertApp] Writing 25100 bytes to script.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert script.ipynb --TagRemovePreprocessor.enabled=True --TagRemovePreprocessor.remove_cell_tags='{\"remove_cell\"}' --to script\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Tune"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now implement Ray Tune to find good parameters for our network.\n",
    "I had asked C too to implement more different activation functions, upon which he modified the `create_network` function too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs: 4\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "num_cpus = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPUs: {num_cpus}\")\n",
    "\n",
    "input_size = 3  # The number of input variables (x1, x2, x3)\n",
    "output_size = 1  # The number of output variables (y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.suggest.skopt import SkOptSearch\n",
    "\n",
    "# Create a function to create a neural network with given hyperparameters\n",
    "def create_network(input_size, output_size, hidden_sizes, activation_classes, output_activation_class=None):\n",
    "    # Create a ModuleList to hold the layers\n",
    "    model = nn.ModuleList()\n",
    "    # Loop over the hidden sizes\n",
    "    for hidden_size, activation_class in zip(hidden_sizes, activation_classes):\n",
    "        # Add a linear layer with the input size and hidden size\n",
    "        model.append(nn.Linear(input_size, hidden_size))\n",
    "        # Add an activation layer with the given activation function\n",
    "        model.append(activation_class())\n",
    "        # Update the input size for the next layer\n",
    "        input_size = hidden_size\n",
    "    # Add the final output layer with the output size\n",
    "    model.append(nn.Linear(input_size, output_size))\n",
    "    # If an output activation function is specified, add it to the model\n",
    "    if output_activation_class:\n",
    "        model.append(output_activation_class())\n",
    "    # Return the model\n",
    "    return model\n",
    "\n",
    "def tune_network(config):\n",
    "    activation_classes = [getattr(nn, act_class_name) for act_class_name in config[\"activation_classes\"]]\n",
    "    hidden_sizes = config[\"hidden_sizes\"]\n",
    "    output_activation_class = getattr(nn, config[\"output_activation_class\"]) if config[\"output_activation_class\"] else None\n",
    "\n",
    "    model = create_network(input_size, output_size, hidden_sizes, activation_classes, output_activation_class=output_activation_class)\n",
    "    network = VariableNetwork(model)\n",
    "    optimizer = optim.Adam(network.parameters(), lr=config[\"lr\"])\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    train_losses, test_losses = train_network(network, optimizer, loss_fn,\n",
    "                                              config[\"batch_size\"], config[\"epochs\"],\n",
    "                                              X_train.dataset, Y_f_train.dataset,\n",
    "                                              X_test.dataset, Y_f_test.dataset)\n",
    "\n",
    "    tune.report(test_loss=test_losses[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "# Define the search space for SkOpt\n",
    "# This tries just one hidden layer.\n",
    "search_space = {\n",
    "    \"hidden_sizes\": Integer(32, 1024),\n",
    "    \"activation_classes\": Categorical([\"ReLU\", \"ELU\", \"LeakyReLU\", \"Tanh\", \"Sigmoid\"]),\n",
    "    \"output_activation_class\": Categorical([None, \"ReLU\", \"ELU\", \"LeakyReLU\", \"Tanh\", \"Sigmoid\"]),\n",
    "    \"lr\": Real(1e-4, 1e-2, \"log-uniform\"),\n",
    "    \"batch_size\": Integer(32, 256),\n",
    "    \"epochs\": Integer(10, 50),\n",
    "}\n",
    "\n",
    "# Initialize SkOpt search algorithm\n",
    "skopt_search = SkOptSearch(space=search_space, metric=\"test_loss\", mode=\"min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "# Set up the scheduler, searcher, and resources\n",
    "asha_scheduler = ASHAScheduler(\n",
    "    metric=\"test_loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=200,\n",
    "    grace_period=50,\n",
    "    reduction_factor=2\n",
    ")\n",
    "\n",
    "resources_per_trial = {\"cpu\": num_cpus, \"gpu\": 0}\n",
    "\n",
    "for i in range(len(functions)):\n",
    "    # Print the function name\n",
    "    print(f\"Approximating function {function_names[i]}\")\n",
    "\n",
    "    # Start the tuning process\n",
    "    analysis = tune.run(\n",
    "        tune_network,\n",
    "        search_alg=skopt_search,\n",
    "        scheduler=asha_scheduler,\n",
    "        num_samples=50,\n",
    "        resources_per_trial=resources_per_trial,\n",
    "        config=search_space,\n",
    "        name=f\"tune_network_{function_names[i]}\"\n",
    "    )\n",
    "\n",
    "    # Get the best set of hyperparameters\n",
    "    best_trial = analysis.get_best_trial(\"test_loss\", \"min\", \"last\")\n",
    "    best_config = best_trial.config\n",
    "    print(f\"Best configuration: {best_config}\")\n",
    "\n",
    "    # Train the network with the best hyperparameters\n",
    "    best_activation_classes = [getattr(nn, act_class_name) for act_class_name in best_config[\"activation_classes\"]]\n",
    "    best_hidden_sizes = best_config[\"hidden_sizes\"]\n",
    "    best_output_activation_class = getattr(nn, best_config[\"output_activation_class\"]) if best_config[\"output_activation_class\"] else None\n",
    "    best_model = create_network(input_size, output_size, best_hidden_sizes, best_activation_classes, output_activation_class=best_output_activation_class)\n",
    "    best_network = VariableNetwork(best_model)\n",
    "    best_optimizer = optim.Adam(best_network.parameters(), lr=best_config[\"lr\"])\n",
    "    best_loss_fn = nn.MSELoss()\n",
    "\n",
    "    best_train_losses, best_test_losses = train_network(best_network, best_optimizer, best_loss_fn,\n",
    "                                                         best_config[\"batch_size\"], best_config[\"epochs\"],\n",
    "                                                         X_train.dataset, outputs_train[i].dataset,\n",
    "                                                         X_test.dataset, outputs_test[i].dataset)\n",
    "\n",
    "    # Print the test loss for the best model\n",
    "    print(f\"Test loss for the best model: {best_test_losses[-1]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna\n",
    "\n",
    "Let's see if switching to Optuna will avoid having to downgrade numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to create a neural network with given hyperparameters\n",
    "def create_network(input_size, output_size, hidden_sizes, activation_classes, output_activation_class=None):\n",
    "    # Create a ModuleList to hold the layers\n",
    "    model = nn.ModuleList()\n",
    "    # Loop over the hidden sizes\n",
    "    for hidden_size, activation_class in zip(hidden_sizes, activation_classes):\n",
    "        # Add a linear layer with the input size and hidden size\n",
    "        model.append(nn.Linear(input_size, hidden_size))\n",
    "        # Add an activation layer with the given activation function\n",
    "        model.append(activation_class())\n",
    "        # Update the input size for the next layer\n",
    "        input_size = hidden_size\n",
    "    # Add the final output layer with the output size\n",
    "    model.append(nn.Linear(input_size, output_size))\n",
    "    # If an output activation function is specified, add it to the model\n",
    "    if output_activation_class:\n",
    "        model.append(output_activation_class())\n",
    "    # Return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the function to accept a trial object from Optuna\n",
    "def tune_network(trial):\n",
    "    activation_classes = [getattr(nn, trial.suggest_categorical(\"activation_class\", [\"ReLU\", \"ELU\", \"LeakyReLU\", \"Tanh\", \"Sigmoid\"])) for _ in range(4)]\n",
    "    hidden_sizes = [trial.suggest_int(\"hidden_sizes\", 32, 1024) for _ in range(4)]\n",
    "    output_activation_class_name = trial.suggest_categorical(\"output_activation_class\", [None, \"ReLU\", \"ELU\", \"LeakyReLU\", \"Tanh\", \"Sigmoid\"])\n",
    "    output_activation_class = getattr(nn, output_activation_class_name) if output_activation_class_name else None\n",
    "    model = create_network(input_size, output_size, hidden_sizes, activation_classes, output_activation_class=output_activation_class)\n",
    "    network = VariableNetwork(model)\n",
    "    optimizer = optim.Adam(network.parameters(), lr=trial.suggest_loguniform(\"lr\", 1e-4, 1e-2))\n",
    "    loss_fn = nn.MSELoss()\n",
    "    train_losses, test_losses = train_network(network, optimizer, loss_fn,\n",
    "                                              trial.suggest_int(\"batch_size\", 32, 256), trial.suggest_int(\"epochs\", 10, 50),\n",
    "                                              X_train.dataset, Y_h_train.dataset, # NOTE: Optimizing FOR h.\n",
    "                                              X_test.dataset, Y_h_test.dataset) # NOTE: Optimizing FOR h.\n",
    "    return test_losses[-1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below optimizes for each function respectively, but I'm currently interested in optimizing for _h_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "for i in range(len(functions)):\n",
    "    # Print the function name\n",
    "    print(f\"Approximating function {function_names[i]}\")\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(tune_network, n_trials=50)\n",
    "    \n",
    "    # Get the best set of hyperparameters\n",
    "    best_trial = study.best_trial\n",
    "    best_config = best_trial.params\n",
    "    print(f\"Best configuration: {best_config}\")\n",
    "\n",
    "    # Train the network with the best hyperparameters\n",
    "    best_activation_classes = [getattr(nn, act_class_name) for act_class_name in best_config[\"activation_classes\"]]\n",
    "    best_hidden_sizes = best_config[\"hidden_sizes\"]\n",
    "    best_output_activation_class = getattr(nn, best_config[\"output_activation_class\"]) if best_config[\"output_activation_class\"] else None\n",
    "    best_model = create_network(input_size, output_size, best_hidden_sizes, best_activation_classes, output_activation_class=best_output_activation_class)\n",
    "    best_network = VariableNetwork(best_model)\n",
    "    best_optimizer = optim.Adam(best_network.parameters(), lr=best_config[\"lr\"])\n",
    "    best_loss_fn = nn.MSELoss()\n",
    "\n",
    "    best_train_losses, best_test_losses = train_network(best_network, best_optimizer, best_loss_fn,\n",
    "                                                         best_config[\"batch_size\"], best_config[\"epochs\"],\n",
    "                                                         X_train.dataset, outputs_train[i].dataset,\n",
    "                                                         X_test.dataset, outputs_test[i].dataset)\n",
    "\n",
    "    # Print the test loss for the best model\n",
    "    print(f\"Test loss for the best model: {best_test_losses[-1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we instead go ahead and just optimize for `h`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:43:28,931]\u001b[0m A new study created in memory with name: no-name-66374228-2900-4fa5-93dc-6a1cf40faeb6\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximating function h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1561/4078058256.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  optimizer = optim.Adam(network.parameters(), lr=trial.suggest_loguniform(\"lr\", 1e-4, 1e-2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 503692336922516032.0000\n",
      "Epoch 1, Test Loss: 500558647139500032.0000\n",
      "Epoch 2, Train Loss: 496831807506319232.0000\n",
      "Epoch 2, Test Loss: 500558647139500032.0000\n",
      "Epoch 3, Train Loss: 511398556689374720.0000\n",
      "Epoch 3, Test Loss: 500558647139500032.0000\n",
      "Epoch 4, Train Loss: 494898688486739712.0000\n",
      "Epoch 4, Test Loss: 500558647139500032.0000\n",
      "Epoch 5, Train Loss: 495184535661206080.0000\n",
      "Epoch 5, Test Loss: 500558647139500032.0000\n",
      "Epoch 6, Train Loss: 496304206365389056.0000\n",
      "Epoch 6, Test Loss: 500558647139500032.0000\n",
      "Epoch 7, Train Loss: 495155251610972416.0000\n",
      "Epoch 7, Test Loss: 500558647139500032.0000\n",
      "Epoch 8, Train Loss: 497787152145934720.0000\n",
      "Epoch 8, Test Loss: 500558647139500032.0000\n",
      "Epoch 9, Train Loss: 498405491805596992.0000\n",
      "Epoch 9, Test Loss: 500558647139500032.0000\n",
      "Epoch 10, Train Loss: 495098599223822208.0000\n",
      "Epoch 10, Test Loss: 500558647139500032.0000\n",
      "Epoch 11, Train Loss: 499026697408510080.0000\n",
      "Epoch 11, Test Loss: 500558647139500032.0000\n",
      "Epoch 12, Train Loss: 495840226417111296.0000\n",
      "Epoch 12, Test Loss: 500558647139500032.0000\n",
      "Epoch 13, Train Loss: 521520561381981248.0000\n",
      "Epoch 13, Test Loss: 500558647139500032.0000\n",
      "Epoch 14, Train Loss: 522491043096960512.0000\n",
      "Epoch 14, Test Loss: 500558647139500032.0000\n",
      "Epoch 15, Train Loss: 497763700645498624.0000\n",
      "Epoch 15, Test Loss: 500558647139500032.0000\n",
      "Epoch 16, Train Loss: 494950317309605056.0000\n",
      "Epoch 16, Test Loss: 500558647139500032.0000\n",
      "Epoch 17, Train Loss: 495859319094193472.0000\n",
      "Epoch 17, Test Loss: 500558647139500032.0000\n",
      "Epoch 18, Train Loss: 515419081891288704.0000\n",
      "Epoch 18, Test Loss: 500558647139500032.0000\n",
      "Epoch 19, Train Loss: 498897424782690176.0000\n",
      "Epoch 19, Test Loss: 500558647139500032.0000\n",
      "Epoch 20, Train Loss: 506542051599935872.0000\n",
      "Epoch 20, Test Loss: 500558647139500032.0000\n",
      "Epoch 21, Train Loss: 499734251505127168.0000\n",
      "Epoch 21, Test Loss: 500558647139500032.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:43:42,345]\u001b[0m Trial 0 finished with value: 5.005586471395e+17 and parameters: {'activation_class': 'Sigmoid', 'hidden_sizes': 626, 'output_activation_class': 'Sigmoid', 'lr': 0.0001975682723298191, 'batch_size': 238, 'epochs': 22}. Best is trial 0 with value: 5.005586471395e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Train Loss: 494980402355449152.0000\n",
      "Epoch 22, Test Loss: 500558647139500032.0000\n",
      "Epoch 1, Train Loss: 499384512234502912.0000\n",
      "Epoch 1, Test Loss: 500558440981069824.0000\n",
      "Epoch 2, Train Loss: 499372784673151808.0000\n",
      "Epoch 2, Test Loss: 500558337901854720.0000\n",
      "Epoch 3, Train Loss: 500533239517032192.0000\n",
      "Epoch 3, Test Loss: 500558269182377984.0000\n",
      "Epoch 4, Train Loss: 499384760563799168.0000\n",
      "Epoch 4, Test Loss: 500558097383686144.0000\n",
      "Epoch 5, Train Loss: 499415787626682880.0000\n",
      "Epoch 5, Test Loss: 500557925584994304.0000\n",
      "Epoch 6, Train Loss: 499371901997787392.0000\n",
      "Epoch 6, Test Loss: 500557822505779200.0000\n",
      "Epoch 7, Train Loss: 499550641598949696.0000\n",
      "Epoch 7, Test Loss: 500557719426564096.0000\n",
      "Epoch 8, Train Loss: 499371716029158144.0000\n",
      "Epoch 8, Test Loss: 500557581987610624.0000\n",
      "Epoch 9, Train Loss: 499433264887620800.0000\n",
      "Epoch 9, Test Loss: 500557444548657152.0000\n",
      "Epoch 10, Train Loss: 499373177756393088.0000\n",
      "Epoch 10, Test Loss: 500557307109703680.0000\n",
      "Epoch 11, Train Loss: 499371243384245184.0000\n",
      "Epoch 11, Test Loss: 500557169670750208.0000\n",
      "Epoch 12, Train Loss: 499372051390657152.0000\n",
      "Epoch 12, Test Loss: 500557100951273472.0000\n",
      "Epoch 13, Train Loss: 499372359679265344.0000\n",
      "Epoch 13, Test Loss: 500556929152581632.0000\n",
      "Epoch 14, Train Loss: 499370934605478976.0000\n",
      "Epoch 14, Test Loss: 500556791713628160.0000\n",
      "Epoch 15, Train Loss: 499371020406030016.0000\n",
      "Epoch 15, Test Loss: 500556619914936320.0000\n",
      "Epoch 16, Train Loss: 499376678234550592.0000\n",
      "Epoch 16, Test Loss: 500556585555197952.0000\n",
      "Epoch 17, Train Loss: 499372263358916864.0000\n",
      "Epoch 17, Test Loss: 500556413756506112.0000\n",
      "Epoch 18, Train Loss: 499370339849235904.0000\n",
      "Epoch 18, Test Loss: 500556310677291008.0000\n",
      "Epoch 19, Train Loss: 499495890308028352.0000\n",
      "Epoch 19, Test Loss: 500556138878599168.0000\n",
      "Epoch 20, Train Loss: 499370112973443904.0000\n",
      "Epoch 20, Test Loss: 500556035799384064.0000\n",
      "Epoch 21, Train Loss: 499372111986642048.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:45:49,281]\u001b[0m Trial 1 finished with value: 5.0055593272016896e+17 and parameters: {'activation_class': 'Tanh', 'hidden_sizes': 924, 'output_activation_class': 'LeakyReLU', 'lr': 0.006368123999707853, 'batch_size': 33, 'epochs': 21}. Best is trial 1 with value: 5.0055593272016896e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Test Loss: 500555932720168960.0000\n",
      "Epoch 1, Train Loss: 497396684605382400.0000\n",
      "Epoch 1, Test Loss: 500539886722351104.0000\n",
      "Epoch 2, Train Loss: 497230366324588224.0000\n",
      "Epoch 2, Test Loss: 499999511117037568.0000\n",
      "Epoch 3, Train Loss: 495449135789548480.0000\n",
      "Epoch 3, Test Loss: 496527974951026688.0000\n",
      "Epoch 4, Train Loss: 489602706494772032.0000\n",
      "Epoch 4, Test Loss: 488785110989012992.0000\n",
      "Epoch 5, Train Loss: 482820776368948352.0000\n",
      "Epoch 5, Test Loss: 483235326047813632.0000\n",
      "Epoch 6, Train Loss: 479132663460597056.0000\n",
      "Epoch 6, Test Loss: 481181816284250112.0000\n",
      "Epoch 7, Train Loss: 477876167451766464.0000\n",
      "Epoch 7, Test Loss: 480212459345412096.0000\n",
      "Epoch 8, Train Loss: 485828534151267840.0000\n",
      "Epoch 8, Test Loss: 479357589054816256.0000\n",
      "Epoch 9, Train Loss: 476143232419959424.0000\n",
      "Epoch 9, Test Loss: 478476639722799104.0000\n",
      "Epoch 10, Train Loss: 475389548669716992.0000\n",
      "Epoch 10, Test Loss: 477634860492521472.0000\n",
      "Epoch 11, Train Loss: 474467233998474880.0000\n",
      "Epoch 11, Test Loss: 476706529081294848.0000\n",
      "Epoch 12, Train Loss: 473554401895504640.0000\n",
      "Epoch 12, Test Loss: 475896292090839040.0000\n",
      "Epoch 13, Train Loss: 472732545125628096.0000\n",
      "Epoch 13, Test Loss: 475174325268250624.0000\n",
      "Epoch 14, Train Loss: 472052332558194944.0000\n",
      "Epoch 14, Test Loss: 474447170125168640.0000\n",
      "Epoch 15, Train Loss: 471415421352912448.0000\n",
      "Epoch 15, Test Loss: 473715926173220864.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:46:10,749]\u001b[0m Trial 2 finished with value: 4.7295015068421325e+17 and parameters: {'activation_class': 'LeakyReLU', 'hidden_sizes': 783, 'output_activation_class': 'ELU', 'lr': 0.00024463158389640533, 'batch_size': 83, 'epochs': 16}. Best is trial 2 with value: 4.7295015068421325e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Train Loss: 478037900754914304.0000\n",
      "Epoch 16, Test Loss: 472950150684213248.0000\n",
      "Epoch 1, Train Loss: 489198128913361600.0000\n",
      "Epoch 1, Test Loss: 500558647139500032.0000\n",
      "Epoch 2, Train Loss: 488946141308161088.0000\n",
      "Epoch 2, Test Loss: 500558647139500032.0000\n",
      "Epoch 3, Train Loss: 491318655588998784.0000\n",
      "Epoch 3, Test Loss: 500558647139500032.0000\n",
      "Epoch 4, Train Loss: 488946121970588992.0000\n",
      "Epoch 4, Test Loss: 500558647139500032.0000\n",
      "Epoch 5, Train Loss: 489010118452027008.0000\n",
      "Epoch 5, Test Loss: 500558647139500032.0000\n",
      "Epoch 6, Train Loss: 489029719033805312.0000\n",
      "Epoch 6, Test Loss: 500558647139500032.0000\n",
      "Epoch 7, Train Loss: 488946123205307520.0000\n",
      "Epoch 7, Test Loss: 500558647139500032.0000\n",
      "Epoch 8, Train Loss: 488946226050617152.0000\n",
      "Epoch 8, Test Loss: 500558647139500032.0000\n",
      "Epoch 9, Train Loss: 488946125682681280.0000\n",
      "Epoch 9, Test Loss: 500558647139500032.0000\n",
      "Epoch 10, Train Loss: 488946137838969152.0000\n",
      "Epoch 10, Test Loss: 500558647139500032.0000\n",
      "Epoch 11, Train Loss: 488946183559789824.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:46:23,758]\u001b[0m Trial 3 finished with value: 5.005586471395e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 876, 'output_activation_class': 'Sigmoid', 'lr': 0.007456916632292656, 'batch_size': 195, 'epochs': 11}. Best is trial 2 with value: 4.7295015068421325e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Test Loss: 500558647139500032.0000\n",
      "Epoch 1, Train Loss: 497340527415990080.0000\n",
      "Epoch 1, Test Loss: 500558647139500032.0000\n",
      "Epoch 2, Train Loss: 497328026694848384.0000\n",
      "Epoch 2, Test Loss: 500558647139500032.0000\n",
      "Epoch 3, Train Loss: 497330577778798720.0000\n",
      "Epoch 3, Test Loss: 500558647139500032.0000\n",
      "Epoch 4, Train Loss: 497325981962282112.0000\n",
      "Epoch 4, Test Loss: 500558647139500032.0000\n",
      "Epoch 5, Train Loss: 505702386430759296.0000\n",
      "Epoch 5, Test Loss: 500558647139500032.0000\n",
      "Epoch 6, Train Loss: 497326278074783040.0000\n",
      "Epoch 6, Test Loss: 500558647139500032.0000\n",
      "Epoch 7, Train Loss: 507586957841689728.0000\n",
      "Epoch 7, Test Loss: 500558647139500032.0000\n",
      "Epoch 8, Train Loss: 497342101077781632.0000\n",
      "Epoch 8, Test Loss: 500558647139500032.0000\n",
      "Epoch 9, Train Loss: 497325981249698944.0000\n",
      "Epoch 9, Test Loss: 500558647139500032.0000\n",
      "Epoch 10, Train Loss: 498419946162622336.0000\n",
      "Epoch 10, Test Loss: 500558647139500032.0000\n",
      "Epoch 11, Train Loss: 499493671177869632.0000\n",
      "Epoch 11, Test Loss: 500558647139500032.0000\n",
      "Epoch 12, Train Loss: 497351656195680448.0000\n",
      "Epoch 12, Test Loss: 500558647139500032.0000\n",
      "Epoch 13, Train Loss: 692959813435415808.0000\n",
      "Epoch 13, Test Loss: 500558647139500032.0000\n",
      "Epoch 14, Train Loss: 497326040198660608.0000\n",
      "Epoch 14, Test Loss: 500558647139500032.0000\n",
      "Epoch 15, Train Loss: 497326901094600064.0000\n",
      "Epoch 15, Test Loss: 500558647139500032.0000\n",
      "Epoch 16, Train Loss: 497325977077176064.0000\n",
      "Epoch 16, Test Loss: 500558647139500032.0000\n",
      "Epoch 17, Train Loss: 497344092271659456.0000\n",
      "Epoch 17, Test Loss: 500558647139500032.0000\n",
      "Epoch 18, Train Loss: 497337542356349568.0000\n",
      "Epoch 18, Test Loss: 500558647139500032.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:46:29,735]\u001b[0m Trial 4 finished with value: 5.005586471395e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 152, 'output_activation_class': 'Sigmoid', 'lr': 0.0003140874697449661, 'batch_size': 61, 'epochs': 19}. Best is trial 2 with value: 4.7295015068421325e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Train Loss: 497328855185387648.0000\n",
      "Epoch 19, Test Loss: 500558647139500032.0000\n",
      "Epoch 1, Train Loss: 498006317309045376.0000\n",
      "Epoch 1, Test Loss: 500558647139500032.0000\n",
      "Epoch 2, Train Loss: 498006305307610368.0000\n",
      "Epoch 2, Test Loss: 500558647139500032.0000\n",
      "Epoch 3, Train Loss: 498006319257983936.0000\n",
      "Epoch 3, Test Loss: 500558647139500032.0000\n",
      "Epoch 4, Train Loss: 498006318700915840.0000\n",
      "Epoch 4, Test Loss: 500558647139500032.0000\n",
      "Epoch 5, Train Loss: 498006308215297344.0000\n",
      "Epoch 5, Test Loss: 500558647139500032.0000\n",
      "Epoch 6, Train Loss: 498008781973638336.0000\n",
      "Epoch 6, Test Loss: 500558647139500032.0000\n",
      "Epoch 7, Train Loss: 499686398980722816.0000\n",
      "Epoch 7, Test Loss: 500558647139500032.0000\n",
      "Epoch 8, Train Loss: 498006309270965056.0000\n",
      "Epoch 8, Test Loss: 500558647139500032.0000\n",
      "Epoch 9, Train Loss: 498006316860425856.0000\n",
      "Epoch 9, Test Loss: 500558647139500032.0000\n",
      "Epoch 10, Train Loss: 498006338107393344.0000\n",
      "Epoch 10, Test Loss: 500558647139500032.0000\n",
      "Epoch 11, Train Loss: 498006313032166592.0000\n",
      "Epoch 11, Test Loss: 500558647139500032.0000\n",
      "Epoch 12, Train Loss: 498006336118147008.0000\n",
      "Epoch 12, Test Loss: 500558647139500032.0000\n",
      "Epoch 13, Train Loss: 498006320699198272.0000\n",
      "Epoch 13, Test Loss: 500558647139500032.0000\n",
      "Epoch 14, Train Loss: 498006331795958976.0000\n",
      "Epoch 14, Test Loss: 500558647139500032.0000\n",
      "Epoch 15, Train Loss: 498006321843511232.0000\n",
      "Epoch 15, Test Loss: 500558647139500032.0000\n",
      "Epoch 16, Train Loss: 498006324039500800.0000\n",
      "Epoch 16, Test Loss: 500558647139500032.0000\n",
      "Epoch 17, Train Loss: 498035844537593856.0000\n",
      "Epoch 17, Test Loss: 500558647139500032.0000\n",
      "Epoch 18, Train Loss: 498006348041270080.0000\n",
      "Epoch 18, Test Loss: 500558647139500032.0000\n",
      "Epoch 19, Train Loss: 498006311100347200.0000\n",
      "Epoch 19, Test Loss: 500558647139500032.0000\n",
      "Epoch 20, Train Loss: 498006319106491072.0000\n",
      "Epoch 20, Test Loss: 500558647139500032.0000\n",
      "Epoch 21, Train Loss: 498006339399295040.0000\n",
      "Epoch 21, Test Loss: 500558647139500032.0000\n",
      "Epoch 22, Train Loss: 498006320661696768.0000\n",
      "Epoch 22, Test Loss: 500558647139500032.0000\n",
      "Epoch 23, Train Loss: 498083003701157120.0000\n",
      "Epoch 23, Test Loss: 500558647139500032.0000\n",
      "Epoch 24, Train Loss: 498006323951265600.0000\n",
      "Epoch 24, Test Loss: 500558647139500032.0000\n",
      "Epoch 25, Train Loss: 498009614966618240.0000\n",
      "Epoch 25, Test Loss: 500558647139500032.0000\n",
      "Epoch 26, Train Loss: 498006318931342400.0000\n",
      "Epoch 26, Test Loss: 500558647139500032.0000\n",
      "Epoch 27, Train Loss: 498006941577905856.0000\n",
      "Epoch 27, Test Loss: 500558647139500032.0000\n",
      "Epoch 28, Train Loss: 498006331328360192.0000\n",
      "Epoch 28, Test Loss: 500558647139500032.0000\n",
      "Epoch 29, Train Loss: 498006332130117312.0000\n",
      "Epoch 29, Test Loss: 500558647139500032.0000\n",
      "Epoch 30, Train Loss: 498006318777485632.0000\n",
      "Epoch 30, Test Loss: 500558647139500032.0000\n",
      "Epoch 31, Train Loss: 498007031078993664.0000\n",
      "Epoch 31, Test Loss: 500558647139500032.0000\n",
      "Epoch 32, Train Loss: 498006321058410368.0000\n",
      "Epoch 32, Test Loss: 500558647139500032.0000\n",
      "Epoch 33, Train Loss: 498006318542686592.0000\n",
      "Epoch 33, Test Loss: 500558647139500032.0000\n",
      "Epoch 34, Train Loss: 498006342777915520.0000\n",
      "Epoch 34, Test Loss: 500558647139500032.0000\n",
      "Epoch 35, Train Loss: 498006338889987968.0000\n",
      "Epoch 35, Test Loss: 500558647139500032.0000\n",
      "Epoch 36, Train Loss: 498006315602862720.0000\n",
      "Epoch 36, Test Loss: 500558647139500032.0000\n",
      "Epoch 37, Train Loss: 498149235045937664.0000\n",
      "Epoch 37, Test Loss: 500558647139500032.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:47:03,464]\u001b[0m Trial 5 finished with value: 5.005586471395e+17 and parameters: {'activation_class': 'Tanh', 'hidden_sizes': 524, 'output_activation_class': 'Sigmoid', 'lr': 0.0003674730556424627, 'batch_size': 43, 'epochs': 38}. Best is trial 2 with value: 4.7295015068421325e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Train Loss: 498006316348437696.0000\n",
      "Epoch 38, Test Loss: 500558647139500032.0000\n",
      "Epoch 1, Train Loss: 495357355862996608.0000\n",
      "Epoch 1, Test Loss: 500558647139500032.0000\n",
      "Epoch 2, Train Loss: 495437568332652352.0000\n",
      "Epoch 2, Test Loss: 500558578420023296.0000\n",
      "Epoch 3, Train Loss: 495357323012736960.0000\n",
      "Epoch 3, Test Loss: 500558509700546560.0000\n",
      "Epoch 4, Train Loss: 495357299940307968.0000\n",
      "Epoch 4, Test Loss: 500558509700546560.0000\n",
      "Epoch 5, Train Loss: 495357265209295872.0000\n",
      "Epoch 5, Test Loss: 500558509700546560.0000\n",
      "Epoch 6, Train Loss: 495357250969825920.0000\n",
      "Epoch 6, Test Loss: 500558440981069824.0000\n",
      "Epoch 7, Train Loss: 495357216977007872.0000\n",
      "Epoch 7, Test Loss: 500558440981069824.0000\n",
      "Epoch 8, Train Loss: 495357224504878976.0000\n",
      "Epoch 8, Test Loss: 500558440981069824.0000\n",
      "Epoch 9, Train Loss: 495357191824305408.0000\n",
      "Epoch 9, Test Loss: 500558406621331456.0000\n",
      "Epoch 10, Train Loss: 495357174211577920.0000\n",
      "Epoch 10, Test Loss: 500558406621331456.0000\n",
      "Epoch 11, Train Loss: 495357151211581440.0000\n",
      "Epoch 11, Test Loss: 500558406621331456.0000\n",
      "Epoch 12, Train Loss: 495357169133674304.0000\n",
      "Epoch 12, Test Loss: 500558337901854720.0000\n",
      "Epoch 13, Train Loss: 495365549227301312.0000\n",
      "Epoch 13, Test Loss: 500558337901854720.0000\n",
      "Epoch 14, Train Loss: 495357089505795456.0000\n",
      "Epoch 14, Test Loss: 500558303542116352.0000\n",
      "Epoch 15, Train Loss: 502375313821490304.0000\n",
      "Epoch 15, Test Loss: 500558303542116352.0000\n",
      "Epoch 16, Train Loss: 495357046986111040.0000\n",
      "Epoch 16, Test Loss: 500558303542116352.0000\n",
      "Epoch 17, Train Loss: 495357018776206912.0000\n",
      "Epoch 17, Test Loss: 500558269182377984.0000\n",
      "Epoch 18, Train Loss: 495357009118051392.0000\n",
      "Epoch 18, Test Loss: 500558269182377984.0000\n",
      "Epoch 19, Train Loss: 495357001620873472.0000\n",
      "Epoch 19, Test Loss: 500558269182377984.0000\n",
      "Epoch 20, Train Loss: 499805450036436800.0000\n",
      "Epoch 20, Test Loss: 500558200462901248.0000\n",
      "Epoch 21, Train Loss: 495522222498231232.0000\n",
      "Epoch 21, Test Loss: 500558200462901248.0000\n",
      "Epoch 22, Train Loss: 562930898450764928.0000\n",
      "Epoch 22, Test Loss: 500558131743424512.0000\n",
      "Epoch 23, Train Loss: 495389691500482304.0000\n",
      "Epoch 23, Test Loss: 500558131743424512.0000\n",
      "Epoch 24, Train Loss: 495356893378140224.0000\n",
      "Epoch 24, Test Loss: 500558131743424512.0000\n",
      "Epoch 25, Train Loss: 495356860824842944.0000\n",
      "Epoch 25, Test Loss: 500558131743424512.0000\n",
      "Epoch 26, Train Loss: 495356871640779520.0000\n",
      "Epoch 26, Test Loss: 500558097383686144.0000\n",
      "Epoch 27, Train Loss: 495358953624470208.0000\n",
      "Epoch 27, Test Loss: 500558097383686144.0000\n",
      "Epoch 28, Train Loss: 495356821315586624.0000\n",
      "Epoch 28, Test Loss: 500558028664209408.0000\n",
      "Epoch 29, Train Loss: 495474936629439360.0000\n",
      "Epoch 29, Test Loss: 500558028664209408.0000\n",
      "Epoch 30, Train Loss: 495356789841876864.0000\n",
      "Epoch 30, Test Loss: 500558028664209408.0000\n",
      "Epoch 31, Train Loss: 495356767288220672.0000\n",
      "Epoch 31, Test Loss: 500557925584994304.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:47:25,592]\u001b[0m Trial 6 finished with value: 5.005579255849943e+17 and parameters: {'activation_class': 'Sigmoid', 'hidden_sizes': 517, 'output_activation_class': None, 'lr': 0.005027124445381254, 'batch_size': 86, 'epochs': 32}. Best is trial 2 with value: 4.7295015068421325e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Train Loss: 495356745558762432.0000\n",
      "Epoch 32, Test Loss: 500557925584994304.0000\n",
      "Epoch 1, Train Loss: 500558602748035072.0000\n",
      "Epoch 1, Test Loss: 500558647139500032.0000\n",
      "Epoch 2, Train Loss: 500558601614524416.0000\n",
      "Epoch 2, Test Loss: 500558647139500032.0000\n",
      "Epoch 3, Train Loss: 500558612774309056.0000\n",
      "Epoch 3, Test Loss: 500558647139500032.0000\n",
      "Epoch 4, Train Loss: 500558588041546944.0000\n",
      "Epoch 4, Test Loss: 500558647139500032.0000\n",
      "Epoch 5, Train Loss: 500558611281556288.0000\n",
      "Epoch 5, Test Loss: 500558647139500032.0000\n",
      "Epoch 6, Train Loss: 500558614667198464.0000\n",
      "Epoch 6, Test Loss: 500558647139500032.0000\n",
      "Epoch 7, Train Loss: 500558610529937024.0000\n",
      "Epoch 7, Test Loss: 500558647139500032.0000\n",
      "Epoch 8, Train Loss: 500558612241265472.0000\n",
      "Epoch 8, Test Loss: 500558647139500032.0000\n",
      "Epoch 9, Train Loss: 500558601673244672.0000\n",
      "Epoch 9, Test Loss: 500558647139500032.0000\n",
      "Epoch 10, Train Loss: 500558591973156480.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:47:51,440]\u001b[0m Trial 7 finished with value: 5.005586471395e+17 and parameters: {'activation_class': 'ReLU', 'hidden_sizes': 982, 'output_activation_class': 'Tanh', 'lr': 0.006780189105988336, 'batch_size': 100, 'epochs': 10}. Best is trial 2 with value: 4.7295015068421325e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Test Loss: 500558647139500032.0000\n",
      "Epoch 1, Train Loss: 499353927632473984.0000\n",
      "Epoch 1, Test Loss: 500558647139500032.0000\n",
      "Epoch 2, Train Loss: 499309336251795200.0000\n",
      "Epoch 2, Test Loss: 500558372261593088.0000\n",
      "Epoch 3, Train Loss: 499293586055339840.0000\n",
      "Epoch 3, Test Loss: 500557204030488576.0000\n",
      "Epoch 4, Train Loss: 500686583775285120.0000\n",
      "Epoch 4, Test Loss: 500553287020314624.0000\n",
      "Epoch 5, Train Loss: 499203224920370432.0000\n",
      "Epoch 5, Test Loss: 500543185257234432.0000\n",
      "Epoch 6, Train Loss: 502145570693178432.0000\n",
      "Epoch 6, Test Loss: 500522466334998528.0000\n",
      "Epoch 7, Train Loss: 505315901600772224.0000\n",
      "Epoch 7, Test Loss: 500480513094451200.0000\n",
      "Epoch 8, Train Loss: 499642051204048832.0000\n",
      "Epoch 8, Test Loss: 500404715511611392.0000\n",
      "Epoch 9, Train Loss: 499083816616867648.0000\n",
      "Epoch 9, Test Loss: 500286518011625472.0000\n",
      "Epoch 10, Train Loss: 500101171483891328.0000\n",
      "Epoch 10, Test Loss: 500099841553072128.0000\n",
      "Epoch 11, Train Loss: 498909598073707008.0000\n",
      "Epoch 11, Test Loss: 499803694968078336.0000\n",
      "Epoch 12, Train Loss: 506471103791401600.0000\n",
      "Epoch 12, Test Loss: 499413299620741120.0000\n",
      "Epoch 13, Train Loss: 497734737048732544.0000\n",
      "Epoch 13, Test Loss: 498856122103365632.0000\n",
      "Epoch 14, Train Loss: 496878825547787968.0000\n",
      "Epoch 14, Test Loss: 498154083929030656.0000\n",
      "Epoch 15, Train Loss: 496113250790461312.0000\n",
      "Epoch 15, Test Loss: 497228604376088576.0000\n",
      "Epoch 16, Train Loss: 498210371535868224.0000\n",
      "Epoch 16, Test Loss: 496137923201073152.0000\n",
      "Epoch 17, Train Loss: 494694379614378944.0000\n",
      "Epoch 17, Test Loss: 494830741314600960.0000\n",
      "Epoch 18, Train Loss: 495285358647709120.0000\n",
      "Epoch 18, Test Loss: 493404090617823232.0000\n",
      "Epoch 19, Train Loss: 498109796373757952.0000\n",
      "Epoch 19, Test Loss: 491745236809154560.0000\n",
      "Epoch 20, Train Loss: 489263826476742464.0000\n",
      "Epoch 20, Test Loss: 490111671767924736.0000\n",
      "Epoch 21, Train Loss: 494158333945520768.0000\n",
      "Epoch 21, Test Loss: 488585240390926336.0000\n",
      "Epoch 22, Train Loss: 486286554044393408.0000\n",
      "Epoch 22, Test Loss: 486921060822810624.0000\n",
      "Epoch 23, Train Loss: 484468277289007808.0000\n",
      "Epoch 23, Test Loss: 485489256165277696.0000\n",
      "Epoch 24, Train Loss: 484931148120021504.0000\n",
      "Epoch 24, Test Loss: 484173003307876352.0000\n",
      "Epoch 25, Train Loss: 483199249599409408.0000\n",
      "Epoch 25, Test Loss: 483190177351598080.0000\n",
      "Epoch 26, Train Loss: 481324183016213120.0000\n",
      "Epoch 26, Test Loss: 482361970217975808.0000\n",
      "Epoch 27, Train Loss: 481513073617467648.0000\n",
      "Epoch 27, Test Loss: 481555959475339264.0000\n",
      "Epoch 28, Train Loss: 479636794634651840.0000\n",
      "Epoch 28, Test Loss: 480989504828604416.0000\n",
      "Epoch 29, Train Loss: 479796582218288128.0000\n",
      "Epoch 29, Test Loss: 480539014298861568.0000\n",
      "Epoch 30, Train Loss: 484807347748137792.0000\n",
      "Epoch 30, Test Loss: 480197547218960384.0000\n",
      "Epoch 31, Train Loss: 480314562355077760.0000\n",
      "Epoch 31, Test Loss: 479790624837468160.0000\n",
      "Epoch 32, Train Loss: 482783265591443904.0000\n",
      "Epoch 32, Test Loss: 479563403887640576.0000\n",
      "Epoch 33, Train Loss: 479083823718522240.0000\n",
      "Epoch 33, Test Loss: 479232897564278784.0000\n",
      "Epoch 34, Train Loss: 477555579749244736.0000\n",
      "Epoch 34, Test Loss: 478975611843379200.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:48:09,801]\u001b[0m Trial 8 finished with value: 4.787612757954396e+17 and parameters: {'activation_class': 'ReLU', 'hidden_sizes': 500, 'output_activation_class': None, 'lr': 0.0001464320277635724, 'batch_size': 217, 'epochs': 35}. Best is trial 2 with value: 4.7295015068421325e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Train Loss: 479501554036974208.0000\n",
      "Epoch 35, Test Loss: 478761275795439616.0000\n",
      "Epoch 1, Train Loss: 491668296999895040.0000\n",
      "Epoch 1, Test Loss: 500479241784131584.0000\n",
      "Epoch 2, Train Loss: 489833892070380864.0000\n",
      "Epoch 2, Test Loss: 497237022511988736.0000\n",
      "Epoch 3, Train Loss: 481653078399909888.0000\n",
      "Epoch 3, Test Loss: 484243681289699328.0000\n",
      "Epoch 4, Train Loss: 472424381949498688.0000\n",
      "Epoch 4, Test Loss: 479668063650709504.0000\n",
      "Epoch 5, Train Loss: 469130815739352384.0000\n",
      "Epoch 5, Test Loss: 476888738773860352.0000\n",
      "Epoch 6, Train Loss: 466116848276297024.0000\n",
      "Epoch 6, Test Loss: 472966986956013568.0000\n",
      "Epoch 7, Train Loss: 462958765758130880.0000\n",
      "Epoch 7, Test Loss: 470194018630762496.0000\n",
      "Epoch 8, Train Loss: 460066270595776512.0000\n",
      "Epoch 8, Test Loss: 465617404559360000.0000\n",
      "Epoch 9, Train Loss: 455937597946287424.0000\n",
      "Epoch 9, Test Loss: 461796842171006976.0000\n",
      "Epoch 10, Train Loss: 450736039625315648.0000\n",
      "Epoch 10, Test Loss: 455746195323617280.0000\n",
      "Epoch 11, Train Loss: 446561307053086016.0000\n",
      "Epoch 11, Test Loss: 449297525266972672.0000\n",
      "Epoch 12, Train Loss: 437988682857010496.0000\n",
      "Epoch 12, Test Loss: 440842040331206656.0000\n",
      "Epoch 13, Train Loss: 429892438515340608.0000\n",
      "Epoch 13, Test Loss: 432114976023379968.0000\n",
      "Epoch 14, Train Loss: 420333697953169408.0000\n",
      "Epoch 14, Test Loss: 423067919472132096.0000\n",
      "Epoch 15, Train Loss: 410132751233056768.0000\n",
      "Epoch 15, Test Loss: 412309026395652096.0000\n",
      "Epoch 16, Train Loss: 401374103860849344.0000\n",
      "Epoch 16, Test Loss: 400339467937775616.0000\n",
      "Epoch 17, Train Loss: 387605178437184192.0000\n",
      "Epoch 17, Test Loss: 392613199729393664.0000\n",
      "Epoch 18, Train Loss: 379415738896503488.0000\n",
      "Epoch 18, Test Loss: 387827094333161472.0000\n",
      "Epoch 19, Train Loss: 369430863679236800.0000\n",
      "Epoch 19, Test Loss: 366498252381487104.0000\n",
      "Epoch 20, Train Loss: 360961486649382208.0000\n",
      "Epoch 20, Test Loss: 357101035736530944.0000\n",
      "Epoch 21, Train Loss: 349933451829663040.0000\n",
      "Epoch 21, Test Loss: 346018405205147648.0000\n",
      "Epoch 22, Train Loss: 341963424016131776.0000\n",
      "Epoch 22, Test Loss: 344578903966220288.0000\n",
      "Epoch 23, Train Loss: 332604202463963840.0000\n",
      "Epoch 23, Test Loss: 333810630681165824.0000\n",
      "Epoch 24, Train Loss: 322165589830032064.0000\n",
      "Epoch 24, Test Loss: 318164030462099456.0000\n",
      "Epoch 25, Train Loss: 308056344249128256.0000\n",
      "Epoch 25, Test Loss: 330012333403275264.0000\n",
      "Epoch 26, Train Loss: 308903812952162304.0000\n",
      "Epoch 26, Test Loss: 302692080793157632.0000\n",
      "Epoch 27, Train Loss: 294561100289970880.0000\n",
      "Epoch 27, Test Loss: 303213695981322240.0000\n",
      "Epoch 28, Train Loss: 290833852208687808.0000\n",
      "Epoch 28, Test Loss: 288623004882042880.0000\n",
      "Epoch 29, Train Loss: 275990726364190048.0000\n",
      "Epoch 29, Test Loss: 277709750680944640.0000\n",
      "Epoch 30, Train Loss: 271225759928759648.0000\n",
      "Epoch 30, Test Loss: 265636803770318848.0000\n",
      "Epoch 31, Train Loss: 275335300525406880.0000\n",
      "Epoch 31, Test Loss: 261692700942663680.0000\n",
      "Epoch 32, Train Loss: 253820070281609216.0000\n",
      "Epoch 32, Test Loss: 269047128062427136.0000\n",
      "Epoch 33, Train Loss: 252865104360740192.0000\n",
      "Epoch 33, Test Loss: 245387991474765824.0000\n",
      "Epoch 34, Train Loss: 258627339218670944.0000\n",
      "Epoch 34, Test Loss: 243897156786716672.0000\n",
      "Epoch 35, Train Loss: 246289959475942752.0000\n",
      "Epoch 35, Test Loss: 231741385726754816.0000\n",
      "Epoch 36, Train Loss: 225654415468762464.0000\n",
      "Epoch 36, Test Loss: 261533065598205952.0000\n",
      "Epoch 37, Train Loss: 233655108375325344.0000\n",
      "Epoch 37, Test Loss: 223393601390903296.0000\n",
      "Epoch 38, Train Loss: 218954446235063648.0000\n",
      "Epoch 38, Test Loss: 213166803222790144.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:48:47,299]\u001b[0m Trial 9 finished with value: 2.0687893674170778e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 687, 'output_activation_class': None, 'lr': 0.0009552522101563955, 'batch_size': 170, 'epochs': 39}. Best is trial 9 with value: 2.0687893674170778e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Train Loss: 210548568565831008.0000\n",
      "Epoch 39, Test Loss: 206878936741707776.0000\n",
      "Epoch 1, Train Loss: 502919994760109568.0000\n",
      "Epoch 1, Test Loss: 500536622547206144.0000\n",
      "Epoch 2, Train Loss: 499868012913905728.0000\n",
      "Epoch 2, Test Loss: 499494388603289600.0000\n",
      "Epoch 3, Train Loss: 495791448984084288.0000\n",
      "Epoch 3, Test Loss: 490901018037452800.0000\n",
      "Epoch 4, Train Loss: 484878014404886528.0000\n",
      "Epoch 4, Test Loss: 480525270403514368.0000\n",
      "Epoch 5, Train Loss: 478907419679147584.0000\n",
      "Epoch 5, Test Loss: 476687012749901824.0000\n",
      "Epoch 6, Train Loss: 475184369154541120.0000\n",
      "Epoch 6, Test Loss: 472915241190031360.0000\n",
      "Epoch 7, Train Loss: 472613541170216320.0000\n",
      "Epoch 7, Test Loss: 470663132138700800.0000\n",
      "Epoch 8, Train Loss: 467511354603576192.0000\n",
      "Epoch 8, Test Loss: 467453245380362240.0000\n",
      "Epoch 9, Train Loss: 465419227104753472.0000\n",
      "Epoch 9, Test Loss: 462772246423797760.0000\n",
      "Epoch 10, Train Loss: 460442233108168704.0000\n",
      "Epoch 10, Test Loss: 457981330664194048.0000\n",
      "Epoch 11, Train Loss: 455450685130664960.0000\n",
      "Epoch 11, Test Loss: 452394127967911936.0000\n",
      "Epoch 12, Train Loss: 449407436090743680.0000\n",
      "Epoch 12, Test Loss: 445131441349591040.0000\n",
      "Epoch 13, Train Loss: 442499398017493056.0000\n",
      "Epoch 13, Test Loss: 437175684448649216.0000\n",
      "Epoch 14, Train Loss: 434203826062134400.0000\n",
      "Epoch 14, Test Loss: 428830975309381632.0000\n",
      "Epoch 15, Train Loss: 424894436487586304.0000\n",
      "Epoch 15, Test Loss: 418101150571560960.0000\n",
      "Epoch 16, Train Loss: 415795800095048640.0000\n",
      "Epoch 16, Test Loss: 409483418951221248.0000\n",
      "Epoch 17, Train Loss: 405502781426391616.0000\n",
      "Epoch 17, Test Loss: 396688745736175616.0000\n",
      "Epoch 18, Train Loss: 393535451277552640.0000\n",
      "Epoch 18, Test Loss: 395900808215920640.0000\n",
      "Epoch 19, Train Loss: 384114689958739968.0000\n",
      "Epoch 19, Test Loss: 375366363415314432.0000\n",
      "Epoch 20, Train Loss: 377201496366755776.0000\n",
      "Epoch 20, Test Loss: 366273814570467328.0000\n",
      "Epoch 21, Train Loss: 362735253439695040.0000\n",
      "Epoch 21, Test Loss: 359332838182486016.0000\n",
      "Epoch 22, Train Loss: 352039576828888000.0000\n",
      "Epoch 22, Test Loss: 371129635875848192.0000\n",
      "Epoch 23, Train Loss: 352893133196351424.0000\n",
      "Epoch 23, Test Loss: 342077961689628672.0000\n",
      "Epoch 24, Train Loss: 338924558972565312.0000\n",
      "Epoch 24, Test Loss: 339319287015538688.0000\n",
      "Epoch 25, Train Loss: 336691688342979200.0000\n",
      "Epoch 25, Test Loss: 321517231689170944.0000\n",
      "Epoch 26, Train Loss: 326487576086854720.0000\n",
      "Epoch 26, Test Loss: 319588310336929792.0000\n",
      "Epoch 27, Train Loss: 316685708366487744.0000\n",
      "Epoch 27, Test Loss: 305679110288441344.0000\n",
      "Epoch 28, Train Loss: 308348073917351168.0000\n",
      "Epoch 28, Test Loss: 298064408150540288.0000\n",
      "Epoch 29, Train Loss: 305322866647257152.0000\n",
      "Epoch 29, Test Loss: 293662925665599488.0000\n",
      "Epoch 30, Train Loss: 295465821013592256.0000\n",
      "Epoch 30, Test Loss: 316098735308275712.0000\n",
      "Epoch 31, Train Loss: 288591775553747712.0000\n",
      "Epoch 31, Test Loss: 275666806536929280.0000\n",
      "Epoch 32, Train Loss: 282889363896782272.0000\n",
      "Epoch 32, Test Loss: 268977807290269696.0000\n",
      "Epoch 33, Train Loss: 269963275704123072.0000\n",
      "Epoch 33, Test Loss: 261818199887052800.0000\n",
      "Epoch 34, Train Loss: 266101885135589792.0000\n",
      "Epoch 34, Test Loss: 269885316700045312.0000\n",
      "Epoch 35, Train Loss: 277478373160720672.0000\n",
      "Epoch 35, Test Loss: 268140323027288064.0000\n",
      "Epoch 36, Train Loss: 257687072929557792.0000\n",
      "Epoch 36, Test Loss: 256500583698006016.0000\n",
      "Epoch 37, Train Loss: 257675971754957728.0000\n",
      "Epoch 37, Test Loss: 263032249702678528.0000\n",
      "Epoch 38, Train Loss: 255894258227064736.0000\n",
      "Epoch 38, Test Loss: 233452105560489984.0000\n",
      "Epoch 39, Train Loss: 236630937842020608.0000\n",
      "Epoch 39, Test Loss: 228744546526167040.0000\n",
      "Epoch 40, Train Loss: 236623774088887040.0000\n",
      "Epoch 40, Test Loss: 221877443575676928.0000\n",
      "Epoch 41, Train Loss: 232061988950327616.0000\n",
      "Epoch 41, Test Loss: 217996974983479296.0000\n",
      "Epoch 42, Train Loss: 221634283960106368.0000\n",
      "Epoch 42, Test Loss: 231031616611287040.0000\n",
      "Epoch 43, Train Loss: 222668282325286592.0000\n",
      "Epoch 43, Test Loss: 206261097106243584.0000\n",
      "Epoch 44, Train Loss: 210949972141991648.0000\n",
      "Epoch 44, Test Loss: 207761123024306176.0000\n",
      "Epoch 45, Train Loss: 205757693513674656.0000\n",
      "Epoch 45, Test Loss: 201540996767416320.0000\n",
      "Epoch 46, Train Loss: 212646632312120736.0000\n",
      "Epoch 46, Test Loss: 204676460332449792.0000\n",
      "Epoch 47, Train Loss: 215718697822790432.0000\n",
      "Epoch 47, Test Loss: 192258490109001728.0000\n",
      "Epoch 48, Train Loss: 202957838916846848.0000\n",
      "Epoch 48, Test Loss: 183975903376703488.0000\n",
      "Epoch 49, Train Loss: 202578962822463488.0000\n",
      "Epoch 49, Test Loss: 180441780947255296.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:49:00,354]\u001b[0m Trial 10 finished with value: 1.8055606143706726e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 256, 'output_activation_class': 'ReLU', 'lr': 0.0015810410094698718, 'batch_size': 157, 'epochs': 50}. Best is trial 10 with value: 1.8055606143706726e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Train Loss: 183858588355647456.0000\n",
      "Epoch 50, Test Loss: 180556061437067264.0000\n",
      "Epoch 1, Train Loss: 500137300445888512.0000\n",
      "Epoch 1, Test Loss: 500558647139500032.0000\n",
      "Epoch 2, Train Loss: 500912843168184448.0000\n",
      "Epoch 2, Test Loss: 500558647139500032.0000\n",
      "Epoch 3, Train Loss: 500783852444332608.0000\n",
      "Epoch 3, Test Loss: 500558647139500032.0000\n",
      "Epoch 4, Train Loss: 500923217653792768.0000\n",
      "Epoch 4, Test Loss: 500558647139500032.0000\n",
      "Epoch 5, Train Loss: 500273890352487616.0000\n",
      "Epoch 5, Test Loss: 500558647139500032.0000\n",
      "Epoch 6, Train Loss: 502121113049993600.0000\n",
      "Epoch 6, Test Loss: 500558647139500032.0000\n",
      "Epoch 7, Train Loss: 500411347599046464.0000\n",
      "Epoch 7, Test Loss: 500558647139500032.0000\n",
      "Epoch 8, Train Loss: 500549203160398592.0000\n",
      "Epoch 8, Test Loss: 500558647139500032.0000\n",
      "Epoch 9, Train Loss: 500180231945504960.0000\n",
      "Epoch 9, Test Loss: 500558647139500032.0000\n",
      "Epoch 10, Train Loss: 501596502037924992.0000\n",
      "Epoch 10, Test Loss: 500558647139500032.0000\n",
      "Epoch 11, Train Loss: 500135591335762176.0000\n",
      "Epoch 11, Test Loss: 500558647139500032.0000\n",
      "Epoch 12, Train Loss: 500144526477034752.0000\n",
      "Epoch 12, Test Loss: 500558647139500032.0000\n",
      "Epoch 13, Train Loss: 500766256793628608.0000\n",
      "Epoch 13, Test Loss: 500558647139500032.0000\n",
      "Epoch 14, Train Loss: 500146946151603392.0000\n",
      "Epoch 14, Test Loss: 500558647139500032.0000\n",
      "Epoch 15, Train Loss: 500140354737403392.0000\n",
      "Epoch 15, Test Loss: 500558647139500032.0000\n",
      "Epoch 16, Train Loss: 500139526282424896.0000\n",
      "Epoch 16, Test Loss: 500558647139500032.0000\n",
      "Epoch 17, Train Loss: 502115505402724032.0000\n",
      "Epoch 17, Test Loss: 500558647139500032.0000\n",
      "Epoch 18, Train Loss: 502082631079913856.0000\n",
      "Epoch 18, Test Loss: 500558647139500032.0000\n",
      "Epoch 19, Train Loss: 502314009884417216.0000\n",
      "Epoch 19, Test Loss: 500558647139500032.0000\n",
      "Epoch 20, Train Loss: 501363370044614400.0000\n",
      "Epoch 20, Test Loss: 500558647139500032.0000\n",
      "Epoch 21, Train Loss: 500251796880182080.0000\n",
      "Epoch 21, Test Loss: 500558647139500032.0000\n",
      "Epoch 22, Train Loss: 502281400000467008.0000\n",
      "Epoch 22, Test Loss: 500558647139500032.0000\n",
      "Epoch 23, Train Loss: 500221059846722880.0000\n",
      "Epoch 23, Test Loss: 500558647139500032.0000\n",
      "Epoch 24, Train Loss: 501618888797019968.0000\n",
      "Epoch 24, Test Loss: 500558647139500032.0000\n",
      "Epoch 25, Train Loss: 501293923284117888.0000\n",
      "Epoch 25, Test Loss: 500558647139500032.0000\n",
      "Epoch 26, Train Loss: 500168177356589888.0000\n",
      "Epoch 26, Test Loss: 500558647139500032.0000\n",
      "Epoch 27, Train Loss: 500190892754371200.0000\n",
      "Epoch 27, Test Loss: 500558647139500032.0000\n",
      "Epoch 28, Train Loss: 500174101848162944.0000\n",
      "Epoch 28, Test Loss: 500558647139500032.0000\n",
      "Epoch 29, Train Loss: 500131712568429696.0000\n",
      "Epoch 29, Test Loss: 500558647139500032.0000\n",
      "Epoch 30, Train Loss: 501288713744064704.0000\n",
      "Epoch 30, Test Loss: 500558647139500032.0000\n",
      "Epoch 31, Train Loss: 500443271023795072.0000\n",
      "Epoch 31, Test Loss: 500558647139500032.0000\n",
      "Epoch 32, Train Loss: 500145288875837312.0000\n",
      "Epoch 32, Test Loss: 500558647139500032.0000\n",
      "Epoch 33, Train Loss: 500984080419489664.0000\n",
      "Epoch 33, Test Loss: 500558647139500032.0000\n",
      "Epoch 34, Train Loss: 500183749200018816.0000\n",
      "Epoch 34, Test Loss: 500558647139500032.0000\n",
      "Epoch 35, Train Loss: 500189545876838976.0000\n",
      "Epoch 35, Test Loss: 500558647139500032.0000\n",
      "Epoch 36, Train Loss: 500219944618462336.0000\n",
      "Epoch 36, Test Loss: 500558647139500032.0000\n",
      "Epoch 37, Train Loss: 500354008548736384.0000\n",
      "Epoch 37, Test Loss: 500558647139500032.0000\n",
      "Epoch 38, Train Loss: 500631406043504832.0000\n",
      "Epoch 38, Test Loss: 500558647139500032.0000\n",
      "Epoch 39, Train Loss: 501759289548886080.0000\n",
      "Epoch 39, Test Loss: 500558647139500032.0000\n",
      "Epoch 40, Train Loss: 500176176896345344.0000\n",
      "Epoch 40, Test Loss: 500558647139500032.0000\n",
      "Epoch 41, Train Loss: 500149307676999360.0000\n",
      "Epoch 41, Test Loss: 500558647139500032.0000\n",
      "Epoch 42, Train Loss: 503529857249125184.0000\n",
      "Epoch 42, Test Loss: 500558647139500032.0000\n",
      "Epoch 43, Train Loss: 500320924065636544.0000\n",
      "Epoch 43, Test Loss: 500558647139500032.0000\n",
      "Epoch 44, Train Loss: 500680384108349632.0000\n",
      "Epoch 44, Test Loss: 500558647139500032.0000\n",
      "Epoch 45, Train Loss: 502491975486100800.0000\n",
      "Epoch 45, Test Loss: 500558647139500032.0000\n",
      "Epoch 46, Train Loss: 500363793842204288.0000\n",
      "Epoch 46, Test Loss: 500558647139500032.0000\n",
      "Epoch 47, Train Loss: 500432134540721536.0000\n",
      "Epoch 47, Test Loss: 500558647139500032.0000\n",
      "Epoch 48, Train Loss: 501382625650175616.0000\n",
      "Epoch 48, Test Loss: 500558647139500032.0000\n",
      "Epoch 49, Train Loss: 500123743739532608.0000\n",
      "Epoch 49, Test Loss: 500558647139500032.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:49:10,870]\u001b[0m Trial 11 finished with value: 5.005586471395e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 165, 'output_activation_class': 'ReLU', 'lr': 0.0011667830207580904, 'batch_size': 157, 'epochs': 50}. Best is trial 10 with value: 1.8055606143706726e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Train Loss: 500134126919028480.0000\n",
      "Epoch 50, Test Loss: 500558647139500032.0000\n",
      "Epoch 1, Train Loss: 500373108434497280.0000\n",
      "Epoch 1, Test Loss: 500531949622788096.0000\n",
      "Epoch 2, Train Loss: 500181692953093504.0000\n",
      "Epoch 2, Test Loss: 499420549525536768.0000\n",
      "Epoch 3, Train Loss: 496308615118333632.0000\n",
      "Epoch 3, Test Loss: 492077564198649856.0000\n",
      "Epoch 4, Train Loss: 485160857029856576.0000\n",
      "Epoch 4, Test Loss: 481549431125049344.0000\n",
      "Epoch 5, Train Loss: 479818635641727232.0000\n",
      "Epoch 5, Test Loss: 478106928937959424.0000\n",
      "Epoch 6, Train Loss: 477319735703121984.0000\n",
      "Epoch 6, Test Loss: 476060050603900928.0000\n",
      "Epoch 7, Train Loss: 474520635844655936.0000\n",
      "Epoch 7, Test Loss: 472898679796137984.0000\n",
      "Epoch 8, Train Loss: 471470189394689408.0000\n",
      "Epoch 8, Test Loss: 470459825566777344.0000\n",
      "Epoch 9, Train Loss: 469156008341477440.0000\n",
      "Epoch 9, Test Loss: 467269523859308544.0000\n",
      "Epoch 10, Train Loss: 466113108736569088.0000\n",
      "Epoch 10, Test Loss: 463682779490549760.0000\n",
      "Epoch 11, Train Loss: 463992321166872192.0000\n",
      "Epoch 11, Test Loss: 459971549789683712.0000\n",
      "Epoch 12, Train Loss: 458106190347271552.0000\n",
      "Epoch 12, Test Loss: 456643259372929024.0000\n",
      "Epoch 13, Train Loss: 453621629499553088.0000\n",
      "Epoch 13, Test Loss: 448479832213291008.0000\n",
      "Epoch 14, Train Loss: 446848598934117312.0000\n",
      "Epoch 14, Test Loss: 443966543139700736.0000\n",
      "Epoch 15, Train Loss: 438810999111662720.0000\n",
      "Epoch 15, Test Loss: 435328642352939008.0000\n",
      "Epoch 16, Train Loss: 430722912677977664.0000\n",
      "Epoch 16, Test Loss: 426764305766023168.0000\n",
      "Epoch 17, Train Loss: 421425495047890304.0000\n",
      "Epoch 17, Test Loss: 415912847554379776.0000\n",
      "Epoch 18, Train Loss: 413121140165260608.0000\n",
      "Epoch 18, Test Loss: 408694622437507072.0000\n",
      "Epoch 19, Train Loss: 407729851571555008.0000\n",
      "Epoch 19, Test Loss: 396430291784171520.0000\n",
      "Epoch 20, Train Loss: 396214465463617344.0000\n",
      "Epoch 20, Test Loss: 391090032527540224.0000\n",
      "Epoch 21, Train Loss: 385913694952209536.0000\n",
      "Epoch 21, Test Loss: 377741445970264064.0000\n",
      "Epoch 22, Train Loss: 374775521458050176.0000\n",
      "Epoch 22, Test Loss: 372431526362611712.0000\n",
      "Epoch 23, Train Loss: 368890305719724608.0000\n",
      "Epoch 23, Test Loss: 359524428083625984.0000\n",
      "Epoch 24, Train Loss: 360068146910729024.0000\n",
      "Epoch 24, Test Loss: 362599006192271360.0000\n",
      "Epoch 25, Train Loss: 349894754852800960.0000\n",
      "Epoch 25, Test Loss: 345498130046779392.0000\n",
      "Epoch 26, Train Loss: 345844697663715200.0000\n",
      "Epoch 26, Test Loss: 336962999237476352.0000\n",
      "Epoch 27, Train Loss: 346460512460145856.0000\n",
      "Epoch 27, Test Loss: 328798953602547712.0000\n",
      "Epoch 28, Train Loss: 328050274595528832.0000\n",
      "Epoch 28, Test Loss: 321539290641203200.0000\n",
      "Epoch 29, Train Loss: 318436236615755456.0000\n",
      "Epoch 29, Test Loss: 312971174483066880.0000\n",
      "Epoch 30, Train Loss: 315354716725699328.0000\n",
      "Epoch 30, Test Loss: 304625949947723776.0000\n",
      "Epoch 31, Train Loss: 311003267145420096.0000\n",
      "Epoch 31, Test Loss: 328539743736299520.0000\n",
      "Epoch 32, Train Loss: 302497606699922752.0000\n",
      "Epoch 32, Test Loss: 302234855754694656.0000\n",
      "Epoch 33, Train Loss: 298799754788960448.0000\n",
      "Epoch 33, Test Loss: 283996174053015552.0000\n",
      "Epoch 34, Train Loss: 287813779315866720.0000\n",
      "Epoch 34, Test Loss: 277706933182398464.0000\n",
      "Epoch 35, Train Loss: 285696640948925408.0000\n",
      "Epoch 35, Test Loss: 287269420168904704.0000\n",
      "Epoch 36, Train Loss: 270937764238265984.0000\n",
      "Epoch 36, Test Loss: 284515538678317056.0000\n",
      "Epoch 37, Train Loss: 276117121368068960.0000\n",
      "Epoch 37, Test Loss: 266849496376279040.0000\n",
      "Epoch 38, Train Loss: 267125788188482816.0000\n",
      "Epoch 38, Test Loss: 257378646812000256.0000\n",
      "Epoch 39, Train Loss: 259145351372234912.0000\n",
      "Epoch 39, Test Loss: 248399313305075712.0000\n",
      "Epoch 40, Train Loss: 252509501586140320.0000\n",
      "Epoch 40, Test Loss: 243215906254094336.0000\n",
      "Epoch 41, Train Loss: 251042180555040096.0000\n",
      "Epoch 41, Test Loss: 236454700017254400.0000\n",
      "Epoch 42, Train Loss: 259528724032727936.0000\n",
      "Epoch 42, Test Loss: 235624534378545152.0000\n",
      "Epoch 43, Train Loss: 243737439270620480.0000\n",
      "Epoch 43, Test Loss: 229318113638744064.0000\n",
      "Epoch 44, Train Loss: 244889133474708256.0000\n",
      "Epoch 44, Test Loss: 261264527062990848.0000\n",
      "Epoch 45, Train Loss: 227641992268599232.0000\n",
      "Epoch 45, Test Loss: 234698573789265920.0000\n",
      "Epoch 46, Train Loss: 245830938192011424.0000\n",
      "Epoch 46, Test Loss: 231664608891371520.0000\n",
      "Epoch 47, Train Loss: 223011078318498880.0000\n",
      "Epoch 47, Test Loss: 221738355354763264.0000\n",
      "Epoch 48, Train Loss: 225496994538046976.0000\n",
      "Epoch 48, Test Loss: 237694021420449792.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:49:28,103]\u001b[0m Trial 12 finished with value: 2.031600963587932e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 339, 'output_activation_class': 'ReLU', 'lr': 0.0012003826944569102, 'batch_size': 151, 'epochs': 49}. Best is trial 10 with value: 1.8055606143706726e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, Train Loss: 217939705443850272.0000\n",
      "Epoch 49, Test Loss: 203160096358793216.0000\n",
      "Epoch 1, Train Loss: 493028956715994752.0000\n",
      "Epoch 1, Test Loss: 500429145285591040.0000\n",
      "Epoch 2, Train Loss: 491034195775057792.0000\n",
      "Epoch 2, Test Loss: 493118629911461888.0000\n",
      "Epoch 3, Train Loss: 477975132129756224.0000\n",
      "Epoch 3, Test Loss: 479954658228436992.0000\n",
      "Epoch 4, Train Loss: 472016676828978432.0000\n",
      "Epoch 4, Test Loss: 475930857987637248.0000\n",
      "Epoch 5, Train Loss: 466856085241758784.0000\n",
      "Epoch 5, Test Loss: 471134891346493440.0000\n",
      "Epoch 6, Train Loss: 461889090361829696.0000\n",
      "Epoch 6, Test Loss: 465447220775223296.0000\n",
      "Epoch 7, Train Loss: 458326983380227776.0000\n",
      "Epoch 7, Test Loss: 458839808727318528.0000\n",
      "Epoch 8, Train Loss: 449002076995169664.0000\n",
      "Epoch 8, Test Loss: 452613343098699776.0000\n",
      "Epoch 9, Train Loss: 440041374467548992.0000\n",
      "Epoch 9, Test Loss: 438681499982626816.0000\n",
      "Epoch 10, Train Loss: 427578326882443008.0000\n",
      "Epoch 10, Test Loss: 424000752009084928.0000\n",
      "Epoch 11, Train Loss: 418747196140497216.0000\n",
      "Epoch 11, Test Loss: 411285827746791424.0000\n",
      "Epoch 12, Train Loss: 404829062196346368.0000\n",
      "Epoch 12, Test Loss: 400923239892647936.0000\n",
      "Epoch 13, Train Loss: 387961339528981888.0000\n",
      "Epoch 13, Test Loss: 385226199577657344.0000\n",
      "Epoch 14, Train Loss: 375047196662875712.0000\n",
      "Epoch 14, Test Loss: 368111304658911232.0000\n",
      "Epoch 15, Train Loss: 387671287934729728.0000\n",
      "Epoch 15, Test Loss: 359013361335140352.0000\n",
      "Epoch 16, Train Loss: 349740306107288128.0000\n",
      "Epoch 16, Test Loss: 339625775881781248.0000\n",
      "Epoch 17, Train Loss: 335081495467198528.0000\n",
      "Epoch 17, Test Loss: 326945142638379008.0000\n",
      "Epoch 18, Train Loss: 325420438790746496.0000\n",
      "Epoch 18, Test Loss: 358510540923863040.0000\n",
      "Epoch 19, Train Loss: 314533912570854592.0000\n",
      "Epoch 19, Test Loss: 322584101565497344.0000\n",
      "Epoch 20, Train Loss: 301086495620134080.0000\n",
      "Epoch 20, Test Loss: 292381547942641664.0000\n",
      "Epoch 21, Train Loss: 290571899590029696.0000\n",
      "Epoch 21, Test Loss: 282007294957322240.0000\n",
      "Epoch 22, Train Loss: 285781843527927872.0000\n",
      "Epoch 22, Test Loss: 273155590698565632.0000\n",
      "Epoch 23, Train Loss: 278151811228990432.0000\n",
      "Epoch 23, Test Loss: 271117972313997312.0000\n",
      "Epoch 24, Train Loss: 282366511832764544.0000\n",
      "Epoch 24, Test Loss: 262504003085008896.0000\n",
      "Epoch 25, Train Loss: 255543629979878432.0000\n",
      "Epoch 25, Test Loss: 283228629037481984.0000\n",
      "Epoch 26, Train Loss: 263783625269309312.0000\n",
      "Epoch 26, Test Loss: 254335404784746496.0000\n",
      "Epoch 27, Train Loss: 245902492302886464.0000\n",
      "Epoch 27, Test Loss: 238491940444700672.0000\n",
      "Epoch 28, Train Loss: 245736503559391296.0000\n",
      "Epoch 28, Test Loss: 233688792618237952.0000\n",
      "Epoch 29, Train Loss: 232230936752328768.0000\n",
      "Epoch 29, Test Loss: 219154331230797824.0000\n",
      "Epoch 30, Train Loss: 224546955119448736.0000\n",
      "Epoch 30, Test Loss: 231428746467344384.0000\n",
      "Epoch 31, Train Loss: 228648761312142496.0000\n",
      "Epoch 31, Test Loss: 228774594117369856.0000\n",
      "Epoch 32, Train Loss: 215843191748482752.0000\n",
      "Epoch 32, Test Loss: 213542870559227904.0000\n",
      "Epoch 33, Train Loss: 203894926152742304.0000\n",
      "Epoch 33, Test Loss: 200212391584071680.0000\n",
      "Epoch 34, Train Loss: 209905774654187328.0000\n",
      "Epoch 34, Test Loss: 192069614627192832.0000\n",
      "Epoch 35, Train Loss: 204183388391870592.0000\n",
      "Epoch 35, Test Loss: 184346919831601152.0000\n",
      "Epoch 36, Train Loss: 202462374492372992.0000\n",
      "Epoch 36, Test Loss: 180291611710717952.0000\n",
      "Epoch 37, Train Loss: 191878207149593248.0000\n",
      "Epoch 37, Test Loss: 191323029052063744.0000\n",
      "Epoch 38, Train Loss: 217598658133301440.0000\n",
      "Epoch 38, Test Loss: 170901198293958656.0000\n",
      "Epoch 39, Train Loss: 226342316725535776.0000\n",
      "Epoch 39, Test Loss: 167360942651211776.0000\n",
      "Epoch 40, Train Loss: 255980217947000960.0000\n",
      "Epoch 40, Test Loss: 183442966654746624.0000\n",
      "Epoch 41, Train Loss: 170661955274942112.0000\n",
      "Epoch 41, Test Loss: 161118929240981504.0000\n",
      "Epoch 42, Train Loss: 174025251144690528.0000\n",
      "Epoch 42, Test Loss: 157257770821746688.0000\n",
      "Epoch 43, Train Loss: 180573839779984288.0000\n",
      "Epoch 43, Test Loss: 163562112797376512.0000\n",
      "Epoch 44, Train Loss: 167553261728043136.0000\n",
      "Epoch 44, Test Loss: 152708833519599616.0000\n",
      "Epoch 45, Train Loss: 170370663570887328.0000\n",
      "Epoch 45, Test Loss: 175252859158265856.0000\n",
      "Epoch 46, Train Loss: 153894308519481472.0000\n",
      "Epoch 46, Test Loss: 143799748497768448.0000\n",
      "Epoch 47, Train Loss: 150655076836547808.0000\n",
      "Epoch 47, Test Loss: 141152665663963136.0000\n",
      "Epoch 48, Train Loss: 149866306152710688.0000\n",
      "Epoch 48, Test Loss: 151639541281718272.0000\n",
      "Epoch 49, Train Loss: 155432865419315968.0000\n",
      "Epoch 49, Test Loss: 145722845874356224.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:49:43,997]\u001b[0m Trial 13 finished with value: 1.7142691947085824e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 296, 'output_activation_class': 'ReLU', 'lr': 0.0018370594277299528, 'batch_size': 131, 'epochs': 50}. Best is trial 13 with value: 1.7142691947085824e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Train Loss: 153925272192138720.0000\n",
      "Epoch 50, Test Loss: 171426919470858240.0000\n",
      "Epoch 1, Train Loss: 493313640222488640.0000\n",
      "Epoch 1, Test Loss: 500558647139500032.0000\n",
      "Epoch 2, Train Loss: 493285405619221568.0000\n",
      "Epoch 2, Test Loss: 500558647139500032.0000\n",
      "Epoch 3, Train Loss: 493670306065702144.0000\n",
      "Epoch 3, Test Loss: 500558647139500032.0000\n",
      "Epoch 4, Train Loss: 493282678338569856.0000\n",
      "Epoch 4, Test Loss: 500558647139500032.0000\n",
      "Epoch 5, Train Loss: 493282932613511232.0000\n",
      "Epoch 5, Test Loss: 500558647139500032.0000\n",
      "Epoch 6, Train Loss: 493313416457927232.0000\n",
      "Epoch 6, Test Loss: 500558647139500032.0000\n",
      "Epoch 7, Train Loss: 493282697339921088.0000\n",
      "Epoch 7, Test Loss: 500558647139500032.0000\n",
      "Epoch 8, Train Loss: 493624714630576704.0000\n",
      "Epoch 8, Test Loss: 500558647139500032.0000\n",
      "Epoch 9, Train Loss: 493282724412868032.0000\n",
      "Epoch 9, Test Loss: 500558647139500032.0000\n",
      "Epoch 10, Train Loss: 493487412816101888.0000\n",
      "Epoch 10, Test Loss: 500558647139500032.0000\n",
      "Epoch 11, Train Loss: 493282689367940864.0000\n",
      "Epoch 11, Test Loss: 500558647139500032.0000\n",
      "Epoch 12, Train Loss: 493335055761964032.0000\n",
      "Epoch 12, Test Loss: 500558647139500032.0000\n",
      "Epoch 13, Train Loss: 496322075833536384.0000\n",
      "Epoch 13, Test Loss: 500558647139500032.0000\n",
      "Epoch 14, Train Loss: 493283029918440704.0000\n",
      "Epoch 14, Test Loss: 500558647139500032.0000\n",
      "Epoch 15, Train Loss: 493282679230171136.0000\n",
      "Epoch 15, Test Loss: 500558647139500032.0000\n",
      "Epoch 16, Train Loss: 493282680022706432.0000\n",
      "Epoch 16, Test Loss: 500558647139500032.0000\n",
      "Epoch 17, Train Loss: 493282674932608192.0000\n",
      "Epoch 17, Test Loss: 500558647139500032.0000\n",
      "Epoch 18, Train Loss: 493282685689992448.0000\n",
      "Epoch 18, Test Loss: 500558647139500032.0000\n",
      "Epoch 19, Train Loss: 493282672429292800.0000\n",
      "Epoch 19, Test Loss: 500558647139500032.0000\n",
      "Epoch 20, Train Loss: 493283155195819072.0000\n",
      "Epoch 20, Test Loss: 500558647139500032.0000\n",
      "Epoch 21, Train Loss: 493295216055097152.0000\n",
      "Epoch 21, Test Loss: 500558647139500032.0000\n",
      "Epoch 22, Train Loss: 510983753635749120.0000\n",
      "Epoch 22, Test Loss: 500558647139500032.0000\n",
      "Epoch 23, Train Loss: 536599049259744256.0000\n",
      "Epoch 23, Test Loss: 500558647139500032.0000\n",
      "Epoch 24, Train Loss: 493282687372151424.0000\n",
      "Epoch 24, Test Loss: 500558647139500032.0000\n",
      "Epoch 25, Train Loss: 493282680078118528.0000\n",
      "Epoch 25, Test Loss: 500558647139500032.0000\n",
      "Epoch 26, Train Loss: 493282680432492096.0000\n",
      "Epoch 26, Test Loss: 500558647139500032.0000\n",
      "Epoch 27, Train Loss: 493282673025784640.0000\n",
      "Epoch 27, Test Loss: 500558647139500032.0000\n",
      "Epoch 28, Train Loss: 493282678610564160.0000\n",
      "Epoch 28, Test Loss: 500558647139500032.0000\n",
      "Epoch 29, Train Loss: 493303729931959808.0000\n",
      "Epoch 29, Test Loss: 500558647139500032.0000\n",
      "Epoch 30, Train Loss: 493282678617340992.0000\n",
      "Epoch 30, Test Loss: 500558647139500032.0000\n",
      "Epoch 31, Train Loss: 493614289881685248.0000\n",
      "Epoch 31, Test Loss: 500558647139500032.0000\n",
      "Epoch 32, Train Loss: 520096125349011200.0000\n",
      "Epoch 32, Test Loss: 500558647139500032.0000\n",
      "Epoch 33, Train Loss: 493282844931681408.0000\n",
      "Epoch 33, Test Loss: 500558647139500032.0000\n",
      "Epoch 34, Train Loss: 493282691334809280.0000\n",
      "Epoch 34, Test Loss: 500558647139500032.0000\n",
      "Epoch 35, Train Loss: 493282685318132352.0000\n",
      "Epoch 35, Test Loss: 500558647139500032.0000\n",
      "Epoch 36, Train Loss: 517827007840804096.0000\n",
      "Epoch 36, Test Loss: 500558647139500032.0000\n",
      "Epoch 37, Train Loss: 493647759707201792.0000\n",
      "Epoch 37, Test Loss: 500558647139500032.0000\n",
      "Epoch 38, Train Loss: 493304018319084480.0000\n",
      "Epoch 38, Test Loss: 500558647139500032.0000\n",
      "Epoch 39, Train Loss: 496265358296179904.0000\n",
      "Epoch 39, Test Loss: 500558647139500032.0000\n",
      "Epoch 40, Train Loss: 493282696902939712.0000\n",
      "Epoch 40, Test Loss: 500558647139500032.0000\n",
      "Epoch 41, Train Loss: 493282670194671552.0000\n",
      "Epoch 41, Test Loss: 500558647139500032.0000\n",
      "Epoch 42, Train Loss: 493282722208630208.0000\n",
      "Epoch 42, Test Loss: 500558647139500032.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:49:59,577]\u001b[0m Trial 14 finished with value: 5.005586471395e+17 and parameters: {'activation_class': 'LeakyReLU', 'hidden_sizes': 332, 'output_activation_class': 'ReLU', 'lr': 0.002389829478287356, 'batch_size': 123, 'epochs': 43}. Best is trial 13 with value: 1.7142691947085824e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, Train Loss: 493400510534818624.0000\n",
      "Epoch 43, Test Loss: 500558647139500032.0000\n",
      "Epoch 1, Train Loss: 497256889483001856.0000\n",
      "Epoch 1, Test Loss: 500556860433104896.0000\n",
      "Epoch 2, Train Loss: 510596447251464192.0000\n",
      "Epoch 2, Test Loss: 500478932546486272.0000\n",
      "Epoch 3, Train Loss: 496292138951114752.0000\n",
      "Epoch 3, Test Loss: 499740026372882432.0000\n",
      "Epoch 4, Train Loss: 496483294062313472.0000\n",
      "Epoch 4, Test Loss: 496154347156013056.0000\n",
      "Epoch 5, Train Loss: 488861018459471872.0000\n",
      "Epoch 5, Test Loss: 488119494137348096.0000\n",
      "Epoch 6, Train Loss: 481399306322968576.0000\n",
      "Epoch 6, Test Loss: 481395396417945600.0000\n",
      "Epoch 7, Train Loss: 475024667149598720.0000\n",
      "Epoch 7, Test Loss: 475711780295802880.0000\n",
      "Epoch 8, Train Loss: 471187263489835008.0000\n",
      "Epoch 8, Test Loss: 471468112089186304.0000\n",
      "Epoch 9, Train Loss: 470853316377575424.0000\n",
      "Epoch 9, Test Loss: 467499734106374144.0000\n",
      "Epoch 10, Train Loss: 462363900008988672.0000\n",
      "Epoch 10, Test Loss: 464007960054464512.0000\n",
      "Epoch 11, Train Loss: 459189254245842944.0000\n",
      "Epoch 11, Test Loss: 460896410867335168.0000\n",
      "Epoch 12, Train Loss: 455669572519854080.0000\n",
      "Epoch 12, Test Loss: 457422950555975680.0000\n",
      "Epoch 13, Train Loss: 452639829339930624.0000\n",
      "Epoch 13, Test Loss: 453802086967017472.0000\n",
      "Epoch 14, Train Loss: 448948560617013248.0000\n",
      "Epoch 14, Test Loss: 452487586456272896.0000\n",
      "Epoch 15, Train Loss: 461476242592366592.0000\n",
      "Epoch 15, Test Loss: 446907736744001536.0000\n",
      "Epoch 16, Train Loss: 442346152172453888.0000\n",
      "Epoch 16, Test Loss: 442659052015583232.0000\n",
      "Epoch 17, Train Loss: 439469246326505472.0000\n",
      "Epoch 17, Test Loss: 439397213332832256.0000\n",
      "Epoch 18, Train Loss: 434706296004935680.0000\n",
      "Epoch 18, Test Loss: 434784349737451520.0000\n",
      "Epoch 19, Train Loss: 429934120613707776.0000\n",
      "Epoch 19, Test Loss: 430129361102831616.0000\n",
      "Epoch 20, Train Loss: 425363734479765504.0000\n",
      "Epoch 20, Test Loss: 424859642389069824.0000\n",
      "Epoch 21, Train Loss: 420526737781161984.0000\n",
      "Epoch 21, Test Loss: 420984791614095360.0000\n",
      "Epoch 22, Train Loss: 416083373224624128.0000\n",
      "Epoch 22, Test Loss: 415772350584193024.0000\n",
      "Epoch 23, Train Loss: 427048223622823936.0000\n",
      "Epoch 23, Test Loss: 407461554506694656.0000\n",
      "Epoch 24, Train Loss: 403152901733613568.0000\n",
      "Epoch 24, Test Loss: 400629395410124800.0000\n",
      "Epoch 25, Train Loss: 395211569144791040.0000\n",
      "Epoch 25, Test Loss: 393276480118849536.0000\n",
      "Epoch 26, Train Loss: 387941436177252352.0000\n",
      "Epoch 26, Test Loss: 388685675475501056.0000\n",
      "Epoch 27, Train Loss: 415959469743144960.0000\n",
      "Epoch 27, Test Loss: 379671191956226048.0000\n",
      "Epoch 28, Train Loss: 376930597341233152.0000\n",
      "Epoch 28, Test Loss: 374211360810074112.0000\n",
      "Epoch 29, Train Loss: 366835549266247680.0000\n",
      "Epoch 29, Test Loss: 364233258428268544.0000\n",
      "Epoch 30, Train Loss: 370891025696161792.0000\n",
      "Epoch 30, Test Loss: 357056299357175808.0000\n",
      "Epoch 31, Train Loss: 365115745492795392.0000\n",
      "Epoch 31, Test Loss: 350039044350017536.0000\n",
      "Epoch 32, Train Loss: 347324622644969472.0000\n",
      "Epoch 32, Test Loss: 364037236120879104.0000\n",
      "Epoch 33, Train Loss: 341272834502295552.0000\n",
      "Epoch 33, Test Loss: 337215818192388096.0000\n",
      "Epoch 34, Train Loss: 334422070647586816.0000\n",
      "Epoch 34, Test Loss: 343288008595734528.0000\n",
      "Epoch 35, Train Loss: 357398706867142656.0000\n",
      "Epoch 35, Test Loss: 324118813639442432.0000\n",
      "Epoch 36, Train Loss: 317828761330384896.0000\n",
      "Epoch 36, Test Loss: 323247588113383424.0000\n",
      "Epoch 37, Train Loss: 313960948161314816.0000\n",
      "Epoch 37, Test Loss: 310430340550230016.0000\n",
      "Epoch 38, Train Loss: 310953926625067008.0000\n",
      "Epoch 38, Test Loss: 304750091682447360.0000\n",
      "Epoch 39, Train Loss: 314047339905941504.0000\n",
      "Epoch 39, Test Loss: 299319088356786176.0000\n",
      "Epoch 40, Train Loss: 300255790952873984.0000\n",
      "Epoch 40, Test Loss: 294118020400283648.0000\n",
      "Epoch 41, Train Loss: 291984400613113856.0000\n",
      "Epoch 41, Test Loss: 289585524232945664.0000\n",
      "Epoch 42, Train Loss: 288217471754698752.0000\n",
      "Epoch 42, Test Loss: 287044363882594304.0000\n",
      "Epoch 43, Train Loss: 284040666843971584.0000\n",
      "Epoch 43, Test Loss: 281655966632509440.0000\n",
      "Epoch 44, Train Loss: 279378882782560256.0000\n",
      "Epoch 44, Test Loss: 281531103343280128.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:50:07,015]\u001b[0m Trial 15 finished with value: 2.6976940412266086e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 55, 'output_activation_class': 'ReLU', 'lr': 0.0027449110693695744, 'batch_size': 126, 'epochs': 45}. Best is trial 13 with value: 1.7142691947085824e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, Train Loss: 275056737988378624.0000\n",
      "Epoch 45, Test Loss: 269769404122660864.0000\n",
      "Epoch 1, Train Loss: 500800724356801280.0000\n",
      "Epoch 1, Test Loss: 500557856865517568.0000\n",
      "Epoch 2, Train Loss: 501003872448091968.0000\n",
      "Epoch 2, Test Loss: 500536588187467776.0000\n",
      "Epoch 3, Train Loss: 500014199716064896.0000\n",
      "Epoch 3, Test Loss: 500365236172226560.0000\n",
      "Epoch 4, Train Loss: 499584814149247744.0000\n",
      "Epoch 4, Test Loss: 499577333011709952.0000\n",
      "Epoch 5, Train Loss: 498245434930626560.0000\n",
      "Epoch 5, Test Loss: 497265300576665600.0000\n",
      "Epoch 6, Train Loss: 495572269968392192.0000\n",
      "Epoch 6, Test Loss: 493213772027002880.0000\n",
      "Epoch 7, Train Loss: 490162187440631424.0000\n",
      "Epoch 7, Test Loss: 487231294900535296.0000\n",
      "Epoch 8, Train Loss: 484859431838428800.0000\n",
      "Epoch 8, Test Loss: 482521227605311488.0000\n",
      "Epoch 9, Train Loss: 481132554693187392.0000\n",
      "Epoch 9, Test Loss: 479885973111439360.0000\n",
      "Epoch 10, Train Loss: 478967384369862464.0000\n",
      "Epoch 10, Test Loss: 478305322067296256.0000\n",
      "Epoch 11, Train Loss: 477164251506254592.0000\n",
      "Epoch 11, Test Loss: 476653580724469760.0000\n",
      "Epoch 12, Train Loss: 475599806276638528.0000\n",
      "Epoch 12, Test Loss: 474993352526266368.0000\n",
      "Epoch 13, Train Loss: 474595868530632896.0000\n",
      "Epoch 13, Test Loss: 473401122250293248.0000\n",
      "Epoch 14, Train Loss: 472236331512198208.0000\n",
      "Epoch 14, Test Loss: 471714643211976704.0000\n",
      "Epoch 15, Train Loss: 471034918222454016.0000\n",
      "Epoch 15, Test Loss: 470065375770312704.0000\n",
      "Epoch 16, Train Loss: 469189109368035136.0000\n",
      "Epoch 16, Test Loss: 468441362736349184.0000\n",
      "Epoch 17, Train Loss: 467513807054783936.0000\n",
      "Epoch 17, Test Loss: 466672901362286592.0000\n",
      "Epoch 18, Train Loss: 465577471905699648.0000\n",
      "Epoch 18, Test Loss: 464855236842881024.0000\n",
      "Epoch 19, Train Loss: 464144902499955776.0000\n",
      "Epoch 19, Test Loss: 463041489333649408.0000\n",
      "Epoch 20, Train Loss: 463708350408056064.0000\n",
      "Epoch 20, Test Loss: 460971933572268032.0000\n",
      "Epoch 21, Train Loss: 459825054458410048.0000\n",
      "Epoch 21, Test Loss: 458761709042008064.0000\n",
      "Epoch 22, Train Loss: 457467224006185344.0000\n",
      "Epoch 22, Test Loss: 456486682045186048.0000\n",
      "Epoch 23, Train Loss: 455836712247444928.0000\n",
      "Epoch 23, Test Loss: 453829712196665344.0000\n",
      "Epoch 24, Train Loss: 452885704838098560.0000\n",
      "Epoch 24, Test Loss: 451080726968795136.0000\n",
      "Epoch 25, Train Loss: 450233877773656832.0000\n",
      "Epoch 25, Test Loss: 448398330913882112.0000\n",
      "Epoch 26, Train Loss: 447009992594352320.0000\n",
      "Epoch 26, Test Loss: 445642095781216256.0000\n",
      "Epoch 27, Train Loss: 444143255127708032.0000\n",
      "Epoch 27, Test Loss: 441234531622584320.0000\n",
      "Epoch 28, Train Loss: 440522186356448512.0000\n",
      "Epoch 28, Test Loss: 437821475731275776.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:50:17,025]\u001b[0m Trial 16 finished with value: 4.3372507336330445e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 355, 'output_activation_class': 'ReLU', 'lr': 0.0006087633942372179, 'batch_size': 182, 'epochs': 29}. Best is trial 13 with value: 1.7142691947085824e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Train Loss: 436575506109157760.0000\n",
      "Epoch 29, Test Loss: 433725073363304448.0000\n",
      "Epoch 1, Train Loss: 497555090654800576.0000\n",
      "Epoch 1, Test Loss: 500413580324110336.0000\n",
      "Epoch 2, Train Loss: 497428761725267072.0000\n",
      "Epoch 2, Test Loss: 493302282713038848.0000\n",
      "Epoch 3, Train Loss: 482466496487279040.0000\n",
      "Epoch 3, Test Loss: 480490670146977792.0000\n",
      "Epoch 4, Train Loss: 475598829928078592.0000\n",
      "Epoch 4, Test Loss: 476199516781936640.0000\n",
      "Epoch 5, Train Loss: 471532388828974976.0000\n",
      "Epoch 5, Test Loss: 471742852557176832.0000\n",
      "Epoch 6, Train Loss: 471363031451859968.0000\n",
      "Epoch 6, Test Loss: 466803674526515200.0000\n",
      "Epoch 7, Train Loss: 461561853056996672.0000\n",
      "Epoch 7, Test Loss: 460767424409501696.0000\n",
      "Epoch 8, Train Loss: 454944400172725824.0000\n",
      "Epoch 8, Test Loss: 453933272448106496.0000\n",
      "Epoch 9, Train Loss: 447230336569442304.0000\n",
      "Epoch 9, Test Loss: 445005512908472320.0000\n",
      "Epoch 10, Train Loss: 437880871029237952.0000\n",
      "Epoch 10, Test Loss: 433449542621331456.0000\n",
      "Epoch 11, Train Loss: 427411776153099072.0000\n",
      "Epoch 11, Test Loss: 421784755042779136.0000\n",
      "Epoch 12, Train Loss: 413787711027941248.0000\n",
      "Epoch 12, Test Loss: 408914284244893696.0000\n",
      "Epoch 13, Train Loss: 402510460715619648.0000\n",
      "Epoch 13, Test Loss: 399705427685670912.0000\n",
      "Epoch 14, Train Loss: 389587045229527040.0000\n",
      "Epoch 14, Test Loss: 383734162298765312.0000\n",
      "Epoch 15, Train Loss: 378359940490628032.0000\n",
      "Epoch 15, Test Loss: 369696181950873600.0000\n",
      "Epoch 16, Train Loss: 365187316823748736.0000\n",
      "Epoch 16, Test Loss: 354650945872723968.0000\n",
      "Epoch 17, Train Loss: 351224990161023616.0000\n",
      "Epoch 17, Test Loss: 344835639931305984.0000\n",
      "Epoch 18, Train Loss: 342782488097952640.0000\n",
      "Epoch 18, Test Loss: 330596242797101056.0000\n",
      "Epoch 19, Train Loss: 330955449299894272.0000\n",
      "Epoch 19, Test Loss: 318191690051485696.0000\n",
      "Epoch 20, Train Loss: 320008203926376320.0000\n",
      "Epoch 20, Test Loss: 307249934447411200.0000\n",
      "Epoch 21, Train Loss: 314908398299929856.0000\n",
      "Epoch 21, Test Loss: 322110796169478144.0000\n",
      "Epoch 22, Train Loss: 305486246896960512.0000\n",
      "Epoch 22, Test Loss: 288584040938733568.0000\n",
      "Epoch 23, Train Loss: 329558018231592128.0000\n",
      "Epoch 23, Test Loss: 301142216074592256.0000\n",
      "Epoch 24, Train Loss: 296002264323222656.0000\n",
      "Epoch 24, Test Loss: 273907983069478912.0000\n",
      "Epoch 25, Train Loss: 275420699623503424.0000\n",
      "Epoch 25, Test Loss: 262659343462170624.0000\n",
      "Epoch 26, Train Loss: 263511098762659776.0000\n",
      "Epoch 26, Test Loss: 255611662906687488.0000\n",
      "Epoch 27, Train Loss: 256303912176415840.0000\n",
      "Epoch 27, Test Loss: 349722762958340096.0000\n",
      "Epoch 28, Train Loss: 254670681840645216.0000\n",
      "Epoch 28, Test Loss: 238391919246311424.0000\n",
      "Epoch 29, Train Loss: 256407521969450624.0000\n",
      "Epoch 29, Test Loss: 231930776604639232.0000\n",
      "Epoch 30, Train Loss: 234101684894457120.0000\n",
      "Epoch 30, Test Loss: 224421696302481408.0000\n",
      "Epoch 31, Train Loss: 230547580810920928.0000\n",
      "Epoch 31, Test Loss: 226073815602429952.0000\n",
      "Epoch 32, Train Loss: 224931094660613088.0000\n",
      "Epoch 32, Test Loss: 263715458740387840.0000\n",
      "Epoch 33, Train Loss: 219174338868217792.0000\n",
      "Epoch 33, Test Loss: 219118700182110208.0000\n",
      "Epoch 34, Train Loss: 219806603356739392.0000\n",
      "Epoch 34, Test Loss: 207014434369961984.0000\n",
      "Epoch 35, Train Loss: 200242517977349632.0000\n",
      "Epoch 35, Test Loss: 190668304237461504.0000\n",
      "Epoch 36, Train Loss: 207085760664083232.0000\n",
      "Epoch 36, Test Loss: 199526347867947008.0000\n",
      "Epoch 37, Train Loss: 212154835350100768.0000\n",
      "Epoch 37, Test Loss: 339368284002451456.0000\n",
      "Epoch 38, Train Loss: 220848280209639872.0000\n",
      "Epoch 38, Test Loss: 176894206680236032.0000\n",
      "Epoch 39, Train Loss: 189107592691854976.0000\n",
      "Epoch 39, Test Loss: 174445353767010304.0000\n",
      "Epoch 40, Train Loss: 196156732679055552.0000\n",
      "Epoch 40, Test Loss: 192749696928710656.0000\n",
      "Epoch 41, Train Loss: 174522835579484704.0000\n",
      "Epoch 41, Test Loss: 174111978405494784.0000\n",
      "Epoch 42, Train Loss: 166314187470771040.0000\n",
      "Epoch 42, Test Loss: 160825308096757760.0000\n",
      "Epoch 43, Train Loss: 202739973211784160.0000\n",
      "Epoch 43, Test Loss: 170730980150083584.0000\n",
      "Epoch 44, Train Loss: 171421138544524320.0000\n",
      "Epoch 44, Test Loss: 157956338662506496.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:50:32,091]\u001b[0m Trial 17 finished with value: 2.45793694085546e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 246, 'output_activation_class': 'ELU', 'lr': 0.002064785379133217, 'batch_size': 122, 'epochs': 45}. Best is trial 13 with value: 1.7142691947085824e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, Train Loss: 157164844580649504.0000\n",
      "Epoch 45, Test Loss: 245793694085545984.0000\n",
      "Epoch 1, Train Loss: 498413430532170368.0000\n",
      "Epoch 1, Test Loss: 500558647139500032.0000\n",
      "Epoch 2, Train Loss: 494692070793022784.0000\n",
      "Epoch 2, Test Loss: 500558647139500032.0000\n",
      "Epoch 3, Train Loss: 507652026862840512.0000\n",
      "Epoch 3, Test Loss: 500558647139500032.0000\n",
      "Epoch 4, Train Loss: 497586691079728192.0000\n",
      "Epoch 4, Test Loss: 500558647139500032.0000\n",
      "Epoch 5, Train Loss: 544119670135741440.0000\n",
      "Epoch 5, Test Loss: 500558647139500032.0000\n",
      "Epoch 6, Train Loss: 493744974504488832.0000\n",
      "Epoch 6, Test Loss: 500558647139500032.0000\n",
      "Epoch 7, Train Loss: 502442620447587520.0000\n",
      "Epoch 7, Test Loss: 500558647139500032.0000\n",
      "Epoch 8, Train Loss: 493724579976177088.0000\n",
      "Epoch 8, Test Loss: 500558647139500032.0000\n",
      "Epoch 9, Train Loss: 517115299829490688.0000\n",
      "Epoch 9, Test Loss: 500558647139500032.0000\n",
      "Epoch 10, Train Loss: 509051865868358976.0000\n",
      "Epoch 10, Test Loss: 500558647139500032.0000\n",
      "Epoch 11, Train Loss: 493871049318643648.0000\n",
      "Epoch 11, Test Loss: 500558647139500032.0000\n",
      "Epoch 12, Train Loss: 504748543539439872.0000\n",
      "Epoch 12, Test Loss: 500558647139500032.0000\n",
      "Epoch 13, Train Loss: 503995999462916928.0000\n",
      "Epoch 13, Test Loss: 500558647139500032.0000\n",
      "Epoch 14, Train Loss: 496315431666487424.0000\n",
      "Epoch 14, Test Loss: 500558647139500032.0000\n",
      "Epoch 15, Train Loss: 494366779950316288.0000\n",
      "Epoch 15, Test Loss: 500558647139500032.0000\n",
      "Epoch 16, Train Loss: 493723489550057024.0000\n",
      "Epoch 16, Test Loss: 500558647139500032.0000\n",
      "Epoch 17, Train Loss: 518327626427148160.0000\n",
      "Epoch 17, Test Loss: 500558647139500032.0000\n",
      "Epoch 18, Train Loss: 494837954251016256.0000\n",
      "Epoch 18, Test Loss: 500558647139500032.0000\n",
      "Epoch 19, Train Loss: 494205075966522048.0000\n",
      "Epoch 19, Test Loss: 500558647139500032.0000\n",
      "Epoch 20, Train Loss: 493831067027575744.0000\n",
      "Epoch 20, Test Loss: 500558647139500032.0000\n",
      "Epoch 21, Train Loss: 511710628425216320.0000\n",
      "Epoch 21, Test Loss: 500558647139500032.0000\n",
      "Epoch 22, Train Loss: 495458808120684032.0000\n",
      "Epoch 22, Test Loss: 500558647139500032.0000\n",
      "Epoch 23, Train Loss: 496934468413958016.0000\n",
      "Epoch 23, Test Loss: 500558647139500032.0000\n",
      "Epoch 24, Train Loss: 493919814680172672.0000\n",
      "Epoch 24, Test Loss: 500558647139500032.0000\n",
      "Epoch 25, Train Loss: 493967668847870784.0000\n",
      "Epoch 25, Test Loss: 500558647139500032.0000\n",
      "Epoch 26, Train Loss: 500411840132761664.0000\n",
      "Epoch 26, Test Loss: 500558647139500032.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:50:48,354]\u001b[0m Trial 18 finished with value: 5.005586471395e+17 and parameters: {'activation_class': 'ReLU', 'hidden_sizes': 426, 'output_activation_class': 'Tanh', 'lr': 0.003844414835443278, 'batch_size': 208, 'epochs': 27}. Best is trial 13 with value: 1.7142691947085824e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Train Loss: 495379331019057024.0000\n",
      "Epoch 27, Test Loss: 500558647139500032.0000\n",
      "Epoch 1, Train Loss: 496725971138953792.0000\n",
      "Epoch 1, Test Loss: 500558647139500032.0000\n",
      "Epoch 2, Train Loss: 496722639366091456.0000\n",
      "Epoch 2, Test Loss: 500558647139500032.0000\n",
      "Epoch 3, Train Loss: 497677541224553088.0000\n",
      "Epoch 3, Test Loss: 500558647139500032.0000\n",
      "Epoch 4, Train Loss: 512722626322800384.0000\n",
      "Epoch 4, Test Loss: 500558647139500032.0000\n",
      "Epoch 5, Train Loss: 496942643191489344.0000\n",
      "Epoch 5, Test Loss: 500558647139500032.0000\n",
      "Epoch 6, Train Loss: 497916963255457280.0000\n",
      "Epoch 6, Test Loss: 500558647139500032.0000\n",
      "Epoch 7, Train Loss: 496777958729991808.0000\n",
      "Epoch 7, Test Loss: 500558647139500032.0000\n",
      "Epoch 8, Train Loss: 498663659992948480.0000\n",
      "Epoch 8, Test Loss: 500558647139500032.0000\n",
      "Epoch 9, Train Loss: 497152872097115712.0000\n",
      "Epoch 9, Test Loss: 500558647139500032.0000\n",
      "Epoch 10, Train Loss: 496828692751169920.0000\n",
      "Epoch 10, Test Loss: 500558647139500032.0000\n",
      "Epoch 11, Train Loss: 496769726663830592.0000\n",
      "Epoch 11, Test Loss: 500558647139500032.0000\n",
      "Epoch 12, Train Loss: 499080137098314304.0000\n",
      "Epoch 12, Test Loss: 500558647139500032.0000\n",
      "Epoch 13, Train Loss: 497792027640228352.0000\n",
      "Epoch 13, Test Loss: 500558647139500032.0000\n",
      "Epoch 14, Train Loss: 497396090911507008.0000\n",
      "Epoch 14, Test Loss: 500558647139500032.0000\n",
      "Epoch 15, Train Loss: 502554406740404160.0000\n",
      "Epoch 15, Test Loss: 500558647139500032.0000\n",
      "Epoch 16, Train Loss: 496718189211861568.0000\n",
      "Epoch 16, Test Loss: 500558647139500032.0000\n",
      "Epoch 17, Train Loss: 496709856250705152.0000\n",
      "Epoch 17, Test Loss: 500558647139500032.0000\n",
      "Epoch 18, Train Loss: 496767483628730752.0000\n",
      "Epoch 18, Test Loss: 500558647139500032.0000\n",
      "Epoch 19, Train Loss: 496738780901476544.0000\n",
      "Epoch 19, Test Loss: 500558647139500032.0000\n",
      "Epoch 20, Train Loss: 497123047594289664.0000\n",
      "Epoch 20, Test Loss: 500558647139500032.0000\n",
      "Epoch 21, Train Loss: 496797722276555264.0000\n",
      "Epoch 21, Test Loss: 500558647139500032.0000\n",
      "Epoch 22, Train Loss: 496717568960500160.0000\n",
      "Epoch 22, Test Loss: 500558647139500032.0000\n",
      "Epoch 23, Train Loss: 499013434823924928.0000\n",
      "Epoch 23, Test Loss: 500558647139500032.0000\n",
      "Epoch 24, Train Loss: 498952634609678720.0000\n",
      "Epoch 24, Test Loss: 500558647139500032.0000\n",
      "Epoch 25, Train Loss: 496765334335457472.0000\n",
      "Epoch 25, Test Loss: 500558647139500032.0000\n",
      "Epoch 26, Train Loss: 498089024030552192.0000\n",
      "Epoch 26, Test Loss: 500558647139500032.0000\n",
      "Epoch 27, Train Loss: 497268304989596736.0000\n",
      "Epoch 27, Test Loss: 500558647139500032.0000\n",
      "Epoch 28, Train Loss: 500821650263127296.0000\n",
      "Epoch 28, Test Loss: 500558647139500032.0000\n",
      "Epoch 29, Train Loss: 498747395580163904.0000\n",
      "Epoch 29, Test Loss: 500558647139500032.0000\n",
      "Epoch 30, Train Loss: 496714914475169856.0000\n",
      "Epoch 30, Test Loss: 500558647139500032.0000\n",
      "Epoch 31, Train Loss: 497489190642823296.0000\n",
      "Epoch 31, Test Loss: 500558647139500032.0000\n",
      "Epoch 32, Train Loss: 498629645853659584.0000\n",
      "Epoch 32, Test Loss: 500558647139500032.0000\n",
      "Epoch 33, Train Loss: 506092615993221376.0000\n",
      "Epoch 33, Test Loss: 500558647139500032.0000\n",
      "Epoch 34, Train Loss: 504345981489069504.0000\n",
      "Epoch 34, Test Loss: 500558647139500032.0000\n",
      "Epoch 35, Train Loss: 505907652291723264.0000\n",
      "Epoch 35, Test Loss: 500558647139500032.0000\n",
      "Epoch 36, Train Loss: 497048084197770368.0000\n",
      "Epoch 36, Test Loss: 500558578420023296.0000\n",
      "Epoch 37, Train Loss: 498094645112968704.0000\n",
      "Epoch 37, Test Loss: 500558578420023296.0000\n",
      "Epoch 38, Train Loss: 499739197053111616.0000\n",
      "Epoch 38, Test Loss: 500558578420023296.0000\n",
      "Epoch 39, Train Loss: 497657134164150464.0000\n",
      "Epoch 39, Test Loss: 500558578420023296.0000\n",
      "Epoch 40, Train Loss: 531432850484525952.0000\n",
      "Epoch 40, Test Loss: 500558578420023296.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:50:54,726]\u001b[0m Trial 19 finished with value: 5.005585784200233e+17 and parameters: {'activation_class': 'Sigmoid', 'hidden_sizes': 69, 'output_activation_class': 'LeakyReLU', 'lr': 0.0016922399573666767, 'batch_size': 139, 'epochs': 42}. Best is trial 13 with value: 1.7142691947085824e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, Train Loss: 496748027581924480.0000\n",
      "Epoch 41, Test Loss: 500558578420023296.0000\n",
      "Epoch 42, Train Loss: 496841833727755392.0000\n",
      "Epoch 42, Test Loss: 500558578420023296.0000\n",
      "Epoch 1, Train Loss: 510111963297087488.0000\n",
      "Epoch 1, Test Loss: 500558647139500032.0000\n",
      "Epoch 2, Train Loss: 496642480356196352.0000\n",
      "Epoch 2, Test Loss: 500558647139500032.0000\n",
      "Epoch 3, Train Loss: 497307586006089728.0000\n",
      "Epoch 3, Test Loss: 500558647139500032.0000\n",
      "Epoch 4, Train Loss: 497945467301134336.0000\n",
      "Epoch 4, Test Loss: 500558647139500032.0000\n",
      "Epoch 5, Train Loss: 499205280412729344.0000\n",
      "Epoch 5, Test Loss: 500558647139500032.0000\n",
      "Epoch 6, Train Loss: 497314053186650112.0000\n",
      "Epoch 6, Test Loss: 500558647139500032.0000\n",
      "Epoch 7, Train Loss: 502564110160887808.0000\n",
      "Epoch 7, Test Loss: 500558647139500032.0000\n",
      "Epoch 8, Train Loss: 496676209808113664.0000\n",
      "Epoch 8, Test Loss: 500558578420023296.0000\n",
      "Epoch 9, Train Loss: 496788706611429376.0000\n",
      "Epoch 9, Test Loss: 500558578420023296.0000\n",
      "Epoch 10, Train Loss: 498448423272316928.0000\n",
      "Epoch 10, Test Loss: 500558578420023296.0000\n",
      "Epoch 11, Train Loss: 497638278187450368.0000\n",
      "Epoch 11, Test Loss: 500558509700546560.0000\n",
      "Epoch 12, Train Loss: 514514096268247040.0000\n",
      "Epoch 12, Test Loss: 500558509700546560.0000\n",
      "Epoch 13, Train Loss: 504367103994757120.0000\n",
      "Epoch 13, Test Loss: 500558509700546560.0000\n",
      "Epoch 14, Train Loss: 499490141249732608.0000\n",
      "Epoch 14, Test Loss: 500558509700546560.0000\n",
      "Epoch 15, Train Loss: 497936346401210368.0000\n",
      "Epoch 15, Test Loss: 500558509700546560.0000\n",
      "Epoch 16, Train Loss: 514693862795509760.0000\n",
      "Epoch 16, Test Loss: 500558509700546560.0000\n",
      "Epoch 17, Train Loss: 497783869089513472.0000\n",
      "Epoch 17, Test Loss: 500558509700546560.0000\n",
      "Epoch 18, Train Loss: 498227851535843328.0000\n",
      "Epoch 18, Test Loss: 500558509700546560.0000\n",
      "Epoch 19, Train Loss: 503741501440786432.0000\n",
      "Epoch 19, Test Loss: 500558509700546560.0000\n",
      "Epoch 20, Train Loss: 497014827613421568.0000\n",
      "Epoch 20, Test Loss: 500558509700546560.0000\n",
      "Epoch 21, Train Loss: 497408208306962432.0000\n",
      "Epoch 21, Test Loss: 500558509700546560.0000\n",
      "Epoch 22, Train Loss: 496925276370894848.0000\n",
      "Epoch 22, Test Loss: 500558509700546560.0000\n",
      "Epoch 23, Train Loss: 512280021264498688.0000\n",
      "Epoch 23, Test Loss: 500558509700546560.0000\n",
      "Epoch 24, Train Loss: 496830933052162048.0000\n",
      "Epoch 24, Test Loss: 500558509700546560.0000\n",
      "Epoch 25, Train Loss: 498201975028973568.0000\n",
      "Epoch 25, Test Loss: 500558509700546560.0000\n",
      "Epoch 26, Train Loss: 500728352504545280.0000\n",
      "Epoch 26, Test Loss: 500558440981069824.0000\n",
      "Epoch 27, Train Loss: 498105919041699840.0000\n",
      "Epoch 27, Test Loss: 500558440981069824.0000\n",
      "Epoch 28, Train Loss: 502427454787289088.0000\n",
      "Epoch 28, Test Loss: 500558440981069824.0000\n",
      "Epoch 29, Train Loss: 497072785940545536.0000\n",
      "Epoch 29, Test Loss: 500558440981069824.0000\n",
      "Epoch 30, Train Loss: 504361105770938368.0000\n",
      "Epoch 30, Test Loss: 500558440981069824.0000\n",
      "Epoch 31, Train Loss: 499507915065917440.0000\n",
      "Epoch 31, Test Loss: 500558440981069824.0000\n",
      "Epoch 32, Train Loss: 500583166604476416.0000\n",
      "Epoch 32, Test Loss: 500558440981069824.0000\n",
      "Epoch 33, Train Loss: 505741345118224384.0000\n",
      "Epoch 33, Test Loss: 500558440981069824.0000\n",
      "Epoch 34, Train Loss: 497081091945922560.0000\n",
      "Epoch 34, Test Loss: 500558440981069824.0000\n",
      "Epoch 35, Train Loss: 497742360478744576.0000\n",
      "Epoch 35, Test Loss: 500558440981069824.0000\n",
      "Epoch 36, Train Loss: 496905919393693696.0000\n",
      "Epoch 36, Test Loss: 500558440981069824.0000\n",
      "Epoch 37, Train Loss: 512817750146547712.0000\n",
      "Epoch 37, Test Loss: 500558440981069824.0000\n",
      "Epoch 38, Train Loss: 499539751141900288.0000\n",
      "Epoch 38, Test Loss: 500558440981069824.0000\n",
      "Epoch 39, Train Loss: 513927402125000704.0000\n",
      "Epoch 39, Test Loss: 500558440981069824.0000\n",
      "Epoch 40, Train Loss: 506528557678198784.0000\n",
      "Epoch 40, Test Loss: 500558440981069824.0000\n",
      "Epoch 41, Train Loss: 498221767983104000.0000\n",
      "Epoch 41, Test Loss: 500558406621331456.0000\n",
      "Epoch 42, Train Loss: 498670930495537152.0000\n",
      "Epoch 42, Test Loss: 500558406621331456.0000\n",
      "Epoch 43, Train Loss: 497176146115297280.0000\n",
      "Epoch 43, Test Loss: 500558406621331456.0000\n",
      "Epoch 44, Train Loss: 498162168466767872.0000\n",
      "Epoch 44, Test Loss: 500558406621331456.0000\n",
      "Epoch 45, Train Loss: 513931277192134656.0000\n",
      "Epoch 45, Test Loss: 500558406621331456.0000\n",
      "Epoch 46, Train Loss: 500695928521359360.0000\n",
      "Epoch 46, Test Loss: 500558406621331456.0000\n",
      "Epoch 47, Train Loss: 496828734431559680.0000\n",
      "Epoch 47, Test Loss: 500558406621331456.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:51:03,130]\u001b[0m Trial 20 finished with value: 5.0055840662133146e+17 and parameters: {'activation_class': 'Tanh', 'hidden_sizes': 234, 'output_activation_class': 'ReLU', 'lr': 0.0036374813074552125, 'batch_size': 252, 'epochs': 48}. Best is trial 13 with value: 1.7142691947085824e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, Train Loss: 497509132715163648.0000\n",
      "Epoch 48, Test Loss: 500558406621331456.0000\n",
      "Epoch 1, Train Loss: 500216120658422848.0000\n",
      "Epoch 1, Test Loss: 500531709104619520.0000\n",
      "Epoch 2, Train Loss: 499793526443938752.0000\n",
      "Epoch 2, Test Loss: 499277063258112000.0000\n",
      "Epoch 3, Train Loss: 499229005826268544.0000\n",
      "Epoch 3, Test Loss: 491018734501101568.0000\n",
      "Epoch 4, Train Loss: 488792407322818112.0000\n",
      "Epoch 4, Test Loss: 482252259573366784.0000\n",
      "Epoch 5, Train Loss: 479989015385694848.0000\n",
      "Epoch 5, Test Loss: 478142525626908672.0000\n",
      "Epoch 6, Train Loss: 476233038361566592.0000\n",
      "Epoch 6, Test Loss: 474453835914412032.0000\n",
      "Epoch 7, Train Loss: 472824801320319744.0000\n",
      "Epoch 7, Test Loss: 470526242941042688.0000\n",
      "Epoch 8, Train Loss: 469031364603230976.0000\n",
      "Epoch 8, Test Loss: 466852946391334912.0000\n",
      "Epoch 9, Train Loss: 464716349391039552.0000\n",
      "Epoch 9, Test Loss: 462465070362787840.0000\n",
      "Epoch 10, Train Loss: 460288864905653312.0000\n",
      "Epoch 10, Test Loss: 458352965594382336.0000\n",
      "Epoch 11, Train Loss: 456723857242486336.0000\n",
      "Epoch 11, Test Loss: 452118665945415680.0000\n",
      "Epoch 12, Train Loss: 449109292337049920.0000\n",
      "Epoch 12, Test Loss: 444847939148316672.0000\n",
      "Epoch 13, Train Loss: 441623853806266240.0000\n",
      "Epoch 13, Test Loss: 437357619263307776.0000\n",
      "Epoch 14, Train Loss: 434566991498606144.0000\n",
      "Epoch 14, Test Loss: 431177814159392768.0000\n",
      "Epoch 15, Train Loss: 425270630142947648.0000\n",
      "Epoch 15, Test Loss: 421460605271015424.0000\n",
      "Epoch 16, Train Loss: 417146199180171392.0000\n",
      "Epoch 16, Test Loss: 410027539768016896.0000\n",
      "Epoch 17, Train Loss: 405057861832409088.0000\n",
      "Epoch 17, Test Loss: 411531706034552832.0000\n",
      "Epoch 18, Train Loss: 396772181546671488.0000\n",
      "Epoch 18, Test Loss: 390310925460045824.0000\n",
      "Epoch 19, Train Loss: 384207896014527872.0000\n",
      "Epoch 19, Test Loss: 382149834762616832.0000\n",
      "Epoch 20, Train Loss: 378084970256904512.0000\n",
      "Epoch 20, Test Loss: 375430891003969536.0000\n",
      "Epoch 21, Train Loss: 368695294500778304.0000\n",
      "Epoch 21, Test Loss: 365536866901950464.0000\n",
      "Epoch 22, Train Loss: 360247990218129408.0000\n",
      "Epoch 22, Test Loss: 354957228580536320.0000\n",
      "Epoch 23, Train Loss: 350739000590577984.0000\n",
      "Epoch 23, Test Loss: 342389329638719488.0000\n",
      "Epoch 24, Train Loss: 343128314112880960.0000\n",
      "Epoch 24, Test Loss: 334948556496437248.0000\n",
      "Epoch 25, Train Loss: 336959737023975104.0000\n",
      "Epoch 25, Test Loss: 324921147890073600.0000\n",
      "Epoch 26, Train Loss: 324857914531996288.0000\n",
      "Epoch 26, Test Loss: 331389712235233280.0000\n",
      "Epoch 27, Train Loss: 317577364236140544.0000\n",
      "Epoch 27, Test Loss: 319016392491794432.0000\n",
      "Epoch 28, Train Loss: 309020161176925760.0000\n",
      "Epoch 28, Test Loss: 328983362318368768.0000\n",
      "Epoch 29, Train Loss: 318290175349953472.0000\n",
      "Epoch 29, Test Loss: 293169313664204800.0000\n",
      "Epoch 30, Train Loss: 297297660305715520.0000\n",
      "Epoch 30, Test Loss: 293499854347304960.0000\n",
      "Epoch 31, Train Loss: 287757972987909056.0000\n",
      "Epoch 31, Test Loss: 281510092363268096.0000\n",
      "Epoch 32, Train Loss: 282269181837453152.0000\n",
      "Epoch 32, Test Loss: 274836451919659008.0000\n",
      "Epoch 33, Train Loss: 280559899537531520.0000\n",
      "Epoch 33, Test Loss: 282891388205400064.0000\n",
      "Epoch 34, Train Loss: 276193061229297664.0000\n",
      "Epoch 34, Test Loss: 272691201654652928.0000\n",
      "Epoch 35, Train Loss: 263072009340301632.0000\n",
      "Epoch 35, Test Loss: 259355379740180480.0000\n",
      "Epoch 36, Train Loss: 251321741453900576.0000\n",
      "Epoch 36, Test Loss: 249990272270729216.0000\n",
      "Epoch 37, Train Loss: 256423355003154752.0000\n",
      "Epoch 37, Test Loss: 248768354075017216.0000\n",
      "Epoch 38, Train Loss: 246590432220928160.0000\n",
      "Epoch 38, Test Loss: 279011297570324480.0000\n",
      "Epoch 39, Train Loss: 248693494800570528.0000\n",
      "Epoch 39, Test Loss: 237295637433942016.0000\n",
      "Epoch 40, Train Loss: 251226904066805440.0000\n",
      "Epoch 40, Test Loss: 224983340585844736.0000\n",
      "Epoch 41, Train Loss: 229659400025447808.0000\n",
      "Epoch 41, Test Loss: 235385012642381824.0000\n",
      "Epoch 42, Train Loss: 222619014187994816.0000\n",
      "Epoch 42, Test Loss: 216727502909865984.0000\n",
      "Epoch 43, Train Loss: 236019897067211232.0000\n",
      "Epoch 43, Test Loss: 245374092960595968.0000\n",
      "Epoch 44, Train Loss: 227962287417277216.0000\n",
      "Epoch 44, Test Loss: 206336740070260736.0000\n",
      "Epoch 45, Train Loss: 215663750409335104.0000\n",
      "Epoch 45, Test Loss: 203464059784265728.0000\n",
      "Epoch 46, Train Loss: 206047247370430304.0000\n",
      "Epoch 46, Test Loss: 199062663198670848.0000\n",
      "Epoch 47, Train Loss: 206632247453649376.0000\n",
      "Epoch 47, Test Loss: 191872200750399488.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:51:19,488]\u001b[0m Trial 21 finished with value: 2.1139062808746394e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 332, 'output_activation_class': 'ReLU', 'lr': 0.0012997677074387136, 'batch_size': 154, 'epochs': 48}. Best is trial 13 with value: 1.7142691947085824e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, Train Loss: 203590831918022656.0000\n",
      "Epoch 48, Test Loss: 211390628087463936.0000\n",
      "Epoch 1, Train Loss: 500309975138908096.0000\n",
      "Epoch 1, Test Loss: 500552702904762368.0000\n",
      "Epoch 2, Train Loss: 500275609109813504.0000\n",
      "Epoch 2, Test Loss: 500346097797955584.0000\n",
      "Epoch 3, Train Loss: 499554104893425728.0000\n",
      "Epoch 3, Test Loss: 498467272944254976.0000\n",
      "Epoch 4, Train Loss: 495527980464044672.0000\n",
      "Epoch 4, Test Loss: 491714759721222144.0000\n",
      "Epoch 5, Train Loss: 488052753380056000.0000\n",
      "Epoch 5, Test Loss: 483871977640034304.0000\n",
      "Epoch 6, Train Loss: 481935435311828480.0000\n",
      "Epoch 6, Test Loss: 480155387819982848.0000\n",
      "Epoch 7, Train Loss: 479070653447348608.0000\n",
      "Epoch 7, Test Loss: 477975709097132032.0000\n",
      "Epoch 8, Train Loss: 477224747510541440.0000\n",
      "Epoch 8, Test Loss: 475987242318299136.0000\n",
      "Epoch 9, Train Loss: 475382661433615424.0000\n",
      "Epoch 9, Test Loss: 473945517944995840.0000\n",
      "Epoch 10, Train Loss: 472949266434478976.0000\n",
      "Epoch 10, Test Loss: 471924375054974976.0000\n",
      "Epoch 11, Train Loss: 470972993207852672.0000\n",
      "Epoch 11, Test Loss: 469625433680248832.0000\n",
      "Epoch 12, Train Loss: 469925470618608704.0000\n",
      "Epoch 12, Test Loss: 467445789317136384.0000\n",
      "Epoch 13, Train Loss: 466570512376437696.0000\n",
      "Epoch 13, Test Loss: 465235564786876416.0000\n",
      "Epoch 14, Train Loss: 464502514661127360.0000\n",
      "Epoch 14, Test Loss: 462970811351826432.0000\n",
      "Epoch 15, Train Loss: 461644898179287872.0000\n",
      "Epoch 15, Test Loss: 459847648573128704.0000\n",
      "Epoch 16, Train Loss: 459700909474874752.0000\n",
      "Epoch 16, Test Loss: 456882368792231936.0000\n",
      "Epoch 17, Train Loss: 455558639027008000.0000\n",
      "Epoch 17, Test Loss: 453984571537489920.0000\n",
      "Epoch 18, Train Loss: 451928514344791168.0000\n",
      "Epoch 18, Test Loss: 448908126352048128.0000\n",
      "Epoch 19, Train Loss: 447756684811567104.0000\n",
      "Epoch 19, Test Loss: 444687273011707904.0000\n",
      "Epoch 20, Train Loss: 443283622798488768.0000\n",
      "Epoch 20, Test Loss: 439905977978847232.0000\n",
      "Epoch 21, Train Loss: 438330930419808192.0000\n",
      "Epoch 21, Test Loss: 434179996299296768.0000\n",
      "Epoch 22, Train Loss: 432117902483379072.0000\n",
      "Epoch 22, Test Loss: 429633824956088320.0000\n",
      "Epoch 23, Train Loss: 425705684841233024.0000\n",
      "Epoch 23, Test Loss: 422165735821803520.0000\n",
      "Epoch 24, Train Loss: 420126110100237440.0000\n",
      "Epoch 24, Test Loss: 415503932308062208.0000\n",
      "Epoch 25, Train Loss: 414287102831618496.0000\n",
      "Epoch 25, Test Loss: 410829633500479488.0000\n",
      "Epoch 26, Train Loss: 407980633796777536.0000\n",
      "Epoch 26, Test Loss: 402188640337264640.0000\n",
      "Epoch 27, Train Loss: 399776161828859712.0000\n",
      "Epoch 27, Test Loss: 395493851474690048.0000\n",
      "Epoch 28, Train Loss: 400309893469899584.0000\n",
      "Epoch 28, Test Loss: 390579137577746432.0000\n",
      "Epoch 29, Train Loss: 389783056822840448.0000\n",
      "Epoch 29, Test Loss: 392461570203975680.0000\n",
      "Epoch 30, Train Loss: 385208604100360832.0000\n",
      "Epoch 30, Test Loss: 377645479221002240.0000\n",
      "Epoch 31, Train Loss: 378146486549183104.0000\n",
      "Epoch 31, Test Loss: 371341944699224064.0000\n",
      "Epoch 32, Train Loss: 372498939842498752.0000\n",
      "Epoch 32, Test Loss: 366435786377134080.0000\n",
      "Epoch 33, Train Loss: 363407681565297472.0000\n",
      "Epoch 33, Test Loss: 362013516250480640.0000\n",
      "Epoch 34, Train Loss: 358022707474174144.0000\n",
      "Epoch 34, Test Loss: 354875383683743744.0000\n",
      "Epoch 35, Train Loss: 352751649710988928.0000\n",
      "Epoch 35, Test Loss: 347271367424475136.0000\n",
      "Epoch 36, Train Loss: 349850123584314304.0000\n",
      "Epoch 36, Test Loss: 353044284506636288.0000\n",
      "Epoch 37, Train Loss: 343571022244478976.0000\n",
      "Epoch 37, Test Loss: 336959803781808128.0000\n",
      "Epoch 38, Train Loss: 336037956767483264.0000\n",
      "Epoch 38, Test Loss: 331916996780228608.0000\n",
      "Epoch 39, Train Loss: 332320908996543872.0000\n",
      "Epoch 39, Test Loss: 335025453590904832.0000\n",
      "Epoch 40, Train Loss: 329919555363781952.0000\n",
      "Epoch 40, Test Loss: 325087105426391040.0000\n",
      "Epoch 41, Train Loss: 318728706478845056.0000\n",
      "Epoch 41, Test Loss: 314806293749563392.0000\n",
      "Epoch 42, Train Loss: 314621961222727424.0000\n",
      "Epoch 42, Test Loss: 308751970410168320.0000\n",
      "Epoch 43, Train Loss: 312451748817876928.0000\n",
      "Epoch 43, Test Loss: 303903639527751680.0000\n",
      "Epoch 44, Train Loss: 307449924254182528.0000\n",
      "Epoch 44, Test Loss: 301726400346324992.0000\n",
      "Epoch 45, Train Loss: 299792326620599168.0000\n",
      "Epoch 45, Test Loss: 295925136480010240.0000\n",
      "Epoch 46, Train Loss: 299278933096923136.0000\n",
      "Epoch 46, Test Loss: 289323771746058240.0000\n",
      "Epoch 47, Train Loss: 293045976115422976.0000\n",
      "Epoch 47, Test Loss: 285282740096466944.0000\n",
      "Epoch 48, Train Loss: 287337899641000832.0000\n",
      "Epoch 48, Test Loss: 279687256703238144.0000\n",
      "Epoch 49, Train Loss: 282504922052872288.0000\n",
      "Epoch 49, Test Loss: 275046115043180544.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:51:40,854]\u001b[0m Trial 22 finished with value: 2.8244890349469696e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 413, 'output_activation_class': 'ReLU', 'lr': 0.0008136863298282382, 'batch_size': 174, 'epochs': 50}. Best is trial 13 with value: 1.7142691947085824e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Train Loss: 276416633271780768.0000\n",
      "Epoch 50, Test Loss: 282448903494696960.0000\n",
      "Epoch 1, Train Loss: 500304259457370432.0000\n",
      "Epoch 1, Test Loss: 500442064547217408.0000\n",
      "Epoch 2, Train Loss: 498572376689981120.0000\n",
      "Epoch 2, Test Loss: 494430862679474176.0000\n",
      "Epoch 3, Train Loss: 485359168089295360.0000\n",
      "Epoch 3, Test Loss: 481039395168714752.0000\n",
      "Epoch 4, Train Loss: 478605713111886208.0000\n",
      "Epoch 4, Test Loss: 476857093454823424.0000\n",
      "Epoch 5, Train Loss: 475251827227982592.0000\n",
      "Epoch 5, Test Loss: 473114596392042496.0000\n",
      "Epoch 6, Train Loss: 472719698636066752.0000\n",
      "Epoch 6, Test Loss: 468928068430331904.0000\n",
      "Epoch 7, Train Loss: 467098619092398784.0000\n",
      "Epoch 7, Test Loss: 465158461533978624.0000\n",
      "Epoch 8, Train Loss: 464065407891541312.0000\n",
      "Epoch 8, Test Loss: 458779129429360640.0000\n",
      "Epoch 9, Train Loss: 455590421419707968.0000\n",
      "Epoch 9, Test Loss: 451367493345214464.0000\n",
      "Epoch 10, Train Loss: 450265490259899904.0000\n",
      "Epoch 10, Test Loss: 443671770944241664.0000\n",
      "Epoch 11, Train Loss: 439527723193254080.0000\n",
      "Epoch 11, Test Loss: 433606532265934848.0000\n",
      "Epoch 12, Train Loss: 428799551655280192.0000\n",
      "Epoch 12, Test Loss: 423263769980829696.0000\n",
      "Epoch 13, Train Loss: 419480164991210880.0000\n",
      "Epoch 13, Test Loss: 409472733072588800.0000\n",
      "Epoch 14, Train Loss: 408621116596226560.0000\n",
      "Epoch 14, Test Loss: 397722183587069952.0000\n",
      "Epoch 15, Train Loss: 392048723988840448.0000\n",
      "Epoch 15, Test Loss: 383288550851870720.0000\n",
      "Epoch 16, Train Loss: 380250520328172608.0000\n",
      "Epoch 16, Test Loss: 374021626334806016.0000\n",
      "Epoch 17, Train Loss: 366643198718163520.0000\n",
      "Epoch 17, Test Loss: 357242048102793216.0000\n",
      "Epoch 18, Train Loss: 364017907255047232.0000\n",
      "Epoch 18, Test Loss: 347107608911413248.0000\n",
      "Epoch 19, Train Loss: 347704631147177088.0000\n",
      "Epoch 19, Test Loss: 336942349034717184.0000\n",
      "Epoch 20, Train Loss: 334416364588744064.0000\n",
      "Epoch 20, Test Loss: 331125210969276416.0000\n",
      "Epoch 21, Train Loss: 325846403714264704.0000\n",
      "Epoch 21, Test Loss: 317279576436768768.0000\n",
      "Epoch 22, Train Loss: 309189662160484288.0000\n",
      "Epoch 22, Test Loss: 312150526491885568.0000\n",
      "Epoch 23, Train Loss: 305533535194001856.0000\n",
      "Epoch 23, Test Loss: 303153394640486400.0000\n",
      "Epoch 24, Train Loss: 296058246772307264.0000\n",
      "Epoch 24, Test Loss: 289811267714023424.0000\n",
      "Epoch 25, Train Loss: 288038175294960096.0000\n",
      "Epoch 25, Test Loss: 279963096682856448.0000\n",
      "Epoch 26, Train Loss: 281771283807197216.0000\n",
      "Epoch 26, Test Loss: 268331225733660672.0000\n",
      "Epoch 27, Train Loss: 273858230893446400.0000\n",
      "Epoch 27, Test Loss: 268074816186089472.0000\n",
      "Epoch 28, Train Loss: 270441159915409568.0000\n",
      "Epoch 28, Test Loss: 271000290210086912.0000\n",
      "Epoch 29, Train Loss: 265417459305933824.0000\n",
      "Epoch 29, Test Loss: 267183610472169472.0000\n",
      "Epoch 30, Train Loss: 250176047457239040.0000\n",
      "Epoch 30, Test Loss: 238333524870955008.0000\n",
      "Epoch 31, Train Loss: 248868080904296480.0000\n",
      "Epoch 31, Test Loss: 257592244125564928.0000\n",
      "Epoch 32, Train Loss: 235949265912693152.0000\n",
      "Epoch 32, Test Loss: 265590108885876736.0000\n",
      "Epoch 33, Train Loss: 235147311092092864.0000\n",
      "Epoch 33, Test Loss: 255922979316170752.0000\n",
      "Epoch 34, Train Loss: 234171716037907456.0000\n",
      "Epoch 34, Test Loss: 300843320710529024.0000\n",
      "Epoch 35, Train Loss: 230190357750984768.0000\n",
      "Epoch 35, Test Loss: 296738740724826112.0000\n",
      "Epoch 36, Train Loss: 212082261444675872.0000\n",
      "Epoch 36, Test Loss: 214649030796378112.0000\n",
      "Epoch 37, Train Loss: 229678029611218240.0000\n",
      "Epoch 37, Test Loss: 222109062572015616.0000\n",
      "Epoch 38, Train Loss: 211942172629728416.0000\n",
      "Epoch 38, Test Loss: 192272079385526272.0000\n",
      "Epoch 39, Train Loss: 204577029559589088.0000\n",
      "Epoch 39, Test Loss: 188499277033504768.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:51:54,834]\u001b[0m Trial 23 finished with value: 1.8321055738442547e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 247, 'output_activation_class': 'ReLU', 'lr': 0.001539619920408435, 'batch_size': 104, 'epochs': 40}. Best is trial 13 with value: 1.7142691947085824e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Train Loss: 206514724930393920.0000\n",
      "Epoch 40, Test Loss: 183210557384425472.0000\n",
      "Epoch 1, Train Loss: 500331917071707328.0000\n",
      "Epoch 1, Test Loss: 500471270324830208.0000\n",
      "Epoch 2, Train Loss: 497549043473763200.0000\n",
      "Epoch 2, Test Loss: 495872425502703616.0000\n",
      "Epoch 3, Train Loss: 487683547889229504.0000\n",
      "Epoch 3, Test Loss: 483387780206952448.0000\n",
      "Epoch 4, Train Loss: 481339203183910784.0000\n",
      "Epoch 4, Test Loss: 482416808360411136.0000\n",
      "Epoch 5, Train Loss: 480582258471257216.0000\n",
      "Epoch 5, Test Loss: 481894815215124480.0000\n",
      "Epoch 6, Train Loss: 480150119042396416.0000\n",
      "Epoch 6, Test Loss: 481416046620704768.0000\n",
      "Epoch 7, Train Loss: 480705187260148224.0000\n",
      "Epoch 7, Test Loss: 480995552142557184.0000\n",
      "Epoch 8, Train Loss: 479721007741984896.0000\n",
      "Epoch 8, Test Loss: 480483763839565824.0000\n",
      "Epoch 9, Train Loss: 478841199415098048.0000\n",
      "Epoch 9, Test Loss: 479911983433383936.0000\n",
      "Epoch 10, Train Loss: 490753921570548672.0000\n",
      "Epoch 10, Test Loss: 479441564255387648.0000\n",
      "Epoch 11, Train Loss: 477780752162263872.0000\n",
      "Epoch 11, Test Loss: 478822573568688128.0000\n",
      "Epoch 12, Train Loss: 477659477406463104.0000\n",
      "Epoch 12, Test Loss: 478226260309311488.0000\n",
      "Epoch 13, Train Loss: 476311358701104768.0000\n",
      "Epoch 13, Test Loss: 477530819204743168.0000\n",
      "Epoch 14, Train Loss: 475422295896398400.0000\n",
      "Epoch 14, Test Loss: 476627639122001920.0000\n",
      "Epoch 15, Train Loss: 475360560463040704.0000\n",
      "Epoch 15, Test Loss: 475867292471656448.0000\n",
      "Epoch 16, Train Loss: 474566775624279232.0000\n",
      "Epoch 16, Test Loss: 475191333338742784.0000\n",
      "Epoch 17, Train Loss: 477466733203444800.0000\n",
      "Epoch 17, Test Loss: 474452530244354048.0000\n",
      "Epoch 18, Train Loss: 474769106017796800.0000\n",
      "Epoch 18, Test Loss: 473713589711011840.0000\n",
      "Epoch 19, Train Loss: 471757642938956224.0000\n",
      "Epoch 19, Test Loss: 472864388777246720.0000\n",
      "Epoch 20, Train Loss: 473038869323152064.0000\n",
      "Epoch 20, Test Loss: 471999966479384576.0000\n",
      "Epoch 21, Train Loss: 472138595094671168.0000\n",
      "Epoch 21, Test Loss: 470715221502066688.0000\n",
      "Epoch 22, Train Loss: 469073588658947200.0000\n",
      "Epoch 22, Test Loss: 469182055616348160.0000\n",
      "Epoch 23, Train Loss: 468446509931060032.0000\n",
      "Epoch 23, Test Loss: 467518047846924288.0000\n",
      "Epoch 24, Train Loss: 465324219030723392.0000\n",
      "Epoch 24, Test Loss: 465803153304977408.0000\n",
      "Epoch 25, Train Loss: 463831239977290176.0000\n",
      "Epoch 25, Test Loss: 464072281484689408.0000\n",
      "Epoch 26, Train Loss: 461773502289105088.0000\n",
      "Epoch 26, Test Loss: 461796979609960448.0000\n",
      "Epoch 27, Train Loss: 459528526075293760.0000\n",
      "Epoch 27, Test Loss: 458675809696088064.0000\n",
      "Epoch 28, Train Loss: 455985650752545792.0000\n",
      "Epoch 28, Test Loss: 455019486857134080.0000\n",
      "Epoch 29, Train Loss: 452445056336899008.0000\n",
      "Epoch 29, Test Loss: 451139653920096256.0000\n",
      "Epoch 30, Train Loss: 447741215155704896.0000\n",
      "Epoch 30, Test Loss: 446576302707703808.0000\n",
      "Epoch 31, Train Loss: 443153645505257792.0000\n",
      "Epoch 31, Test Loss: 442636580746690560.0000\n",
      "Epoch 32, Train Loss: 439270379139004224.0000\n",
      "Epoch 32, Test Loss: 436765532251750400.0000\n",
      "Epoch 33, Train Loss: 432923882530045504.0000\n",
      "Epoch 33, Test Loss: 429634512150855680.0000\n",
      "Epoch 34, Train Loss: 425479867511147136.0000\n",
      "Epoch 34, Test Loss: 422074132759314432.0000\n",
      "Epoch 35, Train Loss: 418265668528977024.0000\n",
      "Epoch 35, Test Loss: 414323503496429568.0000\n",
      "Epoch 36, Train Loss: 410086256545387584.0000\n",
      "Epoch 36, Test Loss: 406250167570792448.0000\n",
      "Epoch 37, Train Loss: 412412214551424832.0000\n",
      "Epoch 37, Test Loss: 398792180199587840.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:52:04,182]\u001b[0m Trial 24 finished with value: 3.900185584462725e+17 and parameters: {'activation_class': 'LeakyReLU', 'hidden_sizes': 228, 'output_activation_class': 'ReLU', 'lr': 0.0016300192019102213, 'batch_size': 110, 'epochs': 38}. Best is trial 13 with value: 1.7142691947085824e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Train Loss: 394886104571107712.0000\n",
      "Epoch 38, Test Loss: 390018558446272512.0000\n",
      "Epoch 1, Train Loss: 496585178855309312.0000\n",
      "Epoch 1, Test Loss: 500554558330634240.0000\n",
      "Epoch 2, Train Loss: 496553060194994496.0000\n",
      "Epoch 2, Test Loss: 500443232778321920.0000\n",
      "Epoch 3, Train Loss: 496206759989528896.0000\n",
      "Epoch 3, Test Loss: 499634335817662464.0000\n",
      "Epoch 4, Train Loss: 494442360935322176.0000\n",
      "Epoch 4, Test Loss: 496408815378366464.0000\n",
      "Epoch 5, Train Loss: 489704628685598144.0000\n",
      "Epoch 5, Test Loss: 490222859881283584.0000\n",
      "Epoch 6, Train Loss: 483661192787057536.0000\n",
      "Epoch 6, Test Loss: 484588515623960576.0000\n",
      "Epoch 7, Train Loss: 478962086243579584.0000\n",
      "Epoch 7, Test Loss: 481016305424531456.0000\n",
      "Epoch 8, Train Loss: 476129896282418624.0000\n",
      "Epoch 8, Test Loss: 478667782947340288.0000\n",
      "Epoch 9, Train Loss: 473965696477897856.0000\n",
      "Epoch 9, Test Loss: 476586269997006848.0000\n",
      "Epoch 10, Train Loss: 471990088303865280.0000\n",
      "Epoch 10, Test Loss: 474487920774873088.0000\n",
      "Epoch 11, Train Loss: 470018791960627520.0000\n",
      "Epoch 11, Test Loss: 472519760601415680.0000\n",
      "Epoch 12, Train Loss: 468287770046571648.0000\n",
      "Epoch 12, Test Loss: 470617639845101568.0000\n",
      "Epoch 13, Train Loss: 466238185307897856.0000\n",
      "Epoch 13, Test Loss: 468717958630211584.0000\n",
      "Epoch 14, Train Loss: 464449040751862912.0000\n",
      "Epoch 14, Test Loss: 466797455413870592.0000\n",
      "Epoch 15, Train Loss: 462756111688887744.0000\n",
      "Epoch 15, Test Loss: 465097850955497472.0000\n",
      "Epoch 16, Train Loss: 460607025476421952.0000\n",
      "Epoch 16, Test Loss: 463058703562571776.0000\n",
      "Epoch 17, Train Loss: 465909012725273152.0000\n",
      "Epoch 17, Test Loss: 461089993633300480.0000\n",
      "Epoch 18, Train Loss: 457491854006868864.0000\n",
      "Epoch 18, Test Loss: 459269820852994048.0000\n",
      "Epoch 19, Train Loss: 455296684938007104.0000\n",
      "Epoch 19, Test Loss: 457291490196979712.0000\n",
      "Epoch 20, Train Loss: 453459821234112832.0000\n",
      "Epoch 20, Test Loss: 455515675838906368.0000\n",
      "Epoch 21, Train Loss: 464487338927306432.0000\n",
      "Epoch 21, Test Loss: 453569093581144064.0000\n",
      "Epoch 22, Train Loss: 449625267903510208.0000\n",
      "Epoch 22, Test Loss: 451445524311048192.0000\n",
      "Epoch 23, Train Loss: 447110298685557056.0000\n",
      "Epoch 23, Test Loss: 449029897264824320.0000\n",
      "Epoch 24, Train Loss: 447363941123751936.0000\n",
      "Epoch 24, Test Loss: 447017516108087296.0000\n",
      "Epoch 25, Train Loss: 442799212732013440.0000\n",
      "Epoch 25, Test Loss: 444136177168023552.0000\n",
      "Epoch 26, Train Loss: 440127192234758720.0000\n",
      "Epoch 26, Test Loss: 441632554831839232.0000\n",
      "Epoch 27, Train Loss: 437190544709536064.0000\n",
      "Epoch 27, Test Loss: 438856837727518720.0000\n",
      "Epoch 28, Train Loss: 438784351765790720.0000\n",
      "Epoch 28, Test Loss: 435995908472045568.0000\n",
      "Epoch 29, Train Loss: 431593521802684352.0000\n",
      "Epoch 29, Test Loss: 432980051156271104.0000\n",
      "Epoch 30, Train Loss: 428966045137801792.0000\n",
      "Epoch 30, Test Loss: 429609910578184192.0000\n",
      "Epoch 31, Train Loss: 425617538770159936.0000\n",
      "Epoch 31, Test Loss: 426338073211568128.0000\n",
      "Epoch 32, Train Loss: 421921662570744128.0000\n",
      "Epoch 32, Test Loss: 422869698141487104.0000\n",
      "Epoch 33, Train Loss: 419301351562337152.0000\n",
      "Epoch 33, Test Loss: 419129400101961728.0000\n",
      "Epoch 34, Train Loss: 415181943556763072.0000\n",
      "Epoch 34, Test Loss: 415733970756435968.0000\n",
      "Epoch 35, Train Loss: 411897536805376576.0000\n",
      "Epoch 35, Test Loss: 412103520800473088.0000\n",
      "Epoch 36, Train Loss: 409000652389370176.0000\n",
      "Epoch 36, Test Loss: 408302096786391040.0000\n",
      "Epoch 37, Train Loss: 439952753882862144.0000\n",
      "Epoch 37, Test Loss: 404579871969247232.0000\n",
      "Epoch 38, Train Loss: 403393026471680896.0000\n",
      "Epoch 38, Test Loss: 399979721477062656.0000\n",
      "Epoch 39, Train Loss: 402875065912412480.0000\n",
      "Epoch 39, Test Loss: 396714824777596928.0000\n",
      "Epoch 40, Train Loss: 392693715285873216.0000\n",
      "Epoch 40, Test Loss: 391701670230228992.0000\n",
      "Epoch 41, Train Loss: 387806106113839680.0000\n",
      "Epoch 41, Test Loss: 387763013421105152.0000\n",
      "Epoch 42, Train Loss: 384011821993951232.0000\n",
      "Epoch 42, Test Loss: 383632938509533184.0000\n",
      "Epoch 43, Train Loss: 379825893409067584.0000\n",
      "Epoch 43, Test Loss: 380369141321695232.0000\n",
      "Epoch 44, Train Loss: 374609023758499840.0000\n",
      "Epoch 44, Test Loss: 381669451260493824.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:52:18,552]\u001b[0m Trial 25 finished with value: 3.7266565361985126e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 141, 'output_activation_class': 'ReLU', 'lr': 0.0006843240639630834, 'batch_size': 72, 'epochs': 45}. Best is trial 13 with value: 1.7142691947085824e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, Train Loss: 371919068337126528.0000\n",
      "Epoch 45, Test Loss: 372665653619851264.0000\n",
      "Epoch 1, Train Loss: 495722721820488512.0000\n",
      "Epoch 1, Test Loss: 498909826374434816.0000\n",
      "Epoch 2, Train Loss: 488579570760923072.0000\n",
      "Epoch 2, Test Loss: 479420226857861120.0000\n",
      "Epoch 3, Train Loss: 470992927809194432.0000\n",
      "Epoch 3, Test Loss: 469865986208563200.0000\n",
      "Epoch 4, Train Loss: 462139532369168256.0000\n",
      "Epoch 4, Test Loss: 459782811746828288.0000\n",
      "Epoch 5, Train Loss: 463673011560118784.0000\n",
      "Epoch 5, Test Loss: 447928220973531136.0000\n",
      "Epoch 6, Train Loss: 450265876172882880.0000\n",
      "Epoch 6, Test Loss: 432641745172299776.0000\n",
      "Epoch 7, Train Loss: 420630208828343040.0000\n",
      "Epoch 7, Test Loss: 412496149530804224.0000\n",
      "Epoch 8, Train Loss: 405785750180095616.0000\n",
      "Epoch 8, Test Loss: 389459559862763520.0000\n",
      "Epoch 9, Train Loss: 382834947198124416.0000\n",
      "Epoch 9, Test Loss: 384533816489803776.0000\n",
      "Epoch 10, Train Loss: 366981765195202944.0000\n",
      "Epoch 10, Test Loss: 350862509839745024.0000\n",
      "Epoch 11, Train Loss: 346655956271190336.0000\n",
      "Epoch 11, Test Loss: 339030733932724224.0000\n",
      "Epoch 12, Train Loss: 345466908745230720.0000\n",
      "Epoch 12, Test Loss: 323058540832882688.0000\n",
      "Epoch 13, Train Loss: 312285253299709888.0000\n",
      "Epoch 13, Test Loss: 308704107294621696.0000\n",
      "Epoch 14, Train Loss: 308083555804174528.0000\n",
      "Epoch 14, Test Loss: 281862142242586624.0000\n",
      "Epoch 15, Train Loss: 294084115469355840.0000\n",
      "Epoch 15, Test Loss: 268507010155151360.0000\n",
      "Epoch 16, Train Loss: 276590915147758464.0000\n",
      "Epoch 16, Test Loss: 255255352419811328.0000\n",
      "Epoch 17, Train Loss: 267777942926973152.0000\n",
      "Epoch 17, Test Loss: 252263701539717120.0000\n",
      "Epoch 18, Train Loss: 259720952905706400.0000\n",
      "Epoch 18, Test Loss: 252896796899016704.0000\n",
      "Epoch 19, Train Loss: 238201840357793728.0000\n",
      "Epoch 19, Test Loss: 221414239942737920.0000\n",
      "Epoch 20, Train Loss: 223649304452079904.0000\n",
      "Epoch 20, Test Loss: 260964205769785344.0000\n",
      "Epoch 21, Train Loss: 220385934886068320.0000\n",
      "Epoch 21, Test Loss: 198993273707036672.0000\n",
      "Epoch 22, Train Loss: 211495091558092544.0000\n",
      "Epoch 22, Test Loss: 192904831147442176.0000\n",
      "Epoch 23, Train Loss: 206764304306379168.0000\n",
      "Epoch 23, Test Loss: 185111698888196096.0000\n",
      "Epoch 24, Train Loss: 200060759225820768.0000\n",
      "Epoch 24, Test Loss: 171636874652155904.0000\n",
      "Epoch 25, Train Loss: 192817411060384704.0000\n",
      "Epoch 25, Test Loss: 163398130946015232.0000\n",
      "Epoch 26, Train Loss: 185776592689061984.0000\n",
      "Epoch 26, Test Loss: 176393327594176512.0000\n",
      "Epoch 27, Train Loss: 171610376564927584.0000\n",
      "Epoch 27, Test Loss: 186387561873145856.0000\n",
      "Epoch 28, Train Loss: 190686815011067360.0000\n",
      "Epoch 28, Test Loss: 146443043170418688.0000\n",
      "Epoch 29, Train Loss: 159845986410094304.0000\n",
      "Epoch 29, Test Loss: 142494997562785792.0000\n",
      "Epoch 30, Train Loss: 156361531376154176.0000\n",
      "Epoch 30, Test Loss: 133011117067730944.0000\n",
      "Epoch 31, Train Loss: 172418520951390592.0000\n",
      "Epoch 31, Test Loss: 147625705365045248.0000\n",
      "Epoch 32, Train Loss: 167392266100741408.0000\n",
      "Epoch 32, Test Loss: 128249974021423104.0000\n",
      "Epoch 33, Train Loss: 152309280708271264.0000\n",
      "Epoch 33, Test Loss: 219200356100341760.0000\n",
      "Epoch 34, Train Loss: 160181830440378080.0000\n",
      "Epoch 34, Test Loss: 166325563475099648.0000\n",
      "Epoch 35, Train Loss: 146573248530891328.0000\n",
      "Epoch 35, Test Loss: 115203426744270848.0000\n",
      "Epoch 36, Train Loss: 159197085502778272.0000\n",
      "Epoch 36, Test Loss: 117682267119157248.0000\n",
      "Epoch 37, Train Loss: 140034340243534448.0000\n",
      "Epoch 37, Test Loss: 127725077478244352.0000\n",
      "Epoch 38, Train Loss: 132778793966912064.0000\n",
      "Epoch 38, Test Loss: 104532193950302208.0000\n",
      "Epoch 39, Train Loss: 215932946141154304.0000\n",
      "Epoch 39, Test Loss: 172482003956924416.0000\n",
      "Epoch 40, Train Loss: 142234013173559344.0000\n",
      "Epoch 40, Test Loss: 159503506141609984.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:52:34,195]\u001b[0m Trial 26 finished with value: 1.2768021224987034e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 265, 'output_activation_class': 'ReLU', 'lr': 0.0025439854268630322, 'batch_size': 95, 'epochs': 41}. Best is trial 26 with value: 1.2768021224987034e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, Train Loss: 153061215714907040.0000\n",
      "Epoch 41, Test Loss: 127680212249870336.0000\n",
      "Epoch 1, Train Loss: 494675444659958272.0000\n",
      "Epoch 1, Test Loss: 479627450439958528.0000\n",
      "Epoch 2, Train Loss: 475769897424017664.0000\n",
      "Epoch 2, Test Loss: 468124462869381120.0000\n",
      "Epoch 3, Train Loss: 460480740252139264.0000\n",
      "Epoch 3, Test Loss: 448023431808548864.0000\n",
      "Epoch 4, Train Loss: 437234389516648896.0000\n",
      "Epoch 4, Test Loss: 413721452160745472.0000\n",
      "Epoch 5, Train Loss: 397994010767850560.0000\n",
      "Epoch 5, Test Loss: 371886821430263808.0000\n",
      "Epoch 6, Train Loss: 367437684966284032.0000\n",
      "Epoch 6, Test Loss: 385206408368357376.0000\n",
      "Epoch 7, Train Loss: 347933923857487168.0000\n",
      "Epoch 7, Test Loss: 332141022274387968.0000\n",
      "Epoch 8, Train Loss: 336464477856073600.0000\n",
      "Epoch 8, Test Loss: 300241990929350656.0000\n",
      "Epoch 9, Train Loss: 303988559461136768.0000\n",
      "Epoch 9, Test Loss: 270825038364540928.0000\n",
      "Epoch 10, Train Loss: 290709769310395584.0000\n",
      "Epoch 10, Test Loss: 261114048588808192.0000\n",
      "Epoch 11, Train Loss: 273942445216945984.0000\n",
      "Epoch 11, Test Loss: 248854906255966208.0000\n",
      "Epoch 12, Train Loss: 462572783764048704.0000\n",
      "Epoch 12, Test Loss: 438602094627258368.0000\n",
      "Epoch 13, Train Loss: 405059193453172992.0000\n",
      "Epoch 13, Test Loss: 383586655941951488.0000\n",
      "Epoch 14, Train Loss: 366830999132328320.0000\n",
      "Epoch 14, Test Loss: 350569661789634560.0000\n",
      "Epoch 15, Train Loss: 343499691078952704.0000\n",
      "Epoch 15, Test Loss: 348046076445458432.0000\n",
      "Epoch 16, Train Loss: 321274433031068864.0000\n",
      "Epoch 16, Test Loss: 314885561665978368.0000\n",
      "Epoch 17, Train Loss: 301578595465941888.0000\n",
      "Epoch 17, Test Loss: 301611260863053824.0000\n",
      "Epoch 18, Train Loss: 300203147940529088.0000\n",
      "Epoch 18, Test Loss: 309923396970348544.0000\n",
      "Epoch 19, Train Loss: 295312403377813632.0000\n",
      "Epoch 19, Test Loss: 265086876157739008.0000\n",
      "Epoch 20, Train Loss: 285395815470586592.0000\n",
      "Epoch 20, Test Loss: 261620768830390272.0000\n",
      "Epoch 21, Train Loss: 275577212604898016.0000\n",
      "Epoch 21, Test Loss: 248126084665573376.0000\n",
      "Epoch 22, Train Loss: 267268997409817280.0000\n",
      "Epoch 22, Test Loss: 245457930722213888.0000\n",
      "Epoch 23, Train Loss: 272600621658042176.0000\n",
      "Epoch 23, Test Loss: 309615636793786368.0000\n",
      "Epoch 24, Train Loss: 272824734891691968.0000\n",
      "Epoch 24, Test Loss: 319270860714147840.0000\n",
      "Epoch 25, Train Loss: 242440047835282496.0000\n",
      "Epoch 25, Test Loss: 227192190726569984.0000\n",
      "Epoch 26, Train Loss: 277789284360132160.0000\n",
      "Epoch 26, Test Loss: 229263241136570368.0000\n",
      "Epoch 27, Train Loss: 256974607999189696.0000\n",
      "Epoch 27, Test Loss: 229592046652882944.0000\n",
      "Epoch 28, Train Loss: 242963314893545600.0000\n",
      "Epoch 28, Test Loss: 212492184119672832.0000\n",
      "Epoch 29, Train Loss: 249563051780427232.0000\n",
      "Epoch 29, Test Loss: 210549123735093248.0000\n",
      "Epoch 30, Train Loss: 229949218366290080.0000\n",
      "Epoch 30, Test Loss: 257138472240807936.0000\n",
      "Epoch 31, Train Loss: 215272671403744224.0000\n",
      "Epoch 31, Test Loss: 206167312200368128.0000\n",
      "Epoch 32, Train Loss: 223093662124145728.0000\n",
      "Epoch 32, Test Loss: 212258555078639616.0000\n",
      "Epoch 33, Train Loss: 238713405901039104.0000\n",
      "Epoch 33, Test Loss: 195172333361561600.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:53:01,615]\u001b[0m Trial 27 finished with value: 1.8570629416065434e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 423, 'output_activation_class': 'LeakyReLU', 'lr': 0.002619678272326394, 'batch_size': 58, 'epochs': 34}. Best is trial 26 with value: 1.2768021224987034e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Train Loss: 207810270451785376.0000\n",
      "Epoch 34, Test Loss: 185706294160654336.0000\n",
      "Epoch 1, Train Loss: 500503830059991616.0000\n",
      "Epoch 1, Test Loss: 485625183290261504.0000\n",
      "Epoch 2, Train Loss: 494061918287117760.0000\n",
      "Epoch 2, Test Loss: 476516588447596544.0000\n",
      "Epoch 3, Train Loss: 467050610153768576.0000\n",
      "Epoch 3, Test Loss: 463409344692617216.0000\n",
      "Epoch 4, Train Loss: 454609494129408896.0000\n",
      "Epoch 4, Test Loss: 441743193189384192.0000\n",
      "Epoch 5, Train Loss: 431034105636666816.0000\n",
      "Epoch 5, Test Loss: 427343679674384384.0000\n",
      "Epoch 6, Train Loss: 398021756186019456.0000\n",
      "Epoch 6, Test Loss: 382741715615744000.0000\n",
      "Epoch 7, Train Loss: 372994117496507712.0000\n",
      "Epoch 7, Test Loss: 431816080659316736.0000\n",
      "Epoch 8, Train Loss: 366324699172365696.0000\n",
      "Epoch 8, Test Loss: 337130193724375040.0000\n",
      "Epoch 9, Train Loss: 322096413175592192.0000\n",
      "Epoch 9, Test Loss: 308790934353477632.0000\n",
      "Epoch 10, Train Loss: 335921973771469952.0000\n",
      "Epoch 10, Test Loss: 488596922701971456.0000\n",
      "Epoch 11, Train Loss: 507641289627800000.0000\n",
      "Epoch 11, Test Loss: 500558647139500032.0000\n",
      "Epoch 12, Train Loss: 497141243186213568.0000\n",
      "Epoch 12, Test Loss: 500558647139500032.0000\n",
      "Epoch 13, Train Loss: 496715178960459264.0000\n",
      "Epoch 13, Test Loss: 500558647139500032.0000\n",
      "Epoch 14, Train Loss: 499923047751786240.0000\n",
      "Epoch 14, Test Loss: 500558647139500032.0000\n",
      "Epoch 15, Train Loss: 500824774199837824.0000\n",
      "Epoch 15, Test Loss: 500558647139500032.0000\n",
      "Epoch 16, Train Loss: 498020922559344768.0000\n",
      "Epoch 16, Test Loss: 500558647139500032.0000\n",
      "Epoch 17, Train Loss: 498815974006216768.0000\n",
      "Epoch 17, Test Loss: 500558647139500032.0000\n",
      "Epoch 18, Train Loss: 497659645849917504.0000\n",
      "Epoch 18, Test Loss: 500558647139500032.0000\n",
      "Epoch 19, Train Loss: 496849148570790400.0000\n",
      "Epoch 19, Test Loss: 500558647139500032.0000\n",
      "Epoch 20, Train Loss: 497113272570383680.0000\n",
      "Epoch 20, Test Loss: 500558647139500032.0000\n",
      "Epoch 21, Train Loss: 497375998331340416.0000\n",
      "Epoch 21, Test Loss: 500558647139500032.0000\n",
      "Epoch 22, Train Loss: 497100219269214912.0000\n",
      "Epoch 22, Test Loss: 500558647139500032.0000\n",
      "Epoch 23, Train Loss: 496914348795639040.0000\n",
      "Epoch 23, Test Loss: 500558647139500032.0000\n",
      "Epoch 24, Train Loss: 507979624439656576.0000\n",
      "Epoch 24, Test Loss: 500558647139500032.0000\n",
      "Epoch 25, Train Loss: 497935927540078656.0000\n",
      "Epoch 25, Test Loss: 500558647139500032.0000\n",
      "Epoch 26, Train Loss: 498239699256353600.0000\n",
      "Epoch 26, Test Loss: 500558647139500032.0000\n",
      "Epoch 27, Train Loss: 497561416962670592.0000\n",
      "Epoch 27, Test Loss: 500558647139500032.0000\n",
      "Epoch 28, Train Loss: 496927618662318656.0000\n",
      "Epoch 28, Test Loss: 500558647139500032.0000\n",
      "Epoch 29, Train Loss: 498396603722452864.0000\n",
      "Epoch 29, Test Loss: 500558647139500032.0000\n",
      "Epoch 30, Train Loss: 496791149988472192.0000\n",
      "Epoch 30, Test Loss: 500558647139500032.0000\n",
      "Epoch 31, Train Loss: 496713101756858368.0000\n",
      "Epoch 31, Test Loss: 500558647139500032.0000\n",
      "Epoch 32, Train Loss: 496971161459617344.0000\n",
      "Epoch 32, Test Loss: 500558647139500032.0000\n",
      "Epoch 33, Train Loss: 499151821132403136.0000\n",
      "Epoch 33, Test Loss: 500558647139500032.0000\n",
      "Epoch 34, Train Loss: 496742982872085760.0000\n",
      "Epoch 34, Test Loss: 500558647139500032.0000\n",
      "Epoch 35, Train Loss: 520858441720845248.0000\n",
      "Epoch 35, Test Loss: 500558647139500032.0000\n",
      "Epoch 36, Train Loss: 500402563948207296.0000\n",
      "Epoch 36, Test Loss: 500558647139500032.0000\n",
      "Epoch 37, Train Loss: 510559850379526720.0000\n",
      "Epoch 37, Test Loss: 500558647139500032.0000\n",
      "Epoch 38, Train Loss: 496936434788493824.0000\n",
      "Epoch 38, Test Loss: 500558647139500032.0000\n",
      "Epoch 39, Train Loss: 499295243108379136.0000\n",
      "Epoch 39, Test Loss: 500558647139500032.0000\n",
      "Epoch 40, Train Loss: 498080789756740736.0000\n",
      "Epoch 40, Test Loss: 500558647139500032.0000\n",
      "Epoch 41, Train Loss: 496845323771666688.0000\n",
      "Epoch 41, Test Loss: 500558647139500032.0000\n",
      "Epoch 42, Train Loss: 499907841695452480.0000\n",
      "Epoch 42, Test Loss: 500558647139500032.0000\n",
      "Epoch 43, Train Loss: 496812179975334400.0000\n",
      "Epoch 43, Test Loss: 500558647139500032.0000\n",
      "Epoch 44, Train Loss: 500512712279141440.0000\n",
      "Epoch 44, Test Loss: 500558647139500032.0000\n",
      "Epoch 45, Train Loss: 496749053653314048.0000\n",
      "Epoch 45, Test Loss: 500558647139500032.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:53:45,874]\u001b[0m Trial 28 finished with value: 5.005586471395e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 628, 'output_activation_class': 'ELU', 'lr': 0.003376042653046873, 'batch_size': 139, 'epochs': 46}. Best is trial 26 with value: 1.2768021224987034e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, Train Loss: 499139158981611968.0000\n",
      "Epoch 46, Test Loss: 500558647139500032.0000\n",
      "Epoch 1, Train Loss: 495913808473638976.0000\n",
      "Epoch 1, Test Loss: 500558647139500032.0000\n",
      "Epoch 2, Train Loss: 496401691318481152.0000\n",
      "Epoch 2, Test Loss: 500558647139500032.0000\n",
      "Epoch 3, Train Loss: 495909484296771968.0000\n",
      "Epoch 3, Test Loss: 500558647139500032.0000\n",
      "Epoch 4, Train Loss: 496014587258467328.0000\n",
      "Epoch 4, Test Loss: 500558647139500032.0000\n",
      "Epoch 5, Train Loss: 495915178113247808.0000\n",
      "Epoch 5, Test Loss: 500558647139500032.0000\n",
      "Epoch 6, Train Loss: 495909492059583680.0000\n",
      "Epoch 6, Test Loss: 500558647139500032.0000\n",
      "Epoch 7, Train Loss: 495909457598469760.0000\n",
      "Epoch 7, Test Loss: 500558647139500032.0000\n",
      "Epoch 8, Train Loss: 496746814534295424.0000\n",
      "Epoch 8, Test Loss: 500558647139500032.0000\n",
      "Epoch 9, Train Loss: 495913002354266816.0000\n",
      "Epoch 9, Test Loss: 500558647139500032.0000\n",
      "Epoch 10, Train Loss: 496880593750082112.0000\n",
      "Epoch 10, Test Loss: 500558647139500032.0000\n",
      "Epoch 11, Train Loss: 495992217854316672.0000\n",
      "Epoch 11, Test Loss: 500558647139500032.0000\n",
      "Epoch 12, Train Loss: 495912707302251072.0000\n",
      "Epoch 12, Test Loss: 500558647139500032.0000\n",
      "Epoch 13, Train Loss: 495909458121883264.0000\n",
      "Epoch 13, Test Loss: 500558647139500032.0000\n",
      "Epoch 14, Train Loss: 503720504809537728.0000\n",
      "Epoch 14, Test Loss: 500558647139500032.0000\n",
      "Epoch 15, Train Loss: 495912498855408640.0000\n",
      "Epoch 15, Test Loss: 500558647139500032.0000\n",
      "Epoch 16, Train Loss: 498328119624585152.0000\n",
      "Epoch 16, Test Loss: 500558647139500032.0000\n",
      "Epoch 17, Train Loss: 499303836799495296.0000\n",
      "Epoch 17, Test Loss: 500558647139500032.0000\n",
      "Epoch 18, Train Loss: 495941316535750016.0000\n",
      "Epoch 18, Test Loss: 500558647139500032.0000\n",
      "Epoch 19, Train Loss: 495920021360949568.0000\n",
      "Epoch 19, Test Loss: 500558647139500032.0000\n",
      "Epoch 20, Train Loss: 495910272635792512.0000\n",
      "Epoch 20, Test Loss: 500558647139500032.0000\n",
      "Epoch 21, Train Loss: 495918511934761344.0000\n",
      "Epoch 21, Test Loss: 500558647139500032.0000\n",
      "Epoch 22, Train Loss: 495912560319627648.0000\n",
      "Epoch 22, Test Loss: 500558647139500032.0000\n",
      "Epoch 23, Train Loss: 497482277770530944.0000\n",
      "Epoch 23, Test Loss: 500558647139500032.0000\n",
      "Epoch 24, Train Loss: 496774003868975680.0000\n",
      "Epoch 24, Test Loss: 500558647139500032.0000\n",
      "Epoch 25, Train Loss: 637856938475148416.0000\n",
      "Epoch 25, Test Loss: 500558647139500032.0000\n",
      "Epoch 26, Train Loss: 496218448136777792.0000\n",
      "Epoch 26, Test Loss: 500558647139500032.0000\n",
      "Epoch 27, Train Loss: 496468872770586496.0000\n",
      "Epoch 27, Test Loss: 500558647139500032.0000\n",
      "Epoch 28, Train Loss: 495922480911029504.0000\n",
      "Epoch 28, Test Loss: 500558647139500032.0000\n",
      "Epoch 29, Train Loss: 498597134843535488.0000\n",
      "Epoch 29, Test Loss: 500558647139500032.0000\n",
      "Epoch 30, Train Loss: 495927587228882944.0000\n",
      "Epoch 30, Test Loss: 500558647139500032.0000\n",
      "Epoch 31, Train Loss: 495912111075325312.0000\n",
      "Epoch 31, Test Loss: 500558647139500032.0000\n",
      "Epoch 32, Train Loss: 495911532392827712.0000\n",
      "Epoch 32, Test Loss: 500558647139500032.0000\n",
      "Epoch 33, Train Loss: 495909459818880640.0000\n",
      "Epoch 33, Test Loss: 500558647139500032.0000\n",
      "Epoch 34, Train Loss: 495939006316264896.0000\n",
      "Epoch 34, Test Loss: 500558647139500032.0000\n",
      "Epoch 35, Train Loss: 622522802772892672.0000\n",
      "Epoch 35, Test Loss: 500558647139500032.0000\n",
      "Epoch 36, Train Loss: 506523770980373376.0000\n",
      "Epoch 36, Test Loss: 500558647139500032.0000\n",
      "Epoch 37, Train Loss: 495920549592779584.0000\n",
      "Epoch 37, Test Loss: 500558647139500032.0000\n",
      "Epoch 38, Train Loss: 495919928173580992.0000\n",
      "Epoch 38, Test Loss: 500558647139500032.0000\n",
      "Epoch 39, Train Loss: 495909457982958400.0000\n",
      "Epoch 39, Test Loss: 500558647139500032.0000\n",
      "Epoch 40, Train Loss: 502993117779177920.0000\n",
      "Epoch 40, Test Loss: 500558647139500032.0000\n",
      "Epoch 41, Train Loss: 505946739901618752.0000\n",
      "Epoch 41, Test Loss: 500558647139500032.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:53:58,813]\u001b[0m Trial 29 finished with value: 5.005586471395e+17 and parameters: {'activation_class': 'Sigmoid', 'hidden_sizes': 280, 'output_activation_class': 'Tanh', 'lr': 0.002041514701410021, 'batch_size': 95, 'epochs': 42}. Best is trial 26 with value: 1.2768021224987034e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, Train Loss: 495918074982631680.0000\n",
      "Epoch 42, Test Loss: 500558647139500032.0000\n",
      "Epoch 1, Train Loss: 499205408399695360.0000\n",
      "Epoch 1, Test Loss: 500558647139500032.0000\n",
      "Epoch 2, Train Loss: 499070266070439552.0000\n",
      "Epoch 2, Test Loss: 500558647139500032.0000\n",
      "Epoch 3, Train Loss: 499878411005714944.0000\n",
      "Epoch 3, Test Loss: 500558578420023296.0000\n",
      "Epoch 4, Train Loss: 499260402327710080.0000\n",
      "Epoch 4, Test Loss: 500558509700546560.0000\n",
      "Epoch 5, Train Loss: 500240503904172160.0000\n",
      "Epoch 5, Test Loss: 500558509700546560.0000\n",
      "Epoch 6, Train Loss: 499698849161967744.0000\n",
      "Epoch 6, Test Loss: 500558509700546560.0000\n",
      "Epoch 7, Train Loss: 499092861842065280.0000\n",
      "Epoch 7, Test Loss: 500558509700546560.0000\n",
      "Epoch 8, Train Loss: 499470131974126656.0000\n",
      "Epoch 8, Test Loss: 500558509700546560.0000\n",
      "Epoch 9, Train Loss: 499135174052367168.0000\n",
      "Epoch 9, Test Loss: 500558509700546560.0000\n",
      "Epoch 10, Train Loss: 500241128850286976.0000\n",
      "Epoch 10, Test Loss: 500558440981069824.0000\n",
      "Epoch 11, Train Loss: 499151629873161664.0000\n",
      "Epoch 11, Test Loss: 500558440981069824.0000\n",
      "Epoch 12, Train Loss: 499975686862390208.0000\n",
      "Epoch 12, Test Loss: 500558440981069824.0000\n",
      "Epoch 13, Train Loss: 499546023486677504.0000\n",
      "Epoch 13, Test Loss: 500558440981069824.0000\n",
      "Epoch 14, Train Loss: 502947907796976576.0000\n",
      "Epoch 14, Test Loss: 500558440981069824.0000\n",
      "Epoch 15, Train Loss: 499473791690889792.0000\n",
      "Epoch 15, Test Loss: 500558406621331456.0000\n",
      "Epoch 16, Train Loss: 499079345084810176.0000\n",
      "Epoch 16, Test Loss: 500558406621331456.0000\n",
      "Epoch 17, Train Loss: 499415372868169024.0000\n",
      "Epoch 17, Test Loss: 500558406621331456.0000\n",
      "Epoch 18, Train Loss: 499077091629604352.0000\n",
      "Epoch 18, Test Loss: 500558406621331456.0000\n",
      "Epoch 19, Train Loss: 499280372548177664.0000\n",
      "Epoch 19, Test Loss: 500558372261593088.0000\n",
      "Epoch 20, Train Loss: 499615224523668544.0000\n",
      "Epoch 20, Test Loss: 500558337901854720.0000\n",
      "Epoch 21, Train Loss: 502372244320435264.0000\n",
      "Epoch 21, Test Loss: 500558337901854720.0000\n",
      "Epoch 22, Train Loss: 499094173034294592.0000\n",
      "Epoch 22, Test Loss: 500558337901854720.0000\n",
      "Epoch 23, Train Loss: 499509245244033856.0000\n",
      "Epoch 23, Test Loss: 500558303542116352.0000\n",
      "Epoch 24, Train Loss: 500001662593662976.0000\n",
      "Epoch 24, Test Loss: 500558303542116352.0000\n",
      "Epoch 25, Train Loss: 499415381603917312.0000\n",
      "Epoch 25, Test Loss: 500558303542116352.0000\n",
      "Epoch 26, Train Loss: 501827750674917760.0000\n",
      "Epoch 26, Test Loss: 500558303542116352.0000\n",
      "Epoch 27, Train Loss: 504220959586185472.0000\n",
      "Epoch 27, Test Loss: 500558303542116352.0000\n",
      "Epoch 28, Train Loss: 499157625354399040.0000\n",
      "Epoch 28, Test Loss: 500558269182377984.0000\n",
      "Epoch 29, Train Loss: 499088719364365824.0000\n",
      "Epoch 29, Test Loss: 500558269182377984.0000\n",
      "Epoch 30, Train Loss: 499243604815473024.0000\n",
      "Epoch 30, Test Loss: 500558269182377984.0000\n",
      "Epoch 31, Train Loss: 502837667749783936.0000\n",
      "Epoch 31, Test Loss: 500558269182377984.0000\n",
      "Epoch 32, Train Loss: 499118986069463744.0000\n",
      "Epoch 32, Test Loss: 500558269182377984.0000\n",
      "Epoch 33, Train Loss: 510021302770698112.0000\n",
      "Epoch 33, Test Loss: 500558269182377984.0000\n",
      "Epoch 34, Train Loss: 499210458593863104.0000\n",
      "Epoch 34, Test Loss: 500558200462901248.0000\n",
      "Epoch 35, Train Loss: 505936141969399296.0000\n",
      "Epoch 35, Test Loss: 500558200462901248.0000\n",
      "Epoch 36, Train Loss: 505822573834257344.0000\n",
      "Epoch 36, Test Loss: 500558200462901248.0000\n",
      "Epoch 37, Train Loss: 502372052916579648.0000\n",
      "Epoch 37, Test Loss: 500558131743424512.0000\n",
      "Epoch 38, Train Loss: 499119532616388352.0000\n",
      "Epoch 38, Test Loss: 500558131743424512.0000\n",
      "Epoch 39, Train Loss: 500141748218094080.0000\n",
      "Epoch 39, Test Loss: 500558131743424512.0000\n",
      "Epoch 40, Train Loss: 509170994855989952.0000\n",
      "Epoch 40, Test Loss: 500558131743424512.0000\n",
      "Epoch 41, Train Loss: 499887480079831744.0000\n",
      "Epoch 41, Test Loss: 500558131743424512.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:54:06,926]\u001b[0m Trial 30 finished with value: 5.0055809738368614e+17 and parameters: {'activation_class': 'Tanh', 'hidden_sizes': 131, 'output_activation_class': 'ReLU', 'lr': 0.009930659759845162, 'batch_size': 118, 'epochs': 43}. Best is trial 26 with value: 1.2768021224987034e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, Train Loss: 507117070343363136.0000\n",
      "Epoch 42, Test Loss: 500558131743424512.0000\n",
      "Epoch 43, Train Loss: 499292078618721344.0000\n",
      "Epoch 43, Test Loss: 500558097383686144.0000\n",
      "Epoch 1, Train Loss: 509032283138561152.0000\n",
      "Epoch 1, Test Loss: 500512124053749760.0000\n",
      "Epoch 2, Train Loss: 499030812752378624.0000\n",
      "Epoch 2, Test Loss: 498313684913750016.0000\n",
      "Epoch 3, Train Loss: 491040190358300032.0000\n",
      "Epoch 3, Test Loss: 486029150734254080.0000\n",
      "Epoch 4, Train Loss: 482392943890027008.0000\n",
      "Epoch 4, Test Loss: 480069282315632640.0000\n",
      "Epoch 5, Train Loss: 477473940451762752.0000\n",
      "Epoch 5, Test Loss: 476988828691726336.0000\n",
      "Epoch 6, Train Loss: 474621422173910912.0000\n",
      "Epoch 6, Test Loss: 473327008294633472.0000\n",
      "Epoch 7, Train Loss: 470640051084667968.0000\n",
      "Epoch 7, Test Loss: 470083964388769792.0000\n",
      "Epoch 8, Train Loss: 467417890641237312.0000\n",
      "Epoch 8, Test Loss: 466006288078209024.0000\n",
      "Epoch 9, Train Loss: 462096084811809216.0000\n",
      "Epoch 9, Test Loss: 462221665976188928.0000\n",
      "Epoch 10, Train Loss: 457736707523358656.0000\n",
      "Epoch 10, Test Loss: 455992211050332160.0000\n",
      "Epoch 11, Train Loss: 451225497402808320.0000\n",
      "Epoch 11, Test Loss: 456731289022627840.0000\n",
      "Epoch 12, Train Loss: 446895330272837184.0000\n",
      "Epoch 12, Test Loss: 442288344798330880.0000\n",
      "Epoch 13, Train Loss: 440089831885109568.0000\n",
      "Epoch 13, Test Loss: 434477860871208960.0000\n",
      "Epoch 14, Train Loss: 437238460793022464.0000\n",
      "Epoch 14, Test Loss: 426305397100380160.0000\n",
      "Epoch 15, Train Loss: 423813812125870784.0000\n",
      "Epoch 15, Test Loss: 416062174977327104.0000\n",
      "Epoch 16, Train Loss: 410705702419585216.0000\n",
      "Epoch 16, Test Loss: 404874987762089984.0000\n",
      "Epoch 17, Train Loss: 401485361976806400.0000\n",
      "Epoch 17, Test Loss: 398036953150259200.0000\n",
      "Epoch 18, Train Loss: 395225981486925952.0000\n",
      "Epoch 18, Test Loss: 409079932543565824.0000\n",
      "Epoch 19, Train Loss: 381773599127885568.0000\n",
      "Epoch 19, Test Loss: 372646343446888448.0000\n",
      "Epoch 20, Train Loss: 371651437368956672.0000\n",
      "Epoch 20, Test Loss: 362915974778716160.0000\n",
      "Epoch 21, Train Loss: 360714952387686592.0000\n",
      "Epoch 21, Test Loss: 355991765943058432.0000\n",
      "Epoch 22, Train Loss: 350045371724471104.0000\n",
      "Epoch 22, Test Loss: 342100158080614400.0000\n",
      "Epoch 23, Train Loss: 350019821481212864.0000\n",
      "Epoch 23, Test Loss: 333368798805491712.0000\n",
      "Epoch 24, Train Loss: 336860134147966912.0000\n",
      "Epoch 24, Test Loss: 336584664158306304.0000\n",
      "Epoch 25, Train Loss: 326025921114959168.0000\n",
      "Epoch 25, Test Loss: 317063350603218944.0000\n",
      "Epoch 26, Train Loss: 319053696580567680.0000\n",
      "Epoch 26, Test Loss: 313769145046925312.0000\n",
      "Epoch 27, Train Loss: 310233737778010496.0000\n",
      "Epoch 27, Test Loss: 305775970390900736.0000\n",
      "Epoch 28, Train Loss: 311695786414125888.0000\n",
      "Epoch 28, Test Loss: 294182479269462016.0000\n",
      "Epoch 29, Train Loss: 294878411043621248.0000\n",
      "Epoch 29, Test Loss: 296203038043930624.0000\n",
      "Epoch 30, Train Loss: 292735671875984000.0000\n",
      "Epoch 30, Test Loss: 282142981564137472.0000\n",
      "Epoch 31, Train Loss: 289051219163041408.0000\n",
      "Epoch 31, Test Loss: 278216488102395904.0000\n",
      "Epoch 32, Train Loss: 273214755143662528.0000\n",
      "Epoch 32, Test Loss: 265312172962217984.0000\n",
      "Epoch 33, Train Loss: 264412088688324544.0000\n",
      "Epoch 33, Test Loss: 273581977871843328.0000\n",
      "Epoch 34, Train Loss: 266938126041856960.0000\n",
      "Epoch 34, Test Loss: 258658925023330304.0000\n",
      "Epoch 35, Train Loss: 252348574796475264.0000\n",
      "Epoch 35, Test Loss: 258622160103276544.0000\n",
      "Epoch 36, Train Loss: 247743236864265408.0000\n",
      "Epoch 36, Test Loss: 246578281531310080.0000\n",
      "Epoch 37, Train Loss: 244648834099262272.0000\n",
      "Epoch 37, Test Loss: 269652357673910272.0000\n",
      "Epoch 38, Train Loss: 242182194291228896.0000\n",
      "Epoch 38, Test Loss: 227856982944514048.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:54:18,564]\u001b[0m Trial 31 finished with value: 2.2341028304388096e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 204, 'output_activation_class': 'ReLU', 'lr': 0.0014887939151863566, 'batch_size': 107, 'epochs': 39}. Best is trial 26 with value: 1.2768021224987034e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Train Loss: 242772518796742272.0000\n",
      "Epoch 39, Test Loss: 223410283043880960.0000\n",
      "Epoch 1, Train Loss: 496809892054528704.0000\n",
      "Epoch 1, Test Loss: 500454846369890304.0000\n",
      "Epoch 2, Train Loss: 504516228148426560.0000\n",
      "Epoch 2, Test Loss: 494401588182384640.0000\n",
      "Epoch 3, Train Loss: 482878181379245184.0000\n",
      "Epoch 3, Test Loss: 480790871181099008.0000\n",
      "Epoch 4, Train Loss: 475614928746252480.0000\n",
      "Epoch 4, Test Loss: 476992745701900288.0000\n",
      "Epoch 5, Train Loss: 472028830159149696.0000\n",
      "Epoch 5, Test Loss: 472559858416091136.0000\n",
      "Epoch 6, Train Loss: 475193729560238144.0000\n",
      "Epoch 6, Test Loss: 467961597709516800.0000\n",
      "Epoch 7, Train Loss: 461943976976124096.0000\n",
      "Epoch 7, Test Loss: 461832610658648064.0000\n",
      "Epoch 8, Train Loss: 459820572969705216.0000\n",
      "Epoch 8, Test Loss: 454826144609337344.0000\n",
      "Epoch 9, Train Loss: 447995270855781952.0000\n",
      "Epoch 9, Test Loss: 445698170874232832.0000\n",
      "Epoch 10, Train Loss: 438564117859966720.0000\n",
      "Epoch 10, Test Loss: 434445528357404672.0000\n",
      "Epoch 11, Train Loss: 430098850839979392.0000\n",
      "Epoch 11, Test Loss: 427442429562454016.0000\n",
      "Epoch 12, Train Loss: 415011092590846208.0000\n",
      "Epoch 12, Test Loss: 411440137331802112.0000\n",
      "Epoch 13, Train Loss: 400538783689031104.0000\n",
      "Epoch 13, Test Loss: 393215113626124288.0000\n",
      "Epoch 14, Train Loss: 387334266126272704.0000\n",
      "Epoch 14, Test Loss: 378468257515962368.0000\n",
      "Epoch 15, Train Loss: 381485021643732800.0000\n",
      "Epoch 15, Test Loss: 364158869594701824.0000\n",
      "Epoch 16, Train Loss: 366470898132803840.0000\n",
      "Epoch 16, Test Loss: 381913680280813568.0000\n",
      "Epoch 17, Train Loss: 363431077445026368.0000\n",
      "Epoch 17, Test Loss: 344021760808583168.0000\n",
      "Epoch 18, Train Loss: 342042404562511808.0000\n",
      "Epoch 18, Test Loss: 331110573720731648.0000\n",
      "Epoch 19, Train Loss: 325541441793886400.0000\n",
      "Epoch 19, Test Loss: 326368551868825600.0000\n",
      "Epoch 20, Train Loss: 314079039007670848.0000\n",
      "Epoch 20, Test Loss: 317887365848760320.0000\n",
      "Epoch 21, Train Loss: 303174660069541504.0000\n",
      "Epoch 21, Test Loss: 294876855222140928.0000\n",
      "Epoch 22, Train Loss: 316248515822503552.0000\n",
      "Epoch 22, Test Loss: 307519967631245312.0000\n",
      "Epoch 23, Train Loss: 296510932063336832.0000\n",
      "Epoch 23, Test Loss: 278917340865757184.0000\n",
      "Epoch 24, Train Loss: 282227387686923296.0000\n",
      "Epoch 24, Test Loss: 279951156673773568.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:54:26,146]\u001b[0m Trial 32 finished with value: 2.902650224189112e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 289, 'output_activation_class': 'ReLU', 'lr': 0.001955969301294596, 'batch_size': 139, 'epochs': 25}. Best is trial 26 with value: 1.2768021224987034e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Train Loss: 271746896959219648.0000\n",
      "Epoch 25, Test Loss: 290265022418911232.0000\n",
      "Epoch 1, Train Loss: 495355449728444736.0000\n",
      "Epoch 1, Test Loss: 500546002755780608.0000\n",
      "Epoch 2, Train Loss: 495194482007672192.0000\n",
      "Epoch 2, Test Loss: 499954740377944064.0000\n",
      "Epoch 3, Train Loss: 493157998332388224.0000\n",
      "Epoch 3, Test Loss: 495596207565963264.0000\n",
      "Epoch 4, Train Loss: 486154902208227520.0000\n",
      "Epoch 4, Test Loss: 485583023891283968.0000\n",
      "Epoch 5, Train Loss: 477424634179990464.0000\n",
      "Epoch 5, Test Loss: 479689126170329088.0000\n",
      "Epoch 6, Train Loss: 473669347778343872.0000\n",
      "Epoch 6, Test Loss: 476646193380720640.0000\n",
      "Epoch 7, Train Loss: 470646532347114432.0000\n",
      "Epoch 7, Test Loss: 473972387260399616.0000\n",
      "Epoch 8, Train Loss: 467952079776419200.0000\n",
      "Epoch 8, Test Loss: 471035007587057664.0000\n",
      "Epoch 9, Train Loss: 465127066259713280.0000\n",
      "Epoch 9, Test Loss: 468130613262548992.0000\n",
      "Epoch 10, Train Loss: 462105354893519424.0000\n",
      "Epoch 10, Test Loss: 465027413491843072.0000\n",
      "Epoch 11, Train Loss: 459475921449639744.0000\n",
      "Epoch 11, Test Loss: 461909095436255232.0000\n",
      "Epoch 12, Train Loss: 455926988329501696.0000\n",
      "Epoch 12, Test Loss: 458494458996981760.0000\n",
      "Epoch 13, Train Loss: 452886276390663808.0000\n",
      "Epoch 13, Test Loss: 454939978422550528.0000\n",
      "Epoch 14, Train Loss: 448581380229718528.0000\n",
      "Epoch 14, Test Loss: 450831206548766720.0000\n",
      "Epoch 15, Train Loss: 444555779435252352.0000\n",
      "Epoch 15, Test Loss: 446154639998451712.0000\n",
      "Epoch 16, Train Loss: 441372634159926336.0000\n",
      "Epoch 16, Test Loss: 442082942282366976.0000\n",
      "Epoch 17, Train Loss: 437260268203822144.0000\n",
      "Epoch 17, Test Loss: 438629548058214400.0000\n",
      "Epoch 18, Train Loss: 430694905593451648.0000\n",
      "Epoch 18, Test Loss: 431143316982071296.0000\n",
      "Epoch 19, Train Loss: 424993473215906944.0000\n",
      "Epoch 19, Test Loss: 426491970479718400.0000\n",
      "Epoch 20, Train Loss: 420348175718079040.0000\n",
      "Epoch 20, Test Loss: 418901114000244736.0000\n",
      "Epoch 21, Train Loss: 412799698663274432.0000\n",
      "Epoch 21, Test Loss: 412540714111467520.0000\n",
      "Epoch 22, Train Loss: 406678948114596032.0000\n",
      "Epoch 22, Test Loss: 405430206774378496.0000\n",
      "Epoch 23, Train Loss: 397995845070177984.0000\n",
      "Epoch 23, Test Loss: 398788881664704512.0000\n",
      "Epoch 24, Train Loss: 391562374245983808.0000\n",
      "Epoch 24, Test Loss: 389906305181024256.0000\n",
      "Epoch 25, Train Loss: 383386511334109952.0000\n",
      "Epoch 25, Test Loss: 381854100494483456.0000\n",
      "Epoch 26, Train Loss: 379125360023317376.0000\n",
      "Epoch 26, Test Loss: 375980715537334272.0000\n",
      "Epoch 27, Train Loss: 370846145863018624.0000\n",
      "Epoch 27, Test Loss: 367286086822526976.0000\n",
      "Epoch 28, Train Loss: 363123531260758848.0000\n",
      "Epoch 28, Test Loss: 363239025038852096.0000\n",
      "Epoch 29, Train Loss: 357533803092293952.0000\n",
      "Epoch 29, Test Loss: 359566346964434944.0000\n",
      "Epoch 30, Train Loss: 350485514197717888.0000\n",
      "Epoch 30, Test Loss: 352857367529914368.0000\n",
      "Epoch 31, Train Loss: 345312878083122752.0000\n",
      "Epoch 31, Test Loss: 342365415260815360.0000\n",
      "Epoch 32, Train Loss: 337335509821256320.0000\n",
      "Epoch 32, Test Loss: 336021645485408256.0000\n",
      "Epoch 33, Train Loss: 330461833673822016.0000\n",
      "Epoch 33, Test Loss: 328826303954288640.0000\n",
      "Epoch 34, Train Loss: 326601414503161664.0000\n",
      "Epoch 34, Test Loss: 324684649810886656.0000\n",
      "Epoch 35, Train Loss: 319831596549102720.0000\n",
      "Epoch 35, Test Loss: 328722365745725440.0000\n",
      "Epoch 36, Train Loss: 316046929850003904.0000\n",
      "Epoch 36, Test Loss: 316842108247867392.0000\n",
      "Epoch 37, Train Loss: 311251406563953088.0000\n",
      "Epoch 37, Test Loss: 308167614339743744.0000\n",
      "Epoch 38, Train Loss: 303620711344287808.0000\n",
      "Epoch 38, Test Loss: 311233258916413440.0000\n",
      "Epoch 39, Train Loss: 298781239843735936.0000\n",
      "Epoch 39, Test Loss: 296036668190752768.0000\n",
      "Epoch 40, Train Loss: 295445115520013696.0000\n",
      "Epoch 40, Test Loss: 293187799203446784.0000\n",
      "Epoch 41, Train Loss: 297390898209994304.0000\n",
      "Epoch 41, Test Loss: 286605916441149440.0000\n",
      "Epoch 42, Train Loss: 284925132273883296.0000\n",
      "Epoch 42, Test Loss: 287272426646011904.0000\n",
      "Epoch 43, Train Loss: 284133372897303616.0000\n",
      "Epoch 43, Test Loss: 277857703714357248.0000\n",
      "Epoch 44, Train Loss: 279476689891754144.0000\n",
      "Epoch 44, Test Loss: 287306975362940928.0000\n",
      "Epoch 45, Train Loss: 275621880162642080.0000\n",
      "Epoch 45, Test Loss: 271661302856810496.0000\n",
      "Epoch 46, Train Loss: 271507103094741824.0000\n",
      "Epoch 46, Test Loss: 265490225126440960.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:54:37,830]\u001b[0m Trial 33 finished with value: 2.6910112439127245e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 106, 'output_activation_class': 'ReLU', 'lr': 0.001567389116205432, 'batch_size': 86, 'epochs': 47}. Best is trial 26 with value: 1.2768021224987034e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, Train Loss: 264793250194527680.0000\n",
      "Epoch 47, Test Loss: 269101124391272448.0000\n",
      "Epoch 1, Train Loss: 493581230716507136.0000\n",
      "Epoch 1, Test Loss: 500516693898952704.0000\n",
      "Epoch 2, Train Loss: 492892632150497856.0000\n",
      "Epoch 2, Test Loss: 498322240488603648.0000\n",
      "Epoch 3, Train Loss: 486471689880147200.0000\n",
      "Epoch 3, Test Loss: 485811172554047488.0000\n",
      "Epoch 4, Train Loss: 476918253894732416.0000\n",
      "Epoch 4, Test Loss: 482242913724530688.0000\n",
      "Epoch 5, Train Loss: 475499480432010432.0000\n",
      "Epoch 5, Test Loss: 481438174292213760.0000\n",
      "Epoch 6, Train Loss: 474754590612063296.0000\n",
      "Epoch 6, Test Loss: 480596704299581440.0000\n",
      "Epoch 7, Train Loss: 481319751271240128.0000\n",
      "Epoch 7, Test Loss: 479876970859986944.0000\n",
      "Epoch 8, Train Loss: 473208623787546944.0000\n",
      "Epoch 8, Test Loss: 479120266341908480.0000\n",
      "Epoch 9, Train Loss: 472491341295683136.0000\n",
      "Epoch 9, Test Loss: 478175682774433792.0000\n",
      "Epoch 10, Train Loss: 471508809016594880.0000\n",
      "Epoch 10, Test Loss: 477076514744041472.0000\n",
      "Epoch 11, Train Loss: 471331700425002688.0000\n",
      "Epoch 11, Test Loss: 476227829206351872.0000\n",
      "Epoch 12, Train Loss: 472908113920633856.0000\n",
      "Epoch 12, Test Loss: 475315509433204736.0000\n",
      "Epoch 13, Train Loss: 468767411534943744.0000\n",
      "Epoch 13, Test Loss: 474336875365007360.0000\n",
      "Epoch 14, Train Loss: 471403069720537024.0000\n",
      "Epoch 14, Test Loss: 473318040402919424.0000\n",
      "Epoch 15, Train Loss: 466813010359628352.0000\n",
      "Epoch 15, Test Loss: 472237735868891136.0000\n",
      "Epoch 16, Train Loss: 465505055467054528.0000\n",
      "Epoch 16, Test Loss: 471078335217139712.0000\n",
      "Epoch 17, Train Loss: 464577622724927616.0000\n",
      "Epoch 17, Test Loss: 469844339573391360.0000\n",
      "Epoch 18, Train Loss: 463159162460374080.0000\n",
      "Epoch 18, Test Loss: 468398516142604288.0000\n",
      "Epoch 19, Train Loss: 462610800574601536.0000\n",
      "Epoch 19, Test Loss: 466535050091954176.0000\n",
      "Epoch 20, Train Loss: 459775343150747072.0000\n",
      "Epoch 20, Test Loss: 464734187484610560.0000\n",
      "Epoch 21, Train Loss: 462374132210522688.0000\n",
      "Epoch 21, Test Loss: 462731942450692096.0000\n",
      "Epoch 22, Train Loss: 473199889619504000.0000\n",
      "Epoch 22, Test Loss: 460659637910241280.0000\n",
      "Epoch 23, Train Loss: 453792307995372800.0000\n",
      "Epoch 23, Test Loss: 458173985717223424.0000\n",
      "Epoch 24, Train Loss: 451262828759108480.0000\n",
      "Epoch 24, Test Loss: 455393492609269760.0000\n",
      "Epoch 25, Train Loss: 486033244894226688.0000\n",
      "Epoch 25, Test Loss: 452622276630675456.0000\n",
      "Epoch 26, Train Loss: 445721786963137792.0000\n",
      "Epoch 26, Test Loss: 449569001559818240.0000\n",
      "Epoch 27, Train Loss: 442634445479056128.0000\n",
      "Epoch 27, Test Loss: 446049636637999104.0000\n",
      "Epoch 28, Train Loss: 438493098267394880.0000\n",
      "Epoch 28, Test Loss: 441710070401597440.0000\n",
      "Epoch 29, Train Loss: 434550297448514048.0000\n",
      "Epoch 29, Test Loss: 437588516705140736.0000\n",
      "Epoch 30, Train Loss: 430144398654649984.0000\n",
      "Epoch 30, Test Loss: 432755716424466432.0000\n",
      "Epoch 31, Train Loss: 424974722939756928.0000\n",
      "Epoch 31, Test Loss: 429458727729364992.0000\n",
      "Epoch 32, Train Loss: 421336757706346944.0000\n",
      "Epoch 32, Test Loss: 422929793323892736.0000\n",
      "Epoch 33, Train Loss: 415176484159836096.0000\n",
      "Epoch 33, Test Loss: 416651856807198720.0000\n",
      "Epoch 34, Train Loss: 409931684881530368.0000\n",
      "Epoch 34, Test Loss: 410958070202499072.0000\n",
      "Epoch 35, Train Loss: 403908848739486848.0000\n",
      "Epoch 35, Test Loss: 405291702669017088.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:54:50,049]\u001b[0m Trial 34 finished with value: 3.9870916707169075e+17 and parameters: {'activation_class': 'LeakyReLU', 'hidden_sizes': 383, 'output_activation_class': 'LeakyReLU', 'lr': 0.0010822811255370357, 'batch_size': 133, 'epochs': 36}. Best is trial 26 with value: 1.2768021224987034e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Train Loss: 398596336758223744.0000\n",
      "Epoch 36, Test Loss: 398709167071690752.0000\n",
      "Epoch 1, Train Loss: 494181821100633152.0000\n",
      "Epoch 1, Test Loss: 482512740749934592.0000\n",
      "Epoch 2, Train Loss: 476457456777883456.0000\n",
      "Epoch 2, Test Loss: 472980696491622400.0000\n",
      "Epoch 3, Train Loss: 467881822653298112.0000\n",
      "Epoch 3, Test Loss: 470468037544247296.0000\n",
      "Epoch 4, Train Loss: 456159868435359168.0000\n",
      "Epoch 4, Test Loss: 453891284847820800.0000\n",
      "Epoch 5, Train Loss: 435180053433562304.0000\n",
      "Epoch 5, Test Loss: 416284173246922752.0000\n",
      "Epoch 6, Train Loss: 411461682839560576.0000\n",
      "Epoch 6, Test Loss: 390178125071253504.0000\n",
      "Epoch 7, Train Loss: 382022273256770368.0000\n",
      "Epoch 7, Test Loss: 380745105578917888.0000\n",
      "Epoch 8, Train Loss: 370802682715960960.0000\n",
      "Epoch 8, Test Loss: 336945097813786624.0000\n",
      "Epoch 9, Train Loss: 337983636792415040.0000\n",
      "Epoch 9, Test Loss: 309908656642588672.0000\n",
      "Epoch 10, Train Loss: 317024063989515776.0000\n",
      "Epoch 10, Test Loss: 292954943256526848.0000\n",
      "Epoch 11, Train Loss: 295567639098239168.0000\n",
      "Epoch 11, Test Loss: 320427890543951872.0000\n",
      "Epoch 12, Train Loss: 303655991809110144.0000\n",
      "Epoch 12, Test Loss: 278589102285127680.0000\n",
      "Epoch 13, Train Loss: 264925881040974400.0000\n",
      "Epoch 13, Test Loss: 248493957204410368.0000\n",
      "Epoch 14, Train Loss: 265299119705220512.0000\n",
      "Epoch 14, Test Loss: 230005514564534272.0000\n",
      "Epoch 15, Train Loss: 247578466089001504.0000\n",
      "Epoch 15, Test Loss: 213334908242755584.0000\n",
      "Epoch 16, Train Loss: 238673389479460256.0000\n",
      "Epoch 16, Test Loss: 210992055122395136.0000\n",
      "Epoch 17, Train Loss: 205827423009399936.0000\n",
      "Epoch 17, Test Loss: 248873752572461056.0000\n",
      "Epoch 18, Train Loss: 218729722113045664.0000\n",
      "Epoch 18, Test Loss: 183705767113654272.0000\n",
      "Epoch 19, Train Loss: 232273781415482144.0000\n",
      "Epoch 19, Test Loss: 183561112615124992.0000\n",
      "Epoch 20, Train Loss: 167185750439731264.0000\n",
      "Epoch 20, Test Loss: 176127932975022080.0000\n",
      "Epoch 21, Train Loss: 212192311776942368.0000\n",
      "Epoch 21, Test Loss: 149825845312094208.0000\n",
      "Epoch 22, Train Loss: 178812141728314528.0000\n",
      "Epoch 22, Test Loss: 156538655857442816.0000\n",
      "Epoch 23, Train Loss: 167020691394284672.0000\n",
      "Epoch 23, Test Loss: 135604864997654528.0000\n",
      "Epoch 24, Train Loss: 182821015056682592.0000\n",
      "Epoch 24, Test Loss: 144382575559835648.0000\n",
      "Epoch 25, Train Loss: 147612850793933440.0000\n",
      "Epoch 25, Test Loss: 121641694390124544.0000\n",
      "Epoch 26, Train Loss: 173138133230653952.0000\n",
      "Epoch 26, Test Loss: 206549323771543552.0000\n",
      "Epoch 27, Train Loss: 161733345457767936.0000\n",
      "Epoch 27, Test Loss: 156402488214290432.0000\n",
      "Epoch 28, Train Loss: 154451939228564128.0000\n",
      "Epoch 28, Test Loss: 115389553447010304.0000\n",
      "Epoch 29, Train Loss: 147849421101465600.0000\n",
      "Epoch 29, Test Loss: 116834681093095424.0000\n",
      "Epoch 30, Train Loss: 257486210774818112.0000\n",
      "Epoch 30, Test Loss: 500558647139500032.0000\n",
      "Epoch 31, Train Loss: 499184003841131712.0000\n",
      "Epoch 31, Test Loss: 500558647139500032.0000\n",
      "Epoch 32, Train Loss: 499239373748193216.0000\n",
      "Epoch 32, Test Loss: 500558647139500032.0000\n",
      "Epoch 33, Train Loss: 499024157720949632.0000\n",
      "Epoch 33, Test Loss: 500558647139500032.0000\n",
      "Epoch 34, Train Loss: 499032974236741120.0000\n",
      "Epoch 34, Test Loss: 500558647139500032.0000\n",
      "Epoch 35, Train Loss: 499031154417001920.0000\n",
      "Epoch 35, Test Loss: 500558647139500032.0000\n",
      "Epoch 36, Train Loss: 499000428870867840.0000\n",
      "Epoch 36, Test Loss: 500558647139500032.0000\n",
      "Epoch 37, Train Loss: 499211439993814144.0000\n",
      "Epoch 37, Test Loss: 500558647139500032.0000\n",
      "Epoch 38, Train Loss: 499198275703809600.0000\n",
      "Epoch 38, Test Loss: 500558647139500032.0000\n",
      "Epoch 39, Train Loss: 499966062610577920.0000\n",
      "Epoch 39, Test Loss: 500558647139500032.0000\n",
      "Epoch 40, Train Loss: 499970173457492928.0000\n",
      "Epoch 40, Test Loss: 500558647139500032.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:55:22,069]\u001b[0m Trial 35 finished with value: 5.005586471395e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 466, 'output_activation_class': 'ReLU', 'lr': 0.002588511831971584, 'batch_size': 75, 'epochs': 41}. Best is trial 26 with value: 1.2768021224987034e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, Train Loss: 498999360345013632.0000\n",
      "Epoch 41, Test Loss: 500558647139500032.0000\n",
      "Epoch 1, Train Loss: 496774598419241472.0000\n",
      "Epoch 1, Test Loss: 500558647139500032.0000\n",
      "Epoch 2, Train Loss: 497073106224173824.0000\n",
      "Epoch 2, Test Loss: 500558647139500032.0000\n",
      "Epoch 3, Train Loss: 496466242833793472.0000\n",
      "Epoch 3, Test Loss: 500558647139500032.0000\n",
      "Epoch 4, Train Loss: 496491747063082240.0000\n",
      "Epoch 4, Test Loss: 500558647139500032.0000\n",
      "Epoch 5, Train Loss: 497766506031268288.0000\n",
      "Epoch 5, Test Loss: 500558647139500032.0000\n",
      "Epoch 6, Train Loss: 497010378559412736.0000\n",
      "Epoch 6, Test Loss: 500558647139500032.0000\n",
      "Epoch 7, Train Loss: 497325540573538240.0000\n",
      "Epoch 7, Test Loss: 500558647139500032.0000\n",
      "Epoch 8, Train Loss: 496973295403004480.0000\n",
      "Epoch 8, Test Loss: 500558647139500032.0000\n",
      "Epoch 9, Train Loss: 496502437141686656.0000\n",
      "Epoch 9, Test Loss: 500558647139500032.0000\n",
      "Epoch 10, Train Loss: 496765083471484992.0000\n",
      "Epoch 10, Test Loss: 500558647139500032.0000\n",
      "Epoch 11, Train Loss: 502458075553939008.0000\n",
      "Epoch 11, Test Loss: 500558647139500032.0000\n",
      "Epoch 12, Train Loss: 561868827461185216.0000\n",
      "Epoch 12, Test Loss: 500558647139500032.0000\n",
      "Epoch 13, Train Loss: 548350466009092864.0000\n",
      "Epoch 13, Test Loss: 500558647139500032.0000\n",
      "Epoch 14, Train Loss: 496499048495015872.0000\n",
      "Epoch 14, Test Loss: 500558647139500032.0000\n",
      "Epoch 15, Train Loss: 496501348417128640.0000\n",
      "Epoch 15, Test Loss: 500558647139500032.0000\n",
      "Epoch 16, Train Loss: 506555633376484736.0000\n",
      "Epoch 16, Test Loss: 500558647139500032.0000\n",
      "Epoch 17, Train Loss: 499277891962121472.0000\n",
      "Epoch 17, Test Loss: 500558647139500032.0000\n",
      "Epoch 18, Train Loss: 496475365173610688.0000\n",
      "Epoch 18, Test Loss: 500558647139500032.0000\n",
      "Epoch 19, Train Loss: 496462939016184256.0000\n",
      "Epoch 19, Test Loss: 500558647139500032.0000\n",
      "Epoch 20, Train Loss: 496744796426529600.0000\n",
      "Epoch 20, Test Loss: 500558647139500032.0000\n",
      "Epoch 21, Train Loss: 496502244686614400.0000\n",
      "Epoch 21, Test Loss: 500558647139500032.0000\n",
      "Epoch 22, Train Loss: 496463131973439360.0000\n",
      "Epoch 22, Test Loss: 500558647139500032.0000\n",
      "Epoch 23, Train Loss: 497514503876477824.0000\n",
      "Epoch 23, Test Loss: 500558647139500032.0000\n",
      "Epoch 24, Train Loss: 500033843630064960.0000\n",
      "Epoch 24, Test Loss: 500558647139500032.0000\n",
      "Epoch 25, Train Loss: 496474494103276544.0000\n",
      "Epoch 25, Test Loss: 500558647139500032.0000\n",
      "Epoch 26, Train Loss: 496463242786724352.0000\n",
      "Epoch 26, Test Loss: 500558647139500032.0000\n",
      "Epoch 27, Train Loss: 542420506645853568.0000\n",
      "Epoch 27, Test Loss: 500558647139500032.0000\n",
      "Epoch 28, Train Loss: 500883741912440640.0000\n",
      "Epoch 28, Test Loss: 500558647139500032.0000\n",
      "Epoch 29, Train Loss: 496909739769257344.0000\n",
      "Epoch 29, Test Loss: 500558647139500032.0000\n",
      "Epoch 30, Train Loss: 496463884592604224.0000\n",
      "Epoch 30, Test Loss: 500558647139500032.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:55:44,218]\u001b[0m Trial 36 finished with value: 5.005586471395e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 597, 'output_activation_class': 'Sigmoid', 'lr': 0.0009352596402817674, 'batch_size': 109, 'epochs': 31}. Best is trial 26 with value: 1.2768021224987034e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Train Loss: 496559101325380224.0000\n",
      "Epoch 31, Test Loss: 500558647139500032.0000\n",
      "Epoch 1, Train Loss: 491346954389144192.0000\n",
      "Epoch 1, Test Loss: 500558647139500032.0000\n",
      "Epoch 2, Train Loss: 491369996329083200.0000\n",
      "Epoch 2, Test Loss: 500558647139500032.0000\n",
      "Epoch 3, Train Loss: 491433621588059072.0000\n",
      "Epoch 3, Test Loss: 500558647139500032.0000\n",
      "Epoch 4, Train Loss: 491534370128422528.0000\n",
      "Epoch 4, Test Loss: 500558647139500032.0000\n",
      "Epoch 5, Train Loss: 491348387547840512.0000\n",
      "Epoch 5, Test Loss: 500558647139500032.0000\n",
      "Epoch 6, Train Loss: 491346167342322176.0000\n",
      "Epoch 6, Test Loss: 500558647139500032.0000\n",
      "Epoch 7, Train Loss: 492423440416815168.0000\n",
      "Epoch 7, Test Loss: 500558647139500032.0000\n",
      "Epoch 8, Train Loss: 491347451898442560.0000\n",
      "Epoch 8, Test Loss: 500558647139500032.0000\n",
      "Epoch 9, Train Loss: 496595928613618944.0000\n",
      "Epoch 9, Test Loss: 500558647139500032.0000\n",
      "Epoch 10, Train Loss: 491361104526741312.0000\n",
      "Epoch 10, Test Loss: 500558647139500032.0000\n",
      "Epoch 11, Train Loss: 522983730402002176.0000\n",
      "Epoch 11, Test Loss: 500558647139500032.0000\n",
      "Epoch 12, Train Loss: 491505256896617536.0000\n",
      "Epoch 12, Test Loss: 500558647139500032.0000\n",
      "Epoch 13, Train Loss: 491345871914032192.0000\n",
      "Epoch 13, Test Loss: 500558647139500032.0000\n",
      "Epoch 14, Train Loss: 491358610793567360.0000\n",
      "Epoch 14, Test Loss: 500558647139500032.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:55:46,925]\u001b[0m Trial 37 finished with value: 5.005586471395e+17 and parameters: {'activation_class': 'Tanh', 'hidden_sizes': 188, 'output_activation_class': 'ELU', 'lr': 0.0014210808895971413, 'batch_size': 163, 'epochs': 15}. Best is trial 26 with value: 1.2768021224987034e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Train Loss: 491667853084154944.0000\n",
      "Epoch 15, Test Loss: 500558647139500032.0000\n",
      "Epoch 1, Train Loss: 499717205025807232.0000\n",
      "Epoch 1, Test Loss: 500558647139500032.0000\n",
      "Epoch 2, Train Loss: 499391283637555776.0000\n",
      "Epoch 2, Test Loss: 500558647139500032.0000\n",
      "Epoch 3, Train Loss: 501846526854942528.0000\n",
      "Epoch 3, Test Loss: 500558647139500032.0000\n",
      "Epoch 4, Train Loss: 499518675965868544.0000\n",
      "Epoch 4, Test Loss: 500558647139500032.0000\n",
      "Epoch 5, Train Loss: 499475707853949248.0000\n",
      "Epoch 5, Test Loss: 500558647139500032.0000\n",
      "Epoch 6, Train Loss: 501252019300160832.0000\n",
      "Epoch 6, Test Loss: 500558647139500032.0000\n",
      "Epoch 7, Train Loss: 501852812335057984.0000\n",
      "Epoch 7, Test Loss: 500558647139500032.0000\n",
      "Epoch 8, Train Loss: 499251414442030784.0000\n",
      "Epoch 8, Test Loss: 500558647139500032.0000\n",
      "Epoch 9, Train Loss: 501545499800183936.0000\n",
      "Epoch 9, Test Loss: 500558647139500032.0000\n",
      "Epoch 10, Train Loss: 499468461713641280.0000\n",
      "Epoch 10, Test Loss: 500558647139500032.0000\n",
      "Epoch 11, Train Loss: 499919243892115776.0000\n",
      "Epoch 11, Test Loss: 500558647139500032.0000\n",
      "Epoch 12, Train Loss: 499796509641183808.0000\n",
      "Epoch 12, Test Loss: 500558647139500032.0000\n",
      "Epoch 13, Train Loss: 499298885833551296.0000\n",
      "Epoch 13, Test Loss: 500558647139500032.0000\n",
      "Epoch 14, Train Loss: 499226479229652800.0000\n",
      "Epoch 14, Test Loss: 500558647139500032.0000\n",
      "Epoch 15, Train Loss: 499415962200072576.0000\n",
      "Epoch 15, Test Loss: 500558647139500032.0000\n",
      "Epoch 16, Train Loss: 499762074395756864.0000\n",
      "Epoch 16, Test Loss: 500558647139500032.0000\n",
      "Epoch 17, Train Loss: 501826625817193152.0000\n",
      "Epoch 17, Test Loss: 500558647139500032.0000\n",
      "Epoch 18, Train Loss: 504177025201483968.0000\n",
      "Epoch 18, Test Loss: 500558647139500032.0000\n",
      "Epoch 19, Train Loss: 505437693608360448.0000\n",
      "Epoch 19, Test Loss: 500558647139500032.0000\n",
      "Epoch 20, Train Loss: 499636836664631680.0000\n",
      "Epoch 20, Test Loss: 500558647139500032.0000\n",
      "Epoch 21, Train Loss: 499628922624952640.0000\n",
      "Epoch 21, Test Loss: 500558647139500032.0000\n",
      "Epoch 22, Train Loss: 500125106813234624.0000\n",
      "Epoch 22, Test Loss: 500558647139500032.0000\n",
      "Epoch 23, Train Loss: 499552977456938240.0000\n",
      "Epoch 23, Test Loss: 500558647139500032.0000\n",
      "Epoch 24, Train Loss: 499778748800977216.0000\n",
      "Epoch 24, Test Loss: 500558647139500032.0000\n",
      "Epoch 25, Train Loss: 502116352959996288.0000\n",
      "Epoch 25, Test Loss: 500558647139500032.0000\n",
      "Epoch 26, Train Loss: 499517437609196800.0000\n",
      "Epoch 26, Test Loss: 500558647139500032.0000\n",
      "Epoch 27, Train Loss: 499531043132457728.0000\n",
      "Epoch 27, Test Loss: 500558647139500032.0000\n",
      "Epoch 28, Train Loss: 499514608171664128.0000\n",
      "Epoch 28, Test Loss: 500558647139500032.0000\n",
      "Epoch 29, Train Loss: 502763518772766592.0000\n",
      "Epoch 29, Test Loss: 500558647139500032.0000\n",
      "Epoch 30, Train Loss: 499392904546708928.0000\n",
      "Epoch 30, Test Loss: 500558647139500032.0000\n",
      "Epoch 31, Train Loss: 503115367569961152.0000\n",
      "Epoch 31, Test Loss: 500558647139500032.0000\n",
      "Epoch 32, Train Loss: 507212383081264064.0000\n",
      "Epoch 32, Test Loss: 500558647139500032.0000\n",
      "Epoch 33, Train Loss: 500385602964666048.0000\n",
      "Epoch 33, Test Loss: 500558647139500032.0000\n",
      "Epoch 34, Train Loss: 499197086493136192.0000\n",
      "Epoch 34, Test Loss: 500558647139500032.0000\n",
      "Epoch 35, Train Loss: 504420536281589632.0000\n",
      "Epoch 35, Test Loss: 500558647139500032.0000\n",
      "Epoch 36, Train Loss: 499248910974620224.0000\n",
      "Epoch 36, Test Loss: 500558647139500032.0000\n",
      "Epoch 37, Train Loss: 501433716290592384.0000\n",
      "Epoch 37, Test Loss: 500558647139500032.0000\n",
      "Epoch 38, Train Loss: 500179831209341120.0000\n",
      "Epoch 38, Test Loss: 500558647139500032.0000\n",
      "Epoch 39, Train Loss: 501914167962538560.0000\n",
      "Epoch 39, Test Loss: 500558647139500032.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:55:56,739]\u001b[0m Trial 38 finished with value: 5.005586471395e+17 and parameters: {'activation_class': 'Sigmoid', 'hidden_sizes': 301, 'output_activation_class': 'Sigmoid', 'lr': 0.005388637114859075, 'batch_size': 191, 'epochs': 40}. Best is trial 26 with value: 1.2768021224987034e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Train Loss: 501911226165593600.0000\n",
      "Epoch 40, Test Loss: 500558647139500032.0000\n",
      "Epoch 1, Train Loss: 489283510217001088.0000\n",
      "Epoch 1, Test Loss: 477410869358100480.0000\n",
      "Epoch 2, Train Loss: 471334236967423680.0000\n",
      "Epoch 2, Test Loss: 467839964235694080.0000\n",
      "Epoch 3, Train Loss: 450497709751180672.0000\n",
      "Epoch 3, Test Loss: 442662247471251456.0000\n",
      "Epoch 4, Train Loss: 421558243156761280.0000\n",
      "Epoch 4, Test Loss: 391795987712049152.0000\n",
      "Epoch 5, Train Loss: 381029288482763648.0000\n",
      "Epoch 5, Test Loss: 347373965603241984.0000\n",
      "Epoch 6, Train Loss: 378351812314318080.0000\n",
      "Epoch 6, Test Loss: 333354951830929408.0000\n",
      "Epoch 7, Train Loss: 326590556394359488.0000\n",
      "Epoch 7, Test Loss: 296382567676903424.0000\n",
      "Epoch 8, Train Loss: 312388632437911552.0000\n",
      "Epoch 8, Test Loss: 273054521528156160.0000\n",
      "Epoch 9, Train Loss: 306380685845490560.0000\n",
      "Epoch 9, Test Loss: 259071722920083456.0000\n",
      "Epoch 10, Train Loss: 288382732558826816.0000\n",
      "Epoch 10, Test Loss: 410522354360254464.0000\n",
      "Epoch 11, Train Loss: 277966298422913312.0000\n",
      "Epoch 11, Test Loss: 385106558968659968.0000\n",
      "Epoch 12, Train Loss: 273668220162170144.0000\n",
      "Epoch 12, Test Loss: 264841564805660672.0000\n",
      "Epoch 13, Train Loss: 242239501984374144.0000\n",
      "Epoch 13, Test Loss: 199964434532139008.0000\n",
      "Epoch 14, Train Loss: 213898549855971520.0000\n",
      "Epoch 14, Test Loss: 175202659580510208.0000\n",
      "Epoch 15, Train Loss: 405896530969053568.0000\n",
      "Epoch 15, Test Loss: 423986733235830784.0000\n",
      "Epoch 16, Train Loss: 398849000365247296.0000\n",
      "Epoch 16, Test Loss: 374270356480851968.0000\n",
      "Epoch 17, Train Loss: 363783552668584064.0000\n",
      "Epoch 17, Test Loss: 354040613840093184.0000\n",
      "Epoch 18, Train Loss: 347933101470368576.0000\n",
      "Epoch 18, Test Loss: 342316521353117696.0000\n",
      "Epoch 19, Train Loss: 323807737509257728.0000\n",
      "Epoch 19, Test Loss: 333340211503169536.0000\n",
      "Epoch 20, Train Loss: 313308165963952000.0000\n",
      "Epoch 20, Test Loss: 288930215302791168.0000\n",
      "Epoch 21, Train Loss: 280268793843069696.0000\n",
      "Epoch 21, Test Loss: 289712105509093376.0000\n",
      "Epoch 22, Train Loss: 280851507449741664.0000\n",
      "Epoch 22, Test Loss: 264658204061859840.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:57:06,371]\u001b[0m Trial 39 finished with value: 2.4831112903655424e+17 and parameters: {'activation_class': 'ReLU', 'hidden_sizes': 765, 'output_activation_class': None, 'lr': 0.0031950952836384936, 'batch_size': 56, 'epochs': 23}. Best is trial 26 with value: 1.2768021224987034e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Train Loss: 279781042298973952.0000\n",
      "Epoch 23, Test Loss: 248311129036554240.0000\n",
      "Epoch 1, Train Loss: 488179950849593472.0000\n",
      "Epoch 1, Test Loss: 474284923440594944.0000\n",
      "Epoch 2, Train Loss: 468967757785475392.0000\n",
      "Epoch 2, Test Loss: 458671789606699008.0000\n",
      "Epoch 3, Train Loss: 446732405347417344.0000\n",
      "Epoch 3, Test Loss: 422959067820982272.0000\n",
      "Epoch 4, Train Loss: 409508522689572992.0000\n",
      "Epoch 4, Test Loss: 412549750722658304.0000\n",
      "Epoch 5, Train Loss: 379530537366548352.0000\n",
      "Epoch 5, Test Loss: 347240512379420672.0000\n",
      "Epoch 6, Train Loss: 350004167176458752.0000\n",
      "Epoch 6, Test Loss: 336720556923551744.0000\n",
      "Epoch 7, Train Loss: 330188035404289408.0000\n",
      "Epoch 7, Test Loss: 317693130247766016.0000\n",
      "Epoch 8, Train Loss: 323798187766018752.0000\n",
      "Epoch 8, Test Loss: 287447523872735232.0000\n",
      "Epoch 9, Train Loss: 302811971613856832.0000\n",
      "Epoch 9, Test Loss: 341491578394640384.0000\n",
      "Epoch 10, Train Loss: 276808798918365504.0000\n",
      "Epoch 10, Test Loss: 237666344651194368.0000\n",
      "Epoch 11, Train Loss: 265829018169858656.0000\n",
      "Epoch 11, Test Loss: 429878157055623168.0000\n",
      "Epoch 12, Train Loss: 265185819769275840.0000\n",
      "Epoch 12, Test Loss: 213009401261326336.0000\n",
      "Epoch 13, Train Loss: 239824300681226944.0000\n",
      "Epoch 13, Test Loss: 216653079716560896.0000\n",
      "Epoch 14, Train Loss: 214308680215811168.0000\n",
      "Epoch 14, Test Loss: 211306292109639680.0000\n",
      "Epoch 15, Train Loss: 210764979910654976.0000\n",
      "Epoch 15, Test Loss: 167663497327411200.0000\n",
      "Epoch 16, Train Loss: 406286198724998016.0000\n",
      "Epoch 16, Test Loss: 500558647139500032.0000\n",
      "Epoch 17, Train Loss: 499434944737963328.0000\n",
      "Epoch 17, Test Loss: 500558647139500032.0000\n",
      "Epoch 18, Train Loss: 501602771206607104.0000\n",
      "Epoch 18, Test Loss: 500558647139500032.0000\n",
      "Epoch 19, Train Loss: 500800111142672000.0000\n",
      "Epoch 19, Test Loss: 500558647139500032.0000\n",
      "Epoch 20, Train Loss: 499439254244552832.0000\n",
      "Epoch 20, Test Loss: 500558647139500032.0000\n",
      "Epoch 21, Train Loss: 499434873170968320.0000\n",
      "Epoch 21, Test Loss: 500558647139500032.0000\n",
      "Epoch 22, Train Loss: 499434897526446208.0000\n",
      "Epoch 22, Test Loss: 500558647139500032.0000\n",
      "Epoch 23, Train Loss: 499434880339518272.0000\n",
      "Epoch 23, Test Loss: 500558647139500032.0000\n",
      "Epoch 24, Train Loss: 499493966661004032.0000\n",
      "Epoch 24, Test Loss: 500558647139500032.0000\n",
      "Epoch 25, Train Loss: 499526842845726528.0000\n",
      "Epoch 25, Test Loss: 500558647139500032.0000\n",
      "Epoch 26, Train Loss: 502657905535203200.0000\n",
      "Epoch 26, Test Loss: 500558647139500032.0000\n",
      "Epoch 27, Train Loss: 504893545967495936.0000\n",
      "Epoch 27, Test Loss: 500558647139500032.0000\n",
      "Epoch 28, Train Loss: 499617365794486848.0000\n",
      "Epoch 28, Test Loss: 500558647139500032.0000\n",
      "Epoch 29, Train Loss: 499540175365156096.0000\n",
      "Epoch 29, Test Loss: 500558647139500032.0000\n",
      "Epoch 30, Train Loss: 499538786336066688.0000\n",
      "Epoch 30, Test Loss: 500558647139500032.0000\n",
      "Epoch 31, Train Loss: 499820259193180800.0000\n",
      "Epoch 31, Test Loss: 500558647139500032.0000\n",
      "Epoch 32, Train Loss: 499442726478894016.0000\n",
      "Epoch 32, Test Loss: 500558647139500032.0000\n",
      "Epoch 33, Train Loss: 499509968782581120.0000\n",
      "Epoch 33, Test Loss: 500558647139500032.0000\n",
      "Epoch 34, Train Loss: 499491509490298688.0000\n",
      "Epoch 34, Test Loss: 500558647139500032.0000\n",
      "Epoch 35, Train Loss: 499899183543006016.0000\n",
      "Epoch 35, Test Loss: 500558647139500032.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:58:21,322]\u001b[0m Trial 40 finished with value: 5.005586471395e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 575, 'output_activation_class': 'ReLU', 'lr': 0.001975153987297643, 'batch_size': 38, 'epochs': 36}. Best is trial 26 with value: 1.2768021224987034e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Train Loss: 499636746605030400.0000\n",
      "Epoch 36, Test Loss: 500558647139500032.0000\n",
      "Epoch 1, Train Loss: 487454947799473152.0000\n",
      "Epoch 1, Test Loss: 475853995252908032.0000\n",
      "Epoch 2, Train Loss: 464390753254529856.0000\n",
      "Epoch 2, Test Loss: 449914213851201536.0000\n",
      "Epoch 3, Train Loss: 447552473751890432.0000\n",
      "Epoch 3, Test Loss: 402885799428751360.0000\n",
      "Epoch 4, Train Loss: 395165387431520768.0000\n",
      "Epoch 4, Test Loss: 385447545012224000.0000\n",
      "Epoch 5, Train Loss: 376932872964775360.0000\n",
      "Epoch 5, Test Loss: 341127605686108160.0000\n",
      "Epoch 6, Train Loss: 353545802436905600.0000\n",
      "Epoch 6, Test Loss: 346954364478291968.0000\n",
      "Epoch 7, Train Loss: 313123420604813504.0000\n",
      "Epoch 7, Test Loss: 323603829880782848.0000\n",
      "Epoch 8, Train Loss: 327758215146944064.0000\n",
      "Epoch 8, Test Loss: 267612901043339264.0000\n",
      "Epoch 9, Train Loss: 273801355825167712.0000\n",
      "Epoch 9, Test Loss: 274638453927313408.0000\n",
      "Epoch 10, Train Loss: 311420818190402880.0000\n",
      "Epoch 10, Test Loss: 484669364088340480.0000\n",
      "Epoch 11, Train Loss: 495750073884107328.0000\n",
      "Epoch 11, Test Loss: 492796335565570048.0000\n",
      "Epoch 12, Train Loss: 481785429538316160.0000\n",
      "Epoch 12, Test Loss: 470569570571124736.0000\n",
      "Epoch 13, Train Loss: 438623210127638272.0000\n",
      "Epoch 13, Test Loss: 393699620296851456.0000\n",
      "Epoch 14, Train Loss: 385481880572842624.0000\n",
      "Epoch 14, Test Loss: 351958413694992384.0000\n",
      "Epoch 15, Train Loss: 327311109830594304.0000\n",
      "Epoch 15, Test Loss: 287379508770635776.0000\n",
      "Epoch 16, Train Loss: 290615808450496256.0000\n",
      "Epoch 16, Test Loss: 275826252902825984.0000\n",
      "Epoch 17, Train Loss: 263447706770413696.0000\n",
      "Epoch 17, Test Loss: 322924503493509120.0000\n",
      "Epoch 18, Train Loss: 425858292362327168.0000\n",
      "Epoch 18, Test Loss: 502383046167625728.0000\n",
      "Epoch 19, Train Loss: 500333456790995648.0000\n",
      "Epoch 19, Test Loss: 502007734745432064.0000\n",
      "Epoch 20, Train Loss: 499947447984733952.0000\n",
      "Epoch 20, Test Loss: 501640669660446720.0000\n",
      "Epoch 21, Train Loss: 500657711153963584.0000\n",
      "Epoch 21, Test Loss: 501174545449746432.0000\n",
      "Epoch 22, Train Loss: 506142736973946112.0000\n",
      "Epoch 22, Test Loss: 499517512707211264.0000\n",
      "Epoch 23, Train Loss: 372111896926515264.0000\n",
      "Epoch 23, Test Loss: 338786676711096320.0000\n",
      "Epoch 24, Train Loss: 282794098190687680.0000\n",
      "Epoch 24, Test Loss: 388118533633998848.0000\n",
      "Epoch 25, Train Loss: 276631786317119936.0000\n",
      "Epoch 25, Test Loss: 208684764331245568.0000\n",
      "Epoch 26, Train Loss: 259922665213497280.0000\n",
      "Epoch 26, Test Loss: 220638706288033792.0000\n",
      "Epoch 27, Train Loss: 419053451988883840.0000\n",
      "Epoch 27, Test Loss: 503866596591140864.0000\n",
      "Epoch 28, Train Loss: 394597781461177152.0000\n",
      "Epoch 28, Test Loss: 245710629418041344.0000\n",
      "Epoch 29, Train Loss: 267068331867626720.0000\n",
      "Epoch 29, Test Loss: 201220368868835328.0000\n",
      "Epoch 30, Train Loss: 214271165338143584.0000\n",
      "Epoch 30, Test Loss: 228465820328525824.0000\n",
      "Epoch 31, Train Loss: 242289408487316704.0000\n",
      "Epoch 31, Test Loss: 176489328703176704.0000\n",
      "Epoch 32, Train Loss: 201020509101935456.0000\n",
      "Epoch 32, Test Loss: 171547711131090944.0000\n",
      "Epoch 33, Train Loss: 208630729730582848.0000\n",
      "Epoch 33, Test Loss: 204467707741995008.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:58:53,134]\u001b[0m Trial 41 finished with value: 2.3357250208373146e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 455, 'output_activation_class': 'LeakyReLU', 'lr': 0.004251577110601089, 'batch_size': 55, 'epochs': 34}. Best is trial 26 with value: 1.2768021224987034e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Train Loss: 236976403931610912.0000\n",
      "Epoch 34, Test Loss: 233572502083731456.0000\n",
      "Epoch 1, Train Loss: 497519420123490240.0000\n",
      "Epoch 1, Test Loss: 486166383529295872.0000\n",
      "Epoch 2, Train Loss: 480229662883198208.0000\n",
      "Epoch 2, Test Loss: 476645952862552064.0000\n",
      "Epoch 3, Train Loss: 473843432940769728.0000\n",
      "Epoch 3, Test Loss: 466158948395778048.0000\n",
      "Epoch 4, Train Loss: 459482339440166016.0000\n",
      "Epoch 4, Test Loss: 456218950963822592.0000\n",
      "Epoch 5, Train Loss: 444520223410532480.0000\n",
      "Epoch 5, Test Loss: 429655609030213632.0000\n",
      "Epoch 6, Train Loss: 421680831537999424.0000\n",
      "Epoch 6, Test Loss: 405024624422682624.0000\n",
      "Epoch 7, Train Loss: 395479644736450112.0000\n",
      "Epoch 7, Test Loss: 375206899869548544.0000\n",
      "Epoch 8, Train Loss: 364308989786230080.0000\n",
      "Epoch 8, Test Loss: 380203939699621888.0000\n",
      "Epoch 9, Train Loss: 344843721258231168.0000\n",
      "Epoch 9, Test Loss: 348180148144570368.0000\n",
      "Epoch 10, Train Loss: 322058509990277248.0000\n",
      "Epoch 10, Test Loss: 336294564887265280.0000\n",
      "Epoch 11, Train Loss: 312283997103411072.0000\n",
      "Epoch 11, Test Loss: 297503725939851264.0000\n",
      "Epoch 12, Train Loss: 295423104468801600.0000\n",
      "Epoch 12, Test Loss: 270092797980180480.0000\n",
      "Epoch 13, Train Loss: 276388483975393408.0000\n",
      "Epoch 13, Test Loss: 250616169264840704.0000\n",
      "Epoch 14, Train Loss: 284228564184980672.0000\n",
      "Epoch 14, Test Loss: 259448666429849600.0000\n",
      "Epoch 15, Train Loss: 287554222953197152.0000\n",
      "Epoch 15, Test Loss: 240935931454947328.0000\n",
      "Epoch 16, Train Loss: 248537071042718176.0000\n",
      "Epoch 16, Test Loss: 221088853220392960.0000\n",
      "Epoch 17, Train Loss: 264567451873091296.0000\n",
      "Epoch 17, Test Loss: 210774042582450176.0000\n",
      "Epoch 18, Train Loss: 219512729365670368.0000\n",
      "Epoch 18, Test Loss: 217932602013646848.0000\n",
      "Epoch 19, Train Loss: 223121653658577376.0000\n",
      "Epoch 19, Test Loss: 291095978331602944.0000\n",
      "Epoch 20, Train Loss: 231033967437396320.0000\n",
      "Epoch 20, Test Loss: 186128059949121536.0000\n",
      "Epoch 21, Train Loss: 225241272344777120.0000\n",
      "Epoch 21, Test Loss: 227825595323514880.0000\n",
      "Epoch 22, Train Loss: 220114254119504096.0000\n",
      "Epoch 22, Test Loss: 182269237992095744.0000\n",
      "Epoch 23, Train Loss: 207786967848126688.0000\n",
      "Epoch 23, Test Loss: 182524049811832832.0000\n",
      "Epoch 24, Train Loss: 198380856107073536.0000\n",
      "Epoch 24, Test Loss: 163730320896557056.0000\n",
      "Epoch 25, Train Loss: 218719633983668224.0000\n",
      "Epoch 25, Test Loss: 169237740280348672.0000\n",
      "Epoch 26, Train Loss: 187270482205226240.0000\n",
      "Epoch 26, Test Loss: 202135987176865792.0000\n",
      "Epoch 27, Train Loss: 164866340667649632.0000\n",
      "Epoch 27, Test Loss: 167620650733666304.0000\n",
      "Epoch 28, Train Loss: 166768724733076288.0000\n",
      "Epoch 28, Test Loss: 395233473377337344.0000\n",
      "Epoch 29, Train Loss: 219245507630532832.0000\n",
      "Epoch 29, Test Loss: 439449233976721408.0000\n",
      "Epoch 30, Train Loss: 512091302161624128.0000\n",
      "Epoch 30, Test Loss: 500963714095120384.0000\n",
      "Epoch 31, Train Loss: 482559390075442112.0000\n",
      "Epoch 31, Test Loss: 433061552455680000.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:59:07,652]\u001b[0m Trial 42 finished with value: 3.893262097181573e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 262, 'output_activation_class': 'LeakyReLU', 'lr': 0.0029041943497938112, 'batch_size': 69, 'epochs': 32}. Best is trial 26 with value: 1.2768021224987034e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Train Loss: 410430864371572992.0000\n",
      "Epoch 32, Test Loss: 389326209718157312.0000\n",
      "Epoch 1, Train Loss: 495029498553343104.0000\n",
      "Epoch 1, Test Loss: 492339007447891968.0000\n",
      "Epoch 2, Train Loss: 478709200501249664.0000\n",
      "Epoch 2, Test Loss: 478303913318023168.0000\n",
      "Epoch 3, Train Loss: 471126399741388032.0000\n",
      "Epoch 3, Test Loss: 473139438482882560.0000\n",
      "Epoch 4, Train Loss: 463988813633446016.0000\n",
      "Epoch 4, Test Loss: 462633570519744512.0000\n",
      "Epoch 5, Train Loss: 475329246753462272.0000\n",
      "Epoch 5, Test Loss: 451204318947704832.0000\n",
      "Epoch 6, Train Loss: 440886260862883072.0000\n",
      "Epoch 6, Test Loss: 430221788799041536.0000\n",
      "Epoch 7, Train Loss: 422416064163077056.0000\n",
      "Epoch 7, Test Loss: 410961265658167296.0000\n",
      "Epoch 8, Train Loss: 397298732920829312.0000\n",
      "Epoch 8, Test Loss: 382298681149227008.0000\n",
      "Epoch 9, Train Loss: 385445270861779456.0000\n",
      "Epoch 9, Test Loss: 374821830281658368.0000\n",
      "Epoch 10, Train Loss: 358532055793543744.0000\n",
      "Epoch 10, Test Loss: 343512240248324096.0000\n",
      "Epoch 11, Train Loss: 361551955202519488.0000\n",
      "Epoch 11, Test Loss: 335115441745690624.0000\n",
      "Epoch 12, Train Loss: 327073209225144704.0000\n",
      "Epoch 12, Test Loss: 414559726697709568.0000\n",
      "Epoch 13, Train Loss: 321096361430871552.0000\n",
      "Epoch 13, Test Loss: 314962493120184320.0000\n",
      "Epoch 14, Train Loss: 314940511866005312.0000\n",
      "Epoch 14, Test Loss: 385077834227384320.0000\n",
      "Epoch 15, Train Loss: 275074236102327744.0000\n",
      "Epoch 15, Test Loss: 262720211738689536.0000\n",
      "Epoch 16, Train Loss: 257697336972924384.0000\n",
      "Epoch 16, Test Loss: 399586508631179264.0000\n",
      "Epoch 17, Train Loss: 258871283064741792.0000\n",
      "Epoch 17, Test Loss: 318669290414800896.0000\n",
      "Epoch 18, Train Loss: 273550505791575520.0000\n",
      "Epoch 18, Test Loss: 385818114790522880.0000\n",
      "Epoch 19, Train Loss: 252859505921065344.0000\n",
      "Epoch 19, Test Loss: 213502240168607744.0000\n",
      "Epoch 20, Train Loss: 244674661083750304.0000\n",
      "Epoch 20, Test Loss: 213266171586150400.0000\n",
      "Epoch 21, Train Loss: 257604606643173504.0000\n",
      "Epoch 21, Test Loss: 301683416313626624.0000\n",
      "Epoch 22, Train Loss: 229842239761505632.0000\n",
      "Epoch 22, Test Loss: 186014157416431616.0000\n",
      "Epoch 23, Train Loss: 244433746520076928.0000\n",
      "Epoch 23, Test Loss: 194998730783457280.0000\n",
      "Epoch 24, Train Loss: 191868603229810752.0000\n",
      "Epoch 24, Test Loss: 213515520207486976.0000\n",
      "Epoch 25, Train Loss: 191019634197978848.0000\n",
      "Epoch 25, Test Loss: 173892728914968576.0000\n",
      "Epoch 26, Train Loss: 227474852333661856.0000\n",
      "Epoch 26, Test Loss: 320381333098463232.0000\n",
      "Epoch 27, Train Loss: 210190790323357760.0000\n",
      "Epoch 27, Test Loss: 182709403420459008.0000\n",
      "Epoch 28, Train Loss: 178792482807825728.0000\n",
      "Epoch 28, Test Loss: 147778623380652032.0000\n",
      "Epoch 29, Train Loss: 166325538996154624.0000\n",
      "Epoch 29, Test Loss: 148162163960184832.0000\n",
      "Epoch 30, Train Loss: 149355829514462432.0000\n",
      "Epoch 30, Test Loss: 188518552846729216.0000\n",
      "Epoch 31, Train Loss: 139389368551002832.0000\n",
      "Epoch 31, Test Loss: 184668870580109312.0000\n",
      "Epoch 32, Train Loss: 146215445938051104.0000\n",
      "Epoch 32, Test Loss: 178459722259628032.0000\n",
      "Epoch 33, Train Loss: 126849756840612944.0000\n",
      "Epoch 33, Test Loss: 129698786749448192.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:59:26,951]\u001b[0m Trial 43 finished with value: 1.1013813129392947e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 398, 'output_activation_class': 'LeakyReLU', 'lr': 0.0026302894514838634, 'batch_size': 95, 'epochs': 34}. Best is trial 43 with value: 1.1013813129392947e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Train Loss: 135889793514622544.0000\n",
      "Epoch 34, Test Loss: 110138131293929472.0000\n",
      "Epoch 1, Train Loss: 494184265030972672.0000\n",
      "Epoch 1, Test Loss: 494396606020321280.0000\n",
      "Epoch 2, Train Loss: 480006364994141440.0000\n",
      "Epoch 2, Test Loss: 480909996394020864.0000\n",
      "Epoch 3, Train Loss: 471665934444521600.0000\n",
      "Epoch 3, Test Loss: 471579746879143936.0000\n",
      "Epoch 4, Train Loss: 462207874098775360.0000\n",
      "Epoch 4, Test Loss: 464433058737553408.0000\n",
      "Epoch 5, Train Loss: 454702551136503936.0000\n",
      "Epoch 5, Test Loss: 448628128844087296.0000\n",
      "Epoch 6, Train Loss: 437259433848720128.0000\n",
      "Epoch 6, Test Loss: 430158429441490944.0000\n",
      "Epoch 7, Train Loss: 413859073610216512.0000\n",
      "Epoch 7, Test Loss: 433399824079912960.0000\n",
      "Epoch 8, Train Loss: 397603582458893120.0000\n",
      "Epoch 8, Test Loss: 382497727113592832.0000\n",
      "Epoch 9, Train Loss: 370418447288218496.0000\n",
      "Epoch 9, Test Loss: 355769836392939520.0000\n",
      "Epoch 10, Train Loss: 348716493101340544.0000\n",
      "Epoch 10, Test Loss: 330361497064570880.0000\n",
      "Epoch 11, Train Loss: 331262115962370048.0000\n",
      "Epoch 11, Test Loss: 329814077712891904.0000\n",
      "Epoch 12, Train Loss: 318890043488319872.0000\n",
      "Epoch 12, Test Loss: 314785849705234432.0000\n",
      "Epoch 13, Train Loss: 296858267794881984.0000\n",
      "Epoch 13, Test Loss: 349634389711257600.0000\n",
      "Epoch 14, Train Loss: 325604831534782336.0000\n",
      "Epoch 14, Test Loss: 322072278902767616.0000\n",
      "Epoch 15, Train Loss: 279349665595669216.0000\n",
      "Epoch 15, Test Loss: 266753718605578240.0000\n",
      "Epoch 16, Train Loss: 249332179693243712.0000\n",
      "Epoch 16, Test Loss: 232813409563836416.0000\n",
      "Epoch 17, Train Loss: 250872672858663040.0000\n",
      "Epoch 17, Test Loss: 219034140865986560.0000\n",
      "Epoch 18, Train Loss: 252974908027497312.0000\n",
      "Epoch 18, Test Loss: 298666253327794176.0000\n",
      "Epoch 19, Train Loss: 232101022619664384.0000\n",
      "Epoch 19, Test Loss: 214897417345040384.0000\n",
      "Epoch 20, Train Loss: 235304609837554304.0000\n",
      "Epoch 20, Test Loss: 197959767136665600.0000\n",
      "Epoch 21, Train Loss: 200955109513767232.0000\n",
      "Epoch 21, Test Loss: 190632243692044288.0000\n",
      "Epoch 22, Train Loss: 201693376269100992.0000\n",
      "Epoch 22, Test Loss: 200386045701783552.0000\n",
      "Epoch 23, Train Loss: 205385661960900480.0000\n",
      "Epoch 23, Test Loss: 185148446628380672.0000\n",
      "Epoch 24, Train Loss: 187620866367910592.0000\n",
      "Epoch 24, Test Loss: 212545218375843840.0000\n",
      "Epoch 25, Train Loss: 186900650473822080.0000\n",
      "Epoch 25, Test Loss: 155614877111549952.0000\n",
      "Epoch 26, Train Loss: 195433836676900992.0000\n",
      "Epoch 26, Test Loss: 163428762652770304.0000\n",
      "Epoch 27, Train Loss: 166981712155632288.0000\n",
      "Epoch 27, Test Loss: 145956526454996992.0000\n",
      "Epoch 28, Train Loss: 151220245725217152.0000\n",
      "Epoch 28, Test Loss: 142529477560238080.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:59:42,783]\u001b[0m Trial 44 finished with value: 1.8810360746632806e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 387, 'output_activation_class': 'LeakyReLU', 'lr': 0.0023514712630467584, 'batch_size': 93, 'epochs': 29}. Best is trial 43 with value: 1.1013813129392947e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Train Loss: 160593851588527392.0000\n",
      "Epoch 29, Test Loss: 188103607466328064.0000\n",
      "Epoch 1, Train Loss: 496958823389064320.0000\n",
      "Epoch 1, Test Loss: 500558647139500032.0000\n",
      "Epoch 2, Train Loss: 498191684338301120.0000\n",
      "Epoch 2, Test Loss: 500558647139500032.0000\n",
      "Epoch 3, Train Loss: 496958408960398400.0000\n",
      "Epoch 3, Test Loss: 500558647139500032.0000\n",
      "Epoch 4, Train Loss: 498527182076307776.0000\n",
      "Epoch 4, Test Loss: 500558647139500032.0000\n",
      "Epoch 5, Train Loss: 496965015479772672.0000\n",
      "Epoch 5, Test Loss: 500558647139500032.0000\n",
      "Epoch 6, Train Loss: 499986604400733760.0000\n",
      "Epoch 6, Test Loss: 500558647139500032.0000\n",
      "Epoch 7, Train Loss: 497059036054988928.0000\n",
      "Epoch 7, Test Loss: 500558647139500032.0000\n",
      "Epoch 8, Train Loss: 496956124955871680.0000\n",
      "Epoch 8, Test Loss: 500558647139500032.0000\n",
      "Epoch 9, Train Loss: 509462897963269760.0000\n",
      "Epoch 9, Test Loss: 500558647139500032.0000\n",
      "Epoch 10, Train Loss: 497259442757651072.0000\n",
      "Epoch 10, Test Loss: 500558647139500032.0000\n",
      "Epoch 11, Train Loss: 497578822006728192.0000\n",
      "Epoch 11, Test Loss: 500558647139500032.0000\n",
      "Epoch 12, Train Loss: 498969235291244160.0000\n",
      "Epoch 12, Test Loss: 500558647139500032.0000\n",
      "Epoch 13, Train Loss: 497131558324311296.0000\n",
      "Epoch 13, Test Loss: 500558647139500032.0000\n",
      "Epoch 14, Train Loss: 497003130669246208.0000\n",
      "Epoch 14, Test Loss: 500558647139500032.0000\n",
      "Epoch 15, Train Loss: 496960131094474688.0000\n",
      "Epoch 15, Test Loss: 500558647139500032.0000\n",
      "Epoch 16, Train Loss: 497209254125967040.0000\n",
      "Epoch 16, Test Loss: 500558647139500032.0000\n",
      "Epoch 17, Train Loss: 518772960014790208.0000\n",
      "Epoch 17, Test Loss: 500558647139500032.0000\n",
      "Epoch 18, Train Loss: 513690497551390592.0000\n",
      "Epoch 18, Test Loss: 500558647139500032.0000\n",
      "Epoch 19, Train Loss: 496961784662883136.0000\n",
      "Epoch 19, Test Loss: 500558647139500032.0000\n",
      "Epoch 20, Train Loss: 497022239437889664.0000\n",
      "Epoch 20, Test Loss: 500558647139500032.0000\n",
      "Epoch 21, Train Loss: 499219164454851456.0000\n",
      "Epoch 21, Test Loss: 500558647139500032.0000\n",
      "Epoch 22, Train Loss: 496955677289343552.0000\n",
      "Epoch 22, Test Loss: 500558647139500032.0000\n",
      "Epoch 23, Train Loss: 497027868008034048.0000\n",
      "Epoch 23, Test Loss: 500558647139500032.0000\n",
      "Epoch 24, Train Loss: 496956703089815488.0000\n",
      "Epoch 24, Test Loss: 500558647139500032.0000\n",
      "Epoch 25, Train Loss: 496966208744023168.0000\n",
      "Epoch 25, Test Loss: 500558647139500032.0000\n",
      "Epoch 26, Train Loss: 531705977034101440.0000\n",
      "Epoch 26, Test Loss: 500558647139500032.0000\n",
      "Epoch 27, Train Loss: 498496221643100416.0000\n",
      "Epoch 27, Test Loss: 500558647139500032.0000\n",
      "Epoch 28, Train Loss: 498053015022809216.0000\n",
      "Epoch 28, Test Loss: 500558647139500032.0000\n",
      "Epoch 29, Train Loss: 497211441865164416.0000\n",
      "Epoch 29, Test Loss: 500558647139500032.0000\n",
      "Epoch 30, Train Loss: 497224289548391488.0000\n",
      "Epoch 30, Test Loss: 500558647139500032.0000\n",
      "Epoch 31, Train Loss: 499702596765492992.0000\n",
      "Epoch 31, Test Loss: 500558647139500032.0000\n",
      "Epoch 32, Train Loss: 497450108004086720.0000\n",
      "Epoch 32, Test Loss: 500558647139500032.0000\n",
      "Epoch 33, Train Loss: 497097277493198656.0000\n",
      "Epoch 33, Test Loss: 500558647139500032.0000\n",
      "Epoch 34, Train Loss: 497096152082672768.0000\n",
      "Epoch 34, Test Loss: 500558647139500032.0000\n",
      "Epoch 35, Train Loss: 497244950258784768.0000\n",
      "Epoch 35, Test Loss: 500558647139500032.0000\n",
      "Epoch 36, Train Loss: 496958175936336128.0000\n",
      "Epoch 36, Test Loss: 500558647139500032.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 14:59:52,028]\u001b[0m Trial 45 finished with value: 5.005586471395e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 192, 'output_activation_class': 'Tanh', 'lr': 0.0018014572367957798, 'batch_size': 102, 'epochs': 37}. Best is trial 43 with value: 1.1013813129392947e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Train Loss: 497221980179050240.0000\n",
      "Epoch 37, Test Loss: 500558647139500032.0000\n",
      "Epoch 1, Train Loss: 502483982164274048.0000\n",
      "Epoch 1, Test Loss: 500213331768901632.0000\n",
      "Epoch 2, Train Loss: 495289601113929088.0000\n",
      "Epoch 2, Test Loss: 487201642446323712.0000\n",
      "Epoch 3, Train Loss: 483362108154980224.0000\n",
      "Epoch 3, Test Loss: 482334826024665088.0000\n",
      "Epoch 4, Train Loss: 481854155833475072.0000\n",
      "Epoch 4, Test Loss: 481410720861257728.0000\n",
      "Epoch 5, Train Loss: 480762766366435328.0000\n",
      "Epoch 5, Test Loss: 480464900343201792.0000\n",
      "Epoch 6, Train Loss: 479785119149569728.0000\n",
      "Epoch 6, Test Loss: 479324912943628288.0000\n",
      "Epoch 7, Train Loss: 478579305736090624.0000\n",
      "Epoch 7, Test Loss: 478241309874716672.0000\n",
      "Epoch 8, Train Loss: 480703299877566720.0000\n",
      "Epoch 8, Test Loss: 477000236124864512.0000\n",
      "Epoch 9, Train Loss: 476000016884365184.0000\n",
      "Epoch 9, Test Loss: 475457827469524992.0000\n",
      "Epoch 10, Train Loss: 474828641415416832.0000\n",
      "Epoch 10, Test Loss: 474236579288711168.0000\n",
      "Epoch 11, Train Loss: 477454719959447616.0000\n",
      "Epoch 11, Test Loss: 472803331522166784.0000\n",
      "Epoch 12, Train Loss: 472563592488976768.0000\n",
      "Epoch 12, Test Loss: 471412071355908096.0000\n",
      "Epoch 13, Train Loss: 471043565970159168.0000\n",
      "Epoch 13, Test Loss: 470002428729622528.0000\n",
      "Epoch 14, Train Loss: 471846517586941056.0000\n",
      "Epoch 14, Test Loss: 468349106838831104.0000\n",
      "Epoch 15, Train Loss: 467458693699768896.0000\n",
      "Epoch 15, Test Loss: 466715644876816384.0000\n",
      "Epoch 16, Train Loss: 465354823131985088.0000\n",
      "Epoch 16, Test Loss: 464378976509362176.0000\n",
      "Epoch 17, Train Loss: 463190149148768128.0000\n",
      "Epoch 17, Test Loss: 461915761225498624.0000\n",
      "Epoch 18, Train Loss: 460126394688090816.0000\n",
      "Epoch 18, Test Loss: 459084140826853376.0000\n",
      "Epoch 19, Train Loss: 457137038618462336.0000\n",
      "Epoch 19, Test Loss: 455152046727757824.0000\n",
      "Epoch 20, Train Loss: 453842762018217920.0000\n",
      "Epoch 20, Test Loss: 450836257430306816.0000\n",
      "Epoch 21, Train Loss: 448876424997847232.0000\n",
      "Epoch 21, Test Loss: 446289330172854272.0000\n",
      "Epoch 22, Train Loss: 443831306100327680.0000\n",
      "Epoch 22, Test Loss: 441061427260686336.0000\n",
      "Epoch 23, Train Loss: 438400508708908800.0000\n",
      "Epoch 23, Test Loss: 435186496115310592.0000\n",
      "Epoch 24, Train Loss: 432301535337880256.0000\n",
      "Epoch 24, Test Loss: 427742321358929920.0000\n",
      "Epoch 25, Train Loss: 425772037807918400.0000\n",
      "Epoch 25, Test Loss: 422044342866149376.0000\n",
      "Epoch 26, Train Loss: 418240355488152000.0000\n",
      "Epoch 26, Test Loss: 415102541844447232.0000\n",
      "Epoch 27, Train Loss: 409291230456713472.0000\n",
      "Epoch 27, Test Loss: 404927352003362816.0000\n",
      "Epoch 28, Train Loss: 402264108611746560.0000\n",
      "Epoch 28, Test Loss: 396083705103253504.0000\n",
      "Epoch 29, Train Loss: 394521257440239872.0000\n",
      "Epoch 29, Test Loss: 388318953987899392.0000\n",
      "Epoch 30, Train Loss: 386186230478785856.0000\n",
      "Epoch 30, Test Loss: 379206854451920896.0000\n",
      "Epoch 31, Train Loss: 381402504093257408.0000\n",
      "Epoch 31, Test Loss: 371397847993548800.0000\n",
      "Epoch 32, Train Loss: 369290062852159040.0000\n",
      "Epoch 32, Test Loss: 364321666035089408.0000\n",
      "Epoch 33, Train Loss: 366089033457972928.0000\n",
      "Epoch 33, Test Loss: 359492267368513536.0000\n",
      "Epoch 34, Train Loss: 356527144861295552.0000\n",
      "Epoch 34, Test Loss: 347816312874991616.0000\n",
      "Epoch 35, Train Loss: 348548240540895360.0000\n",
      "Epoch 35, Test Loss: 341388499179536384.0000\n",
      "Epoch 36, Train Loss: 338810981695037184.0000\n",
      "Epoch 36, Test Loss: 343866042474299392.0000\n",
      "Epoch 37, Train Loss: 333565239671603392.0000\n",
      "Epoch 37, Test Loss: 325376414423449600.0000\n",
      "Epoch 38, Train Loss: 326084040456430912.0000\n",
      "Epoch 38, Test Loss: 318713270879911936.0000\n",
      "Epoch 39, Train Loss: 323081046066235136.0000\n",
      "Epoch 39, Test Loss: 311175809433862144.0000\n",
      "Epoch 40, Train Loss: 311308671750124224.0000\n",
      "Epoch 40, Test Loss: 311600495800090624.0000\n",
      "Epoch 41, Train Loss: 303520273749848000.0000\n",
      "Epoch 41, Test Loss: 300222405878480896.0000\n",
      "Epoch 42, Train Loss: 296324683912981760.0000\n",
      "Epoch 42, Test Loss: 298156698407796736.0000\n",
      "Epoch 43, Train Loss: 291243508493627008.0000\n",
      "Epoch 43, Test Loss: 295686267578875904.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 15:00:07,451]\u001b[0m Trial 46 finished with value: 2.784095382924165e+17 and parameters: {'activation_class': 'LeakyReLU', 'hidden_sizes': 303, 'output_activation_class': None, 'lr': 0.0013162208291948433, 'batch_size': 88, 'epochs': 44}. Best is trial 43 with value: 1.1013813129392947e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, Train Loss: 284133254654003328.0000\n",
      "Epoch 44, Test Loss: 278409538292416512.0000\n",
      "Epoch 1, Train Loss: 498503789863015552.0000\n",
      "Epoch 1, Test Loss: 489330468756389888.0000\n",
      "Epoch 2, Train Loss: 480754584321250176.0000\n",
      "Epoch 2, Test Loss: 476200100897488896.0000\n",
      "Epoch 3, Train Loss: 471519095373981696.0000\n",
      "Epoch 3, Test Loss: 467523167447941120.0000\n",
      "Epoch 4, Train Loss: 457554570295605888.0000\n",
      "Epoch 4, Test Loss: 449252514009710592.0000\n",
      "Epoch 5, Train Loss: 437583377807894016.0000\n",
      "Epoch 5, Test Loss: 443589891687710720.0000\n",
      "Epoch 6, Train Loss: 419410643944073664.0000\n",
      "Epoch 6, Test Loss: 402544297989111808.0000\n",
      "Epoch 7, Train Loss: 387039482475689408.0000\n",
      "Epoch 7, Test Loss: 362929203277987840.0000\n",
      "Epoch 8, Train Loss: 359741455543254784.0000\n",
      "Epoch 8, Test Loss: 359196979776978944.0000\n",
      "Epoch 9, Train Loss: 342666852650291392.0000\n",
      "Epoch 9, Test Loss: 327365671476264960.0000\n",
      "Epoch 10, Train Loss: 327886336846417408.0000\n",
      "Epoch 10, Test Loss: 295444890416840704.0000\n",
      "Epoch 11, Train Loss: 299795057247207168.0000\n",
      "Epoch 11, Test Loss: 308786295788797952.0000\n",
      "Epoch 12, Train Loss: 287510020432532096.0000\n",
      "Epoch 12, Test Loss: 302186133645688832.0000\n",
      "Epoch 13, Train Loss: 291684481029556288.0000\n",
      "Epoch 13, Test Loss: 242729320819195904.0000\n",
      "Epoch 14, Train Loss: 300312219881874240.0000\n",
      "Epoch 14, Test Loss: 236951009258110976.0000\n",
      "Epoch 15, Train Loss: 255930514369447328.0000\n",
      "Epoch 15, Test Loss: 230335986528157696.0000\n",
      "Epoch 16, Train Loss: 232101749297968352.0000\n",
      "Epoch 16, Test Loss: 203192875549196288.0000\n",
      "Epoch 17, Train Loss: 213331568878450368.0000\n",
      "Epoch 17, Test Loss: 187546756366467072.0000\n",
      "Epoch 18, Train Loss: 235615662319747776.0000\n",
      "Epoch 18, Test Loss: 203783759969910784.0000\n",
      "Epoch 19, Train Loss: 199900573566952960.0000\n",
      "Epoch 19, Test Loss: 233498766085193728.0000\n",
      "Epoch 20, Train Loss: 206938634643528896.0000\n",
      "Epoch 20, Test Loss: 165363490800795648.0000\n",
      "Epoch 21, Train Loss: 246022302764010752.0000\n",
      "Epoch 21, Test Loss: 253932158895259648.0000\n",
      "Epoch 22, Train Loss: 202447582045826880.0000\n",
      "Epoch 22, Test Loss: 174093321067560960.0000\n",
      "Epoch 23, Train Loss: 171705358256796416.0000\n",
      "Epoch 23, Test Loss: 263283007073288192.0000\n",
      "Epoch 24, Train Loss: 171898486190977088.0000\n",
      "Epoch 24, Test Loss: 137566900547747840.0000\n",
      "Epoch 25, Train Loss: 178942604601637248.0000\n",
      "Epoch 25, Test Loss: 139911540374503424.0000\n",
      "Epoch 26, Train Loss: 188204818590464160.0000\n",
      "Epoch 26, Test Loss: 130353966830583808.0000\n",
      "Epoch 27, Train Loss: 155798098008258944.0000\n",
      "Epoch 27, Test Loss: 128941532475555840.0000\n",
      "Epoch 28, Train Loss: 180038197121844160.0000\n",
      "Epoch 28, Test Loss: 262936970148184064.0000\n",
      "Epoch 29, Train Loss: 177457808162008672.0000\n",
      "Epoch 29, Test Loss: 119021695620087808.0000\n",
      "Epoch 30, Train Loss: 162274935293455968.0000\n",
      "Epoch 30, Test Loss: 126567789950402560.0000\n",
      "Epoch 31, Train Loss: 192750938248176224.0000\n",
      "Epoch 31, Test Loss: 500558647139500032.0000\n",
      "Epoch 32, Train Loss: 500463546589207744.0000\n",
      "Epoch 32, Test Loss: 500558647139500032.0000\n",
      "Epoch 33, Train Loss: 500308510297444736.0000\n",
      "Epoch 33, Test Loss: 500558647139500032.0000\n",
      "Epoch 34, Train Loss: 500423197977461952.0000\n",
      "Epoch 34, Test Loss: 500558647139500032.0000\n",
      "Epoch 35, Train Loss: 500562005644420352.0000\n",
      "Epoch 35, Test Loss: 500558647139500032.0000\n",
      "Epoch 36, Train Loss: 500327949443188800.0000\n",
      "Epoch 36, Test Loss: 500558647139500032.0000\n",
      "Epoch 37, Train Loss: 500448321388659904.0000\n",
      "Epoch 37, Test Loss: 500558647139500032.0000\n",
      "Epoch 38, Train Loss: 500362348257030208.0000\n",
      "Epoch 38, Test Loss: 500558647139500032.0000\n",
      "Epoch 39, Train Loss: 500332075836544128.0000\n",
      "Epoch 39, Test Loss: 500558647139500032.0000\n",
      "Epoch 40, Train Loss: 500509022416273408.0000\n",
      "Epoch 40, Test Loss: 500558647139500032.0000\n",
      "Epoch 41, Train Loss: 500314172525052864.0000\n",
      "Epoch 41, Test Loss: 500558647139500032.0000\n",
      "Epoch 42, Train Loss: 500315753079825920.0000\n",
      "Epoch 42, Test Loss: 500558647139500032.0000\n",
      "Epoch 43, Train Loss: 501032391848379072.0000\n",
      "Epoch 43, Test Loss: 500558647139500032.0000\n",
      "Epoch 44, Train Loss: 500333061916685056.0000\n",
      "Epoch 44, Test Loss: 500558647139500032.0000\n",
      "Epoch 45, Train Loss: 500319407178867776.0000\n",
      "Epoch 45, Test Loss: 500558647139500032.0000\n",
      "Epoch 46, Train Loss: 500349777902168512.0000\n",
      "Epoch 46, Test Loss: 500558647139500032.0000\n",
      "Epoch 47, Train Loss: 500313536038326656.0000\n",
      "Epoch 47, Test Loss: 500558647139500032.0000\n",
      "Epoch 48, Train Loss: 500341041812251456.0000\n",
      "Epoch 48, Test Loss: 500558647139500032.0000\n",
      "Epoch 49, Train Loss: 500341938131855360.0000\n",
      "Epoch 49, Test Loss: 500558647139500032.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 15:00:29,891]\u001b[0m Trial 47 finished with value: 5.005586471395e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 350, 'output_activation_class': 'ReLU', 'lr': 0.004205236707042736, 'batch_size': 116, 'epochs': 50}. Best is trial 43 with value: 1.1013813129392947e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Train Loss: 500343411315363776.0000\n",
      "Epoch 50, Test Loss: 500558647139500032.0000\n",
      "Epoch 1, Train Loss: 503176402261372160.0000\n",
      "Epoch 1, Test Loss: 500554901928017920.0000\n",
      "Epoch 2, Train Loss: 498444760813520640.0000\n",
      "Epoch 2, Test Loss: 500311634980372480.0000\n",
      "Epoch 3, Train Loss: 496752732809189888.0000\n",
      "Epoch 3, Test Loss: 497736922345504768.0000\n",
      "Epoch 4, Train Loss: 502752557154464192.0000\n",
      "Epoch 4, Test Loss: 488315241566830592.0000\n",
      "Epoch 5, Train Loss: 483831379287682944.0000\n",
      "Epoch 5, Test Loss: 483023876217896960.0000\n",
      "Epoch 6, Train Loss: 479899052102104128.0000\n",
      "Epoch 6, Test Loss: 482074722805219328.0000\n",
      "Epoch 7, Train Loss: 480998155886943936.0000\n",
      "Epoch 7, Test Loss: 481399691385241600.0000\n",
      "Epoch 8, Train Loss: 478566695523319808.0000\n",
      "Epoch 8, Test Loss: 480763967505956864.0000\n",
      "Epoch 9, Train Loss: 483062210570699328.0000\n",
      "Epoch 9, Test Loss: 480251766886105088.0000\n",
      "Epoch 10, Train Loss: 477535604355065920.0000\n",
      "Epoch 10, Test Loss: 479600546764816384.0000\n",
      "Epoch 11, Train Loss: 476972248547119232.0000\n",
      "Epoch 11, Test Loss: 478972004070850560.0000\n",
      "Epoch 12, Train Loss: 476395671159570432.0000\n",
      "Epoch 12, Test Loss: 478356586796941312.0000\n",
      "Epoch 13, Train Loss: 476349106257541376.0000\n",
      "Epoch 13, Test Loss: 477774154871865344.0000\n",
      "Epoch 14, Train Loss: 475229530471316928.0000\n",
      "Epoch 14, Test Loss: 477063423683723264.0000\n",
      "Epoch 15, Train Loss: 474460351260495424.0000\n",
      "Epoch 15, Test Loss: 476410348136562688.0000\n",
      "Epoch 16, Train Loss: 479401445641935360.0000\n",
      "Epoch 16, Test Loss: 475726520623562752.0000\n",
      "Epoch 17, Train Loss: 472917892358015360.0000\n",
      "Epoch 17, Test Loss: 474986136981209088.0000\n",
      "Epoch 18, Train Loss: 472943006841248896.0000\n",
      "Epoch 18, Test Loss: 474239259348303872.0000\n",
      "Epoch 19, Train Loss: 479193860149190976.0000\n",
      "Epoch 19, Test Loss: 473436066104213504.0000\n",
      "Epoch 20, Train Loss: 472173280300474688.0000\n",
      "Epoch 20, Test Loss: 472529209529466880.0000\n",
      "Epoch 21, Train Loss: 485335748029118080.0000\n",
      "Epoch 21, Test Loss: 471660389185093632.0000\n",
      "Epoch 22, Train Loss: 470805848016250944.0000\n",
      "Epoch 22, Test Loss: 470673027743350784.0000\n",
      "Epoch 23, Train Loss: 468866785967846464.0000\n",
      "Epoch 23, Test Loss: 469582552726765568.0000\n",
      "Epoch 24, Train Loss: 467683879516839040.0000\n",
      "Epoch 24, Test Loss: 468449574713819136.0000\n",
      "Epoch 25, Train Loss: 465726094314103680.0000\n",
      "Epoch 25, Test Loss: 467259593894920192.0000\n",
      "Epoch 26, Train Loss: 464369052510206208.0000\n",
      "Epoch 26, Test Loss: 466046866929221632.0000\n",
      "Epoch 27, Train Loss: 464171219038968960.0000\n",
      "Epoch 27, Test Loss: 464697147686649856.0000\n",
      "Epoch 28, Train Loss: 462717267024660224.0000\n",
      "Epoch 28, Test Loss: 463238782951358464.0000\n",
      "Epoch 29, Train Loss: 460364537945148352.0000\n",
      "Epoch 29, Test Loss: 461632396463177728.0000\n",
      "Epoch 30, Train Loss: 458948893619053440.0000\n",
      "Epoch 30, Test Loss: 460094007897227264.0000\n",
      "Epoch 31, Train Loss: 457163114061952640.0000\n",
      "Epoch 31, Test Loss: 458212537343672320.0000\n",
      "Epoch 32, Train Loss: 455065676287554624.0000\n",
      "Epoch 32, Test Loss: 456248465979080704.0000\n",
      "Epoch 33, Train Loss: 453615761343663040.0000\n",
      "Epoch 33, Test Loss: 454344592876109824.0000\n",
      "Epoch 34, Train Loss: 451818764445495168.0000\n",
      "Epoch 34, Test Loss: 452121895760822272.0000\n",
      "Epoch 35, Train Loss: 452681276795261696.0000\n",
      "Epoch 35, Test Loss: 449827043194961920.0000\n",
      "Epoch 36, Train Loss: 446422609331195456.0000\n",
      "Epoch 36, Test Loss: 447276244937998336.0000\n",
      "Epoch 37, Train Loss: 444244519070184512.0000\n",
      "Epoch 37, Test Loss: 444546535523352576.0000\n",
      "Epoch 38, Train Loss: 441967954554606912.0000\n",
      "Epoch 38, Test Loss: 441839950212628480.0000\n",
      "Epoch 39, Train Loss: 438452219158530432.0000\n",
      "Epoch 39, Test Loss: 438768464480436224.0000\n",
      "Epoch 40, Train Loss: 435464530743680704.0000\n",
      "Epoch 40, Test Loss: 435670624828915712.0000\n",
      "Epoch 41, Train Loss: 433407287699099776.0000\n",
      "Epoch 41, Test Loss: 432050207916556288.0000\n",
      "Epoch 42, Train Loss: 430761010211430720.0000\n",
      "Epoch 42, Test Loss: 428338668978044928.0000\n",
      "Epoch 43, Train Loss: 424921347587688192.0000\n",
      "Epoch 43, Test Loss: 424626717722673152.0000\n",
      "Epoch 44, Train Loss: 427040475861600000.0000\n",
      "Epoch 44, Test Loss: 420942426056687616.0000\n",
      "Epoch 45, Train Loss: 416818611397979392.0000\n",
      "Epoch 45, Test Loss: 416763079280295936.0000\n",
      "Epoch 46, Train Loss: 412945834436033344.0000\n",
      "Epoch 46, Test Loss: 411418215818723328.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 15:00:37,875]\u001b[0m Trial 48 finished with value: 4.06889980258943e+17 and parameters: {'activation_class': 'ReLU', 'hidden_sizes': 103, 'output_activation_class': 'ReLU', 'lr': 0.002221940628476254, 'batch_size': 149, 'epochs': 47}. Best is trial 43 with value: 1.1013813129392947e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, Train Loss: 414390207122311552.0000\n",
      "Epoch 47, Test Loss: 406889980258942976.0000\n",
      "Epoch 1, Train Loss: 492736424321636736.0000\n",
      "Epoch 1, Test Loss: 500558647139500032.0000\n",
      "Epoch 2, Train Loss: 492736399642085760.0000\n",
      "Epoch 2, Test Loss: 500558647139500032.0000\n",
      "Epoch 3, Train Loss: 492736412508613248.0000\n",
      "Epoch 3, Test Loss: 500558647139500032.0000\n",
      "Epoch 4, Train Loss: 492736405211145536.0000\n",
      "Epoch 4, Test Loss: 500558647139500032.0000\n",
      "Epoch 5, Train Loss: 492736419568715264.0000\n",
      "Epoch 5, Test Loss: 500558647139500032.0000\n",
      "Epoch 6, Train Loss: 492736423411698816.0000\n",
      "Epoch 6, Test Loss: 500558647139500032.0000\n",
      "Epoch 7, Train Loss: 492736408944762112.0000\n",
      "Epoch 7, Test Loss: 500558647139500032.0000\n",
      "Epoch 8, Train Loss: 492736417081868608.0000\n",
      "Epoch 8, Test Loss: 500558647139500032.0000\n",
      "Epoch 9, Train Loss: 493679531130330752.0000\n",
      "Epoch 9, Test Loss: 500558647139500032.0000\n",
      "Epoch 10, Train Loss: 492736406987854016.0000\n",
      "Epoch 10, Test Loss: 500558647139500032.0000\n",
      "Epoch 11, Train Loss: 492736431001902208.0000\n",
      "Epoch 11, Test Loss: 500558647139500032.0000\n",
      "Epoch 12, Train Loss: 492770795099412864.0000\n",
      "Epoch 12, Test Loss: 500558647139500032.0000\n",
      "Epoch 13, Train Loss: 492736409452758016.0000\n",
      "Epoch 13, Test Loss: 500558647139500032.0000\n",
      "Epoch 14, Train Loss: 492736418200349696.0000\n",
      "Epoch 14, Test Loss: 500558647139500032.0000\n",
      "Epoch 15, Train Loss: 492736414719217536.0000\n",
      "Epoch 15, Test Loss: 500558647139500032.0000\n",
      "Epoch 16, Train Loss: 492748490462362112.0000\n",
      "Epoch 16, Test Loss: 500558647139500032.0000\n",
      "Epoch 17, Train Loss: 492736416467484928.0000\n",
      "Epoch 17, Test Loss: 500558647139500032.0000\n",
      "Epoch 18, Train Loss: 492736414423078720.0000\n",
      "Epoch 18, Test Loss: 500558647139500032.0000\n",
      "Epoch 19, Train Loss: 492738886443063104.0000\n",
      "Epoch 19, Test Loss: 500558647139500032.0000\n",
      "Epoch 20, Train Loss: 492736622318101120.0000\n",
      "Epoch 20, Test Loss: 500558647139500032.0000\n",
      "Epoch 21, Train Loss: 492736414926410176.0000\n",
      "Epoch 21, Test Loss: 500558647139500032.0000\n",
      "Epoch 22, Train Loss: 492736417516484032.0000\n",
      "Epoch 22, Test Loss: 500558647139500032.0000\n",
      "Epoch 23, Train Loss: 492737025707868160.0000\n",
      "Epoch 23, Test Loss: 500558647139500032.0000\n",
      "Epoch 24, Train Loss: 492736412923249472.0000\n",
      "Epoch 24, Test Loss: 500558647139500032.0000\n",
      "Epoch 25, Train Loss: 492736415983626688.0000\n",
      "Epoch 25, Test Loss: 500558647139500032.0000\n",
      "Epoch 26, Train Loss: 492736439259556800.0000\n",
      "Epoch 26, Test Loss: 500558647139500032.0000\n",
      "Epoch 27, Train Loss: 492736415612595648.0000\n",
      "Epoch 27, Test Loss: 500558647139500032.0000\n",
      "Epoch 28, Train Loss: 492736416157295424.0000\n",
      "Epoch 28, Test Loss: 500558647139500032.0000\n",
      "Epoch 29, Train Loss: 492736412917195136.0000\n",
      "Epoch 29, Test Loss: 500558647139500032.0000\n",
      "Epoch 30, Train Loss: 492736401325156672.0000\n",
      "Epoch 30, Test Loss: 500558647139500032.0000\n",
      "Epoch 31, Train Loss: 492736422872450496.0000\n",
      "Epoch 31, Test Loss: 500558647139500032.0000\n",
      "Epoch 32, Train Loss: 492736414395238848.0000\n",
      "Epoch 32, Test Loss: 500558647139500032.0000\n",
      "Epoch 33, Train Loss: 492736407933487872.0000\n",
      "Epoch 33, Test Loss: 500558647139500032.0000\n",
      "Epoch 34, Train Loss: 492736413920297984.0000\n",
      "Epoch 34, Test Loss: 500558647139500032.0000\n",
      "Epoch 35, Train Loss: 492736413127771648.0000\n",
      "Epoch 35, Test Loss: 500558647139500032.0000\n",
      "Epoch 36, Train Loss: 492736414094273728.0000\n",
      "Epoch 36, Test Loss: 500558647139500032.0000\n",
      "Epoch 37, Train Loss: 492736406120354624.0000\n",
      "Epoch 37, Test Loss: 500558647139500032.0000\n",
      "Epoch 38, Train Loss: 492736401153100480.0000\n",
      "Epoch 38, Test Loss: 500558647139500032.0000\n",
      "Epoch 39, Train Loss: 492736408384722752.0000\n",
      "Epoch 39, Test Loss: 500558647139500032.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 15:00:58,115]\u001b[0m Trial 49 finished with value: 5.005586471395e+17 and parameters: {'activation_class': 'ELU', 'hidden_sizes': 495, 'output_activation_class': 'Sigmoid', 'lr': 0.0030489810431101392, 'batch_size': 129, 'epochs': 40}. Best is trial 43 with value: 1.1013813129392947e+17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Train Loss: 497746315017153984.0000\n",
      "Epoch 40, Test Loss: 500558647139500032.0000\n"
     ]
    }
   ],
   "source": [
    "# Print the function name\n",
    "i = 2 # NOTE: function h\n",
    "print(f\"Approximating function {function_names[i]}\")\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(tune_network, n_trials=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration: {'activation_class': 'ELU', 'hidden_sizes': 398, 'output_activation_class': 'LeakyReLU', 'lr': 0.0026302894514838634, 'batch_size': 95, 'epochs': 34}\n"
     ]
    }
   ],
   "source": [
    "# Get the best set of hyperparameters\n",
    "best_trial = study.best_trial\n",
    "best_config = best_trial.params\n",
    "print(f\"Best configuration: {best_config}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.activation.ELU"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'E'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(torch.nn, 'ELU')\n",
    "len(best_config['activation_class'])\n",
    "best_config['activation_class'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network with the best hyperparameters\n",
    "#best_activation_classes = [getattr(torch.nn, act_class_name) for act_class_name in best_config[\"activation_class\"]]\n",
    "#best_hidden_sizes = best_config[\"hidden_sizes\"]\n",
    "#best_output_activation_class = getattr(torch.nn, best_config[\"output_activation_class\"]) if best_config[\"output_activation_class\"] else None\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will modify this cell to account for the fact that best_config['activation_class'] is not a list as we have only one hidden layer\n",
    "\n",
    "That is what I think at least, as best_config['activation_class'] returns just 'ELU'. However `create_network` is set up to work with multiple hidden layers also, so we have to still turn it into a list as we pass it to `create_network`.\n",
    "\n",
    "And so I also make a list of `best_hidden_sizes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_activation_class = getattr(torch.nn, best_config[\"output_activation_class\"]) if best_config[\"output_activation_class\"] else None\n",
    "best_hidden_sizes = best_config[\"hidden_sizes\"]\n",
    "best_output_activation_class = getattr(torch.nn, best_config[\"output_activation_class\"]) if best_config[\"output_activation_class\"] else None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network with the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.nn.modules.activation.LeakyReLU]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "398"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.activation.LeakyReLU"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[best_activation_class]\n",
    "best_hidden_sizes\n",
    "best_output_activation_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 510782872011512192.0000\n",
      "Epoch 1, Test Loss: 500558509700546560.0000\n",
      "Epoch 2, Train Loss: 495909373986508928.0000\n",
      "Epoch 2, Test Loss: 500558337901854720.0000\n",
      "Epoch 3, Train Loss: 496039182151635648.0000\n",
      "Epoch 3, Test Loss: 500558028664209408.0000\n",
      "Epoch 4, Train Loss: 496157790633212992.0000\n",
      "Epoch 4, Test Loss: 500557581987610624.0000\n",
      "Epoch 5, Train Loss: 495923389212775872.0000\n",
      "Epoch 5, Test Loss: 500556997872058368.0000\n",
      "Epoch 6, Train Loss: 496220568679769728.0000\n",
      "Epoch 6, Test Loss: 500556276317552640.0000\n",
      "Epoch 7, Train Loss: 495906839134016768.0000\n",
      "Epoch 7, Test Loss: 500555382964355072.0000\n",
      "Epoch 8, Train Loss: 495987729664613504.0000\n",
      "Epoch 8, Test Loss: 500554523970895872.0000\n",
      "Epoch 9, Train Loss: 500366885237798848.0000\n",
      "Epoch 9, Test Loss: 500553424459268096.0000\n",
      "Epoch 10, Train Loss: 495920627270105408.0000\n",
      "Epoch 10, Test Loss: 500552187508686848.0000\n",
      "Epoch 11, Train Loss: 495920511958128640.0000\n",
      "Epoch 11, Test Loss: 500550881838628864.0000\n",
      "Epoch 12, Train Loss: 495957177624962112.0000\n",
      "Epoch 12, Test Loss: 500549507449094144.0000\n",
      "Epoch 13, Train Loss: 558986975994526016.0000\n",
      "Epoch 13, Test Loss: 500547926901129216.0000\n",
      "Epoch 14, Train Loss: 504844811384867648.0000\n",
      "Epoch 14, Test Loss: 500546174554472448.0000\n",
      "Epoch 15, Train Loss: 495920343942205568.0000\n",
      "Epoch 15, Test Loss: 500544525287030784.0000\n",
      "Epoch 16, Train Loss: 495968062035891840.0000\n",
      "Epoch 16, Test Loss: 500542738580635648.0000\n",
      "Epoch 17, Train Loss: 520503047364619584.0000\n",
      "Epoch 17, Test Loss: 500540848795025408.0000\n",
      "Epoch 18, Train Loss: 509940490218837248.0000\n",
      "Epoch 18, Test Loss: 500538855930200064.0000\n",
      "Epoch 19, Train Loss: 497218398099637376.0000\n",
      "Epoch 19, Test Loss: 500536863065374720.0000\n",
      "Epoch 20, Train Loss: 495948844434672448.0000\n",
      "Epoch 20, Test Loss: 500534698401857536.0000\n",
      "Epoch 21, Train Loss: 495884743505672704.0000\n",
      "Epoch 21, Test Loss: 500532533738340352.0000\n",
      "Epoch 22, Train Loss: 495883738139636928.0000\n",
      "Epoch 22, Test Loss: 500530265995608064.0000\n",
      "Epoch 23, Train Loss: 495886362076481408.0000\n",
      "Epoch 23, Test Loss: 500527895173660672.0000\n",
      "Epoch 24, Train Loss: 526969609999314816.0000\n",
      "Epoch 24, Test Loss: 500525421272498176.0000\n",
      "Epoch 25, Train Loss: 498882323773671488.0000\n",
      "Epoch 25, Test Loss: 500522878651858944.0000\n",
      "Epoch 26, Train Loss: 497249071443889728.0000\n",
      "Epoch 26, Test Loss: 500520370390958080.0000\n",
      "Epoch 27, Train Loss: 495877055169895168.0000\n",
      "Epoch 27, Test Loss: 500517690331365376.0000\n",
      "Epoch 28, Train Loss: 508033010488728128.0000\n",
      "Epoch 28, Test Loss: 500515147710726144.0000\n",
      "Epoch 29, Train Loss: 495866007093806208.0000\n",
      "Epoch 29, Test Loss: 500512261492703232.0000\n",
      "Epoch 30, Train Loss: 496080520881052416.0000\n",
      "Epoch 30, Test Loss: 500509478353895424.0000\n",
      "Epoch 31, Train Loss: 495859356027416128.0000\n",
      "Epoch 31, Test Loss: 500506592135872512.0000\n",
      "Epoch 32, Train Loss: 496158452853416640.0000\n",
      "Epoch 32, Test Loss: 500503534119157760.0000\n",
      "Epoch 33, Train Loss: 495956212888287936.0000\n",
      "Epoch 33, Test Loss: 500500647901134848.0000\n",
      "Epoch 34, Train Loss: 495860800504602112.0000\n",
      "Epoch 34, Test Loss: 500497486805204992.0000\n",
      "Test loss for the best model: 5.00497486805205e+17\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArMAAAIjCAYAAAAQgZNYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACGk0lEQVR4nO3deXhTdfY/8PfN2qb7RhcoZd9Xt4qAgwMKqKi4M4yC4+hvxnXGwXGYGaUuI4riuKPjhjsu4zYOIsvXMoqguIAIshdblkJp6Zo06/39kdybpGua5ubmpu/X8+ShublJPo2xPT05n3MEURRFEBERERFpkE7tBRARERERhYvBLBERERFpFoNZIiIiItIsBrNEREREpFkMZomIiIhIsxjMEhEREZFmMZglIiIiIs1iMEtEREREmsVgloiIiIg0i8EsEVE3zZ8/H/369dPM48YKQRBQUlIS1n379euH+fPnR3Q9XVFRUYGEhARs2LBBPnbllVfi8ssvV21NRD0Vg1kiCsvy5cshCAK++eYbtZdCEST9d+3sEs9BdijuueceFBcXY+LEifKxO+64A//+97+xdetWFVdG1PMY1F4AERG17bnnnoPH44nqc5555pl49dVXg4799re/xWmnnYbrr79ePpacnNzt57LZbDAYwvs1tGvXLuh06uRjqqqq8PLLL+Pll18OOj5+/HiccsopWLp0KV555RVV1kbUEzGYJSKKMU1NTUhKSoLRaIz6cw8YMAADBgwIOva73/0OAwYMwK9//et27+dyueDxeGAymUJ+roSEhLDXaTabw75vd7322mswGAyYNWtWq9suv/xyLFq0CE8//XREAn4i6hzLDIhIUd9//z1mzpyJ1NRUJCcnY+rUqdi0aVPQOU6nE3fffTcGDx6MhIQEZGVlYdKkSVizZo18TmVlJa655hr06dMHZrMZ+fn5uPDCC3HgwIGgx/rkk08wefJkJCUlISUlBeeddx62b98edE6oj9WWDz74AKNGjUJCQgJGjRqF999/v9U5paWlEAQBpaWlQccPHDgAQRCwfPly+dj8+fORnJyMffv24dxzz0VKSgrmzp0r3xb4cb50/4cffhj/+te/MHDgQJjNZpx66qnYvHlzq3W88847GDFiRNBaI1GHG7iORx99VF7Hjh074HA4cNddd+Hkk09GWloakpKSMHnyZHz22WetHqdlzWxJSQkEQcDevXsxf/58pKenIy0tDddccw2sVmvQfVvWzErlERs2bMBtt92GnJwcJCUlYfbs2aiqqgq6r8fjQUlJCQoKCmCxWHDWWWdhx44dIdfhfvDBByguLm4zWD377LPR1NQU9N4lImUxM0tEitm+fTsmT56M1NRU/PnPf4bRaMSzzz6LKVOmYP369SguLgbgDWIWL14sf5xdX1+Pb775Bt999x3OPvtsAMAll1yC7du34+abb0a/fv1w7NgxrFmzBuXl5XJw9uqrr2LevHmYPn06HnzwQVitVixbtgyTJk3C999/L58XymO1ZfXq1bjkkkswYsQILF68GNXV1XJQ3B0ulwvTp0/HpEmT8PDDD8NisXR4/htvvIGGhgb8v//3/yAIApYsWYKLL74Y+/fvl7O5//3vf3HFFVdg9OjRWLx4MU6cOIFrr70WvXv37tZaA7300ktobm7G9ddfD7PZjMzMTNTX1+P555/HnDlzcN1116GhoQEvvPACpk+fjq+//hrjxo3r9HEvv/xy9O/fH4sXL8Z3332H559/Hr169cKDDz7Y6X1vvvlmZGRkYNGiRThw4AAeffRR3HTTTXjrrbfkcxYuXIglS5Zg1qxZmD59OrZu3Yrp06ejubm508d3Op3YvHkzfv/737d5+4gRI5CYmIgNGzZg9uzZnT4eEUWASEQUhpdeekkEIG7evLndcy666CLRZDKJ+/btk48dPnxYTElJEc8880z52NixY8Xzzjuv3cc5ceKECEB86KGH2j2noaFBTE9PF6+77rqg45WVlWJaWpp8PJTHas+4cePE/Px8sba2Vj62evVqEYBYVFQkH/vss89EAOJnn30WdP+ysjIRgPjSSy/Jx+bNmycCEP/yl7+0er558+YFPa50/6ysLLGmpkY+/uGHH4oAxP/85z/ysdGjR4t9+vQRGxoa5GOlpaWt1hqKpKQkcd68ea3WkZqaKh47dizoXJfLJdrt9qBjJ06cEHNzc8Xf/OY3QccBiIsWLZKvL1q0SATQ6rzZs2eLWVlZQceKioqC1iS9H6dNmyZ6PB75+B//+EdRr9fL/80qKytFg8EgXnTRRUGPV1JSIgIIesy27N27VwQgPvHEE+2eM2TIEHHmzJkdPg4RRQ7LDIhIEW63G6tXr8ZFF10UVIOZn5+PX/3qV/jiiy9QX18PAEhPT8f27duxZ8+eNh8rMTERJpMJpaWlOHHiRJvnrFmzBrW1tZgzZw6OHz8uX/R6PYqLi+WPuUN5rLYcOXIEW7Zswbx585CWliYfP/vsszFixIiQH6c97WX62nLFFVcgIyNDvj558mQAwP79+wEAhw8fxrZt23D11VcHfRT+i1/8AqNHj+72WiWXXHIJcnJygo7p9Xq5btbj8aCmpgYulwunnHIKvvvuu5Ae93e/+13Q9cmTJ6O6ulp+v3Tk+uuvhyAIQfd1u934+eefAQDr1q2Dy+XCDTfcEHS/m2++OaS1VVdXA0DQ699SRkYGjh8/HtLjEVH39ehg9n//+x9mzZqFgoICCIKADz74oEv3b25uxvz58zF69GgYDAZcdNFFrc6ZP39+m21tRo4cGZlvgihGVVVVwWq1YujQoa1uGz58ODweDyoqKgB42xzV1tZiyJAhGD16NG6//Xb88MMP8vlmsxkPPvggPvnkE+Tm5uLMM8/EkiVLUFlZKZ8jBcK//OUvkZOTE3RZvXo1jh07FvJjtUUKhgYPHtzqtra+x64wGAxdKlXo27dv0HUpsJKCc2mtgwYNanXfto6Fq3///m0ef/nllzFmzBi5/jknJwf//e9/UVdXF9Ljdvb9dee+7b02mZmZHQaoLYmi2OFtgQE1ESmrRwezTU1NGDt2LJ566qmw7u92u5GYmIhbbrkF06ZNa/Ocxx57DEeOHJEvFRUVyMzMxGWXXdadpRPFlTPPPBP79u3Diy++iFGjRuH555/HSSedhOeff14+5w9/+AN2796NxYsXIyEhAXfeeSeGDx+O77//HgDkFlavvvoq1qxZ0+ry4YcfhvxY3dVeION2u9s8bjabu9RmSq/Xt3m8owBLCYmJia2Ovfbaa5g/fz4GDhyIF154AatWrcKaNWvwy1/+MuQ2Y935/pR+bbKysgB0HFifOHEC2dnZEXk+Iupcjw5mZ86cifvuu6/dIn273Y4FCxagd+/eSEpKQnFxcdDu5KSkJCxbtgzXXXcd8vLy2nyMtLQ05OXlyZdvvvkGJ06cwDXXXKPEt0QUM3JycmCxWLBr165Wt+3cuRM6nQ6FhYXysczMTFxzzTV48803UVFRgTFjxrSaDjVw4ED86U9/wurVq/Hjjz/C4XBg6dKl8m0A0KtXL0ybNq3VZcqUKSE/VluKiooAoM1SiJbfo5Thq62tDTouZQWVJq117969rW5r61gkvfvuuxgwYADee+89XHXVVZg+fTqmTZsW0uaqaGjvtamurg4585uYmIiysrI2b3e5XKioqMDw4cO7v1giCkmPDmY7c9NNN2Hjxo1YsWIFfvjhB1x22WWYMWNGu3V9oXjhhRcwbdo0+QcqUbzS6/U455xz8OGHHwa1vDp69CjeeOMNTJo0CampqQD8dYiS5ORkDBo0CHa7HQBgtVpbBUMDBw5ESkqKfM706dORmpqK+++/H06ns9V6pPZMoTxWW/Lz8zFu3Di8/PLLQR+Xr1mzBjt27Ag6t6ioCHq9Hv/73/+Cjj/99NPtPn4kFRQUYNSoUXjllVfQ2NgoH1+/fj22bdum6HNLmdHATOhXX32FjRs3Kvq8oZo6dSoMBgOWLVsWdPzJJ58M6f5GoxGnnHJKu5PvduzYgebmZpxxxhndXisRhYatudpRXl6Ol156CeXl5SgoKAAALFiwAKtWrcJLL72E+++/v8uPefjwYXzyySd44403Ir1cItW8+OKLWLVqVavjt956K+677z6sWbMGkyZNwg033ACDwYBnn30WdrsdS5Yskc8dMWIEpkyZgpNPPhmZmZn45ptv8O677+Kmm24CAOzevRtTp07F5ZdfjhEjRsBgMOD999/H0aNHceWVVwIAUlNTsWzZMlx11VU46aSTcOWVVyInJwfl5eX473//i4kTJ+LJJ58M6bHas3jxYpx33nmYNGkSfvOb36CmpgZPPPEERo4cGRQ0pqWl4bLLLsMTTzwBQRAwcOBAfPzxx3LdbjTcf//9uPDCCzFx4kRcc801OHHiBJ588kmMGjUqaK2Rdv755+O9997D7Nmzcd5556GsrAzPPPMMRowYoejzhio3Nxe33norli5digsuuAAzZszA1q1b8cknnyA7OzukWtcLL7wQf/vb31BfXy//QSZZs2YNLBaL3FKOiJTHYLYd27Ztg9vtxpAhQ4KO2+12uWaqq15++WWkp6e3uVGMSKtaZrgk8+fPx8iRI/H5559j4cKFWLx4MTweD4qLi/Haa6/JPWYB4JZbbsFHH32E1atXw263o6ioCPfddx9uv/12AEBhYSHmzJmDdevW4dVXX4XBYMCwYcPw9ttv45JLLpEf51e/+hUKCgrwwAMP4KGHHoLdbkfv3r0xefJkubQn1Mdqy4wZM/DOO+/g73//OxYuXIiBAwfipZdewocffthqQMITTzwBp9OJZ555BmazGZdffjkeeughjBo1KpyXuctmzZqFN998EyUlJfjLX/6CwYMHY/ny5Xj55ZdbDZGIpPnz56OyshLPPvssPv30U4wYMQKvvfYa3nnnnVavkVoefPBBWCwWPPfcc1i7di0mTJiA1atXY9KkSSFNJbvqqqvwl7/8BR999FGrqWjvvPMOLr74YqSkpCi1fCJqQRCjvWMgRgmCgPfff18ONN966y3MnTsX27dvb7WhIDk5uVWN7Pz581FbW9tuRwRRFDFkyBCcf/75+Oc//6nEt0BE1Klx48YhJyeHE6paqK2tRUZGBu677z787W9/6/T8a6+9Frt378bnn38uH9uyZQtOOukkfPfddyENhyCiyGDNbDvGjx8Pt9uNY8eOYdCgQUGX9jZ7dWT9+vXYu3cvrr32WgVWS0QUzOl0wuVyBR0rLS3F1q1bW22G62lsNlurY48++igAhPzaLFq0CJs3b8aGDRvkYw888AAuvfRSBrJEUdajywwaGxuDdrSWlZVhy5YtyMzMxJAhQzB37lxcffXVWLp0KcaPH4+qqiqsW7cOY8aMwXnnnQcA8izympoaNDQ0YMuWLQDQ6ofZCy+8gOLi4qh9xEhEPduhQ4cwbdo0/PrXv0ZBQQF27tyJZ555Bnl5ea2GEvQ0b731FpYvX45zzz0XycnJ+OKLL/Dmm2/inHPOwcSJE0N6jL59+7baSLhixQollktEnejRZQalpaU466yzWh2fN28eli9fDqfTifvuuw+vvPIKDh06hOzsbJx++um4++675Sk6/fr1a7PdTuDLWldXh/z8fDz22GO47rrrlPuGiIh86urqcP3112PDhg2oqqpCUlISpk6digceeEBuY9ZTfffdd/jzn/+MLVu2oL6+Hrm5ubjkkktw3333BU1MIyJt6NHBLBERERFpG2tmiYiIiEizGMwSERERkWb1uA1gHo8Hhw8fRkpKSkjNsYmIiIgoukRRRENDAwoKCqDTdZx77XHB7OHDh4PmwRMRERFRbKqoqECfPn06PKfHBbPSVJaKiopWYwiJiIiISH319fUoLCwMaZpejwtmpdKC1NRUBrNEREREMSyUklBuACMiIiIizWIwS0RERESaxWCWiIiIiDSrx9XMEhERkTaJogiXywW32632UigCjEYj9Hp9tx+HwSwRERHFPIfDgSNHjsBqtaq9FIoQQRDQp08fJCcnd+txGMwSERFRTPN4PCgrK4Ner0dBQQFMJhMHH2mcKIqoqqrCwYMHMXjw4G5laBnMEhERUUxzOBzweDwoLCyExWJRezkUITk5OThw4ACcTme3gllVN4CVlJRAEISgy7Bhwzq8T21tLW688Ubk5+fDbDZjyJAhWLlyZZRWTERERGrpbKwpaUuksuuqZ2ZHjhyJtWvXytcNhvaX5HA4cPbZZ6NXr15499130bt3b/z8889IT0+PwkqJiIiIKNaoHswaDAbk5eWFdO6LL76ImpoafPnllzAajQCAfv36Kbg6IiIiIoplqufr9+zZg4KCAgwYMABz585FeXl5u+d+9NFHmDBhAm688Ubk5uZi1KhRuP/++zts0WG321FfXx90ISIiItKqfv364dFHH1V7GTFD1WC2uLgYy5cvx6pVq7Bs2TKUlZVh8uTJaGhoaPP8/fv3491334Xb7cbKlStx5513YunSpbjvvvvafY7FixcjLS1NvhQWFir17RARERHJWu4LankpKSkJ63E3b96M66+/vltrmzJlCv7whz906zFihaplBjNnzpS/HjNmDIqLi1FUVIS3334b1157bavzPR4PevXqhX/961/Q6/U4+eSTcejQITz00ENYtGhRm8+xcOFC3HbbbfL1+vp6BrRERESkuCNHjshfv/XWW7jrrruwa9cu+Vhgf1VRFOF2uzvcOyTJycmJ7EI1TvUyg0Dp6ekYMmQI9u7d2+bt+fn5GDJkSFD7huHDh6OyshIOh6PN+5jNZqSmpgZdiIiISNtEUYTV4VLlIopiSGvMy8uTL2lpaRAEQb6+c+dOpKSk4JNPPsHJJ58Ms9mML774Avv27cOFF16I3NxcJCcn49RTTw3aKA+0LjMQBAHPP/88Zs+eDYvFgsGDB+Ojjz7q1uv773//GyNHjoTZbEa/fv2wdOnSoNuffvppDB48GAkJCcjNzcWll14q3/buu+9i9OjRSExMRFZWFqZNm4ampqZuracjqm8AC9TY2Ih9+/bhqquuavP2iRMn4o033oDH45Hbc+zevRv5+fkwmUzRXCoRERGpyOZ0Y8Rdn6ry3DvumQ6LKTIh1F/+8hc8/PDDGDBgADIyMlBRUYFzzz0X//jHP2A2m/HKK69g1qxZ2LVrF/r27dvu49x9991YsmQJHnroITzxxBOYO3cufv75Z2RmZnZ5Td9++y0uv/xylJSU4IorrsCXX36JG264AVlZWZg/fz6++eYb3HLLLXj11VdxxhlnoKamBp9//jkAbzZ6zpw5WLJkCWbPno2GhgZ8/vnnIf8BEA5Vg9kFCxZg1qxZKCoqwuHDh7Fo0SLo9XrMmTMHAHD11Vejd+/eWLx4MQDg97//PZ588knceuutuPnmm7Fnzx7cf//9uOWWW9T8NoiIiIjCcs899+Dss8+Wr2dmZmLs2LHy9XvvvRfvv/8+PvroI9x0003tPs78+fPl+On+++/H448/jq+//hozZszo8poeeeQRTJ06FXfeeScAYMiQIdixYwceeughzJ8/H+Xl5UhKSsL555+PlJQUFBUVYfz48QC8wazL5cLFF1+MoqIiAMDo0aO7vIauUDWYPXjwIObMmYPq6mrk5ORg0qRJ2LRpk1wLUl5eHtQgubCwEJ9++in++Mc/YsyYMejduzduvfVW3HHHHWp9C0QUYc1ON3YcqcfYPunQ6ziukojalmjUY8c901V77kg55ZRTgq43NjaipKQE//3vf+XA0GazddjtCfDuPZIkJSUhNTUVx44dC2tNP/30Ey688MKgYxMnTsSjjz4Kt9uNs88+G0VFRRgwYABmzJiBGTNmyCUOY8eOxdSpUzF69GhMnz4d55xzDi699FJkZGSEtZZQqBrMrlixosPbS0tLWx2bMGECNm3apNCKiEhtj6/bg6dL9+HRK8bhovG91V4OEcUoQRAi9lG/mpKSkoKuL1iwAGvWrMHDDz+MQYMGITExEZdeemm7e4MkUv99iSAI8Hg8EV8vAKSkpOC7775DaWkpVq9ejbvuugslJSXYvHkz0tPTsWbNGnz55ZdYvXo1nnjiCfztb3/DV199hf79+yuynpjaAEZEVF5jBQAcqFZuswARUazasGED5s+fj9mzZ2P06NHIy8vDgQMHorqG4cOHY8OGDa3WFbgJ32AwYNq0aViyZAl++OEHHDhwAP/3f/8HwBtIT5w4EXfffTe+//57mEwmvP/++4qtV/t/0hBRXLE5vENQrI72h6EQEcWrwYMH47333sOsWbMgCALuvPNOxTKsVVVV2LJlS9Cx/Px8/OlPf8Kpp56Ke++9F1dccQU2btyIJ598Ek8//TQA4OOPP8b+/ftx5plnIiMjAytXroTH48HQoUPx1VdfYd26dTjnnHPQq1cvfPXVV6iqqsLw4cMV+R4ABrNEFGNsTm8Q22R3qbwSIqLoe+SRR/Cb3/wGZ5xxBrKzs3HHHXcoNr30jTfewBtvvBF07N5778Xf//53vP3227jrrrtw7733Ij8/H/fccw/mz58PwNtK9b333kNJSQmam5sxePBgvPnmmxg5ciR++ukn/O9//8Ojjz6K+vp6FBUVYenSpUGzBSJNEJXslRCD6uvrkZaWhrq6OvacJYpBFz21AVsqajF7fG/884pxai+HiGJAc3MzysrK0L9/fyQkJKi9HIqQjv67diVeY80sEcWUZmZmiYioCxjMElFMkcoMWDNLREShYDBLRDFFCmKbHMzMEhFR5xjMElFMaXawzICIiELHYJaIYoYoirDKNbMsMyAios4xmCWimOF0i3B7vA1WrCwzICKiEDCYJaKYYQvY9NXEDWBERBQCBrNEFDOkTgYA4HB54HQrM/WGiIjiB4NZIooZgcEswPZcRETUOQazRBQzWtbJsm6WiIg6w2CWiGJGc4vMLNtzEZGWCYLQ4aWkpKRbj/3BBx9E7DwtM6i9ACIiScuyArbnIiItO3LkiPz1W2+9hbvuugu7du2SjyUnJ6uxrLjDzCwRxQxby2CWZQZE1B5RBBxN6lxEMaQl5uXlyZe0tDQIghB0bMWKFRg+fDgSEhIwbNgwPP300/J9HQ4HbrrpJuTn5yMhIQFFRUVYvHgxAKBfv34AgNmzZ0MQBPl6V3k8Htxzzz3o06cPzGYzxo0bh1WrVoW0BlEUUVJSgr59+8JsNqOgoAC33HJLWOvoLmZmiShmtNoAxswsEbXHaQXuL1Dnuf96GDAldeshXn/9ddx111148sknMX78eHz//fe47rrrkJSUhHnz5uHxxx/HRx99hLfffht9+/ZFRUUFKioqAACbN29Gr1698NJLL2HGjBnQ6/VhreGxxx7D0qVL8eyzz2L8+PF48cUXccEFF2D79u0YPHhwh2v497//jX/+859YsWIFRo4cicrKSmzdurVbr0m4GMwSUcxgZpaIeopFixZh6dKluPjiiwEA/fv3x44dO/Dss89i3rx5KC8vx+DBgzFp0iQIgoCioiL5vjk5OQCA9PR05OXlhb2Ghx9+GHfccQeuvPJKAMCDDz6Izz77DI8++iieeuqpDtdQXl6OvLw8TJs2DUajEX379sVpp50W9lq6g8EsEcUMtuYiopAZLd4MqVrP3Q1NTU3Yt28frr32Wlx33XXycZfLhbS0NADA/PnzcfbZZ2Po0KGYMWMGzj//fJxzzjndet5A9fX1OHz4MCZOnBh0fOLEiXKGtaM1XHbZZXj00UcxYMAAzJgxA+eeey5mzZoFgyH6oSWDWSKKGa03gDEzS0TtEIRuf9SvlsbGRgDAc889h+Li4qDbpJKBk046CWVlZfjkk0+wdu1aXH755Zg2bRrefffdqK2zozUUFhZi165dWLt2LdasWYMbbrgBDz30ENavXw+j0Ri1NQLcAEZEMaR1ay5mZoko/uTm5qKgoAD79+/HoEGDgi79+/eXz0tNTcUVV1yB5557Dm+99Rb+/e9/o6amBgBgNBrhdof/MzI1NRUFBQXYsGFD0PENGzZgxIgRIa0hMTERs2bNwuOPP47S0lJs3LgR27ZtC3tN4WJmlohiRsvMLIcmEFG8uvvuu3HLLbcgLS0NM2bMgN1uxzfffIMTJ07gtttuwyOPPIL8/HyMHz8eOp0O77zzDvLy8pCeng7A29Fg3bp1mDhxIsxmMzIyMtp9rrKyMmzZsiXo2ODBg3H77bdj0aJFGDhwIMaNG4eXXnoJW7Zsweuvvw4AHa5h+fLlcLvdKC4uhsViwWuvvYbExMSgutpoYTBLRDFDqpnV6wS4PSI3gBFR3Prtb38Li8WChx56CLfffjuSkpIwevRo/OEPfwAApKSkYMmSJdizZw/0ej1OPfVUrFy5Ejqd90P1pUuX4rbbbsNzzz2H3r1748CBA+0+12233dbq2Oeff45bbrkFdXV1+NOf/oRjx45hxIgR+OijjzB48OBO15Ceno4HHngAt912G9xuN0aPHo3//Oc/yMrKivhr1RlBFENslhYn6uvrkZaWhrq6OqSmpqq9HCIK8Me3tuD97w8hJ8WMqgY7Lh7fG49cMU7tZRGRypqbm1FWVob+/fsjISFB7eVQhHT037Ur8RprZokoZkitubKSTADYmouIiDrHYJaIYoZUZpCTYgbA1lxERNQ5BrNEFDOkzGx2sjeYZWsuIiLqDINZIooZUmY2O9lbZsDMLBERdYbBLBHFDKkVV5YvM9vIzCwRBehhe9bjXqT+ezKYJaKY0ez0APCXGTAzS0QA5IlSVqtV5ZVQJDkcDgD+qWfhYp9ZIooZ/sysr5sBM7NEBG+wk56ejmPHjgEALBYLBEFQeVXUHR6PB1VVVbBYLDAYuheOMpglopgh18wmeTOzdpcHLrcHBj0/RCLq6fLy8gBADmhJ+3Q6Hfr27dvtP0wYzBJRTPB4RH+ZQYpJPm51upHKYJaoxxMEAfn5+ejVqxecTqfay6EIMJlM8kSz7mAwS0Qxodnlr49NSzTCoBPg8oiw2t1ITTCquDIiiiV6vb7bNZYUX5juIKKYYAvY7JVg0CPJ7P1bm1PAiIioIwxmiSgmSJ0LEow66HQCkkzezAs3gRERUUcYzBJRTGj2bf6ymLwZWYuUmbWzPRcREbWPwSwRxQQpM5to9GZkpcyslWUGRETUAQazRBQTpLZcCUbvjyUpQ9vEwQlERNQBBrNEFBNsLcoMksy+zCxrZomIqAMMZokoJthalBkwM0tERKFgMEtEMUEOZn21slJrLmZmiYioIwxmiSgmWJ1tbwBr5AYwIiLqAINZIooJzS0ysxY5M8syAyIiah+DWSKKCdaWZQbS0ARmZomIqAMMZokoJthalBkwM0tERKFgMEtEMcE/AYyZWSIiCh2DWSKKCdKkr4QWrbmsbM1FREQdYDBLRDHB5vQA8Gdmk31lBk1szUVERB1gMEtEMcHmy8z6a2ZZZkBERJ1jMEtEMUHeACbXzHIDGBERdY7BLBHFBGurcbbMzBIRUecYzBJRTGhvnG2z0wO3R1RtXUREFNsYzBJRTGjZmkv6F/B3OiAiImqJwSwRxQSpzEBqzWU26KDXCUG3ERERtaRqMFtSUgJBEIIuw4YNa/f85cuXtzo/ISEhiismIqXY5Myst7xAEAR/3SzbcxERUTsMai9g5MiRWLt2rXzdYOh4Sampqdi1a5d8XRAExdZGRNFja7EBDPD2mm1odqGJHQ2IiKgdqgezBoMBeXl5IZ8vCEKXziei2Od0e+DybfIKDGbZ0YCIiDqjes3snj17UFBQgAEDBmDu3LkoLy/v8PzGxkYUFRWhsLAQF154IbZv397h+Xa7HfX19UEXIootgTWxiQEbv6SOBtwARkRE7VE1mC0uLsby5cuxatUqLFu2DGVlZZg8eTIaGhraPH/o0KF48cUX8eGHH+K1116Dx+PBGWecgYMHD7b7HIsXL0ZaWpp8KSwsVOrbIaIwSZ0M9DoBRr2/dMhfM8syAyIiapsgimLMNHCsra1FUVERHnnkEVx77bWdnu90OjF8+HDMmTMH9957b5vn2O122O12+Xp9fT0KCwtRV1eH1NTUiK2diMJ34HgTpjxcihSzAdvuni4fv3b5ZqzbeQwPXjIaV5zaV8UVEhFRNNXX1yMtLS2keE31mtlA6enpGDJkCPbu3RvS+UajEePHj+/wfLPZDLPZHKklEpEC5LZcASUGAGDxlRkwM0tERO1RvWY2UGNjI/bt24f8/PyQzne73di2bVvI5xNRbLK1GJggSfJdZ80sERG1R9VgdsGCBVi/fj0OHDiAL7/8ErNnz4Zer8ecOXMAAFdffTUWLlwon3/PPfdg9erV2L9/P7777jv8+te/xs8//4zf/va3an0LRBQBbbXlAvwbwJo4NIGIiNqhapnBwYMHMWfOHFRXVyMnJweTJk3Cpk2bkJOTAwAoLy+HTuePt0+cOIHrrrsOlZWVyMjIwMknn4wvv/wSI0aMUOtbIKIIkDKzCS2DWQ5NICKiTqgazK5YsaLD20tLS4Ou//Of/8Q///lPBVdERGqQyghalhmwZpaIiDoTUzWzRNQzSa25WpUZsGaWiIg6wWCWiFQn18y2zMyaWDNLREQdYzBLRKqztpeZNfsys6yZJSKidjCYJSLVNTvabs3FzCwREXWGwSwRqa69oQlSay7WzBIRUXsYzBKR6mydlBmwNRcREbWHwSwRqc7WTplBkomtuYiIqGMMZolIde1lZqXg1uZ0w+0Ro74uIiKKfQxmiUh1cjBrCp7jItXMBp5DREQUiMEsEalO2gDWMjNrNuigE3znsG6WiIjawGCWiFQnTQBrWTMrCIK/bpbtuYiIqA0MZolIdXJrrhaZWcBfasCOBkRE1BYGs0SkuvbG2QKAhe25iIioAwxmiUh1tnbKDAB/ey4rywyIiKgNDGaJSHW2djaAAf4At4lTwIiIqA0MZolIVaIoBrTmar9m1srBCURE1AYGs0SkqmanR/6amVkiIuoqBrNEpKrAYQhtBbOsmSUioo4wmCUiVVl9GVezQQedNCEhALsZEBFRRxjMEpGqmjuolwWAZPaZJSKiDjCYJSJVSeUDljZKDADAwglgRETUAQazRKQqqS1XQjuZ2SRfmYGVG8CIiKgNDGaJSFUdDUzwHpfKDJiZJSKi1hjMEpGqOhqYAABJJmZmiYiofQxmiUhV/oEJhjZvt5iZmSUiovYxmCUiVVnlzGzbP46YmSUioo4wmCUiVcmtudorMzCzmwEREbWPwSwRqUrOzLZTZpBkYp9ZIiJqH4NZIlKVrZPMrEVuzeWGxyNGbV1ERKQNDGaJSFVSN4P2WnMlBWRspcCXiIhIwmCWiFQlt+ZqJ5hNMOogCN6vm7gJjIiIWmAwS0Sq6qzMQBAEOTtrZXsuIiJqgcEsEanK2klmFvCXIDAzS0RELTGYJSJVddaaCwCSfe25rGzPRURELTCYJSJVScMQOszM+joaNLI9FxERtcBglohUZXN6AHScmbWwZpaIiNrBYJaIVCWVGbTXmgvwj7RlzSwREbXEYJaIVCWVGSR0lJmVamZZZkBERC0wmCUiVXXWZxYIzMyyzICIiIIxmCUiVdlCKDOQa2ZZZkBERC0wmCUi1TjdHjjdIoDQWnM1cQMYERG1wGCWiFQjZWWB0FpzNbFmloiIWmAwS0SqafbVwOoEwKRv/8eRPM6WNbNERNQCg1kiUo2/XtYAQRDaPY/jbImIqD0MZolINVKmtaO2XACQZObQBCIiahuDWSJSjZSZTTR1/KOImVkiImoPg1kiUo3UY9ZiNHR4npyZZc0sERG1wGCWiFQjBbMJHXQyAPwbwNjNgIiIWmIwS0SqsUobwDqtmWVrLiIiahuDWSJSTXMIo2yBgAlgTjc8HlHxdRERkXYwmCUi1fg3gIWWmRVFoNnFulkiIvJjMEtEqpE2dHU0yhYAEgx6SG1oOdKWiIgCMZglItXImdlOglmdTpDraq1sz0VERAEYzBKRamy+wNTSSZkBAFjMUkcDZmaJiMiPwSwRqUbKzHY2AQwAkkzMzBIRUWuqBrMlJSUQBCHoMmzYsJDuu2LFCgiCgIsuukjZRRKRYqSa2VAys9LghEa25yIiogAdj92JgpEjR2Lt2rXydYOh8yUdOHAACxYswOTJk5VcGhEprDnEbgaAf3ACp4AREVEg1YNZg8GAvLy8kM93u92YO3cu7r77bnz++eeora1VbnFEpChbiN0MAMDCwQlERNQG1Wtm9+zZg4KCAgwYMABz585FeXl5h+ffc8896NWrF6699tqQHt9ut6O+vj7oQkSxwRri0ASAmVkiImqbqsFscXExli9fjlWrVmHZsmUoKyvD5MmT0dDQ0Ob5X3zxBV544QU899xzIT/H4sWLkZaWJl8KCwsjtXwi6qbmEFtzAf662iZuACMiogCqBrMzZ87EZZddhjFjxmD69OlYuXIlamtr8fbbb7c6t6GhAVdddRWee+45ZGdnh/wcCxcuRF1dnXypqKiI5LdARN3QpcysbwOYla25iIgogOo1s4HS09MxZMgQ7N27t9Vt+/btw4EDBzBr1iz5mMfjAeCtu921axcGDhzY6n5msxlms1m5RRNR2EIdmgAwM0tERG2LqWC2sbER+/btw1VXXdXqtmHDhmHbtm1Bx/7+97+joaEBjz32GMsHiDTIJrfm6vxHETOzRETUFlWD2QULFmDWrFkoKirC4cOHsWjRIuj1esyZMwcAcPXVV6N3795YvHgxEhISMGrUqKD7p6enA0Cr40SkDV3JzEpDExqZmSUiogCqBrMHDx7EnDlzUF1djZycHEyaNAmbNm1CTk4OAKC8vBw6neoNF4hIAaIo+oPZLoyztbI1FxERBVA1mF2xYkWHt5eWlnZ4+/LlyyO3GCKKKrvLA1H0ft2V1lxNbM1FREQBmPYkIlXYAoLSrgxNsLLMgIiIAjCYJSJVWH0lBiaDDnqd0On58tAEbgAjIqIADGaJSBVdGWULsDUXERG1jcEsEanC35YrtGA2ma25iIioDQxmiUgVXWnLBfhrZpscLojSzjEiIurxGMwSkSq60pYL8NfMekSg2elRbF1ERKQtDGaJSBU2X+1rqJnZwPNYN0tERBIGs0Skiq5mZnU6Qa6vZd0sERFJGMwSkSqsXexmAAAWeXACM7NEROTFYJaIVCG35goxMwsASRycQERELTCYJSJVdLU1FxAw0pZlBkRE5MNglohUIdXMJnShzEDKzDbZmZklIiIvBrNEpAopmO1KZtZfM8vMLBEReTGYJSJVdHWcLcCaWSIiao3BLBGpIpwyAwtrZomIqAUGs0SkCqu8AcwQ8n2STMzMEhFRMAazRKSKZnloQug/hixmZmaJiCgYg1kiUoV/aELomdlkXzDLzCwREUkYzBKRKsIZmiB1Pmhkay4iIvJhMEtEqmgOozWXNDTBytZcRETkw2CWiFRhDaM1l4VDE4iIqAUGs0SkirAmgDEzS0RELTCYJSJV2BzhTADzZWa5AYyIiHwYzBJR1LncHjjcHgBdnQDmy8yyNRcREfkwmCWiqJNKDIDwuhkwM0tERBIGs0QUdVIwKwiA2RD6jyF/n1k3RFFUZG1ERKQtDGaJKOqaHd4SA4tRD0EQQr6fNAHM7RFhd3kUWRsREWkLg1kiijqr01sm0JUSAyC4vpbtuYiICGAwS0QqkDoZdKUtFwDodYIc0LI9FxERAQxmiUgF4bTlkiSZuQmMiIj8GMwSUdRJG8C60pZLYvENTmhiey4iIgKDWSJSgTzKNozMrJTNtTIzS0REYDBLRCroTmZWas/FzCwREQEMZolIBc3ObmRm5WCWmVkiImIwS0QqkMsMjIYu3zeJZQZERBSAwSwRRZ1Nrpnt+o8geQMYW3MREREYzBKRCqSaWSkw7QqpNZeVZQZERAQGs0SkgnCHJgDMzBIRUTAGs0QUddbuDE1gzSwREQVgMEtEUdfcjdZcSWzNRUREARjMElHUdafPrDzOljWzREQEBrNEpAKpRCC8CWBSzSyDWSIiYjBLRCqwOT0AupeZtXIDGBERgcEsEanA5suqhrMBTM7MssyAiIjAYJaIVCDVzCaE1c3AG8wyM0tERACDWSJSga0brbks3ABGREQBGMwSUdTJ42zDqJlNNvszs6IoRnRdRESkPQxmiSiqRFHsVmsuKZvr8oiwuzwRXRsREWkPg1kiiiq7ywOPL6HandZcAOtmiYiIwSwRRZk0/QsILzOr1wlIMHp/dLFuloiIGMwSUVRJ2VSTXgeDPrwfQexoQEREEgazRBRVclsuY/g/fuSOBpwCRkTU4zGYJaKo8rflMnRyZvvkzKydmVkiop6OwSwRRZXcySCMzV+SJF97LmZmiYiIwSwRRZWUmU0IY/OXRGrPZWUwS0TU4zGYJaKosnZj+pdEKjNoZJkBEVGPp2owW1JSAkEQgi7Dhg1r9/z33nsPp5xyCtLT05GUlIRx48bh1VdfjeKKiai7mrsxMEEibQCzsjUXEVGPF/4OjAgZOXIk1q5dK183GNpfUmZmJv72t79h2LBhMJlM+Pjjj3HNNdegV69emD59ejSWS0TdJGVmu1Uza5JqZpmZJSLq6VQPZg0GA/Ly8kI6d8qUKUHXb731Vrz88sv44osvGMwSaUR3RtlKmJklIiKJ6jWze/bsQUFBAQYMGIC5c+eivLw8pPuJooh169Zh165dOPPMM9s9z263o76+PuhCROqx+TZtRaJmlplZIiJSNZgtLi7G8uXLsWrVKixbtgxlZWWYPHkyGhoa2r1PXV0dkpOTYTKZcN555+GJJ57A2Wef3e75ixcvRlpamnwpLCxU4lshohD5hyawmwEREXWfqmUGM2fOlL8eM2YMiouLUVRUhLfffhvXXnttm/dJSUnBli1b0NjYiHXr1uG2227DgAEDWpUgSBYuXIjbbrtNvl5fX8+AlkhFNocHQPdqZpOlPrPsZkBE1OOpXjMbKD09HUOGDMHevXvbPUen02HQoEEAgHHjxuGnn37C4sWL2w1mzWYzzGazEsslojDYnL4yg27VzErBLDOzREQ9neo1s4EaGxuxb98+5Ofnh3wfj8cDu92u4KqIKJJsEelmwDIDIiLyUjUzu2DBAsyaNQtFRUU4fPgwFi1aBL1ejzlz5gAArr76avTu3RuLFy8G4K1/PeWUUzBw4EDY7XasXLkSr776KpYtW6bmt0FEXRCJ1lwWbgAjIiKfsILZiooKCIKAPn36AAC+/vprvPHGGxgxYgSuv/76kB/n4MGDmDNnDqqrq5GTk4NJkyZh06ZNyMnJAQCUl5dDp/Mnj5uamnDDDTfg4MGDSExMxLBhw/Daa6/hiiuuCOfbICIVRKI1VxJbcxERkU9YweyvfvUrXH/99bjqqqtQWVmJs88+GyNHjsTrr7+OyspK3HXXXSE9zooVKzq8vbS0NOj6fffdh/vuuy+cJRNRjLBFYJwtM7NERCQJq2b2xx9/xGmnnQYAePvttzFq1Ch8+eWXeP3117F8+fJIro+I4kwkWnPJmVnWzBIR9XhhBbNOp1PuELB27VpccMEFAIBhw4bhyJEjkVsdEcWdyJQZeDOzTrcIh8sTkXUREZE2hRXMjhw5Es888ww+//xzrFmzBjNmzAAAHD58GFlZWRFdIBHFF3+ZQfj7TwPberE9FxFRzxZWMPvggw/i2WefxZQpUzBnzhyMHTsWAPDRRx/J5QdERG2RM7Om8DsDGvQ6mA3e+zex1ICIqEcLKzUyZcoUHD9+HPX19cjIyJCPX3/99bBYLBFbHBHFH39rru51BkwyG2B3OeTHIyKinims1IjNZoPdbpcD2Z9//hmPPvoodu3ahV69ekV0gUQUP9wef41rd2pmAX83BJYZEBH1bGEFsxdeeCFeeeUVAEBtbS2Ki4uxdOlSXHTRRRxgQETtkkoMgO615gKAJF9ml5lZIqKeLaxg9rvvvsPkyZMBAO+++y5yc3Px888/45VXXsHjjz8e0QUSUfywBQSeUs1ruCxmZmaJiCjMYNZqtSIlJQUAsHr1alx88cXQ6XQ4/fTT8fPPP0d0gUQUP5oD2nIJgtCtx0o2MzNLRERhBrODBg3CBx98gIqKCnz66ac455xzAADHjh1DampqRBdIRPHDGoHpXxLpMRqZmSUi6tHCCmbvuusuLFiwAP369cNpp52GCRMmAPBmacePHx/RBRJR/IjE9C+Jv2aWwSwRUU8WVm+cSy+9FJMmTcKRI0fkHrMAMHXqVMyePTtiiyOi+CIFnhHJzMo1sywzIO1qsrtQdrwJIwtSu116Q9RThd3oMS8vD3l5eTh48CAAoE+fPhyYQEQdkmtmIxDMMjNL8eCv72/Dh1sO4+3/NwGn9c9UezlEmhRWmYHH48E999yDtLQ0FBUVoaioCOnp6bj33nvh8XBOOhG1TR6YEIEyA2kcbhM3gJGG7aps8P1br/JKiLQrrMzs3/72N7zwwgt44IEHMHHiRADAF198gZKSEjQ3N+Mf//hHRBdJRPHB5ohgZtZXZmDlBjDSsJomBwCgqtGh8kqItCusYPbll1/G888/jwsuuEA+NmbMGPTu3Rs33HADg1kialNga67uSjIzM0vaJooiTlh9wWyDXeXVEGlXWGUGNTU1GDZsWKvjw4YNQ01NTbcXRUTxyRrBzCzH2ZLW1Te74HSLAIDjjQxmicIVVjA7duxYPPnkk62OP/nkkxgzZky3F0VE8ckWycwsa2ZJ46QSA4DBLFF3hFVmsGTJEpx33nlYu3at3GN248aNqKiowMqVKyO6QCKKH7ZIDk1gzSxpXHVAAMsyA6LwhZWZ/cUvfoHdu3dj9uzZqK2tRW1tLS6++GJs374dr776aqTXSERxQonMLMfZklZVt8jMiqKo4mqItCvsPrMFBQWtNnpt3boVL7zwAv71r391e2FEFH/8NbNh/+iRSd0MmthnljQqsMyg2elBo92FlASjiisi0qawMrNEROHwZ2a7/6NH6jNr5QQw0qjAYBYAjrM9F1FYGMwSUdQ0R7TPrDeYdbg9cLg4rIW0p7qxZTDLulmicDCYJaKoiWSZQeAmMhvrZkmDapqCg1duAiMKT5d+o1x88cUd3l5bW9udtRBRnIvkBjCjXgeTQQeHy4NGhwtpFtYakrZUtyozYDBLFI4uBbNpaWmd3n711Vd3a0FEFL8i2ZoLAJJMejhcHrbnIk2Sygz6ZCTi4AkbM7NEYepSMPvSSy8ptQ4i6gGkzGxCBDKzgHcT2Amrk4MTSJOkDWDD8lJw8ISNmVmiMLFmloiiRq6ZjVAwm8TBCaRRoijKweyQ3BQAQFUDuxkQhYPBLBFFTbMzsmUGFo60JY1qtLvgcHu7cAzN8wWzzMwShYXBLBFFhSiK/g1gkaqZlTKzHJxAGiNlZRONevTJsAAAjrNmligsDGaJKCocbg/cHu+4zogFs1JmloMTSGOkTgaZSSb0SjED8GZmOdKWqOsYzBJRVDQ7/IMNIlczKwWzzMySttT4OhlkJ5uQnewNZh0uDxr4XibqMgazRBQVVqf3l7RRL8Coj8yPHqn2tollBqQx1b6BCZlJJiSa9EjyvZdZakDUdQxmiSgqpB6zkWrLBfgzs1ZuACON8ZcZeLOyOVKpAYNZoi5jMEtEURHptlxAQGaWH82SxkhlBlnJJgCQSw2ON7I9F1FXMZgloqiIdFsuwL8BjJlZ0pqagA1gQGBmtlm1NRFpFYNZIoqKSE//AgCLmZlZ0qbqFsEsM7NE4WMwS0RRIWVPI5mZTWbNLGmUlJnNahXMsmaWqKsYzBJRVDRHeGAC4J8A1sjMLGmMHMwmcwMYUXcxmCWiqPBvADNE7DGldkacAEZaIoqinIH1Z2a9/zIzS9R1DGaJKCqk1lwRzcyaOQGMtMfqcMPu8g4RabkBjDWzRF3HYJaIokLaAJZojNyPHWZmSYukEgOzQSfXkEs1s1UNHGlL1FUMZokoKmzyBrDIlRnImVluACMNqQ7Y/CUIAgB/Ztbh9qC+mX+cEXUFg1kiigolWnNJmVmHywOn2xOxxyVSUo00ytZXJwt4/79I8f1xxk1gRF3DYJaIokKJ1lyBWV625yKtqG4MHmUryU5hey6icDCYJaKokFtzRTAzazLoYNJ7f4xxcAJphVQzm51kCjqew16zRGFhMEtEUSFt0opkNwPAPwWMm8BIK1pO/5Jkp3ivs8yAqGsYzBJRVNic3prWSGZmASDJxPZcpC1ymUFyi2CWmVmisDCYJaKosCmVmfU9XhMzs6QR0gawrHbKDJiZJeoaBrNEFBU2BcbZAv72XFZmZkkjapo62wDGwQlEXcFgloiiQp4AFvEyA2ZmSVvaq5nlBjCi8DCYJaKosCnQmgsAkqTMLFtzkUbUBAxNCCRlZllmQNQ1DGaJKCpsCrTmAgIys2zNRRpgc7jlP7yyWm0A814/3siRtkRdwWCWiKJC+gWuVM0suxmQFlT7Nn+Z9Dokm4NHO0vdDJxuEXU2Z9TXRqRVDGaJSHEejwi7S6nWXPHfZ7a82srMc5yoCaiXFQQh6LYEox4pCd4Al3WzRKFTNZgtKSmBIAhBl2HDhrV7/nPPPYfJkycjIyMDGRkZmDZtGr7++usorpiIwiGVGABKtObyZWbjNJj96Ug9fvHwZ7j93a1qL4UioL3NX5IcuW6WHQ2IQqV6ZnbkyJE4cuSIfPniiy/aPbe0tBRz5szBZ599ho0bN6KwsBDnnHMODh06FMUVE1FXBQazCYZIbwDzZWbjtMzgh4O1EEVg3U/HYHfF5/fYk9T42m61rJeVSKUGVczMEoXM0PkpCi/AYEBeXl5I577++utB159//nn8+9//xrp163D11VcrsTwiigCpk0GCUQedTujk7K6J98xsZZ03qLG7PNh2sA6n9MtUeUXUHTWdZWal9lzsaEAUMtUzs3v27EFBQQEGDBiAuXPnory8POT7Wq1WOJ1OZGa2/8Pdbrejvr4+6EJE0SVlZqXAM5LkzGyctuaqrG+Wv/6qrEbFlVAkhFxmwMwsUchUDWaLi4uxfPlyrFq1CsuWLUNZWRkmT56MhoaGkO5/xx13oKCgANOmTWv3nMWLFyMtLU2+FBYWRmr5RBQipQYmAECSlJmN0w1SRxnMxhVplK1UTtCS3J6LmVmikKkazM6cOROXXXYZxowZg+nTp2PlypWora3F22+/3el9H3jgAaxYsQLvv/8+EhIS2j1v4cKFqKurky8VFRWR/BaIKARKteUC/EMT4rU1V2WdP5j99kANXG6Piquh7qpuDC0zy24GRKFTvWY2UHp6OoYMGYK9e/d2eN7DDz+MBx54AGvXrsWYMWM6PNdsNsNsbvsvYCKKjmaFBiYA/oli8Voze6zBH8w2OdzYcaQeY/qkq7cg6pbOygy4AYyo61SvmQ3U2NiIffv2IT8/v91zlixZgnvvvRerVq3CKaecEsXVEVG4rEqWGcTxOFuHy4PjvkzeuMJ0AMDXLDXQtPZG2Uqy5Q1gbM1FFCpVg9kFCxZg/fr1OHDgAL788kvMnj0ber0ec+bMAQBcffXVWLhwoXz+gw8+iDvvvBMvvvgi+vXrh8rKSlRWVqKxsVGtb4GIQiCPslWgzMASx+NspaysSa/DjFHeri+sm9W2TrsZBJQZeDwcaUsUClWD2YMHD2LOnDkYOnQoLr/8cmRlZWHTpk3IyckBAJSXl+PIkSPy+cuWLYPD4cCll16K/Px8+fLwww+r9S0QUQhsCpYZSBvA7C5P3NWTSpu/eqWacfqALADA5gM1DHI0yu5yo9H3R1dWUtvlb1L/WZeHI22JQqVqzeyKFSs6vL20tDTo+oEDB5RbDBEpxuarZ7UokZk1+x/T6nQjVR9T1VPdIvWYzUtNwMiCVFhMetRandh9rAHD8lJVXh11lZSVNegEpCa2/evXbNAjLdGIOpsTxxvtyGgng0tEfvHzU5+IYpbN4c2YJigQzJoNehj13kEM8TYFTMrM5qYmwKjX4eSiDACsm9WqwE4GgtD+8BCpPVcV23MRhYTBLBEpzur0ZWYVKDMA/MMYGuOsbjYwmAWA03zTv1g3q02ddTKQsKMBUdcwmCUixTUr2GcWAJJM0hSw+ApmpelfeWne4KbYVzf71f4aiCLrZrVGGpgg1cW2R54CxswsUUgYzBKR4qS2WQlKZWbjdHCCNDBBysyO6ZMGk0GH4412lB1vUnNpFAZ/mUHHvc/l9lyNbM9FFAoGs0SkOKmbgRIbwID4zcxKZQZ5vmA2wahnv1kN66zHrIRTwIi6hsEsESlOyQlggL9mtimOBieIooij9d5gRsrMAkBxf2/dLINZ7emsx6wkJ5llBkRdwWCWiBRnVbpm1teeyxpHG8Dqm11yRjsvzR/Mntafm8C0KuQNYCne25mZJQoNg1kiUpySQxMA/0jbeMrMSiUGaYnGoFrjk4syYNAJOFRrw8ETVrWWR2GQMrPZnW0AS/b+8cLMLFFoGMwSkeJsDqlmVpk5LXKZQRxlZqXNX3kBJQaA93sd1TsNAEsNtKbal2ntdAOYLzNb3eTgtDeiEDCYJSLFyZlZkzI/cqQNYE1xtAEscJRtS6yb1aZQywykUbduj4hajrQl6hSDWSJSXLRac8XTBLCWnQwCncZgVnMcLg8amr1/bHXWzcBk0CHdYgTAUgOiUDCYJSLFNStcZhCPmVn/wITWwewp/TIhCMD+40045juPYtsJqzcrq9cJSEs0dnq+v9csg1mizjCYJSLFKb0BLB4zs5V1rdtySdISjRielwoA+PoAs7NaIA1MyLAYodMJnZ7P9lxEoWMwS0SKcrg8cPk2sSg9zjaeMrMdlRkALDXQmlB7zEqyOTiBKGQMZolIUVJWFlC+NZc1DltztZWZBbgJTGuqm7xBaVYnnQwkcmaWwSxRpxjMEpGipLZcBp0Ak0Gpbgbx1ZrL5fbIGbnctLaDn1N9wezOygbU+uoxKXbJmdlOesxKpPZcLDMg6hyDWSJSlNL1sgBgMcdXmUFVox0e0fsHQHY7mbzsZDMG9UoGwOysFkg1s511MpD4N4DxDxWizjCYJSJFWX0BZoJC9bKAPzMbLxvApIEJvVLMHW4WYt2sdoTaY1aSk8INYEShYjBLRIpqdkptuRTMzMbZBjC5XraNtlyB5LpZdjSIeTVyzWyIwSxbcxGFjMEsESnK5vAAULbMQNoA1uz0wB0H4z+P1vvqZVM6DmZP7ecNZn88VIfGOKkXjlf+bgYhbgDzZWZrmhxx8Z4mUhKDWSJSlFRmoFRbLiA462uNg+xsRwMTAhWkJ6IwMxEeEfiG2dmY1tUyA+k8t0eUBy4QUdsYzBKRoqKxAcxs0MHgqy2Nh/ZcR+s6bssVqLh/FgDWzcY6KTObHWI3A6NehwzfSFuWGhB1jMEsESlKas2lZDArCIK/bjYOPm73Z2Y7/0iam8Bin8vtQa3VCSD0zCzgLzU43sDMLFFHGMwSkaLkzKyCZQaAv262KQ46GsgbwDqpmQX8m8C2HqyVN9tRbKnxlQkIApBuCT2YzZYHJzQrsi6ieMFglogUZY1CZhaIr44G8gawTmpmAaBvpgW5qWY43SK+L69VeGUUDqnEIMNigr6DVmstMTNLFBoGs0SkqGi05gICR9pqO5httLvkzgR5IdTMCoKA03x1s1+VVSu6NgpPTWPXNn9JsjnSligkDGaJSFFSzaySQxOAgMysxssMpIEJKWaDHKB3pph1szGtq50MJPIUMA5OIOoQg1kiUpRVyswaQwvMwiVPAdN4ZjbUgQmBpGD2u/ITcLg8iqyLwieVGYQ6MEEiTwFjZpaoQwxmiUhRzVLNrEnZHzeWONkAJgezqaE11weAQb2SkZlkQrPTg22H6pRaGoVJysxmhdiWSyK18eJIW6KOMZglIkVFawNYslnvez5tZ2Yr60PvMSsRBAGn9ssAwLrZWCSNsg11+pdE3gDWyA1gRB1hMEtEivK35lK2zMDie/xGrWdmfTWzoWz+CsThCbGrujHMMoNkaaStnSNtiTrAYJaIFBWNoQkAkGSKr8xsZ6NsW5KGJ3xz4AQDnxgT7gawzCQTBAHwiP66WyJqjcEsESnKFqXWXPFSM1sp9ZjtYmZ2eH4qUswGNNpd+OlIvRJLozCFuwHMoNch0zdkgSNtidrHYJaIFCUFswnMzIbkWBg1swCg1wk4Ra6bZalBLJGC2cwubgADAnrNchMYUbsYzBKRoqQyA8Uzs76a2SaHdjOzbo+IY76gpas1swD8wxP2cxNYrHB7RJywhldmAARuAmMwS9QeBrNEpCj/BjClJ4D5MrN27WZmqxu9G310gr8tU1cUD/DWzW4+UAMP62ZjQq3VAdH3n0IqGegKtuci6hyDWSJSlPSxv+IbwMzaz8xKm79yUsww6Lv+43lUQRoSjXqcsDqxt6ox0sujMEglBukWY1j/TeUpYMzMErWLwSwRKcbjEdHs9E6kUjozK5cZaDgzezTMzV8Sk0GHk4rSAbBuNlZIPWLDKTEA2GuWKBQMZolIMc0uf5ZU+cys9jeAhTMwoaXT+rFuNpaE28lAwg1gRJ1jMEtEirE5ohjMmrTfmivcgQmBpH6zX5fVQBRZN6s2//Sv7mZmGcwStYfBLBEpRtr8ZTbooNMJij6X1C3B5nRrdmhAuAMTAo3vmw6TXodjDXb8XG2N1NIoTP6BCV0bZSthZpaocwxmiUgx0WrLBfg3gAH+IFprjkagzCDBqMfYwjQAHG0bC7pbZiBlZmusDrjcnoitiyieMJglIsXIbbkULjEAfNlfX/JXq+25/MFseFk8iVRqwE1g6pMys1lhtFoDvOUJOgEQRW9AS0StMZglIsVYfZnZhChkZgVB0Hx7rsoI1MwCAcMTyrgJTG013exmoNcJ8n1ZakDUNgazRKQYKTMbjTIDIHATmPYyszaHG/XN3nXndqNmFgBOLsqAXifg4AkbDtXaIrE8ClO1bwNYVpg1s0Bgr1lmZonawmCWiBQj1cxGo8wAACy+9lxaDGalzV8Wkx4pAfW/4Ug2GzCqIBUAsJmlBqqqaepeZhbw180yM0vUNgazRKQYOZg1dS84C5WUmbVqsMwgsMRAELrf+YF1s+rzeEScsDoBhF8zCwA5nAJG1CEGs0SkGP8GsOj8qJHKGZo0ODjhWIM3mO3Vzc1fEtbNqq/O5pTbxGVYwg9ms6Ves8zMErWJwSwRKcbfmitKmVnfx/NWDQ5OiNTmL8mp/TIgCMD+qiZ+PK0SqZNBSoIBJkP4v26zfVndKmZmidrEYJaIFCNlZhOiVTOr4cysPMq2m5u/JOkWE4bmpgAANh9gqYEapHpZaQNXuDgFjKhjDGaJSDHWKG8ASzZrt2ZW6jEbqcwsABQHjLal6OvuKFsJp4ARdYzBLBEppjnKrbksGm7NdbTeG6h0Z/pXS/66WQazajjezR6zEn9mlq25iNrCYJaIFGP1fdyfGK0+s1puzVXX/VG2LZ3aPwMAsLOyHnW+XfUUPd0dZSuRMrMnrA44OdKWqBUGswr7aOth3PbWFpTuOqb2Uoiizub0/uKNWp9ZkzYngHk8otzNIC9CNbMA0CslAQOykyCKrJtVQyR6zALeTgjySNsmZmeJWmIwq7Cvy6rx3veH+DEf9Uj+PrPRzcxaNbYBrMbqgNMtQhCAXimRac0lKR7gq5tlMBt11REKZvU6AVmsmyVqF4NZhUm7iXdXNqi8EqLoszm9QWX0a2a1lZmVSgyyksww6iP7Y5nDE9QjbQDrzsAEibwJjB0NiFpRNZgtKSmBIAhBl2HDhrV7/vbt23HJJZegX79+EAQBjz76aPQWG6YhvmB211EGs9TzSJnZaLXmSjJpMzMrlRjkRmhgQiBpE9iPh+rQqMFaYi2rbpRqZrv/3zWHgxOI2qV6ZnbkyJE4cuSIfPniiy/aPddqtWLAgAF44IEHkJeXF8VVhm9onjeYPXjCxl8k1ONEuzWXNDRBe5lZb4ASybZckt7pieidngi3R8R3P5+I+ONT+yJVMwv4ByewowFRa6oHswaDAXl5efIlOzu73XNPPfVUPPTQQ7jyyithNkc+g6GEdItJzrbsYXaWephot+bSas1spAcmtCTXzbLUIGpEUfR3M4hAmUEOa2aJ2qV6MLtnzx4UFBRgwIABmDt3LsrLyyP6+Ha7HfX19UGXaJNLDVg3Sz2MNcplBlLNbKPGMrNHIzzKtqViuW62WpHHp9bqbS64PCKAyGRmOQWMqH2qBrPFxcVYvnw5Vq1ahWXLlqGsrAyTJ09GQ0Pkgr7FixcjLS1NvhQWFkbssUM1lHWz1EPZop2ZNUkTwLSZmVUqmD25yBvMbjtUBxf7lEZFtW/zV7LZALOh++9/TgEjap+qwezMmTNx2WWXYcyYMZg+fTpWrlyJ2tpavP322xF7joULF6Kurk6+VFRUROyxQzXEVze7m8Es9TBSmUG0WnNZ5DIDNzy+rJgWSKNseymwAQwABmQnIcmkR7PTg71VjYo8BwWLZL0swMwsUUdULzMIlJ6ejiFDhmDv3r0Re0yz2YzU1NSgS7TJmdlK/hKhnsPp9sDp9gaUFqMhKs8pZWYBf1ZYC6RgNpIDEwLpdAJG9U4DAGw7WKfIc1CwSPWYlUiZWQazRK3FVDDb2NiIffv2IT8/X+2lRNTg3GQIgveHUDV/EFEPERhMJpii86MmwaiDIHi/btJIqUGz040TvlGzSpUZAMBoKZg9xGA2GqTMbHYENn8FPs4Jq5MjbYlaUDWYXbBgAdavX48DBw7gyy+/xOzZs6HX6zFnzhwAwNVXX42FCxfK5zscDmzZsgVbtmyBw+HAoUOHsGXLlohmcpVgMRnQN9MCgHWz1HNIPWZ1AmCK8CCA9giC4K+b1cgmsGP13j9wzQYd0hKNij3P6D7eYPYHZmajItJlBhkWE/Q6719q1WzPRRRE1WD24MGDmDNnDoYOHYrLL78cWVlZ2LRpE3JycgAA5eXlOHLkiHz+4cOHMX78eIwfPx5HjhzBww8/jPHjx+O3v/2tWt9CyIZwEhj1MFIwazEZIEjp0iiQ2nNpJTN7tMFfYqDk6zSmTzoA4Kcj9czsRYFUDpAZgYEJgLdUJMsXGHMTGFGw6BSytWPFihUd3l5aWhp0vV+/fhBF7WzqCDQ0NwVrdhzFrqOsm6WeIdptuSTezKxdM4MTpFG2uSnKlRgAQFGmBSlmAxrsLuw52ogRBdHfP9CTyD1mI5SZBbybwI412Fk3S9RCTNXMxjN2NKCeJtptuSQWrWVmFR6YIAnaBHaoVtHnosiXGQAB7bkYzBIFYTAbJUMDygy0ml0m6gq5LVeUM7MWjdXMVsoDE5SfajimDzeBRYtU15oZoQ1gAHvNErWHwWyU9M9OglEvoMHuwmHfLy+ieCaVGUSrx6wkyaStzKw8ylbBTgYStueKHqXKDAC25yJqicFslJgMOgzITgbATWDUM9jUysyapcysNoJZqZtBNIJZKTP705EGOFzcBKYUURT9wWxy5DLuUnuu4+xmQBSEwWwUSXWzbM9FPYHNlxlVLzOrkTIDhQcmBOqbaUFqggEOt4f1+wpqtLvg8HWMUCIzW9UQv5/uiaKIHw/VyWVKRKFgMBtFQ3OZmaWew6ZWmYGUmdVAmYEoiv5gNgqZWUEQ5H6zrJtVjlQvazHpI9rNI0eeAha/mdlPfqzE+U98gSWrdqm9FNIQBrNRJPWaZWaWegKrSmUG0tAELbTmqrU65Y/7e0VhAxgAjO6dDoDDE5QU6VG2kuyU+N8AVrrrmPff3cdUXglpCYPZKBqW5+3ruOdYI1xsWk5xrtmhcmsuDdTMSlnZzCQTzIbovE5S3eyPzMwqRonNX4A/M1tnc8ZtzfPWCu/7cn9VE+psTpVXQ1rBYDaK+mQkItGoh8Plwc81VrWXQ6QotTaAyeNsNVAzK/WY7ZUSnawsAIz2dTTYWVkPuyv2XyMtqmmSpn9FNphNSzTCII20bYq/7Gyj3YXdx/yfXLLrBoWKwWwU6XQChrBulnoItVpzWTTUmutoFDd/SfpkJCLdYoTTLWIXfw4pwl9mENk/UnQ6Ia57zf54qA6Bbdi3HqxVbS2kLQxmo4x1s9RTqJaZNWtnaEJlnTcgicbmL4kgCHJ2lpvAlFHj26CVHcGBCZLsFKk9V/wFs1sragEAgjf5jC2+60SdYTAbZUM51pZ6CLW6GWgpMxvNgQmBRnN4gqKUGGUriefMrJSJnTosF4A3mOXETAoFg9kokzOz/HiP4pxamdlks3ZqZo+pUGYA+DeBsaOBMo4rGMzGc3suafPX3OK+0OsEVDXY5T/4iDrCYDbKhvkysweqrWwKTXFNvZpZqTWXljKz0dsABgCj+6QD8H5CxJ9DkSdtAMtSpMwgPjOzxxqacajWBkEATu2fiaG+xM9WlhpQCBjMRllOihnpFiPcHhH7qhrVXg6RYqQgKdqtuZI01JrrqEplBgVpCchMMsHlEbGTnxJFnFQzG+kNYIA/M1sVZzWzP/iysoN7JSPZbMDYwnQAwJYKfnpAnWMwG2WCIMilBqybpXgm1cxGcgJSKKTMrNXphscTu/V2DpdH/qg4mhvAgBabwLhjPKJEUZS7GUS6zyzgz8wej7PMrLTZa6zvU4Nxhd73JzOzFAoGsyoYKtfNMjNL8UsuM4h6NwPv84ki0BzDfVSPNXizska9oEhtZWfY0UAZVocbdt9AA2U2gHkfM94ys9LmLykjK/277VAd3DH8RynFBgazKhjCjgbUA/jLDAxRfd4Eg15u7RPLI22P1nuDkV4pCRCkBUfRaG4CU4TUySDBqFOkxKZXHGZmPR5RzsCO8wWxg3ulwGLSo9Huwn6W5FEnGMyqYCg7GlAPoFZmVqcTYPE9pzWG23OpMTAhkNTRYM+xRm4CiyB/iYFZkT9SpNZc9c2uuJngdqC6CfXNLpgMOrl9pV4nYJTv0wP2m6XOMJhVgRTMHqq1oaGZs6cp/oii6G/NFeUNYIB/cEIsZ2Yr63zBbJTrZSV5qQnITjbB7RGx40i9KmuIR9WNyoyylaQlGmHUe4PkeGnPJZUYjCpIhVHvD0ukLC0ngVFnGMyqIM1ilH+B7T7Kj08o/jQ7PfLXagazWsjMRruTgSR4ExhLDSKlWsEes4D3v5uUnY2XUgOpv6xUJyuRNoNtZUcD6gSDWZWwbpbimS3gY+tolxkA/nZgjTHcnqtSLjOIbo/ZQFK/WW4Ci5waBTsZSOJtCtiWFvWykrG+jgY/HalnKQx1iMGsSobmJgNg3SzFJymYNRl00Ouiv7kpyRT7U8DUzswCHGurBCVH2UpypE1gcdDRwOHyYMdhb5lLy2C2d3oispO9/ZBZCkMdYTCrEo61pXhm8328r0ZWFgAsGhicIHUzUDOY9W8Ca4jpkgwtqZYGJigw/UsiteeKh2B2Z2U9HG4P0i1G9M20BN0mCEJAqUFt9BdHmsFgViXD8lIBsMyA4pPN4a2Zjfb0L0msZ2ZFUVR9AxjgDaR7pZjhEb0f5VL3SaNssxWY/iXJiaORtlsDhiW01f1BqqNlMEsdYTCrkkG9kiEI3s0C8fDXNVEgq9qZWV8Q3RSj2cb6ZpdciqFWay6JVGrAfrOREY0yA3kDWBx0M9jSzuYviRzM8v1JHWAwq5JEkx5Fvo9UdrPUgOKMmm25AH83g8bm2AxmpXrZtERj1Mf9tiQNT2DdbGQcj0qZQRxlZn1tt6TxtS2N9b0/y443odaq/eCdlMFgVkVy3SxLDSjO2FQamCDpl+X9QzFWm637N3+p18lAItXNsqNBZESjm0G8bACrb3Zin2+61xhfbWxL6RaT/P8zPz2g9jCYVZE06YSbwCjeqJ2ZnTK0FwBg84GamBxMItXLqrn5SyJNWdpb1RjTG+a0wOZwy+/9aJQZVGk8mP3xYB1EEeiTkSh/T21h3Sx1hsGsipiZpXglB7NqZWazk9A/OwlOt4gNe6tVWUNH5FG2MRDM9kpJQF5qAkQR2H6Ym8C6o9q3+cuk1yHZV+qiBCkz29Ds0nT/1S2+EoP26mUlckcDTgKjdjCYVdEwaXBCZQNEUVR5NUSRI5cZqJSZBYBfDMkBAKzffUy1NbTHPzBB/WAWCKibZalBtwRu/mprZ36kpCYYYPKNfdVyqYGUaR3XTomBRAp2t1TU8XcltYnBrIr6ZSfBqBfQ5HDjUK1N7eUQRYwUzKrVmgsAzhrmLTX4bGdVzP0ClHrM9oqBzCwQODyhVt2FaJw0yjZLwc1fgDTS1vscWt4EJtW0d5aZHVmQCoNOwPFGOw77SnSIAjGYVZFRr8PAHO8kMPabpXhi9X30qeZO/eL+mUgw6lBZ34ydMVaXHktlBoA/M/sDM7PdUtOofFsuiX8TmDZ3+FfWNeNovR06ARjVO7XDcxOMegzL936SybpZaguDWZX5J4E1qrwSosiJhcxsglGPMwZmAwBKd1Wpto62xMLAhEBSZrbseFNMbpjTCqlmVslOBhJ/r1ltZmalrOyQ3BRYTJ3XF3MSGHWEwazK/B0NuPGC4ofarbkkZw311s1+tit26mZdbo8cgOSmqd+aC/AGRgVp3ATWXdVyzazy/121PgXM3182PaTz/XWztYqsh7SNwazK/B0NmJml+OFvzaXcju5QSC26vv35BOpssZFxrGq0wyMCep2g6MjTruLwhO6TygyUrpkFtJ+Z3RpivaxECnq3HaqD2xNbNfCkPgazKpM6Guw71giX26PyaogiQ+3WXJLCTAsG5iTB7RGxYe9xVdcikTd/pZih0ym3472rpKb17GgQvmiMspVoeQOYxyPKAxBCzcwOzElGkkkPq8ONvceY/KFgDGZV1js9ERaTHg63BweqrWovhygi/K251P8Rc9ZQqatBbJQaxNLAhEDS8AQGs+GrjmIwm5Piff9oMTO7/3gjGu0uJBr1GNwrOaT76HWC/OkB62apJfV/0/RwOp2Awb5SA3Y0oHjhz8yqW2YA+Ft0le6ugicGPp6MtU4GksBNYPXcBBYWKTObHZUyA+9zaLGbwZYK7x9Mo3unwaAPPQyR62bZQo5aYDAbA4bmev8yjbX2QUThssbA0ATJKf0yYDHpUdVgx44j6m9uirWBCZLMJBP6ZCQCAH5kdjYsNdwAFhJ/vWxal+43jh0NqB0MZmOAtAlsN4NZihPSiE01W3NJzAY9Jg6SWnSpX2pwNEbLDIDA4QkMZruq2elGo90FIEo1s75gttHukst6tGJriGNsW5LO31nZoOkxvhR5DGZjwLA8b8NolhlQvLA6vL/U1d4AJpkit+hSv9/s0QYpmI2dTgYSjrUNn5SVNeoFpCYoX16TYjbAZNDeSNtmpxs/+T4hGdvJGNuW8tMSkJNihtsjYvthvkfJj8FsDBiS5y0zOFDdxL82KS7YYqjMAPC36Pq+/ARqrerWGMbawIRAo7kJLGxSMJthMUEQlO9SIQgCcnztuao0FMz+dKQeTreIrICyllAJgiAHwFLdLRHAYDYm5CSbkWExwiOCLUcoLjQ7vW3mYiUz2zs9EUNzU+ARgf/tUbdFl9SaKzfGamYBfzD7c7UVdVZuAuuKaHYykEilBsc1VDcb2F82nKB/XCE7GlBrDGZjgCAIAWNtWWpA2uZye+Bwx1YwC/hLDUpVbNHVaHfJdZWxWDObbjGhb6YFAPAjP8btkhpplG0UOhlItJiZ3eqrx+5qiYFEqpvdyo4GFIDBbIyQxtqybpa0zhZQKhMrZQaAv9RgvYotuqS2XMlmA5LN6rcta4uUnf2Bm8C6pFqa/hXFqW45Kb72XA3aac8VbicDyZje6QC8nx5IpR1EDGZjhH+sLYNZ0japXlYQALMhdn7EnNIvA8lmA6qbHKrVhPo7GcTe5i+JfxNYrboL0ZhoTv+SaG2kbZ3Vif3HmwCEn5lNsxgxIDsJALOz5Bc7v2l6OGmsLdtzkdZJmVmLUR+VjTChMup1mCS36FKnq0Gs9pgNNIabwMLiz8xGscxAY71mpeCzKMuCjG68TnKpAetmyYfBbIyQpoAdrmvm9B3StFgamNDSWcOkFl3q1M1KwWws1stKRvqC2YoaG07wY9yQyRvAolgzq7XMrFxiEGZWVjKWY22pBQazMSIt0Yh8X7aG2VnSMnmUbQwGs1Ld7NaDtahWIQA4GsNtuSRpiUb0y/JuAmN2NnTyBjA1MrNaCWbDHJbQkn8TWB1EUf0R1aQ+BrMxhHWzFA+apcxsDHUykOSmJmB4fipEEfhchRZdcluuGA5mAWC0L3PGYDZ00RxlK5EzsxooMxBFUe4NO66bwezw/FQY9QJqmhw4eMIWgdWR1jGYjSFDWTdLccAaw8EsAJw1VL1SAy2UGQDA6N7eqYQcaxs6VfrM+koamhxueeperDpc14zjjXYYdAJGFqR267ESjHoMz/c+xhaWGhAYzMYUZmYpHsRymQEAnDXM36LLHeUWXUc1sAEMAEb72h8xMxsah8uDhmZvMJkdxZrZZLMBCUbfSNsYb88l1bcOy09BQgT+0JXqblk3SwCD2ZgidTTYVdnAOiDSLFuMZ2bHF6YjNcGAWqszqq193B4Rx3wfB8dyzSwAjPJlZg/V2lSpLdaaE74RyXqdgNQEY9SeVxAEudQg1utmI7X5S8LhCRQoNrt2x5Mf/w38/GVIpw7ziLjXWA7RATR/sDJmM1tABNotRaRlUwiP0enzRKh1VCSeJxKvSZuP0c7jtvt8nayjk3UOP1SH2w3HMag+GVj7SXTW14VzDQAWZx7BnmMNsK/+P2BQTjv3DWd97Z0L2Owu/E7YD8EA5GzZ0fr+rR6vs9sjub7g4ykA7kjdh+NNDpxYtw1ZOclde9ywvpdQv9/uPE8o752uP4arrhmX6Xcj2WyEbmtNCI8RxvO08xgXG3fhgM4Kw/bDQH1GG+e0df+OHluZ19CwZxfO1jVgpvE4sLO868/TYo1neKyYovsRxkM6uHc1QK/Ttb5fq6UG3haJ1yPE2zr63kK+rSvraPdKiGvs5Hj2YMAQW72yBVHFFGBJSQnuvvvuoGNDhw7Fzp07273PO++8gzvvvBMHDhzA4MGD8eCDD+Lcc88N+Tnr6+uRlpaGuro6pKZ2r24nJP/9E7D5eeWfh4iIiEhpt3wPZA5Q/Gm6Eq+pnpkdOXIk1q5dK183GNpf0pdffok5c+Zg8eLFOP/88/HGG2/goosuwnfffYdRo0ZFY7ldN/gcwJId8un/+eEw9lU14szBOTipbwd/ZYclRkoXQvr7qZNzIvI3WAiPofBam10evLW5HHanGxef1Kf9ertYec1CeIxvf67BlopaDM9PwRkDA977bd63ncdr93naON6Vc33n25xuvPvtQQDApSf3QaKxnYqrCK7j4AkbPt9zHJlJJkwfmdvFx27n1C69fqGs2X99z7FG/HioFvlpCTi9f1YYzxfqc7ZxXgfrCvmcUN7vXX6MttdRWd+M7YfrkWEx4qS+6SGso6PnCfE23/G9VY2orGtG38xE9M20dPK8rdce+rrauL2zc3y32Zxu7DhSD73gLTMQhNDu19ltZccb0Wh3oXd6onfjXYevY+uld/n77cZ/p24dD+k91MntkVyjEHufGqsezBoMBuTl5YV07mOPPYYZM2bg9ttvBwDce++9WLNmDZ588kk888wzSi4zfEOmey8h2uPchceP7MWRpEKcdNYYBRdGseDB/2zHS9YDAIDPqrLwxkXFMTU1KxyfrvwJ/yrbj+v7D8AZM4arvZw2JQJ4++AX2HaoDol9x+LSk/so/pyfbfoZd/70I87unYvpF5yi+PN1V83+avzxX5uQ70zAxkumqr2cmLbyizLc8/MOnFeUj5N+dVJUn/ujNbvx+Lo9mNuvL/4xe3RUnztU/9lcgT//+wecPiATK66bELHHfXvVTiwr3YcriwrxwCX8fdmTqb4BbM+ePSgoKMCAAQMwd+5clJeXt3vuxo0bMW3atKBj06dPx8aNG9u9j91uR319fdAllg3N86bS2dEg/u0+2oBXNv4MADDoBGzcX41Pt1eqvKruk1oERWLHspKmRLlFlxYGJgQa2TsNggAcqWvWzLhUtUg9ZqM5MEGS4/s0J5angG2J0LCElqTNZGzPRaoGs8XFxVi+fDlWrVqFZcuWoaysDJMnT0ZDQ9uBXGVlJXJzc4OO5ebmorKy/QBg8eLFSEtLky+FhYUR/R4ibWied6PFnqMN8ES5bRBFjyiKuPs/2+H2iDhnRC5umDIQAHDff39Cs6+1lVbZHB4AgCVmNzB6SdPAPt9dBZfbo/jzHZV7zMbWxon2JJsNGOjb+PUjW3R1qFoOZqP/31aeAhbDf3BInQzGRaiTgUQavrD7aEPM99klZakazM6cOROXXXYZxowZg+nTp2PlypWora3F22+/HbHnWLhwIerq6uRLRUVFxB5bCUVZSTDpdWhyuHGolpNN4tWn2yuxYW81TAYd/n7eCPxuykDkpSbg4AkbXviiTO3ldYvN6f2lEqutuSTjCtORbjGivtmF76OQ2dHKwIRAo3unAQB+4PCEDkmjbDOj2GNWIk8Ba4zNPrPNTjd2+gYBRTozm5eWgNxUMzwi8OOh2P7UlZSleplBoPT0dAwZMgR79+5t8/a8vDwcPXo06NjRo0c7rLk1m81ITU0NusQyo16HATlJALz9Zin+NDvduPfjnwAA/+/MAeibZYHFZMDCc4cBAJ76bK+cxdMiuc9sjGdm9ToBZw72lRrsVL7UQCsDEwJJwSyHJ3SsulG9MgN/MBubmdnth+vg9ojISTEjX4H3PocnEBBjwWxjYyP27duH/Pz8Nm+fMGEC1q1bF3RszZo1mDAhcgXlsUAaa8u62fj07Pr9OFRrQ35aAn7vKy8AgAvGFuDkogxYHW48uKr99nSxLtbH2QY6a5g3mC3dVaX4c1VqrGYWAMb0kYLZWnUXEuNqVBhlK5HKDKwON5rssfdR+5YK7x9C3i4Gkd/cKmV7t3B4Qo+majC7YMECrF+/HgcOHMCXX36J2bNnQ6/XY86cOQCAq6++GgsXLpTPv/XWW7Fq1SosXboUO3fuRElJCb755hvcdNNNan0LipDG2u5mMBt3Dp6w4ulS7ycPfz13OCwmf0MRQRBw1/kjAADvfXcI35efUGWN3SXV/MZ6zSwAnDk4B4IA7DhSLwebSrA53Kj3jTvN1VBmdkRBKnQCcLTejmMa/rRAadUqbgBLMhvkPxxjMTsrbc4aV5imyONLdbPMzPZsqgazBw8exJw5czB06FBcfvnlyMrKwqZNm5CT482WlJeX48iRI/L5Z5xxBt544w3861//wtixY/Huu+/igw8+iN0es2EKHGtL8WXxyp2wuzw4rX8mzh/T+hOIsYXpcpuou/+zQ5ObAG1O7WRms5LNGOP7mHL9buVKDaQSg0SjHilm1TsihsxiMmBQL+8mMJYatM3p9qDO5gSgTmYWiO1NYPLmr8JI9033Gu379ODgCVtMBvMUHar+VF2xYkWHt5eWlrY6dtlll+Gyyy5TaEWxQcrM7qtqhNPtgVEfU9UgFKYv9x3Hf7cdgU4ASmaNbPcjtz9PH4pPth3BlopafLDlEC4+SfkeqJEklRkkaCAzCwBnDc3B1opalO6qwhWn9lXkOSoD6mW11kd4dO907D7aiB8O1mHq8NzO79DDnLB6s7KCAKRb1Alms5NNKK+xxlwwV9PkQHmNFYA/6Iy01AQjBuYkYV9VE344WItfDuN7tCdilBSDeqcnIsmkh9Mt4sDxJrWXQxHgcntw90c7AABzi4swoqD9jYi9UhNw0y8HAwAe+GRnTNbBdURLZQYAcJbUomvPcTgVatGltbZcgUb39r5XmZltm1wvazFBr1PnD5X89EQA3tZ+63crX/8dqq2+OtYBOUlISzQq9jxy3WwF36M9FYPZGKTTCRicy01g8eT1r8qx62gD0i1G/OmcIZ2e/5tJ/VCUZcGxBjuWle6LwgojR0sbwADvjv2sJBMa7S58c0CZOmUtbv6SjPaVYWw7VAcxImOk40tNo3qbvyQ3/3IQeqcn4uAJG+a9+DX+sOJ7VMdAllap/rItsW6WGMzGqKHSJjDWzWpedaMdS1fvAgAsOGdoSB9Fmg16/PVc7yjYf32+HxW+j+pinSiK/ppZjWRmdToBvxji62qgUN2s3GNWQ5u/JCPyvZvAqhrsOFqvfoAUa46r2MlAMiwvFav/eCZ+M7E/dALwwZbDmPrIerz77UFV/wCRgstI95dtSW7PdbCWf3D1UAxmY9QQtueKGw+v3o36ZheG56dizmmh12SeMyIXEwdlweHy4P6VPym4wsixuzyQfpdoJTMLAFOGeUsNSncq8xHtMV8QmJuivWA20aSX6/h/YPujVmp8GdAsFQYmBEoyG3DXrBF4/4aJGJaXglqrEwve2YqrXvgaP1dHv1xNFEVs9Q3bUDqYHZafApNeh1qrU67RpZ6FwWyMYkeD+PDjoTqs2FwOALj7gpFdqqnztuoaCZ0AfPJjJb7cd1ypZUaMNDAB0FYwe+bgbOgE7x+PhxWYvFepwYEJgaThCRxr25qaPWbbMrYwHf+5eRL+PGMozAYdvth7HNMf/R+eWb8vKmObJQdP2FDT5IBRL2B4foqiz2U26DHctw9hC0sNeiQGszFKyoT8XGMNChBIO0RRxKKPtkMUvQMRTuuf2eXHGJqXgl+fXgQAuOc/O+CO8VZdUomBSa+DQUNdONItJozv620dpMQABalmVkujbANJO9F/YDDbSrUczMbO5j6jXocbpgzCp384E2cMzEKz04MHPtmJC57cELXsuhRUjshPhdmg/B+243zv0a3cBNYjaee3TQ+TnWxCZpIJogjsPdao9nIoDB9uOYxvfz6BRKNeHlUbjj9OG4K0RCN2VjbIWd5YJbflMmrvR8tZQ32jbXdFtm7W4xFxrCE+MrPbDnITWEs1Kg5M6Ey/7CS8/ttiPHTpGKQlGrHjSD0uemoD7v14B6wOZbukRKteViI9z1aWwvRI2vuN00MIgoAhud5m5ayb1Z5Gu0uuc73pl4OQn5YY9mNlJJnwx2neVl0Pf7oLdVZnRNaoBH9bLu0MBpBM8bXo+nLvcdhdkfs05ITVAafbGwDmJMdO9q4rhuenwqATUN3kwBEFJ6VpUXWMlRm0JAgCLjulEOv+9AtcMLYAHhF44YsynP3I/1Aa4T/cAklB5ViFOxlIpGD2x0N1irXYo9jFYDaGDeVYW8166rO9ONZgR99MC66d1L/bjzf39CIM7pWME1YnHlu3JwIrVIbclksjnQwCjchPRU6KGU0Od0RbdEn1stnJJpgM2vyRm2DUy+0CfzjIj3EDyZlZlTeAdSY72YzH54zHS/NPRe/0RByqtWH+S5tx64rvIz5sweX2yH2Jo5WZ7Z+VhJQEA+wuD/ea9EDa/MnaQwzN8xa0839MbSk73oQXPi8DANx5/ggkRGAjlFGvw12zRgAAXtl4AHuPxeZ7QkujbFsKbNH12c7IZaz8AxO0WWIgGSOVGhyqVXchMUbq55oVQzWzHTlrWK+gNl4fbjmMaRFu47X7aCOanR6kmA0YkJ0UkcfsjE4nBLXoop6FwWwMG5rnKzNgMKsp9328Aw63B2cOycG04b0i9riTB+dg2vBcuDwi7vn4p5isXbT56vC0mJkF/NPASiM4RamyzhvsaHFgQiB5ExgzszK3R0StzVv2E6tlBm1pr43Xr1/4Cnsi8EmgFEyOKUyDLopT0cYWSpvAaqP2nBQbGMzGMOljvcr65piukyS/z3Yew7qdx2DQCbjr/BEQhMj+IP/7ecNh1Av43+6qiG9UigQtZ2YBYNLgbOh1AvYea4zYoAotD0wINMYXzH6+5zh++/I32MagFiesDrmvcoZFuXGtSpHaeN0xYxjMBh027K3G9Ef/h9vf2dqtFnXy5q8o1ctK5MwsOxr0OAxmY1hqghEFvl+Au2P0Y2Xyc7g8uOfjHQCA30zqj0G9kiP+HP2yk/Cbid4a3Ps+/gkOV2xtdLA5vOvRamY2LdGIk+UWXZH5Y+GYFMxqcGBCoNG90zD/jH7QCcDan45i1pNf4DfLN/fovp5SvWy6xaipVnSBjHodfj9lID79w5mYPjIXHhF459uDmPJwKf7x3x044fseu0J6T4yLUr2sRHq+3cca0GhXtlsDxRZt/t/Xgwzh8ATNeHFDGcqONyE72YybfzlIsee56ZeDkJ1swv7jTXhl4wHFniccUrsfrWZmAWDKMN9o2wj1m/UPTNBGTWV7BEFAyQUjsea2X+Di8b2hE4D/23kMFz21AfNe/BrflUdu05xWVDfGdieDruiXnYRnrzoF791wBor7Z8Lh8uC5z8tw5pLP8OT/7Qm5lZfV4ZI3LUc7mO2VmoD8tASIIgd89DQMZmMcOxpow9H6Zjzh6zLwl5nDkJKg3EeOKQlG/Hm6t2/tY2v3RHwncnf4W3NpN5iV6mY37Dsufz/dofWBCS0NzEnGI1eMw7o/TcElJ/WBXidg/e4qXPz0l7jqha/wzYEatZcYNbHcYzZcJ/XNwIrrT8dL15yK4fmpaLC78PDq3fjFQ6V4ddPPnba92nawDh4RyE9LQC8V3vP+UoPaqD83qYfBbIwb6svM7mRmNqY9+MlONDncGFeYjovH91b8+S49uQ9G905Dg92Fpat3Kf58ofIPTdBuMDssLwV5qQlodnrwVVn3A7OjGh9l257+2UlYevlY/N+ffoHLT+kDg07A53uO49JnNuJXz23CV/ur1V6i4mqatNXJIFSCIOCsob3w35sn4bErx6EwMxFVDXbc+cGPOPuR9fjP1sPwtDONMNr9ZVvi8ISeicFsjBsSkJmNxd3rBHz7cw3e+/4QAODuC0ZGZfeuTidgka9V14rNFTHzkZotDjKzgiBgylCp1KB7dbPNTjdO+DZvar2bQXuKspKw5NKx+GzBFMw5rRAGnYAv91Xjin9twhXPbsSX+47H7c+u41KZQYz3mA2XTifgwnG9se62Kbj7gpHITjbhQLUVN7/5PS546gv8b3dVq/+20uaraPWXbcnf0SA2fiZSdGhvTE8PM6hXMnQCUGt1oqrBHrWPbRwuD6wOF6wON6wOF5rsbjQ5XLA53EhJMCI72YScFDOSzYaI79gP5PaNAj1ca8Oh2mYcqbXhcK0Nh+uaYTbo0CfDgt4ZieiTnojeGYnonZ6IJHP03tZuj4iSj7ybvi4/pU9Uf4Cf0i8Ts8YW4D9bD+Oej3fgretPV/S/RShsDm13M5BMGdoLKzZXoHRXFRbNCv9xqhq8mTuTQYe0RO3tdu+KwkwLFl88BjeeNQjLSvfh7W8q8FVZDX713Fc4rV8mbp02GGcMzFL9PRpJ8Vhm0BaTQYd5Z/TDpSf3wfOfl+G5z/fjx0P1uPrFr3HGwCzcMWOY/LNvizzGNk2VtY7unQZBAA7V2nCsoRm9NLzxstnp9v6+q22GCBEZFu+Y+8wkk6Y//VICg9kYl2DUo19WEvYfb8Kuow1tBrNuj4hGuwtN0sXhRpPdFXSs0e723eYNSJscblh9162+8+XjDpc8frMzZoMO2clm5KSY5X9zkk3ITjEjJ9mM7IDjSSZ90C8yURRRZ3PicG2zL0C1+b+uteFIXTMq65vhbufjrPZkWIy+ANciB7h9MhLlY6mJ4QXgHo8Ih9sDu9MDu8sNu8uDT7dXYtuhOqSYDbjdV8caTX+ZOQxrdlTi67IarNxWifPG5Ed9DYHk1lwazswCwMRBWTDoBJQdb8KB403oF0bjd1EUccjX3igvNSGugriO9Mmw4B+zR8tB7VubK/D1gRrMff4rnFyUgVunDsbkwdlx8XrUxPgo20hLMhtw67TB+PXpffHUZ/vw2qaf8eW+alz41AacOzoP88/oj0O1NgiCN6hUQ0qCEYNykrHnWCN+qKjDtBFtB7MOlwc1TQ4cb7SjpsmB6iY7qhsd3q8bHahucuCE1YEEow5ZSd7fY1nJJmQnm5CVJH3t/Tec8d0ej4iqRjsOSQmaWlur34U1HXSSsJj0cnCbkWRCpsWIzCQzMpOMvuu+475LeqJ2O26EQhDj9fOfdtTX1yMtLQ11dXVITU1Vezkh+d2r32LV9koMzU1BWqLRG6Q6XHLA2uxUrj2TSa+DxaxHkskAi0mPBKMeDc1OHG90dLn1SYJRh5wUM7KSzGi0u3C41ibXWHbEoBOQl5aAgvREFPj+zU9PhN3pxsETNhw8YcOhWhsOnbCivrnzNSWbDd7gNj0RiSY97C6P9+J0+792uf1Bq9N7zNHBxoe/nzccv508oEuvR6Q8unY3Hl27BxkWI0Z18xeIRxTh9ngvLo8Ij+9ftyf4eNA5ogiX2wO3R4TN6YZHBP4xexTmFhdF6DtUx5x/bcLG/dUYkpuMtEQjnG4RLo8HLrf3+3a5PfIxt0f0fu32wBnw+khO65eJt383QcXvRj1H6mx4dv1+vPF1udxKLjPJhASDDiaDDkZ9wL/y10LQbSZ9i/MMOugFAZ3Fw10JlwN/EQb+VhQR/Csy8Lb3vz+E8horHrtyHC4cp3ytfKypqLHin2t34/3vDwW9LoN7JWPNbb9QbV0L3tmKd789iClDczAiP9UXrDpQLQWujQ40RLB1V6JRj6xkE7KSzchOMslfZ/m+tjr8GVYpeD1a3xxS0ijJpEdBeiL0OgE1vgA71GRTW4+VaNLDbPD+m2DUIdHo/b0uXRKNOt+/eph9/7Y8b9LgbCRH4RPQrsRrzMxqwNjCdKzaXoldnXQ0MOgEJJkNSDYbkGTW+782GZAUcCzJpIfF5L1u8QWpgdeTzHpYjAYkmvQdzpK3Odw43mhHVaMdVQ1279e+f483OFDVKH1tR5PDjWanBxU1NlTUBDfjzkoyeQPV9ATkp3mDTG/AmoDe6YnITjZDH2Idan2zE4dO2HDohA0HT1i9QW6tL+A9YUN1kzcI31nZ0K1NdYIAJBj0MBt1GF+Yjnln9Av7sbrr/505EO98cxCHam34fM9x1dYhMel1qmVlImnGqDxs3F+N3Ucbu/U4ggCcPSI3QqvSnvy0RJRcMBK/nzIQz67fj9e/+rnDjJMW5aclqr0EVRRmWvDI5eNw/ZkD8PCnu7D2J2+N+fi+6aqua2xhOt799iBKd1V12GJPrxOQmWSSg87MJF8AmmRCZrI3u9nscqO60YHjjd5gWAqKjzd6s7p2lwe2gMRKV+h1AvJSE1CQ7kvWSJc0//XUBEOrTzQb7S6caHKixupATZMdNU1OnGhyoMbqwAlf4B54vdbmhCjC+6ltCAmkzny2YEpUgtmuYGZWA2wON/677Qg8HlEOSr0BqxSoeoNUs0EXsx/dWR2uoAA32WzwBqxpCVGt/bE53L7g1hvoOlwemA16mA06mI06/9cGHcxG/9cJ0te+fw06IaZe68O1Nmw+UIPu/t8sCN4fsHpBgF4nwKAXoBMEGHQ67/GAi6GN6zpBQJrFiFQFW5NFi8vtQemuKlidbhh1Agx6HQx6AUbfa2HU+47pBBj1wcek8/U6QX7/kJe3tMj7/57T7f3Ew/u16L3uCjzmCTjmv93p9sDVSflRaP8viGiZv235v3XL/8tb3t4nw4LrJw+I6tjWWLX5QA3W/nQU88/op2qA39DsRMlHO2B3uZGdbPYGrMm+INVXIpCVZEJqgrFb/91EUYTV4Qt2fWUKUsBb1eAPfC2+7Kp06e1L3PRKMUflo3+3R0St1SF/kmtzumFzuNHscqPZ96/N4UGz0w2b0w2771/p3Gb54r3+zK9PRk6K8h08uhKvMZglIiIiopjSlXgtfquBiYiIiCjuMZglIiIiIs1iMEtEREREmsVgloiIiIg0i8EsEREREWkWg1kiIiIi0iwGs0RERESkWQxmiYiIiEizGMwSERERkWYxmCUiIiIizWIwS0RERESaxWCWiIiIiDSLwSwRERERaRaDWSIiIiLSLAazRERERKRZDGaJiIiISLMYzBIRERGRZjGYJSIiIiLNMqi9gGgTRREAUF9fr/JKiIiIiKgtUpwmxW0d6XHBbENDAwCgsLBQ5ZUQERERUUcaGhqQlpbW4TmCGErIG0c8Hg8OHz6MlJQUCIKg+PPV19ejsLAQFRUVSE1NVfz5egq+rpHH11QZfF0jj69p5PE1VQZf1/CJooiGhgYUFBRAp+u4KrbHZWZ1Oh369OkT9edNTU3lG1kBfF0jj6+pMvi6Rh5f08jja6oMvq7h6SwjK+EGMCIiIiLSLAazRERERKRZDGYVZjabsWjRIpjNZrWXElf4ukYeX1Nl8HWNPL6mkcfXVBl8XaOjx20AIyIiIqL4wcwsEREREWkWg1kiIiIi0iwGs0RERESkWQxmiYiIiEizGMwq7KmnnkK/fv2QkJCA4uJifP3112ovSdNKSkogCELQZdiwYWovS1P+97//YdasWSgoKIAgCPjggw+CbhdFEXfddRfy8/ORmJiIadOmYc+ePeosViM6e03nz5/f6n07Y8YMdRarEYsXL8app56KlJQU9OrVCxdddBF27doVdE5zczNuvPFGZGVlITk5GZdccgmOHj2q0oq1IZTXdcqUKa3er7/73e9UWnHsW7ZsGcaMGSMPRpgwYQI++eQT+Xa+T5XHYFZBb731Fm677TYsWrQI3333HcaOHYvp06fj2LFjai9N00aOHIkjR47Ily+++ELtJWlKU1MTxo4di6eeeqrN25csWYLHH38czzzzDL766iskJSVh+vTpaG5ujvJKtaOz1xQAZsyYEfS+ffPNN6O4Qu1Zv349brzxRmzatAlr1qyB0+nEOeecg6amJvmcP/7xj/jPf/6Dd955B+vXr8fhw4dx8cUXq7jq2BfK6woA1113XdD7dcmSJSqtOPb16dMHDzzwAL799lt88803+OUvf4kLL7wQ27dvB8D3aVSIpJjTTjtNvPHGG+XrbrdbLCgoEBcvXqziqrRt0aJF4tixY9VeRtwAIL7//vvydY/HI+bl5YkPPfSQfKy2tlY0m83im2++qcIKtaflayqKojhv3jzxwgsvVGU98eLYsWMiAHH9+vWiKHrfl0ajUXznnXfkc3766ScRgLhx40a1lqk5LV9XURTFX/ziF+Ktt96q3qLiQEZGhvj888/zfRolzMwqxOFw4Ntvv8W0adPkYzqdDtOmTcPGjRtVXJn27dmzBwUFBRgwYADmzp2L8vJytZcUN8rKylBZWRn0vk1LS0NxcTHft91UWlqKXr16YejQofj973+P6upqtZekKXV1dQCAzMxMAMC3334Lp9MZ9F4dNmwY+vbty/dqF7R8XSWvv/46srOzMWrUKCxcuBBWq1WN5WmO2+3GihUr0NTUhAkTJvB9GiUGtRcQr44fPw63243c3Nyg47m5udi5c6dKq9K+4uJiLF++HEOHDsWRI0dw9913Y/Lkyfjxxx+RkpKi9vI0r7KyEgDafN9Kt1HXzZgxAxdffDH69++Pffv24a9//StmzpyJjRs3Qq/Xq728mOfxePCHP/wBEydOxKhRowB436smkwnp6elB5/K9Grq2XlcA+NWvfoWioiIUFBTghx9+wB133IFdu3bhvffeU3G1sW3btm2YMGECmpubkZycjPfffx8jRozAli1b+D6NAgazpCkzZ86Uvx4zZgyKi4tRVFSEt99+G9dee62KKyNq35VXXil/PXr0aIwZMwYDBw5EaWkppk6dquLKtOHGG2/Ejz/+yPr4CGvvdb3++uvlr0ePHo38/HxMnToV+/btw8CBA6O9TE0YOnQotmzZgrq6Orz77ruYN28e1q9fr/ayegyWGSgkOzsber2+1Y7Fo0ePIi8vT6VVxZ/09HQMGTIEe/fuVXspcUF6b/J9q6wBAwYgOzub79sQ3HTTTfj444/x2WefoU+fPvLxvLw8OBwO1NbWBp3P92po2ntd21JcXAwAfL92wGQyYdCgQTj55JOxePFijB07Fo899hjfp1HCYFYhJpMJJ598MtatWycf83g8WLduHSZMmKDiyuJLY2Mj9u3bh/z8fLWXEhf69++PvLy8oPdtfX09vvrqK75vI+jgwYOorq7m+7YDoijipptuwvvvv4//+7//Q//+/YNuP/nkk2E0GoPeq7t27UJ5eTnfqx3o7HVty5YtWwCA79cu8Hg8sNvtfJ9GCcsMFHTbbbdh3rx5OOWUU3Daaafh0UcfRVNTE6655hq1l6ZZCxYswKxZs1BUVITDhw9j0aJF0Ov1mDNnjtpL04zGxsagDEtZWRm2bNmCzMxM9O3bF3/4wx9w3333YfDgwejfvz/uvPNOFBQU4KKLLlJv0TGuo9c0MzMTd999Ny655BLk5eVh3759+POf/4xBgwZh+vTpKq46tt14441444038OGHHyIlJUWuL0xLS0NiYiLS0tJw7bXX4rbbbkNmZiZSU1Nx8803Y8KECTj99NNVXn3s6ux13bdvH9544w2ce+65yMrKwg8//IA//vGPOPPMMzFmzBiVVx+bFi5ciJkzZ6Jv375oaGjAG2+8gdLSUnz66ad8n0aL2u0U4t0TTzwh9u3bVzSZTOJpp50mbtq0Se0ladoVV1wh5ufniyaTSezdu7d4xRVXiHv37lV7WZry2WefiQBaXebNmyeKorc915133inm5uaKZrNZnDp1qrhr1y51Fx3jOnpNrVareM4554g5OTmi0WgUi4qKxOuuu06srKxUe9kxra3XE4D40ksvyefYbDbxhhtuEDMyMkSLxSLOnj1bPHLkiHqL1oDOXtfy8nLxzDPPFDMzM0Wz2SwOGjRIvP3228W6ujp1Fx7DfvOb34hFRUWiyWQSc3JyxKlTp4qrV6+Wb+f7VHmCKIpiNINnIiIiIqJIYc0sEREREWkWg1kiIiIi0iwGs0RERESkWQxmiYiIiEizGMwSERERkWYxmCUiIiIizWIwS0RERESaxWCWiIiIiDSLwSwRUQ8lCAI++OADtZdBRNQtDGaJiFQwf/58CILQ6jJjxgy1l0ZEpCkGtRdARNRTzZgxAy+99FLQMbPZrNJqiIi0iZlZIiKVmM1m5OXlBV0yMjIAeEsAli1bhpkzZyIxMREDBgzAu+++G3T/bdu24Ze//CUSExORlZWF66+/Ho2NjUHnvPjiixg5ciTMZjPy8/Nx0003Bd1+/PhxzJ49GxaLBYMHD8ZHH32k7DdNRBRhDGaJiGLUnXfeiUsuuQRbt27F3LlzceWVV+Knn34CADQ1NWH69OnIyMjA5s2b8c4772Dt2rVBweqyZctw44034vrrr8e2bdvw0UcfYdCgQUHPcffdd+Pyyy/HDz/8gHPPPRdz585FTU1NVL9PIqLuEERRFNVeBBFRTzN//ny89tprSEhICDr+17/+FX/9618hCAJ+97vfYdmyZfJtp59+Ok466SQ8/fTTeO6553DHHXegoqICSUlJAICVK1di1qxZOHz4MHJzc9G7d29cc801uO+++9pcgyAI+Pvf/457770XgDdATk5OxieffMLaXSLSDNbMEhGp5KyzzgoKVgEgMzNT/nrChAlBt02YMAFbtmwBAPz0008YO3asHMgCwMSJE+HxeLBr1y4IgoDDhw9j6tSpHa5hzJgx8tdJSUlITU3FsWPHwv2WiIiijsEsEZFKkpKSWn3sHymJiYkhnWc0GoOuC4IAj8ejxJKIiBTBmlkiohi1adOmVteHDx8OABg+fDi2bt2KpqYm+fYNGzZAp9Nh6NChSElJQb9+/bBu3bqorpmIKNqYmSUiUondbkdlZWXQMYPBgOzsbADAO++8g1NOOQWTJk3C66+/jq+//hovvPACAGDu3LlYtGgR5s2bh5KSElRVVeHmm2/GVVddhdzcXABASUkJfve736FXr16YOXMmGhoasGHDBtx8883R/UaJiBTEYJaISCWrVq1Cfn5+0LGhQ4di586dALydBlasWIEbbrgB+fn5ePPNNzFixAgAgMViwaeffopbb70Vp556KiwWCy655BI88sgj8mPNmzcPzc3N+Oc//4kFCxYgOzsbl156afS+QSKiKGA3AyKiGCQIAt5//31cdNFFai+FiCimsWaWiIiIiDSLwSwRERERaRZrZomIYhArwIiIQsPMLBERERFpFoNZIiIiItIsBrNEREREpFkMZomIiIhIsxjMEhEREZFmMZglIiIiIs1iMEtEREREmsVgloiIiIg06/8DZyLnAOKttA4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "best_model = create_network(input_size, output_size, [best_hidden_sizes], [best_activation_class], output_activation_class=best_output_activation_class)\n",
    "best_network = VariableNetwork(best_model)\n",
    "best_optimizer = optim.Adam(best_network.parameters(), lr=best_config[\"lr\"])\n",
    "best_loss_fn = nn.MSELoss()\n",
    "\n",
    "best_train_losses, best_test_losses = train_network(best_network, best_optimizer, best_loss_fn,\n",
    "                                                        best_config[\"batch_size\"], best_config[\"epochs\"],\n",
    "                                                        X_train.dataset, outputs_train[i].dataset,\n",
    "                                                        X_test.dataset, outputs_test[i].dataset)\n",
    "\n",
    "# Print the test loss for the best model\n",
    "print(f\"Test loss for the best model: {best_test_losses[-1]}\")\n",
    "\n",
    "\n",
    "plot_losses(best_train_losses, best_test_losses, function_names[i])\n",
    "plot_predictions(best_network, X, outputs[i], function_names[i])\n",
    "\n",
    "# Save the network with hyperparameters in the file name\n",
    "torch.save(best_network, f\"best_network_{function_names[i]}.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two hidden layers\n",
    "\n",
    "<u>The code became messy</u> and I want to reconsider it.\n",
    "\n",
    "- We continue and now want to optimize for two hidden layers. We refactor the code to make use of an objective function, although this was not needed.\n",
    "\n",
    "- We can now also choose between the Adam and the SGD optimizer.\n",
    "\n",
    "- For clarity I will redefine previous functions and focus only on `h`.\n",
    "\n",
    "What I mean is that the **following section is self contained** and that I can just start running the notebook from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a new class that inherits from nn.Module\n",
    "class VariableNetwork(nn.Module):\n",
    "    # Define the constructor that takes the model as an argument\n",
    "    def __init__(self, model):\n",
    "        # Call the parent constructor\n",
    "        super().__init__()\n",
    "        # Assign the model to an attribute\n",
    "        self.model = model\n",
    "\n",
    "    # Override the forward function\n",
    "    def forward(self, x):\n",
    "        # Loop over the layers in the ModuleList\n",
    "        for layer in self.model:\n",
    "            # Apply the layer to the input\n",
    "            x = layer(x)\n",
    "        # Return the final output\n",
    "        return x\n",
    "\n",
    "def h(x1, x2, x3):\n",
    "    return x3 * x1**(x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the range and step size for the input variables\n",
    "x1_range = (0, 10)\n",
    "x2_range = (0, 10)\n",
    "x3_range = (0, 10)\n",
    "dx = 0.5\n",
    "\n",
    "# Generate the input data by sampling uniformly from the ranges\n",
    "x1 = np.arange(*x1_range, dx)\n",
    "x2 = np.arange(*x2_range, dx)\n",
    "x3 = np.arange(*x3_range, dx)\n",
    "X1, X2, X3 = np.meshgrid(x1, x2, x3)\n",
    "X = np.stack([X1.flatten(), X2.flatten(), X3.flatten()], axis=1)\n",
    "\n",
    "# Compute the output data by applying the functions\n",
    "Y_h = h(X[:, 0], X[:, 1], X[:, 2])\n",
    "\n",
    "# Convert the input and output data to torch tensors\n",
    "X = torch.from_numpy(X).float()\n",
    "Y_h = torch.from_numpy(Y_h).float().unsqueeze(1)\n",
    "\n",
    "# Split the data into train and test sets (80% train, 20% test)\n",
    "train_size = int(0.8 * len(X))\n",
    "test_size = len(X) - train_size\n",
    "X_train, X_test = torch.utils.data.random_split(X, [train_size, test_size])\n",
    "Y_h_train, Y_h_test = torch.utils.data.random_split(Y_h, [train_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to create a neural network with given hyperparameters\n",
    "def create_network(input_size, output_size, hidden_sizes, activations, output_activation=None):\n",
    "    # Create a ModuleList to hold the layers\n",
    "    model = nn.ModuleList()\n",
    "    # Loop over the hidden sizes and activations\n",
    "    for hidden_size, activation in zip(hidden_sizes, activations):\n",
    "        # Add a linear layer with the input size and hidden size\n",
    "        model.append(nn.Linear(input_size, hidden_size))\n",
    "        # Use a batch normalization layer between linear and activation layers to improve training stability\n",
    "        #model.append(nn.BatchNorm1d(hidden_size))\n",
    "        # Add an activation layer with the given activation function\n",
    "        model.append(activation())\n",
    "        # Update the input size for the next layer\n",
    "        input_size = hidden_size\n",
    "    # Add the final output layer with the output size\n",
    "    model.append(nn.Linear(input_size, output_size))\n",
    "    # If an output activation function is specified, add it to the model\n",
    "    if output_activation:\n",
    "        model.append(output_activation())\n",
    "    # Return the model\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to train a neural network with given hyperparameters and data\n",
    "def train_network(model, optimizer, loss_fn, batch_size, epochs, x_train, Y_train, X_test=None, Y_test=None):\n",
    "    # Create a data loader for the training data\n",
    "    train_loader = DataLoader(\n",
    "        dataset=torch.utils.data.TensorDataset(X_train, Y_train),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    # Initialize a list to store the training losses\n",
    "    train_losses = []\n",
    "    # Initialize a list to store the test losses if test data is given\n",
    "    if X_test is not None and Y_test is not None:\n",
    "        test_losses = []\n",
    "    # Loop over the number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Initialize a variable to store the running loss for this epoch\n",
    "        running_loss = 0.0\n",
    "        # Loop over the batches of training data\n",
    "        for inputs, targets in train_loader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass: compute the outputs from the inputs\n",
    "            outputs = model(inputs)\n",
    "            # Compute the loss from the outputs and targets\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            # Backward pass: compute the gradients from the loss\n",
    "            loss.backward()\n",
    "            # Update the parameters using the optimizer\n",
    "            optimizer.step()\n",
    "            # Accumulate the running loss\n",
    "            running_loss += loss.item()\n",
    "        # Compute and append the average training loss for this epoch\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        # Print the progress\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}\")\n",
    "        # If test data is given, compute and append the test loss for this epoch\n",
    "        if X_test is not None and Y_test is not None:\n",
    "            # Compute the outputs from the test inputs\n",
    "            outputs = model(X_test)\n",
    "            # Compute the loss from the outputs and test targets\n",
    "            loss = loss_fn(outputs, Y_test)\n",
    "            # Append the test loss\n",
    "            test_loss = loss.item()\n",
    "            test_losses.append(test_loss)\n",
    "            # Print the progress\n",
    "            print(f\"Epoch {epoch+1}, Test Loss: {test_loss:.4f}\")\n",
    "    # Return the train and test losses if test data is given, otherwise return only train losses\n",
    "    if X_test is not None and Y_test is not None:\n",
    "        return train_losses, test_losses\n",
    "    else:\n",
    "        return train_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to plot the losses during training\n",
    "def plot_losses(train_losses, test_losses=None, function_name=None, hyperparameters=\"\"):\n",
    "    # Create a figure and an axis\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    # Plot the train losses\n",
    "    ax.plot(train_losses, label=\"Train Loss\")\n",
    "    # If test losses are given, plot them as well\n",
    "    if test_losses is not None:\n",
    "        ax.plot(test_losses, label=\"Test Loss\")\n",
    "    # Set the title, labels, and legend\n",
    "    ax.set_title(f\"Losses during Training ({hyperparameters})\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend()\n",
    "    # Save and show the plot\n",
    "    # Use format method to insert hyperparameters into file name\n",
    "    plt.savefig(f\"losses_{function_name}_{hyperparameters}.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Define a function to plot the predictions versus the true values\n",
    "def plot_predictions(model, X, Y_true, function_name, hyperparameters=\"\"):\n",
    "    # Create a figure and an axis\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    # Compute the predictions from the inputs\n",
    "    Y_pred = model(X).detach().numpy()\n",
    "    # Plot the predictions and the true values as scatter plots\n",
    "    ax.scatter(Y_true, Y_pred, label=\"Predictions\", s=2, alpha=0.3)\n",
    "    ax.scatter(Y_true, Y_true, label=\"True Values\", s=2, alpha=0.3)\n",
    "    # Set the title, labels, and legend\n",
    "    ax.set_title(f\"Predictions versus True Values ({hyperparameters})\")\n",
    "    ax.set_xlabel(\"True Value\")\n",
    "    ax.set_ylabel(\"Predicted Value\")\n",
    "    ax.legend()\n",
    "    # Save and show the plot\n",
    "    # Use format method to insert hyperparameters into file name\n",
    "    plt.savefig(f\"predictions_{function_name}_{hyperparameters}.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_network(trial, input_size, output_size, create_network, train_loader, val_loader, num_epochs):\n",
    "    hidden_sizes = [trial.suggest_int(\"hidden_size1\", 16, 128), trial.suggest_int(\"hidden_size2\", 16, 128)]\n",
    "    activation_classes = [getattr(nn, trial.suggest_categorical(\"activation_class1\", [\"ReLU\", \"LeakyReLU\", \"Tanh\"])),\n",
    "                          getattr(nn, trial.suggest_categorical(\"activation_class2\", [\"ReLU\", \"LeakyReLU\", \"Tanh\"]))]\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
    "    optimizer_class = trial.suggest_categorical(\"optimizer_class\", [\"Adam\", \"SGD\"])\n",
    "\n",
    "    model = create_network(input_size, output_size, hidden_sizes, activation_classes)\n",
    "    model = VariableNetwork(model)\n",
    "\n",
    "    if optimizer_class == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_class == \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            x, y = batch\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x_val, y_val = batch\n",
    "                y_val_pred = model(x_val)\n",
    "                val_loss += loss_fn(y_val_pred, y_val).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, input_size, output_size, create_network, X_train, Y_h_train, X_test, Y_h_test, num_epochs):\n",
    "    # TODO: Update docstring.\n",
    "    \"\"\"Optimize the hyperparameters of a neural network using Optuna.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): The trial object that samples hyperparameters.\n",
    "        input_size (int): The size of the input layer of the network.\n",
    "        output_size (int): The size of the output layer of the network.\n",
    "        create_network (function): A function that takes input_size, output_size, hidden_sizes,\n",
    "            activation_classes and output_activation_class as arguments and returns a nn.Module object.\n",
    "        train_loader (torch.utils.data.DataLoader): The data loader for the training set.\n",
    "        val_loader (torch.utils.data.DataLoader): The data loader for the validation set.\n",
    "        num_epochs (int): The number of epochs to train the network.\n",
    "\n",
    "    Returns:\n",
    "        float: The validation loss of the network after training.\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert the subsets back to tensors\n",
    "    X_train_tensor = X[X_train.indices]\n",
    "    Y_h_train_tensor = Y_h[Y_h_train.indices]\n",
    "    X_test_tensor = X[X_test.indices]\n",
    "    Y_h_test_tensor = Y_h[Y_h_test.indices]\n",
    "\n",
    "    # Create DataLoaders for training and validation data\n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(torch.utils.data.TensorDataset(X_train_tensor, Y_h_train_tensor),\n",
    "                            batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(torch.utils.data.TensorDataset(X_test_tensor, Y_h_test_tensor),\n",
    "                            batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Sample hyperparameters\n",
    "    hidden_sizes = [\n",
    "        trial.suggest_int(\"hidden_sizes[0]\", 32, 256),\n",
    "        trial.suggest_int(\"hidden_sizes[1]\", 32, 256),\n",
    "    ]\n",
    "    activation_classes = [\n",
    "        trial.suggest_categorical(\"activation_classes[0]\", [\"ReLU\", \"Tanh\", \"Sigmoid\"]),\n",
    "        trial.suggest_categorical(\"activation_classes[1]\", [\"ReLU\", \"Tanh\", \"Sigmoid\"]),\n",
    "    ]\n",
    "    output_activation_class = trial.suggest_categorical(\"output_activation_class\", [None, \"Softmax\", \"Sigmoid\"])\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\"])\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 1e-3)\n",
    "\n",
    "    # Convert activation class names to actual classes\n",
    "    activation_classes = [getattr(nn, act_class_name) for act_class_name in activation_classes]\n",
    "    if output_activation_class:\n",
    "        output_activation_class = getattr(nn, output_activation_class)\n",
    "\n",
    "    # Train the network\n",
    "    model = create_network(input_size, output_size, hidden_sizes, activation_classes, output_activation_class)\n",
    "    optimizer_class = getattr(torch.optim, optimizer_name)\n",
    "    val_loss = tune_network(model, optimizer_class, learning_rate, train_loader, val_loader, num_epochs)\n",
    "\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-02 16:54:21,713]\u001b[0m A new study created in memory with name: no-name-aa6aecd9-4208-4e3a-bfdd-8b7569cc2427\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-05-02 16:57:40,076]\u001b[0m Trial 4 failed with parameters: {} because of the following error: TypeError('tune_network() takes 7 positional arguments but 9 were given').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_906/3438182241.py\", line 6, in <lambda>\n",
      "    study.optimize(lambda trial: tune_network(trial, input_size, output_size, create_network, X_train, Y_h_train, X_test, Y_h_test, num_epochs), n_trials=n_trials, timeout=timeout)\n",
      "TypeError: tune_network() takes 7 positional arguments but 9 were given\n",
      "\u001b[33m[W 2023-05-02 16:57:40,077]\u001b[0m Trial 4 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tune_network() takes 7 positional arguments but 9 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m n_trials \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m      5\u001b[0m timeout \u001b[39m=\u001b[39m \u001b[39m3600\u001b[39m\n\u001b[0;32m----> 6\u001b[0m study\u001b[39m.\u001b[39;49moptimize(\u001b[39mlambda\u001b[39;49;00m trial: tune_network(trial, input_size, output_size, create_network, X_train, Y_h_train, X_test, Y_h_test, num_epochs), n_trials\u001b[39m=\u001b[39;49mn_trials, timeout\u001b[39m=\u001b[39;49mtimeout)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/optuna/study/study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     _optimize(\n\u001b[1;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    435\u001b[0m     )\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[30], line 6\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      4\u001b[0m n_trials \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m      5\u001b[0m timeout \u001b[39m=\u001b[39m \u001b[39m3600\u001b[39m\n\u001b[0;32m----> 6\u001b[0m study\u001b[39m.\u001b[39moptimize(\u001b[39mlambda\u001b[39;00m trial: tune_network(trial, input_size, output_size, create_network, X_train, Y_h_train, X_test, Y_h_test, num_epochs), n_trials\u001b[39m=\u001b[39mn_trials, timeout\u001b[39m=\u001b[39mtimeout)\n",
      "\u001b[0;31mTypeError\u001b[0m: tune_network() takes 7 positional arguments but 9 were given"
     ]
    }
   ],
   "source": [
    "input_size = 3  # The number of input variables (x1, x2, x3)\n",
    "output_size = 1  # The number of output variables (y)\n",
    "num_epochs = 50\n",
    "n_trials = 100\n",
    "timeout = 3600\n",
    "study.optimize(lambda trial: tune_network(trial, input_size, output_size, create_network, X_train, Y_h_train, X_test, Y_h_test, num_epochs), n_trials=n_trials, timeout=timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create and train the best model using the optimization function\n",
    "best_model = find_best_model(optimization_function=h, search_space=search_space, n_trials=n_trials)\n",
    "best_network = VariableNetwork(create_network(*best_model[\"model_args\"]))\n",
    "best_optimizer = best_model[\"optimizer\"](best_network.parameters(), **best_model[\"optimizer_args\"])\n",
    "best_loss_fn = best_model[\"loss_fn\"]\n",
    "\n",
    "# Train the best model and obtain train and test losses\n",
    "best_train_losses, best_test_losses = train_network(\n",
    "    best_network, best_optimizer, best_loss_fn,\n",
    "    batch_size, num_epochs, X_train, Y_h_train, X_test, Y_h_test)\n",
    "\n",
    "# Print the test loss for the best model\n",
    "print(f\"Best Model Test Loss: {best_test_losses[-1]:.4f}\")\n",
    "\n",
    "plot_losses(best_train_losses, best_test_losses, function_name=\"h\")\n",
    "plot_predictions(best_network, X, Y_h, function_name=\"h\")\n",
    "\n",
    "# Save the network with hyperparameters in the file name\n",
    "torch.save(best_network, f\"best_network_h.pt\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3",
   "to": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

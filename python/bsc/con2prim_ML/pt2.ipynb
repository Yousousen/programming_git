{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network to learn conservative-to-primitive conversion in relativistic hydrodynamics\n",
    "\n",
    "We use Optuna to do a type of Bayesian optimization of the hyperparameters of the model. We then train the model using these hyperparameters to recover the primitive pressure from the conserved variables.\n",
    "\n",
    "Use this first cell to convert the notebook to a python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook pt2.ipynb to script\n",
      "[NbConvertApp] Writing 28853 bytes to pt2.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert pt2.ipynb --TagRemovePreprocessor.enabled=True --TagRemovePreprocessor.remove_cell_tags='{\"remove_cell\"}' --to script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import tensorboardX as tbx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if GPU is available and setting the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Defining some constants for convenience\n",
    "c = 1  # Speed of light\n",
    "gamma = 5 / 3  # Adiabatic index\n",
    "\n",
    "\n",
    "# Defining an analytic equation of state (EOS) for an ideal gas\n",
    "def eos_analytic(rho, epsilon):\n",
    "    \"\"\"Computes the pressure from rest-mass density and specific internal energy using an analytic EOS.\n",
    "\n",
    "    Args:\n",
    "        rho (torch.Tensor): The rest-mass density tensor of shape (n_samples,).\n",
    "        epsilon (torch.Tensor): The specific internal energy tensor of shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The pressure tensor of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    return (gamma - 1) * rho * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that samples primitive variables from uniform distributions\n",
    "def sample_primitive_variables(n_samples):\n",
    "    \"\"\"Samples primitive variables from uniform distributions.\n",
    "\n",
    "    Args:\n",
    "        n_samples (int): The number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of (rho, vx, epsilon), where rho is rest-mass density,\n",
    "            vx is velocity in x-direction,\n",
    "            epsilon is specific internal energy,\n",
    "            each being a numpy array of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    # Sampling from uniform distributions with intervals matching Dieseldorst et al.\n",
    "    rho = np.random.uniform(0, 10.1, size=n_samples)  # Rest-mass density\n",
    "    vx = np.random.uniform(0, 0.721 * c, size=n_samples)  # Velocity in x-direction\n",
    "    epsilon = np.random.uniform(0, 2.02, size=n_samples)  # Specific internal energy\n",
    "\n",
    "    # Returning the primitive variables\n",
    "    return rho, vx, epsilon\n",
    "\n",
    "\n",
    "# Defining a function that computes conserved variables from primitive variables\n",
    "def compute_conserved_variables(rho, vx, epsilon):\n",
    "    \"\"\"Computes conserved variables from primitive variables using equations (2) and (3) from the paper.\n",
    "\n",
    "    Args:\n",
    "        rho (torch.Tensor): The rest-mass density tensor of shape (n_samples,).\n",
    "        vx (torch.Tensor): The velocity in x-direction tensor of shape (n_samples,).\n",
    "        epsilon (torch.Tensor): The specific internal energy tensor of shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of (D, Sx, tau), where D is conserved density,\n",
    "            Sx is conserved momentum in x-direction,\n",
    "            tau is conserved energy density,\n",
    "            each being a torch tensor of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    # Computing the Lorentz factor from the velocity\n",
    "    gamma_v = 1 / torch.sqrt(1 - vx ** 2 / c ** 2)\n",
    "\n",
    "    # Computing the pressure from the primitive variables using the EOS\n",
    "    p = eos_analytic(rho, epsilon)\n",
    "\n",
    "    # Computing the conserved variables from the primitive variables\n",
    "    D = rho * gamma_v  # Conserved density\n",
    "    h = 1 + epsilon + p / rho  # Specific enthalpy\n",
    "    W = rho * h * gamma_v  # Lorentz factor associated with the enthalpy\n",
    "    Sx = W ** 2 * vx  # Conserved momentum in x-direction\n",
    "    tau = W ** 2 - p - D  # Conserved energy density\n",
    "\n",
    "    # Returning the conserved variables\n",
    "    return D, Sx, tau\n",
    "\n",
    "\n",
    "# Defining a function that generates input data (conserved variables) from random samples of primitive variables\n",
    "def generate_input_data(n_samples):\n",
    "    \"\"\"Generates input data (conserved variables) from random samples of primitive variables.\n",
    "\n",
    "    Args:\n",
    "        n_samples (int): The number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The input data tensor of shape (n_samples, 3).\n",
    "    \"\"\"\n",
    "    # Sampling the primitive variables using the sample_primitive_variables function\n",
    "    rho, vx, epsilon = sample_primitive_variables(n_samples)\n",
    "\n",
    "    # Converting the numpy arrays to torch tensors and moving them to the device\n",
    "    rho = torch.tensor(rho, dtype=torch.float32).to(device)\n",
    "    vx = torch.tensor(vx, dtype=torch.float32).to(device)\n",
    "    epsilon = torch.tensor(epsilon, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Computing the conserved variables using the compute_conserved_variables function\n",
    "    D, Sx, tau = compute_conserved_variables(rho, vx, epsilon)\n",
    "\n",
    "    # Stacking the conserved variables into a torch tensor\n",
    "    x = torch.stack([D, Sx, tau], axis=1)\n",
    "\n",
    "    # Returning the input data tensor\n",
    "    return x\n",
    "\n",
    "# Defining a function that generates output data (labels) from random samples of primitive variables\n",
    "def generate_labels(n_samples):\n",
    "    \"\"\"Generates output data (labels) from random samples of primitive variables.\n",
    "\n",
    "    Args:\n",
    "        n_samples (int): The number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The output data tensor of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    # Sampling the primitive variables using the sample_primitive_variables function\n",
    "    rho, _, epsilon = sample_primitive_variables(n_samples)\n",
    "\n",
    "    # Converting the numpy arrays to torch tensors and moving them to the device\n",
    "    rho = torch.tensor(rho, dtype=torch.float32).to(device)\n",
    "    epsilon = torch.tensor(epsilon, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Computing the pressure from the primitive variables using the EOS\n",
    "    p = eos_analytic(rho, epsilon)\n",
    "\n",
    "    # Returning the output data tensor\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: torch.Size([80000, 3])\n",
      "Shape of y_train: torch.Size([80000])\n",
      "Shape of x_test: torch.Size([10000, 3])\n",
      "Shape of y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "# Generating the input and output data for train and test sets using the functions defined\n",
    "# Using the same number of samples as Dieseldorst et al.\n",
    "x_train = generate_input_data(80000)\n",
    "y_train = generate_labels(80000)\n",
    "x_test = generate_input_data(10000)\n",
    "y_test = generate_labels(10000)\n",
    "\n",
    "# Checking the shapes of the data tensors\n",
    "print(\"Shape of x_train:\", x_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of x_test:\", x_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a class for the network\n",
    "class Net(nn.Module):\n",
    "    \"\"\"A class for creating a network with a variable number of hidden layers and units.\n",
    "\n",
    "    Attributes:\n",
    "        n_layers (int): The number of hidden layers in the network.\n",
    "        n_units (list): A list of integers representing the number of units in each hidden layer.\n",
    "        hidden_activation (torch.nn.Module): The activation function for the hidden layers.\n",
    "        output_activation (torch.nn.Module): The activation function for the output layer.\n",
    "        layers (torch.nn.ModuleList): A list of linear layers in the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_layers, n_units, hidden_activation, output_activation):\n",
    "        \"\"\"Initializes the network with the given hyperparameters.\n",
    "\n",
    "        Args:\n",
    "            n_layers (int): The number of hidden layers in the network.\n",
    "            n_units (list): A list of integers representing the number of units in each hidden layer.\n",
    "            hidden_activation (torch.nn.Module): The activation function for the hidden layers.\n",
    "            output_activation (torch.nn.Module): The activation function for the output layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_units = n_units\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.output_activation = output_activation\n",
    "\n",
    "        # Creating a list of linear layers with different numbers of units for each layer\n",
    "        self.layers = nn.ModuleList([nn.Linear(3, n_units[0])])  # Changed the input size to 3\n",
    "        for i in range(1, n_layers):\n",
    "            self.layers.append(nn.Linear(n_units[i - 1], n_units[i]))\n",
    "        self.layers.append(nn.Linear(n_units[-1], 1))  # Changed the output size to 1\n",
    "\n",
    "        # Adding some assertions to check that the input arguments are valid\n",
    "        assert isinstance(n_layers, int) and n_layers > 0, \"n_layers must be a positive integer\"\n",
    "        assert isinstance(n_units, list) and len(n_units) == n_layers, \"n_units must be a list of length n_layers\"\n",
    "        assert all(isinstance(n, int) and n > 0 for n in n_units), \"n_units must contain positive integers\"\n",
    "        assert isinstance(hidden_activation, nn.Module), \"hidden_activation must be a torch.nn.Module\"\n",
    "        assert isinstance(output_activation, nn.Module), \"output_activation must be a torch.nn.Module\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Performs a forward pass on the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, 3).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape (batch_size, 1).\n",
    "        \"\"\"\n",
    "        # Looping over the hidden layers and applying the linear transformation and the activation function\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.hidden_activation(layer(x))\n",
    "\n",
    "        # Applying the linear transformation and the activation function on the output layer\n",
    "        x = self.output_activation(self.layers[-1](x))\n",
    "\n",
    "        # Returning the output tensor\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model and search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to create a trial network and optimizer\n",
    "def create_model(trial):\n",
    "    \"\"\"Creates a trial network and optimizer based on the sampled hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): The trial object that contains the hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of (net, loss_fn, optimizer, batch_size, n_epochs,\n",
    "            scheduler), where net is the trial network,\n",
    "            loss_fn is the loss function,\n",
    "            optimizer is the optimizer,\n",
    "            batch_size is the batch size,\n",
    "            n_epochs is the number of epochs,\n",
    "            scheduler is the learning rate scheduler.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sampling the hyperparameters from the search space\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    n_units = [trial.suggest_int(f\"n_units_{i}\", 1, 256) for i in range(n_layers)]\n",
    "    hidden_activation_name = trial.suggest_categorical(\n",
    "        \"hidden_activation\", [\"ReLU\", \"Tanh\", \"Sigmoid\"]\n",
    "    )\n",
    "    output_activation_name = trial.suggest_categorical(\n",
    "        \"output_activation\", [\"Linear\", \"ReLU\"]\n",
    "    )  # Changed to only linear or ReLU\n",
    "    loss_name = trial.suggest_categorical(\n",
    "        \"loss\", [\"MSE\", \"MAE\", \"Huber\", \"LogCosh\"]\n",
    "    )  # Changed to only regression losses\n",
    "    optimizer_name = trial.suggest_categorical(\n",
    "        \"optimizer\", [\"SGD\", \"Adam\", \"RMSprop\", \"Adagrad\"]\n",
    "    )\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)  # Changed the lower bound to 1e-5\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 1, 512)\n",
    "    n_epochs = trial.suggest_int(\"n_epochs\", 10, 200)\n",
    "    scheduler_name = trial.suggest_categorical(\n",
    "        \"scheduler\",\n",
    "        [\"None\", \"StepLR\", \"ExponentialLR\", \"CosineAnnealingLR\", \"ReduceLROnPlateau\"],\n",
    "    )\n",
    "\n",
    "    # Creating the activation functions from their names\n",
    "    if hidden_activation_name == \"ReLU\":\n",
    "        hidden_activation = nn.ReLU()\n",
    "    elif hidden_activation_name == \"Tanh\":\n",
    "        hidden_activation = nn.Tanh()\n",
    "    else:\n",
    "        hidden_activation = nn.Sigmoid()\n",
    "\n",
    "    if output_activation_name == \"ReLU\":\n",
    "        output_activation = nn.ReLU()\n",
    "    else:\n",
    "        output_activation = nn.Identity()  # Changed to use identity function for linear output\n",
    "\n",
    "    # Creating the loss function from its name\n",
    "    if loss_name == \"MSE\":\n",
    "        loss_fn = nn.MSELoss()\n",
    "    elif loss_name == \"MAE\":\n",
    "        loss_fn = nn.L1Loss()\n",
    "    elif loss_name == \"Huber\":\n",
    "        loss_fn = nn.SmoothL1Loss()\n",
    "    else:\n",
    "        # Added creating the log-cosh loss function\n",
    "        def log_cosh_loss(y_pred, y_true):\n",
    "            \"\"\"Computes the log-cosh loss between the predicted and true values.\n",
    "\n",
    "            Args:\n",
    "                y_pred (torch.Tensor): The predicted values tensor of shape (batch_size, 1).\n",
    "                y_true (torch.Tensor): The true values tensor of shape (batch_size, 1).\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: The log-cosh loss tensor of shape ().\n",
    "            \"\"\"\n",
    "            return torch.mean(torch.log(torch.cosh(y_pred - y_true)))\n",
    "\n",
    "        loss_fn = log_cosh_loss\n",
    "\n",
    "    # Creating the network with the sampled hyperparameters\n",
    "    net = Net(\n",
    "        n_layers, n_units, hidden_activation, output_activation\n",
    "    ).to(device)  # Added moving the network to the device\n",
    "\n",
    "    # Creating the optimizer from its name\n",
    "    if optimizer_name == \"SGD\":\n",
    "        optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr=lr)\n",
    "    else:\n",
    "        # Added creating the Adagrad optimizer\n",
    "        optimizer = optim.Adagrad(net.parameters(), lr=lr)\n",
    "\n",
    "    # Creating the learning rate scheduler from its name\n",
    "    if scheduler_name == \"StepLR\":\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif scheduler_name == \"ExponentialLR\":\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    elif scheduler_name == \"CosineAnnealingLR\":\n",
    "        # Added creating the CosineAnnealingLR scheduler\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "    elif scheduler_name == \"ReduceLROnPlateau\":\n",
    "        # Added creating the ReduceLROnPlateau scheduler\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.1, patience=10\n",
    "        )\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    # Returning the network, the loss function, the optimizer, the batch size, the number of epochs and the scheduler\n",
    "    return net, loss_fn, optimizer, batch_size, n_epochs, scheduler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The train and eval loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that computes primitive variables from conserved variables and predicted pressure\n",
    "def compute_primitive_variables(x_batch, y_pred):\n",
    "    \"\"\"Computes primitive variables from conserved variables and predicted pressure using equations (A2) to (A5) from the paper.\n",
    "\n",
    "    Args:\n",
    "        x_batch (torch.Tensor): The input tensor of shape (batch_size, 3), containing conserved variables.\n",
    "        y_pred (torch.Tensor): The predicted pressure tensor of shape (batch_size, 1).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of (rho_pred, vx_pred, epsilon_pred), where rho_pred is rest-mass density,\n",
    "            vx_pred is velocity in x-direction,\n",
    "            epsilon_pred is specific internal energy,\n",
    "            each being a torch tensor of shape (batch_size,).\n",
    "    \"\"\"\n",
    "    # Extracting the conserved variables from x_batch\n",
    "    D_batch = x_batch[:, 0]  # Conserved density\n",
    "    Sx_batch = x_batch[:, 1]  # Conserved momentum in x-direction\n",
    "    tau_batch = x_batch[:, 2]  # Conserved energy density\n",
    "\n",
    "    # Computing the other primitive variables from y_pred and x_batch using equations (A2) to (A5) from the paper\n",
    "    rho_pred = D_batch / torch.sqrt(1 + Sx_batch ** 2 / D_batch ** 2 / c ** 2)  # Rest-mass density\n",
    "    vx_pred = Sx_batch / D_batch / c ** 2 / torch.sqrt(\n",
    "        1 + Sx_batch ** 2 / D_batch ** 2 / c ** 2\n",
    "    )  # Velocity in x-direction\n",
    "    epsilon_pred = (\n",
    "        tau_batch + D_batch\n",
    "    ) / rho_pred - y_pred / rho_pred - 1  # Specific internal energy\n",
    "\n",
    "    # Returning the primitive variables\n",
    "    return rho_pred, vx_pred, epsilon_pred\n",
    "\n",
    "# Defining a function that computes loss and metrics for a given batch\n",
    "def compute_loss_and_metrics(y_pred, y_true, x_batch, loss_fn):\n",
    "    \"\"\"Computes loss and metrics for a given batch.\n",
    "\n",
    "    Args:\n",
    "        y_pred (torch.Tensor): The predicted pressure tensor of shape (batch_size, 1).\n",
    "        y_true (torch.Tensor): The true pressure tensor of shape (batch_size,).\n",
    "        x_batch (torch.Tensor): The input tensor of shape (batch_size, 3), containing conserved variables.\n",
    "        loss_fn (torch.nn.Module or function): The loss function to use.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of (loss, rho_error, vx_error, epsilon_error), where loss is a scalar tensor,\n",
    "            rho_error is relative error for rest-mass density,\n",
    "            vx_error is relative error for velocity in x-direction,\n",
    "            epsilon_error is relative error for specific internal energy,\n",
    "            each being a scalar tensor.\n",
    "    \"\"\"\n",
    "    # Reshaping the target tensor to match the input tensor\n",
    "    y_true = y_true.view(-1, 1)\n",
    "\n",
    "    # Computing the loss using the loss function\n",
    "    loss = loss_fn(y_pred, y_true)\n",
    "\n",
    "    # Computing the other primitive variables from y_pred and x_batch using the compute_primitive_variables function\n",
    "    rho_pred, vx_pred, epsilon_pred = compute_primitive_variables(x_batch, y_pred)\n",
    "\n",
    "    # Computing the true values of the other primitive variables from y_true and x_batch using the compute_primitive_variables function\n",
    "    rho_true, vx_true, epsilon_true = compute_primitive_variables(x_batch, y_true)\n",
    "\n",
    "    # Computing the relative errors for the other primitive variables\n",
    "    rho_error = torch.mean(torch.abs((rho_pred - rho_true) / rho_true))  # Relative error for rest-mass density\n",
    "    vx_error = torch.mean(torch.abs((vx_pred - vx_true) / vx_true))  # Relative error for velocity in x-direction\n",
    "    epsilon_error = torch.mean(\n",
    "        torch.abs((epsilon_pred - epsilon_true) / epsilon_true)\n",
    "    )  # Relative error for specific internal energy\n",
    "\n",
    "    # Returning the loss and metrics\n",
    "    return loss, rho_error, vx_error, epsilon_error\n",
    "\n",
    "\n",
    "# Defining a function that updates the learning rate scheduler with validation loss if applicable\n",
    "def update_scheduler(scheduler, test_loss):\n",
    "    \"\"\"Updates the learning rate scheduler with validation loss if applicable.\n",
    "\n",
    "    Args:\n",
    "        scheduler (torch.optim.lr_scheduler._LRScheduler or None): The learning rate scheduler to use.\n",
    "        test_loss (float): The validation loss to use.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Checking if scheduler is not None\n",
    "    if scheduler is not None:\n",
    "        # Checking if scheduler is ReduceLROnPlateau\n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            # Updating the scheduler with test_loss\n",
    "            scheduler.step(test_loss)\n",
    "        else:\n",
    "            # Updating the scheduler without test_loss\n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a function to train and evaluate a network on the train and test sets\n",
    "def train_and_eval(net, loss_fn, optimizer, batch_size, n_epochs, scheduler):\n",
    "    \"\"\"Trains and evaluates a network on the train and test sets.\n",
    "\n",
    "    Args:\n",
    "        net (Net): The network to train and evaluate.\n",
    "        loss_fn (torch.nn.Module or function): The loss function to use.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer to use.\n",
    "        batch_size (int): The batch size to use.\n",
    "        n_epochs (int): The number of epochs to train for.\n",
    "        scheduler (torch.optim.lr_scheduler._LRScheduler or None): The learning rate scheduler to use.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of (train_losses, test_losses,\n",
    "            train_metrics, test_metrics), where train_losses and test_losses are lists of average losses per epoch for the train and test sets,\n",
    "            train_metrics and test_metrics are lists of dictionaries of metrics per epoch for the train and test sets.\n",
    "    \"\"\"\n",
    "    # Creating data loaders for train and test sets\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(x_test, y_test), batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Initializing lists to store the losses and metrics for each epoch\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_metrics = []\n",
    "    test_metrics = []\n",
    "\n",
    "    # Creating a SummaryWriter object to log data for tensorboard\n",
    "    writer = tbx.SummaryWriter()\n",
    "\n",
    "    # Looping over the epochs\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Setting the network to training mode\n",
    "        net.train()\n",
    "\n",
    "        # Initializing variables to store the total loss and metrics for the train set\n",
    "        train_loss = 0.0\n",
    "        train_rho_error = 0.0\n",
    "        train_vx_error = 0.0\n",
    "        train_epsilon_error = 0.0\n",
    "\n",
    "        # Looping over the batches in the train set\n",
    "        for x_batch, y_batch in train_loader:\n",
    "\n",
    "            # Moving the batch tensors to the device\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            # Zeroing the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Performing a forward pass and computing the loss and metrics\n",
    "            y_pred = net(x_batch)\n",
    "            loss, rho_error, vx_error, epsilon_error = compute_loss_and_metrics(\n",
    "                y_pred, y_batch, x_batch, loss_fn\n",
    "            )\n",
    "\n",
    "\n",
    "            # Performing a backward pass and updating the weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Updating the total loss and metrics for the train set\n",
    "            train_loss += loss.item() * x_batch.size(0)\n",
    "            train_rho_error += rho_error.item() * x_batch.size(0)\n",
    "            train_vx_error += vx_error.item() * x_batch.size(0)\n",
    "            train_epsilon_error += epsilon_error.item() * x_batch.size(0)\n",
    "\n",
    "        # Computing the average loss and metrics for the train set\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_rho_error /= len(train_loader.dataset)\n",
    "        train_vx_error /= len(train_loader.dataset)\n",
    "        train_epsilon_error /= len(train_loader.dataset)\n",
    "\n",
    "        # Appending the average loss and metrics for the train set to the lists\n",
    "        train_losses.append(train_loss)\n",
    "        train_metrics.append(\n",
    "            {\n",
    "                \"rho_error\": train_rho_error,\n",
    "                \"vx_error\": train_vx_error,\n",
    "                \"epsilon_error\": train_epsilon_error,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Logging the average loss and metrics for the train set to tensorboard\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Rho error/train\", train_rho_error, epoch)\n",
    "        writer.add_scalar(\"Vx error/train\", train_vx_error, epoch)\n",
    "        writer.add_scalar(\"Epsilon error/train\", train_epsilon_error, epoch)\n",
    "\n",
    "        # Setting the network to evaluation mode\n",
    "        net.eval()\n",
    "\n",
    "        # Initializing variables to store the total loss and metrics for the test set\n",
    "        test_loss = 0.0\n",
    "        test_rho_error = 0.0\n",
    "        test_vx_error = 0.0\n",
    "        test_epsilon_error = 0.0\n",
    "\n",
    "        # Looping over the batches in the test set\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in test_loader:\n",
    "\n",
    "                # Moving the batch tensors to the device\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                # Performing a forward pass and computing the loss and metrics\n",
    "                y_pred = net(x_batch)\n",
    "                loss, rho_error, vx_error, epsilon_error = compute_loss_and_metrics(\n",
    "                    y_pred, y_batch, x_batch, loss_fn\n",
    "                )\n",
    "\n",
    "\n",
    "                # Updating the total loss and metrics for the test set\n",
    "                test_loss += loss.item() * x_batch.size(0)\n",
    "                test_rho_error += rho_error.item() * x_batch.size(0)\n",
    "                test_vx_error += vx_error.item() * x_batch.size(0)\n",
    "                test_epsilon_error += epsilon_error.item() * x_batch.size(0)\n",
    "\n",
    "        # Computing the average loss and metrics for the test set\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_rho_error /= len(test_loader.dataset)\n",
    "        test_vx_error /= len(test_loader.dataset)\n",
    "        test_epsilon_error /= len(test_loader.dataset)\n",
    "\n",
    "        # Appending the average loss and metrics for the test set to the lists\n",
    "        test_losses.append(test_loss)\n",
    "        test_metrics.append(\n",
    "            {\n",
    "                \"rho_error\": test_rho_error,\n",
    "                \"vx_error\": test_vx_error,\n",
    "                \"epsilon_error\": test_epsilon_error,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Logging the average loss and metrics for the test set to tensorboard\n",
    "        writer.add_scalar(\"Loss/test\", test_loss, epoch)\n",
    "        writer.add_scalar(\"Rho error/test\", test_rho_error, epoch)\n",
    "        writer.add_scalar(\"Vx error/test\", test_vx_error, epoch)\n",
    "        writer.add_scalar(\"Epsilon error/test\", test_epsilon_error, epoch)\n",
    "\n",
    "        # Printing the average loss and metrics for both sets for this epoch\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, \"\n",
    "            f\"Train Rho Error: {train_rho_error:.4f}, Test Rho Error: {test_rho_error:.4f}, \"\n",
    "            f\"Train Vx Error: {train_vx_error:.4f}, Test Vx Error: {test_vx_error:.4f}, \"\n",
    "            f\"Train Epsilon Error: {train_epsilon_error:.4f}, Test Epsilon Error: {test_epsilon_error:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Updating the learning rate scheduler with validation loss if applicable\n",
    "        update_scheduler(scheduler, test_loss)\n",
    "\n",
    "    # Closing the SummaryWriter object\n",
    "    writer.close()\n",
    "\n",
    "    # Returning the losses and metrics lists\n",
    "    return train_losses, test_losses, train_metrics, test_metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining an objective function for Optuna to minimize\n",
    "def objective(trial):\n",
    "    \"\"\"Defines an objective function for Optuna to minimize.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): The trial object that contains the hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        float: The validation loss to minimize.\n",
    "    \"\"\"\n",
    "    # Creating a trial network and optimizer using the create_model function\n",
    "    net, loss_fn, optimizer, batch_size, n_epochs, scheduler = create_model(trial)\n",
    "\n",
    "    # Training and evaluating the network using the train_and_eval function\n",
    "    _, _, _, test_metrics = train_and_eval(\n",
    "        net, loss_fn, optimizer, batch_size, n_epochs, scheduler\n",
    "    )\n",
    "\n",
    "    # Returning the last validation epsilon error as the objective value to minimize\n",
    "    return test_metrics[-1][\"epsilon_error\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best parameters with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-05 12:58:29,603]\u001b[0m A new study created in memory with name: no-name-83175989-ba64-4907-a30e-f2f3d7e73b31\u001b[0m\n",
      "/tmp/ipykernel_16096/1816114563.py:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)  # Changed the lower bound to 1e-5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 2.8636, Test Loss: 2.6807, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.7901, Test Epsilon Error: 0.9323\n",
      "Epoch 2: Train Loss: 2.6295, Test Loss: 2.5572, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.8292, Test Epsilon Error: 0.9172\n",
      "Epoch 3: Train Loss: 2.5408, Test Loss: 2.4948, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.0224, Test Epsilon Error: 0.9137\n",
      "Epoch 4: Train Loss: 2.4920, Test Loss: 2.4578, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.9805, Test Epsilon Error: 0.9142\n",
      "Epoch 5: Train Loss: 2.4617, Test Loss: 2.4338, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.9940, Test Epsilon Error: 0.9163\n",
      "Epoch 6: Train Loss: 2.4414, Test Loss: 2.4175, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.8952, Test Epsilon Error: 0.9185\n",
      "Epoch 7: Train Loss: 2.4271, Test Loss: 2.4057, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.2950, Test Epsilon Error: 0.9206\n",
      "Epoch 8: Train Loss: 2.4165, Test Loss: 2.3970, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.8757, Test Epsilon Error: 0.9226\n",
      "Epoch 9: Train Loss: 2.4086, Test Loss: 2.3903, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.3410, Test Epsilon Error: 0.9245\n",
      "Epoch 10: Train Loss: 2.4025, Test Loss: 2.3852, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.8135, Test Epsilon Error: 0.9261\n",
      "Epoch 11: Train Loss: 2.3978, Test Loss: 2.3812, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.3603, Test Epsilon Error: 0.9276\n",
      "Epoch 12: Train Loss: 2.3940, Test Loss: 2.3780, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.9115, Test Epsilon Error: 0.9289\n",
      "Epoch 13: Train Loss: 2.3909, Test Loss: 2.3754, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.0425, Test Epsilon Error: 0.9300\n",
      "Epoch 14: Train Loss: 2.3884, Test Loss: 2.3733, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.3340, Test Epsilon Error: 0.9311\n",
      "Epoch 15: Train Loss: 2.3863, Test Loss: 2.3715, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.8530, Test Epsilon Error: 0.9320\n",
      "Epoch 16: Train Loss: 2.3846, Test Loss: 2.3701, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.8913, Test Epsilon Error: 0.9328\n",
      "Epoch 17: Train Loss: 2.3831, Test Loss: 2.3688, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.8608, Test Epsilon Error: 0.9335\n",
      "Epoch 18: Train Loss: 2.3819, Test Loss: 2.3678, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.8974, Test Epsilon Error: 0.9341\n",
      "Epoch 19: Train Loss: 2.3809, Test Loss: 2.3669, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.9161, Test Epsilon Error: 0.9347\n",
      "Epoch 20: Train Loss: 2.3800, Test Loss: 2.3661, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.0727, Test Epsilon Error: 0.9352\n",
      "Epoch 21: Train Loss: 2.3792, Test Loss: 2.3655, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.4400, Test Epsilon Error: 0.9356\n",
      "Epoch 22: Train Loss: 2.3785, Test Loss: 2.3649, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.3080, Test Epsilon Error: 0.9360\n",
      "Epoch 23: Train Loss: 2.3780, Test Loss: 2.3644, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.0748, Test Epsilon Error: 0.9363\n",
      "Epoch 24: Train Loss: 2.3775, Test Loss: 2.3640, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.0181, Test Epsilon Error: 0.9366\n",
      "Epoch 25: Train Loss: 2.3770, Test Loss: 2.3636, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.1151, Test Epsilon Error: 0.9369\n",
      "Epoch 26: Train Loss: 2.3766, Test Loss: 2.3632, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.1574, Test Epsilon Error: 0.9371\n",
      "Epoch 27: Train Loss: 2.3763, Test Loss: 2.3630, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.1646, Test Epsilon Error: 0.9373\n",
      "Epoch 28: Train Loss: 2.3760, Test Loss: 2.3627, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.1519, Test Epsilon Error: 0.9375\n",
      "Epoch 29: Train Loss: 2.3757, Test Loss: 2.3625, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.1904, Test Epsilon Error: 0.9377\n",
      "Epoch 30: Train Loss: 2.3755, Test Loss: 2.3623, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.0926, Test Epsilon Error: 0.9378\n",
      "Epoch 31: Train Loss: 2.3753, Test Loss: 2.3621, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.2014, Test Epsilon Error: 0.9380\n",
      "Epoch 32: Train Loss: 2.3751, Test Loss: 2.3619, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.8635, Test Epsilon Error: 0.9381\n",
      "Epoch 33: Train Loss: 2.3749, Test Loss: 2.3618, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.2460, Test Epsilon Error: 0.9382\n",
      "Epoch 34: Train Loss: 2.3748, Test Loss: 2.3617, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.0550, Test Epsilon Error: 0.9383\n",
      "Epoch 35: Train Loss: 2.3747, Test Loss: 2.3616, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.0946, Test Epsilon Error: 0.9384\n",
      "Epoch 36: Train Loss: 2.3746, Test Loss: 2.3615, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.8238, Test Epsilon Error: 0.9384\n",
      "Epoch 37: Train Loss: 2.3745, Test Loss: 2.3614, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.8641, Test Epsilon Error: 0.9385\n",
      "Epoch 38: Train Loss: 2.3744, Test Loss: 2.3613, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.8979, Test Epsilon Error: 0.9386\n",
      "Epoch 39: Train Loss: 2.3743, Test Loss: 2.3612, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.9244, Test Epsilon Error: 0.9386\n",
      "Epoch 40: Train Loss: 2.3742, Test Loss: 2.3612, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.1066, Test Epsilon Error: 0.9387\n",
      "Epoch 41: Train Loss: 2.3742, Test Loss: 2.3611, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.0559, Test Epsilon Error: 0.9387\n",
      "Epoch 42: Train Loss: 2.3741, Test Loss: 2.3611, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.9445, Test Epsilon Error: 0.9388\n",
      "Epoch 43: Train Loss: 2.3741, Test Loss: 2.3610, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.9162, Test Epsilon Error: 0.9388\n",
      "Epoch 44: Train Loss: 2.3740, Test Loss: 2.3610, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.9347, Test Epsilon Error: 0.9388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: Train Loss: 2.3740, Test Loss: 2.3609, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: inf, Test Epsilon Error: 0.9389\n",
      "Epoch 46: Train Loss: 2.3739, Test Loss: 2.3609, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.8826, Test Epsilon Error: 0.9389\n",
      "Epoch 47: Train Loss: 2.3739, Test Loss: 2.3609, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.8453, Test Epsilon Error: 0.9389\n",
      "Epoch 48: Train Loss: 2.3739, Test Loss: 2.3609, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.9310, Test Epsilon Error: 0.9389\n",
      "Epoch 49: Train Loss: 2.3738, Test Loss: 2.3608, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.1335, Test Epsilon Error: 0.9389\n",
      "Epoch 50: Train Loss: 2.3738, Test Loss: 2.3608, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.0274, Test Epsilon Error: 0.9390\n",
      "Epoch 51: Train Loss: 2.3738, Test Loss: 2.3608, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 2.7663, Test Epsilon Error: 0.9390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: Train Loss: 2.3738, Test Loss: 2.3608, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: inf, Test Epsilon Error: 0.9390\n",
      "Epoch 53: Train Loss: 2.3738, Test Loss: 2.3608, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.2023, Test Epsilon Error: 0.9390\n",
      "Epoch 54: Train Loss: 2.3738, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.9563, Test Epsilon Error: 0.9390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: Train Loss: 2.3737, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: inf, Test Epsilon Error: 0.9390\n",
      "Epoch 56: Train Loss: 2.3737, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.1946, Test Epsilon Error: 0.9390\n",
      "Epoch 57: Train Loss: 2.3737, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.8875, Test Epsilon Error: 0.9390\n",
      "Epoch 58: Train Loss: 2.3737, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.0443, Test Epsilon Error: 0.9390\n",
      "Epoch 59: Train Loss: 2.3737, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.9377, Test Epsilon Error: 0.9390\n",
      "Epoch 60: Train Loss: 2.3737, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.8891, Test Epsilon Error: 0.9390\n",
      "Epoch 61: Train Loss: 2.3737, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.3592, Test Epsilon Error: 0.9390\n",
      "Epoch 62: Train Loss: 2.3737, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.9391, Test Epsilon Error: 0.9391\n",
      "Epoch 63: Train Loss: 2.3737, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.9478, Test Epsilon Error: 0.9391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: Train Loss: 2.3737, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: inf, Test Epsilon Error: 0.9391\n",
      "Epoch 65: Train Loss: 2.3737, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.1239, Test Epsilon Error: 0.9391\n",
      "Epoch 66: Train Loss: 2.3737, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.0751, Test Epsilon Error: 0.9391\n",
      "Epoch 67: Train Loss: 2.3737, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.1781, Test Epsilon Error: 0.9391\n",
      "Epoch 68: Train Loss: 2.3737, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.8833, Test Epsilon Error: 0.9391\n",
      "Epoch 69: Train Loss: 2.3737, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.8865, Test Epsilon Error: 0.9391\n",
      "Epoch 70: Train Loss: 2.3737, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.2555, Test Epsilon Error: 0.9391\n",
      "Epoch 71: Train Loss: 2.3737, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.0357, Test Epsilon Error: 0.9391\n",
      "Epoch 72: Train Loss: 2.3737, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.8961, Test Epsilon Error: 0.9391\n",
      "Epoch 73: Train Loss: 2.3737, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.9281, Test Epsilon Error: 0.9391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: Train Loss: 2.3737, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: inf, Test Epsilon Error: 0.9391\n",
      "Epoch 75: Train Loss: 2.3736, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.0127, Test Epsilon Error: 0.9391\n",
      "Epoch 76: Train Loss: 2.3736, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.9974, Test Epsilon Error: 0.9391\n",
      "Epoch 77: Train Loss: 2.3736, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.8924, Test Epsilon Error: 0.9391\n",
      "Epoch 78: Train Loss: 2.3736, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.1531, Test Epsilon Error: 0.9391\n",
      "Epoch 79: Train Loss: 2.3736, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.3690, Test Epsilon Error: 0.9391\n",
      "Epoch 80: Train Loss: 2.3736, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.9228, Test Epsilon Error: 0.9391\n",
      "Epoch 81: Train Loss: 2.3736, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.9601, Test Epsilon Error: 0.9391\n",
      "Epoch 82: Train Loss: 2.3736, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.0152, Test Epsilon Error: 0.9391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83: Train Loss: 2.3736, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: inf, Test Epsilon Error: 0.9391\n",
      "Epoch 84: Train Loss: 2.3736, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.0957, Test Epsilon Error: 0.9391\n",
      "Epoch 85: Train Loss: 2.3736, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.8669, Test Epsilon Error: 0.9391\n",
      "Epoch 86: Train Loss: 2.3736, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.3711, Test Epsilon Error: 0.9391\n",
      "Epoch 87: Train Loss: 2.3736, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.6575, Test Epsilon Error: 0.9391\n",
      "Epoch 88: Train Loss: 2.3736, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.1165, Test Epsilon Error: 0.9391\n",
      "Epoch 89: Train Loss: 2.3736, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.0350, Test Epsilon Error: 0.9391\n",
      "Epoch 90: Train Loss: 2.3736, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 2.8179, Test Epsilon Error: 0.9391\n",
      "Epoch 91: Train Loss: 2.3736, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.6184, Test Epsilon Error: 0.9391\n",
      "Epoch 92: Train Loss: 2.3736, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.8843, Test Epsilon Error: 0.9391\n",
      "Epoch 93: Train Loss: 2.3736, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.9986, Test Epsilon Error: 0.9391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "\u001b[32m[I 2023-05-05 13:00:20,608]\u001b[0m Trial 0 finished with value: 0.9390785496234894 and parameters: {'n_layers': 3, 'n_units_0': 113, 'n_units_1': 4, 'n_units_2': 130, 'hidden_activation': 'Sigmoid', 'output_activation': 'Linear', 'loss': 'MAE', 'optimizer': 'Adagrad', 'lr': 0.0005517360370256159, 'batch_size': 466, 'n_epochs': 94, 'scheduler': 'ExponentialLR'}. Best is trial 0 with value: 0.9390785496234894.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94: Train Loss: 2.3736, Test Loss: 2.3607, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: inf, Test Epsilon Error: 0.9391\n",
      "Epoch 1: Train Loss: 1.9642, Test Loss: 2.1799, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.9998, Test Epsilon Error: 0.7286\n",
      "Epoch 2: Train Loss: 1.9574, Test Loss: 2.0102, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.7866, Test Epsilon Error: 0.4962\n",
      "Epoch 3: Train Loss: 1.9686, Test Loss: 1.8931, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.2114, Test Epsilon Error: 0.5732\n",
      "Epoch 4: Train Loss: 1.9667, Test Loss: 2.0107, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.3032, Test Epsilon Error: 0.4842\n",
      "Epoch 5: Train Loss: 1.9661, Test Loss: 1.8931, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.8028, Test Epsilon Error: 0.5647\n",
      "Epoch 6: Train Loss: 1.9742, Test Loss: 1.8899, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.2908, Test Epsilon Error: 0.5535\n",
      "Epoch 7: Train Loss: 1.9939, Test Loss: 2.1107, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 2.4285, Test Epsilon Error: 0.7153\n",
      "Epoch 8: Train Loss: 2.0026, Test Loss: 2.2687, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.1957, Test Epsilon Error: 0.7988\n",
      "Epoch 9: Train Loss: 2.0003, Test Loss: 1.8988, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.7809, Test Epsilon Error: 0.5715\n",
      "Epoch 10: Train Loss: 1.9994, Test Loss: 1.9533, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.9323, Test Epsilon Error: 0.5032\n",
      "Epoch 11: Train Loss: 2.0007, Test Loss: 1.9214, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.7765, Test Epsilon Error: 0.5177\n",
      "Epoch 12: Train Loss: 2.0200, Test Loss: 1.9845, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 1.5871, Test Epsilon Error: 0.4966\n",
      "Epoch 13: Train Loss: 2.0166, Test Loss: 1.8932, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.9791, Test Epsilon Error: 0.5685\n",
      "Epoch 14: Train Loss: 2.0222, Test Loss: 2.0162, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.7891, Test Epsilon Error: 0.6435\n",
      "Epoch 15: Train Loss: 2.0238, Test Loss: 1.9030, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.7280, Test Epsilon Error: 0.5285\n",
      "Epoch 16: Train Loss: 2.0234, Test Loss: 1.9098, Train Rho Error: 0.0000, Test Rho Error: 0.0000, Train Vx Error: 0.0000, Test Vx Error: 0.0000, Train Epsilon Error: 0.7022, Test Epsilon Error: 0.5922\n"
     ]
    }
   ],
   "source": [
    "# Creating a study object with Optuna with TPE sampler and median pruner\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner())\n",
    "\n",
    "# Running Optuna with 100 trials without sampler and pruner arguments\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Printing the best trial information\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the best network and optimizer using the best hyperparameters\n",
    "net, loss_fn, optimizer, batch_size, n_epochs, scheduler = create_model(trial)\n",
    "\n",
    "# Training and evaluating the best network using the train_and_eval function\n",
    "train_losses, test_losses, train_metrics, test_metrics = train_and_eval(\n",
    "    net, loss_fn, optimizer, batch_size, n_epochs, scheduler\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the losses and metrics for the best network\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot([m[\"rho_error\"] for m in train_metrics], label=\"Train Rho Error\")\n",
    "plt.plot([m[\"rho_error\"] for m in test_metrics], label=\"Test Rho Error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Rho Error\")\n",
    "plt.legend()\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot([m[\"vx_error\"] for m in train_metrics], label=\"Train Vx Error\")\n",
    "plt.plot([m[\"vx_error\"] for m in test_metrics], label=\"Test Vx Error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Vx Error\")\n",
    "plt.legend()\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot([m[\"epsilon_error\"] for m in train_metrics], label=\"Train Epsilon Error\")\n",
    "plt.plot([m[\"epsilon_error\"] for m in test_metrics], label=\"Test Epsilon Error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Epsilon Error\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEPRICATED: Saving and loading the data and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing torch and pickle for saving and loading\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "# Saving the data and the model to files\n",
    "torch.save(x_train, \"x_train.pt\")\n",
    "torch.save(y_train, \"y_train.pt\")\n",
    "torch.save(x_test, \"x_test.pt\")\n",
    "torch.save(y_test, \"y_test.pt\")\n",
    "torch.save(net_best.state_dict(), \"net_best.pt\")\n",
    "pickle.dump(study.best_params, open(\"best_params.pkl\", \"wb\"))\n",
    "\n",
    "# Loading the data and the model from files\n",
    "x_train = torch.load(\"x_train.pt\")\n",
    "y_train = torch.load(\"y_train.pt\")\n",
    "x_test = torch.load(\"x_test.pt\")\n",
    "y_test = torch.load(\"y_test.pt\")\n",
    "net_best = Net(**study.best_params).to(device) # Creating the network with the best hyperparameters\n",
    "net_best.load_state_dict(torch.load(\"net_best.pt\")) # Loading the network weights\n",
    "best_params = pickle.load(open(\"best_params.pkl\", \"rb\")) # Loading the best hyperparameters\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

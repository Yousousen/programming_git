{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network to learn conservative-to-primitive conversion in relativistic hydrodynamics\n",
    "\n",
    "We use Optuna to do a type of Bayesian optimization of the hyperparameters of the model. We then train the model using these hyperparameters to recover the primitive pressure from the conserved variables.\n",
    "\n",
    "Use this first cell to convert the notebook to a python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook script.ipynb to script\n",
      "[NbConvertApp] Writing 23953 bytes to script.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert script.ipynb --TagRemovePreprocessor.enabled=True --TagRemovePreprocessor.remove_cell_tags='{\"remove_cell\"}' --to script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import tensorboardX as tbx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if GPU is available and setting the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Defining some constants for convenience\n",
    "c = 1 # Speed of light\n",
    "gamma = 5/3 # Adiabatic index\n",
    "\n",
    "# Defining an analytic equation of state (EOS) for an ideal gas\n",
    "def eos_analytic(rho, epsilon):\n",
    "    \"\"\"Computes the pressure from the rest-mass density and specific internal energy using an analytic EOS.\n",
    "\n",
    "    Args:\n",
    "        rho (torch.Tensor): The rest-mass density tensor of shape (batch_size, 1).\n",
    "        epsilon (torch.Tensor): The specific internal energy tensor of shape (batch_size, 1).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The pressure tensor of shape (batch_size, 1).\n",
    "    \"\"\"\n",
    "    return (gamma - 1) * rho * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: torch.Size([8000, 3])\n",
      "Shape of y_train: torch.Size([8000])\n",
      "Shape of x_test: torch.Size([2000, 3])\n",
      "Shape of y_test: torch.Size([2000])\n"
     ]
    }
   ],
   "source": [
    "# Defining a function that generates input data (conserved variables) from random samples of primitive variables\n",
    "def generate_input_data(n_samples):\n",
    "    \"\"\"Generates input data (conserved variables) from random samples of primitive variables.\n",
    "\n",
    "    Args:\n",
    "        n_samples (int): The number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The input data tensor of shape (n_samples, 3).\n",
    "    \"\"\"\n",
    "    # Sampling the primitive variables from uniform distributions\n",
    "    rho = np.random.uniform(0.1, 10, size=n_samples) # Rest-mass density\n",
    "    vx = np.random.uniform(-0.9 * c, 0.9 * c, size=n_samples) # Velocity in x-direction\n",
    "    epsilon = np.random.uniform(0.1, 10, size=n_samples) # Specific internal energy\n",
    "\n",
    "    # Computing the Lorentz factor from the velocity\n",
    "    gamma_v = 1 / np.sqrt(1 - vx**2 / c**2)\n",
    "\n",
    "    # Computing the pressure from the primitive variables using the EOS\n",
    "    p = eos_analytic(rho, epsilon)\n",
    "\n",
    "    # Computing the conserved variables from the primitive variables using equations (2) and (3) from the paper \n",
    "    D = rho * gamma_v # Conserved density\n",
    "    h = 1 + epsilon + p / rho # Specific enthalpy\n",
    "    W = rho * h * gamma_v # Lorentz factor associated with the enthalpy\n",
    "    Sx = W**2 * vx # Conserved momentum in x-direction\n",
    "    tau = W**2 - p - D # Conserved energy density\n",
    "\n",
    "    # Stacking the conserved variables into a numpy array\n",
    "    x = np.stack([D, Sx, tau], axis=1)\n",
    "\n",
    "    # Converting the numpy array to a torch tensor and moving it to the device\n",
    "    x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Returning the input data tensor\n",
    "    return x\n",
    "\n",
    "# Defining a function that generates output data (labels) from random samples of primitive variables\n",
    "def generate_output_data(n_samples):\n",
    "    \"\"\"Generates output data (labels) from random samples of primitive variables.\n",
    "\n",
    "    Args:\n",
    "        n_samples (int): The number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The output data tensor of shape (n_samples, 1).\n",
    "    \"\"\"\n",
    "    # Sampling the primitive variables from uniform distributions\n",
    "    rho = np.random.uniform(0.1, 10, size=n_samples) # Rest-mass density\n",
    "    epsilon = np.random.uniform(0.1, 10, size=n_samples) # Specific internal energy\n",
    "\n",
    "    # Computing the pressure from the primitive variables using the EOS\n",
    "    p = eos_analytic(rho, epsilon)\n",
    "\n",
    "    # Converting the numpy array to a torch tensor and moving it to the device\n",
    "    y = torch.tensor(p, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Returning the output data tensor\n",
    "    return y\n",
    "\n",
    "# Generating the input and output data for train and test sets using the functions defined\n",
    "x_train = generate_input_data(8000)\n",
    "y_train = generate_output_data(8000)\n",
    "x_test = generate_input_data(2000)\n",
    "y_test = generate_output_data(2000)\n",
    "\n",
    "# Checking the shapes of the data tensors\n",
    "print(\"Shape of x_train:\", x_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of x_test:\", x_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the neural network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a class for the network\n",
    "class Net(nn.Module):\n",
    "    \"\"\"A class for creating a network with a variable number of hidden layers and units.\n",
    "\n",
    "    Attributes:\n",
    "        n_layers (int): The number of hidden layers in the network.\n",
    "        n_units (list): A list of integers representing the number of units in each hidden layer.\n",
    "        hidden_activation (torch.nn.Module): The activation function for the hidden layers.\n",
    "        output_activation (torch.nn.Module): The activation function for the output layer.\n",
    "        layers (torch.nn.ModuleList): A list of linear layers in the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_layers, n_units, hidden_activation, output_activation):\n",
    "        \"\"\"Initializes the network with the given hyperparameters.\n",
    "\n",
    "        Args:\n",
    "            n_layers (int): The number of hidden layers in the network.\n",
    "            n_units (list): A list of integers representing the number of units in each hidden layer.\n",
    "            hidden_activation (torch.nn.Module): The activation function for the hidden layers.\n",
    "            output_activation (torch.nn.Module): The activation function for the output layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_units = n_units\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.output_activation = output_activation\n",
    "\n",
    "        # Creating a list of linear layers with different numbers of units for each layer\n",
    "        self.layers = nn.ModuleList([nn.Linear(3, n_units[0])]) # Changed the input size to 3\n",
    "        for i in range(1, n_layers):\n",
    "            self.layers.append(nn.Linear(n_units[i - 1], n_units[i]))\n",
    "        self.layers.append(nn.Linear(n_units[-1], 1)) # Changed the output size to 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Performs a forward pass on the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, 3).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape (batch_size, 1).\n",
    "        \"\"\"\n",
    "        # Looping over the hidden layers and applying the linear transformation and the activation function\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.hidden_activation(layer(x))\n",
    "\n",
    "        # Applying the linear transformation and the activation function on the output layer\n",
    "        x = self.output_activation(self.layers[-1](x))\n",
    "\n",
    "        # Returning the output tensor\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to create a trial network and optimizer\n",
    "def create_model(trial):\n",
    "    \"\"\"Creates a trial network and optimizer based on the sampled hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): The trial object that contains the hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of (net, loss_fn, optimizer, batch_size, n_epochs,\n",
    "            scheduler), where net is the trial network,\n",
    "            loss_fn is the loss function,\n",
    "            optimizer is the optimizer,\n",
    "            batch_size is the batch size,\n",
    "            n_epochs is the number of epochs,\n",
    "            scheduler is the learning rate scheduler.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sampling the hyperparameters from the search space\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    n_units = [trial.suggest_int(f\"n_units_{i}\", 1, 256) for i in range(n_layers)]\n",
    "    hidden_activation_name = trial.suggest_categorical(\"hidden_activation\", [\"ReLU\", \"Tanh\", \"Sigmoid\"])\n",
    "    output_activation_name = trial.suggest_categorical(\"output_activation\", [\"Linear\", \"ReLU\"]) # Changed to only linear or ReLU\n",
    "    loss_name = trial.suggest_categorical(\"loss\", [\"MSE\", \"MAE\", \"Huber\", \"LogCosh\"]) # Changed to only regression losses\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"SGD\", \"Adam\", \"RMSprop\", \"Adagrad\"])\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1) # Changed the lower bound to 1e-5\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 1, 512)\n",
    "    n_epochs = trial.suggest_int(\"n_epochs\", 10, 200)\n",
    "    scheduler_name = trial.suggest_categorical(\"scheduler\", [\"None\", \"StepLR\", \"ExponentialLR\", \"CosineAnnealingLR\", \"ReduceLROnPlateau\"])\n",
    "\n",
    "    # Creating the activation functions from their names\n",
    "    if hidden_activation_name == \"ReLU\":\n",
    "        hidden_activation = nn.ReLU()\n",
    "    elif hidden_activation_name == \"Tanh\":\n",
    "        hidden_activation = nn.Tanh()\n",
    "    else:\n",
    "        hidden_activation = nn.Sigmoid()\n",
    "\n",
    "    if output_activation_name == \"ReLU\":\n",
    "        output_activation = nn.ReLU()\n",
    "    else:\n",
    "        output_activation = nn.Identity() # Changed to use identity function for linear output\n",
    "\n",
    "    # Creating the loss function from its name\n",
    "    if loss_name == \"MSE\":\n",
    "        loss_fn = nn.MSELoss()\n",
    "    elif loss_name == \"MAE\":\n",
    "        loss_fn = nn.L1Loss()\n",
    "    elif loss_name == \"Huber\":\n",
    "        loss_fn = nn.SmoothL1Loss()\n",
    "    else:\n",
    "        # Added creating the log-cosh loss function\n",
    "        def log_cosh_loss(y_pred, y_true):\n",
    "            \"\"\"Computes the log-cosh loss between the predicted and true values.\n",
    "\n",
    "            Args:\n",
    "                y_pred (torch.Tensor): The predicted values tensor of shape (batch_size, 1).\n",
    "                y_true (torch.Tensor): The true values tensor of shape (batch_size, 1).\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: The log-cosh loss tensor of shape ().\n",
    "            \"\"\"\n",
    "            return torch.mean(torch.log(torch.cosh(y_pred - y_true)))\n",
    "        \n",
    "        loss_fn = log_cosh_loss\n",
    "\n",
    "    # Creating the network with the sampled hyperparameters\n",
    "    net = Net(n_layers, n_units, hidden_activation, output_activation).to(device) # Added moving the network to the device\n",
    "\n",
    "    # Creating the optimizer from its name\n",
    "    if optimizer_name == \"SGD\":\n",
    "        optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr=lr)\n",
    "    else:\n",
    "        # Added creating the Adagrad optimizer\n",
    "        optimizer = optim.Adagrad(net.parameters(), lr=lr)\n",
    "\n",
    "    # Creating the learning rate scheduler from its name\n",
    "    if scheduler_name == \"StepLR\":\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif scheduler_name == \"ExponentialLR\":\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    elif scheduler_name == \"CosineAnnealingLR\":\n",
    "        # Added creating the CosineAnnealingLR scheduler\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "    elif scheduler_name == \"ReduceLROnPlateau\":\n",
    "        # Added creating the ReduceLROnPlateau scheduler\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=10)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    # Returning the network, the loss function, the optimizer, the batch size, the number of epochs and the scheduler\n",
    "    return net, loss_fn, optimizer, batch_size, n_epochs, scheduler\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The train and eval loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to train and evaluate a network on the train and test sets\n",
    "def train_and_eval(net, loss_fn, optimizer, batch_size, n_epochs, scheduler):\n",
    "    \"\"\"Trains and evaluates a network on the train and test sets.\n",
    "\n",
    "    Args:\n",
    "        net (Net): The network to train and evaluate.\n",
    "        loss_fn (torch.nn.Module or function): The loss function to use.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer to use.\n",
    "        batch_size (int): The batch size to use.\n",
    "        n_epochs (int): The number of epochs to train for.\n",
    "        scheduler (torch.optim.lr_scheduler._LRScheduler or None): The learning rate scheduler to use.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of (train_losses, test_losses,\n",
    "            train_metrics, test_metrics), where train_losses and test_losses are lists of average losses per epoch for the train and test sets,\n",
    "            train_metrics and test_metrics are lists of dictionaries of metrics per epoch for the train and test sets.\n",
    "    \"\"\"\n",
    "    # Creating data loaders for train and test sets\n",
    "    train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test, y_test), batch_size=batch_size)\n",
    "\n",
    "    # Initializing lists to store the losses and metrics for each epoch\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_metrics = []\n",
    "    test_metrics = []\n",
    "\n",
    "    # Looping over the epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # Setting the network to training mode\n",
    "        net.train()\n",
    "        # Initializing variables to store the total loss and metrics for the train set\n",
    "        train_loss = 0.0\n",
    "        train_rho_error = 0.0 # Added relative error for rest-mass density\n",
    "        train_vx_error = 0.0 # Added relative error for velocity in x-direction\n",
    "        train_epsilon_error = 0.0 # Added relative error for specific internal energy\n",
    "        # Looping over the batches in the train set\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            # Zeroing the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            y_pred = net(x_batch)\n",
    "            # Reshaping the target tensor to match the input tensor\n",
    "            y_batch = y_batch.view(-1, 1)\n",
    "            # Computing the loss\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Updating the total loss\n",
    "            train_loss += loss.item() * x_batch.size(0)\n",
    "            # Computing the other primitive variables from y_pred and x_batch using equations (A2) to (A5) from the paper \n",
    "            D_batch = x_batch[:, 0] # Conserved density\n",
    "            Sx_batch = x_batch[:, 1] # Conserved momentum in x-direction\n",
    "            tau_batch = x_batch[:, 2] # Conserved energy density\n",
    "            rho_pred = D_batch / torch.sqrt(1 + Sx_batch**2 / D_batch**2 / c**2) # Rest-mass density\n",
    "            vx_pred = Sx_batch / D_batch / c**2 / torch.sqrt(1 + Sx_batch**2 / D_batch**2 / c**2) # Velocity in x-direction\n",
    "            epsilon_pred = (tau_batch + D_batch) / rho_pred - y_pred / rho_pred - 1 # Specific internal energy\n",
    "            # Computing the true values of the other primitive variables from y_batch and x_batch using equations (A2) to (A5) from the paper \n",
    "            rho_true = D_batch / torch.sqrt(1 + Sx_batch**2 / D_batch**2 / c**2) # Rest-mass density\n",
    "            vx_true = Sx_batch / D_batch / c**2 / torch.sqrt(1 + Sx_batch**2 / D_batch**2 / c**2) # Velocity in x-direction\n",
    "            epsilon_true = (tau_batch + D_batch) / rho_true - y_batch / rho_true - 1 # Specific internal energy\n",
    "            # Updating the total metrics\n",
    "            # Updating the total metrics\n",
    "            train_rho_error += torch.mean(torch.abs((rho_pred - rho_true) / rho_true)) * x_batch.size(0) # Relative error for rest-mass density\n",
    "            train_vx_error += torch.mean(torch.abs((vx_pred - vx_true) / vx_true)) * x_batch.size(0) # Relative error for velocity in x-direction\n",
    "            train_epsilon_error += torch.mean(torch.abs((epsilon_pred - epsilon_true) / epsilon_true)) * x_batch.size(0) # Relative error for specific internal energy\n",
    "\n",
    "        # Setting the network to evaluation mode\n",
    "        net.eval()\n",
    "        # Initializing variables to store the total loss and metrics for the test set\n",
    "        test_loss = 0.0\n",
    "        test_rho_error = 0.0 # Added relative error for rest-mass density\n",
    "        test_vx_error = 0.0 # Added relative error for velocity in x-direction\n",
    "        test_epsilon_error = 0.0 # Added relative error for specific internal energy\n",
    "        # Looping over the batches in the test set\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            # Forward pass\n",
    "            y_pred = net(x_batch)\n",
    "            # Reshaping the target tensor to match the input tensor\n",
    "            y_batch = y_batch.view(-1, 1)\n",
    "            # Computing the loss\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # Updating the total loss\n",
    "            test_loss += loss.item() * x_batch.size(0)\n",
    "            # Computing the other primitive variables from y_pred and x_batch using equations (A2) to (A5) from the paper \n",
    "            D_batch = x_batch[:, 0] # Conserved density\n",
    "            Sx_batch = x_batch[:, 1] # Conserved momentum in x-direction\n",
    "            tau_batch = x_batch[:, 2] # Conserved energy density\n",
    "            rho_pred = D_batch / torch.sqrt(1 + Sx_batch**2 / D_batch**2 / c**2) # Rest-mass density\n",
    "            vx_pred = Sx_batch / D_batch / c**2 / torch.sqrt(1 + Sx_batch**2 / D_batch**2 / c**2) # Velocity in x-direction\n",
    "            epsilon_pred = (tau_batch + D_batch) / rho_pred - y_pred / rho_pred - 1 # Specific internal energy\n",
    "            # Computing the true values of the other primitive variables from y_batch and x_batch using equations (A2) to (A5) from the paper \n",
    "            rho_true = D_batch / torch.sqrt(1 + Sx_batch**2 / D_batch**2 / c**2) # Rest-mass density\n",
    "            vx_true = Sx_batch / D_batch / c**2 / torch.sqrt(1 + Sx_batch**2 / D_batch**2 / c**2) # Velocity in x-direction\n",
    "            epsilon_true = (tau_batch + D_batch) / rho_true - y_batch / rho_true - 1 # Specific internal energy\n",
    "            # Updating the total metrics\n",
    "            test_rho_error += torch.mean(torch.abs((rho_pred - rho_true) / rho_true)) * x_batch.size(0) # Relative error for rest-mass density\n",
    "            test_vx_error += torch.mean(torch.abs((vx_pred - vx_true) / vx_true)) * x_batch.size(0) # Relative error for velocity in x-direction\n",
    "            test_epsilon_error += torch.mean(torch.abs((epsilon_pred - epsilon_true) / epsilon_true)) * x_batch.size(0) # Relative error for specific internal energy\n",
    "\n",
    "        # Computing the average losses and metrics for the train and test sets\n",
    "        train_loss /= len(x_train)\n",
    "        test_loss /= len(x_test)\n",
    "        train_rho_error /= len(x_train)\n",
    "        test_rho_error /= len(x_test)\n",
    "        train_vx_error /= len(x_train)\n",
    "        test_vx_error /= len(x_test)\n",
    "        train_epsilon_error /= len(x_train)\n",
    "        test_epsilon_error /= len(x_test)\n",
    "\n",
    "        # Appending the losses and metrics to the lists\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        train_metrics.append({\"rho_error\": train_rho_error, \"vx_error\": train_vx_error, \"epsilon_error\": train_epsilon_error}) # Changed to a dictionary of metrics\n",
    "        test_metrics.append({\"rho_error\": test_rho_error, \"vx_error\": test_vx_error, \"epsilon_error\": test_epsilon_error}) # Changed to a dictionary of metrics\n",
    "\n",
    "        # Updating the learning rate scheduler with validation loss if applicable\n",
    "        if scheduler is not None: \n",
    "            if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(test_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        # Printing the losses and metrics for the current epoch\n",
    "        print(f\"Epoch {epoch + 1}:\")\n",
    "        print(f\"Train loss: {train_loss:.4f}\")\n",
    "        print(f\"Test loss: {test_loss:.4f}\")\n",
    "        print(f\"Train rho error: {train_rho_error:.4f}\")\n",
    "        print(f\"Test rho error: {test_rho_error:.4f}\")\n",
    "        print(f\"Train vx error: {train_vx_error:.4f}\")\n",
    "        print(f\"Test vx error: {test_vx_error:.4f}\")\n",
    "        print(f\"Train epsilon error: {train_epsilon_error:.4f}\")\n",
    "        print(f\"Test epsilon error: {test_epsilon_error:.4f}\")\n",
    "\n",
    "    # Returning the losses and metrics lists\n",
    "    return train_losses, test_losses, train_metrics, test_metrics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to be minimized by Optuna\n",
    "def objective(trial):\n",
    "    \"\"\"Creates a trial network and optimizer based on the sampled hyperparameters,\n",
    "    trains and evaluates it on the train and test sets, and returns the final test loss.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): The trial object that contains the hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        float: The final test loss.\n",
    "    \"\"\"\n",
    "    # Creating the network and optimizer with the create_model function\n",
    "    net, loss_fn, optimizer, batch_size, n_epochs, scheduler = create_model(trial)\n",
    "\n",
    "    # Training and evaluating the network with the train_and_eval function\n",
    "    train_losses, test_losses, train_metrics, test_metrics = train_and_eval(net, loss_fn, optimizer, batch_size, n_epochs, scheduler)\n",
    "\n",
    "    # Returning the final test loss\n",
    "    return test_losses[-1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-05 09:37:44,903]\u001b[0m A new study created in memory with name: no-name-aafbe84a-f921-484f-9168-120c9e402797\u001b[0m\n",
      "/tmp/ipykernel_17971/1493720043.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1) # Changed the lower bound to 1e-5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train loss: 13.4537\n",
      "Test loss: 12.1221\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.5701\n",
      "Test epsilon error: 0.4912\n",
      "Epoch 2:\n",
      "Train loss: 11.6365\n",
      "Test loss: 11.3136\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.5231\n",
      "Test epsilon error: 0.4999\n",
      "Epoch 3:\n",
      "Train loss: 11.1657\n",
      "Test loss: 11.0610\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.6876\n",
      "Test epsilon error: 0.5112\n",
      "Epoch 4:\n",
      "Train loss: 10.9987\n",
      "Test loss: 10.9772\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.5220\n",
      "Test epsilon error: 0.5194\n",
      "Epoch 5:\n",
      "Train loss: 10.9365\n",
      "Test loss: 10.9423\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.5820\n",
      "Test epsilon error: 0.5250\n",
      "Epoch 6:\n",
      "Train loss: 10.9081\n",
      "Test loss: 10.9236\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 1.0659\n",
      "Test epsilon error: 0.5297\n",
      "Epoch 7:\n",
      "Train loss: 10.8919\n",
      "Test loss: 10.9151\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.6391\n",
      "Test epsilon error: 0.5329\n",
      "Epoch 8:\n",
      "Train loss: 10.8838\n",
      "Test loss: 10.9112\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.6171\n",
      "Test epsilon error: 0.5351\n",
      "Epoch 9:\n",
      "Train loss: 10.8796\n",
      "Test loss: 10.9090\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.5373\n",
      "Test epsilon error: 0.5369\n",
      "Epoch 10:\n",
      "Train loss: 10.8769\n",
      "Test loss: 10.9081\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.5930\n",
      "Test epsilon error: 0.5380\n",
      "Epoch 11:\n",
      "Train loss: 10.8757\n",
      "Test loss: 10.9075\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.7617\n",
      "Test epsilon error: 0.5388\n",
      "Epoch 12:\n",
      "Train loss: 10.8746\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.5767\n",
      "Test epsilon error: 0.5396\n",
      "Epoch 13:\n",
      "Train loss: 10.8740\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.5698\n",
      "Test epsilon error: 0.5400\n",
      "Epoch 14:\n",
      "Train loss: 10.8737\n",
      "Test loss: 10.9071\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.7932\n",
      "Test epsilon error: 0.5404\n",
      "Epoch 15:\n",
      "Train loss: 10.8733\n",
      "Test loss: 10.9071\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.9809\n",
      "Test epsilon error: 0.5410\n",
      "Epoch 16:\n",
      "Train loss: 10.8730\n",
      "Test loss: 10.9071\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.7086\n",
      "Test epsilon error: 0.5411\n",
      "Epoch 17:\n",
      "Train loss: 10.8729\n",
      "Test loss: 10.9071\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.6761\n",
      "Test epsilon error: 0.5414\n",
      "Epoch 18:\n",
      "Train loss: 10.8728\n",
      "Test loss: 10.9071\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.7239\n",
      "Test epsilon error: 0.5416\n",
      "Epoch 19:\n",
      "Train loss: 10.8727\n",
      "Test loss: 10.9071\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 1.0872\n",
      "Test epsilon error: 0.5416\n",
      "Epoch 20:\n",
      "Train loss: 10.8727\n",
      "Test loss: 10.9071\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: inf\n",
      "Test epsilon error: 0.5415\n",
      "Epoch 21:\n",
      "Train loss: 10.8727\n",
      "Test loss: 10.9071\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.5304\n",
      "Test epsilon error: 0.5416\n",
      "Epoch 22:\n",
      "Train loss: 10.8726\n",
      "Test loss: 10.9071\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.5278\n",
      "Test epsilon error: 0.5416\n",
      "Epoch 23:\n",
      "Train loss: 10.8726\n",
      "Test loss: 10.9071\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.6110\n",
      "Test epsilon error: 0.5417\n",
      "Epoch 24:\n",
      "Train loss: 10.8726\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.6070\n",
      "Test epsilon error: 0.5419\n",
      "Epoch 25:\n",
      "Train loss: 10.8725\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.6400\n",
      "Test epsilon error: 0.5418\n",
      "Epoch 26:\n",
      "Train loss: 10.8725\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.5227\n",
      "Test epsilon error: 0.5418\n",
      "Epoch 27:\n",
      "Train loss: 10.8725\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.5774\n",
      "Test epsilon error: 0.5420\n",
      "Epoch 28:\n",
      "Train loss: 10.8725\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.5916\n",
      "Test epsilon error: 0.5420\n",
      "Epoch 29:\n",
      "Train loss: 10.8725\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.6345\n",
      "Test epsilon error: 0.5419\n",
      "Epoch 30:\n",
      "Train loss: 10.8725\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.5592\n",
      "Test epsilon error: 0.5420\n",
      "Epoch 31:\n",
      "Train loss: 10.8724\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.6336\n",
      "Test epsilon error: 0.5420\n",
      "Epoch 32:\n",
      "Train loss: 10.8724\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 1.1145\n",
      "Test epsilon error: 0.5420\n",
      "Epoch 33:\n",
      "Train loss: 10.8724\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.6587\n",
      "Test epsilon error: 0.5420\n",
      "Epoch 34:\n",
      "Train loss: 10.8724\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 1.1757\n",
      "Test epsilon error: 0.5420\n",
      "Epoch 35:\n",
      "Train loss: 10.8724\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.6664\n",
      "Test epsilon error: 0.5420\n",
      "Epoch 36:\n",
      "Train loss: 10.8724\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.6060\n",
      "Test epsilon error: 0.5421\n",
      "Epoch 37:\n",
      "Train loss: 10.8724\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.5770\n",
      "Test epsilon error: 0.5421\n",
      "Epoch 38:\n",
      "Train loss: 10.8724\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.5429\n",
      "Test epsilon error: 0.5421\n",
      "Epoch 39:\n",
      "Train loss: 10.8724\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.6758\n",
      "Test epsilon error: 0.5421\n",
      "Epoch 40:\n",
      "Train loss: 10.8724\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 1.2433\n",
      "Test epsilon error: 0.5421\n",
      "Epoch 41:\n",
      "Train loss: 10.8724\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.6510\n",
      "Test epsilon error: 0.5421\n",
      "Epoch 42:\n",
      "Train loss: 10.8724\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.6580\n",
      "Test epsilon error: 0.5422\n",
      "Epoch 43:\n",
      "Train loss: 10.8724\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.6261\n",
      "Test epsilon error: 0.5422\n",
      "Epoch 44:\n",
      "Train loss: 10.8724\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: inf\n",
      "Test epsilon error: 0.5422\n",
      "Epoch 45:\n",
      "Train loss: 10.8724\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.5632\n",
      "Test epsilon error: 0.5422\n",
      "Epoch 46:\n",
      "Train loss: 10.8724\n",
      "Test loss: 10.9072\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.7095\n",
      "Test epsilon error: 0.5422\n",
      "Epoch 47:\n",
      "Train loss: 10.8723\n",
      "Test loss: 10.9073\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: inf\n",
      "Test epsilon error: 0.5422\n",
      "Epoch 48:\n",
      "Train loss: 10.8723\n",
      "Test loss: 10.9073\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.7935\n",
      "Test epsilon error: 0.5422\n",
      "Epoch 49:\n",
      "Train loss: 10.8723\n",
      "Test loss: 10.9073\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.6955\n",
      "Test epsilon error: 0.5422\n",
      "Epoch 50:\n",
      "Train loss: 10.8723\n",
      "Test loss: 10.9073\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.5479\n",
      "Test epsilon error: 0.5422\n",
      "Epoch 51:\n",
      "Train loss: 10.8723\n",
      "Test loss: 10.9073\n",
      "Train rho error: 0.0000\n",
      "Test rho error: 0.0000\n",
      "Train vx error: 0.0000\n",
      "Test vx error: 0.0000\n",
      "Train epsilon error: 0.5577\n",
      "Test epsilon error: 0.5422\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Creating an Optuna study object\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "# Running the optimization with a given number of trials\n",
    "n_trials = 100 # Change this to a larger number for better results\n",
    "study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "# Printing the best hyperparameters and the best value\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best value:\", study.best_value)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the network and optimizer with the best hyperparameters\n",
    "net_best, loss_fn_best, optimizer_best, batch_size_best, n_epochs_best, scheduler_best = create_model(study.best_trial)\n",
    "\n",
    "# Training and evaluating the network with the best hyperparameters\n",
    "train_losses_best, test_losses_best, train_metrics_best, test_metrics_best = train_and_eval(net_best, loss_fn_best, optimizer_best, batch_size_best, n_epochs_best, scheduler_best)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting the train and test losses per epoch\n",
    "plt.figure()\n",
    "plt.plot(range(1, n_epochs_best + 1), train_losses_best, label=\"Train loss\")\n",
    "plt.plot(range(1, n_epochs_best + 1), test_losses_best, label=\"Test loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss curves\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting the train and test metrics per epoch\n",
    "plt.figure()\n",
    "plt.plot(range(1, n_epochs_best + 1), [m[\"rho_error\"] for m in train_metrics_best], label=\"Train rho error\")\n",
    "plt.plot(range(1, n_epochs_best + 1), [m[\"rho_error\"] for m in test_metrics_best], label=\"Test rho error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Relative error\")\n",
    "plt.legend()\n",
    "plt.title(\"Rest-mass density error curves\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, n_epochs_best + 1), [m[\"vx_error\"] for m in train_metrics_best], label=\"Train vx error\")\n",
    "plt.plot(range(1, n_epochs_best + 1), [m[\"vx_error\"] for m in test_metrics_best], label=\"Test vx error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Relative error\")\n",
    "plt.legend()\n",
    "plt.title(\"Velocity in x-direction error curves\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, n_epochs_best + 1), [m[\"epsilon_error\"] for m in train_metrics_best], label=\"Train epsilon error\")\n",
    "plt.plot(range(1, n_epochs_best + 1), [m[\"epsilon_error\"] for m in test_metrics_best], label=\"Test epsilon error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Relative error\")\n",
    "plt.legend()\n",
    "plt.title(\"Specific internal energy error curves\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading the data and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing torch and pickle for saving and loading\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "# Saving the data and the model to files\n",
    "torch.save(x_train, \"x_train.pt\")\n",
    "torch.save(y_train, \"y_train.pt\")\n",
    "torch.save(x_test, \"x_test.pt\")\n",
    "torch.save(y_test, \"y_test.pt\")\n",
    "torch.save(net_best.state_dict(), \"net_best.pt\")\n",
    "pickle.dump(study.best_params, open(\"best_params.pkl\", \"wb\"))\n",
    "\n",
    "# Loading the data and the model from files\n",
    "x_train = torch.load(\"x_train.pt\")\n",
    "y_train = torch.load(\"y_train.pt\")\n",
    "x_test = torch.load(\"x_test.pt\")\n",
    "y_test = torch.load(\"y_test.pt\")\n",
    "net_best = Net(**study.best_params).to(device) # Creating the network with the best hyperparameters\n",
    "net_best.load_state_dict(torch.load(\"net_best.pt\")) # Loading the network weights\n",
    "best_params = pickle.load(open(\"best_params.pkl\", \"rb\")) # Loading the best hyperparameters\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

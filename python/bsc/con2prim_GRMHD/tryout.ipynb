{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import optuna\n",
    "import tensorboardX as tbx\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INPUTS = 14\n",
    "N_OUTPUTS = 1\n",
    "# Defining a class for the network\n",
    "class Net(nn.Module):\n",
    "    \"\"\"A class for creating a network with a\n",
    "    variable number of hidden layers and units.\n",
    "\n",
    "    Attributes:\n",
    "        n_layers (int): The number of hidden layers in the network.\n",
    "        n_units (list): A list of integers representing the number of units in each hidden layer.\n",
    "        hidden_activation (torch.nn.Module): The activation function for the hidden layers.\n",
    "        output_activation (torch.nn.Module): The activation function for the output layer.\n",
    "        layers (torch.nn.ModuleList): A list of linear layers in the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_layers, n_units, hidden_activation, output_activation, dropout_rate):\n",
    "        \"\"\"Initializes the network with the given hyperparameters.\n",
    "\n",
    "        Args:\n",
    "            n_layers (int): The number of hidden layers in the network.\n",
    "            n_units (list): A list of integers representing the number of units in each hidden layer.\n",
    "            hidden_activation (torch.nn.Module): The activation function for the hidden layers.\n",
    "            output_activation (torch.nn.Module): The activation function for the output layer.\n",
    "            TODO: [ver. Copilot description] dropout_rate (float): The dropout rate to use for all layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_units = n_units\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.output_activation = output_activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Creating a list of linear layers with different numbers of units for each layer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "\n",
    "        self.layers.append(nn.Linear(N_INPUTS, n_units[0]))\n",
    "        self.dropouts.append(nn.Dropout(p=dropout_rate))\n",
    "\n",
    "        for i in range(1, n_layers):\n",
    "            self.layers.append(nn.Linear(n_units[i - 1], n_units[i]))\n",
    "            self.dropouts.append(nn.Dropout(p=dropout_rate))\n",
    "\n",
    "        self.layers.append(nn.Linear(n_units[-1], N_OUTPUTS))\n",
    "\n",
    "        # Adding some assertions to check that the input arguments are valid\n",
    "        assert isinstance(n_layers, int) and n_layers > 0, \"n_layers must be a positive integer\"\n",
    "        assert isinstance(n_units, list) and len(n_units) == n_layers, \"n_units must be a list of length n_layers\"\n",
    "        assert all(isinstance(n, int) and n > 0 for n in n_units), \"n_units must contain positive integers\"\n",
    "        assert isinstance(hidden_activation, nn.Module), \"hidden_activation must be a torch.nn.Module\"\n",
    "        assert isinstance(output_activation, nn.Module), \"output_activation must be a torch.nn.Module\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Performs a forward pass on the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, N_INPUTS).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape (batch_size, N_OUTPUTS).\n",
    "        \"\"\"\n",
    "        # Adding an assertion to check that the input tensor has the expected shape and type\n",
    "        assert isinstance(x, torch.Tensor), \"x must be a torch.Tensor\"\n",
    "        assert x.shape[1] == N_INPUTS, f\"x must have shape (batch_size, {N_INPUTS})\"\n",
    "\n",
    "        for layer, dropout in zip(self.layers[:-1], self.dropouts):\n",
    "            x = dropout(self.hidden_activation(layer(x)))\n",
    "        # Applying the linear transformation and the activation function on the output layer\n",
    "        x = self.output_activation(self.layers[-1](x)) # No dropout at output layer\n",
    "\n",
    "        return x # Returning the output tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer must support momentum with `cycle_momentum` option enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m max_lr \u001b[39m=\u001b[39m \u001b[39m1e-4\u001b[39m\n\u001b[0;32m     11\u001b[0m step_size_up \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 12\u001b[0m scheduler \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39;49mlr_scheduler\u001b[39m.\u001b[39;49mCyclicLR(optimizer, base_lr\u001b[39m=\u001b[39;49mbase_lr, max_lr\u001b[39m=\u001b[39;49mmax_lr, step_size_up\u001b[39m=\u001b[39;49mstep_size_up)\n",
      "File \u001b[1;32mc:\\Users\\bvptr\\anaconda3\\envs\\bsc\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:1231\u001b[0m, in \u001b[0;36mCyclicLR.__init__\u001b[1;34m(self, optimizer, base_lr, max_lr, step_size_up, step_size_down, mode, gamma, scale_fn, scale_mode, cycle_momentum, base_momentum, max_momentum, last_epoch, verbose)\u001b[0m\n\u001b[0;32m   1229\u001b[0m \u001b[39mif\u001b[39;00m cycle_momentum:\n\u001b[0;32m   1230\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mmomentum\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m optimizer\u001b[39m.\u001b[39mdefaults:\n\u001b[1;32m-> 1231\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39moptimizer must support momentum with `cycle_momentum` option enabled\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1233\u001b[0m     base_momentums \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_param(\u001b[39m'\u001b[39m\u001b[39mbase_momentum\u001b[39m\u001b[39m'\u001b[39m, optimizer, base_momentum)\n\u001b[0;32m   1234\u001b[0m     \u001b[39mif\u001b[39;00m last_epoch \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: optimizer must support momentum with `cycle_momentum` option enabled"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "n_layers = 2\n",
    "n_units = [128, 128]\n",
    "hidden_activation = nn.ReLU()\n",
    "output_activation = nn.Identity()\n",
    "dropout_rate = 0.0\n",
    "net = Net(n_layers, n_units, hidden_activation, output_activation, dropout_rate)\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=lr)\n",
    "base_lr = 1e-6\n",
    "max_lr = 1e-4\n",
    "step_size_up = 1\n",
    "scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr, step_size_up=step_size_up)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

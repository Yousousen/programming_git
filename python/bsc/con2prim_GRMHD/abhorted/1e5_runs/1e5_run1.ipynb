{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPvB1xoSUZhR"
      },
      "source": [
        "# Neural network to learn conservative-to-primitive conversion in relativistic hydrodynamics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQOAdSRnyUe_"
      },
      "source": [
        "## How to use this notebook\n",
        "\n",
        "### Local installation\n",
        "\n",
        "1. Install required packages with `pip install -r requirements.txt` to your desired environment.\n",
        "2. If a script version of this notebook is desired, comment (not uncomment) the first line of `nbconvert` cell.\n",
        "\n",
        "### Colab installation\n",
        "\n",
        "1.  Comment (not uncomment) the first line of the drive mounting cell.\n",
        "2.  Comment (not uncomment) the first line of the `pip install` cell.\n",
        "\n",
        "<!-- - For colab we also want to set the runtime to GPU by clicking _Change runtime_ in the _Runtime_ menu, and -->\n",
        "<!-- - We want to wait for the google drive connection popup to appear and follow the instructions. -->\n",
        "\n",
        "### Loading / Generating data\n",
        "3. Set `LOAD_DATA_FROM_CSV` to `True` / `False` to load data from csv files / generate data in this notebook.\n",
        "\n",
        "### Training without optimization\n",
        "\n",
        "4. Set `OPTIMIZE = False` in section _Constants and flags to set_.\n",
        "5. Run the entire notebook.\n",
        "\n",
        "### Training with optimization\n",
        "\n",
        "4. Set `OPTIMIZE = True` in section _Constants and flags to set_.\n",
        "5. Run the entire notebook.\n",
        "\n",
        "### Loading an already trained model\n",
        "\n",
        "4. Run cells in section _Initialization_.\n",
        "5. Run cells with definitions in section _Input data and labels_.\n",
        "6. Run cell with the definition of _Net_ in section _Defining the neural network_.\n",
        "7. Make sure the `net.pth`, `optimizer.pth`, `scheduler.pth`, `var_dict.json` and `train_output.csv` files are in the directory containing this notebook.\n",
        "8. Run the cells in section _Loading_ and continue from there.\n",
        "\n",
        "### Generating the C++ model\n",
        "\n",
        "9. Run section _Porting the model to C++_, this requires a model to be loaded.\n",
        "10. Set the path to the `net.pt` file in the C++ source file.\n",
        "11. `mkdir build && cd build`,\n",
        "12. Configure a `CMakeLists.txt` file as is done [here](https://pytorch.org/cppdocs/installing.html).\n",
        "13. `cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch/ ..`,\n",
        "14. Compile and run, e.g. `cmake --build . --config release && ./<executable name>`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNr7wqgCz9VP"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQG-VpFLyUe_"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rasKmIzdyUe_"
      },
      "source": [
        "\n",
        "Use this first cell to **convert this notebook** to a python script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqdgdNLHUZhV",
        "outputId": "2adfd374-5fd5-4ebf-c877-8cc45f7ea4f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "!jupyter nbconvert pt1_before_restructuring.ipynb --TagRemovePreprocessor.enabled=True --TagRemovePreprocessor.remove_cell_tags='{\"remove_cell\"}' --to script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzcUr0LnUZhw"
      },
      "source": [
        "Next some cells for working on **google colab**,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLdZpOu4yUfB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# check if the drive is mounted\n",
        "drive_mounted = os.path.exists(\"/content/drive\")\n",
        "# change this to your desired folder\n",
        "drive_folder = \"/content/drive/My Drive/bsc/con2prim_GRMHD/1e5_runs\"\n",
        "\n",
        "# define a function to save a file to the drive or the current directory\n",
        "def save_file(file_name):\n",
        "  if drive_mounted:\n",
        "    # save the file to the drive folder\n",
        "    file_path = os.path.join(drive_folder, file_name)\n",
        "    # copy the file from the current directory to the drive folder\n",
        "    shutil.copyfile(file_name, file_path)\n",
        "  else:\n",
        "    # do nothing as the file is already in the current directory\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecHw2_xlUZhx",
        "outputId": "4699aca1-8c15-45a6-bd5b-26fd596bb6e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# %%script echo skipping\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxqdIxKBmzyt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1rcStMLUZhy",
        "outputId": "b90a3984-ff63-4e1b-b8cc-00fb569efe1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.12.2)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.11.1)\n",
            "Requirement already satisfied: cmaes>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from optuna) (0.9.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.4.3)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.40.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "# %%script echo skipping\n",
        "\n",
        "!pip install optuna tensorboard tensorboardX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqDQetVPFyCw"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxXGIEp5FXQC"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA6uyFy0yUfC"
      },
      "source": [
        "Importing the **libraries**,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tREdWQUVUZhz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import optuna\n",
        "import tensorboardX as tbx\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38GvmerjUZhz"
      },
      "source": [
        "### Constants and flags to set\n",
        "Defining some constants and parameters for convenience.\n",
        "\n",
        "**NOTE**: Some **subparameters** still need to be adjusted in the `create_model` function itself as of (Tue May 16 07:42:45 AM CEST 2023) in the case the model is being trained without optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ei6VZDYKUZh0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Checking if GPU is available and setting the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "N_TRIALS = 200 # Number of trials for hyperparameter optimization # NOTE: Set this properly.\n",
        "OPTIMIZE = True # Whether to optimize the hyperparameters or to use predetermined values. # NOTE: Set this properly.\n",
        "ZSCORE_NORMALIZATION = False # Whether to z-score normalize the input data.\n",
        "LOAD_DATA_FROM_CSV = False  # If not true we generate the data in this file and save to {x_train,y_train,x_test,y_test}.csv, otherwise we load the data from files of the same name. # NOTE: Set this before properly.\n",
        "\n",
        "csv_filenames = { # File names to load input data and labels from if LOAD_DATA_FROM_CSV is True.\n",
        "    \"x_train\": \"x_train.csv\",\n",
        "    \"x_val\": \"x_val.csv\",\n",
        "    \"x_test\": \"x_test.csv\",\n",
        "    \"y_train\": \"y_train.csv\",\n",
        "    \"y_val\": \"y_val.csv\",\n",
        "    \"y_test\": \"y_test.csv\",\n",
        "}\n",
        "\n",
        "# Values to use for hyperparameters if OPTIMIZE is False; set these to the best parameters found by Optuna.\n",
        "# NOTE: TODO: Currently (Sat May 27 05:16:57 PM CEST 2023) there are still subparameters to be set in create_model function.\n",
        "N_LAYERS_NO_OPT = 3\n",
        "N_UNITS_NO_OPT = [555, 458, 115]\n",
        "HIDDEN_ACTIVATION_NAME_NO_OPT = \"LeakyReLU\"\n",
        "OUTPUT_ACTIVATION_NAME_NO_OPT = \"Linear\"\n",
        "LOSS_NAME_NO_OPT = \"Huber\"\n",
        "OPTIMIZER_NAME_NO_OPT = \"RMSprop\"\n",
        "LR_NO_OPT = 0.000122770896701404\n",
        "BATCH_SIZE_NO_OPT = 49\n",
        "N_EPOCHS_NO_OPT = 400 # NOTE: Set this properly.\n",
        "SCHEDULER_NAME_NO_OPT = \"ReduceLROnPlateau\"\n",
        "DROPOUT_RATE_NO_OPT = 0.2\n",
        "\n",
        "N_INPUTS = 14  # Number of input features.\n",
        "N_OUTPUTS = 1  # Number of outputs.\n",
        "Gamma = 5/3  # Adiabatic index\n",
        "\n",
        "n_samples = 1e5 # NOTE: Set this properly.\n",
        "train_frac = 0.7  # 70% of data for training\n",
        "val_frac = 0.15  # 15% of data for validation, rest for testing\n",
        "\n",
        "rho_interval = (0, 2) \n",
        "epsilon_interval = (1e-2, 2000)  # Will be sampled in log space\n",
        "vx_interval = (0, 0.999)  \n",
        "vy_interval = (0, 0.999)  \n",
        "vz_interval = (0, 0.999)  \n",
        "Bx_interval = (-10, 10)  \n",
        "By_interval = (-10, 10)  \n",
        "Bz_interval = (-10, 10)  \n",
        "gxx_interval = (0.9, 1.1)\n",
        "gxy_interval = (0, 0.1)\n",
        "gxz_interval = (0, 0.1)\n",
        "gyy_interval = (0.9, 1.1)\n",
        "gyz_interval = (0, 0.1)\n",
        "gzz_interval = (0.9, 1.1)\n",
        "\n",
        "np.random.seed(61) # Comment for true random data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlaP5UL2UZh1"
      },
      "source": [
        "## Input data and labels\n",
        "\n",
        "We either generate the data or load the data. First the definitions for generating the data come below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_EvGFZcUZh1"
      },
      "outputs": [],
      "source": [
        "# Defining an analytic equation of state (EOS) for an ideal gas\n",
        "def eos_analytic(rho, epsilon):\n",
        "    # Adding some assertions to check that the input tensors are valid and have \n",
        "    # the expected shape and type \n",
        "    assert isinstance(rho, torch.Tensor), \"rho must be a torch.Tensor\"\n",
        "    assert isinstance(epsilon, torch.Tensor), \"epsilon must be a torch.Tensor\"\n",
        "    print('rho.shape: ', rho.shape)\n",
        "    print('epsilon.shape: ', epsilon.shape)\n",
        "    assert rho.shape == epsilon.shape, \"rho and epsilon must have the same shape\"\n",
        "    assert rho.ndim == 1, \"rho and epsilon must be one-dimensional tensors\"\n",
        "    assert rho.dtype == torch.float32, \"rho and epsilon must have dtype torch.float32\"\n",
        "\n",
        "    return (Gamma - 1) * rho * epsilon\n",
        "\n",
        "def sample_primitive_variables_and_metric():\n",
        "    rho = np.random.uniform(*rho_interval)  \n",
        "    epsilon = np.random.uniform(*np.log10(epsilon_interval))\n",
        "    vx = np.random.uniform(*vx_interval)  \n",
        "    vy = np.random.uniform(*vy_interval)  \n",
        "    vz = np.random.uniform(*vz_interval)  \n",
        "    Bx = np.random.uniform(*Bx_interval)  \n",
        "    By = np.random.uniform(*By_interval)  \n",
        "    Bz = np.random.uniform(*Bz_interval)  \n",
        "    gxx = np.random.uniform(*gxx_interval)\n",
        "    gxy = np.random.uniform(*gxy_interval)\n",
        "    gxz = np.random.uniform(*gxz_interval)\n",
        "    gyy = np.random.uniform(*gyy_interval)\n",
        "    gyz = np.random.uniform(*gyz_interval)\n",
        "    gzz = np.random.uniform(*gzz_interval)\n",
        "\n",
        "    return rho, epsilon, vx, vy, vz, Bx, By, Bz, gxx, gxy, gxz, gyy, gyz, gzz\n",
        "\n",
        "def check_sample(rho, epsilon, vx, vy, vz, Bx, By, Bz, gxx, gxy, gxz, gyy, gyz, gzz):\n",
        "    wtemp_expr = 1 - (gxx * vx**2 + gyy * vy**2 + gzz * vz**2 + 2 * gxy * vx * vy + 2 * gxz * vx * vz + 2 * gyz * vy * vz)\n",
        "    sdet_expr = gxx * gyy * gzz + 2 * gxy * gxz * gyz - gxx * gyz ** 2 - gyy * gxz ** 2 - gzz * gxy ** 2\n",
        "    if vx**2 + vy**2 + vz**2 >= 1 or wtemp_expr < 0 or sdet_expr < 0:\n",
        "        # print(f\"Sample failed checks. vx^2+vy^2+vz^2: {vx**2 + vy**2 + vz**2}, wtemp_expr: {wtemp_expr}, sdet_expr: {sdet_expr}\")\n",
        "        return False\n",
        "    else:\n",
        "        # print(f\"Sample passed checks. vx^2+vy^2+vz^2: {vx**2 + vy**2 + vz**2}, wtemp_expr: {wtemp_expr}, sdet_expr: {sdet_expr}\")\n",
        "        return True\n",
        "\n",
        "def generate_samples(n_samples):\n",
        "    samples = []\n",
        "    while len(samples) < n_samples:\n",
        "        sample = sample_primitive_variables_and_metric()\n",
        "        if check_sample(*sample):\n",
        "            samples.append(sample)\n",
        "        # print(f\"Number of valid samples: {len(samples)}\")\n",
        "    return zip(*samples)\n",
        "def sdet(gxx, gxy, gxz, gyy, gyz, gzz):\n",
        "    # Determinant of the three metric.\n",
        "    return (gxx * gyy * gzz + 2 * gxy * gxz * gyz - gxx * gyz ** 2 - gyy * gxz ** 2 - gzz * gxy ** 2) ** 0.5\n",
        "\n",
        "# Defining a function that computes conserved variables from primitive variables and the metric\n",
        "# We follow the source code of GRaM-X: A new GPU-accelerated dynamical spacetime GRMHD code for Exascale\n",
        "# computing with the Einstein Toolkit of Shankar et al.\n",
        "def compute_conserved_variables(rho, epsilon, vx, vy, vz, Bx, By, Bz, gxx, gxy, gxz, gyy, gyz, gzz):\n",
        "    pres = eos_analytic(rho, epsilon)\n",
        "    wtemp = 1 / (1 - (gxx * vx**2 + gyy * vy**2 + gzz * vz**2 +\n",
        "        2 * gxy * vx * vy + 2 * gxz * vx * vz +\n",
        "        2 * gyz * vy * vz))**0.5\n",
        "\n",
        "    vlowx = gxx * vx + gxy * vy + gxz * vz\n",
        "    vlowy = gxy * vx + gyy * vy + gyz * vz\n",
        "    vlowz = gxz * vx + gyz * vy + gzz * vz\n",
        "\n",
        "    Bxlow = gxx * Bx + gxy * By + gxz * Bz\n",
        "    Bylow = gxy * Bx + gyy * By + gyz * Bz\n",
        "    Bzlow = gxz * Bx + gyz * By + gzz * Bz\n",
        "\n",
        "    B2 = Bxlow * Bx + Bylow * By + Bzlow * Bz\n",
        "\n",
        "    Bdotv = Bxlow * vx + Bylow * vy + Bzlow * vz\n",
        "    Bdotv2 = Bdotv * Bdotv\n",
        "    wtemp2 = wtemp * wtemp\n",
        "    b2 = B2 / wtemp2 + Bdotv2\n",
        "    ab0 = wtemp * Bdotv\n",
        "\n",
        "    blowx = (gxx * Bx + gxy * By + gxz * Bz) / wtemp + wtemp * Bdotv * vlowx\n",
        "    blowy = (gxy * Bx + gyy * By + gyz * Bz) / wtemp + wtemp * Bdotv * vlowy\n",
        "    blowz = (gxz * Bx + gyz * By + gzz * Bz) / wtemp + wtemp * Bdotv * vlowz\n",
        "\n",
        "    hrhow2 = (rho * (1 + epsilon) + pres + b2) * (wtemp) * (wtemp)\n",
        "\n",
        "    D = sdet(gxx, gxy, gxz, gyy, gyz, gzz) * rho * (wtemp)\n",
        "    Sx = sdet(gxx, gxy, gxz, gyy, gyz, gzz) * (hrhow2 * vlowx - ab0 * blowx)\n",
        "    Sy = sdet(gxx, gxy, gxz, gyy, gyz, gzz) * (hrhow2 * vlowy - ab0 * blowy)\n",
        "    Sz = sdet(gxx, gxy, gxz, gyy, gyz, gzz) * (hrhow2 * vlowz - ab0 * blowz)\n",
        "    tau = sdet(gxx, gxy, gxz, gyy, gyz, gzz) * (hrhow2 - pres - b2 / 2 - ab0 * ab0) - D\n",
        "    Bconsx = sdet(gxx, gxy, gxz, gyy, gyz, gzz) * Bx\n",
        "    Bconsy = sdet(gxx, gxy, gxz, gyy, gyz, gzz) * By\n",
        "    Bconsz = sdet(gxx, gxy, gxz, gyy, gyz, gzz) * Bz\n",
        "\n",
        "    return D, Sx, Sy, Sz, tau, Bconsx, Bconsy, Bconsz\n",
        "\n",
        "def generate_input_data(rho, epsilon, vx, vy, vz, Bx, By, Bz, gxx, gxy, gxz, gyy, gyz, gzz):\n",
        "    rho = torch.tensor(np.array(rho), dtype=torch.float32).to(device)\n",
        "    epsilon = torch.tensor(np.array(epsilon), dtype=torch.float32).to(device)\n",
        "    vx = torch.tensor(np.array(vx), dtype=torch.float32).to(device)\n",
        "    vy = torch.tensor(np.array(vy), dtype=torch.float32).to(device)\n",
        "    vz = torch.tensor(np.array(vz), dtype=torch.float32).to(device)\n",
        "    Bx = torch.tensor(np.array(Bx), dtype=torch.float32).to(device)\n",
        "    By = torch.tensor(np.array(By), dtype=torch.float32).to(device)\n",
        "    Bz = torch.tensor(np.array(Bz), dtype=torch.float32).to(device)\n",
        "    gxx = torch.tensor(np.array(gxx), dtype=torch.float32).to(device)\n",
        "    gxy = torch.tensor(np.array(gxy), dtype=torch.float32).to(device)\n",
        "    gxz = torch.tensor(np.array(gxz), dtype=torch.float32).to(device)\n",
        "    gyy = torch.tensor(np.array(gyy), dtype=torch.float32).to(device)\n",
        "    gyz = torch.tensor(np.array(gyz), dtype=torch.float32).to(device)\n",
        "    gzz = torch.tensor(np.array(gzz), dtype=torch.float32).to(device)\n",
        "\n",
        "    D, Sx, Sy, Sz, tau, Bscriptx, Bscripty, Bscriptz = compute_conserved_variables(\n",
        "        rho, epsilon, vx, vy, vz, Bx, By, Bz, gxx, gxy, gxz, gyy, gyz, gzz\n",
        "    ) \n",
        "\n",
        "    # Add gxx, gxy, gxz, gyy, gyz, gzz to the tensor\n",
        "    x = torch.stack([D, Sx, Sy, Sz, tau, Bscriptx, Bscripty, Bscriptz, gxx, gxy, gxz, gyy, gyz, gzz], axis=1)\n",
        "    return x\n",
        "\n",
        "# Defining a function that generates output data (labels) from given samples of primitive variables\n",
        "# We use the definitions as given in Recovery schemes for primitive variables in\n",
        "# general-relativistic magnetohydrodynamics of Siegel et al.\n",
        "def generate_labels(rho, epsilon, vx, vy, vz):\n",
        "    # Converting the numpy arrays to torch tensors and moving them to the device\n",
        "    rho = torch.tensor(np.array(rho), dtype=torch.float32).to(device)\n",
        "    epsilon = torch.tensor(np.array(epsilon), dtype=torch.float32).to(device)\n",
        "    vx = torch.tensor(np.array(vx), dtype=torch.float32).to(device)\n",
        "    vy = torch.tensor(np.array(vy), dtype=torch.float32).to(device)\n",
        "    vz = torch.tensor(np.array(vz), dtype=torch.float32).to(device)\n",
        "\n",
        "    # Computing the required quantities\n",
        "    pres = eos_analytic(rho, epsilon)\n",
        "    h = 1 + epsilon + pres / rho\n",
        "    W = 1 / torch.sqrt(1 - (vx * vx + vy * vy + vz * vz))\n",
        "\n",
        "    # Returning the output data tensor\n",
        "    return h * W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp_rRsvlyUfD"
      },
      "source": [
        "### Generating or loading input data and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSRavbz7yUfD"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKubR6C8UZh4"
      },
      "outputs": [],
      "source": [
        "if LOAD_DATA_FROM_CSV:\n",
        "    # Load the data from CSV files\n",
        "    x_train = pd.read_csv(csv_filenames[\"x_train\"]).values\n",
        "    y_train = pd.read_csv(csv_filenames[\"y_train\"]).values.squeeze()  # reshape to 1D\n",
        "    x_val = pd.read_csv(csv_filenames[\"x_val\"]).values\n",
        "    y_val = pd.read_csv(csv_filenames[\"y_val\"]).values.squeeze()  # reshape to 1D\n",
        "    x_test = pd.read_csv(csv_filenames[\"x_test\"]).values\n",
        "    y_test = pd.read_csv(csv_filenames[\"y_test\"]).values.squeeze()  # reshape to 1D\n",
        "\n",
        "    # Convert numpy arrays to tensors\n",
        "    x_train = torch.from_numpy(x_train).float().to(device)\n",
        "    y_train = torch.from_numpy(y_train).float().to(device)\n",
        "    x_val = torch.from_numpy(x_val).float().to(device)\n",
        "    y_val = torch.from_numpy(y_val).float().to(device)\n",
        "    x_test = torch.from_numpy(x_test).float().to(device)\n",
        "    y_test = torch.from_numpy(y_test).float().to(device)\n",
        "\n",
        "    # This is an alternative to having if clauses around the cells that visualize these variables.\n",
        "    rho = epsilon = vx = vy = vz = Bx = By = Bz = gxx = gxy = gxz = gyy = gyz  = gzz = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbbRZdtmyUfE"
      },
      "outputs": [],
      "source": [
        "if not LOAD_DATA_FROM_CSV:\n",
        "    # Sample primitive variables and metric\n",
        "    rho, epsilon, vx, vy, vz, Bx, By, Bz, gxx, gxy, gxz, gyy, gyz, gzz = generate_samples(n_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQpEH1iNyUfE",
        "outputId": "c04bf662-c5ce-4ade-c76e-7d1595bf382c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rho.shape:  torch.Size([100000])\n",
            "epsilon.shape:  torch.Size([100000])\n",
            "rho.shape:  torch.Size([100000])\n",
            "epsilon.shape:  torch.Size([100000])\n"
          ]
        }
      ],
      "source": [
        "if not LOAD_DATA_FROM_CSV:\n",
        "    # Generate data and labels.\n",
        "    x = generate_input_data(rho, epsilon, vx, vy, vz, Bx, By, Bz, gxx, gxy, gxz, gyy, gyz, gzz)\n",
        "    y = generate_labels(rho, epsilon, vx, vy, vz)\n",
        "\n",
        "    # Calculate the number of samples in each set\n",
        "    n_train_samples = int(n_samples * train_frac)\n",
        "    n_val_samples = int(n_samples * val_frac)\n",
        "\n",
        "    # Create the data sets\n",
        "    x_train = x[:n_train_samples]\n",
        "    y_train = y[:n_train_samples]\n",
        "    x_val = x[n_train_samples : n_train_samples + n_val_samples]\n",
        "    y_val = y[n_train_samples : n_train_samples + n_val_samples]\n",
        "    x_test = x[n_train_samples + n_val_samples :]\n",
        "    y_test = y[n_train_samples + n_val_samples :]\n",
        "\n",
        "    # Save the data to CSV files; tensors need to be converted numpy arrays for saving in CSV.\n",
        "    pd.DataFrame(x_train.cpu().numpy()).to_csv(csv_filenames[\"x_train\"], index=False)\n",
        "    save_file(csv_filenames[\"x_train\"])\n",
        "    pd.DataFrame(y_train.cpu().numpy()).to_csv(csv_filenames[\"y_train\"], index=False)\n",
        "    save_file(csv_filenames[\"y_train\"])\n",
        "\n",
        "    pd.DataFrame(x_val.cpu().numpy()).to_csv(csv_filenames[\"x_val\"], index=False)\n",
        "    save_file(csv_filenames[\"x_val\"])\n",
        "    pd.DataFrame(y_val.cpu().numpy()).to_csv(csv_filenames[\"y_val\"], index=False)\n",
        "    save_file(csv_filenames[\"y_val\"])\n",
        "\n",
        "    pd.DataFrame(x_test.cpu().numpy()).to_csv(csv_filenames[\"x_test\"], index=False)\n",
        "    save_file(csv_filenames[\"x_test\"])\n",
        "    pd.DataFrame(y_test.cpu().numpy()).to_csv(csv_filenames[\"y_test\"], index=False)\n",
        "    save_file(csv_filenames[\"y_test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxEHi40RyUfE",
        "outputId": "b351efbe-89d0-4cbf-81c7-560f58ada139"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([70000, 14])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([70000])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15000, 14])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15000])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15000, 14])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15000])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.3121e+00, 7.5593e+01, 1.9495e+00,  ..., 1.0449e+00, 1.9199e-02,\n",
              "         1.0056e+00],\n",
              "        [2.2639e+00, 9.4906e+00, 1.2860e+01,  ..., 9.5365e-01, 4.8469e-02,\n",
              "         9.6535e-01],\n",
              "        [7.5294e-01, 1.0326e+01, 2.2564e+01,  ..., 9.8890e-01, 4.7826e-02,\n",
              "         9.4907e-01],\n",
              "        ...,\n",
              "        [9.1813e-01, 2.9421e+01, 1.9176e+01,  ..., 9.8543e-01, 6.4790e-03,\n",
              "         9.6383e-01],\n",
              "        [8.5508e+00, 5.0635e+01, 3.9088e+01,  ..., 9.1652e-01, 6.2200e-02,\n",
              "         9.7333e-01],\n",
              "        [1.9811e+00, 9.4068e+00, 1.5408e+01,  ..., 9.9392e-01, 4.2198e-03,\n",
              "         9.6730e-01]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 52
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 7.3791,  0.4109, -1.5084,  ...,  2.1515,  4.5566,  6.0180],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 52
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 9.4952e-01,  8.4530e+01,  4.6791e+01,  ...,  9.4795e-01,\n",
              "          6.4117e-02,  9.8453e-01],\n",
              "        [ 1.1101e+00,  3.2358e+01, -3.5728e+01,  ...,  1.0028e+00,\n",
              "          4.2319e-02,  9.2391e-01],\n",
              "        [ 1.8930e-01,  2.6863e+01,  1.5945e+01,  ...,  9.6654e-01,\n",
              "          1.6199e-02,  1.0561e+00],\n",
              "        ...,\n",
              "        [ 1.3516e-01, -7.9392e+00, -1.2131e+00,  ...,  1.0494e+00,\n",
              "          8.3238e-02,  9.1046e-01],\n",
              "        [ 1.7712e+00,  4.3202e+01,  4.5530e+01,  ...,  1.0381e+00,\n",
              "          8.8945e-02,  1.0892e+00],\n",
              "        [ 2.4583e+00, -3.2981e+01,  7.4548e+01,  ...,  1.0244e+00,\n",
              "          7.8031e-02,  1.0698e+00]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 52
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 9.1254,  1.8326,  1.4551,  ...,  7.3208, -1.2007,  1.5241],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 52
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.7066e+00,  1.5971e+01,  7.3148e+00,  ...,  1.0045e+00,\n",
              "          2.5816e-02,  1.0476e+00],\n",
              "        [ 2.3870e+00,  1.1095e+01,  1.3234e+01,  ...,  1.0246e+00,\n",
              "          8.5177e-02,  9.3924e-01],\n",
              "        [ 4.2651e+00,  3.8008e+01,  5.5582e+01,  ...,  1.0080e+00,\n",
              "          3.5999e-02,  1.0696e+00],\n",
              "        ...,\n",
              "        [ 1.1491e+00,  1.9284e+01,  4.5160e+01,  ...,  1.0976e+00,\n",
              "          1.8357e-02,  1.0351e+00],\n",
              "        [ 1.7668e+00,  4.2190e+01,  3.4972e+01,  ...,  1.0558e+00,\n",
              "          6.8292e-02,  9.4435e-01],\n",
              "        [ 9.9124e+00, -1.2277e+01,  1.8248e+01,  ...,  1.0422e+00,\n",
              "          7.0388e-02,  1.0822e+00]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 52
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.0361,  2.7455, 17.4197,  ..., -1.6525,  1.5343, -3.0060],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "\n",
        "x_train.shape\n",
        "y_train.shape\n",
        "x_val.shape\n",
        "y_val.shape\n",
        "x_test.shape\n",
        "y_test.shape\n",
        "x_train\n",
        "y_train\n",
        "x_val\n",
        "y_val\n",
        "x_test\n",
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkEyiAXhyUfE",
        "outputId": "d940f0f3-32f1-4db5-d872-b8421aab184a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(False, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 53
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(False, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 53
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(False, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 53
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(False, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 53
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(False, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 53
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(False, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "torch.isnan(x_train).any()\n",
        "torch.isnan(x_val).any()\n",
        "torch.isnan(x_test).any()\n",
        "torch.isnan(y_train).any()\n",
        "torch.isnan(y_val).any()\n",
        "torch.isnan(y_test).any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XWXkFHxyUfF",
        "outputId": "8e9b7542-af4f-44b8-d690-5020b83ce9c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([], device='cuda:0', dtype=torch.int64),\n",
              " tensor([], device='cuda:0', dtype=torch.int64))"
            ]
          },
          "metadata": {},
          "execution_count": 54
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([], device='cuda:0', dtype=torch.int64),\n",
              " tensor([], device='cuda:0', dtype=torch.int64))"
            ]
          },
          "metadata": {},
          "execution_count": 54
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([], device='cuda:0', dtype=torch.int64),\n",
              " tensor([], device='cuda:0', dtype=torch.int64))"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "nan_mask_train = torch.isnan(x_train)     # get a boolean mask indicating NaN values\n",
        "nan_indices_train = torch.where(nan_mask_train)  # get the indices of the NaN values\n",
        "nan_indices_train\n",
        "# len(nan_indices_train)\n",
        "\n",
        "nan_mask_val = torch.isnan(x_val)     # get a boolean mask indicating NaN values\n",
        "nan_indices_val = torch.where(nan_mask_val)  # get the indices of the NaN values\n",
        "nan_indices_val\n",
        "# len(nan_indices_test)\n",
        "\n",
        "nan_mask_test = torch.isnan(x_test)     # get a boolean mask indicating NaN values\n",
        "nan_indices_test = torch.where(nan_mask_test)  # get the indices of the NaN values\n",
        "nan_indices_test\n",
        "# len(nan_indices_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgbP6n8NyUfF"
      },
      "source": [
        "## Visualizing the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BZY02E-6puGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2nFF6bKyUfF"
      },
      "source": [
        "### Visualizing sampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YpNRz6pyUfG",
        "outputId": "0434dd0d-18f0-43ea-e45e-fe9c068a6415"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "if not LOAD_DATA_FROM_CSV:\n",
        "    rho\n",
        "    epsilon\n",
        "    vx\n",
        "    vy\n",
        "    vz \n",
        "    Bx\n",
        "    By\n",
        "    Bz\n",
        "    gxx\n",
        "    gxy\n",
        "    gxz\n",
        "    gyy\n",
        "    gyz\n",
        "    gzz "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Lmz-Ztc2mVyX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naHhJF9OyUfG",
        "outputId": "b7c24cf9-8580-4686-eeb2-24d26fef2a6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100000\n",
            "100000\n",
            "100000\n",
            "100000\n",
            "100000\n",
            "100000\n",
            "100000\n",
            "100000\n",
            "\n",
            "100000\n",
            "100000\n",
            "100000\n",
            "100000\n",
            "100000\n",
            "100000\n"
          ]
        }
      ],
      "source": [
        "if not LOAD_DATA_FROM_CSV:\n",
        "    print(len(rho))\n",
        "    print(len(epsilon))\n",
        "    print(len(vx))\n",
        "    print(len(vy))\n",
        "    print(len(vz))\n",
        "    print(len(Bx))\n",
        "    print(len(By))\n",
        "    print(len(Bz))\n",
        "    print()\n",
        "    print(len(gxx))\n",
        "    print(len(gxy))\n",
        "    print(len(gxz))\n",
        "    print(len(gyy))\n",
        "    print(len(gyz))\n",
        "    print(len(gzz))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMp6XJ6RUZh4"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"last_expr_or_assign\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaQilworyUfH",
        "outputId": "3caaa4dd-e910-44d8-b4c9-48ec1875437e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "plt.hist([np.random.uniform(0, 0.999) for _ in range(n_train_samples)], bins=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrQZUO22yUfH",
        "outputId": "883d0c13-019c-4d5a-a165-5db5ae4e26f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAJHK1SZyUfH"
      },
      "source": [
        "The reason the sampling is not uniformly distributed in the following plots is due to the resampling and the fact that we certain values of e.g. velocity are more likely to violate the speed of light constraint than others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E5YFdqKjUZh5",
        "outputId": "7bfb528d-3062-44d2-fe46-44fbcbf63639"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1600 with 14 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAB8MAAATBCAYAAACvwzUxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzde1xVVf7/8Td3UATSBHREZTQVzUtio6fUzMiTUlNJF0sTEzMdqIRKZTLzGmYpXkKpycBGHdPvWFNiKl7LxEuMlmGZlYal4PxSOGpyP78/erDzJNoRjyKH1/Px2I86e332PmsdiE/w2WstF6vVahUAAAAAAAAAAAAAAE7EtaY7AAAAAAAAAAAAAACAo1EMBwAAAAAAAAAAAAA4HYrhAAAAAAAAAAAAAACnQzEcAAAAAAAAAAAAAOB0KIYDAAAAAAAAAAAAAJwOxXAAAAAAAAAAAAAAgNOhGA4AAAAAAAAAAAAAcDoUwwEAAAAAAAAAAAAATodiOAAAAAAAAAAAAADA6VAMBwAAAAAnMWzYMLVs2dKh9+zTp4/69OlTY+9/rbmUz+Nchw8flouLi1577bU/jJ00aZJcXFyq0btrX+XnkJ6eXtNduSzp6elycXHR4cOHa7orAAAAAICLoBgOAAAAANeAyuJa5eHt7a02bdooLi5O+fn5Nd09w9GjRzVp0iTt3bu3prsCOMSCBQtqfXEeAAAAAFA195ruAAAAAADgN1OmTFFoaKiKioq0bds2LVy4UGvWrNGXX36pevXqXfTaf/zjH6qoqHBof9avX2/z+ujRo5o8ebJatmypLl26XPH3v9b8/vNA7bdgwQJdf/31GjZsmN3XPPbYYxo0aJC8vLyuXMcAAAAAAJeNYjgAAAAAXEP69++vbt26SZJGjBihRo0aafbs2frPf/6jRx55pMprzpw5o/r168vDw8Ph/fH09LQ79kq8/7Xil19+Ub169S7p84Dzqfxvzc3NTW5ubjXdHQAAAADAH2CZdAAAAAC4hvXt21eSdOjQIUm/7svt6+ur7777TgMGDFCDBg00ePBgo+3cPbvP3ac6JSVFf/7zn1WvXj3169dPR44ckdVq1dSpU9WsWTP5+Pjo3nvv1YkTJ2ze/9w9srds2aKbb75ZkvT4448bS7pXLjF97vuXlpaqYcOGevzxx88bk8Vikbe3t5577jnjXHFxsV566SW1bt1aXl5eCgkJ0dixY1VcXHzRzycuLk6+vr765Zdfzmt75JFHFBwcrPLycknSf/7zH0VGRqpp06by8vJSq1atNHXqVKP93DHfeOONys7OVu/evVWvXj39/e9/P+/zkKSSkhJNnDhR4eHh8vf3V/369dWrVy9t3rz5gn1OTk5WixYt5OPjo9tuu01ffvnlRcdYacmSJQoPD5ePj48aNmyoQYMG6ciRIzYxBw8eVFRUlIKDg+Xt7a1mzZpp0KBBKiwsvOi9P/nkEz344INq3ry58fnHx8fr7NmzNnGV338//fST7rvvPvn6+qpx48Z67rnnzvscCwoKNGzYMPn7+ysgIEDR0dEqKCiwa6yV2wZs27ZNTz/9tBo3bqyAgAA9+eSTKikpUUFBgYYOHarrrrtO1113ncaOHSur1Wpzj4qKCs2ZM0cdOnSQt7e3goKC9OSTT+rkyZNGTMuWLZWTk6OtW7ca38+VX9/KPmzdulV/+9vfFBgYqGbNmtm0/X7P8I8++ki33XabGjRoID8/P918881atmyZXWMGAAAAADgeM8MBAAAA4Br23XffSZIaNWpknCsrK5PZbFbPnj312muv/eHy6UuXLlVJSYmeeuopnThxQjNnztRDDz2kvn37asuWLRo3bpy+/fZbzZ8/X88995zefvvtKu8TFhamKVOmaOLEiRo5cqR69eolSbrlllvOi/Xw8ND999+vVatW6Y033rCZUf3++++ruLhYgwYNkvRr0fKvf/2rtm3bppEjRyosLEz79u1TcnKyvvnmG73//vsXHNvDDz+slJQUZWRk6MEHHzTO//LLL/rwww81bNgwYwZvenq6fH19lZCQIF9fX23atEkTJ06UxWLRq6++anPfn3/+Wf3799egQYM0ZMgQBQUFVfn+FotFb731lh555BE98cQTOnXqlBYtWiSz2axdu3adt5T8O++8o1OnTik2NlZFRUWaO3eu+vbtq3379l3wPSRp+vTpevHFF/XQQw9pxIgR+t///qf58+erd+/e2rNnjwICAlRSUiKz2azi4mI99dRTCg4O1k8//aTVq1eroKBA/v7+F7z/ypUr9csvv2j06NFq1KiRdu3apfnz5+vHH3/UypUrbWLLy8tlNpvVvXt3vfbaa9qwYYNmzZqlVq1aafTo0ZIkq9Wqe++9V9u2bdOoUaMUFham9957T9HR0RfsQ1UqxzF58mTt2LFDb775pgICArR9+3Y1b95cL7/8stasWaNXX31VN954o4YOHWpc++STTyo9PV2PP/64nn76aR06dEivv/669uzZo08//VQeHh6aM2eOnnrqKfn6+uqFF16QpPO+Dn/729/UuHFjTZw4UWfOnLlgX9PT0zV8+HB16NBBiYmJCggI0J49e7R27Vo9+uijlzRuAAAAAICDWAEAAAAANS4tLc0qybphwwbr//73P+uRI0esy5cvtzZq1Mjq4+Nj/fHHH61Wq9UaHR1tlWQdP378efeIjo62tmjRwnh96NAhqyRr48aNrQUFBcb5xMREqyRr586draWlpcb5Rx55xOrp6WktKioyzt12223W2267zXi9e/duqyRrWlraH77/unXrrJKsH374oU3cgAEDrH/+85+N1//85z+trq6u1k8++cQmLjU11SrJ+umnn1b9oVmt1oqKCuuf/vQna1RUlM35FStWWCVZP/74Y+PcL7/8ct71Tz75pLVevXrnjVmSNTU19bz4338eZWVl1uLiYpuYkydPWoOCgqzDhw83zlV+Lc79WlqtVuvOnTutkqzx8fHGuZdeesl67q/rhw8ftrq5uVmnT59u8z779u2zuru7G+f37NljlWRduXLlef3+I1V9NklJSVYXFxfrDz/8YJyr/P6bMmWKTexNN91kDQ8PN16///77VknWmTNnGufKysqsvXr1uuD3z7kq/3swm83WiooK47zJZLK6uLhYR40aZXPfZs2a2XxdPvnkE6sk69KlS23uu3bt2vPOd+jQweba3/ehZ8+e1rKysirbDh06ZLVardaCggJrgwYNrN27d7eePXvWJvbc/gMAAAAAri6WSQcAAACAa0hERIQaN26skJAQDRo0SL6+vnrvvff0pz/9ySaucgauPR588EGbWcHdu3eXJA0ZMkTu7u4250tKSvTTTz9d5ih+1bdvX11//fV69913jXMnT55UZmamHn74YePcypUrFRYWpnbt2un//b//ZxyVS8RfbMlxFxcXPfjgg1qzZo1Onz5tnH/33Xf1pz/9ST179jTO+fj4GP9+6tQp/b//9//Uq1cv/fLLL/r6669t7uvl5VXlEu+/5+bmZsx6r6io0IkTJ1RWVqZu3brpv//973nx9913n83X8i9/+Yu6d++uNWvWXPA9Vq1apYqKCj300EM2n09wcLBuuOEG4/Op/BqvW7euymXjL+bcz+bMmTP6f//v/+mWW26R1WrVnj17zosfNWqUzetevXrp+++/N16vWbNG7u7uNt+nbm5ueuqppy6pXzExMXJxcTFed+/eXVarVTExMTb37datm837r1y5Uv7+/rrzzjttPrPw8HD5+vpe9Hvq95544ok/3B88MzNTp06d0vjx4+Xt7W3Tdm7/AQAAAABXF8ukAwAAAMA1JCUlRW3atJG7u7uCgoLUtm1bubraPsfs7u5u7F1sj+bNm9u8riyahoSEVHn+3D2VL4e7u7uioqK0bNkyFRcXy8vLS6tWrVJpaalNMfzgwYP66quv1Lhx4yrvc/z48Yu+z8MPP6w5c+bogw8+0KOPPqrTp09rzZo1evLJJ20KkTk5OZowYYI2bdoki8Vic4/f76n9pz/9yWZp94tZvHixZs2apa+//lqlpaXG+dDQ0PNib7jhhvPOtWnTRitWrLjg/Q8ePCir1VrltdKvS9JXvl9CQoJmz56tpUuXqlevXvrrX/+qIUOGXHSJdEnKzc3VxIkT9cEHH5z39f/9Z+Pt7X3e1+q6666zue6HH35QkyZN5OvraxPXtm3bi/bj9y7le/fc9z948KAKCwsVGBhY5X3/6HvqXFV9HX+vcjuDG2+80e77AgAAAACuPIrhAAAAAHAN+ctf/qJu3bpdNMbLy+u8AvnFXGhW64XOW61Wu+/9RwYNGqQ33nhDH330ke677z6tWLFC7dq1U+fOnY2YiooKdezYUbNnz67yHr8vfP5ejx491LJlS61YsUKPPvqoPvzwQ509e9am4F5QUKDbbrtNfn5+mjJlilq1aiVvb2/997//1bhx41RRUWFzz3NnSl/MkiVLNGzYMN133316/vnnFRgYKDc3NyUlJRkF0stVUVEhFxcXffTRR1V+zc4tOM+aNUvDhg3Tf/7zH61fv15PP/20kpKStGPHjgs+QFFeXq4777xTJ06c0Lhx49SuXTvVr19fP/30k4YNG3beZ/NHs6Qd6VK+d8/9vq2oqFBgYKCWLl1a5fUXevCiKvZ+LwAAAAAArj0UwwEAAAAAdrvUJZ979+6tJk2a6N1331XPnj21adMmvfDCCzYxrVq10ueff6477rij2ktKP/TQQ5o7d64sFoveffddtWzZUj169DDat2zZop9//lmrVq1S7969jfOHDh2q1vtV+r//+z/9+c9/1qpVq2z6/tJLL1UZf/DgwfPOffPNN2rZsuUF36NVq1ayWq0KDQ1VmzZt/rBPHTt2VMeOHTVhwgRt375dt956q1JTUzVt2rQq4/ft26dvvvlGixcv1tChQ43zmZmZf/heF9KiRQtt3LhRp0+ftinWHzhwoNr3vBStWrXShg0bdOutt/5hMdsRy5i3atVKkvTll1+qdevWl30/AAAAAIBjsGc4AAAAAMBu9evXl/TrTGt7uLq66oEHHtCHH36of/7znyorK7OZsS39Wsj+6aef9I9//OO868+ePaszZ8784fs8/PDDKi4u1uLFi7V27Vo99NBDNu2VM4nPnT1cUlKiBQsW2DWOC6nqvjt37lRWVlaV8e+//77Nnuy7du3Szp071b9//wu+x8CBA+Xm5qbJkyefN2vfarXq559/liRZLBaVlZXZtHfs2FGurq4qLi6+pDFYrVbNnTv3gtf8kQEDBqisrEwLFy40zpWXl2v+/PnVvueleOihh1ReXq6pU6ee11ZWVmbz/Vu/fn27v58vpF+/fmrQoIGSkpJUVFRk0+bIlRYAAAAAAJeGmeEAAAAAALu1atVKAQEBSk1NVYMGDVS/fn117979ovsqP/zww5o/f75eeukldezYUWFhYTbtjz32mFasWKFRo0Zp8+bNuvXWW1VeXq6vv/5aK1as0Lp16/5w6fiuXbuqdevWeuGFF1RcXHxewf2WW27Rddddp+joaD399NNycXHRP//5z8suVN59991atWqV7r//fkVGRurQoUNKTU1V+/btdfr06fPiW7durZ49e2r06NEqLi7WnDlz1KhRI40dO/aC79GqVStNmzZNiYmJOnz4sO677z41aNBAhw4d0nvvvaeRI0fqueee06ZNmxQXF6cHH3xQbdq0UVlZmf75z3/Kzc1NUVFRF7x/u3bt1KpVKz333HP66aef5Ofnp3//+9+XtXf8Pffco1tvvVXjx4/X4cOH1b59e61ateq8/cevlNtuu01PPvmkkpKStHfvXvXr108eHh46ePCgVq5cqblz5+qBBx6QJIWHh2vhwoWaNm2aWrdurcDAQPXt2/eS3s/Pz0/JyckaMWKEbr75Zj366KO67rrr9Pnnn+uXX37R4sWLr8QwAQAAAAB/gGI4AAAAAMBuHh4eWrx4sRITEzVq1CiVlZUpLS3tosXwW265RSEhITpy5Mh5RWrp19nj77//vpKTk/XOO+/ovffeU7169fTnP/9ZzzzzjF1Lg0u/Ft2nT5+u1q1bq2vXrjZtjRo10urVq/Xss89qwoQJuu666zRkyBDdcccdMpvNl/YhnGPYsGHKy8vTG2+8oXXr1ql9+/ZasmSJVq5cqS1btpwXP3ToULm6umrOnDk6fvy4/vKXv+j1119XkyZNLvo+48ePV5s2bZScnKzJkydL+nUv9X79+umvf/2rJKlz584ym8368MMP9dNPP6levXrq3LmzPvroI5sl43/Pw8NDH374obG/uLe3t+6//37FxcXZ7O1+KVxdXfXBBx9ozJgxWrJkiVxcXPTXv/5Vs2bN0k033VSte16q1NRUhYeH64033tDf//53ubu7q2XLlhoyZIhuvfVWI27ixIn64YcfNHPmTJ06dUq33XbbJRfDJSkmJkaBgYGaMWOGpk6dKg8PD7Vr107x8fGOHBYAAAAA4BK4WFmvCwAAAAAAAAAAAADgZJgZboeKigodPXpUDRo0kIuLS013BwCuCKvVqlOnTqlp06ZydXWt6e7UWuQMAHUBOcMxyBkA6gJyhmOQMwDUBeQMxyBnAKgLLiVnUAy3w9GjRxUSElLT3QCAq+LIkSNq1qxZTXej1iJnAKhLyBmXh5wBoC4hZ1wecgaAuoSccXnIGQDqEntyBsVwOzRo0EDSrx+on59fDfcGAK4Mi8WikJAQ42ceqoecAaAuIGc4BjkDQF1AznAMcgaAuoCc4RjkDAB1waXkDIrhdqhcSsTPz4/kAcDpsXzS5SFnAKhLyBmXh5wBoC4hZ1wecgaAuoSccXnIGQDqEntyBhtvAAAAAAAAoE4qLy/Xiy++qNDQUPn4+KhVq1aaOnWqrFarEWO1WjVx4kQ1adJEPj4+ioiI0MGDB23uc+LECQ0ePFh+fn4KCAhQTEyMTp8+bRPzxRdfqFevXvL29lZISIhmzpx5VcYIAAAA1GUUwwEAAAAAAFAnvfLKK1q4cKFef/11ffXVV3rllVc0c+ZMzZ8/34iZOXOm5s2bp9TUVO3cuVP169eX2WxWUVGRETN48GDl5OQoMzNTq1ev1scff6yRI0ca7RaLRf369VOLFi2UnZ2tV199VZMmTdKbb755VccLAAAA1DUskw4AAAAAAIA6afv27br33nsVGRkpSWrZsqX+9a9/adeuXZJ+nRU+Z84cTZgwQffee68k6Z133lFQUJDef/99DRo0SF999ZXWrl2r3bt3q1u3bpKk+fPna8CAAXrttdfUtGlTLV26VCUlJXr77bfl6empDh06aO/evZo9e7ZN0RwAAACAYzEzHAAAAIBTYclbAIC9brnlFm3cuFHffPONJOnzzz/Xtm3b1L9/f0nSoUOHlJeXp4iICOMaf39/de/eXVlZWZKkrKwsBQQEGIVwSYqIiJCrq6t27txpxPTu3Vuenp5GjNls1oEDB3Ty5Mnz+lVcXCyLxWJzAAAAALh0FMMBAAAAOBWWvAUA2Gv8+PEaNGiQ2rVrJw8PD910000aM2aMBg8eLEnKy8uTJAUFBdlcFxQUZLTl5eUpMDDQpt3d3V0NGza0ianqHue+x7mSkpLk7+9vHCEhIQ4YLQAAAFD3sEw6AAAAAKfCkrcAAHutWLFCS5cu1bJly4yf42PGjFHTpk0VHR1dY/1KTExUQkKC8dpisVAQBwAAAKqBmeEAAAAAnApL3gIA7PX8888bs8M7duyoxx57TPHx8UpKSpIkBQcHS5Ly8/NtrsvPzzfagoODdfz4cZv2srIynThxwiamqnuc+x7n8vLykp+fn80BAAAA4NJRDAcAAADgVFjyFgBgr19++UWurrZ/HnNzc1NFRYUkKTQ0VMHBwdq4caPRbrFYtHPnTplMJkmSyWRSQUGBsrOzjZhNmzapoqJC3bt3N2I+/vhjlZaWGjGZmZlq27atrrvuuis2PgAAAKCuoxgOAAAAwKmcu+Ttf//7Xy1evFivvfaaFi9eXKP9SkxMVGFhoXEcOXKkRvsDAJDuueceTZ8+XRkZGTp8+LDee+89zZ49W/fff78kycXFRWPGjNG0adP0wQcfaN++fRo6dKiaNm2q++67T5IUFhamu+66S0888YR27dqlTz/9VHFxcRo0aJCaNm0qSXr00Ufl6empmJgY5eTk6N1339XcuXNtlkIHAAAA4HjsGQ4AAADAqZy75K0kdezYUT/88IOSkpIUHR1ts+RtkyZNjOvy8/PVpUsXSVduyVsvLy/HDBIA4BDz58/Xiy++qL/97W86fvy4mjZtqieffFITJ040YsaOHaszZ85o5MiRKigoUM+ePbV27Vp5e3sbMUuXLlVcXJzuuOMOubq6KioqSvPmzTPa/f39tX79esXGxio8PFzXX3+9Jk6cqJEjR17V8QIAAAB1DTPDAQAAADgVlrwFANirQYMGmjNnjn744QedPXtW3333naZNmyZPT08jxsXFRVOmTFFeXp6Kioq0YcMGtWnTxuY+DRs21LJly3Tq1CkVFhbq7bfflq+vr01Mp06d9Mknn6ioqEg//vijxo0bd1XGCABwjPLycr344osKDQ2Vj4+PWrVqpalTp8pqtRoxVqtVEydOVJMmTeTj46OIiAgdPHjQ5j4nTpzQ4MGD5efnp4CAAMXExOj06dM2MV988YV69eolb29vhYSEaObMmVdljADgjCiGAwAAAHAqLHkLAAAAwNFeeeUVLVy4UK+//rq++uorvfLKK5o5c6bmz59vxMycOVPz5s1Tamqqdu7cqfr168tsNquoqMiIGTx4sHJycpSZmanVq1fr448/tlkpxGKxqF+/fmrRooWys7P16quvatKkSXrzzTev6ngBwFmwTDoAAAAAp8KStwAAAAAcbfv27br33nsVGRkpSWrZsqX+9a9/adeuXZJ+nRU+Z84cTZgwQffee68k6Z133lFQUJDef/99DRo0SF999ZXWrl2r3bt3q1u3bpJ+/f1lwIABeu2119S0aVMtXbpUJSUlevvtt+Xp6akOHTpo7969mj17Nr9rAEA1UAwHcNlajs+o9rWHZ0Q6sCcAcD5+RgF1T+WSt3PmzLlgTOWSt1OmTLlgTOWStxdTueRtTajuzzd+tgEA7EWuAYDf3HLLLXrzzTf1zTffqE2bNvr888+1bds2zZ49W5J06NAh5eXlKSIiwrjG399f3bt3V1ZWlgYNGqSsrCwFBAQYhXBJioiIkKurq3bu3Kn7779fWVlZ6t27t82WHWazWa+88opOnjx53pZMxcXFKi4uNl5bLJbLGic/+wE4G4rhAAAAAAAAgBO7nAdEAQC/Gj9+vCwWi9q1ayc3NzeVl5dr+vTpGjx4sCQpLy9PkhQUFGRzXVBQkNGWl5enwMBAm3Z3d3c1bNjQJiY0NPS8e1S2/b4YnpSUpMmTJztolADgfGq0GD5p0qTzfki3bdtWX3/9tSSpqKhIzz77rJYvX67i4mKZzWYtWLDAJpnk5uZq9OjR2rx5s3x9fRUdHa2kpCS5u/82tC1btighIUE5OTkKCQnRhAkTNGzYsKsyRgAAADgGT6cDAAAAAGrKihUrtHTpUi1btsxYunzMmDFq2rSpoqOja6xfiYmJSkhIMF5bLBaFhITUWH8A4FpT4zPDO3TooA0bNhivzy1ix8fHKyMjQytXrpS/v7/i4uI0cOBAffrpp5Kk8vJyRUZGKjg4WNu3b9exY8c0dOhQeXh46OWXX5b069IkkZGRGjVqlJYuXaqNGzdqxIgRatKkicxm89UdrBPjj9OA4/HflS0eoAKujKv9s4ZZSVcGOQOXgu8XAAAA4NI9//zzGj9+vAYNGiRJ6tixo3744QclJSUpOjpawcHBkqT8/Hw1adLEuC4/P19dunSRJAUHB+v48eM29y0rK9OJEyeM64ODg5Wfn28TU/m6MuZcXl5e8vLycswgAcAJ1Xgx3N3dvcof4IWFhVq0aJGWLVumvn37SpLS0tIUFhamHTt2qEePHlq/fr3279+vDRs2KCgoSF26dNHUqVM1btw4TZo0SZ6enkpNTVVoaKhmzZolSQoLC9O2bduUnJxMMbwK/IEacDz+u3KcuvAA1dX+fqmJwgZFGNQW7DcPAAAAAKj0yy+/yNXV1eacm5ubKioqJEmhoaEKDg7Wxo0bjeK3xWLRzp07NXr0aEmSyWRSQUGBsrOzFR4eLknatGmTKioq1L17dyPmhRdeUGlpqTw8PCRJmZmZatu27XlLpAMA/liNF8MPHjyopk2bytvbWyaTSUlJSWrevLmys7NVWlqqiIgII7Zdu3Zq3ry5srKy1KNHD2VlZaljx442s/7MZrNGjx6tnJwc3XTTTcrKyrK5R2XMmDFjLtin4uJiFRcXG68tFovjBnwVUHhDddXE905tmpFIYaPm8QAVcGHkfwAAAAAArpx77rlH06dPV/PmzdWhQwft2bNHs2fP1vDhwyVJLi4uGjNmjKZNm6YbbrhBoaGhevHFF9W0aVPdd999kn79W9Ndd92lJ554QqmpqSotLVVcXJwGDRqkpk2bSpIeffRRTZ48WTExMRo3bpy+/PJLzZ07V8nJyTU1dACo1Wq0GN69e3elp6erbdu2OnbsmCZPnqxevXrpyy+/VF5enjw9PRUQEGBzTVBQkPLy8iRJeXl5NoXwyvbKtovFWCwWnT17Vj4+Puf1Kykp6byleIHahILIlcHnWvN4gArOjp8z4HsAAAAAAK5N8+fP14svvqi//e1vOn78uJo2baonn3xSEydONGLGjh2rM2fOaOTIkSooKFDPnj21du1aeXt7GzFLly5VXFyc7rjjDrm6uioqKkrz5s0z2v39/bV+/XrFxsYqPDxc119/vSZOnKiRI0de1fECgLOo0WJ4//79jX/v1KmTunfvrhYtWmjFihVVFqmvlsTERCUkJBivLRaLQkJCqn0/loOt2/ijNuAYPEB1ZbBaAgDAXuQMAAAA1GUNGjTQnDlzNGfOnAvGuLi4aMqUKZoyZcoFYxo2bKhly5Zd9L06deqkTz75pLpdrRHUQQBcq2p8mfRzBQQEqE2bNvr222915513qqSkRAUFBTbFjfz8fGOJ3ODgYO3atcvmHvn5+UZb5T8rz50b4+fnd8GCu5eXl7y8vBw1LFxETSRIkjJQO9WVB6gAAAAAoLbjASoAAABcK66pYvjp06f13Xff6bHHHlN4eLg8PDy0ceNGRUVFSZIOHDig3NxcmUwmSZLJZNL06dN1/PhxBQYGSpIyMzPl5+en9u3bGzFr1qyxeZ/MzEzjHtcyZhQDwIXxABWAaxn/HwcAAAAAAADUvBothj/33HO655571KJFCx09elQvvfSS3Nzc9Mgjj8jf318xMTFKSEhQw4YN5efnp6eeekomk0k9evSQJPXr10/t27fXY489ppkzZyovL08TJkxQbGysUZgYNWqUXn/9dY0dO1bDhw/Xpk2btGLFCmVk8AfK2qwm/sDMH7WBawsPUAE1i7wIAAAAAAAuF6uJALjSarQY/uOPP+qRRx7Rzz//rMaNG6tnz57asWOHGjduLElKTk6Wq6uroqKiVFxcLLPZrAULFhjXu7m5afXq1Ro9erRMJpPq16+v6Ohom/04QkNDlZGRofj4eM2dO1fNmjXTW2+9JbPZfNXHCwCoPh6guvbUhWIoW2sAAAAAAAAAQO1Vo8Xw5cuXX7Td29tbKSkpSklJuWBMixYtzpvF93t9+vTRnj17qtVHAMC1gQeoAAAAAMD58UAqAAAAHOma2jMcAIAL4QEq1CZ1YdZ8bcLXAwAAAAAAAKibKIYDqJUobAAAANQezPIDAAAAAAA1gWI4AACoFXgIBgAAAAAAAABwKSiGAwAAAAAAAKjVWIUEAAAAVXGt6Q4AAAAAAAAAAAAAAOBozAwHAAAAAFyTmOUHAAAAAAAuB8VwAAAAAAAAAAAA1Co8PAvAHiyTDgAAAAAAAAAAAABwOhTDAQAAAAAAAAAAAABOh2I4AAAAAAAAAAAAAMDpUAwHAAAAAAAAAAAAADgdiuEAAAAAAAAAAAAAAKfjXtMdAAAAAAAAAICa0HJ8RrWuOzwj0sE9AQAAwJXAzHAAAAAAAAAAAAAAgNOhGA4AAAAAAAAAAAAAcDoskw4AAAAAAAAAAIA6gS0ygLqFmeEAAAAAAACok1q2bCkXF5fzjtjYWElSUVGRYmNj1ahRI/n6+ioqKkr5+fk298jNzVVkZKTq1aunwMBAPf/88yorK7OJ2bJli7p27SovLy+1bt1a6enpV2uIAAAAQJ1GMRwAAAAAAAB10u7du3Xs2DHjyMzMlCQ9+OCDkqT4+Hh9+OGHWrlypbZu3aqjR49q4MCBxvXl5eWKjIxUSUmJtm/frsWLFys9PV0TJ040Yg4dOqTIyEjdfvvt2rt3r8aMGaMRI0Zo3bp1V3ewAAAAQB1EMRwAAACAU2GWHwDAXo0bN1ZwcLBxrF69Wq1atdJtt92mwsJCLVq0SLNnz1bfvn0VHh6utLQ0bd++XTt27JAkrV+/Xvv379eSJUvUpUsX9e/fX1OnTlVKSopKSkokSampqQoNDdWsWbMUFhamuLg4PfDAA0pOTq7JoQMAAAB1AnuGAwAAAHAqu3fvVnl5ufH6yy+/1J133mkzyy8jI0MrV66Uv7+/4uLiNHDgQH366aeSfpvlFxwcrO3bt+vYsWMaOnSoPDw89PLLL0v6bZbfqFGjtHTpUm3cuFEjRoxQkyZNZDabr/6gYYM9AAFUR0lJiZYsWaKEhAS5uLgoOztbpaWlioiIMGLatWun5s2bKysrSz169FBWVpY6duyooKAgI8ZsNmv06NHKycnRTTfdpKysLJt7VMaMGTPmgn0pLi5WcXGx8dpisThuoAAAoFr4PQOonZgZDgAAAMCpMMsPAFAd77//vgoKCjRs2DBJUl5enjw9PRUQEGATFxQUpLy8PCPm3EJ4ZXtl28ViLBaLzp49W2VfkpKS5O/vbxwhISGXOzwAAACgTqIYDgAAAMBpVc7yGz58uF2z/CRdcJafxWJRTk6OEVPVLL/Ke1SluLhYFovF5gAAXDsWLVqk/v37q2nTpjXdFSUmJqqwsNA4jhw5UtNdAgAAAGoliuEAAAAAnBaz/AAA9vjhhx+0YcMGjRgxwjgXHByskpISFRQU2MTm5+crODjYiMnPzz+vvbLtYjF+fn7y8fGpsj9eXl7y8/OzOQAAAABcOorhAAAAAJwWs/wAAPZIS0tTYGCgIiN/29MzPDxcHh4e2rhxo3HuwIEDys3NlclkkiSZTCbt27dPx48fN2IyMzPl5+en9u3bGzHn3qMypvIeAAAAAK4ciuEAAAAAnBKz/AAA9qioqFBaWpqio6Pl7u5unPf391dMTIwSEhK0efNmZWdn6/HHH5fJZFKPHj0kSf369VP79u312GOP6fPPP9e6des0YcIExcbGysvLS5I0atQoff/99xo7dqy+/vprLViwQCtWrFB8fHyNjBcAUD0tW7aUi4vLeUdsbKwkqaioSLGxsWrUqJF8fX0VFRV13u8Mubm5ioyMVL169RQYGKjnn39eZWVlNjFbtmxR165d5eXlpdatWys9Pf1qDREAnJL7H4cAAAAAQO3zR7P8oqKiJFU9y2/69Ok6fvy4AgMDJVU9y2/NmjU278csPwConTZs2KDc3FwNHz78vLbk5GS5uroqKipKxcXFMpvNWrBggdHu5uam1atXa/To0TKZTKpfv76io6M1ZcoUIyY0NFQZGRmKj4/X3Llz1axZM7311lsym81XZXy4MlqOz6j2tYdnRP5xEIBrzu7du1VeXm68/vLLL3XnnXfqwQcflCTFx8crIyNDK1eulL+/v+Li4jRw4EB9+umnkqTy8nJFRkYqODhY27dv17FjxzR06FB5eHjo5ZdfliQdOnRIkZGRGjVqlJYuXaqNGzdqxIgRatKkCXkDAKqJYjgAAAAAp2PPLL+GDRvKz89PTz311AVn+c2cOVN5eXlVzvJ7/fXXNXbsWA0fPlybNm3SihUrlJFR/T+MAwBqRr9+/WS1Wqts8/b2VkpKilJSUi54fYsWLc57QOr3+vTpoz179lxWPwEANatx48Y2r2fMmKFWrVrptttuU2FhoRYtWqRly5apb9++kn59ODcsLEw7duxQjx49tH79eu3fv18bNmxQUFCQunTpoqlTp2rcuHGaNGmSPD09lZqaqtDQUM2aNUuSFBYWpm3btik5OZliOABUE8ukAwAAAHA6fzTL7+6771ZUVJR69+6t4OBgrVq1ymivnOXn5uYmk8mkIUOGaOjQoVXO8svMzFTnzp01a9YsZvkBAAAAdURJSYmWLFmi4cOHy8XFRdnZ2SotLVVERIQR065dOzVv3lxZWVmSpKysLHXs2FFBQUFGjNlslsViUU5OjhFz7j0qYyrvUZXi4mJZLBabAwDwG2aGAwAAAHA6zPIDAAAAcKW8//77Kigo0LBhwyRJeXl58vT0VEBAgE1cUFCQ8vLyjJhzC+GV7ZVtF4uxWCw6e/asfHx8zutLUlKSJk+e7IhhAYBTohgOAAAAAIDY/xUAAAD2WbRokfr376+mTZvWdFeUmJiohIQE47XFYlFISEgN9ggAri0UwwEAAAAAAAAAAOzwww8/aMOGDTZbLQUHB6ukpEQFBQU2s8Pz8/MVHBxsxOzatcvmXvn5+UZb5T8rz50b4+fnV+WscEny8vKSl5fXZY8LAJwVe4YDAAAAAAAAAADYIS0tTYGBgYqM/G1loPDwcHl4eGjjxo3GuQMHDig3N1cmk0mSZDKZtG/fPh0/ftyIyczMlJ+fn9q3b2/EnHuPypjKewAALh3FcAAAAAAAAAAAgD9QUVGhtLQ0RUdHy939t4V3/f39FRMTo4SEBG3evFnZ2dl6/PHHZTKZ1KNHD0lSv3791L59ez322GP6/PPPtW7dOk2YMEGxsbHGzO5Ro0bp+++/19ixY/X1119rwYIFWrFiheLj42tkvADgDFgmHQAAAAAAAAAA4A9s2LBBubm5Gj58+HltycnJcnV1VVRUlIqLi2U2m7VgwQKj3c3NTatXr9bo0aNlMplUv359RUdHa8qUKUZMaGioMjIyFB8fr7lz56pZs2Z66623ZDabr8r4AMAZUQwHAAAAAAAAAAD4A/369ZPVaq2yzdvbWykpKUpJSbng9S1atNCaNWsu+h59+vTRnj17LqufAIDfsEw6AAAAAAAAAAAAAMDpUAwHAAAAAAAAAAAAADgdlkkHAAAAAAAAAAAAroCW4zOqfe3hGZEO7AlQNzEzHAAAAAAAAAAAAADgdCiGAwAAAAAAAAAAAACcDsVwAAAAAAAAAAAAAIDToRgOAAAAAAAAAAAAAHA6FMMBAAAAAAAAAAAAAE7HvaY7AAAAAABAbddyfEa1rjs8I9LBPQEAXOvIGQAAAFcPM8MBAAAAAAAAAAAAAE6HYjgAAAAAAAAAAAAAwOlQDAcAAAAAAAAAAAAAOB2K4QAAAAAAAAAAAAAAp+Ne0x0AAAAAAAAAAAAAYKvl+IxqXXd4RqSDewLUXswMBwAAAAAAAAAAAAA4HYrhAAAAAAAAAAAAAACnQzEcAAAAAAAAAAAAAOB0KIYDAAAAAAAAAAAAAJzONVMMnzFjhlxcXDRmzBjjXFFRkWJjY9WoUSP5+voqKipK+fn5Ntfl5uYqMjJS9erVU2BgoJ5//nmVlZXZxGzZskVdu3aVl5eXWrdurfT09KswIgDAlULOAAAAAAAAAAAAf+SaKIbv3r1bb7zxhjp16mRzPj4+Xh9++KFWrlyprVu36ujRoxo4cKDRXl5ersjISJWUlGj79u1avHix0tPTNXHiRCPm0KFDioyM1O233669e/dqzJgxGjFihNatW3fVxgcAcBxyBgAAAAAAAAAAsEeNF8NPnz6twYMH6x//+Ieuu+4643xhYaEWLVqk2bNnq2/fvgoPD1daWpq2b9+uHTt2SJLWr1+v/fv3a8mSJerSpYv69++vqVOnKiUlRSUlJZKk1NRUhYaGatasWQoLC1NcXJweeOABJScn18h4AQDVR84AAAAAAAAAAAD2cq/pDsTGxioyMlIRERGaNm2acT47O1ulpaWKiIgwzrVr107NmzdXVlaWevTooaysLHXs2FFBQUFGjNls1ujRo5WTk6ObbrpJWVlZNveojDl3ad3fKy4uVnFxsfHaYrE4YKQAgMtFzgAAAM6m5fiMal13eEakg3sCAAAAAIDzqdGZ4cuXL9d///tfJSUlndeWl5cnT09PBQQE2JwPCgpSXl6eEXNuUaOyvbLtYjEWi0Vnz56tsl9JSUny9/c3jpCQkGqNDwDgOOQMAAAAAFfCTz/9pCFDhqhRo0by8fFRx44d9dlnnxntVqtVEydOVJMmTeTj46OIiAgdPHjQ5h4nTpzQ4MGD5efnp4CAAMXExOj06dM2MV988YV69eolb29vhYSEaObMmVdlfAAAAEBdVmPF8CNHjuiZZ57R0qVL5e3tXVPdqFJiYqIKCwuN48iRIzXdJQCo08gZAAAAAK6EkydP6tZbb5WHh4c++ugj7d+/X7NmzbLZlmnmzJmaN2+eUlNTtXPnTtWvX19ms1lFRUVGzODBg5WTk6PMzEytXr1aH3/8sUaOHGm0WywW9evXTy1atFB2drZeffVVTZo0SW+++eZVHS8AAABQ19RYMTw7O1vHjx9X165d5e7uLnd3d23dulXz5s2Tu7u7goKCVFJSooKCApvr8vPzFRwcLEkKDg5Wfn7+ee2VbReL8fPzk4+PT5V98/Lykp+fn80BAKg55AwAwKVilh8AwB6vvPKKQkJClJaWpr/85S8KDQ1Vv3791KpVK0m/5os5c+ZowoQJuvfee9WpUye98847Onr0qN5//31J0ldffaW1a9fqrbfeUvfu3dWzZ0/Nnz9fy5cv19GjRyVJS5cuVUlJid5++2116NBBgwYN0tNPP63Zs2fX1NABAACAOqHG9gy/4447tG/fPptzjz/+uNq1a6dx48YpJCREHh4e2rhxo6KioiRJBw4cUG5urkwmkyTJZDJp+vTpOn78uAIDAyVJmZmZ8vPzU/v27Y2YNWvW2LxPZmamcQ8AwLWPnAEAuBSVs/xuv/12ffTRR2rcuLEOHjxY5Sy/xYsXKzQ0VC+++KLMZrP2799vrEIyePBgHTt2TJmZmSotLdXjjz+ukSNHatmyZZJ+m+UXERGh1NRU7du3T8OHD1dAQIDNbEAAwLXrgw8+kNls1oMPPqitW7fqT3/6k/72t7/piSeekCQdOnRIeXl5ioiIMK7x9/dX9+7dlZWVpUGDBikrK0sBAQHq1q2bERMRESFXV1ft3LlT999/v7KystS7d295enoaMWazWa+88opOnjxpk6Mkqbi4WMXFxcZri8VypT4C1CItx2dU67rDMyId3BMAAIDao8aK4Q0aNNCNN95oc65+/fpq1KiRcT4mJkYJCQlq2LCh/Pz89NRTT8lkMqlHjx6SpH79+ql9+/Z67LHHNHPmTOXl5WnChAmKjY2Vl5eXJGnUqFF6/fXXNXbsWA0fPlybNm3SihUrlJFRvf95BABcfeQMAMClOHeWX6XQ0FDj338/y0+S3nnnHQUFBen999/XoEGDjFl+u3fvNoob8+fP14ABA/Taa6+padOmNrP8PD091aFDB+3du1ezZ8+mGA4AtcT333+vhQsXKiEhQX//+9+1e/duPf300/L09FR0dLTy8vIkSUFBQTbXBQUFGW15eXnGA7eV3N3d1bBhQ5uYc3PRuffMy8s7rxielJSkyZMnO26gAAAAQB1VY8uk2yM5OVl33323oqKi1Lt3bwUHB2vVqlVGu5ubm1avXi03NzeZTCYNGTJEQ4cO1ZQpU4yY0NBQZWRkKDMzU507d9asWbP01ltvyWw218SQAABXCDkDAFDpgw8+ULdu3fTggw8qMDBQN910k/7xj38Y7X80y0/SH87yq4ypapbfgQMHdPLkySs9TACAA1RUVKhr1656+eWXddNNN2nkyJF64oknlJqaWqP9SkxMVGFhoXEcOXKkRvsDAAAA1FY1NjO8Klu2bLF57e3trZSUFKWkpFzwmhYtWpy3pO3v9enTR3v27HFEFwEA1whyBgDgQq7VWX4seQsA154mTZoY2yZVCgsL07///W9JUnBwsCQpPz9fTZo0MWLy8/PVpUsXI+b48eM29ygrK9OJEyeM64ODg5Wfn28TU/m6MuZcXl5exgpWAAAAAKrvmp4ZDgAAAACX6lqd5ZeUlCR/f3/jCAkJqdH+AACkW2+9VQcOHLA5980336hFixaSfl09Kjg4WBs3bjTaLRaLdu7cKZPJJEkymUwqKChQdna2EbNp0yZVVFSoe/fuRszHH3+s0tJSIyYzM1Nt27Y97+EpAMC166efftKQIUPUqFEj+fj4qGPHjvrss8+MdqvVqokTJ6pJkyby8fFRRESEDh48aHOPEydOaPDgwfLz81NAQIBiYmJ0+vRpm5gvvvhCvXr1kre3t0JCQjRz5syrMj4AcEbX1MxwAAAAALhc1+osv8TERCUkJBivLRYLBXEAqGHx8fG65ZZb9PLLL+uhhx7Srl279Oabb+rNN9+UJLm4uGjMmDGaNm2abrjhBoWGhurFF19U06ZNdd9990n6NcfcddddxoNXpaWliouL06BBg9S0aVNJ0qOPPqrJkycrJiZG48aN05dffqm5c+cqOTm5poYOALhEJ0+e1K233qrbb79dH330kRo3bqyDBw/aPNQ0c+ZMzZs3T4sXLzZyhtls1v79++Xt7S1JGjx4sI4dO6bMzEyVlpbq8ccf18iRI7Vs2TJJv/6e0K9fP0VERCg1NVX79u3T8OHDFRAQoJEjR9bI2FH7tByfUa3rDs+IdHBPgJpHMRwAAACAU7mUWX6Vxe/KWX6jR4+WZDvLLzw8XFLVs/xeeOEFlZaWysPDQ9LFZ/mx5C0AXHtuvvlmvffee0pMTNSUKVMUGhqqOXPmaPDgwUbM2LFjdebMGY0cOVIFBQXq2bOn1q5daxQ1JGnp0qWKi4vTHXfcIVdXV0VFRWnevHlGu7+/v9avX6/Y2FiFh4fr+uuv18SJEylqAEAt8sorrygkJERpaWnGuXO3TbJarZozZ44mTJige++9V5L0zjvvKCgoSO+//74GDRqkr776SmvXrtXu3bvVrVs3SdL8+fM1YMAAvfbaa2ratKmWLl2qkpISvf322/L09FSHDh20d+9ezZ49m7wBANXAMukAAAAAnEp8fLx27Nihl19+Wd9++62WLVumN998U7GxsZJsZ/l98MEH2rdvn4YOHXrBWX67du3Sp59+WuUsP09PT8XExCgnJ0fvvvuu5s6dazP7GwBw7bv77ru1b98+FRUV6auvvtITTzxh0+7i4qIpU6YoLy9PRUVF2rBhg9q0aWMT07BhQy1btkynTp1SYWGh3n77bfn6+trEdOrUSZ988omKior0448/aty4cVd8bAAAx/nggw/UrVs3PfjggwoMDNRNN92kf/zjH0b7oUOHlJeXp4iICOOcv7+/unfvrqysLElSVlaWAgICjEK4JEVERMjV1VU7d+40Ynr37i1PT08jxmw268CBAzp58uSVHiYAOB2K4QAAAACcSuUsv3/961+68cYbNXXq1Cpn+T311FMaOXKkbr75Zp0+fbrKWX7t2rXTHXfcoQEDBqhnz57GsrnSb7P8Dh06pPDwcD377LPM8gMAAACc1Pfff6+FCxfqhhtu0Lp16zR69Gg9/fTTWrx4sSQpLy9PkhQUFGRzXVBQkNGWl5enwMBAm3Z3d3c1bNjQJqaqe5z7HucqLi6WxWKxOQAAv2GZdAAAAABO5+6779bdd999wfbKWX5Tpky5YEzlLL+LqZzlB1xt1d0DUGIfQAAAgOqoqKhQt27d9PLLL0uSbrrpJn355ZdKTU1VdHR0jfUrKSlJkydPrrH3B4BrHcVwAAAAAAAAAHBSPEAFOEaTJk3Uvn17m3NhYWH697//LUkKDg6WJOXn56tJkyZGTH5+vrp06WLEHD9+3OYeZWVlOnHihHF9cHCw8vPzbWIqX1fGnCsxMdFmqyaLxaKQkJDqDBEAnBLLpAMAAAAAAAAAAFzErbfeqgMHDtic++abb9SiRQtJUmhoqIKDg7Vx40aj3WKxaOfOnTKZTJIkk8mkgoICZWdnGzGbNm1SRUWFunfvbsR8/PHHKi0tNWIyMzPVtm1bXXfddef1y8vLS35+fjYHAOA3FMMBAAAAAAAAAAAuIj4+Xjt27NDLL7+sb7/9VsuWLdObb76p2NhYSb9uxTRmzBhNmzZNH3zwgfbt26ehQ4eqadOmuu+++yT9OpP8rrvu0hNPPKFdu3bp008/VVxcnAYNGqSmTZtKkh599FF5enoqJiZGOTk5evfddzV37lyb2d8AAPuxTDoAAAAAAAAAAMBF3HzzzXrvvfeUmJioKVOmKDQ0VHPmzNHgwYONmLFjx+rMmTMaOXKkCgoK1LNnT61du1be3t5GzNKlSxUXF6c77rhDrq6uioqK0rx584x2f39/rV+/XrGxsQoPD9f111+viRMnauTIkVd1vADgLCiGAwAAAAAAAAAA/IG7775bd9999wXbXVxcNGXKFE2ZMuWCMQ0bNtSyZcsu+j6dOnXSJ598Uu1+AgB+wzLpAAAAAAAAAAAAAACnQzEcAAAAAAAAAAAAAOB0KIYDAAAAAAAAAAAAAJwOxXAAAAAAAAAAAAAAgNOhGA4AAAAAAAAAAAAAcDoUwwEAAAAAAAAAAAAATodiOAAAAAAAAAAAAADA6VAMBwAAAAAAAAAAAAA4HYrhAAAAAAAAAAAAAACn417THQAAAAAAAAAAAABQs1qOz6j2tYdnRDqwJ4DjMDMcAAAAAAAAAAAAAOB0KIYDAAAAAAAAAAAAAJwOy6QDAAAAAAAAAM5T3eVyWSoXAABcK5gZDgAAAAAAAAAAAABwOhTDAQAAAAAAAAAAAABOh2I4AAAAAAAAAAAAAMDpsGc4AAAAAAB1CPu/AgAAAADqCmaGAwAAAAAAAAAAAACcDsVwAAAAAAAAAAAAAIDToRgOAAAAAAAAAAAAAHA6FMMBAAAAAAAAAAAAAE7HvaY7AAAAAAAAAAAAAKD2ajk+o1rXHZ4R6eCeALaYGQ4AAAAAAAAAAAAAcDoUwwEAAAAAAAAAAAAATodiOAAAAAAAAOqkSZMmycXFxeZo166d0V5UVKTY2Fg1atRIvr6+ioqKUn5+vs09cnNzFRkZqXr16ikwMFDPP/+8ysrKbGK2bNmirl27ysvLS61bt1Z6evrVGB4AAABQ51EMBwAAAAAAQJ3VoUMHHTt2zDi2bdtmtMXHx+vDDz/UypUrtXXrVh09elQDBw402svLyxUZGamSkhJt375dixcvVnp6uiZOnGjEHDp0SJGRkbr99tu1d+9ejRkzRiNGjNC6deuu6jgBAACAuohiOAAAAACnwiw/AMClcHd3V3BwsHFcf/31kqTCwkItWrRIs2fPVt++fRUeHq60tDRt375dO3bskCStX79e+/fv15IlS9SlSxf1799fU6dOVUpKikpKSiRJqampCg0N1axZsxQWFqa4uDg98MADSk5OrrExAwAAAHUFxXAAAAAATodZfgAAex08eFBNmzbVn//8Zw0ePFi5ubmSpOzsbJWWlioiIsKIbdeunZo3b66srCxJUlZWljp27KigoCAjxmw2y2KxKCcnx4g59x6VMZX3qEpxcbEsFovNAQAAAODSudd0BwAAAADA0Spn+f1e5Sy/ZcuWqW/fvpKktLQ0hYWFaceOHerRo4cxy2/Dhg0KCgpSly5dNHXqVI0bN06TJk2Sp6enzSw/SQoLC9O2bduUnJwss9l8VccKAKi+7t27Kz09XW3bttWxY8c0efJk9erVS19++aXy8vLk6empgIAAm2uCgoKUl5cnScrLy7MphFe2V7ZdLMZisejs2bPy8fE5r19JSUmaPHmyo4YJXHUtx2dU67rDMyId3BMAAFDXMTMcAAAAgNNhlh8AwB79+/fXgw8+qE6dOslsNmvNmjUqKCjQihUrarRfiYmJKiwsNI4jR47UaH8AAACA2opiOAAAAACnUjnLb+3atVq4cKEOHTqkXr166dSpU1dtll9VkpKS5O/vbxwhISGOGC4AwIECAgLUpk0bffvttwoODlZJSYkKCgpsYvLz843VR4KDg5Wfn39ee2XbxWL8/PyqnBUuSV5eXvLz87M5AAA1a9KkSXJxcbE52rVrZ7QXFRUpNjZWjRo1kq+vr6Kios77+Z+bm6vIyEjVq1dPgYGBev7551VWVmYTs2XLFnXt2lVeXl5q3bq10tPTr8bwAMBpUQwHAAAA4FSY5QcAqK7Tp0/ru+++U5MmTRQeHi4PDw9t3LjRaD9w4IByc3NlMpkkSSaTSfv27dPx48eNmMzMTPn5+al9+/ZGzLn3qIypvAcAoPbo0KGDjh07Zhzbtm0z2uLj4/Xhhx9q5cqV2rp1q44ePaqBAwca7eXl5YqMjFRJSYm2b9+uxYsXKz09XRMnTjRiDh06pMjISN1+++3au3evxowZoxEjRmjdunVXdZwA4EzYMxwAAACAUzt3lt+dd95pzPI7d3b472f57dq1y+Yejprl5+Xl5ahhAQAc4LnnntM999yjFi1a6OjRo3rppZfk5uamRx55RP7+/oqJiVFCQoIaNmwoPz8/PfXUUzKZTOrRo4ckqV+/fmrfvr0ee+wxzZw5U3l5eZowYYJiY2ONn/mjRo3S66+/rrFjx2r48OHatGmTVqxYoYyM6u2pDACoOe7u7sbvBOcqLCzUokWLtGzZMvXt21eSlJaWprCwMO3YsUM9evTQ+vXrtX//fm3YsEFBQUHq0qWLpk6dqnHjxmnSpEny9PRUamqqQkNDNWvWLElSWFiYtm3bpuTkZJnN5qs6VgBwFtWaGf799987uh8AACdFzgAA2OtK5Qxm+QGA83FUzvjxxx/1yCOPqG3btnrooYfUqFEj7dixQ40bN5YkJScn6+6771ZUVJR69+6t4OBgrVq1yrjezc1Nq1evlpubm0wmk4YMGaKhQ4dqypQpRkxoaKgyMjKUmZmpzp07a9asWXrrrbcoagDAVeLI3zMOHjyopk2b6s9//rMGDx6s3NxcSVJ2drZKS0sVERFhxLZr107NmzdXVlaWJCkrK0sdO3a02W7JbDbLYrEoJyfHiDn3HpUxlfeoSnFxsSwWi80BAPhNtYrhrVu31u23364lS5aoqKjI0X0CADgRcgYAwF6OyhnPPfectm7dqsOHD2v79u26//77q5zlt3nzZmVnZ+vxxx+/4Cy/zz//XOvWratylt/333+vsWPH6uuvv9aCBQu0YsUKxcfHO+SzAABcnKNyxvLly3X06FEVFxfrxx9/1PLly9WqVSuj3dvbWykpKTpx4oTOnDmjVatWnTcjsEWLFlqzZo1++eUX/e9//9Nrr70md3fbxRj79OmjPXv2qLi4WN99952GDRtW7T4DAC6No3JG9+7dlZ6errVr12rhwoU6dOiQevXqpVOnTikvL0+enp42q09JUlBQkPLy8iRJeXl5NoXwyvbKtovFWCwWnT17tsp+JSUlyd/f3zhCQkKqPUYAcEbVWib9v//9r9LS0pSQkKC4uDg9/PDDiomJ0V/+8hdH9w8AUMuRMwAA9nJUzqic5ffzzz+rcePG6tmz53mz/FxdXRUVFaXi4mKZzWYtWLDAuL5ylt/o0aNlMplUv359RUdHVznLLz4+XnPnzlWzZs2Y5Qen13J89ZZ0Pjwj0sE9Afg9AwBgP0fljP79+xv/3qlTJ3Xv3l0tWrTQihUrLrhV0tWQmJiohIQE47XFYqEgjlqF3zNwpVVrZniXLl00d+5cHT16VG+//baOHTumnj176sYbb9Ts2bP1v//9z9H9BADUUuQMAIC9HJUzmOUHAM6P3zMAAPa6UjkjICBAbdq00bfffqvg4GCVlJSooKDAJiY/P9/4XSM4OFj5+fnntVe2XSzGz8/vggV3Ly8v+fn52RwAgN9Uqxheyd3dXQMHDtTKlSv1yiuv6Ntvv9Vzzz2nkJAQDR06VMeOHXNUPwEAtRw5AwBgL3IGAMBe5AwAgL0cnTNOnz6t7777Tk2aNFF4eLg8PDy0ceNGo/3AgQPKzc2VyWSSJJlMJu3bt0/Hjx83YjIzM+Xn56f27dsbMefeozKm8h4AgEt3WcXwzz77TH/729/UpEkTzZ49W88995y+++47ZWZm6ujRo7r33nsd1U8AQC1HzgAA2IucAQCwFzkDAGCvy80Zzz33nLZu3arDhw9r+/btuv/+++Xm5qZHHnlE/v7+iomJUUJCgjZv3qzs7Gw9/vjjMplM6tGjhySpX79+at++vR577DF9/vnnWrdunSZMmKDY2Fh5eXlJkkaNGqXvv/9eY8eO1ddff60FCxZoxYoVio+Pv+KfDwA4q2rtGT579mylpaXpwIEDGjBggN555x0NGDBArq6/1tZDQ0OVnp6uli1bOrKvAIBaiJwBALAXOQMAYC9yBgDAXo7KGT/++KMeeeQR/fzzz2rcuLF69uypHTt2qHHjxpKk5ORkubq6KioqSsXFxTKbzVqwYIFxvZubm1avXq3Ro0fLZDKpfv36io6O1pQpU4yY0NBQZWRkKD4+XnPnzlWzZs301ltvyWw2O/6DAYA6olrF8IULF2r48OEaNmyYmjRpUmVMYGCgFi1adFmdAwDUfuQMAIC9yBkAAHuRMwAA9nJUzli+fPlF2729vZWSkqKUlJQLxrRo0UJr1qy56H369OmjPXv2XDQGAGC/ai2TfvDgQSUmJl4wcUiSp6enoqOjL3qfhQsXqlOnTvLz85Ofn59MJpM++ugjo72oqEixsbFq1KiRfH19FRUVpfz8fJt75ObmKjIyUvXq1VNgYKCef/55lZWV2cRs2bJFXbt2lZeXl1q3bq309PRLHzQAoFoclTMAAM6PnAEAsBc5AwBgL3IGANRt1SqGp6WlaeXKleedX7lypRYvXmz3fZo1a6YZM2YoOztbn332mfr27at7771XOTk5kqT4+Hh9+OGHWrlypbZu3aqjR49q4MCBxvXl5eWKjIxUSUmJtm/frsWLFys9PV0TJ040Yg4dOqTIyEjdfvvt2rt3r8aMGaMRI0Zo3bp11Rk6AOASOSpn8AAVADg/R+UMAIDzI2cAAOxFzgCAuq1axfCkpCRdf/31550PDAzUyy+/bPd97rnnHg0YMEA33HCD2rRpo+nTp8vX11c7duxQYWGhFi1apNmzZ6tv374KDw9XWlqatm/frh07dkiS1q9fr/3792vJkiXq0qWL+vfvr6lTpyolJUUlJSWSpNTUVIWGhmrWrFkKCwtTXFycHnjgASUnJ1dn6ACAS+SonMEDVADg/ByVMwAAzo+cAQCwFzkDAOq2ahXDc3NzFRoaet75Fi1aKDc3t1odKS8v1/Lly3XmzBmZTCZlZ2ertLRUERERRky7du3UvHlzZWVlSZKysrLUsWNHBQUFGTFms1kWi8UojmRlZdncozKm8h5VKS4ulsVisTkAANXjqJzBA1QA4PyuxO8ZAADnRM4AANiLnAEAdVu1iuGBgYH64osvzjv/+eefq1GjRpd0r3379snX11deXl4aNWqU3nvvPbVv3155eXny9PRUQECATXxQUJDy8vIkSXl5eTaF8Mr2yraLxVgsFp09e7bKPiUlJcnf3984QkJCLmlMAIDfODJnVOIBKgBwTlciZwAAnBM5AwBgL3IGANRt1SqGP/LII3r66ae1efNmlZeXq7y8XJs2bdIzzzyjQYMGXdK92rZtq71792rnzp0aPXq0oqOjtX///up0y2ESExNVWFhoHEeOHKnR/gBAbebInMEDVADg3ByZMwAAzo2cAQCwFzkDAOo29+pcNHXqVB0+fFh33HGH3N1/vUVFRYWGDh16yXtseHp6qnXr1pKk8PBw7d69W3PnztXDDz+skpISFRQU2BQ38vPzFRwcLEkKDg7Wrl27bO6Xn59vtFX+s/LcuTF+fn7y8fGpsk9eXl7y8vK6pHEAAKrmyJxR+QBVYWGh/u///k/R0dHaunXrlei23RITE5WQkGC8tlgsFMQBoJocmTMAAM6NnAEAsBc5AwDqtmoVwz09PfXuu+9q6tSp+vzzz+Xj46OOHTuqRYsWl92hiooKFRcXKzw8XB4eHtq4caOioqIkSQcOHFBubq5MJpMkyWQyafr06Tp+/LgCAwMlSZmZmfLz81P79u2NmDVr1ti8R2ZmpnEPAMCV5cicwQNUAODcruTvGQAA50LOAADYi5wBAHVbtYrhldq0aaM2bdpU+/rExET1799fzZs316lTp7Rs2TJt2bJF69atk7+/v2JiYpSQkKCGDRvKz89PTz31lEwmk3r06CFJ6tevn9q3b6/HHntMM2fOVF5eniZMmKDY2FijMDFq1Ci9/vrrGjt2rIYPH65NmzZpxYoVysjIuJyhAwAu0eXmjKrwABUAOKcrkTMAAM6JnAEAsBc5AwDqpmoVw8vLy5Wenq6NGzfq+PHjqqiosGnftGmTXfc5fvy4hg4dqmPHjsnf31+dOnXSunXrdOedd0qSkpOT5erqqqioKBUXF8tsNmvBggXG9W5ublq9erVGjx4tk8mk+vXrKzo6WlOmTDFiQkNDlZGRofj4eM2dO1fNmjXTW2+9JbPZXJ2hAwAukaNyBg9QAYDzc1TOAAA4P3IGAMBe5AwAqNuqVQx/5plnlJ6ersjISN14441ycXGp1psvWrToou3e3t5KSUlRSkrKBWNatGhx3iy+3+vTp4/27NlTrT4CAC6Po3IGD1ABgPNzVM4AADg/cgbgnFqOr97D6IdnRDq4J3Am5AwAqNuqVQxfvny5VqxYoQEDBji6PwAAJ+OonMEDVADg/Pg9AwBgL3IGAMBe5AwAqNtcq3ORp6enWrdu7ei+AACcEDkDAGAvcgYAwF7kDACAvcgZAFC3VasY/uyzz2ru3LmyWq2O7g8AwMmQMwAA9iJnAADsRc4AANiLnAEAdVu1lknftm2bNm/erI8++kgdOnSQh4eHTfuqVasc0jkAQO1HzgAA2IucAQCwFzkDAGAvcgYA1G3VKoYHBATo/vvvd3RfAABOiJwBALAXOQMAYC9yBgDAXuQMAKjbqlUMT0tLc3Q/AABOipwBALAXOQNwTi3HZ1TrusMzIh3cEzgTcgYAwF7kDACo26q1Z7gklZWVacOGDXrjjTd06tQpSdLRo0d1+vRph3UOAOAcyBkAAHuRMwAA9iJnAADsRc4AgLqrWjPDf/jhB911113Kzc1VcXGx7rzzTjVo0ECvvPKKiouLlZqa6uh+AgBqKXIGAMBe5AwAgL3IGQAAe5EzAKBuq9bM8GeeeUbdunXTyZMn5ePjY5y///77tXHjRod1DgBQ+5EzAAD2ImcAAOxFzgAA2IucAQB1W7Vmhn/yySfavn27PD09bc63bNlSP/30k0M6BgBwDuQMAIC9yBkAAHuRMwAA9iJnAEDdVq1ieEVFhcrLy887/+OPP6pBgwaX3SkAgPMgZwAA7EXOAADYi5wBALAXOQNwTi3HZ1TrusMzIh3cE1zrqrVMer9+/TRnzhzjtYuLi06fPq2XXnpJAwYMcFTfAABOgJwBALAXOQMAYC9yBgDAXuQMAKjbqjUzfNasWTKbzWrfvr2Kior06KOP6uDBg7r++uv1r3/9y9F9BADUYuQMAIC9yBkAAHuRMwAA9iJnAEDdVq2Z4c2aNdPnn3+uv//974qPj9dNN92kGTNmaM+ePQoMDHR0HwEAtRg5AwBgL3IGAMBeVyJnzJgxQy4uLhozZoxxrqioSLGxsWrUqJF8fX0VFRWl/Px8m+tyc3MVGRmpevXqKTAwUM8//7zKyspsYrZs2aKuXbvKy8tLrVu3Vnp6erX6CAC4dPyeAQB1W7VmhkuSu7u7hgwZ4si+AACcFDkDAGAvcgYAwF6OzBm7d+/WG2+8oU6dOtmcj4+PV0ZGhlauXCl/f3/FxcVp4MCB+vTTTyVJ5eXlioyMVHBwsLZv365jx45p6NCh8vDw0MsvvyxJOnTokCIjIzVq1CgtXbpUGzdu1IgRI9SkSROZzWaH9B8AcHH8ngEAdVe1iuHvvPPORduHDh1arc4AAJwPOQMAYK8rkTNmzJihxMREPfPMM8Y+gUVFRXr22We1fPlyFRcXy2w2a8GCBQoKCjKuy83N1ejRo7V582b5+voqOjpaSUlJcnf/7VeoLVu2KCEhQTk5OQoJCdGECRM0bNiwS+4jAODSOTJnnD59WoMHD9Y//vEPTZs2zThfWFioRYsWadmyZerbt68kKS0tTWFhYdqxY4d69Oih9evXa//+/dqwYYOCgoLUpUsXTZ06VePGjdOkSZPk6emp1NRUhYaGatasWZKksLAwbdu2TcnJyRTDAeAq4G9TAFC3VasY/swzz9i8Li0t1S+//CJPT0/Vq1eP5AEAMJAzAAD2cnTOYJYfADgvR+aM2NhYRUZGKiIiwqYYnp2drdLSUkVERBjn2rVrp+bNmysrK0s9evRQVlaWOnbsaPNAldls1ujRo5WTk6ObbrpJWVlZNveojDl3OXYAwJXD36YAoG6r1p7hJ0+etDlOnz6tAwcOqGfPnvrXv/7l6D4CAGoxcgYAwF6OzBnnzvK77rrrjPOVs/xmz56tvn37Kjw8XGlpadq+fbt27NghScYsvyVLlqhLly7q37+/pk6dqpSUFJWUlEiSzSy/sLAwxcXF6YEHHlBycrLjPhAAwAU5KmcsX75c//3vf5WUlHReW15enjw9PRUQEGBzPigoSHl5eUbMuYXwyvbKtovFWCwWnT17tsp+FRcXy2Kx2BwAgOrhb1MAULdVqxhelRtuuEEzZsw47ykrAAB+j5wBALBXdXPGubP8zvVHs/wkXXCWn8ViUU5OjhFT1Sy/yntUhcIGAFxZl5ozjhw5omeeeUZLly6Vt7f3Fe7dpUlKSpK/v79xhISE1HSXAMCpXO7fpmbMmCEXFxebVT6KiooUGxurRo0aydfXV1FRUcrPz7e5Ljc3V5GRkapXr54CAwP1/PPPq6yszCZmy5Yt6tq1q7y8vNS6dWulp6dXq48AgF85rBguSe7u7jp69KgjbwkAcFLkDACAvS41Z1yrs/wobADAlXcpOSM7O1vHjx9X165d5e7uLnd3d23dulXz5s2Tu7u7goKCVFJSooKCApvr8vPzFRwcLEkKDg4+r9BR+fqPYvz8/OTj41Nl3xITE1VYWGgcR44csWtMAAD7VfdvUxfbjunDDz/UypUrtXXrVh09elQDBw402iu3YyopKdH27du1ePFipaena+LEiUZM5XZMt99+u/bu3asxY8ZoxIgRWrduXfUHCgB1XLX2DP/ggw9sXlutVh07dkyvv/66br31Vod0DADgHMgZAAB7OSJnVM7yy8zMvOZm+SUmJiohIcF4bbFYKIgDQDU5Imfccccd2rdvn825xx9/XO3atdO4ceMUEhIiDw8Pbdy4UVFRUZKkAwcOKDc3VyaTSZJkMpk0ffp0HT9+XIGBgZKkzMxM+fn5qX379kbMmjVrbN4nMzPTuEdVvLy85OXlZdc4AEgtx2dU+9rDMyId2BNcixz5t6lzt2OaNm2acb5yO6Zly5apb9++kqS0tDSFhYVpx44d6tGjh7Ed04YNGxQUFKQuXbpo6tSpGjdunCZNmiRPT0+b7ZgkKSwsTNu2bVNycrLMZvNlfhIAUDdVqxh+33332bx2cXFR48aN1bdvX+OHNAAAEjkDAGA/R+SMc2f5VSovL9fHH3+s119/XevWrTNm+Z07O/z3s/x27dplc19HzPKjsAEAjuOInNGgQQPdeOONNufq16+vRo0aGedjYmKUkJCghg0bys/PT0899ZRMJpN69OghSerXr5/at2+vxx57TDNnzlReXp4mTJig2NhY42f+qFGj9Prrr2vs2LEaPny4Nm3apBUrVigjo/rFOwCA/Rz5t6lzt2M6txj+R9sx9ejR44LbMY0ePVo5OTm66aabLrgd07nLsf9ecXGxiouLjddsxwQAtqpVDK+oqHB0PwAAToqcAQCwlyNyxrU8yw8A4DhX6/eM5ORkubq6KioqSsXFxTKbzVqwYIHR7ubmptWrV2v06NEymUyqX7++oqOjNWXKFCMmNDRUGRkZio+P19y5c9WsWTO99dZbzPADgKvEUTmjcjum3bt3n9d2tbZjqurB26SkJE2ePLna4wIAZ1etYjgAAAAAXIuY5Qc4D5a8RU3YsmWLzWtvb2+lpKQoJSXlgte0aNHivAekfq9Pnz7as2ePI7oIAKgBbMcEALVXtYrh5/5g/SOzZ8+uzlsAAJwEOQMAYK+rlTOY5QcAtR+/ZwAA7OWInMF2TABQe1WrGL5nzx7t2bNHpaWlatu2rSTpm2++kZubm00ycHFxcUwvAQC1FjkDAGCvK5UzmOUHAM6H3zMAAPZyRM5gOyYAqL2qVQy/55571KBBAy1evFjXXXedJOnkyZN6/PHH1atXLz377LMO7SQAoPYiZwAA7EXOAADYi5wBALCXI3IG2zEBzoPtmOoe1+pcNGvWLCUlJRmJQ5Kuu+46TZs2TbNmzXJY5wAAtR85AwBgL3IGAMBe5AwAgL2uVs5ITk7W3XffraioKPXu3VvBwcFatWqV0V65HZObm5tMJpOGDBmioUOHVrkdU2Zmpjp37qxZs2axHRMAXKZqzQy3WCz63//+d975//3vfzp16tRldwoA4DzIGQAAe5EzAAD2ImcAAOx1pXIG2zEBQO1QrZnh999/vx5//HGtWrVKP/74o3788Uf9+9//VkxMjAYOHOjoPgIAajFyBgDAXuQMAIC9yBkAAHuRMwCgbqvWzPDU1FQ999xzevTRR1VaWvrrjdzdFRMTo1dffdWhHQQA1G7kDACAvcgZAAB7kTMAAPYiZwBA3VatYni9evW0YMECvfrqq/ruu+8kSa1atVL9+vUd2jkAQO1HzgAA2IucAQCwFzkDAGAvcgYA1G3VWia90rFjx3Ts2DHdcMMNql+/vqxWq6P6BQBwMuQMAIC9yBkAAHuRMwAA9iJnAEDdVK1i+M8//6w77rhDbdq00YABA3Ts2DFJUkxMjJ599lmHdhAAULuRMwAA9iJnAADsRc4AANiLnAEAdVu1iuHx8fHy8PBQbm6u6tWrZ5x/+OGHtXbtWod1DgBQ+5EzAAD2ImcAAOxFzgAA2IucAQB1W7X2DF+/fr3WrVunZs2a2Zy/4YYb9MMPPzikYwAA50DOAADYi5wBALAXOQMAYC9yBgDUbdWaGX7mzBmbJ6gqnThxQl5eXpfdKQCA8yBnAADsRc4AANiLnAEAsBc5AwDqtmoVw3v16qV33nnHeO3i4qKKigrNnDlTt99+u8M6BwCo/cgZAAB7kTMAAPYiZwAA7EXOAIC6rVrLpM+cOVN33HGHPvvsM5WUlGjs2LHKycnRiRMn9Omnnzq6jwCAWoycAQCwFzkDAGAvcgYAwF7kDACo26o1M/zGG2/UN998o549e+ree+/VmTNnNHDgQO3Zs0etWrVydB8BALUYOQMAYC9yBgDAXuQMAIC9yBkAULdd8szw0tJS3XXXXUpNTdULL7xwJfoEAHAS5AwAgL3IGQAAe5EzAAD2ImcAAC55ZriHh4e++OKLK9EXAICTIWcAAOxFzgAA2IucAQCwFzkDAFCtZdKHDBmiRYsWObovAAAnRM4AANiLnAEAsBc5AwBgL3IGANRtl7xMuiSVlZXp7bff1oYNGxQeHq769evbtM+ePdshnQMA1H7kDACAvcgZAAB7kTMAAPYiZwBA3XZJxfDvv/9eLVu21JdffqmuXbtKkr755hubGBcXF8f1DgBQa5EzAAD2ImcAcLSW4zOqdd3hGZEO7gkcjZwBALAXOQMAIF1iMfyGG27QsWPHtHnzZknSww8/rHnz5ikoKOiKdA4AUHuRMwAA9iJnAADsRc4AANiLnAEAkC6xGG61Wm1ef/TRRzpz5oxDOwQAcA7kDACAvcgZAAB7kTMAOBqriTgvcgYAQJJcL+fi3ycTAAAuhJwBALAXOQMAYC9yBgDAXuQMAKibLqkY7uLict4eGuypAQCoCjkDAGAvcgYAwF7kDACAvcgZAACpGsukDxs2TF5eXpKkoqIijRo1SvXr17eJW7VqleN6CAColcgZAAB7kTMAAPYiZwAA7EXOAABIl1gMj46Otnk9ZMgQh3YGAOA8yBkAAHuRMwAA9iJnAADsRc4AAEiXWAxPS0u7Uv0AADgZcgYAwF7kDACAvcgZAAB7kTMAOFrL8RnVuu7wjEgH9wSX4pL2DAcAAAAAAAAAAAAAoDao0WJ4UlKSbr75ZjVo0ECBgYG67777dODAAZuYoqIixcbGqlGjRvL19VVUVJTy8/NtYnJzcxUZGal69eopMDBQzz//vMrKymxitmzZoq5du8rLy0utW7dWenr6lR4eAMCByBkAAAAAAAAAAOBS1GgxfOvWrYqNjdWOHTuUmZmp0tJS9evXT2fOnDFi4uPj9eGHH2rlypXaunWrjh49qoEDBxrt5eXlioyMVElJibZv367FixcrPT1dEydONGIOHTqkyMhI3X777dq7d6/GjBmjESNGaN26dVd1vACA6iNnAAAAAAAAAACAS1GjxfC1a9dq2LBh6tChgzp37qz09HTl5uYqOztbklRYWKhFixZp9uzZ6tu3r8LDw5WWlqbt27drx44dkqT169dr//79WrJkibp06aL+/ftr6tSpSklJUUlJiSQpNTVVoaGhmjVrlsLCwhQXF6cHHnhAycnJNTZ2AMClIWcAAAAAcLSFCxeqU6dO8vPzk5+fn0wmkz766COjndWnAAAAgNrtmtozvLCwUJLUsGFDSVJ2drZKS0sVERFhxLRr107NmzdXVlaWJCkrK0sdO3ZUUFCQEWM2m2WxWJSTk2PEnHuPypjKe/xecXGxLBaLzQEAuLaQMwAAF0JhAwBgr2bNmmnGjBnKzs7WZ599pr59++ree+81fj9g9SkAAACgdrtmiuEVFRUaM2aMbr31Vt14442SpLy8PHl6eiogIMAmNigoSHl5eUbMuUWNyvbKtovFWCwWnT179ry+JCUlyd/f3zhCQkIcMkYAgGOQMwAAF0NhAwBgr3vuuUcDBgzQDTfcoDZt2mj69Ony9fXVjh07WH0KAAAAcALXTDE8NjZWX375pZYvX17TXVFiYqIKCwuN48iRIzXdJQDAOcgZAICLobABAKiO8vJyLV++XGfOnJHJZKqx1ackVqACAAAAHOWaKIbHxcVp9erV2rx5s5o1a2acDw4OVklJiQoKCmzi8/PzFRwcbMT8fknDytd/FOPn5ycfH5/z+uPl5WUsqVh5AACuDeQMAMCloLABAPgj+/btk6+vr7y8vDRq1Ci99957at++fY2tPiWxAhUAXIvYjgkAaqcaLYZbrVbFxcXpvffe06ZNmxQaGmrTHh4eLg8PD23cuNE4d+DAAeXm5spkMkmSTCaT9u3bp+PHjxsxmZmZ8vPzU/v27Y2Yc+9RGVN5DwDAtY+cAQC4FBQ2AAD2atu2rfbu3audO3dq9OjRio6O1v79+2u0T6xABQDXHrZjAoDayb0m3zw2NlbLli3Tf/7zHzVo0MD4w5K/v798fHzk7++vmJgYJSQkqGHDhvLz89NTTz0lk8mkHj16SJL69eun9u3b67HHHtPMmTOVl5enCRMmKDY2Vl5eXpKkUaNG6fXXX9fYsWM1fPhwbdq0SStWrFBGRkaNjR0AcGnIGQCAS1FZ2CgsLNT//d//KTo6Wlu3bq3RPiUmJiohIcF4bbFYKIgDwDXA09NTrVu3lvTrQ7a7d+/W3Llz9fDDDxurT537ENXvV5/atWuXzf0ud/Up6dcVqCp/RwEAXBvuuecem9fTp0/XwoULtWPHDjVr1kyLFi3SsmXL1LdvX0lSWlqawsLCtGPHDvXo0cPYjmnDhg0KCgpSly5dNHXqVI0bN06TJk2Sp6enzXZMkhQWFqZt27YpOTlZZrP5qo8ZAJxBjc4MX7hwoQoLC9WnTx81adLEON59910jJjk5WXfffbeioqLUu3dvBQcHa9WqVUa7m5ubVq9eLTc3N5lMJg0ZMkRDhw7VlClTjJjQ0FBlZGQoMzNTnTt31qxZs/TWW2+RPACgFiFnAAAuRWVhIzw8XElJSercubPmzp1bY9tqSGytAQC1RUVFhYqLi1l9CgBwQWzHBAC1R43ODLdarX8Y4+3trZSUFKWkpFwwpkWLFlqzZs1F79OnTx/t2bPnkvsIALg2kDMAAJejqsJGVFSUpKoLG9OnT9fx48cVGBgoqerCxu/zCYUNAKh9EhMT1b9/fzVv3lynTp3SsmXLtGXLFq1bt47VpwAA59m3b59MJpOKiork6+trbMe0d+/eq7IdU1UP3iYlJWny5MmOGiIAOJ0aLYYDAAAAgKNR2AAA2Ov48eMaOnSojh07Jn9/f3Xq1Enr1q3TnXfeKenX1adcXV0VFRWl4uJimc1mLViwwLi+cvWp0aNHy2QyqX79+oqOjq5y9an4+HjNnTtXzZo1Y/UpAKil2I4JAGofiuEAAAAAnAqFDQCAvRYtWnTRdlafAgCcq3I7JkkKDw/X7t27NXfuXD388MPGdkznzg7//XZMu3btsrmfo7ZjqnxoFwBwPorhAAAAAJwKhQ0AAAAAVwPbMQHAtY9iOAAAAAAAAAAAwEWwHRMA1E4UwwEAAAAAAAAAuAQtx1evOHl4RqSDe4Krhe2YAKB2ohgOAAAAAAAAAABwEWzHBAC1k2tNdwAAAAAAAAAAAAAAAEejGA4AAAAAAAAAAAAAcDoskw4AAAAAACD2fwUAAAAAZ8PMcAAAAAAAAAAAAACA02FmOAAAAAAAAAAAAABcAaxAVbOYGQ4AAAAAAAAAAAAAcDoUwwEAAAAAAAAAAAAATodiOAAAAAAAAAAAAADA6VAMBwAAAAAAAAAAAAA4HYrhAAAAAAAAAAAAAACnQzEcAAAAAAAAAAAAAOB0KIYDAAAAAAAAAAAAAJwOxXAAAAAAAAAAAAAAgNOhGA4AAAAAAAAAAAAAcDoUwwEAAAAAAAAAAAAATodiOAAAAAAAAAAAAADA6VAMBwAAAAAAAAAAAAA4HYrhAAAAAAAAAAAAAACnQzEcAAAAAAAAAAAAAOB0KIYDAAAAAAAAAAAAAJwOxXAAAAAAAAAAAAAAgNOhGA4AAAAAAAAAAAAAcDoUwwEAAAAAAAAAAAAATse9pjsAAAAAAAAAAEBd0HJ8RrWvPTwj0oE9AQCgbmBmOAAAAAAAAAAAAADA6VAMBwAAAAAAAAAAAAA4HZZJBwAAAAAAuAwseQsAAAAA1yZmhgMAAAAAAKBOSkpK0s0336wGDRooMDBQ9913nw4cOGATU1RUpNjYWDVq1Ei+vr6KiopSfn6+TUxubq4iIyNVr149BQYG6vnnn1dZWZlNzJYtW9S1a1d5eXmpdevWSk9Pv9LDAwAAAOo8iuEAAAAAnAqFDQCAvbZu3arY2Fjt2LFDmZmZKi0tVb9+/XTmzBkjJj4+Xh9++KFWrlyprVu36ujRoxo4cKDRXl5ersjISJWUlGj79u1avHix0tPTNXHiRCPm0KFDioyM1O233669e/dqzJgxGjFihNatW3dVxwsAAADUNSyTDgAAAMCpVBY2br75ZpWVlenvf/+7+vXrp/3796t+/fqSfi1sZGRkaOXKlfL391dcXJwGDhyoTz/9VNJvhY3g4GBt375dx44d09ChQ+Xh4aGXX35Z0m+FjVGjRmnp0qXauHGjRowYoSZNmshsNtfY+AEA9lu7dq3N6/T0dAUGBio7O1u9e/dWYWGhFi1apGXLlqlv376SpLS0NIWFhWnHjh3q0aOH1q9fr/3792vDhg0KCgpSly5dNHXqVI0bN06TJk2Sp6enUlNTFRoaqlmzZkmSwsLCtG3bNiUnJ5MzAKCWSEpK0qpVq/T111/Lx8dHt9xyi1555RW1bdvWiCkqKtKzzz6r5cuXq7i4WGazWQsWLFBQUJARk5ubq9GjR2vz5s3y9fVVdHS0kpKS5O7+W7lmy5YtSkhIUE5OjkJCQjRhwgQNGzbsag4XwDWA7Zgcg5nhAAAAAJzK2rVrNWzYMHXo0EGdO3dWenq6cnNzlZ2dLUlGYWP27Nnq27evwsPDlZaWpu3bt2vHjh2SZBQ2lixZoi5duqh///6aOnWqUlJSVFJSIkk2hY2wsDDFxcXpgQceUHJyco2NHQBweQoLCyVJDRs2lCRlZ2ertLRUERERRky7du3UvHlzZWVlSZKysrLUsWNHm0KH2WyWxWJRTk6OEXPuPSpjKu/xe8XFxbJYLDYHAKBmsZoIANROFMMBAAAAODUKGwAAe1RUVGjMmDG69dZbdeONN0qS8vLy5OnpqYCAAJvYoKAg5eXlGTHn5ovK9sq2i8VYLBadPXv2vL4kJSXJ39/fOEJCQhwyRgBA9fHQLQDUThTDAQAAADgtChsAAHvFxsbqyy+/1PLly2u6K0pMTFRhYaFxHDlypKa7BAD4HR66BYDagWI4AAAAAKdFYQMAYI+4uDitXr1amzdvVrNmzYzzwcHBKikpUUFBgU18fn6+goODjZj8/Pzz2ivbLhbj5+cnHx+f8/rj5eUlPz8/mwMAcO3goVsAqD0ohgMAAABwShQ2AAB/xGq1Ki4uTu+99542bdqk0NBQm/bw8HB5eHho48aNxrkDBw4oNzdXJpNJkmQymbRv3z4dP37ciMnMzJSfn5/at29vxJx7j8qYynsAAGoXHroFgNqDYjgAAAAAp0JhAwBgr9jYWC1ZskTLli1TgwYNlJeXp7y8PGPmnb+/v2JiYpSQkKDNmzcrOztbjz/+uEwmk3r06CFJ6tevn9q3b6/HHntMn3/+udatW6cJEyYoNjZWXl5ekqRRo0bp+++/19ixY/X1119rwYIFWrFiheLj42ts7ACA6uGhWwCoXSiGAwAAAHAqFDYAAPZauHChCgsL1adPHzVp0sQ43n33XSMmOTlZd999t6KiotS7d28FBwdr1apVRrubm5tWr14tNzc3mUwmDRkyREOHDtWUKVOMmNDQUGVkZCgzM1OdO3fWrFmz9NZbb8lsNl/V8QIAqo+HbgGgdnKv6Q4AAAAAgCMtXLhQktSnTx+b82lpaRo2bJikXwsbrq6uioqKUnFxscxmsxYsWGDEVhY2Ro8eLZPJpPr16ys6OrrKwkZ8fLzmzp2rZs2aUdgAgFrGarX+YYy3t7dSUlKUkpJywZgWLVpozZo1F71Pnz59tGfPnkvuIwDg2hAbG6tly5bpP//5j/HQrfTrw7Y+Pj42D902bNhQfn5+euqppy740O3MmTOVl5dX5UO3r7/+usaOHavhw4dr06ZNWrFihTIyMmps7ABQm1EMBwAAAOBUKGwAAAAAcDQeugWA2oliOAAAAAAAAAAAwEXw0C0A1E7sGQ4AAAAAAAAAAAAAcDoUwwEAAAAAAAAAAAAATodiOAAAAAAAAAAAAADA6VAMBwAAAAAAAAAAAAA4HYrhAAAAAAAAAAAAAACnQzEcAAAAAAAAAAAAAOB0arQY/vHHH+uee+5R06ZN5eLiovfff9+m3Wq1auLEiWrSpIl8fHwUERGhgwcP2sScOHFCgwcPlp+fnwICAhQTE6PTp0/bxHzxxRfq1auXvL29FRISopkzZ17poQEAAAAAAAAAAAAAalCNFsPPnDmjzp07KyUlpcr2mTNnat68eUpNTdXOnTtVv359mc1mFRUVGTGDBw9WTk6OMjMztXr1an388ccaOXKk0W6xWNSvXz+1aNFC2dnZevXVVzVp0iS9+eabV3x8AADH4QEqAAAAAAAAAABwKWq0GN6/f39NmzZN999//3ltVqtVc+bM0YQJE3TvvfeqU6dOeuedd3T06FGjAPLVV19p7dq1euutt9S9e3f17NlT8+fP1/Lly3X06FFJ0tKlS1VSUqK3335bHTp00KBBg/T0009r9uzZV3OoAIDLxANUAAAAAAAAAADgUlyze4YfOnRIeXl5ioiIMM75+/ure/fuysrKkiRlZWUpICBA3bp1M2IiIiLk6uqqnTt3GjG9e/eWp6enEWM2m3XgwAGdPHnyKo0GAHC5eIAKAAAAAAAAAABcimu2GJ6XlydJCgoKsjkfFBRktOXl5SkwMNCm3d3dXQ0bNrSJqeoe577H7xUXF8tisdgcAIBrV00+QEXOAAAAAAAAAADg2uRe0x24FiUlJWny5Mk13Q0AgJ0c+QBVaGjoefeobLvuuuvOe29yBgAAAAAAAADgWtJyfEa1rjs8I9LBPal512wxPDg4WJKUn5+vJk2aGOfz8/PVpUsXI+b48eM215WVlenEiRPG9cHBwcrPz7eJqXxdGfN7iYmJSkhIMF5bLBaFhIRc3oAAAE6JnAEAAIDLwR+pAAAAAODKuWaL4aGhoQoODtbGjRuN4rfFYtHOnTs1evRoSZLJZFJBQYGys7MVHh4uSdq0aZMqKirUvXt3I+aFF15QaWmpPDw8JEmZmZlq27ZtlTP8JMnLy0teXl5XeIQAAEepyQeoyBkAAAAAAOBq4AEqAAAuXY3uGX769Gnt3btXe/fulfTrnq979+5Vbm6uXFxcNGbMGE2bNk0ffPCB9u3bp6FDh6pp06a67777JElhYWG666679MQTT2jXrl369NNPFRcXp0GDBqlp06aSpEcffVSenp6KiYlRTk6O3n33Xc2dO9dmFh8AoHY79wGqSpUPUJlMJkm2D1BVquoBqo8//lilpaVGzB89QAUAAAAAAAAAAK5NNToz/LPPPtPtt99uvK4sUEdHRys9PV1jx47VmTNnNHLkSBUUFKhnz55au3atvL29jWuWLl2quLg43XHHHXJ1dVVUVJTmzZtntPv7+2v9+vWKjY1VeHi4rr/+ek2cOFEjR468egMFAFy206dP69tvvzVeVz5A1bBhQzVv3tx4gOqGG25QaGioXnzxxQs+QJWamqrS0tIqH6CaPHmyYmJiNG7cOH355ZeaO3eukpOTa2LIAAAAAAAAAADgMtRoMbxPnz6yWq0XbHdxcdGUKVM0ZcqUC8Y0bNhQy5Ytu+j7dOrUSZ988km1+wkAqHk8QAUAAAAAAAAAAC7FNbtnOAAA5+IBKgAAAAAAAAAAcClqdM9wAAAAAAAAAAAAAACuBIrhAAAAAAAAAAAAAACnQzEcAAAAAAAAAAAAAOB0KIYDAAAAAAAAAAAAAJwOxXAAAAAAAAAAAAAAgNOhGA4AAAAAAAAAAAAAcDoUwwEAAAAAAAAAAAAATodiOAAAAAAAAAAAAADA6VAMBwAAAAAAAAAAAAA4HYrhAAAAAAAAAAAAAACnQzEcAAAAAAAAddLHH3+se+65R02bNpWLi4vef/99m3ar1aqJEyeqSZMm8vHxUUREhA4ePGgTc+LECQ0ePFh+fn4KCAhQTEyMTp8+bRPzxRdfqFevXvL29lZISIhmzpx5pYcGAAAAQBTDAQAAADgZChsAAHudOXNGnTt3VkpKSpXtM2fO1Lx585SamqqdO3eqfv36MpvNKioqMmIGDx6snJwcZWZmavXq1fr44481cuRIo91isahfv35q0aKFsrOz9eqrr2rSpEl68803r/j4AAAAgLqOYjgAAAAAp0JhAwBgr/79+2vatGm6//77z2uzWq2aM2eOJkyYoHvvvVedOnXSO++8o6NHjxoPWn311Vdau3at3nrrLXXv3l09e/bU/PnztXz5ch09elSStHTpUpWUlOjtt99Whw4dNGjQID399NOaPXv21RwqAOAy8dAtANROFMMBAAAAOBUKGwAARzh06JDy8vIUERFhnPP391f37t2VlZUlScrKylJAQIC6detmxERERMjV1VU7d+40Ynr37i1PT08jxmw268CBAzp58mSV711cXCyLxWJzAABqFg/dAkDtRDEcAAAAQJ1BYQMAYK+8vDxJUlBQkM35oKAgoy0vL0+BgYE27e7u7mrYsKFNTFX3OPc9fi8pKUn+/v7GERIScvkDAgBcFh66BYDaiWI4AAAAgDqDwgYAoDZITExUYWGhcRw5cqSmuwQAuAgeugWAaxfFcAAAAAC4CihsAEDtEhwcrP/P3r3HSVnX/eN/c9rl5IKosHCDSHkCFFEsXA+FSq5K3ppUloqomMENJmCipKlohpmCqCh3qWBfNcL71jIxEDloKqghJKKSBxK9ZaFSWVFZTtfvD39Mrpxml9md3Znn8/GYh841n5l9fy5m57XXvK9DRMSqVasqLV+1alXqseLi4li9enWlxzdu3Bjvv/9+pTHbeo3P/4wvKiwsjKKioko3AOouO90C1F2a4QAAQN7Q2AAgXV26dIni4uKYPXt2all5eXk899xzUVJSEhERJSUl8eGHH8bChQtTY+bMmRObN2+O3r17p8Y89dRTsWHDhtSYWbNmxQEHHBC77757Lc0GgFxlp1uAHdMMBwAA8obGBgCft3bt2li8eHEsXrw4Ij47ze3ixYtjxYoV0aBBgxg+fHj87Gc/i0ceeSSWLFkS55xzTnTo0CFOO+20iIjo2rVrnHjiifGDH/wgnn/++XjmmWdi2LBh8b3vfS86dOgQERFnnnlmFBQUxKBBg2Lp0qXxu9/9LiZMmBAjR47M0qwByDQ73QLUXZrhAABATtHYACBdf/nLX+LQQw+NQw89NCIiRo4cGYceemhcddVVERExatSouOiii+LCCy+Mr3zlK7F27dqYMWNGNG3aNPUa999/fxx44IFx/PHHx8knnxxHH310/OpXv0o93qpVq3j88cdj+fLl0atXr7jkkkviqquuigsvvLB2JwtAjbHTLUDd1TjbBQAAAGTSX/7ylzj22GNT97c0qAcOHBhTpkyJUaNGxccffxwXXnhhfPjhh3H00Udvs7ExbNiwOP7446Nhw4bRv3//uPXWW1OPb2lsDB06NHr16hV77rmnxgZAPdSnT59IkmS7jzdo0CCuvfbauPbaa7c7pk2bNvHAAw/s8Of06NEj/vznP1e7TgCyb+3atfHGG2+k7m/Z6bZNmzax9957p3a63W+//aJLly7x05/+dLs73U6aNCk2bNiwzZ1ux4wZE4MGDYrLLrssXn755ZgwYUKMHz8+G1MGyAma4QAAQE7R2AAAgH/b5/Lp1Xre32/ol+FK6jc73QL5IBczQzMcAAAAoJ7JxS+pAKAus9MtQP3kmuEAAAAAAAAA5BzNcAAAAAAAAAByjmY4AAAAAAAAADlHMxwAAAAAAACAnKMZDgAAAAAAAEDO0QwHAAAAAAAAIOdohgMAAAAAAACQczTDAQAAAAAAAMg5muEAAAAAAAAA5BzNcAAAAAAAAAByjmY4AAAAAAAAADlHMxwAAAAAAACAnKMZDgAAAAAAAEDO0QwHAAAAAAAAIOdohgMAAAAAAACQczTDAQAAAAAAAMg5muEAAAAAAAAA5BzNcAAAAAAAAAByjmY4AAAAAAAAADlHMxwAAAAAAACAnKMZDgAAAAAAAEDO0QwHAAAAAAAAIOc0znYBAAAAANSOfS6fXq3n/f2GfhmuBIC6TmYAkAscGQ4AAAAAAABAznFkOAAAAAAAAADVUpfPJuLIcAAAAAAAAAByjmY4AAAAAAAAADlHMxwAAAAAAACAnKMZDgAAAAAAAEDOyatm+MSJE2OfffaJpk2bRu/eveP555/PdkkA1FEyA4B0yQwA0iUzAEiXzADIjLxphv/ud7+LkSNHxtVXXx0vvvhiHHLIIVFaWhqrV6/OdmkA1DEyA4B0yQwA0iUzAEiXzADInLxpho8bNy5+8IMfxHnnnRfdunWLSZMmRfPmzeOee+7JdmkA1DEyA4B0yQwA0iUzAEiXzADInMbZLqA2rF+/PhYuXBijR49OLWvYsGH07ds35s+fv9X4ioqKqKioSN1fs2ZNRESUl5dX6+dvrvikWs8DqI7qflZteV6SJJksp96RGUA+kRm7RmYA+URm7BqZAeQTmbFrZAaQT2ojM/KiGf7Pf/4zNm3aFO3atau0vF27dvHaa69tNX7s2LExZsyYrZZ36tSpxmoEyJRWt+za8z/66KNo1apVRmqpj2QGkE9kxq6RGUA+kRm7RmYA+URm7BqZAeST2siMvGiGV9Xo0aNj5MiRqfubN2+O999/P/bYY49o0KBBlV6rvLw8OnXqFO+8804UFRVlutR6zbrZMetn+6yb7duVdZMkSXz00UfRoUOHGqouN8mMnTOv+iMX5xRhXjVBZlSPzKgac8wN+TDHiPyYZ3XnKDOqR2ZUjTnmjnyYpzlun8yoHplRNeaYG/JhjhH5Mc/ayIy8aIbvueee0ahRo1i1alWl5atWrYri4uKtxhcWFkZhYWGlZa1bt96lGoqKinL2jbqrrJsds362z7rZvuqum3ze63YLmVFzzKv+yMU5RZhXpskMmVFbzDE35MMcI/JjntWZo8yQGbXFHHNHPszTHLdNZsiM2mKOuSEf5hiRH/OsycxoWJ2C6puCgoLo1atXzJ49O7Vs8+bNMXv27CgpKcliZQDUNTIDgHTJDADSJTMASJfMAMisvDgyPCJi5MiRMXDgwDj88MPjq1/9atxyyy3x8ccfx3nnnZft0gCoY2QGAOmSGQCkS2YAkC6ZAZA5edMMP+OMM+If//hHXHXVVVFWVhY9e/aMGTNmRLt27Wr05xYWFsbVV1+91WlKsG52xvrZPutm+6ybzJAZmWVe9UcuzinCvKhZMqPmmGNuyIc5RuTHPPNhjjVNZtQcc8wd+TBPcyQdMqPmmGNuyIc5RuTHPGtjjg2SJElq7NUBAAAAAAAAIAvy4prhAAAAAAAAAOQXzXAAAAAAAAAAco5mOAAAAAAAAAA5RzMcAAAAAAAAgJyjGZ4BEydOjH322SeaNm0avXv3jueff36H4x988ME48MADo2nTpnHwwQfHY489VkuV1r6qrJspU6ZEgwYNKt2aNm1ai9XWnqeeeipOOeWU6NChQzRo0CB+//vf7/Q58+bNi8MOOywKCwtj3333jSlTptR4ndlS1fUzb968rd47DRo0iLKystopuJaMHTs2vvKVr8Ruu+0Wbdu2jdNOOy2WLVu20+fl02dOrvj73/8egwYNii5dukSzZs3iy1/+clx99dWxfv36bJe2y66//vo48sgjo3nz5tG6detsl1NtVc3++qA62VTXVfdzs6678847o0ePHlFUVBRFRUVRUlISf/rTn7JdFjUgH7YzqjLHX//613HMMcfE7rvvHrvvvnv07du3Xnz+Vjczpk6dGg0aNIjTTjutZgvMgKrO8cMPP4yhQ4dG+/bto7CwMPbff/+ce79GRNxyyy1xwAEHRLNmzaJTp04xYsSIWLduXS1VW3W2U+s3mVGZzKjb8iE3ZMbWZEbdITMqkxl1l7zYmryopoRdMnXq1KSgoCC55557kqVLlyY/+MEPktatWyerVq3a5vhnnnkmadSoUXLjjTcmr7zySnLllVcmTZo0SZYsWVLLlde8qq6byZMnJ0VFRcnKlStTt7KyslquunY89thjyRVXXJE89NBDSUQkDz/88A7Hv/XWW0nz5s2TkSNHJq+88kpy2223JY0aNUpmzJhROwXXsqqun7lz5yYRkSxbtqzS+2fTpk21U3AtKS0tTSZPnpy8/PLLyeLFi5OTTz452XvvvZO1a9du9zn59JmTS/70pz8l5557bjJz5szkzTffTP7whz8kbdu2TS655JJsl7bLrrrqqmTcuHHJyJEjk1atWmW7nGqpar7VF1X97K0PqvO5WR888sgjyfTp05O//e1vybJly5Kf/OQnSZMmTZKXX34526WRQfmwnVHVOZ555pnJxIkTk0WLFiWvvvpqcu655yatWrVK3n333VquPH3VzYzly5cn//Ef/5Ecc8wxyamnnlo7xVZTVedYUVGRHH744cnJJ5+cPP3008ny5cuTefPmJYsXL67lyqumqvO8//77k8LCwuT+++9Pli9fnsycOTNp3759MmLEiFquPH22U+svmbE1mVF35UNuyIytyYy6Q2ZsTWbUTfJia/Ki+jTDd9FXv/rVZOjQoan7mzZtSjp06JCMHTt2m+O/+93vJv369au0rHfv3skPf/jDGq0zG6q6biZPnlxvGyO7Ip0PgFGjRiXdu3evtOyMM85ISktLa7CyuqEqzfAPPvigVmqqK1avXp1ERPLkk09ud0w+febkuhtvvDHp0qVLtsvImPr8mV/VfKuPcqUZ/kXpfG7WV7vvvnty1113ZbsMMigftjN29fN048aNyW677Zbce++9NVXiLqvOHDdu3JgceeSRyV133ZUMHDiwzn9JVdU53nnnncmXvvSlZP369bVVYkZUdZ5Dhw5NjjvuuErLRo4cmRx11FE1Wmem2E6tX2TGzsmMuiMfckNmbE1m1B0yY+dkRt0gL7YmL6rPadJ3wfr162PhwoXRt2/f1LKGDRtG3759Y/78+dt8zvz58yuNj4goLS3d7vj6qjrrJiJi7dq10blz5+jUqVOceuqpsXTp0toot87Ll/fNrurZs2e0b98+vvGNb8QzzzyT7XJq3Jo1ayIiok2bNtsd472TO9asWbPDf2tqR3Xzjbohnc/N+mbTpk0xderU+Pjjj6OkpCTb5ZAh+bCdkYnP008++SQ2bNhQZ3+nqzvHa6+9Ntq2bRuDBg2qjTJ3SXXm+Mgjj0RJSUkMHTo02rVrFwcddFD8/Oc/j02bNtVW2VVWnXkeeeSRsXDhwtRpDt9666147LHH4uSTT66VmmtDffvcyVUyQ2bUl8yIyI/ckBnbVt8+d3KVzJAZ9SUz5IW8+LxMfOY03qVn57l//vOfsWnTpmjXrl2l5e3atYvXXnttm88pKyvb5vhcu7ZxddbNAQccEPfcc0/06NEj1qxZEzfddFMceeSRsXTp0ujYsWNtlF1nbe99U15eHp9++mk0a9YsS5XVDe3bt49JkybF4YcfHhUVFXHXXXdFnz594rnnnovDDjss2+XViM2bN8fw4cPjqKOOioMOOmi74/LlMyfXvfHGG3HbbbfFTTfdlO1S8l518o26Id3PzfpiyZIlUVJSEuvWrYuWLVvGww8/HN26dct2WWRIPmxnZOLz9LLLLosOHTpstaFcV1Rnjk8//XTcfffdsXjx4lqocNdVZ45vvfVWzJkzJ84666x47LHH4o033oj/+q//ig0bNsTVV19dG2VXWXXmeeaZZ8Y///nPOProoyNJkti4cWMMHjw4fvKTn9RGybXCdmrdIDNkRn3JjIj8yA2ZsW0yo26QGTKjvmSGvJAXn5eJvHBkOHVGSUlJnHPOOdGzZ8/4+te/Hg899FDstdde8d///d/ZLo067oADDogf/vCH0atXrzjyyCPjnnvuiSOPPDLGjx+f7dJqzNChQ+Pll1+OqVOnZrsUquDyyy+PBg0a7PD2xT92/u///i9OPPHE+M53vhM/+MEPslT5jlVnXlDbcu1z84ADDojFixfHc889F0OGDImBAwfGK6+8ku2yoNbccMMNMXXq1Hj44YejadOm2S4nIz766KMYMGBA/PrXv44999wz2+XUmM2bN0fbtm3jV7/6VfTq1SvOOOOMuOKKK2LSpEnZLi2j5s2bFz//+c/jjjvuiBdffDEeeuihmD59elx33XXZLg3yjsyo3/IhN2QG1B0yo/6SF+yII8N3wZ577hmNGjWKVatWVVq+atWqKC4u3uZziouLqzS+vqrOuvmiJk2axKGHHhpvvPFGTZRYr2zvfVNUVGTPye346le/Gk8//XS2y6gRw4YNi0cffTSeeuqpnZ41IV8+c+qLSy65JM4999wdjvnSl76U+v/33nsvjj322DjyyCPjV7/6VQ1XV31VnVd9lol8o/ZV5XOzvigoKIh99903IiJ69eoVL7zwQkyYMMFOhDkiH7YzduXz9KabboobbrghnnjiiejRo0dNlrlLqjrHN998M/7+97/HKaecklq2efPmiIho3LhxLFu2LL785S/XbNFVVJ1/x/bt20eTJk2iUaNGqWVdu3aNsrKyWL9+fRQUFNRozdVRnXn+9Kc/jQEDBsQFF1wQEREHH3xwfPzxx3HhhRfGFVdcEQ0b1v9jE2yn1g0yQ2ZsUdczIyI/ckNmbJvMqBtkhszYoq5nhryQF5+Xibyo/2smiwoKCqJXr14xe/bs1LLNmzfH7Nmzt3vNxpKSkkrjIyJmzZqVc9d4rM66+aJNmzbFkiVLon379jVVZr2RL++bTFq8eHHOvXeSJIlhw4bFww8/HHPmzIkuXbrs9DneO3XLXnvtFQceeOAOb1v+MPu///u/6NOnT/Tq1SsmT55cp/+Yqcq86rtM5Bu1pzqfm/XV5s2bo6KiIttlkCH5sJ1R3c/TG2+8Ma677rqYMWNGHH744bVRarVVdY4HHnhgLFmyJBYvXpy6/ed//mcce+yxsXjx4ujUqVNtlp+W6vw7HnXUUfHGG2+kvoCLiPjb3/4W7du3r7N/L1Rnnp988slWf79t+WIuSZKaK7YW1bfPnVwlM2RGfcmMiPzIDZmxbfXtcydXyQyZUV8yQ17Ii8/LyGdOwi6ZOnVqUlhYmEyZMiV55ZVXkgsvvDBp3bp1UlZWliRJkgwYMCC5/PLLU+OfeeaZpHHjxslNN92UvPrqq8nVV1+dNGnSJFmyZEm2plBjqrpuxowZk8ycOTN58803k4ULFybf+973kqZNmyZLly7N1hRqzEcffZQsWrQoWbRoURIRybhx45JFixYlb7/9dpIkSXL55ZcnAwYMSI1/6623kubNmyeXXnpp8uqrryYTJ05MGjVqlMyYMSNbU6hRVV0/48ePT37/+98nr7/+erJkyZLk4osvTho2bJg88cQT2ZpCjRgyZEjSqlWrZN68ecnKlStTt08++SQ1Jp8/c3LJu+++m+y7777J8ccfn7z77ruV/r3ru7fffjtZtGhRMmbMmKRly5ap3/WPPvoo26WlbWf5Vl/t7LO3Pkrnc7M+uvzyy5Mnn3wyWb58efLSSy8ll19+edKgQYPk8ccfz3ZpZFA+bGdUdY433HBDUlBQkPzP//xPpd/pupwhVZ3jFw0cODA59dRTa6na6qnqHFesWJHstttuybBhw5Jly5Yljz76aNK2bdvkZz/7WbamkJaqzvPqq69Odtttt+S3v/1t8tZbbyWPP/548uUvfzn57ne/m60p7JTt1PpLZsiMJKkfmZEk+ZEbMkNm1GUyQ2YkSf3IDHkhLzKZF5rhGXDbbbcle++9d1JQUJB89atfTRYsWJB67Otf/3oycODASuOnTZuW7L///klBQUHSvXv3ZPr06bVcce2pyroZPnx4amy7du2Sk08+OXnxxRezUHXNmzt3bhIRW922rI+BAwcmX//617d6Ts+ePZOCgoLkS1/6UjJ58uRar7u2VHX9/OIXv0i+/OUvJ02bNk3atGmT9OnTJ5kzZ052iq9B21onEVHpvZDvnzm5YvLkydv9967vBg4cuM15zZ07N9ulVcmO8q2+2tlnb32UzudmfXT++ecnnTt3TgoKCpK99torOf744zXCc1Q+bGdUZY6dO3fe5u/01VdfXfuFV0FV/x0/rz58SZUkVZ/js88+m/Tu3TspLCxMvvSlLyXXX399snHjxlquuuqqMs8NGzYk11xzTWo7pVOnTsl//dd/JR988EHtF54m26n1m8yQGfUlM5IkP3JDZsiMukxmyIz6khnyQl5kKi8aJEmOHDsPAAAAAAAAAP+/unsBUgAAAAAAAACoJs1wAAAAAAAAAHKOZjgAAAAAAAAAOUczHAAAAAAAAICcoxkOAAAAAAAAQM7RDAcAAAAAAAAg52iGAwAAAAAAAJBzNMMBAAAAAAAAyDma4ZBl8+bNiwYNGsSHH36Y7VIAqMe+mCdTpkyJ1q1bZ7UmAAAAAIBs0gwHAMgBRx55ZKxcuTJatWqV7VIAAAAAAOqExtkuAPLd+vXrs10CADmgoKAgiouLs10GAAAAAECd4chwqGV9+vSJYcOGxfDhw2PPPfeM0tLSiIhYuHBhHH744dG8efM48sgjY9myZZWed+edd8aXv/zlKCgoiAMOOCD+3//7f9koH4BdsHnz5hg7dmx06dIlmjVrFoccckj8z//8T0T8+zTn06dPjx49ekTTpk3jiCOOiJdffjn1/LfffjtOOeWU2H333aNFixbRvXv3eOyxxyo9f0eX3dhZljRo0CDuuuuu+Na3vhXNmzeP/fbbLx555JHMrwgAatSvfvWr6NChQ2zevLnS8lNPPTXOO++86Nu3b5SWlkaSJBER8f7770fHjh3jqquuyka5AGTRjjLjuOOOi4YNG8Zf/vKXSo/dcsst0blz562eA0Bu21FmnH/++bHPPvtEgwYNtrpBtmmGQxbce++9UVBQEM8880xMmjQpIiKuuOKKuPnmm+Mvf/lLNG7cOM4///zU+IcffjguvvjiuOSSS+Lll1+OH/7wh3HeeefF3LlzszUFAKph7Nix8Zvf/CYmTZoUS5cujREjRsTZZ58dTz75ZGrMpZdeGjfffHO88MILsddee8Upp5wSGzZsiIiIoUOHRkVFRTz11FOxZMmS+MUvfhEtW7ZM62enmyVjxoyJ7373u/HSSy/FySefHGeddVa8//77mVsJANS473znO/Gvf/2r0mf8+++/HzNmzIizzz477r333njhhRfi1ltvjYiIwYMHx3/8x39ohgPkoR1lxhVXXBF9+/aNyZMnV3rO5MmT49xzz42GDX21DJBPdpQZZ511VrzwwguxcuXKWLlyZbz77rtxxBFHxDHHHJPFiuEzDZItu4IDtaJPnz5RXl4eL774YkR8diTfscceG0888UQcf/zxERHx2GOPRb9+/eLTTz+Npk2bxlFHHRXdu3ePX/3qV6nX+e53vxsff/xxTJ8+PSvzAKBqKioqok2bNvHEE09ESUlJavkFF1wQn3zySVx44YVx7LHHxtSpU+OMM86IiH8fqTdlypT47ne/Gz169Ij+/fvH1VdfvdXrb8mTDz74IFq3bh1TpkyJ4cOHp44UTydLGjRoEFdeeWVcd911ERHx8ccfR8uWLeNPf/pTnHjiiTW1agCoAaeddlrssccecffdd0fEZ0dxjBkzJt55551o2LBhPPjgg3HOOefE8OHD47bbbotFixbFfvvtl+WqAciGHWXG//zP/8TgwYNj5cqVUVhYGC+++GIcfvjh8dZbb8U+++yT3cIBqHU7287Y4uKLL44//OEPqYM9IJvsvgdZ0KtXr62W9ejRI/X/7du3j4iI1atXR0TEq6++GkcddVSl8UcddVS8+uqrNVglAJn0xhtvxCeffBLf+MY3omXLlqnbb37zm3jzzTdT4z7fKG/Tpk0ccMABqc/7H/3oR/Gzn/0sjjrqqLj66qvjpZdeSvvnp5sln8+jFi1aRFFRUSqPAKg/zjrrrPjf//3fqKioiIiI+++/P773ve+lvqD6zne+E9/61rfihhtuiJtuukkjHCCP7SgzTjvttGjUqFE8/PDDERExZcqUOPbYYzXCAfLUzrYzIj5rkN99993xyCOPaIRTJ2iGQxa0aNFiq2VNmjRJ/f+W62i49hJA7li7dm1EREyfPj0WL16cur3yyiup64bvzAUXXBBvvfVWDBgwIJYsWRKHH3543HbbbRmt8/N5FPFZJskjgPrnlFNOiSRJYvr06fHOO+/En//85zjrrLNSj3/yySexcOHCaNSoUbz++utZrBSAbNtRZhQUFMQ555wTkydPjvXr18cDDzxQ6dJ+AOSXnW1nzJ07Ny666KL4zW9+U+mAC8gmzXCoB7p27RrPPPNMpWXPPPNMdOvWLUsVAVBV3bp1i8LCwlixYkXsu+++lW6dOnVKjVuwYEHq/z/44IP429/+Fl27dk0t69SpUwwePDgeeuihuOSSS+LXv/51Wj9flgDkl6ZNm8bpp58e999/f/z2t7+NAw44IA477LDU45dcckk0bNgw/vSnP8Wtt94ac+bMyWK1AGTTzjLjggsuiCeeeCLuuOOO2LhxY5x++ulZrBaAbNpRZrzxxhvx7W9/O37yk5/ICuqUxtkuANi5Sy+9NL773e/GoYceGn379o0//vGP8dBDD8UTTzyR7dIASNNuu+0WP/7xj2PEiBGxefPmOProo2PNmjXxzDPPRFFRUXTu3DkiIq699trYY489ol27dnHFFVfEnnvuGaeddlpERAwfPjxOOumk2H///eODDz6IuXPnVmqU74gsAcg/Z511Vnzzm9+MpUuXxtlnn51aPn369Ljnnnti/vz5cdhhh8Wll14aAwcOjJdeeil23333LFYMQLZsLzMiPtux9ogjjojLLrsszj///GjWrFmWqgSgLthWZnz66adxyimnxKGHHhoXXnhhlJWVpcYXFxdnq1SICM1wqBdOO+20mDBhQtx0001x8cUXR5cuXWLy5MnRp0+fbJcGQBVcd911sddee8XYsWPjrbfeitatW8dhhx0WP/nJT1KnIr/hhhvi4osvjtdffz169uwZf/zjH6OgoCAiIjZt2hRDhw6Nd999N4qKiuLEE0+M8ePHp/WzZQlA/jnuuOOiTZs2sWzZsjjzzDMjIuIf//hHDBo0KK655prUERxjxoyJxx9/PAYPHhy/+93vslkyAFmyrcz4vEGDBsWzzz7rFOkAbDMzVq1aFa+99lq89tpr0aFDh0rjkyTJRpmQ0iDxLgQAyLp58+bFscceGx988EG0bt062+UAAACkXHfddfHggw/GSy+9lO1SAACqxDXDAQAAAADYytq1a+Pll1+O22+/PS666KJslwMAUGWa4QAAAAAAbGXYsGHRq1ev6NOnj1OkAwD1ktOkAwAAAAAAAJBzHBkOAAAAAAAAQM7RDAcAAAAAAAAg52iGAwAAAAAAAJBzNMMBAAAAAAAAyDma4QAAAAAAAADkHM1wAAAAAAAAAHKOZjgAAAAAAAAAOUczHAAAAAAAAICcoxkOAAAAAAAAQM7RDAcAAAAAAAAg52iGAwAAAAAAAJBzNMMBAAAAAAAAyDmNs11AfbB58+Z47733YrfddosGDRpkuxyAGpEkSXz00UfRoUOHaNjQvlLVJTOAfCAzMkNmAPlAZmSGzADygczIDJkB5IOqZIZmeBree++96NSpU7bLAKgV77zzTnTs2DHbZdRbMgPIJzJj18gMIJ/IjF0jM4B8IjN2jcwA8kk6maEZnobddtstIj5boUVFRVmuBqBmlJeXR6dOnVKfeVSPzADygczIDJkB5AOZkRkyA8gHMiMzZAaQD6qSGZrhadhyKpGioiLhAeQ8p0/aNTIDyCcyY9fIDCCfyIxdIzOAfCIzdo3MAPJJOpnhwhsAAAAAAAAA5BzNcAAAAAAAAAByjmY4AAAAAAAAADlHMxwAAAAAAACAnKMZDgAAAAAAAEDO0QwHAAAAAAAAIOdohgMAAAAAAACQczTDAQAAAAAAAMg5muEAAAAAAAAA5BzNcAAAAAAAAAByjmY4AAAAAAAAADlHMxwAAAAAAACAnKMZDgAAAAAAAEDO0QwHAAAAAAAAIOc0znYBALVpn8un1/rP/PsN/Wr9ZwL/tiu/935/Aahp1c0pGVW3ZOPvjdretvGeq9981gCQLpkB5JqsNsOvueaaGDNmTKVlBxxwQLz22msREbFu3bq45JJLYurUqVFRURGlpaVxxx13RLt27VLjV6xYEUOGDIm5c+dGy5YtY+DAgTF27Nho3PjfU5s3b16MHDkyli5dGp06dYorr7wyzj333FqZY4TwAACA2pQv2xnA9tkOBwAAIKIOHBnevXv3eOKJJ1L3P//l0ogRI2L69Onx4IMPRqtWrWLYsGFx+umnxzPPPBMREZs2bYp+/fpFcXFxPPvss7Fy5co455xzokmTJvHzn/88IiKWL18e/fr1i8GDB8f9998fs2fPjgsuuCDat28fpaWltTvZKrLxvn2O8qM+8bsM9ZffX6i/bGdA5slFAAAA6pusN8MbN24cxcXFWy1fs2ZN3H333fHAAw/EcccdFxERkydPjq5du8aCBQviiCOOiMcffzxeeeWVeOKJJ6Jdu3bRs2fPuO666+Kyyy6La665JgoKCmLSpEnRpUuXuPnmmyMiomvXrvH000/H+PHjfUkFAAA5ynYGuc4OwjUjG5dVAgAAoOZkvRn++uuvR4cOHaJp06ZRUlISY8eOjb333jsWLlwYGzZsiL59+6bGHnjggbH33nvH/Pnz44gjjoj58+fHwQcfXOl0hqWlpTFkyJBYunRpHHrooTF//vxKr7FlzPDhw7dbU0VFRVRUVKTul5eXZ27CUAWOvID84/ce8pvPgMzJh+0M1wumvvDZBvnHDjsAANQVWW2G9+7dO6ZMmRIHHHBArFy5MsaMGRPHHHNMvPzyy1FWVhYFBQXRunXrSs9p165dlJWVRUREWVlZpS+otjy+5bEdjSkvL49PP/00mjVrtlVdY8eO3eoag9QMe93jizHS5fqvAKTLdgYAAAAAEVluhp900kmp/+/Ro0f07t07OnfuHNOmTdvml0e1ZfTo0TFy5MjU/fLy8ujUqVPW6qkP6lNTW/N1++rTuqlP7zkyx/VfAUiH7YyakY2j/OrT36cA1E+yBgAgt2X9NOmf17p169h///3jjTfeiG984xuxfv36+PDDDysdtbFq1arUtf+Ki4vj+eefr/Qaq1atSj225b9bln1+TFFR0Xa/CCssLIzCwsJMTYscUZ82jjSKyVWu/wpURzZy0ZejdYvtDKjM9gIAAAD5ok41w9euXRtvvvlmDBgwIHr16hVNmjSJ2bNnR//+/SMiYtmyZbFixYooKSmJiIiSkpK4/vrrY/Xq1dG2bduIiJg1a1YUFRVFt27dUmMee+yxSj9n1qxZqdeAmuaLJsicfLj+KwCZZzsj/9SnHVnZPttSQFX4zAAAYFuy2gz/8Y9/HKecckp07tw53nvvvbj66qujUaNG8f3vfz9atWoVgwYNipEjR0abNm2iqKgoLrrooigpKYkjjjgiIiJOOOGE6NatWwwYMCBuvPHGKCsriyuvvDKGDh2aOuJi8ODBcfvtt8eoUaPi/PPPjzlz5sS0adNi+vTc/QM5G6cvJL/Z4KQ2uP5r7siHBkU+zDEb6lPe1Kdac5HtjLonH34n8mGO1AzvHQAgn+lnADUtq83wd999N77//e/Hv/71r9hrr73i6KOPjgULFsRee+0VERHjx4+Phg0bRv/+/aOioiJKS0vjjjvuSD2/UaNG8eijj8aQIUOipKQkWrRoEQMHDoxrr702NaZLly4xffr0GDFiREyYMCE6duwYd911l9PdboeNcKCucv3XmmGDA4373ODfsTLbGQAAALVLbwGoq7LaDJ86deoOH2/atGlMnDgxJk6cuN0xnTt33ur0hF/Up0+fWLRoUbVqBKBucv1XqJ9sHFMbbGcAQGb42w0AgPquTl0zHADS5fqvUJkvKgEAAAAAKtMMB6BecP1XAAAg06655poYM2ZMpWUHHHBAvPbaaxERsW7durjkkkti6tSplS6t0a5du9T4FStWxJAhQ2Lu3LnRsmXLGDhwYIwdOzYaN/73127z5s2LkSNHxtKlS6NTp05x5ZVXxrnnnlsrc4T6xOV/gNqQD581+TBHSJdmOEAd5Q+Wylz/lXw48jkf5ghQl/kchvzUvXv3eOKJJ1L3P9/EHjFiREyfPj0efPDBaNWqVQwbNixOP/30eOaZZyIiYtOmTdGvX78oLi6OZ599NlauXBnnnHNONGnSJH7+859HRMTy5cujX79+MXjw4Lj//vtj9uzZccEFF0T79u1ta2yDz2KgrrIDFdW1K9mWq9/1Qm3SDAegXnD917rHl1QAAOSCxo0bR3Fx8VbL16xZE3fffXc88MADcdxxx0VExOTJk6Nr166xYMGCOOKII+Lxxx+PV155JZ544olo165d9OzZM6677rq47LLL4pprromCgoKYNGlSdOnSJW6++eaIiOjatWs8/fTTMX78eM1wcpbtRXKVHajym882qJ80wwEAqDIbgABArnj99dejQ4cO0bRp0ygpKYmxY8fG3nvvHQsXLowNGzZE3759U2MPPPDA2HvvvWP+/PlxxBFHxPz58+Pggw+udNRfaWlpDBkyJJYuXRqHHnpozJ8/v9JrbBkzfPjw7dZUUVERFRUVqfvl5eWZmzBZV9t/S+/KUYX+7ofK7EAFUP9ohgNADvAFBQAAVF3v3r1jypQpccABB8TKlStjzJgxccwxx8TLL78cZWVlUVBQEK1bt670nHbt2kVZWVlERJSVlVVqhG95fMtjOxpTXl4en376aTRr1myrusaOHbvVqXipW2yDQX6yAxVknsuFUtMaZrsAAAAAAMiGk046Kb7zne9Ejx49orS0NB577LH48MMPY9q0aVmta/To0bFmzZrU7Z133slqPQD8eweqGTNmxJ133hnLly+PY445Jj766KNa24FqW8aOHRutWrVK3Tp16pSJ6QLkDEeGAwAAAEBEtG7dOvbff/9444034hvf+EasX78+Pvzww0rNjVWrVqVOkVtcXBzPP/98pddYtWpV6rEt/92y7PNjioqKtnlUeEREYWFhFBYWZmpa5Ll8OIp9V+boyELSddJJJ6X+v0ePHtG7d+/o3LlzTJs2bbuf57Vh9OjRMXLkyNT98vLyvGmI+3yrWz/P52nd4oj7f3NkOAAAAABExNq1a+PNN9+M9u3bR69evaJJkyYxe/bs1OPLli2LFStWRElJSURElJSUxJIlS2L16tWpMbNmzYqioqLo1q1basznX2PLmC2vAUD99PkdqIqLi1M7UH3eF3eg2tbOUVse29GYne1AVVRUVOkGwL85MhwAAACAvPTjH/84TjnllOjcuXO89957cfXVV0ejRo3i+9//frRq1SoGDRoUI0eOjDZt2kRRUVFcdNFFUVJSEkcccURERJxwwgnRrVu3GDBgQNx4441RVlYWV155ZQwdOjR1ZPfgwYPj9ttvj1GjRsX5558fc+bMiWnTpsX06bl/NBvUB46co7q27EA1YMCASjtQ9e/fPyK2vQPV9ddfH6tXr462bdtGxLZ3oHrssccq/Rw7UEHdITPqJ81wAAAAAPLSu+++G9///vfjX//6V+y1115x9NFHx4IFC2KvvfaKiIjx48dHw4YNo3///lFRURGlpaVxxx13pJ7fqFGjePTRR2PIkCFRUlISLVq0iIEDB8a1116bGtOlS5eYPn16jBgxIiZMmBAdO3aMu+66K0pLS2t9vgBUnx2oAOonzXAAAAAA8tLUqVN3+HjTpk1j4sSJMXHixO2O6dy581ZH8X1Rnz59YtGiRdWqEYC6wQ5UQD7IxaPfNcMBAAAAAKCOy8UGRX1iByrYsep+RtW2+lInmaMZDgAAAAAAtUATBiD/2JkpuzTDAQAAAAAAAOoQO1BlhmY4AAAAAAAAUG9oFJMuzXAAAAAAAKgCTRgAqB8aZrsAAAAAAAAAAMg0R4YDAAAAAAAAUC3VPWPK32/ol+FKtubIcAAAAAAAAAByjmY4AAAAAAAAADlHMxwAAAAAAACAnOOa4QAAAAAAQCV1+fqvAJAuR4YDAAAAAAAAkHM0wwEAAAAAAADIOU6TDgAAAAAAOaq6pzsHgFzgyHAAAAAAAAAAco5mOAAAAAAAAAA5RzMcAAAAAAAAgJyjGQ4AAAAAAABAztEMBwAAAAAAACDnaIYDAAAAAAAAkHM0wwEAAAAAAADIOZrhAAAAAAAAAOQczXAAAAAAAAAAco5mOAAAAAAAAAA5RzMcAAAAAAAAgJyjGQ4AAAAAAABAztEMBwAAAAAAACDnaIYDAAAAAAAAkHM0wwEAAAAAAADIOZrhAAAAAAAAAOQczXAAAAAAAAAAco5mOAAAAAAAAAA5RzMcAAAAAAAAgJyjGQ4AAAAAAABAztEMBwAAAAAAACDnaIYDAAAAAAAAkHM0wwEAAAAAAADIOZrhAAAAAAAAAOQczXAAAAAAAAAAco5mOAAAAAAAAAA5RzMcAAAAAAAAgJxTZ5rhN9xwQzRo0CCGDx+eWrZu3boYOnRo7LHHHtGyZcvo379/rFq1qtLzVqxYEf369YvmzZtH27Zt49JLL42NGzdWGjNv3rw47LDDorCwMPbdd9+YMmVKLcwIAADINtsZAAAAAPmrTjTDX3jhhfjv//7v6NGjR6XlI0aMiD/+8Y/x4IMPxpNPPhnvvfdenH766anHN23aFP369Yv169fHs88+G/fee29MmTIlrrrqqtSY5cuXR79+/eLYY4+NxYsXx/Dhw+OCCy6ImTNn1tr8AACA2mc7AwAAACC/Zb0Zvnbt2jjrrLPi17/+dey+++6p5WvWrIm77747xo0bF8cdd1z06tUrJk+eHM8++2wsWLAgIiIef/zxeOWVV+K+++6Lnj17xkknnRTXXXddTJw4MdavXx8REZMmTYouXbrEzTffHF27do1hw4bFt7/97Rg/fnxW5gsAANQ82xkAAAAAZL0ZPnTo0OjXr1/07du30vKFCxfGhg0bKi0/8MADY++994758+dHRMT8+fPj4IMPjnbt2qXGlJaWRnl5eSxdujQ15ouvXVpamnqNbamoqIjy8vJKNwDqDqe8BWBnbGcAAAAAkNVm+NSpU+PFF1+MsWPHbvVYWVlZFBQUROvWrSstb9euXZSVlaXGfP4Lqi2Pb3lsR2PKy8vj008/3WZdY8eOjVatWqVunTp1qtb8AMg8p7wFYGdsZwAAAAAQkcVm+DvvvBMXX3xx3H///dG0adNslbFNo0ePjjVr1qRu77zzTrZLAiCc8haAnbOdAQAAAMAWWWuGL1y4MFavXh2HHXZYNG7cOBo3bhxPPvlk3HrrrdG4ceNo165drF+/Pj788MNKz1u1alUUFxdHRERxcfFWp8Ddcn9nY4qKiqJZs2bbrK2wsDCKiooq3QDIPqe8BWBnbGcAAAAAsEXWmuHHH398LFmyJBYvXpy6HX744XHWWWel/r9JkyYxe/bs1HOWLVsWK1asiJKSkoiIKCkpiSVLlsTq1atTY2bNmhVFRUXRrVu31JjPv8aWMVteA4D6wSlvAUiH7QwAAAAAtmicrR+82267xUEHHVRpWYsWLWKPPfZILR80aFCMHDky2rRpE0VFRXHRRRdFSUlJHHHEERERccIJJ0S3bt1iwIABceONN0ZZWVlceeWVMXTo0CgsLIyIiMGDB8ftt98eo0aNivPPPz/mzJkT06ZNi+nTp9fuhAGoti2nvJ01a1adPOXtyJEjU/fLy8s1xAGyyHYGAAAAAFtk7cjwdIwfPz6++c1vRv/+/eNrX/taFBcXx0MPPZR6vFGjRvHoo49Go0aNoqSkJM4+++w455xz4tprr02N6dKlS0yfPj1mzZoVhxxySNx8881x1113RWlpaTamBEA1OOUtAJlkOwOAbbnhhhuiQYMGMXz48NSydevWxdChQ2OPPfaIli1bRv/+/bfaZlixYkX069cvmjdvHm3bto1LL700Nm7cWGnMvHnz4rDDDovCwsLYd999Y8qUKbUwIwAAoE41w+fNmxe33HJL6n7Tpk1j4sSJ8f7778fHH38cDz30UKphsUXnzp3jsccei08++ST+8Y9/xE033RSNG1c+4L1Pnz6xaNGiqKioiDfffDPOPffcWpgNAJnilLcA7ArbGQDszAsvvBD//d//HT169Ki0fMSIEfHHP/4xHnzwwXjyySfjvffei9NPPz31+KZNm6Jfv36xfv36ePbZZ+Pee++NKVOmxFVXXZUas3z58ujXr18ce+yxsXjx4hg+fHhccMEFMXPmzFqbHwCZZQcqgPqjTjXDAWBbtpzy9vO3z5/ytlWrVqlT3s6dOzcWLlwY55133nZPefvXv/41Zs6cuc1T3r711lsxatSoeO211+KOO+6IadOmxYgRI7I5fQAAoAatXbs2zjrrrPj1r38du+++e2r5mjVr4u67745x48bFcccdF7169YrJkyfHs88+GwsWLIiIiMcffzxeeeWVuO+++6Jnz55x0kknxXXXXRcTJ06M9evXR0TEpEmTokuXLnHzzTdH165dY9iwYfHtb387xo8fn5X5ArBr7EAFUL9ohgOQE5zyFgAAqI6hQ4dGv379om/fvpWWL1y4MDZs2FBp+YEHHhh77713zJ8/PyIi5s+fHwcffHC0a9cuNaa0tDTKy8tj6dKlqTFffO3S0tLUa2xLRUVFlJeXV7oBkH12oAKofxrvfAgA1D3z5s2rdH/LKW8nTpy43edsOeXtjmw55S0AAJD7pk6dGi+++GK88MILWz1WVlYWBQUF0bp160rL27VrF2VlZakxn2+Eb3l8y2M7GlNeXh6ffvppNGvWbKufPXbs2BgzZky15wVAzfj8DlQ/+9nPUst3tgPVEUccsd0dqIYMGRJLly6NQw89dLs7UH3+dOxfVFFRERUVFan7dqACqMyR4QAAAADknXfeeScuvvjiuP/++6Np06bZLqeS0aNHx5o1a1K3d955J9slAeS9LTtQjR07dqvHamsHqm0ZO3ZstGrVKnXr1KlTteYHkKs0wwEAAADIOwsXLozVq1fHYYcdFo0bN47GjRvHk08+Gbfeems0btw42rVrF+vXr48PP/yw0vNWrVoVxcXFERFRXFwcq1at2urxLY/taExRUdE2jwqPiCgsLIyioqJKNwCyxw5UAPWXZjgAAAAAeef444+PJUuWxOLFi1O3ww8/PM4666zU/zdp0iRmz56des6yZctixYoVUVJSEhERJSUlsWTJkli9enVqzKxZs6KoqCi6deuWGvP519gyZstrAFD32YEKoP5yzXAAAAAA8s5uu+0WBx10UKVlLVq0iD322CO1fNCgQTFy5Mho06ZNFBUVxUUXXRQlJSVxxBFHRETECSecEN26dYsBAwbEjTfeGGVlZXHllVfG0KFDo7CwMCIiBg8eHLfffnuMGjUqzj///JgzZ05MmzYtpk+fXrsTBqDatuxA9XnnnXdeHHjggXHZZZdFp06dUjtQ9e/fPyK2vQPV9ddfH6tXr462bdtGxLZ3oHrssccq/Rw7UAHsGs1wAAAAANiG8ePHR8OGDaN///5RUVERpaWlcccdd6Qeb9SoUTz66KMxZMiQKCkpiRYtWsTAgQPj2muvTY3p0qVLTJ8+PUaMGBETJkyIjh07xl133RWlpaXZmBIA1WAHKoD6SzMcAAAAACJi3rx5le43bdo0Jk6cGBMnTtzuczp37rzVUXxf1KdPn1i0aFEmSgSgjrIDFUDdpBkOAAAAAABQBXagAqgfGma7AAAAAAAAAADINM1wAAAAAAAAAHKOZjgAAAAAAAAAOUczHAAAAAAAAICcoxkOAAAAAAAAQM7RDAcAAAAAAAAg52iGAwAAAAAAAJBzNMMBAAAAAAAAyDma4QAAAAAAAADkHM1wAAAAAAAAAHKOZjgAAAAAAAAAOUczHAAAAAAAAICcoxkOAAAAAAAAQM7RDAcAAAAAAAAg52iGAwAAAAAAAJBzNMMBAAAAAAAAyDma4QAAAAAAAADkHM1wAAAAAAAAAHKOZjgAAAAAAAAAOUczHAAAAAAAAICcoxkOAAAAAAAAQM7RDAcAAAAAAAAg52iGAwAAAAAAAJBzNMMBAAAAAAAAyDma4QAAAAAAAADkHM1wAAAAAAAAAHKOZjgAAAAAAAAAOUczHAAAAAAAAICcoxkOAAAAAAAAQM7RDAcAAAAAAAAg52iGAwAAAAAAAJBzNMMBAAAAAAAAyDma4QAAAAAAAADkHM1wAAAAAAAAAHKOZjgAAAAAAAAAOadazfC33nor03UAkKNkBgDpkhkApEtmAJAumQGQ36rVDN93333j2GOPjfvuuy/WrVuX6ZoAyCEyA4B0yQwA0iUzAEiXzADIb9Vqhr/44ovRo0ePGDlyZBQXF8cPf/jDeP755zNdGwA5QGYAkC6ZAUC6ZAYA6ZIZAPmtWs3wnj17xoQJE+K9996Le+65J1auXBlHH310HHTQQTFu3Lj4xz/+kek6AainZAYA6ZIZAKRLZgCQLpkBkN+q1QzfonHjxnH66afHgw8+GL/4xS/ijTfeiB//+MfRqVOnOOecc2LlypWZqhOAek5mAJAumQFAumQGAOmSGQD5aZea4X/5y1/iv/7rv6J9+/Yxbty4+PGPfxxvvvlmzJo1K95777049dRTM1UnAPWczAAgXTIDgHTJDADSJTMA8lPj6jxp3LhxMXny5Fi2bFmcfPLJ8Zvf/CZOPvnkaNjws956ly5dYsqUKbHPPvtkslYA6iGZAUC6ZAYA6ZIZAKRLZgDkt2odGX7nnXfGmWeeGW+//Xb8/ve/j29+85up4Niibdu2cffdd2ekSADqL5kBQLoylRl33nln9OjRI4qKiqKoqChKSkriT3/6U+rxdevWxdChQ2OPPfaIli1bRv/+/WPVqlWVXmPFihXRr1+/aN68ebRt2zYuvfTS2LhxY6Ux8+bNi8MOOywKCwtj3333jSlTpuzaCgAgbbYzAEiXzADIb9Vqhr/++usxevToaN++/XbHFBQUxMCBA3f4Or6kAsh9mcoMAHJfpjKjY8eOccMNN8TChQvjL3/5Sxx33HFx6qmnxtKlSyMiYsSIEfHHP/4xHnzwwXjyySfjvffei9NPPz31/E2bNkW/fv1i/fr18eyzz8a9994bU6ZMiauuuio1Zvny5dGvX7849thjY/HixTF8+PC44IILYubMmbu4FgBIh+0MANIlMwDyW7Wa4ZMnT44HH3xwq+UPPvhg3HvvvWm/ji+pAHJfpjIDgNyXqcw45ZRT4uSTT4799tsv9t9//7j++uujZcuWsWDBglizZk3cfffdMW7cuDjuuOOiV69eMXny5Hj22WdjwYIFERHx+OOPxyuvvBL33Xdf9OzZM0466aS47rrrYuLEibF+/fqIiJg0aVJ06dIlbr755ujatWsMGzYsvv3tb8f48eMzszIA2CHbGQCkS2YA5LdqNcPHjh0be+6551bL27ZtGz//+c/Tfh1fUgHkvkxlhrOJAOS+TGXG523atCmmTp0aH3/8cZSUlMTChQtjw4YN0bdv39SYAw88MPbee++YP39+RETMnz8/Dj744GjXrl1qTGlpaZSXl6d23J0/f36l19gyZstrbEtFRUWUl5dXugFQPTWRGQDkJpkBkN+q1QxfsWJFdOnSZavlnTt3jhUrVlSrEF9SAeSmTGWGs4kA5L5MbmcsWbIkWrZsGYWFhTF48OB4+OGHo1u3blFWVhYFBQXRunXrSuPbtWsXZWVlERFRVlZWaRtjy+NbHtvRmPLy8vj000+3WdPYsWOjVatWqVunTp2qNCcA/q0mvpsCIDfJDID8Vq1meNu2beOll17aavlf//rX2GOPPar0Wr6kAshtmcoMZxMByH2Z3M444IADYvHixfHcc8/FkCFDYuDAgfHKK69kqtRqGT16dKxZsyZ1e+edd7JaD0B9lsnMACC3yQyA/FatZvj3v//9+NGPfhRz586NTZs2xaZNm2LOnDlx8cUXx/e+970qvZYvqQByWyYzYwtnEwHITZnMjIKCgth3332jV69eMXbs2DjkkENiwoQJUVxcHOvXr48PP/yw0vhVq1ZFcXFxREQUFxdvdamNLfd3NqaoqCiaNWu2zZoKCwtTl/vYcgOgempiOwOA3CQzAPJb4+o86brrrou///3vcfzxx0fjxp+9xObNm+Occ86p8jU2tnxJFRHRq1eveOGFF2LChAlxxhlnpL6k+vzR4V/8kur555+v9HqZ+pKqsLCwSvMAYNsymRlLliyJkpKSWLduXbRs2TJ1NpHFixfXytlEtpUbY8eOjTFjxlRpHgBsWyYz44s2b94cFRUV0atXr2jSpEnMnj07+vfvHxERy5YtixUrVkRJSUlERJSUlMT1118fq1evjrZt20ZExKxZs6KoqCi6deuWGvPYY49V+hmzZs1KvQYANasmMwOA3CIzAPJbtY4MLygoiN/97nfx2muvxf333x8PPfRQvPnmm3HPPfdEQUHBLhW0rS+pttjWl1RLliyJ1atXp8Zs60uqz7/GljG+pAKoHZnMDGcTAchtmcqM0aNHx1NPPRV///vfY8mSJTF69OiYN29enHXWWdGqVasYNGhQjBw5MubOnRsLFy6M8847L0pKSuKII46IiIgTTjghunXrFgMGDIi//vWvMXPmzLjyyitj6NChqZ1mBw8eHG+99VaMGjUqXnvttbjjjjti2rRpMWLEiBpZNwBUlqnMuPPOO6NHjx6pM3aUlJTEn/70p9Tj69ati6FDh8Yee+wRLVu2jP79+2910MWKFSuiX79+0bx582jbtm1ceumlsXHjxkpj5s2bF4cddlgUFhbGvvvuG1OmTNml+QOQvprsZwBQ91XryPAt9t9//9h///2r/fzRo0fHSSedFHvvvXd89NFH8cADD8S8efNi5syZlb6katOmTRQVFcVFF1203S+pbrzxxigrK9vml1S33357jBo1Ks4///yYM2dOTJs2LaZPn74rUweginY1MyKcTQQgX+xqZqxevTrOOeecWLlyZbRq1Sp69OgRM2fOjG984xsRETF+/Pho2LBh9O/fPyoqKqK0tDTuuOOO1PMbNWoUjz76aAwZMiRKSkqiRYsWMXDgwLj22mtTY7p06RLTp0+PESNGxIQJE6Jjx45x1113RWlpafUnDkCV7WpmdOzYMW644YbYb7/9IkmSuPfee+PUU0+NRYsWRffu3WPEiBExffr0ePDBB6NVq1YxbNiwOP300+OZZ56JiM8u4dSvX78oLi6OZ599NlauXBnnnHNONGnSJHW04fLly6Nfv34xePDguP/++2P27NlxwQUXRPv27eUGQC3a1cy48847484774y///3vERHRvXv3uOqqq+Kkk06KiM92oLrkkkti6tSplbYzPn8WwhUrVsSQIUNi7ty50bJlyxg4cGCMHTs2dcR6xGc7UI0cOTKWLl0anTp1iiuvvDLOPffcatcNkO+q1QzftGlTTJkyJWbPnh2rV6+OzZs3V3p8zpw5ab2OL6kAcl+mMmNbnPIWILdkKjPuvvvuHT7etGnTmDhxYkycOHG7Yzp37rxVJnxRnz59YtGiRWnVBEBmZSozTjnllEr3r7/++rjzzjtjwYIF0bFjx7j77rvjgQceiOOOOy4iIiZPnhxdu3aNBQsWxBFHHBGPP/54vPLKK/HEE09Eu3btomfPnnHdddfFZZddFtdcc00UFBTEpEmTokuXLnHzzTdHRETXrl3j6aefjvHjx/t+CqAWZCoz7EAFUD9Vqxl+8cUXx5QpU6Jfv35x0EEHRYMGDar1w31JBZD7MpUZziYCkPsylRkA5L6ayIxNmzbFgw8+GB9//HGUlJTEwoULY8OGDdG3b9/UmAMPPDD23nvvmD9/fhxxxBExf/78OPjggysd9VdaWhpDhgyJpUuXxqGHHhrz58+v9BpbxgwfPny7tVRUVERFRUXqfnl5+S7PDyBfZSoz7EAFUD9Vqxk+derUmDZtWpx88smZrgeAHJOpzHA2EYDcZzsDgHRlMjOWLFkSJSUlsW7dumjZsmU8/PDD0a1bt1i8eHEUFBRUuhRTRES7du2irKwsIiLKysoqNcK3PL7lsR2NKS8vj08//XSbl2QaO3ZsjBkzZpfnBkDNbGfYgQqg/qhWM/zz12wFgB3JVGY4mwhA7rOdAUC6MpkZBxxwQCxevDjWrFkT//M//xMDBw6MJ598MiOvXV2jR4+OkSNHpu6Xl5dHp06dslgRQP2VycywAxVA/dOwOk+65JJLYsKECZEkSabrASDHyAwA0iUzAEhXJjNjS5OkV69eMXbs2DjkkENiwoQJUVxcHOvXr48PP/yw0vhVq1ZFcXFxREQUFxfHqlWrtnp8y2M7GlNUVLTNpkZERGFhYRQVFVW6AVA9mcyMLTtQPffcczFkyJAYOHBgvPLKKxmosvpGjx4da9asSd3eeeedrNYDUNdU68jwp59+OubOnRt/+tOfonv37tGkSZNKjz/00EMZKQ6A+k9mAJAumQFAumoyMzZv3hwVFRXRq1evaNKkScyePTv69+8fERHLli2LFStWRElJSURElJSUxPXXXx+rV6+Otm3bRkTErFmzoqioKLp165Ya88UzVM2aNSv1GgDUrExmxuePMu/Vq1e88MILMWHChDjjjDNSO1B9/ujwL+5A9fzzz1d6vUztQFVYWJj2HADyTbWa4a1bt45vfetbma4FgBwkMwBIl8wAIF2ZyozRo0fHSSedFHvvvXd89NFH8cADD8S8efNi5syZ0apVqxg0aFCMHDky2rRpE0VFRXHRRRdFSUlJHHHEERERccIJJ0S3bt1iwIABceONN0ZZWVlceeWVMXTo0FRjYvDgwXH77bfHqFGj4vzzz485c+bEtGnTYvr06btcPwA7V5PbGXagAqj7qtUMnzx5cqbrACBHyQwA0iUzAEhXpjJj9erVcc4558TKlSujVatW0aNHj5g5c2Z84xvfiIiI8ePHR8OGDaN///5RUVERpaWlcccdd6Se36hRo3j00UdjyJAhUVJSEi1atIiBAwfGtddemxrTpUuXmD59eowYMSImTJgQHTt2jLvuuitKS0szMgcAdixTmWEHKoD6qVrN8IiIjRs3xrx58+LNN9+MM888M3bbbbd47733oqioKFq2bJnJGgGo52QGAOmSGQCkKxOZcffdd+/w8aZNm8bEiRNj4sSJ2x3TuXPnrY7i+6I+ffrEokWL0qoJgMzLRGbYgQqgfqpWM/ztt9+OE088MVasWBEVFRXxjW98I3bbbbf4xS9+ERUVFTFp0qRM1wlAPSUzAEiXzAAgXTIDgHRlKjPsQAVQPzWszpMuvvjiOPzww+ODDz6IZs2apZZ/61vfitmzZ2esOADqP5kBQLpkBgDpkhkApEtmAOS3ah0Z/uc//zmeffbZKCgoqLR8n332if/7v//LSGEA5AaZAUC6ZAYA6ZIZAKRLZgDkt2odGb558+bYtGnTVsvffffd2G233Xa5KAByh8wAIF0yA4B0yQwA0iUzAPJbtZrhJ5xwQtxyyy2p+w0aNIi1a9fG1VdfHSeffHKmagMgB8gMANIlMwBIl8wAIF0yAyC/Ves06TfffHOUlpZGt27dYt26dXHmmWfG66+/HnvuuWf89re/zXSNANRjMgOAdMkMANIlMwBIl8wAyG/VaoZ37Ngx/vrXv8bUqVPjpZdeirVr18agQYPirLPOimbNmmW6RgDqMZkBQLpkBgDpkhkApEtmAOS3ajXDIyIaN24cZ599diZrASBHyQwA0iUzAEiXzAAgXTIDIH9Vqxn+m9/8ZoePn3POOdUqBoDcIzMASJfMACBdMgOAdMkMgPxWrWb4xRdfXOn+hg0b4pNPPomCgoJo3ry58AAgRWYAkC6ZAUC6ZAYA6ZIZAPmtYXWe9MEHH1S6rV27NpYtWxZHH310/Pa3v810jQDUYzIDgHTJDADSJTMASJfMAMhv1WqGb8t+++0XN9xww1Z7WQHAF8kMANIlMwBIl8wAIF0yAyB/ZKwZHhHRuHHjeO+99zL5kgDkKJkBQLpkBgDpkhkApEtmAOSHal0z/JFHHql0P0mSWLlyZdx+++1x1FFHZaQwAHKDzAAgXTIDgHTJDADSJTMA8lu1muGnnXZapfsNGjSIvfbaK4477ri4+eabM1EXADlCZgCQLpkBQLpkBgDpkhkA+a1azfDNmzdnug4AcpTMACBdMgOAdMkMANIlMwDyW0avGQ4AAAAAAAAAdUG1jgwfOXJk2mPHjRtXnR8BQI6QGQCkS2YAkC6ZAUC6ZAZAfqtWM3zRokWxaNGi2LBhQxxwwAEREfG3v/0tGjVqFIcddlhqXIMGDTJTJQD1lswAIF0yA4B0yQwA0iUzAPJbtZrhp5xySuy2225x7733xu677x4RER988EGcd955ccwxx8Qll1yS0SIBqL9kBgDpkhkApEtmAJAumQGQ36p1zfCbb745xo4dmwqOiIjdd989fvazn8XNN9+cseIAqP9kBgDpkhkApEtmAJAumQGQ36rVDC8vL49//OMfWy3/xz/+ER999NEuFwVA7pAZAKRLZgCQLpkBQLpkBkB+q1Yz/Fvf+lacd9558dBDD8W7774b7777bvzv//5vDBo0KE4//fRM1whAPSYzAEiXzAAgXTIDgHTJDID8Vq1rhk+aNCl+/OMfx5lnnhkbNmz47IUaN45BgwbFL3/5y4wWCED9JjMASJfMACBdMgOAdMkMgPxWrWZ48+bN44477ohf/vKX8eabb0ZExJe//OVo0aJFRosDoP6TGQCkS2YAkC6ZAUC6ZAZAfqvWadK3WLlyZaxcuTL222+/aNGiRSRJkqm6AMgxMgOAdMkMANIlMwBIl8wAyE/Vaob/61//iuOPPz7233//OPnkk2PlypURETFo0KC45JJLMlogAPWbzAAgXTIDgHTJDADSJTMA8lu1muEjRoyIJk2axIoVK6J58+ap5WeccUbMmDEjY8UBUP/JDADSJTMASJfMACBdMgMgv1XrmuGPP/54zJw5Mzp27Fhp+X777Rdvv/12RgoDIDfIDADSJTMASJfMACBdMgMgv1XryPCPP/640h5UW7z//vtRWFi4y0UBkDtkBgDpkhkApEtmAJAumQGQ36rVDD/mmGPiN7/5Tep+gwYNYvPmzXHjjTfGsccem7HiAKj/ZAYA6ZIZAKRLZgCQLpkBkN+qdZr0G2+8MY4//vj4y1/+EuvXr49Ro0bF0qVL4/33349nnnkm0zUCUI/JDADSJTMASJfMACBdMgMgv1XryPCDDjoo/va3v8XRRx8dp556anz88cdx+umnx6JFi+LLX/5ypmsEoB6TGQCkS2YAkC6ZAUC6ZAZAfqvykeEbNmyIE088MSZNmhRXXHFFTdQEQI6QGQCkS2YAkC6ZAUC6ZAYAVT4yvEmTJvHSSy/VRC0A5BiZAUC6ZAYA6ZIZAKRLZgBQrdOkn3322XH33XdnuhYAcpDMACBdMgOAdMkMANIlMwDyW5VPkx4RsXHjxrjnnnviiSeeiF69ekWLFi0qPT5u3LiMFAdA/SczAEiXzAAgXTIDgHTJDID8VqVm+FtvvRX77LNPvPzyy3HYYYdFRMTf/va3SmMaNGiQueoAqLdkBgDpkhkApEtmAJAumQFARBWb4fvtt1+sXLky5s6dGxERZ5xxRtx6663Rrl27GikOgPpLZgCQLpkBQLpkBgDpkhkARFTxmuFJklS6/6c//Sk+/vjjjBYEQG6QGQCkS2YAkC6ZAUC6ZAYAEVVshn/RF8MEALZHZgCQLpkBQLpkBgDpkhkA+alKzfAGDRpsdQ0N19QAYFtkBgDpkhkApEtmAJAumQFARBWvGZ4kSZx77rlRWFgYERHr1q2LwYMHR4sWLSqNe+ihhzJXIQD1kswAIF0yA4B0yQwA0iUzAIioYjN84MCBle6fffbZGS0GgNwhMwBIl8wAIF0yA4B0yQwAIqrYDJ88eXJN1QFAjpEZAKRLZgCQLpkBQLpkBgARVbxmeKaNHTs2vvKVr8Ruu+0Wbdu2jdNOOy2WLVtWacy6deti6NChsccee0TLli2jf//+sWrVqkpjVqxYEf369YvmzZtH27Zt49JLL42NGzdWGjNv3rw47LDDorCwMPbdd9+YMmVKTU8PAADIAtsZAAAAAERkuRn+5JNPxtChQ2PBggUxa9as2LBhQ5xwwgnx8ccfp8aMGDEi/vjHP8aDDz4YTz75ZLz33ntx+umnpx7ftGlT9OvXL9avXx/PPvts3HvvvTFlypS46qqrUmOWL18e/fr1i2OPPTYWL14cw4cPjwsuuCBmzpxZq/MFAABqnu0MAAAAACKy3AyfMWNGnHvuudG9e/c45JBDYsqUKbFixYpYuHBhRESsWbMm7r777hg3blwcd9xx0atXr5g8eXI8++yzsWDBgoiIePzxx+OVV16J++67L3r27BknnXRSXHfddTFx4sRYv359RERMmjQpunTpEjfffHN07do1hg0bFt/+9rdj/PjxWZs7AFXjKD8A0mU7AwAAAICILDfDv2jNmjUREdGmTZuIiFi4cGFs2LAh+vbtmxpz4IEHxt577x3z58+PiIj58+fHwQcfHO3atUuNKS0tjfLy8li6dGlqzOdfY8uYLa8BQN3nKD8AqquubGdUVFREeXl5pRsAAAAANadxtgvYYvPmzTF8+PA46qij4qCDDoqIiLKysigoKIjWrVtXGtuuXbsoKytLjfn8F1RbHt/y2I7GlJeXx6effhrNmjWr9FhFRUVUVFSk7vuSCiD7ZsyYUen+lClTom3btrFw4cL42te+ljrK74EHHojjjjsuIiImT54cXbt2jQULFsQRRxyROsrviSeeiHbt2kXPnj3juuuui8suuyyuueaaKCgoqHSUX0RE165d4+mnn47x48dHaWlprc8bgF1Tl7Yzxo4dG2PGjMnY3AAAAADYsTpzZPjQoUPj5ZdfjqlTp2a7lBg7dmy0atUqdevUqVO2SwLgCxzlB0A66tJ2xujRo2PNmjWp2zvvvJPtkgDynssxAQBAbqsTzfBhw4bFo48+GnPnzo2OHTumlhcXF8f69evjww8/rDR+1apVUVxcnBrzxQ2QLfd3NqaoqGirozUifEkFUNdl8yi/L7IDFUDdVde2MwoLC6OoqKjSDYDscjkmANJlByqA+imrzfAkSWLYsGHx8MMPx5w5c6JLly6VHu/Vq1c0adIkZs+enVq2bNmyWLFiRZSUlERERElJSSxZsiRWr16dGjNr1qwoKiqKbt26pcZ8/jW2jNnyGl/kSyqAus1RfgDsSF3dzgCg7pkxY0ace+650b179zjkkENiypQpsWLFili4cGFEROpyTOPGjYvjjjsuevXqFZMnT45nn302FixYEBGRuhzTfffdFz179oyTTjoprrvuupg4cWKsX78+IqLS5Zi6du0aw4YNi29/+9sxfvz4rM0dgKqxAxVA/ZTVZvjQoUPjvvvuiwceeCB22223KCsri7KystSRd61atYpBgwbFyJEjY+7cubFw4cI477zzoqSkJI444oiIiDjhhBOiW7duMWDAgPjrX/8aM2fOjCuvvDKGDh0ahYWFERExePDgeOutt2LUqFHx2muvxR133BHTpk2LESNGZG3uAFSPo/wA2BnbGQBUl8sxAbA9dqACqJ+y2gy/8847Y82aNdGnT59o37596va73/0uNWb8+PHxzW9+M/r37x9f+9rXori4OB566KHU440aNYpHH300GjVqFCUlJXH22WfHOeecE9dee21qTJcuXWL69Okxa9asOOSQQ+Lmm2+Ou+66K0pLS2t1vgBUn6P8AEiX7QwAqsPlmACoCjtQAdQPjbP5w5Mk2emYpk2bxsSJE2PixInbHdO5c+d47LHHdvg6ffr0iUWLFlW5RgDqhqFDh8YDDzwQf/jDH1JH+UV8dnRfs2bNKh3l16ZNmygqKoqLLrpou0f53XjjjVFWVrbNo/xuv/32GDVqVJx//vkxZ86cmDZtWkyfPj1rcwegamxnAFAdWy7H9PTTT2e7lBg9enSMHDkydb+8vFxDHKAOyeYOVF88c+HYsWNjzJgxGZsbQK7J6pHhAJAuR/kBAAA1xeWYAKiKLTtQTZ06NdulxOjRo2PNmjWp2zvvvJPtkgDqlKweGQ4A6XKUHwAAkGlJksRFF10UDz/8cMybN2+Hl2Pq379/RGz7ckzXX399rF69Otq2bRsR274c0xe3Q1yOCaB+2rID1VNPPbXdHag+f3T4F3egev755yu9XiZ2oNpyxkMAtubIcAAAAADy0tChQ+O+++6LBx54IHU5prKystR1vD9/Oaa5c+fGwoUL47zzztvu5Zj++te/xsyZM7d5Oaa33norRo0aFa+99lrccccdMW3atBgxYkTW5g5A1SRJEsOGDYuHH3445syZs8MdqLbY1g5US5YsidWrV6fGbGsHqs+/xpYxdqACqB5HhgMAAACQl+68886I+OzsUJ83efLkOPfccyPis8sxNWzYMPr37x8VFRVRWload9xxR2rslssxDRkyJEpKSqJFixYxcODAbV6OacSIETFhwoTo2LGjyzEB1DNDhw6NBx54IP7whz+kdqCK+GzHqWbNmlXagapNmzZRVFQUF1100XZ3oLrxxhujrKxsmztQ3X777TFq1Kg4//zzY86cOTFt2rSYPn161uYOUJ9phgMAAACQl1yOCYB02YEKoH7SDAcAAAAAANgBO1AB1E+uGQ4AAAAAAABAztEMBwAAAAAAACDnaIYDAAAAAAAAkHM0wwEAAAAAAADIOZrhAAAAAAAAAOQczXAAAAAAAAAAco5mOAAAAAAAAAA5RzMcAAAAAAAAgJyjGQ4AAAAAAABAztEMBwAAAAAAACDnaIYDAAAAAAAAkHM0wwEAAAAAAADIOZrhAAAAAAAAAOQczXAAAAAAAAAAco5mOAAAAAAAAAA5RzMcAAAAAAAAgJyjGQ4AAAAAAABAztEMBwAAAAAAACDnaIYDAAAAAAAAkHM0wwEAAAAAAADIOZrhAAAAAAAAAOQczXAAAAAAAAAAco5mOAAAAAAAAAA5RzMcAAAAAAAAgJyjGQ4AAAAAAABAztEMBwAAAAAAACDnaIYDAAAAAAAAkHM0wwEAAAAAAADIOZrhAAAAAAAAAOQczXAAAAAAAAAAco5mOAAAAAAAAAA5RzMcAAAAAAAAgJyjGQ4AAAAAAABAztEMBwAAAAAAACDnaIYDAAAAAAAAkHM0wwEAAAAAAADIOZrhAAAAAAAAAOQczXAAAAAAAAAAco5mOAAAAAAAAAA5RzMcAAAAAAAAgJyjGQ4AAAAAAABAztEMBwAAAAAAACDnaIYDAAAAAAAAkHM0wwEAAAAAAADIOZrhAAAAAAAAAOQczXAAAAAAAAAAco5mOAAAAAAAAAA5J6vN8KeeeipOOeWU6NChQzRo0CB+//vfV3o8SZK46qqron379tGsWbPo27dvvP7665XGvP/++3HWWWdFUVFRtG7dOgYNGhRr166tNOall16KY445Jpo2bRqdOnWKG2+8saanBgAAZIntDAAAAAAistwM//jjj+OQQw6JiRMnbvPxG2+8MW699daYNGlSPPfcc9GiRYsoLS2NdevWpcacddZZsXTp0pg1a1Y8+uij8dRTT8WFF16Yery8vDxOOOGE6Ny5cyxcuDB++ctfxjXXXBO/+tWvanx+AGSOxgYA6bKdAQAAAEBERONs/vCTTjopTjrppG0+liRJ3HLLLXHllVfGqaeeGhERv/nNb6Jdu3bx+9//Pr73ve/Fq6++GjNmzIgXXnghDj/88IiIuO222+Lkk0+Om266KTp06BD3339/rF+/Pu65554oKCiI7t27x+LFi2PcuHGVvswCoG7b0tg4//zz4/TTT9/q8S2NjXvvvTe6dOkSP/3pT6O0tDReeeWVaNq0aUR81thYuXJlzJo1KzZs2BDnnXdeXHjhhfHAAw9ExL8bG3379o1JkybFkiVL4vzzz4/WrVvLDIB6xHYGAAAAABF1+Jrhy5cvj7Kysujbt29qWatWraJ3794xf/78iIiYP39+tG7dOvUFVURE3759o2HDhvHcc8+lxnzta1+LgoKC1JjS0tJYtmxZfPDBB9v82RUVFVFeXl7pBkB2nXTSSfGzn/0svvWtb2312BcbGz169Ijf/OY38d5776WOIN/S2Ljrrruid+/ecfTRR8dtt90WU6dOjffeey8iolJjo3v37vG9730vfvSjH8W4ceNqc6oA1CDbGQB8njNQAZAumQFQP9XZZnhZWVlERLRr167S8nbt2qUeKysri7Zt21Z6vHHjxtGmTZtKY7b1Gp//GV80duzYaNWqVerWqVOnXZ8QADVGYwOAdNnOAODzXFoDgHTJDID6KaunSa+rRo8eHSNHjkzdLy8v90UVQB2WycZGly5dtnqNLY/tvvvuW/3ssWPHxpgxYzIzEQBymu0MgLrHpTUASJfMAKif6uyR4cXFxRERsWrVqkrLV61alXqsuLg4Vq9eXenxjRs3xvvvv19pzLZe4/M/44sKCwujqKio0g0AtmX06NGxZs2a1O2dd97JdkkA7IDtDADS5QxUAKRLZgDUXXW2Gd6lS5coLi6O2bNnp5aVl5fHc889FyUlJRERUVJSEh9++GEsXLgwNWbOnDmxefPm6N27d2rMU089FRs2bEiNmTVrVhxwwAHbPMIPgPpHYwOAdNnOACBdLq0BQLpkBkDdldVm+Nq1a2Px4sWxePHiiPhs76nFixfHihUrokGDBjF8+PD42c9+Fo888kgsWbIkzjnnnOjQoUOcdtppERHRtWvXOPHEE+MHP/hBPP/88/HMM8/EsGHD4nvf+1506NAhIiLOPPPMKCgoiEGDBsXSpUvjd7/7XUyYMKHS6QkBqN80NgD4PNsZANR3zkAFQLpkBsCOZfWa4X/5y1/i2GOPTd3f8sXRwIEDY8qUKTFq1Kj4+OOP48ILL4wPP/wwjj766JgxY0Y0bdo09Zz7778/hg0bFscff3w0bNgw+vfvH7feemvq8VatWsXjjz8eQ4cOjV69esWee+4ZV111letrANQza9eujTfeeCN1f0tjo02bNrH33nunGhv77bdfdOnSJX76059ut7ExadKk2LBhwzYbG2PGjIlBgwbFZZddFi+//HJMmDAhxo8fn40pA1BNtjMAyITPn4Gqffv2qeWrVq2Knj17psbU1BmoCgsLMzIPAGqezACou7LaDO/Tp08kSbLdxxs0aBDXXnttXHvttdsd06ZNm3jggQd2+HN69OgRf/7zn6tdJwDZp7EBQLpsZwCQCZ8/A9WWRsaWM1ANGTIkIiqfgapXr14Rse0zUF1xxRWxYcOGaNKkSUQ4AxVArpEZAHVXVpvhAJAujQ0AACDTnIEKgHTJDID6STMcAAAAgLzkDFQApEtmANRPmuEAAAAA5CVnoAIgXTIDoH5qmO0CAAAAAAAAACDTNMMBAAAAAAAAyDma4QAAAAAAAADkHM1wAAAAAAAAAHKOZjgAAAAAAAAAOUczHAAAAAAAAICcoxkOAAAAAAAAQM7RDAcAAAAAAAAg52iGAwAAAAAAAJBzNMMBAAAAAAAAyDma4QAAAAAAAADkHM1wAAAAAAAAAHKOZjgAAAAAAAAAOUczHAAAAAAAAICcoxkOAAAAAAAAQM7RDAcAAAAAAAAg52iGAwAAAAAAAJBzNMMBAAAAAAAAyDma4QAAAAAAAADkHM1wAAAAAAAAAHKOZjgAAAAAAAAAOUczHAAAAAAAAICcoxkOAAAAAAAAQM7RDAcAAAAAAAAg52iGAwAAAAAAAJBzNMMBAAAAAAAAyDma4QAAAAAAAADkHM1wAAAAAAAAAHKOZjgAAAAAAAAAOUczHAAAAAAAAICcoxkOAAAAAAAAQM7RDAcAAAAAAAAg52iGAwAAAAAAAJBzNMMBAAAAAAAAyDma4QAAAAAAAADkHM1wAAAAAAAAAHKOZjgAAAAAAAAAOUczHAAAAAAAAICcoxkOAAAAAAAAQM7RDAcAAAAAAAAg52iGAwAAAAAAAJBzNMMBAAAAAAAAyDma4QAAAAAAAADkHM1wAAAAAAAAAHKOZjgAAAAAAAAAOUczHAAAAAAAAICcoxkOAAAAAAAAQM7RDAcAAAAAAAAg52iGAwAAAAAAAJBzNMMBAAAAAAAAyDl51QyfOHFi7LPPPtG0adPo3bt3PP/889kuCYA6SmYAkC6ZAUC6ZAYA6ZIZAJmRN83w3/3udzFy5Mi4+uqr48UXX4xDDjkkSktLY/Xq1dkuDYA6RmYAkC6ZAUC6ZAYA6ZIZAJmTN83wcePGxQ9+8IM477zzolu3bjFp0qRo3rx53HPPPdkuDYA6RmYAkC6ZAUC6ZAYA6ZIZAJnTONsF1Ib169fHwoULY/To0allDRs2jL59+8b8+fO3Gl9RUREVFRWp+2vWrImIiPLy8mr9/M0Vn1TreQDVUd3Pqi3PS5Ikk+XUOzIDyCcyY9fIDCCfyIxdIzOAfCIzdo3MAPJJbWRGXjTD//nPf8amTZuiXbt2lZa3a9cuXnvtta3Gjx07NsaMGbPV8k6dOtVYjQCZ0uqWXXv+Rx99FK1atcpILfWRzADyiczYNTIDyCcyY9fIDCCfyIxdIzOAfFIbmZEXzfCqGj16dIwcOTJ1f/PmzfH+++/HHnvsEQ0aNKjSa5WXl0enTp3inXfeiaKiokyXWq9ZNztm/WyfdbN9u7JukiSJjz76KDp06FBD1eUmmVE7rJsds362z7rZPplR+2RG7bBudsz62T7rZvtkRu2TGbXDutkx62f7rJvtkxm1T2ZUjTnmBnPMDbWVGXnRDN9zzz2jUaNGsWrVqkrLV61aFcXFxVuNLywsjMLCwkrLWrduvUs1FBUV5eybdVdZNztm/WyfdbN91V03+bzX7RYyo26zbnbM+tk+62b7ZEb1yYy6zbrZMetn+6yb7ZMZ1Scz6jbrZsesn+2zbrZPZlSfzKgd5pgbzDE31HRmNKzyK9dDBQUF0atXr5g9e3Zq2ebNm2P27NlRUlKSxcoAqGtkBgDpkhkApEtmAJAumQGQWXlxZHhExMiRI2PgwIFx+OGHx1e/+tW45ZZb4uOPP47zzjsv26UBUMfIDADSJTMASJfMACBdMgMgc/KmGX7GGWfEP/7xj7jqqquirKwsevbsGTNmzIh27drV6M8tLCyMq6++eqvTlGDd7Iz1s33WzfZZN5khM+oe62bHrJ/ts262z7rJDJlR91g3O2b9bJ91s33WTWbIjLrHutkx62f7rJvts24yQ2bUHHPMDeaYG2prjg2SJElq9CcAAAAAAAAAQC3Li2uGAwAAAAAAAJBfNMMBAAAAAAAAyDma4QAAAAAAAADkHM1wAAAAAAAAAHKOZngNuv766+PII4+M5s2bR+vWrbc5ZsWKFdGvX79o3rx5tG3bNi699NLYuHFj7RZaB+yzzz7RoEGDSrcbbrgh22VlzcSJE2OfffaJpk2bRu/eveP555/Pdkl1wjXXXLPV++TAAw/MdllZ8dRTT8Upp5wSHTp0iAYNGsTvf//7So8nSRJXXXVVtG/fPpo1axZ9+/aN119/PTvFkhaZkT6ZUZnM2DaZ8W8yI/fIjPTJjMpkxrbJjH+TGfVDVX6XN2zYENdee218+ctfjqZNm8YhhxwSM2bM2KXXrOsyvX5y5TNiZ7/f2zJv3rw47LDDorCwMPbdd9+YMmXKVmNy4b1TE+smV943EVVfPytXrowzzzwz9t9//2jYsGEMHz58m+MefPDBOPDAA6Np06Zx8MEHx2OPPZb54qny7+jO/l3q4t8CmZzjhg0b4rLLLouDDz44WrRoER06dIhzzjkn3nvvvZqexk5l+t/y8wYPHhwNGjSIW265JcNVV01NzPHVV1+N//zP/4xWrVpFixYt4itf+UqsWLGipqawU5me49q1a2PYsGHRsWPHaNasWXTr1i0mTZpUk1PYqarMcenSpdG/f//Utvv23oO7+veGZngNWr9+fXznO9+JIUOGbPPxTZs2Rb9+/WL9+vXx7LPPxr333htTpkyJq666qpYrrRuuvfbaWLlyZep20UUXZbukrPjd734XI0eOjKuvvjpefPHFOOSQQ6K0tDRWr16d7dLqhO7du1d6nzz99NPZLikrPv744zjkkENi4sSJ23z8xhtvjFtvvTUmTZoUzz33XLRo0SJKS0tj3bp1tVwp6ZIZVSMzPiMzdkxmfEZm5B6ZUTUy4zMyY8dkxmdkRt1X1d/lK6+8Mv77v/87brvttnjllVdi8ODB8a1vfSsWLVpU7desy2pi/UTkxmfEzn6/v2j58uXRr1+/OPbYY2Px4sUxfPjwuOCCC2LmzJmpMbny3qmJdRORG++biKqvn4qKithrr73iyiuvjEMOOWSbY5599tn4/ve/H4MGDYpFixbFaaedFqeddlq8/PLLmSw971X1dzSdf5e69rdApuf4ySefxIsvvhg//elP48UXX4yHHnooli1bFv/5n/9Zm9PaSk38W27x8MMPx4IFC6JDhw41PY0dqok5vvnmm3H00UfHgQceGPPmzYuXXnopfvrTn0bTpk1ra1qV1MQcR44cGTNmzIj77rsvXn311Rg+fHgMGzYsHnnkkdqaViVVneMnn3wSX/rSl+KGG26I4uLijLzmNiXUuMmTJyetWrXaavljjz2WNGzYMCkrK0stu/POO5OioqKkoqKiFivMvs6dOyfjx4/Pdhl1wle/+tVk6NChqfubNm1KOnTokIwdOzaLVdUNV199dXLIIYdku4w6JyKShx9+OHV/8+bNSXFxcfLLX/4ytezDDz9MCgsLk9/+9rdZqJCqkBk7JzP+TWZsn8zYNpmRW2TGzsmMf5MZ2ycztk1m1E1V/V1u3759cvvtt1dadvrppydnnXVWtV+zLquJ9ZOLnxFf/P3ellGjRiXdu3evtOyMM85ISktLU/dz6b2zRabWTS6+b5IkvfXzeV//+teTiy++eKvl3/3ud5N+/fpVWta7d+/khz/84S5WyOdV9Xd0Z/8udfFvgUzPcVuef/75JCKSt99+OzNFV0NNzfPdd99N/uM//iN5+eWXs77tVBNzPOOMM5Kzzz67ZgquhpqYY/fu3ZNrr7220pjDDjssueKKKzJYefp25W+D7b0HM/H3hiPDs2j+/Plx8MEHR7t27VLLSktLo7y8PJYuXZrFyrLjhhtuiD322CMOPfTQ+OUvf5mXp3Fcv359LFy4MPr27Zta1rBhw+jbt2/Mnz8/i5XVHa+//np06NAhvvSlL8VZZ52V1VOa1FXLly+PsrKySu+jVq1aRe/evb2P6jGZUZnMkBnpkBk7JzNyk8yoTGbIjHTIjJ2TGdlXnd/lioqKrY5+atasWeoI1Vz6fKiJ9bNFPn5GzJ8/v9K6jPjs74kt6zKX3jtVtbN1s0U+vm/Sle46pPqq8zu6s3+Xuva3QE3McVvWrFkTDRo02O4lqmpaTc1z8+bNMWDAgLj00kuje/fuNVN8mmpijps3b47p06fH/vvvH6WlpdG2bdvo3bt3WpfCqAk19e945JFHxiOPPBL/93//F0mSxNy5c+Nvf/tbnHDCCTUzkR2oib8NMvWamuFZVFZWVukLqohI3S8rK8tGSVnzox/9KKZOnRpz586NH/7wh/Hzn/88Ro0ale2yat0///nP2LRp0zbfF/n2ntiW3r17x5QpU2LGjBlx5513xvLly+OYY46Jjz76KNul1Slb3iveR7lFZvybzPiMzNgxmZEemZGbZMa/yYzPyIwdkxnpkRnZV53f5dLS0hg3bly8/vrrsXnz5pg1a1Y89NBDsXLlymq/Zl1VE+snIn8/I7b390R5eXl8+umnOfXeqaqdrZuI/H3fpGt76zDX3zu1qTq/ozv7d6lrfwvUxBy/aN26dXHZZZfF97///SgqKspM4VVUU/P8xS9+EY0bN44f/ehHmS+6impijqtXr461a9fGDTfcECeeeGI8/vjj8a1vfStOP/30ePLJJ2tmIjtQU/+Ot912W3Tr1i06duwYBQUFceKJJ8bEiRPja1/7WuYnsRM18bdBpl5TM7yKLr/88mjQoMEOb6+99lq2y6wTqrKuRo4cGX369IkePXrE4MGD4+abb47bbrstKioqsjwL6pKTTjopvvOd70SPHj2itLQ0Hnvssfjwww9j2rRp2S4NtklmpE9mkGkyg/pGZqRPZpBpMoNcNmHChNhvv/3iwAMPjIKCghg2bFicd9550bChrwQj0ls/PiOoDu8bqP82bNgQ3/3udyNJkrjzzjuzXU5GLVy4MCZMmBBTpkyJBg0aZLucGrF58+aIiDj11FNjxIgR0bNnz7j88svjm9/8ZkyaNCnL1WXObbfdFgsWLIhHHnkkFi5cGDfffHMMHTo0nnjiiWyXVqc0znYB9c0ll1wS55577g7HfOlLX0rrtYqLi+P555+vtGzVqlWpx+q7XVlXvXv3jo0bN8bf//73OOCAA2qgurppzz33jEaNGqXeB1usWrUqJ94Tmda6devYf//944033sh2KXXKlvfKqlWron379qnlq1atip49e2apqvwkM9InM6pOZlSNzNg2mVF3yIz0yYyqkxlVIzO2TWZkX3V+l/faa6/4/e9/H+vWrYt//etf0aFDh7j88stTn5O59PlQE+tnW/LlM6K4uHib67KoqCiaNWsWjRo1ypn3TlXtbN1sS768b9K1vXWY6++d2lSdz8Sd/bvUtb8FamKOW2xphL/99tsxZ86crB0VHlEz8/zzn/8cq1evjr333jv1+KZNm+KSSy6JW265Jf7+979ndhI7URNz3HPPPaNx48bRrVu3SmO6du261eVQakNNzPHTTz+Nn/zkJ/Hwww9Hv379IiKiR48esXjx4rjpppu2OsV6TauJvysz9Zp2A62ivfbaKw488MAd3goKCtJ6rZKSkliyZEmsXr06tWzWrFlRVFS01S9ofbQr62rx4sXRsGHDaNu2bS1XnV0FBQXRq1evmD17dmrZ5s2bY/bs2VFSUpLFyuqmtWvXxptvvlnpjy8iunTpEsXFxZXeR+Xl5fHcc895H9UymZE+mVF1MqNqZMa2yYy6Q2akT2ZUncyoGpmxbTIj+3bld7lp06bxH//xH7Fx48b43//93zj11FN3+TXrmppYP9uSL58RJSUlldZlxGd/T2xZl7n03qmqna2bbcmX9026qrMOqZrq/I7u7N+lrv0tUBNzjPh3I/z111+PJ554IvbYY4+amUCaamKeAwYMiJdeeikWL16cunXo0CEuvfTSmDlzZs1NZjtqYo4FBQXxla98JZYtW1ZpzN/+9rfo3LlzhmewczUxxw0bNsSGDRu2OuNPo0aNUkfG16aa+NsgY6+ZUGPefvvtZNGiRcmYMWOSli1bJosWLUoWLVqUfPTRR0mSJMnGjRuTgw46KDnhhBOSxYsXJzNmzEj22muvZPTo0VmuvHY9++yzyfjx45PFixcnb775ZnLfffcle+21V3LOOedku7SsmDp1alJYWJhMmTIleeWVV5ILL7wwad26dVJWVpbt0rLukksuSebNm5csX748eeaZZ5K+ffsme+65Z7J69epsl1brPvroo9RnSkQk48aNSxYtWpS8/fbbSZIkyQ033JC0bt06+cMf/pC89NJLyamnnpp06dIl+fTTT7NcOdsjM9IjMyqTGdsnM/5NZuQemZEemVGZzNg+mfFvMqPu29nv8oABA5LLL788NX7BggXJ//7v/yZvvvlm8tRTTyXHHXdc0qVLl+SDDz5I+zXrk5pYP7nyGbGz3+/LL788GTBgQGr8W2+9lTRv3jy59NJLk1dffTWZOHFi0qhRo2TGjBmpMbny3qmJdZMr75skqfr6SZIkNb5Xr17JmWeemSxatChZunRp6vFnnnkmady4cXLTTTclr776anL11VcnTZo0SZYsWVKrc8t1Vf1MTOffpa79LZDpOa5fvz75z//8z6Rjx47J4sWLk5UrV6ZuFRUVWZljktTMv+UXde7cORk/fnxNT2W7amKODz30UNKkSZPkV7/6VfL6668nt912W9KoUaPkz3/+c63PL0lqZo5f//rXk+7duydz585N3nrrrWTy5MlJ06ZNkzvuuKPW55ckVZ9jRUVFKjPat2+f/PjHP04WLVqUvP7662m/Zjo0w2vQ/9fevcdbVdf5439xO4DowUThQCKSVzCvlHAmNW9Jio6T9MtMuSjWYGAKKo6TeS0pC8kStVLBRknxO1YmpSKCmkIY4j3NW6HJAScThFFA2L8/erDHIxc3h3Nj83w+Hvuhe6/PXnw+i815n89+fdZaQ4YMKSRZ6zFjxoxim7/85S+Fo48+utC+ffvC9ttvXzjnnHMKK1eubLpON4G5c+cW+vbtW+jYsWOhXbt2hV69ehWuuOKKwnvvvdfUXWsyP/7xjws77bRToaKionDggQcWZs+e3dRdahZOPPHEQteuXQsVFRWFj3/844UTTzyx8NJLLzV1t5rEjBkz1vnzZciQIYVCoVBYvXp14Vvf+lahS5cuhbZt2xaOOOKIwgsvvNC0nWaD1IzSqBlrUzPWTc34P2pG+VEzSqNmrE3NWDc14/+oGZuHDf1b/uxnP1v8+yoUCoWZM2cWevXqVWjbtm2hU6dOhUGDBhX+9re/bdQ+Nzf1fXzK5WfER/37HjJkSOGzn/3sWu/Zb7/9ChUVFYVPfOIThYkTJ66133L47DTEsSmXz02hULfjs672PXr0qNVmypQphd13371QUVFR2GuvvQpTp05tnAFtYTbmZ2Kh8NF/L83xd4H6HOOrr766zs/vh+dbTaG+/y4/rKnD8EKhYcZ44403FnbddddCu3btCvvuu2/hV7/6VUMPY4Pqe4wLFiwoDB06tNCtW7dCu3btCnvssUdh3LhxhdWrVzfGcNZpY8a4vn9zH64rm/r7RotCoVAo/TxyAAAAAAAAAGj+3DMcAAAAAAAAgLIjDAcAAAAAAACg7AjDAQAAAAAAACg7wnAAAAAAAAAAyo4wHAAAAAAAAICyIwwHAAAAAAAAoOwIwwEAAAAAAAAoO8JwAAAAAAAAAMqOMBwAAAAAAACAsiMMhyY0dOjQtGjRovjo1KlTPv/5z+epp55q6q4B0MyoGQCUSs0AAACAfxKGQxP7/Oc/nwULFmTBggWZPn16WrdunWOPPbapuwVAM6RmAFAqNQMAAACE4dDk2rZtm6qqqlRVVWW//fbLf/zHf+S1117Lm2++mZ///OfZeuut8+KLLxbbf/3rX8+ee+6Z//3f/23CXgPQFDZUMw4//PCMHDmyVvs333wzFRUVmT59ehP1GICmsqGacckll9Q6c3zNY9KkSU3dbQAawTvvvJOTTz45HTp0SNeuXTN+/PgceuihOfvss/P8889nq622yuTJk4vtp0yZkvbt2+e5557Le++9l7322itf+9rXittffvnlbLPNNrnpppuaYjgANKBNqRkPPfRQ2rRpk5qamlr7PPvss3PwwQc39lDYggnDoRlZunRpbrnlluy6667p1KlTBg8enGOOOSYnn3xy3n///UydOjU33HBDbr311my11VZN3V0AmtCHa8bpp5+eyZMnZ/ny5cU2t9xySz7+8Y/n8MMPb8KeAtDUPlwzzj333OJZ4wsWLMgPfvCDbLXVVvnUpz7V1F0FoBGMHj06jzzySO66665MmzYtDz/8cB5//PEkyZ577pkf/OAH+frXv5758+fn9ddfz/Dhw/O9730vvXv3Trt27XLrrbfm5ptvzq9//eusWrUqp5xySj73uc/ltNNOa+KRAVDfNqVmHHLIIfnEJz6R//qv/yrub+XKlbn11lvVDBpVi0KhUGjqTsCWaujQobnlllvSrl27JMmyZcvStWvX3H333TnggAOSJP/4xz+yzz775Ljjjsudd96Zb3zjG/nP//zPpuw2AE3go2rGe++9l27duuX666/Pl770pSTJvvvumxNOOCEXX3xxU3YdgEZWyjxjjdmzZ+ewww7LzTffXKwfAJSvd955J506dcrkyZPzxS9+MUmyePHidOvWLV/96lfzwx/+MEly7LHHZsmSJamoqEirVq1yzz33pEWLFsX9fP/738+VV16ZL3/5y/nv//7vPP300+nUqVNTDAmABlIfNePKK6/MpEmT8txzzyVJ7rzzzgwZMiQ1NTXp0KFDk4yLLY8zw6GJHXbYYXniiSfyxBNPZM6cOenfv3+OPvro/PWvf02SfOxjH8uNN96Y6667Lrvsskv+4z/+o4l7DEBT2VDNaNeuXQYNGlS8NOHjjz+eZ555JkOHDm3aTgPQJD5qnpEk8+fPz7/927/l3HPPFYQDbCFeeeWVrFy5MgceeGDxtY4dO2aPPfao1e6mm27KU089lccffzyTJk2qFYQnyTnnnJPdd98911xzTW666SZBOEAZqo+aMXTo0Lz00kuZPXt2kmTSpEn50pe+JAinUQnDoYl16NAhu+66a3bdddd8+tOfzg033JBly5blZz/7WbHNQw89lFatWmXBggVZtmxZE/YWgKb0UTXj9NNPz7Rp0/L6669n4sSJOfzww9OjR48m7jUATeGjasayZcvyr//6r6murs5ll13WxL0FoLl58skns2zZsixbtiwLFixYa/uiRYvy5z//Oa1atcqLL77YBD0EoLnYUM3o3LlzjjvuuEycODELFy7M7373O5dIp9EJw6GZadGiRVq2bJl33303SfLoo4/me9/7Xn7zm99k6623zsiRI5u4hwA0Fx+uGXvvvXc+9alP5Wc/+1kmT55scgFA0QdrRqFQyCmnnJLVq1fnv/7rv9Y62w+A8vWJT3wibdq0yWOPPVZ8bfHixfnzn/9cfP7WW29l6NCh+eY3v5mhQ4fm5JNPLs451jjttNOy99575+abb87555+fP/3pT402BgAaR33VjNNPPz233357fvrTn2aXXXbJZz7zmUYbAyRJ66buAGzpli9fnpqamiT/vD/4Nddck6VLl+a4447LO++8k0GDBuUb3/hGjj766Oy444759Kc/neOOO654jw4AthwbqhlrnH766Rk5cmQ6dOiQL3zhC03VVQCa2IZqxiWXXJL7778/9913X5YuXZqlS5cm+eclD9u3b9+U3QaggW2zzTYZMmRIzjvvvGy33Xbp3LlzLr744rRs2bK4OGr48OHp3r17Lrzwwixfvjz7779/zj333EyYMCFJMmHChMyaNStPPfVUunfvnqlTp+bkk0/O7NmzU1FR0ZTDA6Ae1UfNSJL+/funsrIy3/72t12ViibhzHBoYvfcc0+6du2arl27pm/fvnnsscdyxx135NBDD81ZZ52VDh065IorrkjyzzP+rrjiivz7v/97/va3vzVxzwFobBuqGWucdNJJad26dU466aS0a9eu6ToLQJPaUM148MEHs3Tp0vzLv/xLsU3Xrl1z++23N3W3AWgEV111Vaqrq3PsscfmyCOPzGc+85n06tUr7dq1y89//vP89re/zX/913+ldevW6dChQ2655Zb87Gc/y+9+97s8//zzOe+883Lttdeme/fuSZJrr702//M//5NvfetbTTwyAOrbptSMNVq2bJmhQ4dm1apVGTx4cBOOhi1Vi0KhUGjqTgAAUD/+8pe/ZJdddsljjz2WAw44oKm7AwAANHPLli3Lxz/+8YwbNy7Dhg1r6u4A0IzVtWYMGzYsb775Zu66664G7B2sm8ukAwCUgZUrV+bvf/97LrzwwvTr108QDgAArNO8efPy/PPP58ADD8zixYuLl6w9/vjjm7hnADQ3m1ozFi9enKeffjqTJ08WhNNkhOEAAGXgkUceyWGHHZbdd989/+///b+m7g4AANCM/eAHP8gLL7yQioqK9OnTJw8//HC23377pu4WAM3QptSM448/PnPmzMnw4cPzuc99roF7CuvmMukAAAAAAAAAlJ2WTd0BAAAAAAAAAKhvwnAAAAAAAAAAyo4wHAAAAAAAAICyIwwHAAAAAAAAoOwIwwEAAAAAAAAoO8JwAAAAAAAAAMqOMBwAAAAAAACAsiMMBwAAAAAAAKDsCMMBAAAAAAAAKDvCcAAAAAAAAADKjjAcAAAAAAAAgLIjDAcAAAAAAACg7LRu6g5sDlavXp033ngj22yzTVq0aNHU3QFoEIVCIe+88066deuWli2tlaorNQPYEqgZ9UPNALYEakb9UDOALYGaAUBDEIaX4I033kj37t2buhsAjeK1117Ljjvu2NTd2GypGcCWRM3YNGoGsCVRMzaNmgFsSdQMAOqTMLwE22yzTZJ/FuHKysom7g1Aw1iyZEm6d+9e/JlH3agZwJZAzagfagawJVAz6oeaAWwJ1AwAGoIwvARrLj9VWVlpwgGUPZfc2zRqBrAlUTM2jZoBbEnUjE2jZgBbEjUDgPrkxhsAAAAAAAAAlB1hOAAAAAAAAABlRxgOAAAAAAAAQNkRhgMAAAAAAABQdoThAAAAAAAAAJQdYTgAAAAAAAAAZUcYDgAAAAAAAEDZEYYDAAAAAAAAUHaE4QAAAAAAAACUHWE4AAAAAAAAAGVHGA4AAAAAAABA2RGGAwAAAAAAAFB2hOEAAAAAAAAAlJ3WTd0B6t/O/zG1zu/9y3cH1GNPAGDzVteaqp4C5cg8A4By5Hd+AIDyJgwHANiC+fIPAAAAAChXwnCaVFN8Ae9LfwAAAICPtjl9h7IpVzABAKB8CcObMb/EAwAAAAAAANSNMLwRCLW3bO6tCDSGzemMDQAAAAAAaAzCcAAAAACg3jgxAACA5qJlU/7hl1xySVq0aFHrseeeexa3v/feexkxYkQ6deqUrbfeOgMHDszChQtr7WP+/PkZMGBAttpqq3Tu3DnnnXde3n///VptZs6cmQMOOCBt27bNrrvumkmTJjXG8AAAAAAAAABoIk1+Zvhee+2V+++/v/i8dev/69KoUaMyderU3HHHHenYsWNGjhyZE044IY888kiSZNWqVRkwYECqqqry6KOPZsGCBRk8eHDatGmTK664Ikny6quvZsCAARk+fHhuvfXWTJ8+Paeffnq6du2a/v37N+5gAaCZ2ZzO2HApeAAAKH9bwu0GzW0AABpPk4fhrVu3TlVV1VqvL168ODfeeGMmT56cww8/PEkyceLE9OrVK7Nnz06/fv1y33335bnnnsv999+fLl26ZL/99svll1+e888/P5dcckkqKipy/fXXp2fPnhk3blySpFevXvn973+f8ePHC8Pr0ZYwUdmcmFQBAAAAa/jeBgCALVWTh+EvvvhiunXrlnbt2qW6ujpjx47NTjvtlLlz52blypU58sgji2333HPP7LTTTpk1a1b69euXWbNmZe+9906XLl2Kbfr3758zzjgjzz77bPbff//MmjWr1j7WtDn77LMba4iblc1lcrS59BOgnFl4U/82pzP1AQAAAACauyYNw/v27ZtJkyZljz32yIIFC3LppZfm4IMPzjPPPJOamppUVFRk2223rfWeLl26pKamJklSU1NTKwhfs33Ntg21WbJkSd599920b99+rX4tX748y5cvLz5fsmTJJo8V6kLoD//nkksuyaWXXlrrtT322CPPP/98kuS9997LOeeck9tuuy3Lly9P//79c+2119aqAfPnz88ZZ5yRGTNmZOutt86QIUMyduzYWrfomDlzZkaPHp1nn3023bt3z4UXXpihQ4c2yhgpH35+AwAAAAA0vSYNw48++uji/++zzz7p27dvevTokSlTpqwzpG4sY8eOXStwAcEGNL299tor999/f/H5B0PsUaNGZerUqbnjjjvSsWPHjBw5MieccEIeeeSRJMmqVasyYMCAVFVV5dFHH82CBQsyePDgtGnTJldccUWS5NVXX82AAQMyfPjw3HrrrZk+fXpOP/30dO3atdFureFnDQANTa0B+D8W3QIAQHlr8sukf9C2226b3XffPS+99FI+97nPZcWKFXn77bdrnR2+cOHC4j3Gq6qqMmfOnFr7WLhwYXHbmv+uee2DbSorK9cbuF9wwQUZPXp08fmSJUvSvXv3TR4fAJumdevWxZ/vH7R48eLceOONmTx5cg4//PAkycSJE9OrV6/Mnj07/fr1y3333Zfnnnsu999/f7p06ZL99tsvl19+ec4///xccsklqaioyPXXX5+ePXtm3LhxSZJevXrl97//fcaPH99oYTgAQENzqxOobUtYdAsAAFuqZhWGL126NC+//HIGDRqUPn36pE2bNpk+fXoGDhyYJHnhhRcyf/78VFdXJ0mqq6vzne98J4sWLUrnzp2TJNOmTUtlZWV69+5dbPPb3/621p8zbdq04j7WpW3btmnbtm1DDBGaNV+K0dy9+OKL6datW9q1a5fq6uqMHTs2O+20U+bOnZuVK1fmyCOPLLbdc889s9NOO2XWrFnp169fZs2alb333rvWGRz9+/fPGWeckWeffTb7779/Zs2aVWsfa9qcffbZ6+2TW2s0f1vKGZBbyjgBAOqbRbcAAFC+mjQMP/fcc3PcccelR48eeeONN3LxxRenVatWOemkk9KxY8cMGzYso0ePznbbbZfKysqceeaZqa6uTr9+/ZIkRx11VHr37p1BgwblyiuvTE1NTS688MKMGDGiGGYPHz4811xzTcaMGZPTTjstDzzwQKZMmZKpU31hDLA56du3byZNmpQ99tgjCxYsyKWXXpqDDz44zzzzTGpqalJRUVHrSiJJ0qVLl9TU1CRJampqagXha7av2bahNkuWLMm77767ziuKuLUGAHw0iy6B5syiWzYX6ikAwMZr0jD89ddfz0knnZS///3v2WGHHXLQQQdl9uzZ2WGHHZIk48ePT8uWLTNw4MBa92Vao1WrVrn77rtzxhlnpLq6Oh06dMiQIUNy2WWXFdv07NkzU6dOzahRo3L11Vdnxx13zA033GDlLdDsmeTWdvTRRxf/f5999knfvn3To0ePTJkyZb23vWgMbq0BAACbL4tuAQCgvDVpGH7bbbdtcHu7du0yYcKETJgwYb1tevTosdZl0D/s0EMPzbx58+rURwCap2233Ta77757XnrppXzuc5/LihUr8vbbb9f6omrhwoXFyx1WVVVlzpw5tfaxcOHC4rY1/13z2gfbVFZWrjdwd2sNAADYfFl0y5ZgU26pVK4L7gGALUezumc4QHPmTO3mZenSpXn55ZczaNCg9OnTJ23atMn06dMzcODAJMkLL7yQ+fPnp7q6OklSXV2d73znO1m0aFE6d+6cJJk2bVoqKyvTu3fvYpsPL7CaNm1acR8AbB4uueSStc6m22OPPfL8888nSd57772cc845ue2222pdgeqDZ+3Nnz8/Z5xxRmbMmJGtt946Q4YMydixY9O69f9NoWbOnJnRo0fn2WefTffu3XPhhRdm6NChjTLGZNO+2GbdhAWARbfQtDan7142p74CwJZMGA40qcb+EteEY/N17rnn5rjjjkuPHj3yxhtv5OKLL06rVq1y0kknpWPHjhk2bFhGjx6d7bbbLpWVlTnzzDNTXV2dfv36JUmOOuqo9O7dO4MGDcqVV16ZmpqaXHjhhRkxYkTxS6bhw4fnmmuuyZgxY3LaaaflgQceyJQpUzJ1qrABYHOz11575f777y8+/2CIPWrUqEydOjV33HFHOnbsmJEjR+aEE07II488kiRZtWpVBgwYkKqqqjz66KNZsGBBBg8enDZt2uSKK65Ikrz66qsZMGBAhg8fnltvvTXTp0/P6aefnq5du7olUz3yJTPQ2Cy6hfph0R4A0FwIwwHYLLz++us56aST8ve//z077LBDDjrooMyePTs77LBDkmT8+PFp2bJlBg4cWOssvzVatWqVu+++O2eccUaqq6vToUOHDBkyJJdddlmxTc+ePTN16tSMGjUqV199dXbcccfccMMNQo165ksR6qqxQ7Gm+KwK8OpP69ati2fkfdDixYtz4403ZvLkOlXgTQAATL9JREFUyTn88MOTJBMnTkyvXr0ye/bs9OvXL/fdd1+ee+653H///enSpUv222+/XH755Tn//PNzySWXpKKiItdff3169uyZcePGJUl69eqV3//+9xk/fry6AbAZsegWyoOruwAA6yMMBzbZ5hRsbU59pbbbbrttg9vbtWuXCRMmZMKECett06NHj7XOyPiwQw89NPPmzatTHwFoPl588cV069Yt7dq1S3V1dcaOHZuddtopc+fOzcqVK3PkkUcW2+65557ZaaedMmvWrPTr1y+zZs3K3nvvXeuy6f37988ZZ5yRZ599Nvvvv39mzZpVax9r2px99tnr7dPy5cuzfPny4vMlS5bU34ChDAk2aAwW3VJXW8r3C1vKOAGA8iUMB2hgJo4ATcvP4S1P3759M2nSpOyxxx5ZsGBBLr300hx88MF55plnUlNTk4qKilr3fk2SLl26pKamJklSU1NTKwhfs33Ntg21WbJkSd5999113gN27Nixa93LnIbh3z0usU+pLLoFAIDyJgwHAADKytFHH138/3322Sd9+/ZNjx49MmXKlHWG1I3lggsuyOjRo4vPlyxZku7duzdZf2gehLYAAADQcIThAABAWdt2222z++6756WXXsrnPve5rFixIm+//Xats8MXLlxYvMd4VVVV5syZU2sfCxcuLG5b8981r32wTWVl5XoD97Zt2xbvHwsAAI3F4jsAtmTCcAAANpovU5oXfx8btnTp0rz88ssZNGhQ+vTpkzZt2mT69OkZOHBgkuSFF17I/PnzU11dnSSprq7Od77znSxatCidO3dOkkybNi2VlZXp3bt3sc2HL4k7bdq04j4AAGBdNuV2LlvK7+8AUJ+E4QAANBr38aUxnHvuuTnuuOPSo0ePvPHGG7n44ovTqlWrnHTSSenYsWOGDRuW0aNHZ7vttktlZWXOPPPMVFdXp1+/fkmSo446Kr17986gQYNy5ZVXpqamJhdeeGFGjBhRPLN7+PDhueaaazJmzJicdtppeeCBBzJlypRMneozvqXy8239toRjI9gAAABonoThAABAWXn99ddz0kkn5e9//3t22GGHHHTQQZk9e3Z22GGHJMn48ePTsmXLDBw4MMuXL0///v1z7bXXFt/fqlWr3H333TnjjDNSXV2dDh06ZMiQIbnsssuKbXr27JmpU6dm1KhRufrqq7PjjjvmhhtuSP/+/Rt9vAAAAACsmzAcAAAoK7fddtsGt7dr1y4TJkzIhAkT1tumR48ea10G/cMOPfTQzJs3r059hM3RlnCGd1No7OPqTHSA2tS39XM7JgDKgTAcAACaCV/EAaXy8wIAAAA+Wsum7gAAAAAAAAAA1DdnhgMAAADEGfcAAADlxpnhAAAAAAAAAJQdYTgAAAAAAAAAZUcYDgAAAAAAAEDZcc9wAAAAgC1EXe+L/pfvDqjnngCwser6MxwAtmTODAcAAAAAAACg7AjDAQAAAAAAACg7wnAAAAAAAAAAyo4wHAAAAAAAAICyIwwHAAAAAAAAoOwIwwEAAAAAAAAoO8JwAAAAAAAAAMqOMBwAAAAAAACAsiMMBwAAAAAAAKDsCMMBAAAAAAAAKDvCcAAAAAAAAADKjjAcAAAAAAAAgLIjDAcAAAAAAACg7AjDAQAAAAAAACg7wnAAAAAAAAAAyo4wHAAAAAAAAICyIwwHAAAAAAAAoOwIwwEAAAAAAAAoO8JwAAAAAAAAAMqOMBwAAAAAAACAsiMMBwAAAAAAAKDsCMMBAAAAAAAAKDvCcAAAAAAAAADKjjAcAAAAAAAAgLIjDAcAAAAAAACg7AjDAQAAAAAAACg7wnAAAAAAAAAAyo4wHAAAAAAAAICyIwwHAAAAAAAAoOwIwwEAAAAAAAAoO8JwAAAAAAAAAMqOMBwAAAAAAACAsiMMBwAAAAAAAKDsCMMBAAAAAAAAKDvNJgz/7ne/mxYtWuTss88uvvbee+9lxIgR6dSpU7beeusMHDgwCxcurPW++fPnZ8CAAdlqq63SuXPnnHfeeXn//fdrtZk5c2YOOOCAtG3bNrvuumsmTZrUCCMCAAAAAAAAoKk0izD8sccey09+8pPss88+tV4fNWpUfvOb3+SOO+7Igw8+mDfeeCMnnHBCcfuqVasyYMCArFixIo8++mhuvvnmTJo0KRdddFGxzauvvpoBAwbksMMOyxNPPJGzzz47p59+eu69995GGx8AAAAAAAAAjavJw/ClS5fm5JNPzs9+9rN87GMfK76+ePHi3Hjjjbnqqqty+OGHp0+fPpk4cWIeffTRzJ49O0ly33335bnnnsstt9yS/fbbL0cffXQuv/zyTJgwIStWrEiSXH/99enZs2fGjRuXXr16ZeTIkfniF7+Y8ePHN8l4AQAAAAAAAGh4TR6GjxgxIgMGDMiRRx5Z6/W5c+dm5cqVtV7fc889s9NOO2XWrFlJklmzZmXvvfdOly5dim369++fJUuW5Nlnny22+fC++/fvX9zHuixfvjxLliyp9QCg+XBrDQAAAAAA4KM0aRh+22235fHHH8/YsWPX2lZTU5OKiopsu+22tV7v0qVLampqim0+GISv2b5m24baLFmyJO++++46+zV27Nh07Nix+OjevXudxgdA/XNrDQAAoCFYdAsAAOWnycLw1157LWeddVZuvfXWtGvXrqm6sU4XXHBBFi9eXHy89tprTd0lAOLWGgAAQMOw6BYAAMpTk4Xhc+fOzaJFi3LAAQekdevWad26dR588MH86Ec/SuvWrdOlS5esWLEib7/9dq33LVy4MFVVVUmSqqqqtVbjrnn+UW0qKyvTvn37dfatbdu2qaysrPUAoOm5tQYAAFDfLLoFAIDy1WRh+BFHHJGnn346TzzxRPHxqU99KieffHLx/9u0aZPp06cX3/PCCy9k/vz5qa6uTpJUV1fn6aefzqJFi4ptpk2blsrKyvTu3bvY5oP7WNNmzT4A2Dy4tQYAANAQLLoFAIDy1bqp/uBtttkmn/zkJ2u91qFDh3Tq1Kn4+rBhwzJ69Ohst912qayszJlnnpnq6ur069cvSXLUUUeld+/eGTRoUK688srU1NTkwgsvzIgRI9K2bdskyfDhw3PNNddkzJgxOe200/LAAw9kypQpmTp1auMOGIA6W3NrjWnTpjXLW2uMHj26+HzJkiUCcQAA2EysWXT72GOPrbWtsRbdruvKhWPHjs2ll15a53EBAAD/1GRnhpdi/PjxOfbYYzNw4MAccsghqaqqyp133lnc3qpVq9x9991p1apVqqurc8opp2Tw4MG57LLLim169uyZqVOnZtq0adl3330zbty43HDDDenfv39TDAmAOnBrDQDq6rvf/W5atGiRs88+u/jae++9lxEjRqRTp07ZeuutM3DgwLV+/s+fPz8DBgzIVlttlc6dO+e8887L+++/X6vNzJkzc8ABB6Rt27bZddddM2nSpEYYEQD1Zc2i21tvvbVZLrpdvHhx8fHaa681dZcAAGCz1GRnhq/LzJkzaz1v165dJkyYkAkTJqz3PT169Mhvf/vbDe730EMPzbx58+qjiwA0gTW31vigU089NXvuuWfOP//8dO/evXhrjYEDByZZ9601vvOd72TRokXp3LlzknXfWuPDNcWtNQA2X4899lh+8pOfZJ999qn1+qhRozJ16tTccccd6dixY0aOHJkTTjghjzzySJJk1apVGTBgQKqqqvLoo49mwYIFGTx4cNq0aZMrrrgiSfLqq69mwIABGT58eG699dZMnz49p59+erp27WrhLcBm4oOLbtdYtWpVHnrooVxzzTW59957i4tuP3h2+IcX3c6ZM6fWfutr0e2aqx4CAAB116zPDAeA5P9urfHBxwdvrdGxY8firTVmzJiRuXPn5tRTT13vrTWefPLJ3Hvvveu8tcYrr7ySMWPG5Pnnn8+1116bKVOmZNSoUU05fADqYOnSpTn55JPzs5/9LB/72MeKry9evDg33nhjrrrqqhx++OHp06dPJk6cmEcffTSzZ89Oktx333157rnncsstt2S//fbL0UcfncsvvzwTJkzIihUrkiTXX399evbsmXHjxqVXr14ZOXJkvvjFL2b8+PFNMl4ANt6aRbdPPPFE8fGpT30qJ598cvH/1yy6XWNdi26ffvrpLFq0qNhmXYtuP7iPNW0sugUAgIYnDAegLLi1BgAfNGLEiAwYMCBHHnlkrdfnzp2blStX1np9zz33zE477ZRZs2YlSWbNmpW999671v1d+/fvnyVLluTZZ58ttvnwvvv371/cx7osX748S5YsqfUAoOlYdAsAAOWvWV0mHQBK5dYaAKzPbbfdlscffzyPPfbYWttqampSUVFR63K3SdKlS5fU1NQU23wwCF+zfc22DbVZsmRJ3n333XVe9nbs2LG59NJL6zwuABrf+PHj07JlywwcODDLly9P//79c+211xa3r1l0e8YZZ6S6ujodOnTIkCFD1rnodtSoUbn66quz4447WnQLAACNRBgOAACUjddeey1nnXVWpk2blnbt2jV1d2q54IILMnr06OLzJUuWpHv37k3YIwA+zKJbAAAoLy6TDgAAlI25c+dm0aJFOeCAA9K6deu0bt06Dz74YH70ox+ldevW6dKlS1asWJG333671vsWLlyYqqqqJElVVVUWLly41vY12zbUprKycp1nhSdJ27ZtU1lZWesBAAAAQMMRhgMAAGXjiCOOyNNPP50nnnii+PjUpz6Vk08+ufj/bdq0yfTp04vveeGFFzJ//vxUV1cnSaqrq/P0009n0aJFxTbTpk1LZWVlevfuXWzzwX2sabNmHwAAAAA0PZdJBwAAysY222yTT37yk7Ve69ChQzp16lR8fdiwYRk9enS22267VFZW5swzz0x1dXX69euXJDnqqKPSu3fvDBo0KFdeeWVqampy4YUXZsSIEWnbtm2SZPjw4bnmmmsyZsyYnHbaaXnggQcyZcqUTJ06tXEHDAAAAMB6CcMBAIAtyvjx49OyZcsMHDgwy5cvT//+/XPttdcWt7dq1Sp33313zjjjjFRXV6dDhw4ZMmRILrvssmKbnj17ZurUqRk1alSuvvrq7LjjjrnhhhvSv3//phgSAAAAAOsgDAcAAMrazJkzaz1v165dJkyYkAkTJqz3PT169Mhvf/vbDe730EMPzbx58+qjiwAAAAA0APcMBwAAAAAAAKDsCMMBAAAAAAAAKDvCcAAAAAAAAADKjjAcAAAAAAAAgLIjDAcAAAAAAACg7AjDAQAAAAAAACg7wnAAAAAAAAAAyo4wHAAAAAAAAICyIwwHAAAAAAAAoOwIwwEAAAAAAAAoO8JwAAAAAAAAAMqOMBwAAAAAAACAsiMMBwAAAAAAAKDsCMMBAAAAAAAAKDvCcAAAAAAAAADKjjAcAAAAAAAAgLIjDAcAAAAAAACg7AjDAQAAAAAAACg7wnAAAAAAAAAAyo4wHAAAAAAAAICyIwwHAAAAAAAAoOwIwwEAAAAAAAAoO8JwAAAAAAAAAMqOMBwAAAAAAACAsiMMBwAAAAAAAKDsCMMBAAAAAAAAKDvCcAAAAAAAAADKjjAcAAAAAAAAgLIjDAcAAAAAAACg7AjDAQAAAAAAACg7wnAAAAAAAAAAyo4wHAAAAAAAAICyIwwHAAAAAAAAoOzUKQx/5ZVX6rsfAJQpNQOAUqkZAJRKzQAAAEpRpzB81113zWGHHZZbbrkl7733Xn33CYAyomYAUCo1A4BSqRkAAEAp6hSGP/7449lnn30yevToVFVV5d///d8zZ86c+u4bAGVAzQCgVGoGAKVSMwAAgFLUKQzfb7/9cvXVV+eNN97ITTfdlAULFuSggw7KJz/5yVx11VV5880367ufAGym1AwASqVmAFAqNQMAAChFncLwNVq3bp0TTjghd9xxR773ve/lpZdeyrnnnpvu3btn8ODBWbBgQX31E4DNnJoBQKnUDABKpWYAAAAbsklh+B//+Md8/etfT9euXXPVVVfl3HPPzcsvv5xp06bljTfeyPHHH19f/QRgM6dmAFAqNQOAUqkZAADAhrSuy5uuuuqqTJw4MS+88EKOOeaY/PznP88xxxyTli3/ma337NkzkyZNys4771yffQVgM6RmAFAqNQOAUqkZAABAKeoUhl933XU57bTTMnTo0HTt2nWdbTp37pwbb7xxkzoHwOZPzQCgVGoGAKVSMwAAgFLUKQx/8cUXP7JNRUVFhgwZUpfdA1BG1AwASqVmAFAqNQMAAChFne4ZPnHixNxxxx1rvX7HHXfk5ptv3uROAVA+1AwASqVmAFAqNQMAAChFncLwsWPHZvvtt1/r9c6dO+eKK64oeT/XXXdd9tlnn1RWVqaysjLV1dX53e9+V9z+3nvvZcSIEenUqVO23nrrDBw4MAsXLqy1j/nz52fAgAHZaqut0rlz55x33nl5//33a7WZOXNmDjjggLRt2za77rprJk2atHEDBqDO6qtmAFD+1AwASqVmAAAApahTGD5//vz07Nlzrdd79OiR+fPnl7yfHXfcMd/97nczd+7c/PGPf8zhhx+e448/Ps8++2ySZNSoUfnNb36TO+64Iw8++GDeeOONnHDCCcX3r1q1KgMGDMiKFSvy6KOP5uabb86kSZNy0UUXFdu8+uqrGTBgQA477LA88cQTOfvss3P66afn3nvvrcvQAdhI9VUzACh/agYApVIzAACAUtQpDO/cuXOeeuqptV5/8skn06lTp5L3c9xxx+WYY47Jbrvtlt133z3f+c53svXWW2f27NlZvHhxbrzxxlx11VU5/PDD06dPn0ycODGPPvpoZs+enSS577778txzz+WWW27Jfvvtl6OPPjqXX355JkyYkBUrViRJrr/++vTs2TPjxo1Lr169MnLkyHzxi1/M+PHj6zJ0ADZSfdUMVxMBKH/1VTMAKH9qBgAAUIo6heEnnXRSvvGNb2TGjBlZtWpVVq1alQceeCBnnXVWvvzlL9epI6tWrcptt92WZcuWpbq6OnPnzs3KlStz5JFHFtvsueee2WmnnTJr1qwkyaxZs7L33nunS5cuxTb9+/fPkiVLimeXz5o1q9Y+1rRZs491Wb58eZYsWVLrAUDd1FfNcDURgPLXEPMMAMpTfdUMi24BAKC8ta7Lmy6//PL85S9/yRFHHJHWrf+5i9WrV2fw4MEbfV+mp59+OtXV1Xnvvfey9dZb55e//GV69+6dJ554IhUVFdl2221rte/SpUtqamqSJDU1NbWC8DXb12zbUJslS5bk3XffTfv27dfq09ixY3PppZdu1DgAWLf6qhnHHXdcreff+c53ct1112X27NnZcccdc+ONN2by5Mk5/PDDkyQTJ05Mr169Mnv27PTr1694NZH7778/Xbp0yX777ZfLL788559/fi655JJUVFTUuppIkvTq1Su///3vM378+PTv37+ejggA61Of8wwAylt91Yw1i2532223FAqF3HzzzTn++OMzb9687LXXXhk1alSmTp2aO+64Ix07dszIkSNzwgkn5JFHHknyf4tuq6qq8uijj2bBggUZPHhw2rRpU+zHmkW3w4cPz6233prp06fn9NNPT9euXc0zAACggdUpDK+oqMjtt9+eyy+/PE8++WTat2+fvffeOz169Njofe2xxx554oknsnjx4vy///f/MmTIkDz44IN16Va9ueCCCzJ69Oji8yVLlqR79+5N2COAzVd91ow1Vq1alTvuuKPkq4n069dvvVcTOeOMM/Lss89m//33X+/VRM4+++z19mX58uVZvnx58bmriQDUXUPUDADKU33VDItuAQCgvNXpMulr7L777vn//r//L8cee2ydv6CqqKjIrrvumj59+mTs2LHZd999c/XVV6eqqiorVqzI22+/Xav9woULU1VVlSSpqqpa69JUa55/VJvKysp1nhWeJG3bti1eHmvNA4BNUx814+mnn87WW2+dtm3bZvjw4cWridTU1DTK1UTWZezYsenYsWPxYfEUwKbb1JrhkrcAW476mGes4RZ+AABQfup0ZviqVasyadKkTJ8+PYsWLcrq1atrbX/ggQfq3KHVq1dn+fLl6dOnT9q0aZPp06dn4MCBSZIXXngh8+fPT3V1dZKkuro63/nOd7Jo0aJ07tw5STJt2rRUVlamd+/exTa//e1va/0Z06ZNK+4DgIZVnzXD1UQAylt91QyXvAUof/U5z3ALPwAAKF91CsPPOuusTJo0KQMGDMgnP/nJtGjRok5/+AUXXJCjjz46O+20U955551Mnjw5M2fOzL333puOHTtm2LBhGT16dLbbbrtUVlbmzDPPTHV1dfr165ckOeqoo9K7d+8MGjQoV155ZWpqanLhhRdmxIgRadu2bZJk+PDhueaaazJmzJicdtppeeCBBzJlypRMnTq1Tn0GYOPUV81I/u9qIknSp0+fPPbYY7n66qtz4oknFq8m8sEvqj58NZE5c+bU2l99XU1kTc0BYNPUV81wyVuA8lef8wyLbgEAoHzVKQy/7bbbMmXKlBxzzDGb9IcvWrQogwcPzoIFC9KxY8fss88+uffee/O5z30uSTJ+/Pi0bNkyAwcOzPLly9O/f/9ce+21xfe3atUqd999d84444xUV1enQ4cOGTJkSC677LJim549e2bq1KkZNWpUrr766uy444654YYbfEEF0Ejqq2asi6uJAJSXhqgZq1atyh133FHyJW/79eu33kvennHGGXn22Wez//77r/eSt2effXa99R2A9avPmmHRLQAAlK86heEfnCRsihtvvHGD29u1a5cJEyZkwoQJ623To0ePtYKLDzv00EMzb968OvURgE1TXzXD1UQAyl991YykeV7ydvny5Vm+fHnxufu/AtRdfdaMD7PoFgAAykfLurzpnHPOydVXX51CoVDf/QGgzNRXzVhzNZE99tgjRxxxRB577LG1riZy7LHHZuDAgTnkkENSVVWVO++8s/j+NVcTadWqVaqrq3PKKadk8ODB67yayLRp07Lvvvtm3LhxriYC0Ijqc56x5pK3f/jDH3LGGWdkyJAhee655+qhl3U3duzYdOzYsfhwuVuAuquvmnHBBRfkoYceyl/+8pc8/fTTueCCCzJz5sycfPLJtRbdzpgxI3Pnzs2pp5663kW3Tz75ZO699951Lrp95ZVXMmbMmDz//PO59tprM2XKlIwaNWqTjwMAALBhdToz/Pe//31mzJiR3/3ud9lrr73Spk2bWts/GD4AsGWrr5rhaiIA5a8+5xnN8ZK37v8KUH/qq2a4hR8AAJS3OoXh2267bb7whS/Ud18AKENqBgClasia0Rwueev+rwD1p75qhkW3AABQ3uoUhk+cOLG++wFAmVIzAChVfdWMCy64IEcffXR22mmnvPPOO5k8eXJmzpyZe++9t9Ylb7fbbrtUVlbmzDPPXO8lb6+88srU1NSs85K311xzTcaMGZPTTjstDzzwQKZMmZKpU6fWyxgA2DDzDAAAoBR1umd4krz//vu5//7785Of/CTvvPNOkuSNN97I0qVL661zAJQHNQOAUtVHzVhzyds99tgjRxxxRB577LG1Lnl77LHHZuDAgTnkkENSVVVV63K6ay5526pVq1RXV+eUU07J4MGD13nJ22nTpmXffffNuHHjXPIWoJGZZwAAAB+lTmeG//Wvf83nP//5zJ8/P8uXL8/nPve5bLPNNvne976X5cuX5/rrr6/vfgKwmVIzAChVfdUMl7wFKH/mGQAAQCnqdGb4WWedlU996lP5xz/+kfbt2xdf/8IXvpDp06fXW+cA2PypGQCUSs0AoFRqBgAAUIo6nRn+8MMP59FHH01FRUWt13feeef87W9/q5eOAVAe1AwASqVmAFAqNQMAAChFnc4MX716dVatWrXW66+//nq22WabTe4UAOVDzQCgVGoGAKVSMwAAgFLUKQw/6qij8sMf/rD4vEWLFlm6dGkuvvjiHHPMMfXVNwDKgJoBQKnUDABKpWYAAAClqNNl0seNG5f+/fund+/eee+99/KVr3wlL774Yrbffvv84he/qO8+ArAZUzMAKJWaAUCp1AwAAKAUdQrDd9xxxzz55JO57bbb8tRTT2Xp0qUZNmxYTj755LRv376++wjAZkzNAKBUagYApVIzAACAUtQpDE+S1q1b55RTTqnPvgBQptQMAEqlZgBQKjUDAAD4KHUKw3/+859vcPvgwYPr1BkAyo+aAUCp1AwASqVmAAAApahTGH7WWWfVer5y5cr87//+byoqKrLVVluZcABQpGYAUCo1A4BSqRkAAEApWtblTf/4xz9qPZYuXZoXXnghBx10UH7xi1/Udx8B2IypGQCUSs0AoFRqBgAAUIo6heHrsttuu+W73/3uWitzAeDD1AwASqVmAFAqNQMAAPiwegvDk6R169Z544036nOXAJQpNQOAUqkZAJRKzQAAAD6oTvcMv+uuu2o9LxQKWbBgQa655pp85jOfqZeOAVAe1AwASqVmAFAqNQMAAChFncLwf/u3f6v1vEWLFtlhhx1y+OGHZ9y4cfXRLwDKhJoBQKnUDABKpWYAAAClqFMYvnr16vruBwBlSs0AoFRqBgClUjMAAIBS1Os9wwEAAAAAAACgOajTmeGjR48uue1VV11Vlz8CgDKhZgBQKjUDgFKpGQAAQCnqFIbPmzcv8+bNy8qVK7PHHnskSf785z+nVatWOeCAA4rtWrRoUT+9BGCzpWYAUCo1A4BSqRkAAEAp6hSGH3fccdlmm21y880352Mf+1iS5B//+EdOPfXUHHzwwTnnnHPqtZMAbL7UDABKpWYAUCo1AwAAKEWd7hk+bty4jB07tjjZSJKPfexj+fa3v51x48bVW+cA2PypGQCUSs0AoFRqBgAAUIo6heFLlizJm2++udbrb775Zt55551N7hQA5UPNAKBUagYApVIzAACAUtQpDP/CF76QU089NXfeeWdef/31vP766/nv//7vDBs2LCeccEJ99xGAzZiaAUCp1AwASqVmAAAApajTPcOvv/76nHvuufnKV76SlStX/nNHrVtn2LBh+f73v1+vHQRg86ZmAFAqNQOAUqkZAABAKeoUhm+11Va59tpr8/3vfz8vv/xykmSXXXZJhw4d6rVzAGz+1AwASqVmAFAqNQMAAChFnS6TvsaCBQuyYMGC7LbbbunQoUMKhUJ99QuAMqNmAFAqNQOAUqkZAADAhtQpDP/73/+eI444IrvvvnuOOeaYLFiwIEkybNiwnHPOOfXaQQA2b2oGAKVSMwAolZoBAACUok5h+KhRo9KmTZvMnz8/W221VfH1E088Mffcc0+9dQ6AzZ+aAUCp1AwASqVmAAAApajTPcPvu+++3Hvvvdlxxx1rvb7bbrvlr3/9a710DIDyoGYAUCo1A4BSqRkAAEAp6nRm+LJly2qtul3jrbfeStu2bTe5UwCUDzUDgFKpGQCUSs0AAABKUacw/OCDD87Pf/7z4vMWLVpk9erVufLKK3PYYYfVW+cA2PypGQCUSs0AoFRqBgAAUIo6XSb9yiuvzBFHHJE//vGPWbFiRcaMGZNnn302b731Vh555JH67iMAmzE1A4BSqRkAlErNAAAASlGnM8M/+clP5s9//nMOOuigHH/88Vm2bFlOOOGEzJs3L7vsskt99xGAzZiaAUCp1AwASqVmAAAApdjoM8NXrlyZz3/+87n++uvzzW9+syH6BECZUDMAKJWaAUCp1AwAAKBUG31meJs2bfLUU081RF8AKDNqBgClUjMAKJWaAQAAlKpOl0k/5ZRTcuONN9Z3XwAoQ2oGAKVSMwAolZoBAACUYqMvk54k77//fm666abcf//96dOnTzp06FBr+1VXXVUvnQNg86dmAFAqNQOAUqkZAABAKTYqDH/llVey884755lnnskBBxyQJPnzn/9cq02LFi3qr3cAbLbUDABKpWYAUCo1AwAA2BgbFYbvtttuWbBgQWbMmJEkOfHEE/OjH/0oXbp0aZDOAbD5UjMAKJWaAUCp1AwAAGBjbNQ9wwuFQq3nv/vd77Js2bJ67RAA5UHNAKBUagYApVIzAACAjbFRYfiHfXgCAgDro2YAUCo1A4BSqRkAAMCGbFQY3qJFi7Xuu+Q+TACsi5oBQKnUDABKpWYAAAAbY6PuGV4oFDJ06NC0bds2SfLee+9l+PDh6dChQ612d955Z/31EIDNkpoBQKnUDABKpWYAAAAbY6PC8CFDhtR6fsopp9RrZwAoH2oGAKVSMwAolZoBAABsjI0KwydOnNhQ/QCgzKgZAJRKzQCgVGoGAACwMTbqnuEAAAAAAAAAsDkQhgMAAAAAAABQdpo0DB87dmw+/elPZ5tttknnzp3zb//2b3nhhRdqtXnvvfcyYsSIdOrUKVtvvXUGDhyYhQsX1mozf/78DBgwIFtttVU6d+6c8847L++//36tNjNnzswBBxyQtm3bZtddd82kSZMaengAAAAAAAAANJEmDcMffPDBjBgxIrNnz860adOycuXKHHXUUVm2bFmxzahRo/Kb3/wmd9xxRx588MG88cYbOeGEE4rbV61alQEDBmTFihV59NFHc/PNN2fSpEm56KKLim1effXVDBgwIIcddlieeOKJnH322Tn99NNz7733Nup4Aag7C6gAAAAAAICN0aRh+D333JOhQ4dmr732yr777ptJkyZl/vz5mTt3bpJk8eLFufHGG3PVVVfl8MMPT58+fTJx4sQ8+uijmT17dpLkvvvuy3PPPZdbbrkl++23X44++uhcfvnlmTBhQlasWJEkuf7669OzZ8+MGzcuvXr1ysiRI/PFL34x48ePb7KxA7BxLKACAADqm0W3AABQ3prVPcMXL16cJNluu+2SJHPnzs3KlStz5JFHFtvsueee2WmnnTJr1qwkyaxZs7L33nunS5cuxTb9+/fPkiVL8uyzzxbbfHAfa9qs2ceHLV++PEuWLKn1AKBpWUAFAADUN4tuAQCgvDWbMHz16tU5++yz85nPfCaf/OQnkyQ1NTWpqKjItttuW6ttly5dUlNTU2zzwSB8zfY12zbUZsmSJXn33XfX6svYsWPTsWPH4qN79+71MkYA6k9zWUAFQPPjLD8ASmXRLQAAlLdmE4aPGDEizzzzTG677bam7kouuOCCLF68uPh47bXXmrpLAHxAc1pA5WoiAM2Ps/wAqKvmsujWPAMAAOpH66buQJKMHDkyd999dx566KHsuOOOxderqqqyYsWKvP3227XCjYULF6aqqqrYZs6cObX2t+aMjg+2+fBZHgsXLkxlZWXat2+/Vn/atm2btm3b1svYAKh/axZQ/f73v2/qrmTs2LG59NJLm7obAHzAPffcU+v5pEmT0rlz58ydOzeHHHJI8Sy/yZMn5/DDD0+STJw4Mb169crs2bPTr1+/4ll+999/f7p06ZL99tsvl19+ec4///xccsklqaioqHWWX5L06tUrv//97zN+/Pj079+/0ccNwKZpykW3H/5+yjwDAADqR5OeGV4oFDJy5Mj88pe/zAMPPJCePXvW2t6nT5+0adMm06dPL772wgsvZP78+amurk6SVFdX5+mnn86iRYuKbaZNm5bKysr07t272OaD+1jTZs0+ANh8rFlANWPGjPUuoPqgDy+gWtfiqDXbNtRmfQuoXE0EoPlzlh8ApXDVQgAAKD9NGoaPGDEit9xySyZPnpxtttkmNTU1qampKV6GtmPHjhk2bFhGjx6dGTNmZO7cuTn11FNTXV2dfv36JUmOOuqo9O7dO4MGDcqTTz6Ze++9NxdeeGFGjBhRPLt7+PDheeWVVzJmzJg8//zzufbaazNlypSMGjWqycYOwMZprguo2rZtm8rKyloPAJqP5nRrjbFjx6Zjx47FR/fu3etljABsuua26NY8AwAA6keThuHXXXddFi9enEMPPTRdu3YtPm6//fZim/Hjx+fYY4/NwIEDc8ghh6Sqqip33nlncXurVq1y9913p1WrVqmurs4pp5ySwYMH57LLLiu26dmzZ6ZOnZpp06Zl3333zbhx43LDDTe4dCHAZsQCKgDqwll+AGxIc110CwAA1I8mvWd4oVD4yDbt2rXLhAkTMmHChPW26dGjR377299ucD+HHnpo5s2bt9F9BKB5uO6665L88+f5B02cODFDhw5N8s8FVC1btszAgQOzfPny9O/fP9dee22x7ZoFVGeccUaqq6vToUOHDBkyZJ0LqEaNGpWrr746O+64owVUAJupNWf5PfTQQ+s9y++DZ4d/+Cy/OXPm1NpffZzlt2bxFQDNw4gRIzJ58uT8+te/Li66Tf652LZ9+/a1Ft1ut912qayszJlnnrneRbdXXnllampq1rno9pprrsmYMWNy2mmn5YEHHsiUKVMyderUJhs7AABsCZo0DAeAUllABUCpCoVCzjzzzPzyl7/MzJkzN3iW38CBA5Os+yy/73znO1m0aFE6d+6cZN1n+X24pjjLD2DzYtEtAACUN2E4AABQVpzlB0CpLLoFAIDy1qT3DAcAAKhv1113XRYvXpxDDz00Xbt2LT5uv/32Ypvx48fn2GOPzcCBA3PIIYekqqoqd955Z3H7mrP8WrVqlerq6pxyyikZPHjwOs/ymzZtWvbdd9+MGzfOWX4AAAAAzYgzwwEAgLLiLD8AAAAAEmeGAwAAAAAAAFCGhOEAAAAAAAAAlB1hOAAAAAAAAABlRxgOAAAAAAAAQNkRhgMAAAAAAABQdoThAAAAAAAAAJQdYTgAAAAAAAAAZUcYDgAAAAAAAEDZEYYDAAAAAAAAUHaE4QAAAAAAAACUHWE4AAAAAAAAAGVHGA4AAAAAAABA2RGGAwAAAAAAAFB2hOEAAAAAAAAAlB1hOAAAAAAAAABlRxgOAAAAAAAAQNkRhgMAAAAAAABQdoThAAAAAAAAAJQdYTgAAAAAAAAAZUcYDgAAAAAAAEDZEYYDAAAAAAAAUHaE4QAAAAAAAACUHWE4AAAAAAAAAGVHGA4AAAAAAABA2RGGAwAAAAAAAFB2hOEAAAAAAAAAlB1hOAAAAAAAAABlRxgOAAAAAAAAQNkRhgMAAAAAAABQdoThAAAAAAAAAJQdYTgAAAAAAAAAZUcYDgAAAAAAAEDZEYYDAAAAAAAAUHaE4QAAAAAAAACUHWE4AAAAAAAAAGVHGA4AAAAAAABA2RGGAwAAAAAAAFB2hOEAAAAAAAAAlB1hOAAAAAAAAABlRxgOAAAAAAAAQNkRhgMAAAAAAABQdoThAAAAAAAAAJQdYTgAAAAAAAAAZUcYDgAAAAAAAEDZEYYDAAAAAAAAUHaE4QAAAAAAAACUHWE4AAAAAAAAAGVHGA4AAAAAAABA2WnSMPyhhx7Kcccdl27duqVFixb51a9+VWt7oVDIRRddlK5du6Z9+/Y58sgj8+KLL9Zq89Zbb+Xkk09OZWVltt122wwbNixLly6t1eapp57KwQcfnHbt2qV79+658sorG3poAAAAAAAAADShJg3Dly1bln333TcTJkxY5/Yrr7wyP/rRj3L99dfnD3/4Qzp06JD+/fvnvffeK7Y5+eST8+yzz2batGm5++6789BDD+VrX/tacfuSJUty1FFHpUePHpk7d26+//3v55JLLslPf/rTBh8fAPXHAioAAKC+mWcAAEB5a9Iw/Oijj863v/3tfOELX1hrW6FQyA9/+MNceOGFOf7447PPPvvk5z//ed54443ixORPf/pT7rnnntxwww3p27dvDjrooPz4xz/ObbfdljfeeCNJcuutt2bFihW56aabstdee+XLX/5yvvGNb+Sqq65qzKECsIksoAKgVIINAEplngEAAOWt2d4z/NVXX01NTU2OPPLI4msdO3ZM3759M2vWrCTJrFmzsu222+ZTn/pUsc2RRx6Zli1b5g9/+EOxzSGHHJKKiopim/79++eFF17IP/7xj3X+2cuXL8+SJUtqPQBoWhZQAVAqwQYApTLPAACA8tZsw/CampokSZcuXWq93qVLl+K2mpqadO7cudb21q1bZ7vttqvVZl37+OCf8WFjx45Nx44di4/u3btv+oAAaDAWUAHwQYINAOqDeQYAAGz+mm0Y3pQuuOCCLF68uPh47bXXmrpLAGyABVQAlEqwAUCpzDMAAGDz12zD8KqqqiTJwoULa72+cOHC4raqqqosWrSo1vb3338/b731Vq0269rHB/+MD2vbtm0qKytrPQBgXSygAti8CDYA2ByYZwAAQP1otmF4z549U1VVlenTpxdfW7JkSf7whz+kuro6SVJdXZ233347c+fOLbZ54IEHsnr16vTt27fY5qGHHsrKlSuLbaZNm5Y99tgjH/vYxxppNAA0JAuoANgcCDYANi/mGQAAsPlr0jB86dKleeKJJ/LEE08k+eclC5944onMnz8/LVq0yNlnn51vf/vbueuuu/L0009n8ODB6datW/7t3/4tSdKrV698/vOfz1e/+tXMmTMnjzzySEaOHJkvf/nL6datW5LkK1/5SioqKjJs2LA8++yzuf3223P11Vdn9OjRTTRqAOqbBVQAlEqwAUCpzDMAAGDz16Rh+B//+Mfsv//+2X///ZMko0ePzv7775+LLrooSTJmzJiceeaZ+drXvpZPf/rTWbp0ae655560a9euuI9bb701e+65Z4444ogcc8wxOeigg/LTn/60uL1jx46577778uqrr6ZPnz4555xzctFFF+VrX/ta4w4WgE1iARUA9UGwAcAHmWcAAEB5a92Uf/ihhx6aQqGw3u0tWrTIZZddlssuu2y9bbbbbrtMnjx5g3/OPvvsk4cffrjO/QSg6f3xj3/MYYcdVny+5oujIUOGZNKkSRkzZkyWLVuWr33ta3n77bdz0EEHrXMB1ciRI3PEEUekZcuWGThwYH70ox8Vt69ZQDVixIj06dMn22+/vQVUAJuhpUuX5qWXXio+XxNsbLfddtlpp52KwcZuu+2Wnj175lvf+tZ6g43rr78+K1euXGewcemll2bYsGE5//zz88wzz+Tqq6/O+PHjm2LIANSReQYAAJS3FoUNpdEk+eeZIh07dszixYvrdCnDnf9jagP0CmDd/vLdAXV636b+rOOf1Axgc1KuNWPmzJm1go011gQbhUIhF198cX76058Wg41rr702u+++e7HtW2+9lZEjR+Y3v/lNrWBj6623LrZ56qmnMmLEiDz22GPZfvvtc+aZZ+b8888vuZ9qBrA5KdeasblQM4DNiZoBQHPSpGeGAwAA1DdXoAIAAAAgaeJ7hgMAAAAAAABAQxCGAwAAAAAAAFB2hOEAAAAAAAAAlB1hOAAAAAAAAABlRxgOAAAAAAAAQNkRhgMAAAAAAABQdoThAAAAAAAAAJQdYTgAAAAAAAAAZUcYDgAAAAAAAEDZEYYDAAAAAAAAUHaE4QAAAAAAAACUHWE4AAAAAAAAAGVHGA4AAAAAAABA2RGGAwAAAAAAAFB2hOEAAAAAAAAAlB1hOAAAAAAAAABlRxgOAAAAAAAAQNkRhgMAAAAAAABQdoThAAAAAAAAAJQdYTgAAAAAAAAAZUcYDgAAAAAAAEDZEYYDAAAAAAAAUHaE4QAAAAAAAACUHWE4AAAAAAAAAGVHGA4AAAAAAABA2RGGAwAAAAAAAFB2hOEAAAAAAAAAlB1hOAAAAAAAAABlRxgOAAAAAAAAQNkRhgMAAAAAAABQdoThAAAAAAAAAJQdYTgAAAAAAAAAZUcYDgAAAAAAAEDZEYYDAAAAAAAAUHaE4QAAAAAAAACUHWE4AAAAAAAAAGVHGA4AAAAAAABA2RGGAwAAAAAAAFB2hOEAAAAAAAAAlB1hOAAAAAAAAABlRxgOAAAAAAAAQNkRhgMAAAAAAABQdoThAAAAAAAAAJQdYTgAAAAAAAAAZUcYDgAAAAAAAEDZEYYDAAAAAAAAUHaE4QAAAAAAAACUHWE4AAAAAAAAAGVHGA4AAAAAAABA2RGGAwAAAAAAAFB2tqgwfMKECdl5553Trl279O3bN3PmzGnqLgHQTKkZAJRKzQCgVGoGAAA0ri0mDL/99tszevToXHzxxXn88cez7777pn///lm0aFFTdw2AZkbNAKBUagYApVIzAACg8W0xYfhVV12Vr371qzn11FPTu3fvXH/99dlqq61y0003NXXXAGhm1AwASqVmAFAqNQMAABpf66buQGNYsWJF5s6dmwsuuKD4WsuWLXPkkUdm1qxZa7Vfvnx5li9fXny+ePHiJMmSJUvq9OevXv6/dXofQF3U9WfVmvcVCoX67M5mR80AtiRqxqZRM4AtiZqxadQMYEuiZgDQnGwRYfj//M//ZNWqVenSpUut17t06ZLnn39+rfZjx47NpZdeutbr3bt3b7A+AtSXjj/ctPe/88476dixY730ZXOkZgBbEjVj06gZwJZEzdg0agawJVEzAGhOtogwfGNdcMEFGT16dPH56tWr89Zbb6VTp05p0aLFRu1ryZIl6d69e1577bVUVlbWd1ebBWMsD8ZYHjZljIVCIe+88066devWQL0rT2pG43BsNszxWT/HZv3UjManZmwcYywPxlge1IzGp2Y0Dsdmwxyf9XNs1k/NAKC52SLC8O233z6tWrXKwoULa72+cOHCVFVVrdW+bdu2adu2ba3Xtt12203qQ2VlZdn/YmSM5cEYy0Ndx2jVrZrR3Dk2G+b4rJ9js35qRt2pGY3DGMuDMZYHNaPu1IzmzbHZMMdn/Ryb9VMzAGguWjZ1BxpDRUVF+vTpk+nTpxdfW716daZPn57q6uom7BkAzY2aAUCp1AwASqVmAABA09gizgxPktGjR2fIkCH51Kc+lQMPPDA//OEPs2zZspx66qlN3TUAmhk1A4BSqRkAlErNAACAxrfFhOEnnnhi3nzzzVx00UWpqanJfvvtl3vuuSddunRp0D+3bdu2ufjii9e6tFU5McbyYIzlYUsYY2NQM5ofx2bDHJ/1c2zWz7GpH2pGwzHG8mCM5WFLGGNjUDOaH8dmwxyf9XNs1s+xAaC5aVEoFApN3QkAAAAAAAAAqE9bxD3DAQAAAAAAANiyCMMBAAAAAAAAKDvCcAAAAAAAAADKjjAcAAAAAAAAgLIjDN9IEyZMyM4775x27dqlb9++mTNnzgbb33HHHdlzzz3Trl277L333vntb39ba3uhUMhFF12Url27pn379jnyyCPz4osvNuQQPlJ9jnHlypU5//zzs/fee6dDhw7p1q1bBg8enDfeeKOhh/GR6vvv8oOGDx+eFi1a5Ic//GE993rjNMQY//SnP+Vf//Vf07Fjx3To0CGf/vSnM3/+/IYawkeq7zEuXbo0I0eOzI477pj27dund+/euf766xtyCB9pY8b47LPPZuDAgdl55503+Bnc2ONG3WzMcV65cmUuu+yy7LLLLmnXrl323Xff3HPPPZu0z+auvo/PJZdckhYtWtR67Lnnng09jHr30EMP5bjjjku3bt3SokWL/OpXv/rI98ycOTMHHHBA2rZtm1133TWTJk1aq005fHYa4tiUy+cm2fjjs2DBgnzlK1/J7rvvnpYtW+bss89eZ7uN+R2IujPPWJt5xtrMMxqPeUZt5hnNi3nGhplnrJt5xvqZZ2yYeQYAm70CJbvtttsKFRUVhZtuuqnw7LPPFr761a8Wtt1228LChQvX2f6RRx4ptGrVqnDllVcWnnvuucKFF15YaNOmTeHpp58utvnud79b6NixY+FXv/pV4cknnyz867/+a6Fnz56Fd999t7GGVUt9j/Htt98uHHnkkYXbb7+98PzzzxdmzZpVOPDAAwt9+vRpzGGtpSH+Lte48847C/vuu2+hW7duhfHjxzfwSNavIcb40ksvFbbbbrvCeeedV3j88ccLL730UuHXv/71evfZ0BpijF/96lcLu+yyS2HGjBmFV199tfCTn/yk0KpVq8Kvf/3rxhpWLRs7xjlz5hTOPffcwi9+8YtCVVXVOj+DG7tP6mZjj/OYMWMK3bp1K0ydOrXw8ssvF6699tpCu3btCo8//nid99mcNcTxufjiiwt77bVXYcGCBcXHm2++2VhDqje//e1vC9/85jcLd955ZyFJ4Ze//OUG27/yyiuFrbbaqjB69OjCc889V/jxj39caNWqVeGee+4ptimXz05DHJty+dwUCht/fF599dXCN77xjcLNN99c2G+//QpnnXXWWm025ncg6s48Y23mGeYZ5hkNyzxj82WesWHmGetnnrF+5hkbZp4BwOZOGL4RDjzwwMKIESOKz1etWlXo1q1bYezYsets/6UvfakwYMCAWq/17du38O///u+FQqFQWL16daGqqqrw/e9/v7j97bffLrRt27bwi1/8ogFG8NHqe4zrMmfOnEKSwl//+tf66XQdNNQ4X3/99cLHP/7xwjPPPFPo0aNHk35J1RBjPPHEEwunnHJKw3S4DhpijHvttVfhsssuq9XmgAMOKHzzm9+sx56XbmPH+EHr+wxuyj4p3cYe565duxauueaaWq+dcMIJhZNPPrnO+2zOGuL4XHzxxYV99923QfrbVEr5omHMmDGFvfbaq9ZrJ554YqF///7F5+X02Vmjvo5NOX5uCoXSjs8Hffazn13nl1R1+V2PjWeesTbzDPOMpmSesWHmGU3LPGPDzDNKY56xfuYZG2aeAcDmyGXSS7RixYrMnTs3Rx55ZPG1li1b5sgjj8ysWbPW+Z5Zs2bVap8k/fv3L7Z/9dVXU1NTU6tNx44d07dv3/XusyE1xBjXZfHixWnRokW23Xbbeun3xmqoca5evTqDBg3Keeedl7322qthOl+ihhjj6tWrM3Xq1Oy+++7p379/OnfunL59+5Z06aiG0FB/j//yL/+Su+66K3/7299SKBQyY8aM/PnPf85RRx3VMAPZgLqMsSn2ydrqcpyXL1+edu3a1Xqtffv2+f3vf1/nfTZXDXF81njxxRfTrVu3fOITn8jJJ5/cpJdXbSwf9bOtnD47G6vU31O2xM9Nqeryux4bxzzDPMM8wzyjsZlnbL7MMzbMPKN+mWesn3nGpjPPAKAxCcNL9D//8z9ZtWpVunTpUuv1Ll26pKamZp3vqamp2WD7Nf/dmH02pIYY44e99957Of/883PSSSelsrKyfjq+kRpqnN/73vfSunXrfOMb36j/Tm+khhjjokWLsnTp0nz3u9/N5z//+dx33335whe+kBNOOCEPPvhgwwxkAxrq7/HHP/5xevfunR133DEVFRX5/Oc/nwkTJuSQQw6p/0F8hLqMsSn2ydrqcpz79++fq666Ki+++GJWr16dadOm5c4778yCBQvqvM/mqiGOT5L07ds3kyZNyj333JPrrrsur776ag4++OC88847DTqepra+n21LlizJu+++W1afnY31Uccm2XI/N6Xa2N/12HjmGeYZ5hnmGY3NPGPzZZ6xYeYZ9cs8Y/3MMzadeQYAjal1U3eALcfKlSvzpS99KYVCIdddd11Td6dezZ07N1dffXUef/zxtGjRoqm70yBWr16dJDn++OMzatSoJMl+++2XRx99NNdff30++9nPNmX36s2Pf/zjzJ49O3fddVd69OiRhx56KCNGjEi3bt3WWrEK9enqq6/OV7/61ey5555p0aJFdtlll5x66qm56aabmrprzUIpx+foo48u/v8+++yTvn37pkePHpkyZUqGDRvWFN1mM+BzA5s/84zNm3mGeQYNyzxjw8wzaCg+NwDQfDgzvETbb799WrVqlYULF9Z6feHChamqqlrne6qqqjbYfs1/N2afDakhxrjGmi+o/vrXv2batGlNdrZG0jDjfPjhh7No0aLstNNOad26dVq3bp2//vWvOeecc7Lzzjs3yDg2pCHGuP3226d169bp3bt3rTa9evVqkss8NcQY33333fznf/5nrrrqqhx33HHZZ599MnLkyJx44on5wQ9+0DAD2YC6jLEp9sna6nKcd9hhh/zqV7/KsmXL8te//jXPP/98tt5663ziE5+o8z6bq4Y4Puuy7bbbZvfdd89LL71Ur/1vbtb3s62ysjLt27cvq8/OxvqoY7MuW8rnplSl/q5H3ZlnmGeYZ5hnNDbzjM2XecaGmWfUL/OM9TPP2HTmGQA0JmF4iSoqKtKnT59Mnz69+Nrq1aszffr0VFdXr/M91dXVtdonybRp04rte/bsmaqqqlptlixZkj/84Q/r3WdDaogxJv/3BdWLL76Y+++/P506dWqYAZSoIcY5aNCgPPXUU3niiSeKj27duuW8887Lvffe23CDWY+GGGNFRUU+/elP54UXXqjV5s9//nN69OhRzyP4aA0xxpUrV2blypVp2bL2j8ZWrVoVz1hpTHUZY1Psk7VtynFu165dPv7xj+f999/Pf//3f+f444/f5H02Nw1xfNZl6dKlefnll9O1a9d663tzVMrP73L57GysUn5P+bAt5XNTqrocQzaOeYZ5hnmGeUZjM8/YfJlnbJh5Rv0yz1g/84xNZ54BQKMqULLbbrut0LZt28KkSZMKzz33XOFrX/taYdttty3U1NQUCoVCYdCgQYX/+I//KLZ/5JFHCq1bty784Ac/KPzpT38qXHzxxYU2bdoUnn766WKb7373u4Vtt9228Otf/7rw1FNPFY4//vhCz549C++++26jj69QqP8xrlixovCv//qvhR133LHwxBNPFBYsWFB8LF++vEnGWCg0zN/lh/Xo0aMwfvz4hh7KejXEGO+8885CmzZtCj/96U8LL774YuHHP/5xoVWrVoWHH3640cdXKDTMGD/72c8W9tprr8KMGTMKr7zySmHixImFdu3aFa699tpGH1+hsPFjXL58eWHevHmFefPmFbp27Vo499xzC/PmzSu8+OKLJe+T+rGxf3ezZ88u/Pd//3fh5ZdfLjz00EOFww8/vNCzZ8/CP/7xj5L3uTlpiONzzjnnFGbOnFl49dVXC4888kjhyCOPLGy//faFRYsWNfbwNsk777xT/HecpHDVVVcV5s2bV/jrX/9aKBQKhf/4j/8oDBo0qNj+lVdeKWy11VaF8847r/CnP/2pMGHChEKrVq0K99xzT7FNuXx2GuLYlMvnplDY+ONTKBSK7fv06VP4yle+Upg3b17h2WefLW6vy+9AbDzzDPMM8wzzjMZmnrH5Ms/YMPOM9TPPWD/zjA0zzwBgcycM30g//vGPCzvttFOhoqKicOCBBxZmz55d3PbZz362MGTIkFrtp0yZUth9990LFRUVhb322qswderUWttXr15d+Na3vlXo0qVLoW3btoUjjjii8MILLzTGUNarPsf46quvFpKs8zFjxoxGGtG61fff5Yc19ZdUhULDjPHGG28s7LrrroV27doV9t1338KvfvWrhh7GBtX3GBcsWFAYOnRooVu3boV27doV9thjj8K4ceMKq1evbozhrNPGjHF9/+Y++9nPlrxP6s/G/N3NnDmz0KtXr0Lbtm0LnTp1KgwaNKjwt7/9baP2ubmp7+Nz4oknFrp27VqoqKgofPzjHy+ceOKJhZdeeqmxhlNvZsyYsc5/x2uOx5AhQ9b6Nz1jxozCfvvtV6ioqCh84hOfKEycOHGt/ZbDZ6chjk25fG4Khbodn3W179GjR602G/s7EHVjnmGeYZ5hntHYzDM2X+YZG2aesW7mGetnnrFh5hkAbO5aFAqFwqadWw4AAAAAAAAAzYt7hgMAAAAAAABQdoThAAAAAAAAAJQdYTgAAAAAAAAAZUcYDgAAAAAAAEDZEYYDAAAAAAAAUHaE4QAAAAAAAACUHWE4AAAAAAAAAGVHGA4AAAAAAABA2RGGAwAAAAAAAFB2hOEAAAAAAAAAlB1hODSyd955JyeffHI6dOiQrl27Zvz48Tn00ENz9tln5/nnn89WW22VyZMnF9tPmTIl7du3z3PPPZckadGixVqPnXfeuYlGA0BD2lDNuOyyy/LJT35yrffst99++da3vpWHHnoobdq0SU1NTa3tZ599dg4++ODGGgIAjWRTakZingGwpdlQ3Zg5c+Y668LQoUOTJDvvvPM6twMANEfCcGhko0ePziOPPJK77ror06ZNy8MPP5zHH388SbLnnnvmBz/4Qb7+9a9n/vz5ef311zN8+PB873vfS+/evZMkCxYsKD5eeuml7LrrrjnkkEOackgANJAN1YzTTjstf/rTn/LYY48V28+bNy9PPfVUTj311BxyyCH5xCc+kf/6r/8qbl+5cmVuvfXWnHbaaY0+FgAa1qbUjMQ8A2BLs6G68S//8i+16sIDDzyQdu3aFevCY489Vtz2+uuvp1+/fhbcAgDNVotCoVBo6k7AluKdd95Jp06dMnny5Hzxi19MkixevDjdunXLV7/61fzwhz9Mkhx77LFZsmRJKioq0qpVq9xzzz1rrbAtFAoZOHBg5s+fn4cffjjt27dv7OEA0IBKqRnHHHNMdt5551x77bVJkm984xt5+umnM2PGjCTJlVdemUmTJhWvLnLnnXdmyJAhqampSYcOHZpmYADUu/qoGWuYZwCUv1K/n0qSv//97znwwAPz+c9/PhMmTFhrX2eddVZ+/etf57HHHssOO+zQWEMAACiZM8OhEb3yyitZuXJlDjzwwOJrHTt2zB577FGr3U033ZSnnnoqjz/+eCZNmrTOS03953/+Z2bNmpVf//rXvqACKEOl1IyvfvWr+cUvfpH33nsvK1asyOTJk2ud9T106NC89NJLmT17dpJk0qRJ+dKXviQIBygz9VEz1jDPACh/pX4/tXLlygwcODA9evTI1VdfvdZ+fvrTn+bGG2/MXXfdJQgHAJqt1k3dAWBtTz75ZJYtW5aWLVtmwYIF6dq1a63tt9xyS8aPH5+ZM2fm4x//eBP1EoCmdtxxx6Vt27b55S9/mYqKiqxcubJ4ZkeSdO7cOccdd1wmTpyYnj175ne/+11mzpzZdB0GoMl8VM1IzDMAqO2MM87Ia6+9ljlz5qR169pfI8+YMSNnnnlmfvGLX2SfffZpoh4CAHw0Z4ZDI/rEJz6RNm3a1LpX3+LFi/PnP/+5+Pytt97K0KFD881vfjNDhw7NySefnHfffbe4fdasWTn99NPzk5/8JP369WvU/gPQeEqpGa1bt86QIUMyceLETJw4MV/+8pfXOovv9NNPz+23356f/vSn2WWXXfKZz3ym0cYAQOOoj5phngGw5Silblx11VWZMmVKfv3rX6dTp0613v/SSy/li1/8Yv7zP/8zJ5xwQqP1GwCgLpwZDo1om222yZAhQ3Leeedlu+22S+fOnXPxxRenZcuWxUuhDx8+PN27d8+FF16Y5cuXZ//998+5556bCRMmpKamJl/4whfy5S9/Of37909NTU2SpFWrVi5HBVBmSqkZyT/D7l69eiVJHnnkkbX2079//1RWVubb3/52LrvsskbrPwCNZ1NrhnkGwJblo+rG/fffnzFjxmTChAnZfvvti3Whffv2qaioyHHHHZf9998/X/va14rbkqSqqqqphgQAsF4tCoVCoak7AVuSd955J8OHD8+vfvWrVFZWZsyYMbntttty+OGHp1evXvn617+eefPmZbfddkuSzJkzJwcddFDxnn2HHXbYWvvs0aNH/vKXvzTySABoaBuqGWPHji22O+SQQ/LWW2/lmWeeWed+LrroolxxxRV57bXX1rr1BgDlYVNqxsyZM80zALYwG6obbdu2zaWXXrrWe4YMGZJLLrkkPXv2XOc+fc0MADRHwnBoYsuWLcvHP/7xjBs3LsOGDWvq7gDQjK2rZhQKhey22275+te/ntGjR6/zfcOGDcubb76Zu+66qzG7C0ATqmvNAGDL5PspAKBcuUw6NLJ58+bl+eefz4EHHpjFixcXL1l7/PHHN3HPAGhuPqpmvPnmm7nttttSU1OTU089da33L168OE8//XQmT54sCAcoc5taMwDYsvh+CgDYUgjDoQn84Ac/yAsvvJCKior06dMnDz/8cLbffvum7hYAzdCGakbnzp2z/fbb56c//Wk+9rGPrfXe448/PnPmzMnw4cPzuc99rrG7DkAj25SaAcCWx/dTAMCWwGXSAQAAAAAAACg7LZu6AwAAAAAAAABQ34ThAAAAAAAAAJQdYTgAAAAAAAAAZUcYDgAAAAAAAEDZEYYDAAAAAAAAUHaE4QAAAAAAAACUHWE4AAAAAAAAAGVHGA4AAAAAAABA2RGGAwAAAAAAAFB2/n8Z+mjUIpjnowAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#%%script echo skipping\n",
        "\n",
        "if not LOAD_DATA_FROM_CSV:\n",
        "    def plot_histogram(data, xlabel, ylabel, position, bins=20, xlog_scale=False, ylog_scale=False):\n",
        "        plt.subplot(4, 5, position)\n",
        "        plt.hist(data, bins=bins)\n",
        "        plt.xlabel(xlabel)\n",
        "        plt.ylabel(ylabel)\n",
        "        if xlog_scale: # Setting does not work expected for epsilon.\n",
        "            plt.xscale(\"log\")\n",
        "        if ylog_scale: # Setting does not work expected for epsilon.\n",
        "            plt.yscale(\"log\")\n",
        "\n",
        "    # Plotting the histograms of rho, vx, epsilon, b0, b1, b2, and b3\n",
        "    bins = 20\n",
        "    plt.figure(figsize=(20, 16))\n",
        "\n",
        "    plot_histogram(rho, \"rho\", \"Frequency\", 1, bins=bins)\n",
        "    plot_histogram(epsilon, \"epsilon\", \"Frequency\", 2, bins=bins)\n",
        "    plot_histogram(vx, \"vx\", \"Frequency\", 3, bins=bins)\n",
        "    plot_histogram(vy, \"vy\", \"Frequency\", 4, bins=bins)\n",
        "    plot_histogram(vz, \"vz\", \"Frequency\", 5, bins=bins)\n",
        "    plot_histogram(Bx, \"Bx\", \"Frequency\", 6, bins=bins)\n",
        "    plot_histogram(By, \"By\", \"Frequency\", 7, bins=bins)\n",
        "    plot_histogram(Bz, \"Bz\", \"Frequency\", 8, bins=bins)\n",
        "\n",
        "    # Plot these\n",
        "    plot_histogram(gxx, \"gxx\", \"Frequency\", 9, bins=bins)\n",
        "    plot_histogram(gxy, \"gxy\", \"Frequency\", 10, bins=bins)\n",
        "    plot_histogram(gxz, \"gxz\", \"Frequency\", 11, bins=bins)\n",
        "    plot_histogram(gyy, \"gyy\", \"Frequency\", 12, bins=bins)\n",
        "    plot_histogram(gyz, \"gyz\", \"Frequency\", 13, bins=bins)\n",
        "    plot_histogram(gzz, \"gzz\", \"Frequency\", 14, bins=bins)\n",
        "\n",
        "\n",
        "    plt.suptitle(\"Primitive variables and metric\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcEYWxrayUfI"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"last_expr_or_assign\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cQqnwx5ayUfI",
        "outputId": "cfd65216-c5fb-470b-cd9c-7740f20b93f2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x1600 with 14 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABTYAAAX/CAYAAACZzaZEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdeVxU1eP/8Tcgm8uAGyCJSuonM7fERMqtJLFosTSXrNRcyrBUSs0WtVXTT2mZW/VJ/VRm2sesXDBzq5S0UHMnK03LQEsBNQWF8/uj79yfIyCgyHDl9Xw85lHce+bec86cO3Pm7Z17PYwxRgAAAAAAAABgI57urgAAAAAAAAAAFBXBJgAAAAAAAADbIdgEAAAAAAAAYDsEmwAAAAAAAABsh2ATAAAAAAAAgO0QbAIAAAAAAACwHYJNAAAAAAAAALZDsAkAAAAAAADAdgg2AQAAAAAAANgOwSYAlEJ16tRRnz593F2NMu/48ePq37+/QkJC5OHhoaFDh+ZbltescPbt2ycPDw/Nnj27yM8dO3asPDw89OeffxZYtqRfD2e7/v3vfxfrdvfs2aOOHTsqICBAHh4eWrRoUbFu/2LMnz9fVapU0fHjx91dlXx5eHho7NixhS47ePDgS1uhy8S5/Tp79mx5eHho37591rJWrVppxIgRJV85lCrt27dX+/btrb8v5jPgYrhrvwCAS49gEwAuMecXvu+//z7P9e3bt1ejRo0uej9Lly4t9Bd4FM7LL7+s2bNna9CgQXrvvfd0//33u7tKhTZt2jS+wF0GevfurW3btumll17Se++9pxYtWri7SpKk7OxsjRkzRo8++qgqVqzo7uoU2vr16zV27FilpaW5uyouLtXx+vLLL7stDB85cqSmTp2qlJQUt+wfZdPcuXM1efJkd1cDAFCCyrm7AgCA3JKTk+XpWbR/e1q6dKmmTp1KuFmMVq1apVatWmnMmDEFlr2Q1+xSmjZtmqpVq1bqziKtXbu2Tp48KW9vb3dXpdQ7efKkEhMT9fTTT5e6Mwk///xzJScna+DAge6uynmdPHlS5cr9/+nu+vXr9dxzz6lPnz4KDAx0X8XOcamO15dfflldu3ZV586di3W7hXHnnXfK4XBo2rRpev7550t8/yidLvVnwNy5c7V9+/Zcv7DgswcALl+l5xsYAMDi6+tru8n3iRMn3F2FYnfo0KFChx92fM1K0pkzZ5SVlSUPDw/5+fnJy8vL3VUq9Q4fPixJxRrAFddxOmvWLN1www264oorimV7xhidPHmyWLZ1Nj8/P5dgEyXH09NTXbt21X//+18ZY9xdnUvKLp9/l+o4Kwp3fQbw2QMAly+CTQAohc69PuDp06f13HPPqX79+vLz81PVqlXVunVrrVixQpLUp08fTZ06VdI/k3fnw+nEiRN6/PHHFRYWJl9fX1111VX697//nevL5smTJ/XYY4+pWrVqqlSpku644w79/vvvua6n5rzW4c6dO3XvvfeqcuXKat26tSRp69at6tOnj6688kr5+fkpJCREDz74oP766y+XfTm38eOPP+q+++5TQECAqlevrmeffVbGGB04cMA64yckJESvvvpqrn6aMmWKrrnmGpUvX16VK1dWixYtNHfu3AL799ChQ+rXr5+Cg4Pl5+enpk2bas6cOdb6NWvWyMPDQ3v37tWSJUus/jz7+nHnOvc1c16CYN26dYqPj1f16tVVoUIF3XXXXVZgdfZzb7vtNn3xxRdq1qyZ/Pz81LBhQy1cuDDPPjvXude3q1Onjnbs2KG1a9dadT/7GmdnO336tKpUqaK+ffvmWpeRkSE/Pz898cQTkqSsrCyNHj1aERERCggIUIUKFdSmTRutXr3a5XlnX29y8uTJqlu3rnx9fbVz5848r3NW2DHj9Oeff6pbt25yOByqWrWqhgwZolOnTuVZ9mxpaWkaOnSodRzUq1dPr7zyinJyclzKzZs3TxEREapUqZIcDocaN26s119/vcDtO02aNEm1a9eWv7+/2rVrp+3bt+cqs3v3bnXt2lVVqlSRn5+fWrRooc8++8xaP3bsWNWuXVuSNHz4cHl4eKhOnTrW+s2bN+uWW26Rw+FQxYoV1aFDB3377bcu+3COi7Vr1+qRRx5RUFCQatasaa1ftmyZ2rRpowoVKqhSpUqKjY3Vjh07CmzfqVOnlJCQoOjo6Fzrzpw5oxdeeMF6zevUqaOnnnpKmZmZLuWcY3758uVq0aKF/P39NXPmzDz398Ybb8jLy8vl5+OvvvqqPDw8FB8fby3Lzs5WpUqVNHLkSGvZ2e9dY8eO1fDhwyVJ4eHh+R7XixYtUqNGjeTr66trrrlGCQkJuepUmP6/FMer07///W9df/31qlq1qvz9/RUREaGPP/7YpYyHh4dOnDihOXPmWNs93xmhhT2+i+Lmm2/Wr7/+qi1btpy3XPv27V0+u85+FPQT/WPHjmno0KGqU6eOfH19FRQUpJtvvlmbNm1yKbdhwwbdeuutqly5sipUqKAmTZrkOq5XrVplHROBgYG68847tWvXLpcy5/v8k6T3339fERER8vf3V5UqVdSjRw8dOHCgwL5ybvenn36yzigOCAhQ37599ffff7uUvdjjzPkZN3/+fD333HO64oorVKlSJXXt2lXp6enKzMzU0KFDFRQUpIoVK6pv3765tj1r1izddNNNCgoKkq+vrxo2bKjp06cX2M5zPwOcdcnrcfZ73qeffqrY2FiFhobK19dXdevW1QsvvKDs7GyrTPv27bVkyRL9+uuvubaR3zU2i/KaF+a1AQCUPP4JGwBKSHp6ep43PTl9+nSBzx07dqzGjRun/v37q2XLlsrIyND333+vTZs26eabb9ZDDz2kgwcPasWKFXrvvfdcnmuM0R133KHVq1erX79+atasmZYvX67hw4fr999/16RJk6yyffr00fz583X//ferVatWWrt2rWJjY/Ot1z333KP69evr5ZdftkLSFStW6JdfflHfvn0VEhKiHTt26K233tKOHTv07bff5vqi3717d1199dUaP368lixZohdffFFVqlTRzJkzddNNN+mVV17RBx98oCeeeELXXXed2rZtK0l6++239dhjj6lr165WsLV161Zt2LBB9957b751PnnypNq3b6+ffvpJgwcPVnh4uBYsWKA+ffooLS1NQ4YM0dVXX6333ntPw4YNU82aNfX4449LkqpXr17ga3WuRx99VJUrV9aYMWO0b98+TZ48WYMHD9ZHH33kUm7Pnj3q3r27Hn74YfXu3VuzZs3SPffco4SEBN18881F2ufkyZOtax8+/fTTkqTg4OA8y3p7e+uuu+7SwoULNXPmTPn4+FjrFi1apMzMTPXo0UPSP0HnO++8o549e2rAgAE6duyY/vOf/ygmJkYbN25Us2bNXLY9a9YsnTp1SgMHDpSvr6+qVKmSK0SUij5munXrpjp16mjcuHH69ttv9cYbb+jo0aP673//m2+f/P3332rXrp1+//13PfTQQ6pVq5bWr1+vUaNG6Y8//rCuybZixQr17NlTHTp00CuvvCJJ2rVrl9atW6chQ4acv+Ml/fe//9WxY8cUFxenU6dO6fXXX9dNN92kbdu2Wa/Bjh07rLMdn3zySVWoUEHz589X586d9b///U933XWX7r77bgUGBmrYsGHq2bOnbr31Vutaljt27FCbNm3kcDg0YsQIeXt7a+bMmWrfvr3Wrl2ryMhIlzo98sgjql69ukaPHm2dWfbee++pd+/eiomJ0SuvvKK///5b06dPV+vWrbV582aXQOFcSUlJysrKUvPmzXOt69+/v+bMmaOuXbvq8ccf14YNGzRu3Djt2rVLn3zyiUvZ5ORk9ezZUw899JAGDBigq666Ks/9tWnTRjk5Ofrmm2902223SZK+/vpreXp66uuvv7bKbd68WcePH7feI851991368cff9SHH36oSZMmqVq1apJcj+tvvvlGCxcu1COPPKJKlSrpjTfeUJcuXbR//35VrVr1gvq/IEU5Xp1ef/113XHHHerVq5eysrI0b9483XPPPVq8eLH1nv3ee+9ZnxnOSwbUrVs3320W9fgujIiICEnSunXrdO211+Zb7umnn1b//v1dlr3//vtavny5goKCzruPhx9+WB9//LEGDx6shg0b6q+//tI333yjXbt2WWN0xYoVuu2221SjRg0NGTJEISEh2rVrlxYvXmwd119++aVuueUWXXnllRo7dqxOnjypKVOm6IYbbtCmTZtyHRN5ff699NJLevbZZ9WtWzf1799fhw8f1pQpU9S2bVtt3ry5UGdfd+vWTeHh4Ro3bpw2bdqkd955R0FBQdb7kVR8x9m4cePk7++vJ598Uj/99JOmTJkib29veXp66ujRoxo7dqy+/fZbzZ49W+Hh4Ro9erT13OnTp+uaa67RHXfcoXLlyunzzz/XI488opycHMXFxRXYTifn5+3Z0tLSFB8f7/Laz549WxUrVlR8fLwqVqyoVatWafTo0crIyNDEiRMl/TOO0tPT9dtvv1lzm/NdA7ior3lhXhsAgBsYAMAlNWvWLCPpvI9rrrnG5Tm1a9c2vXv3tv5u2rSpiY2NPe9+4uLiTF5v64sWLTKSzIsvvuiyvGvXrsbDw8P89NNPxhhjkpKSjCQzdOhQl3J9+vQxksyYMWOsZWPGjDGSTM+ePXPt7++//8617MMPPzSSzFdffZVrGwMHDrSWnTlzxtSsWdN4eHiY8ePHW8uPHj1q/P39XfrkzjvvzNVvhTF58mQjybz//vvWsqysLBMVFWUqVqxoMjIyrOW1a9cusN/PLnt2/Zyve3R0tMnJybGWDxs2zHh5eZm0tDSX50oy//vf/6xl6enppkaNGubaa6+1ljn77FzOfe3du9dads0115h27doVqu7Lly83ksznn3/usvzWW281V155pfX3mTNnTGZmpkuZo0ePmuDgYPPggw9ay/bu3WskGYfDYQ4dOuRS3rlu1qxZ1rKijpk77rjDpewjjzxiJJkffvjBWnbu6/HCCy+YChUqmB9//NHluU8++aTx8vIy+/fvN8YYM2TIEONwOMyZM2dy1el8nO3y9/c3v/32m7V8w4YNRpIZNmyYtaxDhw6mcePG5tSpU9aynJwcc/3115v69evn2ubEiRNd9tW5c2fj4+Njfv75Z2vZwYMHTaVKlUzbtm2tZc5x0bp1a5f2HDt2zAQGBpoBAwa4bDclJcUEBATkWn6ud955x0gy27Ztc1m+ZcsWI8n079/fZfkTTzxhJJlVq1ZZy5xjPiEh4bz7MsaY7Oxs43A4zIgRI4wx//RV1apVzT333GO8vLzMsWPHjDHGvPbaa8bT09McPXrUeu65710TJ07MdaycXdbHx8d6TzTGmB9++MFIMlOmTLGWFbb/L9XxakzuYyYrK8s0atTI3HTTTS7LK1So4HIcnE9hj29jcvdrXm1y8vHxMYMGDSpUHZzWrVtnvL29c+03LwEBASYuLi7f9WfOnDHh4eGmdu3aLmPDGOPy3tysWTMTFBRk/vrrL2vZDz/8YDw9Pc0DDzxgLcvv82/fvn3Gy8vLvPTSSy7Lt23bZsqVK5dr+bmc2z23zXfddZepWrWq9XdxHGerV682kkyjRo1MVlaWtbxnz57Gw8PD3HLLLS7lo6KiTO3atV2W5fW+HRMT4/KZYYwx7dq1cxnbeX0GnC0nJ8fcdtttpmLFimbHjh3n3d9DDz1kypcv7/JeGhsbm6uu+e23qK95Qa8NAMA9+Ck6AJSQqVOnasWKFbkeTZo0KfC5gYGB2rFjh/bs2VPk/S5dulReXl567LHHXJY//vjjMsZo2bJlkmT93PKRRx5xKffoo4/mu+2HH3441zJ/f3/r/0+dOqU///xTrVq1kqRcPw2U5HKWjpeXl1q0aCFjjPr162ctDwwM1FVXXaVffvnFZdlvv/2m7777Lt/65WXp0qUKCQlRz549rWXe3t567LHHdPz4ca1du7ZI2yvIwIEDXc44bNOmjbKzs/Xrr7+6lAsNDdVdd91l/e1wOPTAAw9o8+bNl/yuwjfddJOqVavmchbp0aNHtWLFCnXv3t1a5uXlZZ3RmZOToyNHjujMmTNq0aJFnq9tly5dCnWWa1HHzLlnAznH6NKlS/Pdx4IFC9SmTRtVrlxZf/75p/WIjo5Wdna2vvrqK0n/jKsTJ05Yl3koqs6dO7tcd7Jly5aKjIy06nbkyBGtWrVK3bp107Fjx6x6/PXXX4qJidGePXv0+++/57v97OxsffHFF+rcubOuvPJKa3mNGjV077336ptvvlFGRobLcwYMGOByXbkVK1YoLS1NPXv2dOkLLy8vRUZGFvjTY+clAipXruyy3NnGs38eLsk643nJkiUuy8PDwxUTE3PefUn/XKvx+uuvt16jXbt26a+//tKTTz4pY4wSExMl/XMWZ6NGjS7qmqTR0dEuZzU2adJEDofDeu+5kP6/FM4+Zo4ePar09HS1adMmz+OlsIp6fBeW85grrJSUFHXt2lXNmjXTtGnTCiwfGBioDRs26ODBg3mu37x5s/bu3auhQ4fmGhvO9+Y//vhDW7ZsUZ8+fVSlShVrfZMmTXTzzTfn+d5y7uffwoULlZOTo27durkcVyEhIapfv36hf9J/7nbbtGmjv/76yxpXxXmcPfDAAy7Xho6MjJQxRg8++KBLucjISB04cEBnzpyxlp09Bp2/SGnXrp1++eUXpaenF6qteXnhhRe0ePFizZ49Ww0bNsxzf873zjZt2ujvv//W7t27i7yf4njNz31tAADuQbAJACWkZcuWio6OzvU4NxzIy/PPP6+0tDT961//UuPGjTV8+HBt3bq1UPv99ddfFRoaqkqVKrksv/rqq631zv96enoqPDzcpVy9evXy3fa5ZaV/gpshQ4YoODhY/v7+ql69ulUury87tWrVcvk7ICBAfn5+1s9Ez15+9OhR6++RI0eqYsWKatmyperXr6+4uDitW7cu37o6/frrr6pfv36uO5if2x/F5dz2OV/vs9si/dPP5/7k+l//+pcknffansWhXLly6tKliz799FPrOmoLFy7U6dOnXYJNSZozZ46aNGliXeu1evXqWrJkSZ6vbV7jIy9FHTP169d3+btu3bry9PQ8bz/t2bNHCQkJql69usvDeZ3IQ4cOSfon2P/Xv/6lW265RTVr1tSDDz6Y5zUW83Nu3aR/Xkdn3X766ScZY/Tss8/mqsuYMWNc6pKXw4cP6++//87zZ9tXX321cnJycl3P79zXwfkPJDfddFOuOnzxxRfn3f/ZzDnX6HW+h5z7nhESEqLAwMBcx1Zhx4f0T4CQlJSkkydP6uuvv1aNGjXUvHlzNW3a1Po5+jfffKM2bdoUept5Ofd4lf45Zp3H64X0/6WwePFitWrVSn5+fqpSpYqqV6+u6dOnX1SgJBXt+C4sY0ye1xrNy5kzZ9StWzdlZ2dr4cKF8vX1lfRPoJySkuLyyMrKkiRNmDBB27dvV1hYmFq2bKmxY8e6/CPYzz//LElq1KhRvvt1js38Xtc///wz1w2C8jqujDGqX79+ruNq165dhT6uCvrMKM7jLK/PX0kKCwvLtTwnJ8dlHKxbt07R0dHWtSmrV6+up556SlLe79uFkZCQoOeee06jRo1Sly5dXNbt2LFDd911lwICAuRwOFS9enXdd999F7y/C3nNC/t5DgAoWVxjEwBsoG3btvr555/16aef6osvvtA777yjSZMmacaMGbmuS1aSzj6Dwqlbt25av369hg8frmbNmqlixYrKyclRp06d8ry+Yl53KM3vrqVnBylXX321kpOTtXjxYiUkJOh///ufpk2bptGjR+u55567iFYVr8K0pbDyCwfOvnnCherRo4dmzpypZcuWqXPnzpo/f74aNGigpk2bWmXef/999enTR507d9bw4cMVFBQkLy8vjRs3zgoPzpbX+MhLUcfMuQoTmuTk5Ojmm2/WiBEj8lzvDJGDgoK0ZcsWLV++XMuWLdOyZcs0a9YsPfDAAy43mLpQzvY88cQT+Z5Fdb5/TLgQ574Ozjq89957CgkJyVW+oLuIO681efToUZebETkVNsQq7PiQpNatW+v06dNKTEzU119/bQWYbdq00ddff63du3fr8OHDFx1s2uF4/frrr3XHHXeobdu2mjZtmmrUqCFvb2/NmjWrUDdPy09Rj+/CSktLy/UPVfkZPny4EhMT9eWXX7qMrQMHDuQK6FavXq327durW7duatOmjT755BN98cUXmjhxol555RUtXLhQt9xyywXXuyB5HVceHh5atmxZnuPofNd6PFthx2BxHGf57augOvz888/q0KGDGjRooNdee01hYWHy8fHR0qVLNWnSpEK9b59r79696tWrl26++Wa9+OKLLuvS0tLUrl07ORwOPf/886pbt678/Py0adMmjRw58oL2dyGK8/0BAFB8CDYBwCacd67u27evdYOMsWPHWsFmfl9yateurS+//FLHjh1zOWvT+dMt552Xa9eurZycHO3du9flrLOffvqp0HU8evSoVq5cqeeee87lJgMX8hP6wqhQoYK6d++u7t27KysrS3fffbdeeukljRo1Sn5+fnk+p3bt2tq6datycnJczto8tz9KmvNMvrNfxx9//FGSrBsYOM8OSUtLc/lJZV5nmRb2S69T27ZtVaNGDX300Udq3bq1Vq1aZd3IxOnjjz/WlVdeqYULF7ps33mm4YW4kDGzZ88el5Djp59+Uk5OznlveFO3bl0dP348zzt5n8vHx0e33367br/9duXk5OiRRx7RzJkz9eyzzxYYOuZV7x9//NGqm/Pny97e3oWqy7mqV6+u8uXLKzk5Ode63bt3y9PTM9fZVudy/tQ6KCjogurQoEEDSf8EEY0bN7aWO99D9uzZY50BLUmpqalKS0u7qGOrZcuW8vHx0ddff62vv/7aurt527Zt9fbbb2vlypXW3+dT1OPiXEXp/0t1vP7vf/+Tn5+fli9fbp3RKP1zs66L2e6lOL5///13ZWVluYyH/MybN0+TJ0/W5MmT1a5dO5d1ISEhuS4PcfY/utSoUUOPPPKIHnnkER06dEjNmzfXSy+9pFtuucUa79u3b893vDvHZn6va7Vq1VShQoXz1r9u3boyxig8PNz6h5JL4VIeZ4X1+eefKzMzU5999pnLWYyF/bn9uU6ePGndMO3DDz/M9YuKNWvW6K+//tLChQtdjvG9e/fm2lZhx3xxvOYAgNKBn6IDgA04r2nnVLFiRdWrV8/62bAkawKelpbmUvbWW29Vdna23nzzTZflkyZNkoeHh3VGi/PssXOvaTZlypRC19N5NsO5Zy847zhdnM7tEx8fHzVs2FDGmPPeaf7WW29VSkqKy/Ukz5w5oylTpqhixYq5vlCXlIMHD7rczTYjI0P//e9/1axZM+usOucXdOe1BiXpxIkTeZ5JWKFChVxj4Xw8PT3VtWtXff7553rvvfd05syZXD9Dz+v13bBhg3WNwwtxIWNm6tSpLn87x+j5zs7q1q2bEhMTtXz58lzr0tLSrGvHnTuuPD09revgnn285WfRokUu18jcuHGjNmzYYNUtKChI7du318yZM/XHH3/kev7hw4fPu30vLy917NhRn376qctP71NTUzV37ly1bt1aDofjvNuIiYmRw+HQyy+/nOexUlAdIiIi5OPjo++//95l+a233iop92v32muvSZJ1t+4L4efnp+uuu04ffvih9u/f73LG5smTJ/XGG2+obt26qlGjxnm3k9/7ZGEVpf8v1fHq5eUlDw8PlzM/9+3bp0WLFl30dqXiPb6TkpIkSddff/15y23fvl39+/fXfffdZ92l/Gx+fn55XsYlOzs718+Qg4KCFBoaah2vzZs3V3h4uCZPnpyrL5xtrVGjhpo1a6Y5c+a4lNm+fbu++OILa2yfz9133y0vLy8999xzud7PjDG53lsu1KU8zgorr7GSnp6eZ7heGA8//LB+/PFHffLJJ3leniev/WVlZeV5DdYKFSoU6qfpxfGaAwBKB87YBAAbaNiwodq3b6+IiAhVqVJF33//vT7++GMNHjzYKhMRESFJeuyxxxQTEyMvLy/16NFDt99+u2688UY9/fTT2rdvn5o2baovvvhCn376qYYOHWp9+Y6IiFCXLl00efJk/fXXX2rVqpXWrl1rnTVYmLMgHA6H2rZtqwkTJuj06dO64oor9MUXX+R5VsXF6tixo0JCQnTDDTcoODhYu3bt0ptvvqnY2Nhc1xM928CBAzVz5kz16dNHSUlJqlOnjj7++GOtW7dOkydPPu9zL6V//etf6tevn7777jsFBwfr3XffVWpqqssXxY4dO6pWrVrq16+fhg8fLi8vL7377ruqXr269u/f77K9iIgITZ8+XS+++KLq1aunoKAg3XTTTeetQ/fu3TVlyhSNGTNGjRs3znWW1W233aaFCxfqrrvuUmxsrPbu3asZM2aoYcOGOn78+AW1+0LGzN69e3XHHXeoU6dOSkxM1Pvvv697773X5Qyucw0fPlyfffaZbrvtNvXp00cRERE6ceKEtm3bpo8//lj79u1TtWrV1L9/fx05ckQ33XSTatasqV9//VVTpkxRs2bNCnXWWb169dS6dWsNGjRImZmZmjx5sqpWreryE/ipU6eqdevWaty4sQYMGKArr7xSqampSkxM1G+//aYffvjhvPt48cUXtWLFCrVu3VqPPPKIypUrp5kzZyozM1MTJkwosI4Oh0PTp0/X/fffr+bNm6tHjx7WGFqyZIluuOGGXP8QcjY/Pz917NhRX375pZ5//nlredOmTdW7d2+99dZb1k9HN27cqDlz5qhz58668cYbC6zb+bRp00bjx49XQECAdaZoUFCQrrrqKiUnJ6tPnz4FbsP5Pvn000+rR48e8vb21u23316kM7MK2/+X6niNjY3Va6+9pk6dOunee+/VoUOHNHXqVNWrVy/XtZcjIiL05Zdf6rXXXlNoaKjCw8MVGRmZ53YvxfG9YsUK1apVS9dee+15y/Xt21fSP2fcvv/++y7rrr/+epcbNZ3t2LFjqlmzprp27aqmTZuqYsWK+vLLL/Xdd9/p1VdflfTPP05Mnz5dt99+u5o1a6a+ffuqRo0a2r17t3bs2GH9Y8fEiRN1yy23KCoqSv369dPJkyc1ZcoUBQQEaOzYsQW2tW7dunrxxRc1atQo7du3T507d1alSpW0d+9effLJJxo4cKCeeOKJArdTkEt9nBVGx44drTPbH3roIR0/flxvv/22goKC8vwHm/NZsmSJ/vvf/6pLly7aunWryxiuWLGiOnfurOuvv16VK1dW79699dhjj8nDw0Pvvfdenj8Bj4iI0EcffaT4+Hhdd911qlixom6//fY8932xrzkAoJQoobuvA0CZNWvWLCPJfPfdd3mub9eunbnmmmtcltWuXdv07t3b+vvFF180LVu2NIGBgcbf3980aNDAvPTSSyYrK8sqc+bMGfPoo4+a6tWrGw8PD3P2W/yxY8fMsGHDTGhoqPH29jb169c3EydONDk5OS77PXHihImLizNVqlQxFStWNJ07dzbJyclGkhk/frxVbsyYMUaSOXz4cK72/Pbbb+auu+4ygYGBJiAgwNxzzz3m4MGDRpIZM2ZMgdvo3bu3qVChQoH9NHPmTNO2bVtTtWpV4+vra+rWrWuGDx9u0tPT8+zns6Wmppq+ffuaatWqGR8fH9O4cWMza9asXOVq165tYmNjC9yes+zZr1l+r/vq1auNJLN69epc+1m+fLlp0qSJ8fX1NQ0aNDALFizItZ+kpCQTGRlpfHx8TK1atcxrr71m7Wvv3r1WuZSUFBMbG2sqVapkJJl27doV2IacnBwTFhZmJJkXX3wxz/Uvv/yyqV27tvH19TXXXnutWbx4sendu7epXbu2VW7v3r1Gkpk4cWKubTjXnd3fRR0zO3fuNF27djWVKlUylStXNoMHDzYnT5502c+5r4cx/xwHo0aNMvXq1TM+Pj6mWrVq5vrrrzf//ve/rWPp448/Nh07djRBQUFWHz/00EPmjz/+OG/fnd3mV1991YSFhRlfX1/Tpk0b88MPP+Qq//PPP5sHHnjAhISEGG9vb3PFFVeY2267zXz88ceF6sdNmzaZmJgYU7FiRVO+fHlz4403mvXr17uUKei9Z/Xq1SYmJsYEBAQYPz8/U7duXdOnTx/z/fffn7etxhizcOFC4+HhYfbv3++y/PTp0+a5554z4eHhxtvb24SFhZlRo0aZU6dOuZQryrHltGTJEiPJ3HLLLS7L+/fvbySZ//znP7mec+4YMsaYF154wVxxxRXG09PT5biRZOLi4nJtI6+xVJj+N+bSHa//+c9/TP369a33ilmzZlnHx9l2795t2rZta/z9/Y2kXO04W2GPb2dfnd2vebUpOzvb1KhRwzzzzDPnbYsx//SxpDwfeb03O2VmZprhw4ebpk2bmkqVKpkKFSqYpk2bmmnTpuUq+80335ibb77ZKtekSRMzZcoUlzJffvmlueGGG4y/v79xOBzm9ttvNzt37nQpc77PP2OM+d///mdat25tKlSoYCpUqGAaNGhg4uLiTHJy8nn7IL/t5tW3F3ucOT+Hzv2Mye89I6+6ffbZZ6ZJkybGz8/P1KlTx7zyyivm3XffzVXXdu3auYzncz8DnPvM63H2uFu3bp1p1aqV8ff3N6GhoWbEiBFm+fLluT5Pjx8/bu69914TGBjoso28PnuMubjXPK/XBgBQ8jyM4WrHAID8bdmyRddee63ef/999erVy93VuSzVqVNHjRo10uLFi91dFaBQsrOz1bBhQ3Xr1k0vvPCCu6uDUmjRokW699579fPPPxd4iQAAAIALxTU2AQCWkydP5lo2efJkeXp6FnhTDgBlh5eXl55//nlNnTr1gn+mjMvbK6+8osGDBxNqAgCAS4prbAIALBMmTFBSUpJuvPFGlStXTsuWLdOyZcs0cODAAu+0DKBs6d69e64bTAFOF3PTIQAAgMIi2AQAWK6//nqtWLFCL7zwgo4fP65atWpp7Nixevrpp91dNQAAAAAAXHCNTQAAAAAAAAC2wzU2AQAAAAAAANgOwSYAAAAAAAAA2yHYBAAAAAAAAGA7BJsAAAAAAAAAbIdgEwAAAAAAAIDtEGwCAAAAAAAAsB2CTQAAAAAAAAC2Q7AJAAAAAAAAwHYINgEAAAAAAADYDsEmAAAAAAAAANsh2AQAAAAAAABgOwSbAAAAAAAAAGyHYBMAAAAAAACA7RBsAgAAAAAAALAdgk0AAAAAAAAAtkOwCQAAAAAAAMB2CDYBAAAAAAAA2A7BJgAAAAAAAADbIdgEAAAAAAAAYDsEmwAAAAAAAABsh2ATAAAAAAAAgO0QbAIAAAAAAACwHYJNAAAAAAAAALZDsAkAAAAAAADAdgg2AQAAAAAAANgOwSYAAAAAAAAA2yHYBAAAAAAAAGA7BJsAAAAAAAAAbIdgEwAAAAAAAIDtEGwCAAAAAAAAsB2CTQAAAAAAAAC2Q7AJAAAAAAAAwHYINgEAAAAAAADYDsEmAAAAAAAAANsh2AQAAAAAAABgOwSbAAAAAAAAAGyHYBMAAAAAAACA7RBsAgAAAAAAALAdgk0AAAAAAAAAtkOwCQAAAAAAAMB2CDYBAAAAAAAA2A7BJgAAAAAAAADbIdgEAAAAAAAAYDsEmwAAAAAAAABsh2ATAAAAAAAAgO0QbAIAAAAAAACwHYJNAAAAAAAAALZDsAkAAAAAAADAdgg2AQAAAAAAANgOwSYAAAAAAAAA2yHYBAAAAAAAAGA7BJsAAAAAAAAAbIdgEwAAAAAAAIDtEGwCAAAAAAAAsB2CTQAAAAAAAAC2Q7AJAAAAAAAAwHYINgEAAAAAAADYDsEmAAAAAAAAANsh2AQAAAAAAABgOwSbAAAAAAAAAGyHYBMAAAAAAACA7RBsAgAAAAAAALAdgk0AAAAAAAAAtkOwCQAAAAAAAMB2CDYBAAAAAAAA2A7BJgAAAAAAAADbIdgEAAAAAAAAYDsEmwAAAAAAAABsh2ATAAAAAAAAgO0QbAIAAAAAAACwHYJNAAAAAAAAALZDsAkAAAAAAADAdgg2AQAAAAAAANgOwSYAAAAAAAAA2yHYBAAAAAAAAGA7BJsAAAAAAAAAbIdgEwAAAAAAAIDtEGwCAAAAAAAAsB2CTQAAAAAAAAC2Q7AJAAAAAAAAwHYINgEAAAAAAADYDsEmAAAAAAAAANsh2AQAAAAAAABgOwSbAAAAAAAAAGyHYBMAAAAAAACA7RBsAgAAAAAAALAdgk0AAAAAAAAAtkOwCQAAAAAAAMB2CDYBAAAAAAAA2A7BJgAAAAAAAADbIdgEAAAAAAAAYDsEmwAAAAAAAABsh2ATAAAAAAAAgO0QbAIAAAAAAACwHYJNAAAAAAAAALZDsAkAAAAAAADAdgg2AQAAAAAAANgOwSYAAAAAAAAA2yHYBAAAAAAAAGA7BJsAAAAAAAAAbIdgEwAAAAAAAIDtEGwCAAAAAAAAsB2CTQAAAAAAAAC2Q7AJAAAAAAAAwHYINgEAAAAAAADYDsEmAAAAAAAAANsh2AQAAAAAAABgOwSbAAAAAAAAAGyHYBMAAAAAAACA7RBsAgAAAAAAALAdgk0AAAAAAAAAtkOwCQAAAAAAAMB2CDYBAAAAAAAA2A7BJgAAAAAAAADbIdgEAAAAAAAAYDsEmwAAAAAAAABsh2ATAAAAAAAAgO0QbAIAAAAAAACwHYJNAAAAAAAAALZDsAkAAAAAAADAdgg2AQAAAAAAANgOwSYAAAAAAAAA2yHYBAAAAAAAAGA7BJsAAAAAAAAAbIdgEwAAAAAAAIDtEGwCAAAAAAAAsB2CTQAAAAAAAAC2Q7AJAAAAAAAAwHYINgEAAAAAAADYDsEmAAAAAAAAANsh2AQAAAAAAABgOwSbAAAAAAAAAGyHYBMAAAAAAACA7RBsAgAAAAAAALAdgk0AAAAAAAAAtkOwCQAAAAAAAMB2CDYBAAAAAAAA2A7BJgAAAAAAAADbIdgEAAAAAAAAYDsEmwAAAAAAAABsh2ATAAAAAAAAgO0QbAIAAAAAAACwHYJNAAAAAAAAALZDsAkAAAAAAADAdgg2AQAAAAAAANgOwSYAAAAAAAAA2yHYBAAAAAAAAGA7BJsAAAAAAAAAbIdgEwAAAAAAAIDtEGwCAAAAAAAAsB2CTQAAAAAAAAC2Q7AJAAAAAAAAwHYINgEAAAAAAADYDsEmAAAAAAAAANsh2AQAAAAAAABgOwSbAAAAAAAAAGyHYBMAAAAAAACA7RBsAgAAAAAAALAdgk0AAAAAAAAAtkOwCQAAAAAAAMB2CDYBAAAAAAAA2A7BJgAAAAAAAADbIdgEAAAAAAAAYDsEmwAAAAAAAABsh2ATAAAAAAAAgO0QbAIAAAAAAACwHYJNAAAAAAAAALZDsAkAAAAAAADAdgg2AQAAAAAAANgOwSYAAAAAAAAA2yHYBAAAAAAAAGA7BJsAAAAAAAAAbIdgEwAAAAAAAIDtEGwCAAAAAAAAsB2CTQAAAAAAAAC2Q7AJAAAAAAAAwHYINgEAAAAAAADYDsEmAAAAAAAAANsh2AQAAAAAAABgOwSbAAAAAAAAAGyHYBMAAAAAAACA7RBsAgAAAAAAALAdgk0AAAAAAAAAtkOwCQAAAAAAAMB2CDYBAAAAAAAA2A7BJgAAAAAAAADbIdgEAAAAAAAAYDsEmwAAAAAAAABsh2ATAAAAAAAAgO0QbAIAAAAAAACwHYJNAAAAAAAAALZDsAkAAAAAAADAdgg2AQAAAAAAANgOwSYAAAAAAAAA2yHYBAAAAAAAAGA7BJsAAAAAAAAAbIdgEwAAAAAAAIDtEGwCAAAAAAAAsB2CTQAAAAAAAAC2Q7AJAAAAAAAAwHYINgEAAAAAAADYDsEmAAAAAAAAANsh2AQAAAAAAABgOwSbAAAAAAAAAGyHYBMAAAAAAACA7RBsAgAAAAAAALAdgk0AAAAAAAAAtkOwCQAAAAAAAMB2CDYBAAAAAAAA2A7BJgAAAAAAAADbIdgEAAAAAAAAYDsEmwAAAAAAAABsh2ATAAAAAAAAgO0QbAIAAAAAAACwHYJNAAAAAAAAALZDsAkAAAAAAADAdgg2AQAAAAAAANgOwSYAAAAAAAAA2yHYBAAAAAAAAGA7BJsAAAAAAAAAbKecuyvgTjk5OTp48KAqVaokDw8Pd1cHAAC4gTFGx44dU2hoqDw9+TdfoLCYSwMAAMm98+kyHWwePHhQYWFh7q4GAAAoBQ4cOKCaNWu6uxqAbTCXBgAAZ3PHfLpMB5uVKlWS9E/HOxwON9cGAAC4Q0ZGhsLCwqx5AYDCYS4NAAAk986ny3Sw6fzJjMPhYDIGAEAZx09pgaJhLg0AAM7mjvk0F5ICAAAAAAAAYDsEmwAAAAAAAABsh2ATAAAAAAAAgO2U6WtsOjUas1yevuXPW2bf+NgSqg0AAABgH8ylAQCAu3DGJgAAAAAAAADbIdgEAAAAAAAAYDsEmwAAAAAAAABsh2ATAAAAAAAAgO0QbAIAAAAAAACwHYJNAAAAAAAAALZDsAkAAAAAAADAdgg2AQAAAAAAANgOwSYAAAAAAAAA2yHYBAAAAAAAAGA7BJsAAAAAAAAAbKfIwebvv/+u++67T1WrVpW/v78aN26s77//3lpvjNHo0aNVo0YN+fv7Kzo6Wnv27HHZxpEjR9SrVy85HA4FBgaqX79+On78uEuZrVu3qk2bNvLz81NYWJgmTJiQqy4LFixQgwYN5Ofnp8aNG2vp0qVFbQ4AAABQYphLAwAAFJ8iBZtHjx7VDTfcIG9vby1btkw7d+7Uq6++qsqVK1tlJkyYoDfeeEMzZszQhg0bVKFCBcXExOjUqVNWmV69emnHjh1asWKFFi9erK+++koDBw601mdkZKhjx46qXbu2kpKSNHHiRI0dO1ZvvfWWVWb9+vXq2bOn+vXrp82bN6tz587q3Lmztm/ffjH9AQAAAFwSzKUBAACKl4cxxhS28JNPPql169bp66+/znO9MUahoaF6/PHH9cQTT0iS0tPTFRwcrNmzZ6tHjx7atWuXGjZsqO+++04tWrSQJCUkJOjWW2/Vb7/9ptDQUE2fPl1PP/20UlJS5OPjY+170aJF2r17tySpe/fuOnHihBYvXmztv1WrVmrWrJlmzJhRqPZkZGQoICBAYUPny9O3/HnL7hsfW6htAgAAe3HOB9LT0+VwONxdHVzGmEsDAIDLkTvn00U6Y/Ozzz5TixYtdM899ygoKEjXXnut3n77bWv93r17lZKSoujoaGtZQECAIiMjlZiYKElKTExUYGCgNRGTpOjoaHl6emrDhg1WmbZt21oTMUmKiYlRcnKyjh49apU5ez/OMs79AAAAAKUJc2kAAIDiVaRg85dfftH06dNVv359LV++XIMGDdJjjz2mOXPmSJJSUlIkScHBwS7PCw4OttalpKQoKCjIZX25cuVUpUoVlzJ5bePsfeRXxrk+L5mZmcrIyHB5AAAAACWBuTQAAEDxKleUwjk5OWrRooVefvllSdK1116r7du3a8aMGerdu/clqWBxGjdunJ577jl3VwMAAABlEHNpAACA4lWkMzZr1Kihhg0buiy7+uqrtX//fklSSEiIJCk1NdWlTGpqqrUuJCREhw4dcll/5swZHTlyxKVMXts4ex/5lXGuz8uoUaOUnp5uPQ4cOFBwowEAAIBiwFwaAACgeBUp2LzhhhuUnJzssuzHH39U7dq1JUnh4eEKCQnRypUrrfUZGRnasGGDoqKiJElRUVFKS0tTUlKSVWbVqlXKyclRZGSkVearr77S6dOnrTIrVqzQVVddZd01MioqymU/zjLO/eTF19dXDofD5QEAAACUBObSAAAAxatIweawYcP07bff6uWXX9ZPP/2kuXPn6q233lJcXJwkycPDQ0OHDtWLL76ozz77TNu2bdMDDzyg0NBQde7cWdI//yrdqVMnDRgwQBs3btS6des0ePBg9ejRQ6GhoZKke++9Vz4+PurXr5927Nihjz76SK+//rri4+OtugwZMkQJCQl69dVXtXv3bo0dO1bff/+9Bg8eXExdAwAAABQf5tIAAADFq0jX2Lzuuuv0ySefaNSoUXr++ecVHh6uyZMnq1evXlaZESNG6MSJExo4cKDS0tLUunVrJSQkyM/PzyrzwQcfaPDgwerQoYM8PT3VpUsXvfHGG9b6gIAAffHFF4qLi1NERISqVaum0aNHa+DAgVaZ66+/XnPnztUzzzyjp556SvXr19eiRYvUqFGji+kPAAAA4JJgLg0AAFC8PIwxxt2VcJeMjAwFBAQobOh8efqWP2/ZfeNjS6hWAACgJDnnA+np6fy0FigC5tIAAEBy73y6SD9FBwAAAAAAAIDSgGATAAAAAAAAgO0QbAIAAAAAAACwHYJNAAAAAAAAALZDsAkAAAAAAADAdgg2AQAAAAAAANgOwSYAAAAAAAAA2yHYBAAAAAAAAGA7BJsAAAAAAAAAbIdgEwAAAAAAAIDtEGwCAAAAAAAAsB2CTQAAAAAAAAC2Q7AJAAAAAAAAwHYINgEAAAAAAADYDsEmAAAAAAAAANsh2AQAAAAAAABgOwSbAAAAAAAAAGyHYBMAAAAAAACA7RBsAgAAAAAAALAdgk0AAAAAAAAAtkOwCQAAAAAAAMB2CDYBAAAAAAAA2A7BJgAAAAAAAADbIdgEAAAAAAAAYDsEmwAAAAAAAABs56KCzfHjx8vDw0NDhw61lp06dUpxcXGqWrWqKlasqC5duig1NdXlefv371dsbKzKly+voKAgDR8+XGfOnHEps2bNGjVv3ly+vr6qV6+eZs+enWv/U6dOVZ06deTn56fIyEht3LjxYpoDAAAAlBjm0gAAABfngoPN7777TjNnzlSTJk1clg8bNkyff/65FixYoLVr1+rgwYO6++67rfXZ2dmKjY1VVlaW1q9frzlz5mj27NkaPXq0VWbv3r2KjY3VjTfeqC1btmjo0KHq37+/li9fbpX56KOPFB8frzFjxmjTpk1q2rSpYmJidOjQoQttEgAAAFAimEsDAABcPA9jjCnqk44fP67mzZtr2rRpevHFF9WsWTNNnjxZ6enpql69uubOnauuXbtKknbv3q2rr75aiYmJatWqlZYtW6bbbrtNBw8eVHBwsCRpxowZGjlypA4fPiwfHx+NHDlSS5Ys0fbt26199ujRQ2lpaUpISJAkRUZG6rrrrtObb74pScrJyVFYWJgeffRRPfnkk4VqR0ZGhgICAhQ2dL48fcuft+y+8bFF7SYAAGADzvlAenq6HA6Hu6uDMoC5NAAAuJy4cz59QWdsxsXFKTY2VtHR0S7Lk5KSdPr0aZflDRo0UK1atZSYmChJSkxMVOPGja2JmCTFxMQoIyNDO3bssMqcu+2YmBhrG1lZWUpKSnIp4+npqejoaKsMAAAAUBoxlwYAACge5Yr6hHnz5mnTpk367rvvcq1LSUmRj4+PAgMDXZYHBwcrJSXFKnP2RMy53rnufGUyMjJ08uRJHT16VNnZ2XmW2b17d751z8zMVGZmpvV3RkZGAa0FAAAAig9zaQAAgOJTpDM2Dxw4oCFDhuiDDz6Qn5/fparTJTNu3DgFBARYj7CwMHdXCQAAAGUEc2kAAIDiVaRgMykpSYcOHVLz5s1Vrlw5lStXTmvXrtUbb7yhcuXKKTg4WFlZWUpLS3N5XmpqqkJCQiRJISEhue7s6Py7oDIOh0P+/v6qVq2avLy88izj3EZeRo0apfT0dOtx4MCBojQfAAAAuGDMpQEAAIpXkYLNDh06aNu2bdqyZYv1aNGihXr16mX9v7e3t1auXGk9Jzk5Wfv371dUVJQkKSoqStu2bXO54+KKFSvkcDjUsGFDq8zZ23CWcW7Dx8dHERERLmVycnK0cuVKq0xefH195XA4XB4AAABASWAuDQAAULyKdI3NSpUqqVGjRi7LKlSooKpVq1rL+/Xrp/j4eFWpUkUOh0OPPvqooqKi1KpVK0lSx44d1bBhQ91///2aMGGCUlJS9MwzzyguLk6+vr6SpIcfflhvvvmmRowYoQcffFCrVq3S/PnztWTJEmu/8fHx6t27t1q0aKGWLVtq8uTJOnHihPr27XtRHQIAAABcCsylAQAAileRbx5UkEmTJsnT01NdunRRZmamYmJiNG3aNGu9l5eXFi9erEGDBikqKkoVKlRQ79699fzzz1tlwsPDtWTJEg0bNkyvv/66atasqXfeeUcxMTFWme7du+vw4cMaPXq0UlJS1KxZMyUkJOS6CDoAAABgF8ylAQAACs/DGGPcXQl3ycjI+OfC50Pny9O3/HnL7hsfW0K1AgAAJck5H0hPT+entUARMJcGAACSe+fTRbrGJgAAAAAAAACUBgSbAAAAAAAAAGyHYBMAAAAAAACA7RBsAgAAAAAAALAdgk0AAAAAAAAAtkOwCQAAAAAAAMB2CDYBAAAAAAAA2A7BJgAAAAAAAADbIdgEAAAAAAAAYDsEmwAAAAAAAABsh2ATAAAAAAAAgO0QbAIAAAAAAACwHYJNAAAAAAAAALZDsAkAAAAAAADAdgg2AQAAAAAAANgOwSYAAAAAAAAA2yHYBAAAAAAAAGA7BJsAAAAAAAAAbIdgEwAAAAAAAIDtEGwCAAAAAAAAsB2CTQAAAAAAAAC2Q7AJAAAAAAAAwHYINgEAAAAAAADYDsEmAAAAAAAAANsh2AQAAAAAAABgO0UKNseNG6frrrtOlSpVUlBQkDp37qzk5GSXMqdOnVJcXJyqVq2qihUrqkuXLkpNTXUps3//fsXGxqp8+fIKCgrS8OHDdebMGZcya9asUfPmzeXr66t69epp9uzZueozdepU1alTR35+foqMjNTGjRuL0hwAAAAAAAAANlWkYHPt2rWKi4vTt99+qxUrVuj06dPq2LGjTpw4YZUZNmyYPv/8cy1YsEBr167VwYMHdffdd1vrs7OzFRsbq6ysLK1fv15z5szR7NmzNXr0aKvM3r17FRsbqxtvvFFbtmzR0KFD1b9/fy1fvtwq89FHHyk+Pl5jxozRpk2b1LRpU8XExOjQoUMX0x8AAADAJcFJAgAAAMWrSMFmQkKC+vTpo2uuuUZNmzbV7NmztX//fiUlJUmS0tPT9Z///EevvfaabrrpJkVERGjWrFlav369vv32W0nSF198oZ07d+r9999Xs2bNdMstt+iFF17Q1KlTlZWVJUmaMWOGwsPD9eqrr+rqq6/W4MGD1bVrV02aNMmqy2uvvaYBAwaob9++atiwoWbMmKHy5cvr3XffLa6+AQAAAIoNJwkAAAAUr4u6xmZ6erokqUqVKpKkpKQknT59WtHR0VaZBg0aqFatWkpMTJQkJSYmqnHjxgoODrbKxMTEKCMjQzt27LDKnL0NZxnnNrKyspSUlORSxtPTU9HR0VYZAAAAoDThJAEAAIDidcHBZk5OjoYOHaobbrhBjRo1kiSlpKTIx8dHgYGBLmWDg4OVkpJilTk71HSud647X5mMjAydPHlSf/75p7Kzs/Ms49xGXjIzM5WRkeHyAAAAANzBbicJMJcGAAClzQUHm3Fxcdq+fbvmzZtXnPW5pMaNG6eAgADrERYW5u4qAQAAoAyy40kCzKUBAEBpc0HB5uDBg7V48WKtXr1aNWvWtJaHhIQoKytLaWlpLuVTU1MVEhJilTn3AujOvwsq43A45O/vr2rVqsnLyyvPMs5t5GXUqFFKT0+3HgcOHChawwEAAIBiYMeTBJhLAwCA0qZIwaYxRoMHD9Ynn3yiVatWKTw83GV9RESEvL29tXLlSmtZcnKy9u/fr6ioKElSVFSUtm3b5nJh8hUrVsjhcKhhw4ZWmbO34Szj3IaPj48iIiJcyuTk5GjlypVWmbz4+vrK4XC4PAAAAICSZNeTBJhLAwCA0qZIwWZcXJzef/99zZ07V5UqVVJKSopSUlJ08uRJSVJAQID69eun+Ph4rV69WklJSerbt6+ioqLUqlUrSVLHjh3VsGFD3X///frhhx+0fPlyPfPMM4qLi5Ovr68k6eGHH9Yvv/yiESNGaPfu3Zo2bZrmz5+vYcOGWXWJj4/X22+/rTlz5mjXrl0aNGiQTpw4ob59+xZX3wAAAADFxu4nCQAAAJQ25YpSePr06ZKk9u3buyyfNWuW+vTpI0maNGmSPD091aVLF2VmZiomJkbTpk2zynp5eWnx4sUaNGiQoqKiVKFCBfXu3VvPP/+8VSY8PFxLlizRsGHD9Prrr6tmzZp65513FBMTY5Xp3r27Dh8+rNGjRyslJUXNmjVTQkJCrmsFAQAAAKVBXFyc5s6dq08//dQ6SUD65+QAf39/l5MEqlSpIofDoUcffTTfkwQmTJiglJSUPE8SePPNNzVixAg9+OCDWrVqlebPn68lS5ZYdYmPj1fv3r3VokULtWzZUpMnT+YkAQAAYDsexhjj7kq4S0ZGxj8XPh86X56+5c9bdt/42BKqFQAAKEnO+UB6ejo/rcUl5eHhkefys08SOHXqlB5//HF9+OGHLicJnP0T8V9//VWDBg3SmjVrrJMExo8fr3Ll/v85C2vWrNGwYcO0c+dO1axZU88++6y1D6c333xTEydOtE4SeOONNxQZGVno9jCXBgAAknvn0wSbTMYAACjTCDaBC8NcGgAASO6dT1/QXdEBAAAAAAAAwJ0INgEAAAAAAADYDsEmAAAAAAAAANsh2AQAAAAAAABgOwSbAAAAAAAAAGyHYBMAAAAAAACA7RBsAgAAAAAAALAdgk0AAAAAAAAAtkOwCQAAAAAAAMB2CDYBAAAAAAAA2A7BJgAAAAAAAADbIdgEAAAAAAAAYDsEmwAAAAAAAABsh2ATAAAAAAAAgO0QbAIAAAAAAACwHYJNAAAAAAAAALZDsAkAAAAAAADAdgg2AQAAAAAAANgOwSYAAAAAAAAA2yHYBAAAAAAAAGA7BJsAAAAAAAAAbIdgEwAAAAAAAIDtEGwCAAAAAAAAsB2CTQAAAAAAAAC2Q7AJAAAAAAAAwHZsH2xOnTpVderUkZ+fnyIjI7Vx40Z3VwkAAACwBebSAADAzmwdbH700UeKj4/XmDFjtGnTJjVt2lQxMTE6dOiQu6sGAAAAlGrMpQEAgN3ZOth87bXXNGDAAPXt21cNGzbUjBkzVL58eb377rvurhoAAABQqjGXBgAAdlfO3RW4UFlZWUpKStKoUaOsZZ6enoqOjlZiYmKez8nMzFRmZqb1d3p6uiQpJ/PvAveXkZFxkTUGAAClkfMz3hjj5poAJYe5NAAAKC7unE/bNtj8888/lZ2dreDgYJflwcHB2r17d57PGTdunJ577rlcy3+f3qfA/QVMvpBaAgAAuzh27JgCAgLcXQ2gRDCXBgAAxc0d82nbBpsXYtSoUYqPj7f+zsnJ0ZEjR1S1alV5eHi4sWalR0ZGhsLCwnTgwAE5HA53V6dUoE9yo09yo0/yRr/kRp/k5u4+Mcbo2LFjCg0NLfF9A3Zy7lw6LS1NtWvX1v79+8vkPwq4+73L3Wg/7S+r7S/LbZdoP+3Pu/3unE/bNtisVq2avLy8lJqa6rI8NTVVISEheT7H19dXvr6+LssCAwMvVRVtzeFwlMmD9Hzok9zok9zok7zRL7nRJ7m5s0/KYiiDsq245tLSP8dPWX4/K+vv57Sf9pfV9pfltku0n/bnbr+75tO2vXmQj4+PIiIitHLlSmtZTk6OVq5cqaioKDfWDAAAACjdmEsDAIDLgW3P2JSk+Ph49e7dWy1atFDLli01efJknThxQn379nV31QAAAIBSjbk0AACwO1sHm927d9fhw4c1evRopaSkqFmzZkpISMh1EXQUnq+vr8aMGZPnz4zKKvokN/okN/okb/RLbvRJbvQJ4B4XO5cu68cu7af9tL9str8st12i/bS/9LXfw7jjXuwAAAAAAAAAcBFse41NAAAAAAAAAGUXwSYAAAAAAAAA2yHYBAAAAAAAAGA7BJsAAAAAAAAAbIdgswwYN26crrvuOlWqVElBQUHq3LmzkpOTXcq0b99eHh4eLo+HH37Ypcz+/fsVGxur8uXLKygoSMOHD9eZM2dKsinFZuzYsbna26BBA2v9qVOnFBcXp6pVq6pixYrq0qWLUlNTXbZxOfWHJNWpUydXn3h4eCguLk5S2RgjX331lW6//XaFhobKw8NDixYtcllvjNHo0aNVo0YN+fv7Kzo6Wnv27HEpc+TIEfXq1UsOh0OBgYHq16+fjh8/7lJm69atatOmjfz8/BQWFqYJEyZc6qZdlPP1y+nTpzVy5Eg1btxYFSpUUGhoqB544AEdPHjQZRt5ja/x48e7lLFTvxQ0Vvr06ZOrvZ06dXIpc7mNlYL6JK/3Fw8PD02cONEqc7mNE+ByNnXqVNWpU0d+fn6KjIzUxo0b3V2lApXU/G/NmjVq3ry5fH19Va9ePc2ePTtXXUqi/0rTvGbBggVq0KCB/Pz81LhxYy1durTIdSnu9pfkZ3VJt78w3/9K03gvTF2Ku/3F9d2mNLZ/+vTpatKkiRwOhxwOh6KiorRs2bIi7c+ubS9M+y/n1z4v48ePl4eHh4YOHVqk/dqqDwwuezExMWbWrFlm+/btZsuWLebWW281tWrVMsePH7fKtGvXzgwYMMD88ccf1iM9Pd1af+bMGdOoUSMTHR1tNm/ebJYuXWqqVatmRo0a5Y4mXbQxY8aYa665xqW9hw8fttY//PDDJiwszKxcudJ8//33plWrVub666+31l9u/WGMMYcOHXLpjxUrVhhJZvXq1caYsjFGli5dap5++mmzcOFCI8l88sknLuvHjx9vAgICzKJFi8wPP/xg7rjjDhMeHm5OnjxplenUqZNp2rSp+fbbb83XX39t6tWrZ3r27GmtT09PN8HBwaZXr15m+/bt5sMPPzT+/v5m5syZJdXMIjtfv6SlpZno6Gjz0Ucfmd27d5vExETTsmVLExER4bKN2rVrm+eff95l/Jz9HmS3filorPTu3dt06tTJpb1HjhxxKXO5jZWC+uTsvvjjjz/Mu+++azw8PMzPP/9slbncxglwuZo3b57x8fEx7777rtmxY4cZMGCACQwMNKmpqe6u2nmVxPzvl19+MeXLlzfx8fFm586dZsqUKcbLy8skJCRYZUqq/0rLvGbdunXGy8vLTJgwwezcudM888wzxtvb22zbtq1IdSnu9pfUZ7U72l+Y73+labwXVJdL0f7i+G5TWtv/2WefmSVLlpgff/zRJCcnm6eeesp4e3ub7du3F2p/dm57Ydp/Ob/259q4caOpU6eOadKkiRkyZEih92u3PiDYLIMOHTpkJJm1a9day9q1a+cy0M+1dOlS4+npaVJSUqxl06dPNw6Hw2RmZl7K6l4SY8aMMU2bNs1zXVpamvH29jYLFiywlu3atctIMomJicaYy68/8jJkyBBTt25dk5OTY4wpe2Pk3AlwTk6OCQkJMRMnTrSWpaWlGV9fX/Phhx8aY4zZuXOnkWS+++47q8yyZcuMh4eH+f33340xxkybNs1UrlzZpU9GjhxprrrqqkvcouKR1xeDc23cuNFIMr/++qu1rHbt2mbSpEn5PsfO/ZLfl6U777wz3+dc7mOlMOPkzjvvNDfddJPLsst5nACXk5YtW5q4uDjr7+zsbBMaGmrGjRvnxloVrCTmfyNGjDDXXHONy7a7d+9uYmJirL/d0X/unNd069bNxMbGutQnMjLSPPTQQ4Wuy8Vy52d1aWj/ud//StN4L0xdirv9xhTPdxu7tN8YYypXrmzeeeedMvfaOznbb0zZee2PHTtm6tevb1asWOHS5stxDPBT9DIoPT1dklSlShWX5R988IGqVaumRo0aadSoUfr777+tdYmJiWrcuLGCg4OtZTExMcrIyNCOHTtKpuLFbM+ePQoNDdWVV16pXr16af/+/ZKkpKQknT59WtHR0VbZBg0aqFatWkpMTJR0efbH2bKysvT+++/rwQcflIeHh7W8rI2Rs+3du1cpKSku4yIgIECRkZEu4yIwMFAtWrSwykRHR8vT01MbNmywyrRt21Y+Pj5WmZiYGCUnJ+vo0aMl1JpLKz09XR4eHgoMDHRZPn78eFWtWlXXXnutJk6c6PJThsuxX9asWaOgoCBdddVVGjRokP766y9rXVkfK6mpqVqyZIn69euXa11ZGyeA3WRlZSkpKcnl89DT01PR0dHW52Fpdqnnf4mJiS7bcJZxbqO09F9JzmsK6pPC1OVSKYnP6tLQ/nO//5Wm8V6YuhR3+50u9ruNHdqfnZ2tefPm6cSJE4qKiipzr/257XcqC699XFycYmNjc9XzchwD5QpdEpeFnJwcDR06VDfccIMaNWpkLb/33ntVu3ZthYaGauvWrRo5cqSSk5O1cOFCSVJKSorLoJZk/Z2SklJyDSgmkZGRmj17tq666ir98ccfeu6559SmTRtt375dKSkp8vHxyRXKBAcHW2293PrjXIsWLVJaWpr69OljLStrY+Rczjbk1cazx0VQUJDL+nLlyqlKlSouZcLDw3Ntw7mucuXKl6T+JeXUqVMaOXKkevbsKYfDYS1/7LHH1Lx5c1WpUkXr16/XqFGj9Mcff+i1116TdPn1S6dOnXT33XcrPDxcP//8s5566indcsstSkxMlJeXV5kfK3PmzFGlSpV09913uywva+MEsKM///xT2dnZeX4e7t692021KpySmP/lVyYjI0MnT57U0aNHS0X/leS8Jr8+OXsbBdXlUiipz2p3tz+v73+labwXpi4X41J+/y3N7d+2bZuioqJ06tQpVaxYUZ988okaNmyoLVu2lInXPr/2S5f/ay9J8+bN06ZNm/Tdd9/lWnc5Hv8Em2VMXFyctm/frm+++cZl+cCBA63/b9y4sWrUqKEOHTro559/Vt26dUu6mpfcLbfcYv1/kyZNFBkZqdq1a2v+/Pny9/d3Y81Kh//85z+65ZZbFBoaai0ra2MERXf69Gl169ZNxhhNnz7dZV18fLz1/02aNJGPj48eeughjRs3Tr6+viVd1UuuR48e1v83btxYTZo0Ud26dbVmzRp16NDBjTUrHd5991316tVLfn5+LsvL2jgBULKY/+FsZeWzOr/vf2VFWf3+e9VVV2nLli1KT0/Xxx9/rN69e2vt2rXurlaJya/9DRs2vOxf+wMHDmjIkCFasWJFrrn25YqfopchgwcP1uLFi7V69WrVrFnzvGUjIyMlST/99JMkKSQkJNedqZx/h4SEXILalqzAwED961//0k8//aSQkBBlZWUpLS3NpUxqaqrV1su5P3799Vd9+eWX6t+//3nLlbUx4mxDXm08e1wcOnTIZf2ZM2d05MiRy37sOEPNX3/9VStWrHA5WzMvkZGROnPmjPbt2yfp8u0XpyuvvFLVqlVzOV7K6lj5+uuvlZycXOB7jFT2xglgB9WqVZOXl9d5Pw/t4lLM//Ir43A45O/vX2r6ryTnNfmVOXt9QXUpCZfqs9qd7c/v+19pGu+FqcuFutTff0tz+318fFSvXj1FRERo3Lhxatq0qV5//fUy89rn1/68XG6vfVJSkg4dOqTmzZurXLlyKleunNauXas33nhD5cqVU3Bw8GU3Bgg2ywBjjAYPHqxPPvlEq1atyvVzibxs2bJFklSjRg1JUlRUlLZt2+by4e4ML5yndNvZ8ePH9fPPP6tGjRqKiIiQt7e3Vq5caa1PTk7W/v37retyXM79MWvWLAUFBSk2Nva85craGAkPD1dISIjLuMjIyNCGDRtcxkVaWpqSkpKsMqtWrVJOTo71gRkVFaWvvvpKp0+ftsqsWLFCV111lW1/RusMNffs2aMvv/xSVatWLfA5W7Zskaenp/UTr8uxX87222+/6a+//nI5XsriWJH+OSM8IiJCTZs2LbBsWRsngB34+PgoIiLC5fMwJydHK1eudLl+mR1civlfVFSUyzacZZzbKC39V5LzmoL6pDB1KQmX6rPaHe0v6PtfaRrvhalLcbc/Lxfy3aa0tj8vOTk5yszMvOxf+4Lan5fL7bXv0KGDtm3bpi1btliPFi1aqFevXtb/X3ZjoNC3GYJtDRo0yAQEBJg1a9aYP/74w3r8/fffxhhjfvrpJ/P888+b77//3uzdu9d8+umn5sorrzRt27a1tnHmzBnTqFEj07FjR7NlyxaTkJBgqlevbkaNGuWuZl2Uxx9/3KxZs8bs3bvXrFu3zkRHR5tq1aqZQ4cOGWOMefjhh02tWrXMqlWrzPfff2+ioqJMVFSU9fzLrT+csrOzTa1atczIkSNdlpeVMXLs2DGzefNms3nzZiPJvPbaa2bz5s3W3b3Hjx9vAgMDzaeffmq2bt1q7rzzThMeHm5OnjxpbaNTp07m2muvNRs2bDDffPONqV+/vunZs6e1Pi0tzQQHB5v777/fbN++3cybN8+UL1/ezJw5s8TbW1jn65esrCxzxx13mJo1a5otW7a4vMc475i3fv16M2nSJLNlyxbz888/m/fff99Ur17dPPDAA9Y+7NYv5+uTY8eOmSeeeMIkJiaavXv3mi+//NI0b97c1K9f35w6dcraxuU2Vgo6fowxJj093ZQvX95Mnz491/Mvx3ECXK7mzZtnfH19zezZs83OnTvNwIEDTWBgoMvdU0ujkpj//fLLL6Z8+fJm+PDhZteuXWbq1KnGy8vLJCQkWGVKqv9Ky7xm3bp1ply5cubf//632bVrlxkzZozx9vY227Zts8oUpi7F2f6S/Kx2R/sL+v5nTOka7wXVpbjbX1zfbUpr+5988kmzdu1as3fvXrN161bz5JNPGg8PD/PFF18Uan92bntB7b/cX/v8nHsn+MttDBBslgGS8nzMmjXLGGPM/v37Tdu2bU2VKlWMr6+vqVevnhk+fLhJT0932c6+ffvMLbfcYvz9/U21atXM448/bk6fPu2GFl287t27mxo1ahgfHx9zxRVXmO7du5uffvrJWn/y5EnzyCOPmMqVK5vy5cubu+66y/zxxx8u27ic+sNp+fLlRpJJTk52WV5Wxsjq1avzPFZ69+5tjDEmJyfHPPvssyY4ONj4+vqaDh065Oqrv/76y/Ts2dNUrFjROBwO07dvX3Ps2DGXMj/88INp3bq18fX1NVdccYUZP358STXxgpyvX/bu3Zvve8zq1auNMcYkJSWZyMhIExAQYPz8/MzVV19tXn75ZZcvDsbYq1/O1yd///236dixo6levbrx9vY2tWvXNgMGDMj1hfVyGysFHT/GGDNz5kzj7+9v0tLScj3/chwnwOVsypQpplatWsbHx8e0bNnSfPvtt+6uUoFKav63evVq06xZM+Pj42OuvPJKa859tpLov9I0r5k/f77517/+ZXx8fMw111xjlixZ4rK+MHUpzvaX9Gd1Sbe/oO9/xpSu8V6YuhRn+4vzu01pbP+DDz5oateubXx8fEz16tVNhw4drFCzsPuza9sLav/l/trn59xg83IbAx7GGFP48zsBAAAAAAAAwP24xiYAAAAAAAAA2yHYBAAAAAAAAGA7BJsAAAAAAAAAbIdgEwAAAAAAAIDtEGwCAAAAAAAAsB2CTQAAAAAAAAC2Q7AJAAAAAAAAwHYINgEAAAAAAADYDsEmgDKnT58+8vDwkIeHh7y9vRUcHKybb75Z7777rnJyctxdPQAAAKDUOXz4sAYNGqRatWrJ19dXISEhiomJ0bp169xdNQBlWDl3VwAA3KFTp06aNWuWsrOzlZqaqoSEBA0ZMkQff/yxPvvsM5Urx9sjAAAA4NSlSxdlZWVpzpw5uvLKK5WamqqVK1fqr7/+cnfVAJRhHsYY4+5KAEBJ6tOnj9LS0rRo0SKX5atWrVKHDh309ttvq3///u6pHAAAAFDKpKWlqXLlylqzZo3atWuXa/2aNWvUsWNHrVy5Um3atJEkTZgwQf/+97+1bds2BQcHl3SVAZQR/BQdAP7PTTfdpKZNm2rhwoXurgoAAABQalSsWFEVK1bUokWLlJmZmWt9+/btNXToUN1///1KT0/X5s2b9eyzz+qdd94h1ARwSRFsAsBZGjRooH379rm7GgAAAECpUa5cOc2ePVtz5sxRYGCgbrjhBj311FPaunWrVebFF19U5cqVNXDgQN13333q3bu37rjjDjfWGkBZQLAJAGcxxsjDw8Pd1QAAAABKlS5duujgwYP67LPP1KlTJ61Zs0bNmzfX7NmzJUk+Pj764IMP9L///U+nTp3SpEmT3FthAGUCwSYAnGXXrl0KDw93dzUAAACAUsfPz08333yznn32Wa1fv159+vTRmDFjrPXr16+XJB05ckRHjhxxVzUBlCEEmwDwf1atWqVt27apS5cu7q4KAAAAUOo1bNhQJ06ckCT9/PPPGjZsmN5++21FRkaqd+/eysnJcXMNAVzuCDYBlEmZmZlKSUnR77//rk2bNunll1/WnXfeqdtuu00PPPCAu6sHAAAAlBp//fWXbrrpJr3//vvaunWr9u7dqwULFmjChAm68847lZ2drfvuu08xMTHq27evZs2apa1bt+rVV191d9UBXObKubsCAOAOCQkJqlGjhsqVK6fKlSuradOmeuONN9S7d295evJvPgAAAIBTxYoVFRkZqUmTJunnn3/W6dOnFRYWpgEDBuipp57SSy+9pF9//VWLFy+WJNWoUUNvvfWWevbsqY4dO6pp06ZubgGAy5WHMca4uxIAAAAAAAAAUBSclgQAAAAAAADAdgg2AQAAAAAAANgOwSYAAAAAAAAA2yHYBAAAAAAAAGA7BJsAAAAAAAAAbIdgEwAAAAAAAIDtEGwCAAAAAAAAsB2CTQAAAAAAAAC2Q7AJAAAAAAAAwHYINgEAAAAAAADYTjl3V8CdcnJydPDgQVWqVEkeHh7urg4AAHADY4yOHTum0NBQeXryb75AYTGXBgAAknvn02U62Dx48KDCwsLcXQ0AAFAKHDhwQDVr1nR3NQDbYC4NAADO5o75dJkONitVqiTpn453OBxurg0AAHCHjIwMhYWFWfMCAIXDXBoAAEjunU+X6WDT+ZMZh8PBZAwAgDKOn9ICRcNcGgAAnM0d82kuJAUAAAAAAADAdgg2AQAAAAAAANgOwSYAAAAAAAAA2ynT19h0ajRmuTx9y5+3zL7xsSVUGwAAAMA+mEsDAAB34YxNAAAAAAAAALZDsAkAAAAAAADAdgg2AQAAAAAAANgOwSYAAAAAAAAA2yHYBAAAAAAAAGA7BJsAAAAAAAAAbIdgEwAAAAAAAIDtEGwCAAAAAAAAsB2CTQAAAAAAAAC2Q7AJAAAAAAAAwHYINgEAAAAAAADYDsEmAAAAAAAAANspcrD5+++/67777lPVqlXl7++vxo0b6/vvv7fWG2M0evRo1ahRQ/7+/oqOjtaePXtctnHkyBH16tVLDodDgYGB6tevn44fP+5SZuvWrWrTpo38/PwUFhamCRMm5KrLggUL1KBBA/n5+alx48ZaunRpUZsDAAAAlBjm0gAAAMWnSMHm0aNHdcMNN8jb21vLli3Tzp079eqrr6py5cpWmQkTJuiNN97QjBkztGHDBlWoUEExMTE6deqUVaZXr17asWOHVqxYocWLF+urr77SwIEDrfUZGRnq2LGjateuraSkJE2cOFFjx47VW2+9ZZVZv369evbsqX79+mnz5s3q3LmzOnfurO3bt19MfwAAAACXBHNpAACA4uVhjDGFLfzkk09q3bp1+vrrr/Ncb4xRaGioHn/8cT3xxBOSpPT0dAUHB2v27Nnq0aOHdu3apYYNG+q7775TixYtJEkJCQm69dZb9dtvvyk0NFTTp0/X008/rZSUFPn4+Fj7XrRokXbv3i1J6t69u06cOKHFixdb+2/VqpWaNWumGTNmFKo9GRkZCggIUNjQ+fL0LX/esvvGxxZqmwAAwF6c84H09HQ5HA53VweXMebSAADgcuTO+XSRztj87LPP1KJFC91zzz0KCgrStddeq7fffttav3fvXqWkpCg6OtpaFhAQoMjISCUmJkqSEhMTFRgYaE3EJCk6Olqenp7asGGDVaZt27bWREySYmJilJycrKNHj1plzt6Ps4xzP3nJzMxURkaGywMAAAAoCcylAQAAileRgs1ffvlF06dPV/369bV8+XINGjRIjz32mObMmSNJSklJkSQFBwe7PC84ONhal5KSoqCgIJf15cqVU5UqVVzK5LWNs/eRXxnn+ryMGzdOAQEB1iMsLKwozQcAAAAuGHNpAACA4lWkYDMnJ0fNmzfXyy+/rGuvvVYDBw7UgAEDCv1zFXcbNWqU0tPTrceBAwfcXSUAAACUEcylAQAAileRgs0aNWqoYcOGLsuuvvpq7d+/X5IUEhIiSUpNTXUpk5qaaq0LCQnRoUOHXNafOXNGR44ccSmT1zbO3kd+ZZzr8+Lr6yuHw+HyAAAAAEoCc2kAAIDiVaRg84YbblBycrLLsh9//FG1a9eWJIWHhyskJEQrV6601mdkZGjDhg2KioqSJEVFRSktLU1JSUlWmVWrViknJ0eRkZFWma+++kqnT5+2yqxYsUJXXXWVddfIqKgol/04yzj3AwAAAJQmzKUBAACKV5GCzWHDhunbb7/Vyy+/rJ9++klz587VW2+9pbi4OEmSh4eHhg4dqhdffFGfffaZtm3bpgceeEChoaHq3LmzpH/+VbpTp04aMGCANm7cqHXr1mnw4MHq0aOHQkNDJUn33nuvfHx81K9fP+3YsUMfffSRXn/9dcXHx1t1GTJkiBISEvTqq69q9+7dGjt2rL7//nsNHjy4mLoGAAAAKD7MpQEAAIpXuaIUvu666/TJJ59o1KhRev755xUeHq7JkyerV69eVpkRI0boxIkTGjhwoNLS0tS6dWslJCTIz8/PKvPBBx9o8ODB6tChgzw9PdWlSxe98cYb1vqAgAB98cUXiouLU0REhKpVq6bRo0dr4MCBVpnrr79ec+fO1TPPPKOnnnpK9evX16JFi9SoUaOL6Q8AAADgkmAuDQAAULw8jDHG3ZVwl4yMjH/u6Dh0vjx9y5+37L7xsSVUKwAAUJKc84H09HSuGQgUAXNpAAAguXc+XaSfogMAAAAAAABAaUCwCQAAAAAAAMB2CDYBAAAAAAAA2A7BJgAAAAAAAADbIdgEAAAAAAAAYDsEmwAAAAAAAABsh2ATAAAAAAAAgO0QbAIAAAAAAACwHYJNAAAAAAAAALZDsAkAAAAAAADAdgg2AQAAAAAAANgOwSYAAAAAAAAA2yHYBAAAAAAAAGA7BJsAAAAAAAAAbIdgEwAAAAAAAIDtEGwCAAAAAAAAsB2CTQAAAAAAAAC2Q7AJAAAAAAAAwHYINgEAAAAAAADYDsEmAAAAAAAAANsh2AQAAAAAAABgOwSbAAAAAAAAAGyHYBMAAAAAAACA7RBsAgAAAAAAALAdgk0AAAAAAAAAtnNRweb48ePl4eGhoUOHWstOnTqluLg4Va1aVRUrVlSXLl2Umprq8rz9+/crNjZW5cuXV1BQkIYPH64zZ864lFmzZo2aN28uX19f1atXT7Nnz861/6lTp6pOnTry8/NTZGSkNm7ceDHNAQAAAEoMc2kAAICLc8HB5nfffaeZM2eqSZMmLsuHDRumzz//XAsWLNDatWt18OBB3X333db67OxsxcbGKisrS+vXr9ecOXM0e/ZsjR492iqzd+9excbG6sYbb9SWLVs0dOhQ9e/fX8uXL7fKfPTRR4qPj9eYMWO0adMmNW3aVDExMTp06NCFNgkAAAAoEcylAQAALp6HMcYU9UnHjx9X8+bNNW3aNL344otq1qyZJk+erPT0dFWvXl1z585V165dJUm7d+/W1VdfrcTERLVq1UrLli3TbbfdpoMHDyo4OFiSNGPGDI0cOVKHDx+Wj4+PRo4cqSVLlmj79u3WPnv06KG0tDQlJCRIkiIjI3XdddfpzTfflCTl5OQoLCxMjz76qJ588slCtSMjI0MBAQEKGzpfnr7lz1t23/jYonYTAACwAed8ID09XQ6Hw93VQRnAXBoAAFxO3DmfvqAzNuPi4hQbG6vo6GiX5UlJSTp9+rTL8gYNGqhWrVpKTEyUJCUmJqpx48bWREySYmJilJGRoR07dlhlzt12TEyMtY2srCwlJSW5lPH09FR0dLRVJi+ZmZnKyMhweQAAAAAlibk0AABA8ShX1CfMmzdPmzZt0nfffZdrXUpKinx8fBQYGOiyPDg4WCkpKVaZsydizvXOdecrk5GRoZMnT+ro0aPKzs7Os8zu3bvzrfu4ceP03HPPFa6hAAAAQDFjLg0AAFB8inTG5oEDBzRkyBB98MEH8vPzu1R1umRGjRql9PR063HgwAF3VwkAAABlBHNpAACA4lWkYDMpKUmHDh1S8+bNVa5cOZUrV05r167VG2+8oXLlyik4OFhZWVlKS0tzeV5qaqpCQkIkSSEhIbnu7Oj8u6AyDodD/v7+qlatmry8vPIs49xGXnx9feVwOFweAAAAQElgLg0AAFC8ihRsdujQQdu2bdOWLVusR4sWLdSrVy/r/729vbVy5UrrOcnJydq/f7+ioqIkSVFRUdq2bZvLHRdXrFghh8Ohhg0bWmXO3oazjHMbPj4+ioiIcCmTk5OjlStXWmUAAACA0oS5NAAAQPEq0jU2K1WqpEaNGrksq1ChgqpWrWot79evn+Lj41WlShU5HA49+uijioqKUqtWrSRJHTt2VMOGDXX//fdrwoQJSklJ0TPPPKO4uDj5+vpKkh5++GG9+eabGjFihB588EGtWrVK8+fP15IlS6z9xsfHq3fv3mrRooVatmypyZMn68SJE+rbt+9FdQgAAABwKTCXBgAAKF5FvnlQQSZNmiRPT0916dJFmZmZiomJ0bRp06z1Xl5eWrx4sQYNGqSoqChVqFBBvXv31vPPP2+VCQ8P15IlSzRs2DC9/vrrqlmzpt555x3FxMRYZbp3767Dhw9r9OjRSklJUbNmzZSQkJDrIugAAACAXTCXBgAAKDwPY4xxdyXcJSMjQwEBAQobOl+evuXPW3bf+NgSqhUAAChJzvlAeno61wwEioC5NAAAkNw7ny7SNTYBAAAAAAAAoDQg2AQAAAAAAABgOwSbAAAAAAAAAGyHYBMAAAAAAACA7RBsAgAAAAAAALAdgk0AAAAAAAAAtkOwCQAAAAAAAMB2CDYBAAAAAAAA2A7BJgAAAAAAAADbIdgEAAAAAAAAYDsEmwAAAAAAAABsh2ATAAAAAAAAgO0QbAIAAAAAAACwHYJNAAAAAAAAALZDsAkAAAAAAADAdgg2AQAAAAAAANgOwSYAAAAAAAAA2yHYBAAAAAAAAGA7BJsAAAAAAAAAbIdgEwAAAAAAAIDtEGwCAAAAAAAAsB2CTQAAAAAAAAC2Q7AJAAAAAAAAwHYINgEAAAAAAADYDsEmAAAAAAAAANspUrA5btw4XXfddapUqZKCgoLUuXNnJScnu5Q5deqU4uLiVLVqVVWsWFFdunRRamqqS5n9+/crNjZW5cuXV1BQkIYPH64zZ864lFmzZo2aN28uX19f1atXT7Nnz85Vn6lTp6pOnTry8/NTZGSkNm7cWJTmAAAAACWGuTQAAEDxKlKwuXbtWsXFxenbb7/VihUrdPr0aXXs2FEnTpywygwbNkyff/65FixYoLVr1+rgwYO6++67rfXZ2dmKjY1VVlaW1q9frzlz5mj27NkaPXq0VWbv3r2KjY3VjTfeqC1btmjo0KHq37+/li9fbpX56KOPFB8frzFjxmjTpk1q2rSpYmJidOjQoYvpDwAAAOCSYC4NAABQvDyMMeZCn3z48GEFBQVp7dq1atu2rdLT01W9enXNnTtXXbt2lSTt3r1bV199tRITE9WqVSstW7ZMt912mw4ePKjg4GBJ0owZMzRy5EgdPnxYPj4+GjlypJYsWaLt27db++rRo4fS0tKUkJAgSYqMjNR1112nN998U5KUk5OjsLAwPfroo3ryyScLVf+MjAwFBAQobOh8efqWP2/ZfeNji9w/AACg9HPOB9LT0+VwONxdHZQhzKUBAMDlwJ3z6Yu6xmZ6erokqUqVKpKkpKQknT59WtHR0VaZBg0aqFatWkpMTJQkJSYmqnHjxtZETJJiYmKUkZGhHTt2WGXO3oazjHMbWVlZSkpKcinj6emp6Ohoq0xeMjMzlZGR4fIAAAAA3IG5NAAAwMW54GAzJydHQ4cO1Q033KBGjRpJklJSUuTj46PAwECXssHBwUpJSbHKnD0Rc653rjtfmYyMDJ08eVJ//vmnsrOz8yzj3EZexo0bp4CAAOsRFhZW9IYDAAAAF4m5NAAAwMW74GAzLi5O27dv17x584qzPpfUqFGjlJ6ebj0OHDjg7ioBAACgDGIuDQAAcPHKXciTBg8erMWLF+urr75SzZo1reUhISHKyspSWlqay780p6amKiQkxCpz7h0XnXd6PLvMuXd/TE1NlcPhkL+/v7y8vOTl5ZVnGec28uLr6ytfX9+iNxgAAAAoJsylAQAAikeRztg0xmjw4MH65JNPtGrVKoWHh7usj4iIkLe3t1auXGktS05O1v79+xUVFSVJioqK0rZt21zuuLhixQo5HA41bNjQKnP2NpxlnNvw8fFRRESES5mcnBytXLnSKgMAAACUJsylAQAAileRztiMi4vT3Llz9emnn6pSpUrWNXgCAgLk7++vgIAA9evXT/Hx8apSpYocDoceffRRRUVFqVWrVpKkjh07qmHDhrr//vs1YcIEpaSk6JlnnlFcXJz1L8APP/yw3nzzTY0YMUIPPvigVq1apfnz52vJkiVWXeLj49W7d2+1aNFCLVu21OTJk3XixAn17du3uPoGAAAAKDbMpQEAAIpXkYLN6dOnS5Lat2/vsnzWrFnq06ePJGnSpEny9PRUly5dlJmZqZiYGE2bNs0q6+XlpcWLF2vQoEGKiopShQoV1Lt3bz3//PNWmfDwcC1ZskTDhg3T66+/rpo1a+qdd95RTEyMVaZ79+46fPiwRo8erZSUFDVr1kwJCQm5LoIOAAAAlAbMpQEAAIqXhzHGuLsS7pKRkfHPHR2Hzpenb/nzlt03PraEagUAAEqScz6Qnp4uh8Ph7uoAtsFcGgAASO6dT1/wXdEBAAAAAAAAwF0INgEAAAAAAADYDsEmAAAAAAAAANsh2AQAAAAAAABgOwSbAAAAAAAAAGyHYBMAAAAAAACA7RBsAgAAAAAAALAdgk0AAAAAAAAAtkOwCQAAAAAAAMB2CDYBAAAAAAAA2A7BJgAAAAAAAADbIdgEAAAAAAAAYDsEmwAAAAAAAABsh2ATAAAAAAAAgO0QbAIAAAAAAACwHYJNAAAAAAAAALZDsAkAAAAAAADAdgg2AQAAAAAAANgOwSYAAAAAAAAA2yHYBAAAAAAAAGA7BJsAAAAAAAAAbIdgEwAAAAAAAIDtEGwCAAAAAAAAsB2CTQAAAAAAAAC2Q7AJAAAAAAAAwHZsH2xOnTpVderUkZ+fnyIjI7Vx40Z3VwkAAACwBebSAADAzmwdbH700UeKj4/XmDFjtGnTJjVt2lQxMTE6dOiQu6sGAAAAlGrMpQEAgN3ZOth87bXXNGDAAPXt21cNGzbUjBkzVL58eb377rvurhoAAABQqjGXBgAAdlfO3RW4UFlZWUpKStKoUaOsZZ6enoqOjlZiYmKez8nMzFRmZqb1d3p6uiQpJ/PvAveXkZFxkTUGAAClkfMz3hjj5poAJYe5NAAAKC7unE/bNtj8888/lZ2dreDgYJflwcHB2r17d57PGTdunJ577rlcy3+f3qfA/QVMvpBaAgAAuzh27JgCAgLcXQ2gRDCXBgAAxc0d82nbBpsXYtSoUYqPj7f+TktLU+3atbV//36+yJwlIyNDYWFhOnDggBwOh7urU6rQN/mjb/JH3+SNfskffZO/S9E3xhgdO3ZMoaGhxbI94HLFXLrk8Xlw6dHHlx59fOnRx5cefXx+7pxP2zbYrFatmry8vJSamuqyPDU1VSEhIXk+x9fXV76+vrmWBwQEMDDz4HA46Jd80Df5o2/yR9/kjX7JH32Tv+LuG0IZlDXMpe2Fz4NLjz6+9OjjS48+vvTo4/y5az5t25sH+fj4KCIiQitXrrSW5eTkaOXKlYqKinJjzQAAAIDSjbk0AAC4HNj2jE1Jio+PV+/evdWiRQu1bNlSkydP1okTJ9S3b193Vw0AAAAo1ZhLAwAAu7N1sNm9e3cdPnxYo0ePVkpKipo1a6aEhIRcF0HPj6+vr8aMGZPnT2rKMvolf/RN/uib/NE3eaNf8kff5I++AYoPc+nSjz6+9OjjS48+vvTo40uPPi69PIw77sUOAAAAAAAAABfBttfYBAAAAAAAAFB2EWwCAAAAAAAAsB2CTQAAAAAAAAC2Q7AJAAAAAAAAwHYu+2Bz6tSpqlOnjvz8/BQZGamNGzeet/yCBQvUoEED+fn5qXHjxlq6dGkJ1bRkFaVfZs+eLQ8PD5eHn59fCda25Hz11Ve6/fbbFRoaKg8PDy1atKjA56xZs0bNmzeXr6+v6tWrp9mzZ1/yepa0ovbLmjVrco0ZDw8PpaSklEyFS9C4ceN03XXXqVKlSgoKClLnzp2VnJxc4PMu9/eaC+mXsvJeM336dDVp0kQOh0MOh0NRUVFatmzZeZ9zuY8Xp6L2TVkZM0BpVdR5dlkwduzYXO9LDRo0sNafOnVKcXFxqlq1qipWrKguXbooNTXVZRv79+9XbGysypcvr6CgIA0fPlxnzpxxKVOY+efl8voUNA81xmj06NGqUaOG/P39FR0drT179riUOXLkiHr16iWHw6HAwED169dPx48fdymzdetWtWnTRn5+fgoLC9OECRNy1aWgz+PC1KU0KqiP+/Tpk2tcd+rUyaUMfXx+hZkbl6b3h8LUpbQpTB+3b98+11h++OGHXcrQxzZkLmPz5s0zPj4+5t133zU7duwwAwYMMIGBgSY1NTXP8uvWrTNeXl5mwoQJZufOneaZZ54x3t7eZtu2bSVc80urqP0ya9Ys43A4zB9//GE9UlJSSrjWJWPp0qXm6aefNgsXLjSSzCeffHLe8r/88ospX768iY+PNzt37jRTpkwxXl5eJiEhoWQqXEKK2i+rV682kkxycrLLuMnOzi6ZCpegmJgYM2vWLLN9+3azZcsWc+utt5patWqZ48eP5/ucsvBecyH9Ulbeaz777DOzZMkS8+OPP5rk5GTz1FNPGW9vb7N9+/Y8y5eF8eJU1L4pK2MGKI2KOp8sK8aMGWOuueYal/elw4cPW+sffvhhExYWZlauXGm+//5706pVK3P99ddb68+cOWMaNWpkoqOjzebNm83SpUtNtWrVzKhRo6wyhZl/Xk6vT0Hz0PHjx5uAgACzaNEi88MPP5g77rjDhIeHm5MnT1plOnXqZJo2bWq+/fZb8/XXX5t69eqZnj17WuvT09NNcHCw6dWrl9m+fbv58MMPjb+/v5k5c6ZVpjCfx4WpS2lUUB/37t3bdOrUyWVcHzlyxKUMfXx+hZkbl6b3h4LqUhoVpo/btWtnBgwY4DKW09PTrfX0sT1d1sFmy5YtTVxcnPV3dna2CQ0NNePGjcuzfLdu3UxsbKzLssjISPPQQw9d0nqWtKL2y6xZs0xAQEAJ1a70KEyAN2LECHPNNde4LOvevbuJiYm5hDVzr6IEm0ePHi2ROpUmhw4dMpLM2rVr8y1TVt5rzlaYfimr7zXGGFO5cmXzzjvv5LmuLI6Xs52vb8rymAHcrajzybJizJgxpmnTpnmuS0tLM97e3mbBggXWsl27dhlJJjEx0RjzT8Dk6enp8o8006dPNw6Hw2RmZhpjCjf/vFxfn3PnoTk5OSYkJMRMnDjRWpaWlmZ8fX3Nhx9+aIwxZufOnUaS+e6776wyy5YtMx4eHub33383xhgzbdo0U7lyZauPjTFm5MiR5qqrrrL+LujzuDB1sYP8gs0777wz3+fQx0V37ty4NL0/FKYudpDX94927dqZIUOG5Psc+tieLtufomdlZSkpKUnR0dHWMk9PT0VHRysxMTHP5yQmJrqUl6SYmJh8y9vRhfSLJB0/fly1a9dWWFiY7rzzTu3YsaMkqlvqlYUxczGaNWumGjVq6Oabb9a6devcXZ0SkZ6eLkmqUqVKvmXK4rgpTL9IZe+9Jjs7W/PmzdOJEycUFRWVZ5myOF6kwvWNVPbGDFAaXOh8sqzYs2ePQkNDdeWVV6pXr17av3+/JCkpKUmnT5926bcGDRqoVq1aVr8lJiaqcePGCg4OtsrExMQoIyPDen8r6HOhLL0+e/fuVUpKiktbAwICFBkZ6dKngYGBatGihVUmOjpa/4+9ew+rqsz7P/4BYeMREFE2lAfUxEOewmLQMg+MiE6N6VNqPnlMJwec1My0mTxUj5o5mjWO1lTYjDamz4xWOqGIqc8ompKMh8xJR0NHwNIAIQWE+/dHP/e0FRQU2Cz2+3Vd67rYa91r7+99r83iu7/ca21PT0/t3bvX0aZnz56y2WyONtHR0Tp27Ji+++47R5sbjXtZYrGy7du3q0mTJgoLC9PEiRN1/vx5xzbGuPyuzY2r0/mhLLFYQWmfP1avXq3AwEDdfffdmjlzpr7//nvHNsbYmmpsYfPbb79VUVGR0xtSkoKCgkq9z19GRka52lvRrYxLWFiY3n33XX344YdatWqViouL1b17d505c6YqQq7WSnvP5OTk6NKlSy6KyvWCg4O1YsUK/eUvf9Ff/vIXNW3aVL169dLnn3/u6tAqVXFxsSZPnqwePXro7rvvLrWdO5xrfqys4+JO55pDhw6pfv368vHx0VNPPaX169erffv2JbZ1t/dLecbGnd4zQHVyK/mku4iIiNDKlSuVkJCg5cuX6+TJk3rggQd08eJFZWRkyGazyd/f32mfH49baef8q9tu1OZq/ulOx+dqf27U14yMDDVp0sRpu5eXlwICAipk3H+8/WaxWFX//v31xz/+UUlJSXrllVe0Y8cOxcTEqKioSBJjXF4l5cbV6fxQlliqu9I+fzz++ONatWqVPv30U82cOVN/+tOf9N///d+O7YyxNXm5OgBUf5GRkU6zZbp376527drpzTff1EsvveTCyFBdhYWFKSwszPG4e/fuOnHihJYsWaI//elPLoyscsXGxurw4cP6+9//7upQqpWyjos7nWvCwsKUmpqq7Oxs/e///q9GjRqlHTt2lFrAcyflGRt3es8AsIaYmBjHz506dVJERISaN2+utWvXqk6dOi6MDLh1w4YNc/zcsWNHderUSa1atdL27dvVt29fF0ZmTXxmqHyljfGECRMcP3fs2FHBwcHq27evTpw4oVatWlV1mKggNXbGZmBgoGrVqnXdt0plZmbKbreXuI/dbi9Xeyu6lXG5lre3t7p27arjx49XRoiWUtp7xtfXl+T1Gvfdd1+Nfs/ExcVp48aN+vTTT3XnnXfesK07nGuuKs+4XKsmn2tsNptat26t8PBwzZ8/X507d9bSpUtLbOtO7xepfGNzrZr8ngGqk4rIJ92Fv7+/2rRpo+PHj8tut6ugoEBZWVlObX48bqWd869uu1Gbq/mnOx2fq/25UV/tdrvOnTvntP3KlSu6cOFChYz7j7ffLJaaomXLlgoMDHT8vWWMy6603Lg6nR/KEkt1Vp7PHxEREZLk9F5mjK2nxhY2bTabwsPDlZSU5FhXXFyspKSkUu/VFRkZ6dRekhITE294by+ruZVxuVZRUZEOHTqk4ODgygrTMtzhPVNRUlNTa+R7xhijuLg4rV+/Xtu2bVNoaOhN93GH982tjMu13OlcU1xcrPz8/BK3ucP75UZuNDbXcqf3DOBKFZFPuovc3FydOHFCwcHBCg8Pl7e3t9O4HTt2TGlpaY5xi4yM1KFDh5yKRImJifL19XXMXL/Z3wV3Oj6hoaGy2+1Ofc3JydHevXudxjQrK0spKSmONtu2bVNxcbGjqBEZGamdO3eqsLDQ0SYxMVFhYWFq2LCho82Nxr0ssdQUZ86c0fnz5x1/bxnjm7tZblydzg9liaU6upXPH6mpqZLk9F5mjC3Itd9dVLnWrFljfHx8zMqVK80XX3xhJkyYYPz9/R3fcPXEE0+YGTNmONrv2rXLeHl5mUWLFpmjR4+a2bNnG29vb3Po0CFXdaFSlHdc5s6dazZv3mxOnDhhUlJSzLBhw0zt2rXNkSNHXNWFSnPx4kVz4MABc+DAASPJLF682Bw4cMB8/fXXxhhjZsyYYZ544glH+3/961+mbt265tlnnzVHjx41y5YtM7Vq1TIJCQmu6kKlKO+4LFmyxGzYsMF89dVX5tChQ+bpp582np6eZuvWra7qQqWZOHGi8fPzM9u3bzfp6emO5fvvv3e0ccdzza2Mi7uca2bMmGF27NhhTp48aQ4ePGhmzJhhPDw8zJYtW4wx7vl+uaq8Y+Mu7xmgOrpZPumunnnmGbN9+3Zz8uRJs2vXLhMVFWUCAwPNuXPnjDHGPPXUU6ZZs2Zm27ZtZv/+/SYyMtJERkY69r9y5Yq5++67Tb9+/UxqaqpJSEgwjRs3NjNnznS0KUv+WZOOz83y0AULFhh/f3/z4YcfmoMHD5qf//znJjQ01Fy6dMnxHP379zddu3Y1e/fuNX//+9/NXXfdZYYPH+7YnpWVZYKCgswTTzxhDh8+bNasWWPq1q1r3nzzTUebsvw9Lkss1dGNxvjixYtm2rRpJjk52Zw8edJs3brV3HPPPeauu+4yly9fdjwHY3xjZcmNq9P54WaxVEc3G+Pjx4+bF1980ezfv9+cPHnSfPjhh6Zly5amZ8+ejudgjK2pRhc2jTHmjTfeMM2aNTM2m83cd999Zs+ePY5tDz74oBk1apRT+7Vr15o2bdoYm81mOnToYDZt2lTFEVeN8ozL5MmTHW2DgoLMgAEDzOeff+6CqCvfp59+aiRdt1wdj1GjRpkHH3zwun26dOlibDabadmypYmPj6/yuCtbecfllVdeMa1atTK1a9c2AQEBplevXmbbtm2uCb6SlTQukpzeB+54rrmVcXGXc83YsWNN8+bNjc1mM40bNzZ9+/Z1FO6Mcc/3y1XlHRt3ec8A1dWN8kl3NXToUBMcHGxsNpu54447zNChQ83x48cd2y9dumR++ctfmoYNG5q6deuaRx55xKSnpzs9x6lTp0xMTIypU6eOCQwMNM8884wpLCx0alOW/LOmHJ+b5aHFxcXmhRdeMEFBQcbHx8f07dvXHDt2zOk5zp8/b4YPH27q169vfH19zZgxY8zFixed2vzjH/8w999/v/Hx8TF33HGHWbBgwXWx3OzvcVliqY5uNMbff/+96devn2ncuLHx9vY2zZs3N+PHj7+uSM4Y31hZcuPqdH4oSyzVzc3GOC0tzfTs2dMEBAQYHx8f07p1a/Pss8+a7Oxsp+dhjK3HwxhjKn4eKAAAAAAAAABUnhp7j00AAAAAAAAANReFTQAAAAAAAACWQ2ETAAAAAAAAgOVQ2AQAAAAAAABgORQ2AQAAAAAAAFgOhU0AAAAAAAAAlkNhEwAAAAAAAIDlUNgEAADV3s6dO/XQQw8pJCREHh4e2rBhQ7mfwxijRYsWqU2bNvLx8dEdd9yh//mf/6n4YAEAAABUCS9XBwAAAHAzeXl56ty5s8aOHavBgwff0nM8/fTT2rJlixYtWqSOHTvqwoULunDhQgVHCgAAAKCqMGMTgFv45ptvNHHiRDVr1kw+Pj6y2+2Kjo7Wrl27XB0agDKIiYnRyy+/rEceeaTE7fn5+Zo2bZruuOMO1atXTxEREdq+fbtj+9GjR7V8+XJ9+OGHevjhhxUaGqrw8HD99Kc/raIeAABgbeTTAKojZmwCcAtDhgxRQUGB3nvvPbVs2VKZmZlKSkrS+fPnXR0agAoQFxenL774QmvWrFFISIjWr1+v/v3769ChQ7rrrrv08ccfq2XLltq4caP69+8vY4yioqK0cOFCBQQEuDp8AACqPfJpANWRhzHGuDoIAKhMWVlZatiwobZv364HH3zwuu1jx47VuXPntHHjRse6wsJC3XHHHZo/f77GjRtXleECuAkPDw+tX79egwYNkiSlpaWpZcuWSktLU0hIiKNdVFSU7rvvPs2bN09PPfWUVq5cqS5duujVV19VUVGRpkyZooYNG2rbtm0u6gkAANZws3x65cqVGjNmzHXrZ8+erTlz5lRBhADcFZeiA6jx6tevr/r162vDhg3Kz8+/bvuTTz6phIQEpaenO9Zt3LhR33//vYYOHVqVoQK4BYcOHVJRUZHatGnj+H2vX7++duzYoRMnTkiSiouLlZ+frz/+8Y964IEH1KtXL73zzjv69NNPdezYMRf3AACA6u1m+fTQoUOVnp7uWP785z/Ly8tLPXr0cEG0ANwJhU0ANZ6Xl5dWrlyp9957T/7+/urRo4eef/55HTx4UJLUvXt3hYWF6U9/+pNjn/j4eD366KOqX7++q8IGUEa5ubmqVauWUlJSlJqa6liOHj2qpUuXSpKCg4Pl5eWlNm3aOPZr166dpB9mfAIAgNLdLJ+uU6eO7Ha77Ha78vLyFBsbq3nz5nEvawCVjsImALcwZMgQnT17Vh999JH69++v7du365577tHKlSsl/TBrMz4+XpKUmZmpTz75RGPHjnVhxADKqmvXrioqKtK5c+fUunVrp8Vut0uSevTooStXrjhmcErSP//5T0lS8+bNXRI3AABWcrN8WpKys7P1s5/9TAMHDtSzzz7rumABuA3usQnAbT355JNKTEzU119/rfPnzyskJETbt2/X7t279eabbzqKHgBcLzc3V8ePH5f0QyFz8eLF6t27twICAtSsWTP993//t3bt2qXf/va36tq1q7755hslJSWpU6dOGjhwoIqLi3Xvvfeqfv36eu2111RcXKzY2Fj5+vpqy5YtLu4dAADW9ON8uqioSAMHDtR3332nHTt2qHbt2q4OD4AbYMYmALfVvn175eXlSZIaNWqkQYMGKT4+vtSbnwNwnf3796tr167q2rWrJGnq1Knq2rWrZs2aJemH20eMHDlSzzzzjMLCwjRo0CDt27dPzZo1kyR5enrq448/VmBgoHr27KmBAweqXbt2WrNmjcv6BACA1f04n54yZYoOHTqkDRs2UNQEUGWYsQmgxjt//rweffRRjR07Vp06dVKDBg20f/9+TZo0SQMHDtQ777wjSUpMTNTPfvYzFRUVXfftygAAAIC7ulk+ff/992v8+PFav3697r33Xsd+V790CAAqi5erAwCAyla/fn1FRERoyZIlOnHihAoLC9W0aVONHz9ezz//vKNdVFSUgoOD1aFDB4qaAAAAwP93s3x64sSJKioq0sMPP+y03+zZszVnzhzXBA3ALTBjEwD+v9zcXN1xxx2Kj4/X4MGDXR0OAAAAAAC4AWZsAnB7xcXF+vbbb/Xb3/5W/v7+1/2nGQAAAAAAVD8UNgG4vbS0NIWGhurOO+/UypUr5eXFqREAAAAAgOqOS9EBAAAAAAAAWI6nqwMAAAAAAAAAgPKisAkAAAAAAADAcihsAgAAAAAAALAcCpsAAAAAAAAALIfCJgAAAAAAAADLobAJAAAAAAAAwHIobAIAAAAAAACwHC9XB+BKxcXFOnv2rBo0aCAPDw9XhwMAAFzAGKOLFy8qJCREnp78zxcoK3JpAAAguTafduvC5tmzZ9W0aVNXhwEAAKqB06dP684773R1GIBlkEsDAIAfc0U+7daFzQYNGkj6YeB9fX1dHA0AAHCFnJwcNW3a1JEXACgbcmkAACC5Np9268Lm1UtmfH19ScYAAHBzXEoLlA+5NAAA+DFX5NPcSAoAAAAAAACA5VDYBAAAAAAAAGA5FDYBAAAAAAAAWI5b32Pzqrtnb5anT90btjm1YGAVRQMAAAAAVafFjE1lasdnIgBAdUNhEwAAAACAUtS0wm9F96esz1ee53Slmna8gZqOS9EBAACAamTnzp166KGHFBISIg8PD23YsMFp++jRo+Xh4eG09O/f36nNhQsXNGLECPn6+srf31/jxo1Tbm6uU5uDBw/qgQceUO3atdW0aVMtXLiwsrsGAABQoZixCQAAAFQjeXl56ty5s8aOHavBgweX2KZ///6Kj493PPbx8XHaPmLECKWnpysxMVGFhYUaM2aMJkyYoPfff1+SlJOTo379+ikqKkorVqzQoUOHNHbsWPn7+2vChAmV17kKxKyq6qumzeADcGvKcy4oi/KcL5iZ7D4obAIAAADVSExMjGJiYm7YxsfHR3a7vcRtR48eVUJCgvbt26du3bpJkt544w0NGDBAixYtUkhIiFavXq2CggK9++67stls6tChg1JTU7V48WLLFDbLqqI/WEuu/dBKQRdVpTJ+d1zJCv3h9xsoPwqbAAAAgMVs375dTZo0UcOGDdWnTx+9/PLLatSokSQpOTlZ/v7+jqKmJEVFRcnT01N79+7VI488ouTkZPXs2VM2m83RJjo6Wq+88oq+++47NWzY8LrXzM/PV35+vuNxTk5OJfYQuDXMqkJ1ZIWialm5si81aRxRcShsAgAAABbSv39/DR48WKGhoTpx4oSef/55xcTEKDk5WbVq1VJGRoaaNGnitI+Xl5cCAgKUkZEhScrIyFBoaKhTm6CgIMe2kgqb8+fP19y5cyupV9ZS02aBWgEFjRtjfAC4KwqbAAAAgIUMGzbM8XPHjh3VqVMntWrVStu3b1ffvn0r7XVnzpypqVOnOh7n5OSoadOmlfZ67qaiC1NWKXS56+wvd73k2F37XdEqY2ayVc4ZFc1d+12TUNgEAAAALKxly5YKDAzU8ePH1bdvX9ntdp07d86pzZUrV3ThwgXHfTntdrsyMzOd2lx9XNq9O318fK77kqLKwIdMAABQVhQ2AQAAAAs7c+aMzp8/r+DgYElSZGSksrKylJKSovDwcEnStm3bVFxcrIiICEebX//61yosLJS3t7ckKTExUWFhYSVehg5IFJ1dgTGvvqxybKwSJ3CrKGwCAAAA1Uhubq6OHz/ueHzy5EmlpqYqICBAAQEBmjt3roYMGSK73a4TJ05o+vTpat26taKjoyVJ7dq1U//+/TV+/HitWLFChYWFiouL07BhwxQSEiJJevzxxzV37lyNGzdOzz33nA4fPqylS5dqyZIlLukzAPwYl6wDKCsKmwAAAEA1sn//fvXu3dvx+Op9LUeNGqXly5fr4MGDeu+995SVlaWQkBD169dPL730ktNl4qtXr1ZcXJz69u0rT09PDRkyRK+//rpju5+fn7Zs2aLY2FiFh4crMDBQs2bN0oQJE6quowBwm5iNCMDDGGPKs8O///1vPffcc/rkk0/0/fffq3Xr1oqPj1e3bt0kScYYzZ49W3/4wx+UlZWlHj16aPny5brrrrscz3HhwgVNmjRJH3/8sSPRWrp0qerXr+9oc/DgQcXGxmrfvn1q3LixJk2apOnTpzvFsm7dOr3wwgs6deqU7rrrLr3yyisaMGBAmfuSk5MjPz8/NZ28Vp4+dW/Ylv8EAQBQM13NB7Kzs+Xr6+vqcADLqKzfHQoVAAB3UJPqTK7Mpz3L0/i7775Tjx495O3trU8++URffPGFfvvb3zrdh2fhwoV6/fXXtWLFCu3du1f16tVTdHS0Ll++7GgzYsQIHTlyRImJidq4caN27tzp9N/hnJwc9evXT82bN1dKSopeffVVzZkzR2+99Zajze7duzV8+HCNGzdOBw4c0KBBgzRo0CAdPnz4dsYDAAAAAAAAgAWUa8bmjBkztGvXLv3f//1fiduNMQoJCdEzzzyjadOmSZKys7MVFBSklStXatiwYTp69Kjat2+vffv2OWZ5JiQkaMCAATpz5oxCQkK0fPly/frXv1ZGRoZsNpvjtTds2KAvv/xSkjR06FDl5eVp48aNjtf/yU9+oi5dumjFihVl6g8zNgEAADM2gVvDjE0AAG5dTaozWWbG5kcffaRu3brp0UcfVZMmTdS1a1f94Q9/cGw/efKkMjIyFBUV5Vjn5+eniIgIJScnS5KSk5Pl7+/vKGpKUlRUlDw9PbV3715Hm549ezqKmpIUHR2tY8eO6bvvvnO0+fHrXG1z9XUAAAAAAAAA1FzlKmz+61//ctwvc/PmzZo4caJ+9atf6b333pMkZWRkSJKCgoKc9gsKCnJsy8jIUJMmTZy2e3l5KSAgwKlNSc/x49corc3V7SXJz89XTk6O0wIAAAAAAADAesr1rejFxcXq1q2b5s2bJ0nq2rWrDh8+rBUrVmjUqFGVEmBFmj9/vubOnevqMAAAAAAAAADcpnLN2AwODlb79u2d1rVr105paWmSJLvdLknKzMx0apOZmenYZrfbde7cOaftV65c0YULF5zalPQcP36N0tpc3V6SmTNnKjs727GcPn365p0GAAAAAAAAUO2Uq7DZo0cPHTt2zGndP//5TzVv3lySFBoaKrvdrqSkJMf2nJwc7d27V5GRkZKkyMhIZWVlKSUlxdFm27ZtKi4uVkREhKPNzp07VVhY6GiTmJiosLAwxzewR0ZGOr3O1TZXX6ckPj4+8vX1dVoAAAAAAAAAWE+5CptTpkzRnj17NG/ePB0/flzvv/++3nrrLcXGxkqSPDw8NHnyZL388sv66KOPdOjQIY0cOVIhISEaNGiQpB9mePbv31/jx4/XZ599pl27dikuLk7Dhg1TSEiIJOnxxx+XzWbTuHHjdOTIEX3wwQdaunSppk6d6ojl6aefVkJCgn7729/qyy+/1Jw5c7R//37FxcVV0NAAAAAAAAAAqK7KdY/Ne++9V+vXr9fMmTP14osvKjQ0VK+99ppGjBjhaDN9+nTl5eVpwoQJysrK0v3336+EhATVrl3b0Wb16tWKi4tT37595enpqSFDhuj11193bPfz89OWLVsUGxur8PBwBQYGatasWZowYYKjTffu3fX+++/rN7/5jZ5//nnddddd2rBhg+6+++7bGQ8AAAAAFazFjE2uDgEAANRAHsYY4+ogXCUnJ0d+fn5qOnmtPH3q3rDtqQUDqygqAABQla7mA9nZ2dymBiiH8vzuUNgEAMBZTaozuTKfLtel6AAAAAAAAABQHVDYBAAAAAAAAGA5FDYBAAAAAAAAWA6FTQAAAAAAAACWQ2ETAAAAAAAAgOVQ2AQAAAAAAABgORQ2AQAAAAAAAFgOhU0AAAAAAAAAlkNhEwAAAAAAAIDlUNgEAAAAAAAAYDlerg4AAAAAAAAAcCctZmwqU7tTCwZWciTWxoxNAAAAAAAAAJZDYRMAAAAAAACA5VDYBAAAAAAAAGA5FDYBAAAAAAAAWA6FTQAAAAAAAACWQ2ETAAAAAAAAgOVQ2AQAAAAAAABgORQ2AQAAAAAAAFgOhU0AAACgGtm5c6ceeughhYSEyMPDQxs2bHDabozRrFmzFBwcrDp16igqKkpfffWVU5sLFy5oxIgR8vX1lb+/v8aNG6fc3FynNgcPHtQDDzyg2rVrq2nTplq4cGFldw0AAKBCUdgEAAAAqpG8vDx17txZy5YtK3H7woUL9frrr2vFihXau3ev6tWrp+joaF2+fNnRZsSIETpy5IgSExO1ceNG7dy5UxMmTHBsz8nJUb9+/dS8eXOlpKTo1Vdf1Zw5c/TWW29Vev8AAAAqiperAwAAAADwHzExMYqJiSlxmzFGr732mn7zm9/o5z//uSTpj3/8o4KCgrRhwwYNGzZMR48eVUJCgvbt26du3bpJkt544w0NGDBAixYtUkhIiFavXq2CggK9++67stls6tChg1JTU7V48WKnAigAAEB1xoxNAAAAwCJOnjypjIwMRUVFOdb5+fkpIiJCycnJkqTk5GT5+/s7ipqSFBUVJU9PT+3du9fRpmfPnrLZbI420dHROnbsmL777rsSXzs/P185OTlOCwAAgCtR2AQAAAAsIiMjQ5IUFBTktD4oKMixLSMjQ02aNHHa7uXlpYCAAKc2JT3Hj1/jWvPnz5efn59jadq06e13CAAA4DZQ2AQAAABwUzNnzlR2drZjOX36tKtDAgAAbu62CpsLFiyQh4eHJk+e7Fh3+fJlxcbGqlGjRqpfv76GDBmizMxMp/3S0tI0cOBA1a1bV02aNNGzzz6rK1euOLXZvn277rnnHvn4+Kh169ZauXLlda+/bNkytWjRQrVr11ZERIQ+++yz2+kOAAAAUK3Z7XZJui6/zszMdGyz2+06d+6c0/YrV67owoULTm1Keo4fv8a1fHx85Ovr67QAAAC40i1/edC+ffv05ptvqlOnTk7rp0yZok2bNmndunXy8/NTXFycBg8erF27dkmSioqKNHDgQNntdu3evVvp6ekaOXKkvL29NW/ePEk/3Dto4MCBeuqpp7R69WolJSXpySefVHBwsKKjoyVJH3zwgaZOnaoVK1YoIiJCr732muO+QNdeegMAAADUBKGhobLb7UpKSlKXLl0k/fAN53v37tXEiRMlSZGRkcrKylJKSorCw8MlSdu2bVNxcbEiIiIcbX7961+rsLBQ3t7ekqTExESFhYWpYcOG5Yrp7tmb5elTt4J6CAAAUHa3NGMzNzdXI0aM0B/+8AenxCc7O1vvvPOOFi9erD59+ig8PFzx8fHavXu39uzZI0nasmWLvvjiC61atUpdunRRTEyMXnrpJS1btkwFBQWSpBUrVig0NFS//e1v1a5dO8XFxem//uu/tGTJEsdrLV68WOPHj9eYMWPUvn17rVixQnXr1tW77757O+MBAAAAuFRubq5SU1OVmpoq6Yd/+qempiotLc1xtdTLL7+sjz76SIcOHdLIkSMVEhKiQYMGSZLatWun/v37a/z48frss8+0a9cuxcXFadiwYQoJCZEkPf7447LZbBo3bpyOHDmiDz74QEuXLtXUqVNd1GsAAIDyu6XCZmxsrAYOHOj0bYySlJKSosLCQqf1bdu2VbNmzZy+pbFjx45ONyuPjo5WTk6Ojhw54mhz7XNHR0c7nqOgoEApKSlObTw9PRUVFeVoAwAAAFjR/v371bVrV3Xt2lWSNHXqVHXt2lWzZs2SJE2fPl2TJk3ShAkTdO+99yo3N1cJCQmqXbu24zlWr16ttm3bqm/fvhowYIDuv/9+vfXWW47tfn5+2rJli06ePKnw8HA988wzmjVrliZMmFC1nQUAALgN5b4Ufc2aNfr888+1b9++67ZlZGTIZrPJ39/faf2139J4s29gLK1NTk6OLl26pO+++05FRUUltvnyyy9LjT0/P1/5+fmOxzk5OTfpLQAAAFC1evXqJWNMqds9PDz04osv6sUXXyy1TUBAgN5///0bvk6nTp30f//3f7ccJwAAgKuVa8bm6dOn9fTTT2v16tVO/xG2ivnz58vPz8+xNG3a1NUhAQAAAAAAALgF5SpspqSk6Ny5c7rnnnvk5eUlLy8v7dixQ6+//rq8vLwUFBSkgoICZWVlOe137bc03uwbGEtr4+vrqzp16igwMFC1atW64bdBlmTmzJnKzs52LKdPny5P9wEAAAAAAABUE+UqbPbt21eHDh1y3Mw8NTVV3bp104gRIxw/e3t7KykpybHPsWPHlJaWpsjISEk/fAPjoUOHdO7cOUebxMRE+fr6qn379o42P36Oq22uPofNZlN4eLhTm+LiYiUlJTnalMTHx0e+vr5OCwAAAAAAAADrKdc9Nhs0aKC7777baV29evXUqFEjx/px48Zp6tSpCggIkK+vryZNmqTIyEj95Cc/kST169dP7du31xNPPKGFCxcqIyNDv/nNbxQbGysfHx9J0lNPPaXf/e53mj59usaOHatt27Zp7dq12rRpk+N1p06dqlGjRqlbt26677779NprrykvL09jxoy5rQEBAAAAAAAAUP2V+8uDbmbJkiXy9PTUkCFDlJ+fr+joaP3+9793bK9Vq5Y2btyoiRMnKjIyUvXq1dOoUaOcbn4eGhqqTZs2acqUKVq6dKnuvPNOvf3224qOjna0GTp0qL755hvNmjVLGRkZ6tKlixISEq77QiEAAAAAAAAANY+HudFXLtZwOTk5P3yJ0OS18vSpe8O2pxYMrKKoAABAVbqaD2RnZ3ObGqAcypNLAwCAW2OFepQr8+ly3WMTAAAAAAAAAKoDCpsAAAAAAAAALIfCJgAAAAAAAADLobAJAAAAAAAAwHIobAIAAAAAAACwHAqbAAAAAAAAACyHwiYAAAAAAAAAy6GwCQAAAAAAAMByKGwCAAAAAAAAsBwKmwAAAAAAAAAsh8ImAAAAAAAAAMvxcnUAAAAAAAAAAK7XYsamMrc9tWBgJUZSPTFjEwAAAAAAAIDlUNgEAAAAAAAAYDkUNgEAAAAAAABYDoVNAAAAAAAAAJZDYRMAAAAAAACA5VDYBAAAAAAAAGA5FDYBAAAAAAAAWA6FTQAAAAAAAACWQ2ETAAAAAAAAgOVQ2AQAAAAAAABgORQ2AQAAAAuZM2eOPDw8nJa2bds6tl++fFmxsbFq1KiR6tevryFDhigzM9PpOdLS0jRw4EDVrVtXTZo00bPPPqsrV65UdVcAAABui5erAwAAAABQPh06dNDWrVsdj728/pPWT5kyRZs2bdK6devk5+enuLg4DR48WLt27ZIkFRUVaeDAgbLb7dq9e7fS09M1cuRIeXt7a968eVXeFwAAgFtFYRMAAACwGC8vL9nt9uvWZ2dn65133tH777+vPn36SJLi4+PVrl077dmzRz/5yU+0ZcsWffHFF9q6dauCgoLUpUsXvfTSS3ruuec0Z84c2Wy2qu4OAADALeFSdAAAAMBivvrqK4WEhKhly5YaMWKE0tLSJEkpKSkqLCxUVFSUo23btm3VrFkzJScnS5KSk5PVsWNHBQUFOdpER0crJydHR44cKfU18/PzlZOT47QAAAC4UrkKm/Pnz9e9996rBg0aqEmTJho0aJCOHTvm1Kai7umzfft23XPPPfLx8VHr1q21cuXK6+JZtmyZWrRoodq1aysiIkKfffZZeboDAAAAWE5ERIRWrlyphIQELV++XCdPntQDDzygixcvKiMjQzabTf7+/k77BAUFKSMjQ5KUkZHhVNS8uv3qttLMnz9ffn5+jqVp06YV2zEAAIByKldhc8eOHYqNjdWePXuUmJiowsJC9evXT3l5eY42U6ZM0ccff6x169Zpx44dOnv2rAYPHuzYfvWePgUFBdq9e7fee+89rVy5UrNmzXK0OXnypAYOHKjevXsrNTVVkydP1pNPPqnNmzc72nzwwQeaOnWqZs+erc8//1ydO3dWdHS0zp07dzvjAQAAAFRrMTExevTRR9WpUydFR0frb3/7m7KysrR27dpKfd2ZM2cqOzvbsZw+fbpSXw8AAOBmylXYTEhI0OjRo9WhQwd17txZK1euVFpamlJSUiT9554+ixcvVp8+fRQeHq74+Hjt3r1be/bskSTHPX1WrVqlLl26KCYmRi+99JKWLVumgoICSdKKFSsUGhqq3/72t2rXrp3i4uL0X//1X1qyZIkjlsWLF2v8+PEaM2aM2rdvrxUrVqhu3bp69913K2psAAAAgGrP399fbdq00fHjx2W321VQUKCsrCynNpmZmY57ctrt9uuuqLr6uKT7dl7l4+MjX19fpwUAAMCVbusem9nZ2ZKkgIAASRV3T5/k5GSn57ja5upzFBQUKCUlxamNp6enoqKiHG1Kwn2BAAAAUNPk5ubqxIkTCg4OVnh4uLy9vZWUlOTYfuzYMaWlpSkyMlKSFBkZqUOHDjld6ZSYmChfX1+1b9++yuMHAAC4Vbdc2CwuLtbkyZPVo0cP3X333ZJUYff0Ka1NTk6OLl26pG+//VZFRUUltuG+QAAAAKjJpk2bph07dujUqVPavXu3HnnkEdWqVUvDhw+Xn5+fxo0bp6lTp+rTTz9VSkqKxowZo8jISP3kJz+RJPXr10/t27fXE088oX/84x/avHmzfvOb3yg2NlY+Pj4u7h0AAEDZed3qjrGxsTp8+LD+/ve/V2Q8lWrmzJmaOnWq43FOTg7FTQAAAFjKmTNnNHz4cJ0/f16NGzfW/fffrz179qhx48aSpCVLlsjT01NDhgxRfn6+oqOj9fvf/96xf61atbRx40ZNnDhRkZGRqlevnkaNGqUXX3zRVV0CAAC4JbdU2IyLi9PGjRu1c+dO3XnnnY71P76nz49nbV57T59rv7382nv6lHbfH19fX9WpU0e1atVSrVq1Smxzs/sC8V9oAAAAWNmaNWtuuL127dpatmyZli1bVmqb5s2b629/+1tFhwYAAFClynUpujFGcXFxWr9+vbZt26bQ0FCn7RV1T5/IyEin57ja5upz2Gw2hYeHO7UpLi5WUlKSow0AAAAAAACAmqtcMzZjY2P1/vvv68MPP1SDBg0c97P08/NTnTp1nO7pExAQIF9fX02aNKnUe/osXLhQGRkZ193T56mnntLvfvc7TZ8+XWPHjtW2bdu0du1abdq0yRHL1KlTNWrUKHXr1k333XefXnvtNeXl5WnMmDEVNTYAAAAAAAAAqqlyFTaXL18uSerVq5fT+vj4eI0ePVpSxdzTJzQ0VJs2bdKUKVO0dOlS3XnnnXr77bcVHR3taDN06FB98803mjVrljIyMtSlSxclJCRc94VCAAAAAAAAAGoeD2OMcXUQrpKTk/PDt6NPXitPn7o3bHtqwcAqigoAAFSlq/lAdna2fH19XR0OYBnlyaUBAEDlc1XtypX5dLnusQkAAAAAAAAA1QGFTQAAAAAAAACWQ2ETAAAAAAAAgOVQ2AQAAAAAAABgORQ2AQAAAAAAAFgOhU0AAAAAAAAAluPl6gAAAAAAAAAA3J4WMzaVqd2pBQMrOZKqw4xNAAAAAAAAAJZDYRMAAAAAAACA5VDYBAAAAAAAAGA5FDYBAAAAAAAAWA6FTQAAAAAAAACWQ2ETAAAAAAAAgOVQ2AQAAAAAAABgORQ2AQAAAAAAAFgOhU0AAAAAAAAAlkNhEwAAAAAAAIDlUNgEAAAAAAAAYDkUNgEAAAAAAABYDoVNAAAAAAAAAJZDYRMAAAAAAACA5VDYBAAAAAAAAGA5FDYBAAAAAAAAWI6XqwMAAAAAAAAAUDVazNhUpnanFgys5EhuHzM2AQAAAAAAAFiO5Quby5YtU4sWLVS7dm1FRETos88+c3VIAAAAgCWQSwMAACuzdGHzgw8+0NSpUzV79mx9/vnn6ty5s6Kjo3Xu3DlXhwYAAABUa+TSAADA6ixd2Fy8eLHGjx+vMWPGqH379lqxYoXq1q2rd99919WhAQAAANUauTQAALA6y355UEFBgVJSUjRz5kzHOk9PT0VFRSk5ObnEffLz85Wfn+94nJ2dLUkqzv/+pq+Xk5NzmxEDAIDq6OrfeGOMiyMBqk5V59IAAMB6mk1ZV6Z2V3MBV+TTli1sfvvttyoqKlJQUJDT+qCgIH355Zcl7jN//nzNnTv3uvX/Xj76pq/n99qtRAkAAKzi4sWL8vPzc3UYQJWo6lwaAADUfK7Ipy1b2LwVM2fO1NSpUx2Ps7Ky1Lx5c6WlpfFB5kdycnLUtGlTnT59Wr6+vq4Op1phbErH2JSOsSkdY1M6xqZ0FT02xhhdvHhRISEhFRAdUHNdm0sXFxfrwoULatSokTw8PFwYWeVzx3Oyu/XZ3foruV+f3a2/kvv12d36K1WfPrsyn7ZsYTMwMFC1atVSZmam0/rMzEzZ7fYS9/Hx8ZGPj8916/38/NzmTV8evr6+jEspGJvSMTalY2xKx9iUjrEpXUWODf/ghLupqFza39+/skKsltzxnOxufXa3/kru12d366/kfn12t/5K1aPPrsqnLfvlQTabTeHh4UpKSnKsKy4uVlJSkiIjI10YGQAAAFC9kUsDAICawLIzNiVp6tSpGjVqlLp166b77rtPr732mvLy8jRmzBhXhwYAAABUa+TSAADA6ixd2Bw6dKi++eYbzZo1SxkZGerSpYsSEhKuuwl6aXx8fDR79uwSL093Z4xL6Rib0jE2pWNsSsfYlI6xKR1jA1SM282l3Yk7nnfcrc/u1l/J/frsbv2V3K/P7tZfyT37fC0P44rvYgcAAAAAAACA22DZe2wCAAAAAAAAcF8UNgEAAAAAAABYDoVNAAAAAAAAAJZDYRMAAAAAAACA5dT4wuayZcvUokUL1a5dWxEREfrss89u2H7dunVq27atateurY4dO+pvf/tbFUVatcozLitXrpSHh4fTUrt27SqMturs3LlTDz30kEJCQuTh4aENGzbcdJ/t27frnnvukY+Pj1q3bq2VK1dWepyuUN6x2b59+3XvGw8PD2VkZFRNwFVk/vz5uvfee9WgQQM1adJEgwYN0rFjx266nzuca25lbNzlfLN8+XJ16tRJvr6+8vX1VWRkpD755JMb7uMO7xmp/GPjLu8ZAFWrtDzGw8ND+/btK3W/Xr16Xdf+qaeeqsLIb12LFi2ui33BggU33Ofy5cuKjY1Vo0aNVL9+fQ0ZMkSZmZlVFPHtOXXqlMaNG6fQ0FDVqVNHrVq10uzZs1VQUHDD/ax0jN3ps7A75p1z5sy5Lv62bdvecB8rH+OSzlEeHh6KjY0tsb0Vj+/NPnMbYzRr1iwFBwerTp06ioqK0ldffXXT5y3vucBqanRh84MPPtDUqVM1e/Zsff755+rcubOio6N17ty5Etvv3r1bw4cP17hx43TgwAENGjRIgwYN0uHDh6s48spV3nGRJF9fX6WnpzuWr7/+ugojrjp5eXnq3Lmzli1bVqb2J0+e1MCBA9W7d2+lpqZq8uTJevLJJ7V58+ZKjrTqlXdsrjp27JjTe6dJkyaVFKFr7NixQ7GxsdqzZ48SExNVWFiofv36KS8vr9R93OVccytjI7nH+ebOO+/UggULlJKSov3796tPnz76+c9/riNHjpTY3l3eM1L5x0Zyj/cMgKrVvXt3p/NKenq6nnzySYWGhqpbt2433Hf8+PFO+y1cuLCKor59L774olPskyZNumH7KVOm6OOPP9a6deu0Y8cOnT17VoMHD66iaG/Pl19+qeLiYr355ps6cuSIlixZohUrVuj555+/6b5WOMbu9lnYXfPODh06OMX/97//vdS2Vj/G+/btc+prYmKiJOnRRx8tdR+rHd+bfeZeuHChXn/9da1YsUJ79+5VvXr1FB0drcuXL5f6nLdS/7EcU4Pdd999JjY21vG4qKjIhISEmPnz55fY/rHHHjMDBw50WhcREWF+8YtfVGqcVa284xIfH2/8/PyqKLrqQ5JZv379DdtMnz7ddOjQwWnd0KFDTXR0dCVG5nplGZtPP/3USDLfffddlcRUXZw7d85IMjt27Ci1jbuca65VlrFx1/ONMcY0bNjQvP322yVuc9f3zFU3Ght3fs8AqDoFBQWmcePG5sUXX7xhuwcffNA8/fTTVRNUBWvevLlZsmRJmdtnZWUZb29vs27dOse6o0ePGkkmOTm5EiKsfAsXLjShoaE3bGOVY+zun4XdIe+cPXu26dy5c5nb17Rj/PTTT5tWrVqZ4uLiErdb/fhe+5m7uLjY2O128+qrrzrWZWVlGR8fH/PnP/+51Ocp77nAimrsjM2CggKlpKQoKirKsc7T01NRUVFKTk4ucZ/k5GSn9pIUHR1dansrupVxkaTc3Fw1b95cTZs2venMGXfiDu+Z29WlSxcFBwfrpz/9qXbt2uXqcCpddna2JCkgIKDUNu76vinL2Ejud74pKirSmjVrlJeXp8jIyBLbuOt7pixjI7nfewZA1fvoo490/vx5jRkz5qZtV69ercDAQN19992aOXOmvv/++yqIsGIsWLBAjRo1UteuXfXqq6/qypUrpbZNSUlRYWGh09+ntm3bqlmzZpb9+5SdnX3TPEWq/seYz8Luk3d+9dVXCgkJUcuWLTVixAilpaWV2rYmHeOCggKtWrVKY8eOlYeHR6ntrH58f+zkyZPKyMhwOoZ+fn6KiIgo9Rjeav3HarxcHUBl+fbbb1VUVKSgoCCn9UFBQfryyy9L3CcjI6PE9jXpnoC3Mi5hYWF699131alTJ2VnZ2vRokXq3r27jhw5ojvvvLMqwq62SnvP5OTk6NKlS6pTp46LInO94OBgrVixQt26dVN+fr7efvtt9erVS3v37tU999zj6vAqRXFxsSZPnqwePXro7rvvLrWdO5xrrlXWsXGn882hQ4cUGRmpy5cvq379+lq/fr3at29fYlt3e8+UZ2zc6T0DwHXeeecdRUdH3/S88vjjj6t58+YKCQnRwYMH9dxzz+nYsWP661//WkWR3rpf/epXuueeexQQEKDdu3dr5syZSk9P1+LFi0tsn5GRIZvNJn9/f6f1Vv37dPz4cb3xxhtatGjRDdtZ4Ri7+2dhd8k7IyIitHLlSoWFhSk9PV1z587VAw88oMOHD6tBgwbXta9Jx3jDhg3KysrS6NGjS21j9eN7ravHqTzH8FbOBVZUYwubqDiRkZFOM2W6d++udu3a6c0339RLL73kwshQnYWFhSksLMzxuHv37jpx4oSWLFmiP/3pTy6MrPLExsbq8OHDN7y3jbsq69i40/kmLCxMqampys7O1v/+7/9q1KhR2rFjR6kFPHdSnrFxp/cMgNs3Y8YMvfLKKzdsc/ToUacv4Dhz5ow2b96stWvX3vT5J0yY4Pi5Y8eOCg4OVt++fXXixAm1atXq1gO/ReXp79SpUx3rOnXqJJvNpl/84heaP3++fHx8KjvUCnMrx/jf//63+vfvr0cffVTjx4+/4b7V7Rjjeu6Sd8bExDh+7tSpkyIiItS8eXOtXbtW48aNc2Fkle+dd95RTEyMQkJCSm1j9eOLsquxhc3AwEDVqlXrum/ly8zMlN1uL3Efu91ervZWdCvjci1vb2917dpVx48fr4wQLaW094yvr69bz9YszX333Vdji35xcXHauHGjdu7cedP/ALrDuebHyjM216rJ5xubzabWrVtLksLDw7Vv3z4tXbpUb7755nVt3e09U56xuVZNfs8AuH3PPPPMDWf4SFLLli2dHsfHx6tRo0Z6+OGHy/16ERERkn6YDeiKotet9PeqiIgIXblyRadOnXL6Z/VVdrtdBQUFysrKcpq16eq/T+Xt89mzZ9W7d291795db731Vrlfz9XHuCTu/FnYnfNOf39/tWnTptT4a8ox/vrrr7V169Zyz5K2+vG9epwyMzMVHBzsWJ+ZmakuXbqUuE9F1H+soMbeY9Nmsyk8PFxJSUmOdcXFxUpKSir1Pl2RkZFO7SUpMTHxhvf1sppbGZdrFRUV6dChQ06/TO7KHd4zFSk1NbXGvW+MMYqLi9P69eu1bds2hYaG3nQfd3nf3MrYXMudzjfFxcXKz88vcZu7vGdKc6OxuZY7vWcAlF/jxo3Vtm3bGy42m83R3hij+Ph4jRw5Ut7e3uV+vdTUVEly2TmpvP39sdTUVHl6eqpJkyYlbg8PD5e3t7fT36djx44pLS3NpX+fytPnf//73+rVq5fCw8MVHx8vT8/yfzx29TEuiTt+Fibv/OF+kidOnCg1fqsf46vi4+PVpEkTDRw4sFz7Wf34hoaGym63Ox3DnJwc7d27t9RjWBH1H0tw7XcXVa41a9YYHx8fs3LlSvPFF1+YCRMmGH9/f5ORkWGMMeaJJ54wM2bMcLTftWuX8fLyMosWLTJHjx41s2fPNt7e3ubQoUOu6kKlKO+4zJ0712zevNmcOHHCpKSkmGHDhpnatWubI0eOuKoLlebixYvmwIED5sCBA0aSWbx4sTlw4ID5+uuvjTHGzJgxwzzxxBOO9v/6179M3bp1zbPPPmuOHj1qli1bZmrVqmUSEhJc1YVKU96xWbJkidmwYYP56quvzKFDh8zTTz9tPD09zdatW13VhUoxceJE4+fnZ7Zv327S09Mdy/fff+9o467nmlsZG3c538yYMcPs2LHDnDx50hw8eNDMmDHDeHh4mC1bthhj3Pc9Y0z5x8Zd3jMAXGPr1q1Gkjl69Oh1286cOWPCwsLM3r17jTHGHD9+3Lz44otm//795uTJk+bDDz80LVu2ND179qzqsMtt9+7dZsmSJSY1NdWcOHHCrFq1yjRu3NiMHDnS0eba/hpjzFNPPWWaNWtmtm3bZvbv328iIyNNZGSkK7pQbmfOnDGtW7c2ffv2NWfOnHHKVX7cxqrH2N0+C7tj3vnMM8+Y7du3m5MnT5pdu3aZqKgoExgYaM6dO2eMqXnH2JgfvtG7WbNm5rnnnrtuW004vjf7zL1gwQLj7+9vPvzwQ3Pw4EHz85//3ISGhppLly45nqNPnz7mjTfecDy+2bmgJqjRhU1jjHnjjTdMs2bNjM1mM/fdd5/Zs2ePY9uDDz5oRo0a5dR+7dq1pk2bNsZms5kOHTqYTZs2VXHEVaM84zJ58mRH26CgIDNgwADz+eefuyDqyvfpp58aSdctV8dj1KhR5sEHH7xuny5duhibzWZatmxp4uPjqzzuqlDesXnllVdMq1atTO3atU1AQIDp1auX2bZtm2uCr0QljYkkp/eBu55rbmVs3OV8M3bsWNO8eXNjs9lM48aNTd++fR2FO2Pc9z1jTPnHxl3eMwBcY/jw4aZ79+4lbjt58qSRZD799FNjjDFpaWmmZ8+eJiAgwPj4+JjWrVubZ5991mRnZ1dhxLcmJSXFREREGD8/P1O7dm3Trl07M2/ePHP58mVHm2v7a4wxly5dMr/85S9Nw4YNTd26dc0jjzziVBiszuLj40vNVa6y+jF2p8/C7ph3Dh061AQHBxubzWbuuOMOM3ToUHP8+HHH9pp2jI0xZvPmzUaSOXbs2HXbasLxvdln7uLiYvPCCy+YoKAg4+PjY/r27XvdWDRv3tzMnj3bad2NzgU1gYcxxlTCRFAAAAAAAAAAqDQ19h6bAAAAAAAAAGouCpsAAAAAAAAALIfCJgAAAAAAAADLobAJAAAAAAAAwHIobAIAAAAAAACwHAqbAAAAAAAAACyHwiYAAAAAAAAAy6GwCQAAqr2dO3fqoYceUkhIiDw8PLRhw4ZyP4cxRosWLVKbNm3k4+OjO+64Q//zP/9T8cECAAAAqBIUNgHUWL169dLkyZNdHQaACpCXl6fOnTtr2bJlt/wcTz/9tN5++20tWrRIX375pT766CPdd999FRglAADV0+jRo+Xh4eFYGjVqpP79++vgwYOuDg0AbguFTQAAUO3FxMTo5Zdf1iOPPFLi9vz8fE2bNk133HGH6tWrp4iICG3fvt2x/ejRo1q+fLk+/PBDPfzwwwoNDVV4eLh++tOfVlEPAABwrf79+ys9PV3p6elKSkqSl5eXfvazn7k6LAC4LRQ2AdRIo0eP1o4dO7R06VLHf6ZPnDihcePGKTQ0VHXq1FFYWJiWLl3qtF9JszwHDRqk0aNHV13wAMotLi5OycnJWrNmjQ4ePKhHH31U/fv311dffSVJ+vjjj9WyZUtt3LhRoaGhatGihZ588klduHDBxZEDAFA1fHx8ZLfbZbfb1aVLF82YMUOnT5/WN998I0k6c+aMhg8froCAANWrV0/dunXT3r17HfsvX75crVq1ks1mU1hYmP70pz85Pb+Hh4fefvttPfLII6pbt67uuusuffTRR47t3333nUaMGKHGjRurTp06uuuuuxQfHy9J+uMf/6j69es7/m5L0i9/+Uu1bdtW33//fWUOCwCL83J1AABQGZYuXap//vOfuvvuu/Xiiy9Kkho2bKg777xT69atU6NGjbR7925NmDBBwcHBeuyxx1wcMYBblZaWpvj4eKWlpSkkJESSNG3aNCUkJCg+Pl7z5s3Tv/71L3399ddat26d/vjHP6qoqEhTpkzRf/3Xf2nbtm0u7gEAAFUrNzdXq1atUuvWrdWoUSPl5ubqwQcf1B133KGPPvpIdrtdn3/+uYqLiyVJ69ev19NPP63XXntNUVFR2rhxo8aMGaM777xTvXv3djzv3LlztXDhQr366qt64403NGLECH399dcKCAjQCy+8oC+++EKffPKJAgMDdfz4cV26dEmSNHLkSG3cuFEjRozQ7t27tXnzZr399ttKTk5W3bp1XTJGAKyBwiaAGsnPz082m01169aV3W53rJ87d67j59DQUCUnJ2vt2rUUNgELO3TokIqKitSmTRun9fn5+WrUqJEkqbi4WPn5+frjH//oaPfOO+8oPDxcx44dU1hYWJXHDQBAVdq4caPq168v6Yd7VwcHB2vjxo3y9PTU+++/r2+++Ub79u1TQECAJKl169aOfRctWqTRo0frl7/8pSRp6tSp2rNnjxYtWuRU2Bw9erSGDx8uSZo3b55ef/11ffbZZ+rfv7/S0tLUtWtXdevWTZLUokULp/jefPNNderUSb/61a/017/+VXPmzFF4eHiljQeAmoHCJgC3smzZMr377rtKS0vTpUuXVFBQoC5durg6LAC3ITc3V7Vq1VJKSopq1arltO3qB7jg4GB5eXk5FT/btWsn6YcZnxQ2AQA1Xe/evbV8+XJJP1wW/vvf/14xMTH67LPPlJqaqq5duzqKmtc6evSoJkyY4LSuR48e193WqVOnTo6f69WrJ19fX507d06SNHHiRA0ZMkSff/65+vXrp0GDBql79+6O9g0bNtQ777yj6Ohode/eXTNmzKiQfgOo2bjHJgC3sWbNGk2bNk3jxo3Tli1blJqaqjFjxqigoMDRxtPTU8YYp/0KCwurOlQA5dC1a1cVFRXp3Llzat26tdNydcZ2jx49dOXKFZ04ccKx3z//+U9JUvPmzV0SNwAAValevXqOv4/33nuv3n77beXl5ekPf/iD6tSpUyGv4e3t7fTYw8PDcTl7TEyMvv76a02ZMkVnz55V3759NW3aNKf2O3fuVK1atZSenq68vLwKiQlAzUZhE0CNZbPZVFRU5Hi8a9cude/eXb/85S/VtWtXtW7d2qnIIUmNGzdWenq643FRUZEOHz5cZTEDKFlubq5SU1OVmpoqSTp58qRSU1OVlpamNm3aaMSIERo5cqT++te/6uTJk/rss880f/58bdq0SZIUFRWle+65R2PHjtWBAweUkpKiX/ziF/rpT3963SXsAAC4Aw8PD3l6eurSpUvq1KmTUlNTS/1SvXbt2mnXrl1O63bt2qX27duX6zUbN26sUaNGadWqVXrttdf01ltvObbt3r1br7zyij7++GPVr19fcXFx5e8UALdDYRNAjdWiRQvt3btXp06d0rfffqu77rpL+/fv1+bNm/XPf/5TL7zwgvbt2+e0T58+fbRp0yZt2rRJX375pSZOnKisrCzXdACAw/79+9W1a1d17dpV0g/39uratatmzZolSYqPj9fIkSP1zDPPKCwsTIMGDdK+ffvUrFkzST/Mxv74448VGBionj17auDAgWrXrp3WrFnjsj4BAFCV8vPzlZGRoYyMDB09elSTJk1Sbm6uHnroIQ0fPlx2u12DBg3Srl279K9//Ut/+ctflJycLEl69tlntXLlSi1fvlxfffWVFi9erL/+9a/Xzbi8kVmzZunDDz/U8ePHdeTIEW3cuNFxW5iLFy/qiSee0K9+9SvFxMRo9erV+uCDD/S///u/lTIWAGoO7rEJoMaaNm2aRo0apfbt2+vSpUv68ssvdeDAAQ0dOlQeHh4aPny4fvnLX+qTTz5x7DN27Fj94x//0MiRI+Xl5aUpU6Y43RAdgGv06tXruttE/Ji3t7fmzp3r9AVh1woJCdFf/vKXyggPAIBqLyEhQcHBwZKkBg0aqG3btlq3bp169eolSdqyZYueeeYZDRgwQFeuXFH79u21bNkySdKgQYO0dOlSLVq0SE8//bRCQ0MVHx/v2LcsbDabZs6cqVOnTqlOnTp64IEHHP9gfPrpp1WvXj3NmzdPktSxY0fNmzdPv/jFLxQZGak77rij4gYCQI3iYW70KQEAAAAAAAAAqiEuRQcAAAAAAABgORQ2AQAAAAAAAFgOhU0AAAAAAAAAlkNhEwAAAAAAAIDlUNgEAAAAAAAAYDkUNgEAAAAAAABYDoVNAAAAAAAAAJZDYRMAAAAAAACA5VDYBAAAAAAAAGA5FDYBAAAAAAAAWI6XqwNwpeLiYp09e1YNGjSQh4eHq8MBAAAuYIzRxYsXFRISIk9P/ucLlBW5NAAAkFybT7t1YfPs2bNq2rSpq8MAAADVwOnTp3XnnXe6OgzAMsilAQDAj7kin3brwmaDBg0k/TDwvr6+Lo4GAAC4Qk5Ojpo2berICwCUDbk0AACQXJtPu3Vh8+olM76+viRjAAC4OS6lBcqHXBoAAPyYK/JpbiQFAAAAAAAAwHIobAIAAAAAAACwHAqbAAAAAAAAACzHre+xCVhZixmbytz21IKBlRgJAAAAAKCq8FkQ+A8Km8BtsMoflLLGyR+9G7PK8QYAAAAAic+CqPkqvLC5c+dOvfrqq0pJSVF6errWr1+vQYMGObaPHj1a7733ntM+0dHRSkhIcDy+cOGCJk2apI8//lienp4aMmSIli5dqvr16zvaHDx4ULGxsdq3b58aN26sSZMmafr06RXdHaDKlad4BlhVZSRY7pq0uWu/gZqMfBpATUXeAqCiVXhhMy8vT507d9bYsWM1ePDgEtv0799f8fHxjsc+Pj5O20eMGKH09HQlJiaqsLBQY8aM0YQJE/T+++9LknJyctSvXz9FRUVpxYoVOnTokMaOHSt/f39NmDChortULszoAgAAwO1w93waAACgrCq8sBkTE6OYmJgbtvHx8ZHdbi9x29GjR5WQkKB9+/apW7dukqQ33nhDAwYM0KJFixQSEqLVq1eroKBA7777rmw2mzp06KDU1FQtXryYRKwG4b95ACqDFc4tzNwG3Bv5NADAnVghP0f15ZJ7bG7fvl1NmjRRw4YN1adPH7388stq1KiRJCk5OVn+/v6OJEySoqKi5Onpqb179+qRRx5RcnKyevbsKZvN5mgTHR2tV155Rd99950aNmxY4uvm5+crPz/f8TgnJ6eSeghcj0IFUH783gBAyVyRT1dVLs0H3IrBlWSwMs4DVY8xh1VVeWGzf//+Gjx4sEJDQ3XixAk9//zziomJUXJysmrVqqWMjAw1adLEOUgvLwUEBCgjI0OSlJGRodDQUKc2QUFBjm2lFTbnz5+vuXPnVkKvgJrBlX/M+ENaffHBCACqF1fl0+TSKCtyh5ur6Ny3MsbcXfNzd+03bqwyJlzwHqoYVV7YHDZsmOPnjh07qlOnTmrVqpW2b9+uvn37Vuprz5w5U1OnTnU8zsnJUdOmTSv1NSsCX7IBVC5Xzgrkd7FmsMJxtEKMAMrGVfm0VXPpyuCu51R3vZLCCv/8d/VzukpNK6LXpGPjau56nnZHLrkU/cdatmypwMBAHT9+XH379pXdbte5c+ec2ly5ckUXLlxw3EfIbrcrMzPTqc3Vx6Xda0j64V5E195Y3ZU4aVU9Tm6wKqv8h5DzGgBUvarKp90hl7ZCDkihq/piHAFn/E6gKri8sHnmzBmdP39ewcHBkqTIyEhlZWUpJSVF4eHhkqRt27apuLhYERERjja//vWvVVhYKG9vb0lSYmKiwsLCSr0MHTUXyV3VY3yAmqumzXwA3AH5NPAf5KlVjzGHlfH+tb4KL2zm5ubq+PHjjscnT55UamqqAgICFBAQoLlz52rIkCGy2+06ceKEpk+frtatWys6OlqS1K5dO/Xv31/jx4/XihUrVFhYqLi4OA0bNkwhISGSpMcff1xz587VuHHj9Nxzz+nw4cNaunSplixZUtHdsRR+IQGUB+eMG+OfJgBchXy65uC8X3EYS9yuin4P8c9goHqo8MLm/v371bt3b8fjq/fhGTVqlJYvX66DBw/qvffeU1ZWlkJCQtSvXz+99NJLTpe1rF69WnFxcerbt688PT01ZMgQvf76647tfn5+2rJli2JjYxUeHq7AwEDNmjVLEyZMqOjuoBKQlKCqWOHemQAAXIt8GgBqlpr02aAm9QU1g4cxxrg6CFfJycmRn5+fsrOz5evrWyHPWdN+ySvjW/iA21EZX5IF4NbUlNkHlZEPAO6gsn53+PvtXvi8AQA3ZoWc25X5tMvvsYnqjQQC1Q3vSQAAgJqD3A4AcDs8XR0AAAAAAAAAAJQXhU0AAAAAAAAAlkNhEwAAAAAAAIDlUNgEAAAAAAAAYDkUNgEAAAAAAABYDoVNAAAAAAAAAJbj5eoArKLFjE2uDgEAAAAAAADA/8eMTQAAAAAAAACWQ2ETAAAAAAAAgOVwKToAALglZb1Ny6kFAys5EgDVHbd1AgAAlYEZmwAAAAAAAAAsh8ImAAAAAAAAAMuhsAkAAAAAAADAcihsAgAAAAAAALAcCpsAAAAAAAAALIfCJgAAAAAAAADLobAJAAAAAAAAwHIobAIAAAAAAACwHAqbAAAAAAAAACyHwiYAAAAAAAAAy6GwCQAAAAAAAMByKGwCAAAAAAAAsBwKmwAAAAAAAAAsh8ImAAAAAAAAAMuhsAkAAAAAAADAcihsAgAAAAAAALAcCpsAAAAAAAAALIfCJgAAAAAAAADLqfDC5s6dO/XQQw8pJCREHh4e2rBhg9N2Y4xmzZql4OBg1alTR1FRUfrqq6+c2ly4cEEjRoyQr6+v/P39NW7cOOXm5jq1OXjwoB544AHVrl1bTZs21cKFCyu6KwAAAECVI58GAAAomwovbObl5alz585atmxZidsXLlyo119/XStWrNDevXtVr149RUdH6/Lly442I0aM0JEjR5SYmKiNGzdq586dmjBhgmN7Tk6O+vXrp+bNmyslJUWvvvqq5syZo7feequiuwMAAABUKfJpAACAsvEwxphKe3IPD61fv16DBg2S9MN/l0NCQvTMM89o2rRpkqTs7GwFBQVp5cqVGjZsmI4ePar27dtr37596tatmyQpISFBAwYM0JkzZxQSEqLly5fr17/+tTIyMmSz2SRJM2bM0IYNG/Tll1+WOb6cnBz5+fkpOztbvr6+N2zbYsamWxgBAABwasFAV4dwQ+XJB4CqVp3zaXJpAACqF1fl3a7Mp6v0HpsnT55URkaGoqKiHOv8/PwUERGh5ORkSVJycrL8/f0dSZgkRUVFydPTU3v37nW06dmzpyMJk6To6GgdO3ZM3333Xamvn5+fr5ycHKcFAAAAsApX5tPk0gAAoLqp0sJmRkaGJCkoKMhpfVBQkGNbRkaGmjRp4rTdy8tLAQEBTm1Keo4fv0ZJ5s+fLz8/P8fStGnT2+sQAAAAUIVcmU+TSwMAgOrGy9UBVKWZM2dq6tSpjsc5OTlq2rSp7p69WZ4+dV0YGQAAAFC9lZZLAwAAuEqVzti02+2SpMzMTKf1mZmZjm12u13nzp1z2n7lyhVduHDBqU1Jz/Hj1yiJj4+PfH19nRYAAADAKlyZT5NLAwCA6qZKC5uhoaGy2+1KSkpyrMvJydHevXsVGRkpSYqMjFRWVpZSUlIcbbZt26bi4mJFREQ42uzcuVOFhYWONomJiQoLC1PDhg2rqDcAAABA1SKfBgAA+I8KL2zm5uYqNTVVqampkn64wXlqaqrS0tLk4eGhyZMn6+WXX9ZHH32kQ4cOaeTIkQoJCXF802O7du3Uv39/jR8/Xp999pl27dqluLg4DRs2TCEhIZKkxx9/XDabTePGjdORI0f0wQcfaOnSpU6XxgAAAABWRD4NAABQNhV+j839+/erd+/ejsdXk6NRo0Zp5cqVmj59uvLy8jRhwgRlZWXp/vvvV0JCgmrXru3YZ/Xq1YqLi1Pfvn3l6empIUOG6PXXX3ds9/Pz05YtWxQbG6vw8HAFBgZq1qxZmjBhQkV3BwAAAKhS5NMAAABl42GMMa4OwlVycnJ++EbHyWv58iAAAFzs1IKBLnndq/lAdnY29wwEyqE8vzstZmyqoqgAAHBf7phPV+k9NgEAAAAAAACgIlT4pegAAAAA3Mfdszdz9RMAAHAJZmwCAAAAAAAAsBwKmwAAAAAAAAAsh8ImAAAAAAAAAMuhsAkAAAAAAADAcihsAgAAAAAAALAcCpsAAAAAAAAALIfCJgAAAAAAAADLobAJAAAAAAAAwHIobAIAAAAAAACwHAqbAAAAAAAAACyHwiYAAAAAAAAAy6GwCQAAAAAAAMByKGwCAAAAAAAAsBwKmwAAAAAAAAAsh8ImAAAAAAAAAMuhsAkAAAAAAADAcihsAgAAAAAAALAcCpsAAAAAAAAALIfCJgAAAAAAAADLobAJAAAAAAAAwHIobAIAAAAAAACwHAqbAAAAAAAAACzHy9UBAAAAAAAAALg9LWZsKlO7UwsGVnIkVYcZmwAAAAAAAAAsh8ImAAAAAAAAAMuhsAkAAAAAAADAclxS2JwzZ448PDyclrZt2zq2X758WbGxsWrUqJHq16+vIUOGKDMz0+k50tLSNHDgQNWtW1dNmjTRs88+qytXrlR1VwAAAIAqRS4NAADwA5d9eVCHDh20devW/wTi9Z9QpkyZok2bNmndunXy8/NTXFycBg8erF27dkmSioqKNHDgQNntdu3evVvp6ekaOXKkvL29NW/evCrvCwAAAFCVyKUBAABcWNj08vKS3W6/bn12drbeeecdvf/+++rTp48kKT4+Xu3atdOePXv0k5/8RFu2bNEXX3yhrVu3KigoSF26dNFLL72k5557TnPmzJHNZqvq7gAAAABVhlwaAADAhffY/OqrrxQSEqKWLVtqxIgRSktLkySlpKSosLBQUVFRjrZt27ZVs2bNlJycLElKTk5Wx44dFRQU5GgTHR2tnJwcHTlypGo7AgAAAFQxcmkAAAAXzdiMiIjQypUrFRYWpvT0dM2dO1cPPPCADh8+rIyMDNlsNvn7+zvtExQUpIyMDElSRkaGUyJ2dfvVbaXJz89Xfn6+43FOTk4F9QgAAACoGuTSAAAAP3BJYTMmJsbxc6dOnRQREaHmzZtr7dq1qlOnTqW97vz58zV37txKe34AAACgspFLAwAA/MBll6L/mL+/v9q0aaPjx4/LbreroKBAWVlZTm0yMzMd9xGy2+3XfbPj1ccl3WvoqpkzZyo7O9uxnD59umI7AgAAAFQxcmkAAOCuqkVhMzc3VydOnFBwcLDCw8Pl7e2tpKQkx/Zjx44pLS1NkZGRkqTIyEgdOnRI586dc7RJTEyUr6+v2rdvX+rr+Pj4yNfX12kBAAAArIxcGgAAuCuXXIo+bdo0PfTQQ2revLnOnj2r2bNnq1atWho+fLj8/Pw0btw4TZ06VQEBAfL19dWkSZMUGRmpn/zkJ5Kkfv36qX379nriiSe0cOFCZWRk6De/+Y1iY2Pl4+Pjii4BAAAAVYJcGgAA4AcuKWyeOXNGw4cP1/nz59W4cWPdf//92rNnjxo3bixJWrJkiTw9PTVkyBDl5+crOjpav//97x3716pVSxs3btTEiRMVGRmpevXqadSoUXrxxRdd0R0AAFABWszYVOa2pxYMrMRIgOqNXBoAAOAHHsYY4+ogXCUnJ0d+fn5qOnmtPH3qujocAABQRhVZ2LyaD2RnZ3NpLVAO5NIAAFhTRU8ScGU+XS3usQkAAAAAAAAA5UFhEwAAAAAAAIDlUNgEAAAAAAAAYDkUNgEAAAAAAABYDoVNAAAAAAAAAJZDYRMAAAAAAACA5VDYBAAAAAAAAGA5FDYBAAAAAAAAWA6FTQAAAAAAAACWQ2ETAAAAAAAAgOVQ2AQAAAAAAABgORQ2AQAAAAAAAFgOhU0AAAAAAAAAlkNhEwAAAAAAAIDlUNgEAAAAAAAAYDlerg4AAAAAAAAAQNVoMWNTmdqdWjCwkiO5fczYBAAAAAAAAGA5FDYBAAAAAAAAWA6FTQAAAAAAAACWQ2ETAAAAAAAAgOVQ2AQAAAAAAABgORQ2AQAAAAAAAFiOl6sDAAAAKK8WMzaVqd2pBQMrORIAAAAArsKMTQAAAAAAAACWQ2ETAAAAAAAAgOVQ2AQAAAAAAABgORQ2AQAAAAAAAFgOhU0AAAAAAAAAlkNhEwAAAAAAAIDlWL6wuWzZMrVo0UK1a9dWRESEPvvsM1eHBAAAAFgCuTQAALAyL1cHcDs++OADTZ06VStWrFBERIRee+01RUdH69ixY2rSpImrwwMAAACqLXJpAABwIy1mbCpTu+L87ys5ktJ5GGOMy179NkVEROjee+/V7373O0lScXGxmjZtqkmTJmnGjBk33T8nJ0d+fn5qOnmtPH3qVna4AACgGirO/16nX3tM2dnZ8vX1dXU4QJUhlwYAABXBlfm0ZWdsFhQUKCUlRTNnznSs8/T0VFRUlJKTk0vcJz8/X/n5+Y7H2dnZklxbWQYAAK51NQ+w8P96gXIjlwYAABXFlfm0ZQub3377rYqKihQUFOS0PigoSF9++WWJ+8yfP19z5869bv2/l4+ujBABAICFXLx4UX5+fq4OA6gS5NIAAKCiuSKftmxh81bMnDlTU6dOdTwuLi7WhQsX1KhRI3l4eLgwssqXk5Ojpk2b6vTp025zmR19ps81FX2mzzWVq/psjNHFixcVEhJSZa8JWJE759IS52V36bPknv2mz+7RZ8k9++2OfZaqtt+uzKctW9gMDAxUrVq1lJmZ6bQ+MzNTdru9xH18fHzk4+PjtM7f37+yQqyWfH193eoXWaLP7oI+uwf67B5c0WdmasLdkEvfOs7L7sMd+02f3Yc79tsd+yxVXb9dlU97uuRVK4DNZlN4eLiSkpIc64qLi5WUlKTIyEgXRgYAAABUb+TSAACgJrDsjE1Jmjp1qkaNGqVu3brpvvvu02uvvaa8vDyNGTPG1aEBAAAA1Rq5NAAAsDpLFzaHDh2qb775RrNmzVJGRoa6dOmihISE626Cjh8uHZo9e/Z1lw/VZPTZPdBn90Cf3YM79hlwJXLp8nHHc5Q79llyz37TZ/fhjv12xz5L7tNvD+OK72IHAAAAAAAAgNtg2XtsAgAAAAAAAHBfFDYBAAAAAAAAWA6FTQAAAAAAAACWQ2ETAAAAAAAAgOVQ2KyBtm/fLg8PjxKXffv2lbpfr169rmv/1FNPVWHkt6dFixbXxb9gwYIb7nP58mXFxsaqUaNGql+/voYMGaLMzMwqivj2nTp1SuPGjVNoaKjq1KmjVq1aafbs2SooKLjhflY71suWLVOLFi1Uu3ZtRURE6LPPPrth+3Xr1qlt27aqXbu2OnbsqL/97W9VFOntmz9/vu699141aNBATZo00aBBg3Ts2LEb7rNy5crrjmft2rWrKOLbN2fOnOvib9u27Q33sfIxvqqkc5aHh4diY2NLbG/F47xz50499NBDCgkJkYeHhzZs2OC03RijWbNmKTg4WHXq1FFUVJS++uqrmz5vec8JAFBe7ppPS+6XU7tLPi25V04tkVeTV5NXu0teTWGzBurevbvS09OdlieffFKhoaHq1q3bDfcdP368034LFy6soqgrxosvvugU/6RJk27YfsqUKfr444+1bt067dixQ2fPntXgwYOrKNrb9+WXX6q4uFhvvvmmjhw5oiVLlmjFihV6/vnnb7qvVY71Bx98oKlTp2r27Nn6/PPP1blzZ0VHR+vcuXMltt+9e7eGDx+ucePG6cCBAxo0aJAGDRqkw4cPV3Hkt2bHjh2KjY3Vnj17lJiYqMLCQvXr1095eXk33M/X19fpeH799ddVFHHF6NChg1P8f//730tta/VjfNW+ffuc+pyYmChJevTRR0vdx2rHOS8vT507d9ayZctK3L5w4UK9/vrrWrFihfbu3at69eopOjpaly9fLvU5y3tOAIBb4c75tOReObU75NOS++XUEnk1eTV5tdvk1QY1XkFBgWncuLF58cUXb9juwQcfNE8//XTVBFUJmjdvbpYsWVLm9llZWcbb29usW7fOse7o0aNGkklOTq6ECKvGwoULTWho6A3bWOlY33fffSY2NtbxuKioyISEhJj58+eX2P6xxx4zAwcOdFoXERFhfvGLX1RqnJXl3LlzRpLZsWNHqW3i4+ONn59f1QVVwWbPnm06d+5c5vY17Rhf9fTTT5tWrVqZ4uLiErdb/ThLMuvXr3c8Li4uNna73bz66quOdVlZWcbHx8f8+c9/LvV5yntOAICK4C75tDHk1MbUvHzaGHJqY8irS1ITj7Mx5NXGuFdezYxNN/DRRx/p/PnzGjNmzE3brl69WoGBgbr77rs1c+ZMff/991UQYcVZsGCBGjVqpK5du+rVV1/VlStXSm2bkpKiwsJCRUVFOda1bdtWzZo1U3JyclWEWymys7MVEBBw03ZWONYFBQVKSUlxOkaenp6Kiooq9RglJyc7tZek6Ohoyx7T7OxsSbrpMc3NzVXz5s3VtGlT/fznP9eRI0eqIrwK89VXXykkJEQtW7bUiBEjlJaWVmrbmnaMpR/e66tWrdLYsWPl4eFRajurH+cfO3nypDIyMpyOpZ+fnyIiIko9lrdyTgCAiuBO+bRETl2T8mmJnPoq8urr1cTjTF79A3fKq71cHQAq3zvvvKPo6GjdeeedN2z3+OOPq3nz5goJCdHBgwf13HPP6dixY/rrX/9aRZHenl/96le65557FBAQoN27d2vmzJlKT0/X4sWLS2yfkZEhm80mf39/p/VBQUHKyMiogogr3vHjx/XGG29o0aJFN2xnlWP97bffqqioSEFBQU7rg4KC9OWXX5a4T0ZGRontrXhMi4uLNXnyZPXo0UN33313qe3CwsL07rvvqlOnTsrOztaiRYvUvXt3HTly5Ka/99VBRESEVq5cqbCwMKWnp2vu3Ll64IEHdPjwYTVo0OC69jXpGF+1YcMGZWVlafTo0aW2sfpxvtbV41WeY3kr5wQAqAjukk9L5NQ1LZ+WyKkl8mryamdWP87Xcve8msKmhcyYMUOvvPLKDdscPXrU6ebAZ86c0ebNm7V27dqbPv+ECRMcP3fs2FHBwcHq27evTpw4oVatWt164LehPH2eOnWqY12nTp1ks9n0i1/8QvPnz5ePj09lh1qhbuVY//vf/1b//v316KOPavz48Tfctzoea1wvNjZWhw8fvuF9cSQpMjJSkZGRjsfdu3dXu3bt9Oabb+qll16q7DBvW0xMjOPnTp06KSIiQs2bN9fatWs1btw4F0ZWdd555x3FxMQoJCSk1DZWP84AUB24Yz4tuWdOTT6NHyOvJq/+MasfZzijsGkhzzzzzA3/6yBJLVu2dHocHx+vRo0a6eGHHy7360VEREj64b+WrvrjfCt9vioiIkJXrlzRqVOnFBYWdt12u92ugoICZWVlOf2HOTMzU3a7/XbCvm3l7ffZs2fVu3dvde/eXW+99Va5X686HOuSBAYGqlatWtd9q+aNjpHdbi9X++oqLi5OGzdu1M6dO8v9X0Nvb2917dpVx48fr6ToKpe/v7/atGlTavw15Rhf9fXXX2vr1q3lnuFh9eN89XhlZmYqODjYsT4zM1NdunQpcZ9bOScAwI+5Yz4tuWdOTT79H+6cU0vk1eTVN2f14+zueTWFTQtp3LixGjduXOb2xhjFx8dr5MiR8vb2LvfrpaamSpLTL0ZVK2+ffyw1NVWenp5q0qRJidvDw8Pl7e2tpKQkDRkyRJJ07NgxpaWlOf33xhXK0+9///vf6t27t8LDwxUfHy9Pz/LfOrc6HOuS2Gw2hYeHKykpSYMGDZL0w2UkSUlJiouLK3GfyMhIJSUlafLkyY51iYmJLj+mZWWM0aRJk7R+/Xpt375doaGh5X6OoqIiHTp0SAMGDKiECCtfbm6uTpw4oSeeeKLE7VY/xteKj49XkyZNNHDgwHLtZ/XjHBoaKrvdrqSkJEfClZOTo71792rixIkl7nMr5wQA+DF3zKcl98ypyaf/wx1zaom8WiKvLiurH2e3z6td+91FqExbt241kszRo0ev23bmzBkTFhZm9u7da4wx5vjx4+bFF180+/fvNydPnjQffvihadmypenZs2dVh31Ldu/ebZYsWWJSU1PNiRMnzKpVq0zjxo3NyJEjHW2u7bMxxjz11FOmWbNmZtu2bWb//v0mMjLSREZGuqILt+TMmTOmdevWpm/fvubMmTMmPT3dsfy4jZWP9Zo1a4yPj49ZuXKl+eKLL8yECROMv7+/ycjIMMYY88QTT5gZM2Y42u/atct4eXmZRYsWmaNHj5rZs2cbb29vc+jQIVd1oVwmTpxo/Pz8zPbt252O5/fff+9oc22f586dazZv3mxOnDhhUlJSzLBhw0zt2rXNkSNHXNGFcnvmmWfM9u3bzcmTJ82uXbtMVFSUCQwMNOfOnTPG1Lxj/GNFRUWmWbNm5rnnnrtuW004zhcvXjQHDhwwBw4cMJLM4sWLzYEDB8zXX39tjDFmwYIFxt/f33z44Yfm4MGD5uc//7kJDQ01ly5dcjxHnz59zBtvvOF4fLNzAgBUJHfKp41xz5zaHfJpY9wvpzaGvJq8+j9qwnEmry4dhc0abPjw4aZ79+4lbjt58qSRZD799FNjjDFpaWmmZ8+eJiAgwPj4+JjWrVubZ5991mRnZ1dhxLcuJSXFREREGD8/P1O7dm3Trl07M2/ePHP58mVHm2v7bIwxly5dMr/85S9Nw4YNTd26dc0jjzzilMRUd/Hx8UZSictVNeFYv/HGG6ZZs2bGZrOZ++67z+zZs8ex7cEHHzSjRo1yar927VrTpk0bY7PZTIcOHcymTZuqOOJbV9rxjI+Pd7S5ts+TJ092jE9QUJAZMGCA+fzzz6s++Fs0dOhQExwcbGw2m7njjjvM0KFDzfHjxx3ba9ox/rHNmzcbSebYsWPXbasJx/nTTz8t8f18tV/FxcXmhRdeMEFBQcbHx8f07dv3urFo3ry5mT17ttO6G50TAKAiuVM+bYx75tTukk8b4145tTHk1eTV/1ETjjN5dek8jDGmMmaCAgAAAAAAAEBlKf/NQwAAAAAAAADAxShsAgAAAAAAALAcCpsAAAAAAAAALIfCJgAAAAAAAADLobAJAAAAAAAAwHIobAIAAAAAAACwHAqbAAAAAAAAACyHwiYAAAAAAAAAy6GwCQAAAAAAAMByKGwCqPZGjx4tDw8Px9KoUSP1799fBw8edHVoAAAAQLVHPg2gpqKwCcAS+vfvr/T0dKWnpyspKUleXl762c9+5uqwAAAAAEsgnwZQE1HYBGAJPj4+stvtstvt6tKli2bMmKHTp0/rm2++kSSdOXNGw4cPV0BAgOrVq6du3bpp7969jv2XL1+uVq1ayWazKSwsTH/605+cnt/Dw0Nvv/22HnnkEdWtW1d33XWXPvroI8f27777TiNGjFDjxo1Vp04d3XXXXYqPj5ck9enTR3FxcU7P980338hmsykpKamyhgQAAAAos+qcT8+ZM8dpRunVZeXKlZU/MAAsjcImAMvJzc3VqlWr1Lp1azVq1Ei5ubl68MEH9e9//1sfffSR/vGPf2j69OkqLi6WJK1fv15PP/20nnnmGR0+fFi/+MUvNGbMGH366adOzzt37lw99thjOnjwoAYMGKARI0bowoULkqQXXnhBX3zxhT755BMdPXpUy5cvV2BgoCTpySef1Pvvv6/8/HzHc61atUp33HGH+vTpU0WjAgAAAJRNdcunp02b5phNmp6erkWLFqlu3brq1q1b1Q4MAMvxMMYYVwcBADcyevRorVq1SrVr15Yk5eXlKTg4WBs3btQ999yjt956S9OmTdOpU6cUEBBw3f49evRQhw4d9NZbbznWPfbYY8rLy9OmTZsk/fAf5t/85jd66aWXHK9Rv359ffLJJ+rfv78efvhhBQYG6t13373u+S9fvqyQkBCtWLFCjz32mCSpc+fOGjx4sGbPnl3h4wEAAACUR3XPp39sz5496t27t9577z1Hbg0ApWHGJgBL6N27t1JTU5WamqrPPvtM0dHRiomJ0ddff63U1FR17dq1xCRMko4ePaoePXo4revRo4eOHj3qtK5Tp06On+vVqydfX1+dO3dOkjRx4kStWbNGXbp00fTp07V7925H29q1a+uJJ55wJGmff/65Dh8+rNGjR1dE1wEAAIDbVp3z6avS0tI0aNAgTZs2jaImgDKhsAnAEurVq6fWrVurdevWuvfee/X2228rLy9Pf/jDH1SnTp0KeQ1vb2+nxx4eHo7Lb64mfVOmTNHZs2fVt29fTZs2zdH2ySefVGJios6cOaP4+Hj16dNHzZs3r5C4AAAAgNtV3fPpvLw8Pfzww4qMjNSLL75YIfEAqPkobAKwJA8PD3l6eurSpUvq1KmTUlNTHffvuVa7du20a9cup3W7du1S+/bty/WajRs31qhRo7Rq1Sq99tprTpfidOzYUd26ddMf/vAHvf/++xo7dmz5OwUAAABUkeqUTxtj9N///d8qLi7Wn/70J3l4eNxapwC4HS9XBwAAZZGfn6+MjAxJP3yj4u9+9zvl5ubqoYceUvfu3TVv3jwNGjRI8+fPV3BwsA4cOKCQkBBFRkbq2Wef1WOPPaauXbsqKipKH3/8sf76179q69atZX79WbNmKTw8XB06dFB+fr42btyodu3aObV58sknFRcXp3r16umRRx6p0P4DAAAAt6M659Nz5szR1q1btWXLFuXm5io3N1eS5OfnV2GzSQHUTMzYBGAJCQkJCg4OVnBwsCIiIrRv3z6tW7dOvXr1ks1m05YtW9SkSRMNGDBAHTt21IIFC1SrVi1J0qBBg7R06VItWrRIHTp00Jtvvqn4+Hj16tWrzK9vs9k0c+ZMderUST179lStWrW0Zs0apzbDhw+Xl5eXhg8f7rgxOwAAAFAdVOd8eseOHcrNzVX37t0dMQYHB+uDDz6ojKEAUIPwregAUEFOnTqlVq1aad++fbrnnntcHQ4AAAAAADUahU0AuE2FhYU6f/68pk2bppMnT153/yEAAAAAAFDxuBQdAG7Trl27FBwcrH379mnFihWuDgcAAAAAALfAjE0AAAAAAAAAlsOMTQAAAAAAAACWQ2ETAAAAAAAAgOVQ2AQAAAAAAABgORQ2AQAAAAAAAFgOhU0AAAAAAAAAlkNhEwAAAAAAAIDlUNgEAAAAAAAAYDlerg7AlYqLi3X27Fk1aNBAHh4erg4HAAC4gDFGFy9eVEhIiDw9+Z8vUFbk0gAAQHJxPm0q2I4dO8zPfvYzExwcbCSZ9evXO20fNWqUkeS0REdHO7U5f/68efzxx02DBg2Mn5+fGTt2rLl48aJTm3/84x/m/vvvNz4+PubOO+80r7zySrljPX369HWxsLCwsLCwsLjncvr06XLnEoA7I5dmYWFhYWFh+fHiiny6wmds5uXlqXPnzho7dqwGDx5cYpv+/fsrPj7e8djHx8dp+4gRI5Senq7ExEQVFhZqzJgxmjBhgt5//31JUk5Ojvr166eoqCitWLFChw4d0tixY+Xv768JEyaUOdYGDRpIkk6fPi1fX9/ydhUAANQAOTk5atq0qSMvAFA25NIAAEBybT5d4YXNmJgYxcTE3LCNj4+P7HZ7iduOHj2qhIQE7du3T926dZMkvfHGGxowYIAWLVqkkJAQrV69WgUFBXr33Xdls9nUoUMHpaamavHixeUqbF69ZMbX15dkDAAAN8eltKgudu7cqVdffVUpKSlKT0/X+vXrNWjQIMf20aNH67333nPaJzo6WgkJCY7HFy5c0KRJk/Txxx/L09NTQ4YM0dKlS1W/fn1Hm4MHDyo2Nlb79u1T48aNNWnSJE2fPr3McZJLAwCAH3NFPu2SG0lt375dTZo0UVhYmCZOnKjz5887tiUnJ8vf399R1JSkqKgoeXp6au/evY42PXv2lM1mc7SJjo7WsWPH9N1335X6uvn5+crJyXFaAAAAgOrk6hVQy5YtK7VN//79lZ6e7lj+/Oc/O20fMWKEjhw5osTERG3cuFE7d+50mgBw9Qqo5s2bKyUlRa+++qrmzJmjt956q9L6BQAAUNGq/MuD+vfvr8GDBys0NFQnTpzQ888/r5iYGCUnJ6tWrVrKyMhQkyZNnIP08lJAQIAyMjIkSRkZGQoNDXVqExQU5NjWsGHDEl97/vz5mjt3biX0CgAAAKgYVroCCgAAwJWqfMbmsGHD9PDDD6tjx44aNGiQNm7cqH379mn79u2V/tozZ85Udna2Yzl9+nSlvyYAAABQ0VxxBRRXPwEAgOqmymdsXqtly5YKDAzU8ePH1bdvX9ntdp07d86pzZUrV3ThwgXHf6XtdrsyMzOd2lx9XNp/rqUf/rN97RcVofpqMWNTmdqdWjCwkiMBAACoPlx1BVRpVz/dPXuzPH3q3jBm8jUAAFAZXF7YPHPmjM6fP6/g4GBJUmRkpLKyspSSkqLw8HBJ0rZt21RcXKyIiAhHm1//+tcqLCyUt7e3JCkxMVFhYWGlXoZ+IzUlGStrIVCyRn8AAABwvWHDhjl+7tixozp16qRWrVpp+/bt6tu3b6W97syZMzV16lTH46vfgFrdkSMDAFBzVXhhMzc3V8ePH3c8PnnypFJTUxUQEKCAgADNnTtXQ4YMkd1u14kTJzR9+nS1bt1a0dHRkqR27dqpf//+Gj9+vFasWKHCwkLFxcVp2LBhCgkJkSQ9/vjjmjt3rsaNG6fnnntOhw8f1tKlS7VkyZKK7k6NxWzIG2N8AACAVVTVFVDV7eqn8hQsAQBAzVThhc39+/erd+/ejsdX/6s7atQoLV++XAcPHtR7772nrKwshYSEqF+/fnrppZeckqTVq1crLi5Offv2laenp4YMGaLXX3/dsd3Pz09btmxRbGyswsPDFRgYqFmzZnGjc6CKMPMBqLn4/QaspzpcAXUznFsAAEBlqPDCZq9evWSMKXX75s2bb/ocAQEBev/992/YplOnTvq///u/csdnRVb4b7RVklUrjCXcC7ODAQDXcvcroFyZr/F3GQAAa3H5PTatgiQHgFUK+K5Uk86VHG8ArsIVUNVfTfp7V9Pw9xsA3AuFTRdy19mD7tpv3JgrPyC463uSD2UAUD1xBRRQ85GHAUDFoLCJG3LXgk9lqOjkpTL+G83xBioXH2IAALerMvK1iv67Y4UYcXM1LW+paf0B8AMKm0A1UxmJIAVL66tpx5AZurhdfDgBgIrDORUAKg7n1KpFYbOC8YEZcFbRvxP8jrkX7pMFAHAX5DgoCQUS4Nbwu+M+KGzC8kgCAUicCwAAQPnUpMIH/wzG7XLlLSRq0u+iq7njWFLYBIBKUJOKbDWpLwAAAKhYNe3e/1a4ZVJNKkq5mhXGnM9jN0ZhE7gNnGAASJwLAACobFYodMG9uHKWrFXek1aJsyxqUl/Ko6z9Ls7/vpIjKR2FTQAAqhkuJwOAmqGmfRCuaf0Bqgq/O0DlobAJAIAbIKEGAACuRj6C21HT3j81rT+uQmETAAALs8KlecwqBQAAAFAZKGwCAIBKxX+jAQAAAFQGT1cHAAAAAAAAAADlxYxNAABQY5Vltqgrv8URAICqxpUUAGoSZmwCAAAAAAAAsBxmbAIAAAAAAABuoibN3GbGJgAAAAAAAADLobAJAAAAAAAAwHIobAIAAAAAAACwHAqbAAAAAAAAACyHwiYAAAAAAAAAy6GwCQAAAAAAAMByKGwCAAAAAAAAsBwKmwAAAAAAAAAsh8ImAAAAAAAAAMuhsAkAAAAAAADAcihsAgAAAAAAALAcCpsAAAAAAAAALIfCJgAAAAAAAADLobAJAAAAAAAAwHIobAIAAAAAAACwnAovbO7cuVMPPfSQQkJC5OHhoQ0bNjhtN8Zo1qxZCg4OVp06dRQVFaWvvvrKqc2FCxc0YsQI+fr6yt/fX+PGjVNubq5Tm4MHD+qBBx5Q7dq11bRpUy1cuLCiuwIAAAAAAACgmqrwwmZeXp46d+6sZcuWlbh94cKFev3117VixQrt3btX9erVU3R0tC5fvuxoM2LECB05ckSJiYnauHGjdu7cqQkTJji25+TkqF+/fmrevLlSUlL06quvas6cOXrrrbcqujsAAABAlWKiAAAAQNlUeGEzJiZGL7/8sh555JHrthlj9Nprr+k3v/mNfv7zn6tTp0764x//qLNnzzoStqNHjyohIUFvv/22IiIidP/99+uNN97QmjVrdPbsWUnS6tWrVVBQoHfffVcdOnTQsGHD9Ktf/UqLFy+u6O4AAAAAVYqJAgAAAGVTpffYPHnypDIyMhQVFeVY5+fnp4iICCUnJ0uSkpOT5e/vr27dujnaREVFydPTU3v37nW06dmzp2w2m6NNdHS0jh07pu+++67U18/Pz1dOTo7TAgAAAFQnTBQAAAAomyotbGZkZEiSgoKCnNYHBQU5tmVkZKhJkyZO2728vBQQEODUpqTn+PFrlGT+/Pny8/NzLE2bNr29DgEAAABVyNUTBQAAAKoTt/pW9JkzZyo7O9uxnD592tUhAQAAAGXmyokCXP0EAACqmyotbNrtdklSZmam0/rMzEzHNrvdrnPnzjltv3Llii5cuODUpqTn+PFrlMTHx0e+vr5OCwAAAICb4+onAABQ3VRpYTM0NFR2u11JSUmOdTk5Odq7d68iIyMlSZGRkcrKylJKSoqjzbZt21RcXKyIiAhHm507d6qwsNDRJjExUWFhYWrYsGEV9QYAAACoWq6cKMDVTwAAoLqp8MJmbm6uUlNTlZqaKumH+wClpqYqLS1NHh4emjx5sl5++WV99NFHOnTokEaOHKmQkBANGjRIktSuXTv1799f48eP12effaZdu3YpLi5Ow4YNU0hIiCTp8ccfl81m07hx43TkyBF98MEHWrp0qaZOnVrR3QEAAACqDVdOFODqJwAAUN14VfQT7t+/X71793Y8vlpsHDVqlFauXKnp06crLy9PEyZMUFZWlu6//34lJCSodu3ajn1Wr16tuLg49e3bV56enhoyZIhef/11x3Y/Pz9t2bJFsbGxCg8PV2BgoGbNmqUJEyZUdHcAAACAKpWbm6vjx487Hl+dKBAQEKBmzZo5JgrcddddCg0N1QsvvFDqRIEVK1aosLCwxIkCc+fO1bhx4/Tcc8/p8OHDWrp0qZYsWeKKLgMAANwSD2OMcXUQrpKTk/PD/YEmr5WnT11XhwMAAFygOP97nX7tMWVnZzMDDdXC9u3bnSYKXHV1ooAxRrNnz9Zbb73lmCjw+9//Xm3atHG0vXDhguLi4vTxxx87TRSoX7++o83BgwcVGxurffv2KTAwUJMmTdJzzz1X5jjJpQEAgOTafJrCJskYAABujcImcGvIpQEAgOTafLpKvzwIAAAAAAAAACoChU0AAAAAAAAAlkNhEwAAAAAAAIDlUNgEAAAAAAAAYDkUNgEAAAAAAABYDoVNAAAAAAAAAJZDYRMAAAAAAACA5VDYBAAAAAAAAGA5FDYBAAAAAAAAWA6FTQAAAAAAAACWQ2ETAAAAAAAAgOVQ2AQAAAAAAABgORQ2AQAAAAAAAFgOhU0AAAAAAAAAlkNhEwAAAAAAAIDlUNgEAAAAAAAAYDkUNgEAAAAAAABYDoVNAAAAAAAAAJZDYRMAAAAAAACA5VDYBAAAAAAAAGA5FDYBAAAAAAAAWA6FTQAAAAAAAACWQ2ETAAAAAAAAgOVQ2AQAAAAAAABgORQ2AQAAAAAAAFgOhU0AAAAAAAAAlkNhEwAAAAAAAIDlUNgEAAAAAAAAYDkUNgEAAAAAAABYDoVNAAAAAAAAAJbjksLmnDlz5OHh4bS0bdvWsf3y5cuKjY1Vo0aNVL9+fQ0ZMkSZmZlOz5GWlqaBAweqbt26atKkiZ599llduXKlqrsCAAAAAAAAwAVcNmOzQ4cOSk9Pdyx///vfHdumTJmijz/+WOvWrdOOHTt09uxZDR482LG9qKhIAwcOVEFBgXbv3q333ntPK1eu1KxZs1zRFQAAAKDKMEkAAADgB14ue2EvL9nt9uvWZ2dn65133tH777+vPn36SJLi4+PVrl077dmzRz/5yU+0ZcsWffHFF9q6dauCgoLUpUsXvfTSS3ruuec0Z84c2Wy2qu4OAAAAUGU6dOigrVu3Oh57ef0nrZ8yZYo2bdqkdevWyc/PT3FxcRo8eLB27dol6T+TBOx2u3bv3q309HSNHDlS3t7emjdvXpX3BQAA4Fa5bMbmV199pZCQELVs2VIjRoxQWlqaJCklJUWFhYWKiopytG3btq2aNWum5ORkSVJycrI6duyooKAgR5vo6Gjl5OToyJEjVdsRAAAAoIpdnSRwdQkMDJT0n0kCixcvVp8+fRQeHq74+Hjt3r1be/bskSTHJIFVq1apS5cuiomJ0UsvvaRly5apoKDAld0CAAAoF5cUNiMiIrRy5UolJCRo+fLlOnnypB544AFdvHhRGRkZsv2/9u4+Lqoy8f//m7sBNYEQBcYbvFvvyJsiRSq1kpWsNUv3k5Zr3qUfC9taSs1qM90eH1czzVzN2m9ibbpmj0zddCkiTNcwE3XVVBJ1NZObSgG1BJLr90c/ZptERZph5gyv5+MxjwdzzjVzrus6h4tr3pxzxmZTeHi402uioqJUUFAgSSooKHAKNavWV627mLKyMpWWljo9AAAAAKvxxEkCzKUBAIC38cil6AMHDnT83K1bNyUkJCg2NlarVq1SgwYN3LbdWbNmacaMGW57fwAAAMDdqk4S6Nixo/Lz8zVjxgz16dNHe/fudetJAsylAQCAt/HYpeg/FR4erg4dOigvL0/R0dEqLy9XcXGxU5nCwkLHPTmjo6MvuAF61fPq7ttZZdq0aSopKXE8vvzyS9c2BAAAAHCzgQMH6n/+53/UrVs3JScna8OGDSouLtaqVavcul3m0gAAwNt4RbB55swZHTp0SDExMYqPj1dQUJAyMzMd63Nzc3Xs2DElJiZKkhITE7Vnzx4VFRU5ymRkZCg0NFRdunS56HaCg4MVGhrq9AAAAACsrK5OEmAuDQAAvI1Hgs3HH39cH3/8sf7zn//ok08+0d13362AgADde++9CgsL07hx45SamqqsrCzl5ORozJgxSkxMVO/evSVJAwYMUJcuXTRy5Ej9+9//1vvvv6+nn35aKSkpCg4O9kSTAAAAAI+oq5MEAAAAvI1H7rF5/Phx3Xvvvfr222/VtGlT3XTTTdq6dauaNm0qSZo/f778/f01dOhQlZWVKTk5WYsXL3a8PiAgQO+9954efPBBJSYmqlGjRho1apRmzpzpieYAAAAAdebxxx/XoEGDFBsbqxMnTmj69OnVniQQERGh0NBQPfzwwxc9SWDOnDkqKCjgJAEAAGBJHgk2V65cecn1ISEhWrRokRYtWnTRMrGxsdqwYYOrqwYAAAB4NU4SAAAA+JFHgk0AAAAAtcNJAgAAAD/yii8PAgAAAAAAAIArQbAJAAAAAAAAwHIINgEAAAAAAABYDsEmAAAAAAAAAMsh2AQAAAAAAABgOQSbAAAAAAAAACyHYBMAAAAAAACA5RBsAgAAAAAAALAcgk0AAAAAAAAAlkOwCQAAAAAAAMByCDYBAAAAAAAAWA7BJgAAAAAAAADLIdgEAAAAAAAAYDkEmwAAAAAAAAAsh2ATAAAAAAAAgOUQbAIAAAAAAACwHIJNAAAAAAAAAJZDsAkAAAAAAADAcgg2AQAAAAAAAFgOwSYAAAAAAAAAyyHYBAAAAAAAAGA5BJsAAAAAAAAALIdgEwAAAAAAAIDlEGwCAAAAAAAAsByCTQAAAAAAAACWQ7AJAAAAAAAAwHIINgEAAAAAAABYDsEmAAAAAAAAAMsh2AQAAAAAAABgOQSbAAAAAAAAACyHYBMAAAAAAACA5Vg+2Fy0aJFat26tkJAQJSQkaNu2bZ6uEgAAAGAJzKUBAICVWTrYfOutt5Samqrp06drx44d6t69u5KTk1VUVOTpqgEAAABejbk0AACwOksHm/PmzdP48eM1ZswYdenSRUuWLFHDhg21dOlST1cNAAAA8GrMpQEAgNUFeroCtVVeXq6cnBxNmzbNsczf319JSUnKzs6u9jVlZWUqKytzPC8pKZEkVZZ9597KAgAAr1U1DzDGeLgmQN1hLg0AAFzFk/Npywab33zzjc6fP6+oqCin5VFRUTpw4EC1r5k1a5ZmzJhxwfKvXh7tjioCAAALOX36tMLCwjxdDaBOMJcGAACu5on5tGWDzdqYNm2aUlNTHc8rKyt18uRJNWnSRH5+fh6sWd0oLS1Vy5Yt9eWXXyo0NNTT1fEI+oA+kOiDKvQDfSDRB9KP/1k+ffq07Ha7p6sCeLX6Ppf2Zozl3o995N3YP96N/eP9PDmftmywGRkZqYCAABUWFjotLywsVHR0dLWvCQ4OVnBwsNOy8PBwd1XRa4WGhtb7wYA+oA8k+qAK/UAfSPQBZ2qivmEu7Zvq+1huBewj78b+8W7sH+/mqfm0Zb88yGazKT4+XpmZmY5llZWVyszMVGJiogdrBgAAAHg35tIAAMAXWPaMTUlKTU3VqFGjdP3116tXr1568cUXdfbsWY0ZM8bTVQMAAAC8GnNpAABgdZYONocNG6avv/5azzzzjAoKCtSjRw+lp6dfcBN0/Cg4OFjTp0+/4BKi+oQ+oA8k+qAK/UAfSPQBUJ8xl/YdjOXej33k3dg/3o39g0vxM574LnYAAAAAAAAA+AUse49NAAAAAAAAAPUXwSYAAAAAAAAAyyHYBAAAAAAAAGA5BJsAAAAAAAAALIdg00IWLVqk1q1bKyQkRAkJCdq2bdtFy1ZUVGjmzJlq166dQkJC1L17d6WnpzuVmTVrlnr27KnGjRurWbNmuuuuu5Sbm+tU5uabb5afn5/TY+LEiW5pX024ug+effbZC9rXqVMnpzLnzp1TSkqKmjRpoquuukpDhw5VYWGhW9pXE67ug9atW1/QB35+fkpJSXGU8abjYNOmTRo0aJDsdrv8/Py0Zs2ay75m48aNuu666xQcHKz27dtr2bJlF5S5XL9603Hgjj6w4njgjn6w2pjgjj6w2pgAAL7qSuZ8kvT222+rU6dOCgkJUdeuXbVhwwbHuoqKCk2dOlVdu3ZVo0aNZLfbdf/99+vEiRPubobPcuX++bmJEyfKz89PL774ootrXX+4Y//s379fd955p8LCwtSoUSP17NlTx44dc1cTfJ6r99GZM2c0adIktWjRQg0aNFCXLl20ZMkSdzYB3sLAElauXGlsNptZunSp+fzzz8348eNNeHi4KSwsrLb8lClTjN1uN+vXrzeHDh0yixcvNiEhIWbHjh2OMsnJySYtLc3s3bvX7Nq1y9x+++2mVatW5syZM44y/fr1M+PHjzf5+fmOR0lJidvbWx139MH06dNNXFycU/u+/vprp/eZOHGiadmypcnMzDTbt283vXv3NjfccINb23ox7uiDoqIip/ZnZGQYSSYrK8tRxpuOgw0bNpinnnrKrF692kgy77777iXLHz582DRs2NCkpqaaffv2mYULF5qAgACTnp7uKFOTfvWm48AdfWC18cAY9/SD1cYEd/SB1cYEAPBFVzrn27JliwkICDBz5swx+/btM08//bQJCgoye/bsMcYYU1xcbJKSksxbb71lDhw4YLKzs02vXr1MfHx8XTbLZ7h6//zU6tWrTffu3Y3dbjfz5893c0t8kzv2T15enomIiDCTJ082O3bsMHl5eWbt2rUXfU9cmjv20fjx4027du1MVlaWOXLkiHnllVdMQECAWbt2bV01Cx5CsGkRvXr1MikpKY7n58+fN3a73cyaNava8jExMeYvf/mL07IhQ4aYESNGXHQbRUVFRpL5+OOPHcv69etnHnnkkV9WeRdxRx9Mnz7ddO/e/aLbLC4uNkFBQebtt992LNu/f7+RZLKzs2vZktqri+PgkUceMe3atTOVlZWOZd50HPxUTYKcKVOmmLi4OKdlw4YNM8nJyY7nl+tXbzsOfspVffBz3j4e/Jyr+sFqY8JPuetYsNKYAAC+4krnfPfcc4+54447nJYlJCSY//3f/73oNrZt22YkmaNHj7qm0vWIu/bP8ePHTfPmzc3evXtNbGwswWYtuWP/DBs2zPzud79zT4XrIXfso7i4ODNz5kynMtddd5156qmnXFhzeCMuRbeA8vJy5eTkKCkpybHM399fSUlJys7OrvY1ZWVlCgkJcVrWoEED/etf/7rodkpKSiRJERERTsuXL1+uyMhIXXPNNZo2bZq+++672jal1tzZBwcPHpTdblfbtm01YsQIp8sJcnJyVFFR4bTdTp06qVWrVhfdrrvUxXFQXl6uN998U2PHjpWfn5/TOm84DmojOzvbqc8kKTk52dFnNelXbzoOauNyfVAdbx4Paqum/WCVMaE2rvRY8MUxAQC8XW3mfLX9W+/n56fw8HCX1Lu+cNf+qays1MiRIzV58mTFxcW5p/L1gDv2T2VlpdavX68OHTooOTlZzZo1U0JCQo1uAYQLuet36IYbbtC6dev01VdfyRijrKwsffHFFxowYIB7GgKvEejpCuDyvvnmG50/f15RUVFOy6OionTgwIFqX5OcnKx58+apb9++ateunTIzM7V69WqdP3++2vKVlZV69NFHdeONN+qaa65xLL/vvvsUGxsru92u3bt3a+rUqcrNzdXq1atd18AacFcfJCQkaNmyZerYsaPy8/M1Y8YM9enTR3v37lXjxo1VUFAgm812wYQvKipKBQUFLm/npdTFcbBmzRoVFxdr9OjRTsu95TiojYKCgmr7rLS0VN9//71OnTp12X71puOgNi7XBw0aNHBa5+3jQW3VpB+sNCbUxpUeC744JgCAt6vNnO9i4/vF/jadO3dOU6dO1b333qvQ0FDXVLyecNf+mT17tgIDA/X73//e9ZWuR9yxf4qKinTmzBn9+c9/1nPPPafZs2crPT1dQ4YMUVZWlvr16+eexvgod/0OLVy4UBMmTFCLFi0UGBgof39//fWvf1Xfvn1d3wh4FYJNH7VgwQKNHz9enTp1kp+fn9q1a6cxY8Zo6dKl1ZZPSUnR3r17LziTb8KECY6fu3btqpiYGPXv31+HDh1Su3bt3NqGX6omfTBw4EDHz926dVNCQoJiY2O1atUqjRs3zhPVdqkrPQ5ee+01DRw4UHa73Wm5lY8DXDlfHA9qytfHhCvFmAAAvqeiokL33HOPjDF6+eWXPV0d6McrQhYsWKAdO3ZccIUEPK+yslKSNHjwYP3hD3+QJPXo0UOffPKJlixZQrDpJRYuXKitW7dq3bp1io2N1aZNm5SSkiK73X7B2Z7wLVyKbgGRkZEKCAi44Ft3CwsLFR0dXe1rmjZtqjVr1ujs2bM6evSoDhw4oKuuukpt27a9oOykSZP03nvvKSsrSy1atLhkXRISEiRJeXl5tWxN7bi7D6qEh4erQ4cOjvZFR0ervLxcxcXFNd6uu7i7D44ePaoPP/xQDzzwwGXr4qnjoDaio6Or7bPQ0FA1aNCgRv3qTcdBbVyuD37KCuNBbV1JP1Tx5jGhNq6kD3x1TAAAb1ebOd/Fxvefl68KNY8ePaqMjAzO1qwFd+yfzZs3q6ioSK1atVJgYKACAwN19OhRPfbYY2rdurVb2uGr3LF/IiMjFRgYqC5dujiVqSaXtwAAWApJREFU6dy5M9+KXgvu2Efff/+9nnzySc2bN0+DBg1St27dNGnSJA0bNkxz5851T0PgNQg2LcBmsyk+Pl6ZmZmOZZWVlcrMzFRiYuIlXxsSEqLmzZvrhx9+0DvvvKPBgwc71hljNGnSJL377rv66KOP1KZNm8vWZdeuXZKkmJiY2jWmltzVBz935swZHTp0yNG++Ph4BQUFOW03NzdXx44du+x2Xc3dfZCWlqZmzZrpjjvuuGxdPHUc1EZiYqJTn0lSRkaGo89q0q/edBzUxuX6QLLWeFBbNemHn/PmMaE2rqQPfHVMAABvV5s5X03G96pQ8+DBg/rwww/VpEkT9zTAx7lj/4wcOVK7d+/Wrl27HA+73a7Jkyfr/fffd19jfJA79o/NZlPPnj2Vm5vrVOaLL75QbGysi1vg+9yxjyoqKlRRUSF/f+eIKyAgwHHGLXyYZ7+7CDW1cuVKExwcbJYtW2b27dtnJkyYYMLDw01BQYExxpiRI0eaJ554wlF+69at5p133jGHDh0ymzZtMrfeeqtp06aNOXXqlKPMgw8+aMLCwszGjRtNfn6+4/Hdd98ZY4zJy8szM2fONNu3bzdHjhwxa9euNW3btjV9+/at07ZXcUcfPPbYY2bjxo3myJEjZsuWLSYpKclERkaaoqIiR5mJEyeaVq1amY8++shs377dJCYmmsTExDpr90+5ow+M+fFb6Fq1amWmTp16wTa97Tg4ffq02blzp9m5c6eRZObNm2d27tzp+EbPJ554wowcOdJR/vDhw6Zhw4Zm8uTJZv/+/WbRokUmICDApKenO8pcrl+N8a7jwB19YLXxwBj39IPVxgR39IEx1hoTAMAXXemcb8uWLSYwMNDMnTvX7N+/30yfPt0EBQWZPXv2GGOMKS8vN3feeadp0aKF2bVrl9Pf+rKyMo+00cpcvX+qw7ei15479s/q1atNUFCQefXVV83BgwfNwoULTUBAgNm8eXOdt88XuGMf9evXz8TFxZmsrCxz+PBhk5aWZkJCQszixYvrvH2oWwSbFrJw4ULTqlUrY7PZTK9evczWrVsd6/r162dGjRrleL5x40bTuXNnExwcbJo0aWJGjhxpvvrqK6f3k1TtIy0tzRhjzLFjx0zfvn1NRESECQ4ONu3btzeTJ082JSUlddHcarm6D4YNG2ZiYmKMzWYzzZs3N8OGDTN5eXlOZb7//nvz0EMPmauvvto0bNjQ3H333SY/P9+t7bwUV/eBMca8//77RpLJzc29YJ23HQdZWVnVHrdV7R41apTp16/fBa/p0aOHsdlspm3bto5j/Kcu1a/GeNdx4I4+sOJ44I5+sNqY4K7fByuNCQDgq65kzmeMMatWrTIdOnQwNpvNxMXFmfXr1zvWHTly5KJ/67OysuqoRb7FlfunOgSbv4w79s9rr71m2rdvb0JCQkz37t3NmjVr3N0Mn+bqfZSfn29Gjx5t7Ha7CQkJMR07djQvvPCCqaysrIvmwIP8jDHGPeeCAgAAAAAAAIB7cI9NAAAAAAAAAJZDsAkAAAAAAADAcgg2AQAAAAAAAFgOwSYAAAAAAAAAyyHYBAAAAAAAAGA5BJsAAAAAAAAALIdgEwAAAAAAAIDlEGwCAAAAAAAAsByCTQAAAAAAAACWQ7AJAAAAAAAAwHIINgH4pNOnT2vEiBFq1KiRYmJiNH/+fN1888169NFHdeDAATVs2FArVqxwlF+1apUaNGigffv26dy5c4qLi9OECRMc6w8dOqTGjRtr6dKlnmgOAAAAUGd+yVx606ZNCgoKUkFBgdN7Pvroo+rTp09dNwWAjyPYBOCTUlNTtWXLFq1bt04ZGRnavHmzduzYIUnq1KmT5s6dq4ceekjHjh3T8ePHNXHiRM2ePVtdunRRSEiIli9frtdff11r167V+fPn9bvf/U6//vWvNXbsWA+3DAAAAHCvXzKX7tu3r9q2bau//e1vjverqKjQ8uXLmUsDcDk/Y4zxdCUAwJVOnz6tJk2aaMWKFfrtb38rSSopKZHdbtf48eP14osvSpJ+85vfqLS0VDabTQEBAUpPT5efn5/jfZ5//nnNmTNHw4cP1zvvvKM9e/aoSZMmnmgSAAAAUCdcMZeeM2eOli1bpn379kmSVq9erVGjRqmgoECNGjXySLsA+KZAT1cAAFzt8OHDqqioUK9evRzLwsLC1LFjR6dyS5cuVYcOHeTv76/PP//cKdSUpMcee0xr1qzRX/7yF/3zn/8k1AQAAIDPc8VcevTo0Xr66ae1detW9e7dW8uWLdM999xDqAnA5bgUHUC99e9//1tnz57V2bNnlZ+ff8H6oqIiffHFFwoICNDBgwc9UEMAAADAO11qLt2sWTMNGjRIaWlpKiws1D//+U8uQwfgFgSbAHxO27ZtFRQUpM8++8yxrKSkRF988YXj+cmTJzV69Gg99dRTGj16tEaMGKHvv//e6X3Gjh2rrl276vXXX9fUqVO1f//+OmsDAAAA4Amumks/8MADeuutt/Tqq6+qXbt2uvHGG+usDQDqDy5FB+BzGjdurFGjRmny5MmKiIhQs2bNNH36dPn7+zsukZk4caJatmypp59+WmVlZbr22mv1+OOPa9GiRZKkRYsWKTs7W7t371bLli21fv16jRgxQlu3bpXNZvNk8wAAAAC3ccVcWpKSk5MVGhqq5557TjNnzvRUcwD4OM7YBOCT5s2bp8TERP3mN79RUlKSbrzxRnXu3FkhISF64403tGHDBv3tb39TYGCgGjVqpDfffFN//etf9c9//lMHDhzQ5MmTtXjxYrVs2VKStHjxYn3zzTf64x//6OGWAQAAAO71S+bSVfz9/TV69GidP39e999/vwdbA8CX8a3oAOqFs2fPqnnz5nrhhRc0btw4T1cHAAAAsIzazqXHjRunr7/+WuvWrXNj7QDUZ1yKDsAn7dy5UwcOHFCvXr1UUlLiuPxl8ODBHq4ZAAAA4N1+6Vy6pKREe/bs0YoVKwg1AbgVwSYAnzV37lzl5ubKZrMpPj5emzdvVmRkpKerBQAAAHi9XzKXHjx4sLZt26aJEyfq17/+tZtrCqA+41J0AAAAAAAAAJbDlwcBAAAAAAAAsByCTQAAAAAAAACWQ7AJAAAAAAAAwHIINgEAAAAAAABYDsEmAAAAAAAAAMsh2AQAAAAAAABgOQSbAAAAAAAAACwn0NVvuGnTJj3//PPKyclRfn6+3n33Xd11112O9aNHj9brr7/u9Jrk5GSlp6c7np88eVIPP/yw/vGPf8jf319Dhw7VggULdNVVVznK7N69WykpKfrss8/UtGlTPfzww5oyZcoV1bWyslInTpxQ48aN5efnV7sGAwAASzPG6PTp07Lb7fL353++QE0xlwYAAJJn59MuDzbPnj2r7t27a+zYsRoyZEi1ZW677TalpaU5ngcHBzutHzFihPLz85WRkaGKigqNGTNGEyZM0IoVKyRJpaWlGjBggJKSkrRkyRLt2bNHY8eOVXh4uCZMmFDjup44cUItW7asRSsBAICv+fLLL9WiRQtPVwOwDObSAADgpzwxn3Z5sDlw4EANHDjwkmWCg4MVHR1d7br9+/crPT1dn332ma6//npJ0sKFC3X77bdr7ty5stvtWr58ucrLy7V06VLZbDbFxcVp165dmjdv3hUFm40bN5b0Y8eHhobW+HUAAMB3lJaWqmXLlo55AYCaYS4NAAAkz86nXR5s1sTGjRvVrFkzXX311br11lv13HPPqUmTJpKk7OxshYeHO0JNSUpKSpK/v78+/fRT3X333crOzlbfvn1ls9kcZZKTkzV79mydOnVKV199dbXbLSsrU1lZmeP56dOnJUmhoaFMxgAAqOe4lBbewiq3dqr6nWEuDQAAJM/Mp+v8RlK33Xab3njjDWVmZmr27Nn6+OOPNXDgQJ0/f16SVFBQoGbNmjm9JjAwUBERESooKHCUiYqKcipT9byqTHVmzZqlsLAwx4NLZwAAAOBtqm7ttGjRoouWue2225Sfn+94/P3vf3daP2LECH3++efKyMjQe++9p02bNjld2VR1a6fY2Fjl5OTo+eef17PPPqtXX33Vbe0CAABwtTo/Y3P48OGOn7t27apu3bqpXbt22rhxo/r37+/WbU+bNk2pqamO51WnygIAAADewkq3dgIAAPAkj3/1Z9u2bRUZGam8vDxJUnR0tIqKipzK/PDDDzp58qRj8hYdHa3CwkKnMlXPLzbBk36cAFZdKsMlMwAAALCqqls7dezYUQ8++KC+/fZbx7rL3dqpqkx1t3bKzc3VqVOnqt1mWVmZSktLnR4AAACe5JF7bP7U8ePH9e233yomJkaSlJiYqOLiYuXk5Cg+Pl6S9NFHH6myslIJCQmOMk899ZQqKioUFBQkScrIyFDHjh0ven9NQJJaP7G+RuX+8+c73FwTAACA2rnttts0ZMgQtWnTRocOHdKTTz6pgQMHKjs7WwEBATW+tVObNm2cyvz01k7VzalnzZqlGTNmuKlVAHDlavr5TuIzHuCrXB5snjlzxnH2pSQdOXJEu3btUkREhCIiIjRjxgwNHTpU0dHROnTokKZMmaL27dsrOTlZktS5c2fddtttGj9+vJYsWaKKigpNmjRJw4cPl91ulyTdd999mjFjhsaNG6epU6dq7969WrBggebPn+/q5jgQiAEAAMAbeOrWTtzWCQAAeBuXB5vbt2/XLbfc4nheNfkZNWqUXn75Ze3evVuvv/66iouLZbfbNWDAAP3pT39ScHCw4zXLly/XpEmT1L9/f8e3OL700kuO9WFhYfrggw+UkpKi+Ph4RUZG6plnnqn39wO6kv9W1RRBrW8gmAcAwHf99NZO/fv3d9utnYKDg53m7ACccfYgANQ9lwebN998s4wxF13//vvvX/Y9IiIitGLFikuW6datmzZv3nzF9XM3wsX6xZOTF3cca7g0JqsAAG/ErZ0AAEB95fF7bOLyCLAuzR39Q597L8JFAICv89VbO3mKVeYOVrjCxgp1hG/g8xiAmiLYlHTN9PflH9zQ09WwNKtMGHFpTFYBAPA8bu1UM54MPpgz1S+EbADgvQg24bWYQKCuWOGsXyucnSHxAQ4AXKG+39oJrmGVv9+uDomt8hmCcBwAXINgE/AyVpmMuZoVwkXgYjz5oYwPPJdWk76sLPuuDmoC+K6aXP3EWAVYj6+Fr77WHgA/ItjEJRE2oTrsQ9/ga/uxvk5WrdBuK9QRgPfwtb9P9RH7EPUB/4gGvAPBJgDAZazwQYaQDQDqnhX+PkjWqSfwS3Cc45fwtUCXzwbWR7AJ1BEmEACswCqTVcZUAKhf6uu4X1/bDdfhGKp7hKV1i2ATAAAAAAC4HSEbfglP/gOeY9d7EWwCgAVY5Sw6AADwX3wQBoDLY6x0nfp4tijBJgD4GCYG3ot9AwAAAMDTfCkAJdgEAKAavhZCWqE9VqgjAHgaYyUAq/DkeMVYWX8QbAIAgFphwggAAJgPXJ4vnR0H17HCcVHTOlaWfefmmlwcwSYAAAAAAIBFECYD/0WwCQAAAAAA4GEElsCVI9gEAAAAAAAQ4SK8D8fkpRFsAgAAAEA9xodmAIBV+Xu6AgAAAAAAAABwpQg2AQAAAAAAAFgOwSYAAAAAAAAAyyHYBAAAAAAAAGA5BJsAAAAAAAAALIdgEwAAAAAAAIDlEGwCAAAAAAAAsByCTQAAAAAAAACWQ7AJAAAAAAAAwHIINgEAAAAAAABYDsEmAAAAAAAAAMsh2AQAAAAAAABgOQSbAAAAAAAAACyHYBMAAAAAAACA5RBsAgAAAAAAALAcgk0AAAAAAAAAluPyYHPTpk0aNGiQ7Ha7/Pz8tGbNGqf1xhg988wziomJUYMGDZSUlKSDBw86lTl58qRGjBih0NBQhYeHa9y4cTpz5oxTmd27d6tPnz4KCQlRy5YtNWfOHFc3BQAAAAAAAICXcnmwefbsWXXv3l2LFi2qdv2cOXP00ksvacmSJfr000/VqFEjJScn69y5c44yI0aM0Oeff66MjAy999572rRpkyZMmOBYX1paqgEDBig2NlY5OTl6/vnn9eyzz+rVV191dXMAAAAAAAAAeCGXB5sDBw7Uc889p7vvvvuCdcYYvfjii3r66ac1ePBgdevWTW+88YZOnDjhOLNz//79Sk9P1//7f/9PCQkJuummm7Rw4UKtXLlSJ06ckCQtX75c5eXlWrp0qeLi4jR8+HD9/ve/17x581zdHAAAAKBOcQUUAABAzdTpPTaPHDmigoICJSUlOZaFhYUpISFB2dnZkqTs7GyFh4fr+uuvd5RJSkqSv7+/Pv30U0eZvn37ymazOcokJycrNzdXp06duuj2y8rKVFpa6vQAAAAAvAlXQAEAANRMYF1urKCgQJIUFRXltDwqKsqxrqCgQM2aNXNaHxgYqIiICKcybdq0ueA9qtZdffXV1W5/1qxZmjFjxi9vCAAAAOAmAwcO1MCBA6td9/MroCTpjTfeUFRUlNasWaPhw4c7roD67LPPHCcLLFy4ULfffrvmzp0ru93udAWUzWZTXFycdu3apXnz5jkFoAAAAN6sXn0r+rRp01RSUuJ4fPnll56uEgAAAFBjnrwCiqufAACAt6nTYDM6OlqSVFhY6LS8sLDQsS46OlpFRUVO63/44QedPHnSqUx17/HTbVQnODhYoaGhTg8AAADAKlx5BVR17/HTbfzcrFmzFBYW5ni0bNnylzcIAADgF6jTYLNNmzaKjo5WZmamY1lpaak+/fRTJSYmSpISExNVXFysnJwcR5mPPvpIlZWVSkhIcJTZtGmTKioqHGUyMjLUsWPHi16GDgAAAKD2uPoJAAB4G5cHm2fOnNGuXbu0a9cuST9eLrNr1y4dO3ZMfn5+evTRR/Xcc89p3bp12rNnj+6//37Z7XbdddddkqTOnTvrtttu0/jx47Vt2zZt2bJFkyZN0vDhw2W32yVJ9913n2w2m8aNG6fPP/9cb731lhYsWKDU1FRXNwcAAADwGp68AoqrnwAAgLdxebC5fft2XXvttbr22mslSampqbr22mv1zDPPSJKmTJmihx9+WBMmTFDPnj115swZpaenKyQkxPEey5cvV6dOndS/f3/dfvvtuummm5y+oTEsLEwffPCBjhw5ovj4eD322GN65plnuNE5AAAAfBpXQAEAAPyXnzHGeLoSnlJaWvrj/YEeXSX/4Iaerg4AAPCAyrLv9OWL96ikpIQz0OAVzpw5o7y8PEnStddeq3nz5umWW25RRESEWrVqpdmzZ+vPf/6zXn/9dbVp00Z//OMftXv3bu3bt89xssDAgQNVWFioJUuWqKKiQmPGjNH111+vFStWSJJKSkrUsWNHDRgwQFOnTtXevXs1duxYzZ8/v8YnCzCXBgAAkmfn04F1ujUAAAAAl7R9+3bdcsstjudVt1saNWqUli1bpilTpujs2bOaMGGCiouLddNNN1V7BdSkSZPUv39/+fv7a+jQoXrppZcc66uugEpJSVF8fLwiIyO5AgoAAFgOZ2zyX2YAAOo1ztgEaoe5NAAAkDw7n67Tb0UHAAAAAAAAAFcg2AQAAAAAAABgOQSbAAAAAAAAACyHYBMAAAAAAACA5RBsAgAAAAAAALAcgk0AAAAAAAAAlkOwCQAAAAAAAMByCDYBAAAAAAAAWA7BJgAAAAAAAADLIdgEAAAAAAAAYDkEmwAAAAAAAAAsh2ATAAAAAAAAgOUQbAIAAAAAAACwHIJNAAAAAAAAAJZDsAkAAAAAAADAcgg2AQAAAAAAAFgOwSYAAAAAAAAAyyHYBAAAAAAAAGA5BJsAAAAAAAAALIdgEwAAAAAAAIDlEGwCAAAAAAAAsByCTQAAAAAAAACWQ7AJAAAAAAAAwHIINgEAAAAAAABYDsEmAAAAAAAAAMsh2AQAAAAAAABgOQSbAAAAAAAAACyHYBMAAAAAAACA5RBsAgAAAAAAALAcjwSbzz77rPz8/JwenTp1cqw/d+6cUlJS1KRJE1111VUaOnSoCgsLnd7j2LFjuuOOO9SwYUM1a9ZMkydP1g8//FDXTQEAAAAAAADgAR47YzMuLk75+fmOx7/+9S/Huj/84Q/6xz/+obffflsff/yxTpw4oSFDhjjWnz9/XnfccYfKy8v1ySef6PXXX9eyZcv0zDPPeKIpAAAAQJ3hJAEAAIAfBXpsw4GBio6OvmB5SUmJXnvtNa1YsUK33nqrJCktLU2dO3fW1q1b1bt3b33wwQfat2+fPvzwQ0VFRalHjx7605/+pKlTp+rZZ5+VzWar6+YAAAAAdSYuLk4ffvih43lg4H+n9X/4wx+0fv16vf322woLC9OkSZM0ZMgQbdmyRdJ/TxKIjo7WJ598ovz8fN1///0KCgrS//3f/9V5WwAAAGrLY2dsHjx4UHa7XW3bttWIESN07NgxSVJOTo4qKiqUlJTkKNupUye1atVK2dnZkqTs7Gx17dpVUVFRjjLJyckqLS3V559/XrcNAQAAAOpY1UkCVY/IyEhJ/z1JYN68ebr11lsVHx+vtLQ0ffLJJ9q6daskOU4SePPNN9WjRw8NHDhQf/rTn7Ro0SKVl5d7slkAAABXxCPBZkJCgpYtW6b09HS9/PLLOnLkiPr06aPTp0+roKBANptN4eHhTq+JiopSQUGBJKmgoMAp1KxaX7XuYsrKylRaWur0AAAAAKyGkwQAAAA8dCn6wIEDHT9369ZNCQkJio2N1apVq9SgQQO3bXfWrFmaMWOG294fAAAAcLeqkwQ6duyo/Px8zZgxQ3369NHevXvdfpJAWVmZ4zknCQAAAE/z2KXoPxUeHq4OHTooLy9P0dHRKi8vV3FxsVOZwsJCxz05o6OjL7gBetXz6u7bWWXatGkqKSlxPL788kvXNgQAAABws4EDB+p//ud/1K1bNyUnJ2vDhg0qLi7WqlWr3LrdWbNmKSwszPFo2bKlW7cHAABwOV4RbJ45c0aHDh1STEyM4uPjFRQUpMzMTMf63NxcHTt2TImJiZKkxMRE7dmzR0VFRY4yGRkZCg0NVZcuXS66neDgYIWGhjo9AAAAACvjJAEAAFBfeSTYfPzxx/Xxxx/rP//5jz755BPdfffdCggI0L333quwsDCNGzdOqampysrKUk5OjsaMGaPExET17t1bkjRgwAB16dJFI0eO1L///W+9//77evrpp5WSkqLg4GBPNAkAAADwCE4SAAAA9ZVH7rF5/Phx3Xvvvfr222/VtGlT3XTTTdq6dauaNm0qSZo/f778/f01dOhQlZWVKTk5WYsXL3a8PiAgQO+9954efPBBJSYmqlGjRho1apRmzpzpieYAAAAAdebxxx/XoEGDFBsbqxMnTmj69OnVniQQERGh0NBQPfzwwxc9SWDOnDkqKCjgJAEAAGBJHgk2V65cecn1ISEhWrRokRYtWnTRMrGxsdqwYYOrqwYAAAB4NU4SAAAA+JFHgk0AAAAAtcNJAgAAAD/yii8PAgAAAAAAAIArQbAJAAAAAAAAwHIINgEAAAAAAABYDsEmAAAAAAAAAMsh2AQAAAAAAABgOQSbAAAAAAAAACyHYBMAAAAAAACA5RBsAgAAAAAAALAcgk0AAAAAAAAAlkOwCQAAAAAAAMByCDYBAAAAAAAAWA7BJgAAAAAAAADLIdgEAAAAAAAAYDkEmwAAAAAAAAAsh2ATAAAAAAAAgOUQbAIAAAAAAACwHIJNAAAAAAAAAJZDsAkAAAAAAADAcgg2AQAAAAAAAFgOwSYAAAAAAAAAyyHYBAAAAAAAAGA5BJsAAAAAAAAALIdgEwAAAAAAAIDlEGwCAAAAAAAAsByCTQAAAAAAAACWQ7AJAAAAAAAAwHIINgEAAAAAAABYDsEmAAAAAAAAAMsh2AQAAAAAAABgOQSbAAAAAAAAACyHYBMAAAAAAACA5Vg+2Fy0aJFat26tkJAQJSQkaNu2bZ6uEgAAAGAJzKUBAICVWTrYfOutt5Samqrp06drx44d6t69u5KTk1VUVOTpqgEAAABejbk0AACwOksHm/PmzdP48eM1ZswYdenSRUuWLFHDhg21dOlST1cNAAAA8GrMpQEAgNUFeroCtVVeXq6cnBxNmzbNsczf319JSUnKzs6u9jVlZWUqKytzPC8pKZEkVZZ9597KAgAAr1U1DzDGeLgmQN1hLg0AAFzFk/Npywab33zzjc6fP6+oqCin5VFRUTpw4EC1r5k1a5ZmzJhxwfKvXh7tjioCAAALOX36tMLCwjxdDaBOMJcGAACu5on5tGWDzdqYNm2aUlNTHc8rKyt18uRJNWnSRH5+fh6sGUpLS9WyZUt9+eWXCg0N9XR18DPsH+/HPvJu7B/vZozR6dOnZbfbPV0VwKvV97k0Yzl9INEHVegH+kCiDyT6oIon59OWDTYjIyMVEBCgwsJCp+WFhYWKjo6u9jXBwcEKDg52WhYeHu6uKqIWQkND6/Vg4O3YP96PfeTd2D/eizM1Ud8wl649xnL6QKIPqtAP9IFEH0j0geS5+bRlvzzIZrMpPj5emZmZjmWVlZXKzMxUYmKiB2sGAAAAeDfm0gAAwBdY9oxNSUpNTdWoUaN0/fXXq1evXnrxxRd19uxZjRkzxtNVAwAAALwac2kAAGB1lg42hw0bpq+//lrPPPOMCgoK1KNHD6Wnp19wE3R4v+DgYE2fPv2Cy5vgHdg/3o995N3YPwC8EXPpK8NYTh9I9EEV+oE+kOgDiT7wBn7GE9/FDgAAAAAAAAC/gGXvsQkAAAAAAACg/iLYBAAAAAAAAGA5BJsAAAAAAAAALIdgEwAAAAAAAIDlEGzCLRYtWqTWrVsrJCRECQkJ2rZt2yXLv/322+rUqZNCQkLUtWtXbdiwwbGuoqJCU6dOVdeuXdWoUSPZ7Xbdf//9OnHihLub4dNcuY9+buLEifLz89OLL77o4lrXH+7YP/v379edd96psLAwNWrUSD179tSxY8fc1QSf5ur9c+bMGU2aNEktWrRQgwYN1KVLFy1ZssSdTQCAeudKxu6KigrNnDlT7dq1U0hIiLp376709HSnMrNmzVLPnj3VuHFjNWvWTHfddZdyc3Odytx8883y8/NzekycONEt7asJV/fBs88+e0H7OnXq5FTm3LlzSklJUZMmTXTVVVdp6NChKiwsdEv7asrV/dC6desL+sHPz08pKSmOMt50LGzatEmDBg2S3W6Xn5+f1qxZc9nXbNy4Udddd52Cg4PVvn17LVu27IIyl+tXbzoW3NEHVhsT3NEHVhsT3NEHVhsPfIIBXGzlypXGZrOZpUuXms8//9yMHz/ehIeHm8LCwmrLb9myxQQEBJg5c+aYffv2maefftoEBQWZPXv2GGOMKS4uNklJSeatt94yBw4cMNnZ2aZXr14mPj6+LpvlU1y9j35q9erVpnv37sZut5v58+e7uSW+yR37Jy8vz0RERJjJkyebHTt2mLy8PLN27dqLvicuzh37Z/z48aZdu3YmKyvLHDlyxLzyyismICDArF27tq6aBQA+7UrH7ilTphi73W7Wr19vDh06ZBYvXmxCQkLMjh07HGWSk5NNWlqa2bt3r9m1a5e5/fbbTatWrcyZM2ccZfr162fGjx9v8vPzHY+SkhK3t7c67uiD6dOnm7i4OKf2ff31107vM3HiRNOyZUuTmZlptm/fbnr37m1uuOEGt7b1UtzRD0VFRU59kJGRYSSZrKwsRxlvOhY2bNhgnnrqKbN69Wojybz77ruXLH/48GHTsGFDk5qaavbt22cWLlxoAgICTHp6uqNMTfrVm44Fd/SB1cYEd/SB1cYEd/SB1cYDX0CwCZfr1auXSUlJcTw/f/68sdvtZtasWdWWv+eee8wdd9zhtCwhIcH87//+70W3sW3bNiPJHD161DWVrmfctY+OHz9umjdvbvbu3WtiY2MJNmvJHftn2LBh5ne/+517KlzPuGP/xMXFmZkzZzqVue6668xTTz3lwpoDQP11pWN3TEyM+ctf/uK0bMiQIWbEiBEX3UZRUZGRZD7++GPHsn79+plHHnnkl1XeRdzRB9OnTzfdu3e/6DaLi4tNUFCQefvttx3L9u/fbySZ7OzsWrbkl6mLY+GRRx4x7dq1M5WVlY5l3nQs/FRNwpwpU6aYuLg4p2XDhg0zycnJjueX61dvPBaquKoPfs7bx4SfclUfWHFMqOKu48BK44FVcSk6XKq8vFw5OTlKSkpyLPP391dSUpKys7OrfU12drZTeUlKTk6+aHlJKikpkZ+fn8LDw11S7/rEXfuosrJSI0eO1OTJkxUXF+eeytcD7tg/lZWVWr9+vTp06KDk5GQ1a9ZMCQkJNbrUAs7c9ftzww03aN26dfrqq69kjFFWVpa++OILDRgwwD0NAYB6pDZjd1lZmUJCQpyWNWjQQP/6178uup2SkhJJUkREhNPy5cuXKzIyUtdcc42mTZum7777rrZNqTV39sHBgwdlt9vVtm1bjRgxwuk2Nzk5OaqoqHDabqdOndSqVatLzvXdpS6OhfLycr355psaO3as/Pz8nNZ5w7FQG5eby9SkX73tWLhStf3MKnnnmFAbNe0DK40JV+pKjwNfHA+8UaCnKwDf8s033+j8+fOKiopyWh4VFaUDBw5U+5qCgoJqyxcUFFRb/ty5c5o6daruvfdehYaGuqbi9Yi79tHs2bMVGBio3//+966vdD3ijv1TVFSkM2fO6M9//rOee+45zZ49W+np6RoyZIiysrLUr18/9zTGB7nr92fhwoWaMGGCWrRoocDAQPn7++uvf/2r+vbt6/pGAEA9U5uxOzk5WfPmzVPfvn3Vrl07ZWZmavXq1Tp//ny15SsrK/Xoo4/qxhtv1DXXXONYft999yk2NlZ2u127d+/W1KlTlZubq9WrV7uugTXgrj5ISEjQsmXL1LFjR+Xn52vGjBnq06eP9u7dq8aNG6ugoEA2m+2CkxEuNdd3p7o4FtasWaPi4mKNHj3aabm3HAu1cbG5TGlpqb7//nudOnXqsv3qbcfClbpcHzRo0MBpnbePCbVRkz6w2phwpa70OPDF8cAbEWzCUioqKnTPPffIGKOXX37Z09XB/y8nJ0cLFizQjh07LvhPFDyvsrJSkjR48GD94Q9/kCT16NFDn3zyiZYsWUKw6QUWLlyorVu3at26dYqNjdWmTZuUkpIiu91+wX+FAQDut2DBAo0fP16dOnWSn5+f2rVrpzFjxmjp0qXVlk9JSdHevXsvOItvwoQJjp+7du2qmJgY9e/fX4cOHVK7du3c2oZfqiZ9MHDgQMfP3bp1U0JCgmJjY7Vq1SqNGzfOE9V2uSs9Fl577TUNHDhQdrvdabmVjwVcOV8cE2qiPowJV4LxoG5wKTpcKjIyUgEBARd8q1lhYaGio6OrfU10dHSNyleFmkePHlVGRgZna9aSO/bR5s2bVVRUpFatWikwMFCBgYE6evSoHnvsMbVu3dot7fBV7tg/kZGRCgwMVJcuXZzKdO7cmW9Fv0Lu2D/ff/+9nnzySc2bN0+DBg1St27dNGnSJA0bNkxz5851T0MAoB6pzdjdtGlTrVmzRmfPntXRo0d14MABXXXVVWrbtu0FZSdNmqT33ntPWVlZatGixSXrkpCQIEnKy8urZWtqx919UCU8PFwdOnRwtC86Olrl5eUqLi6u8Xbdyd39cPToUX344Yd64IEHLlsXTx0LtXGxuUxoaKgaNGhQo371tmPhSl2uD37KCmNCbVxJH1Tx9jHhSl1JH/jqeOCNCDbhUjabTfHx8crMzHQsq6ysVGZmphITE6t9TWJiolN5ScrIyHAqXxVqHjx4UB9++KGaNGningbUA+7YRyNHjtTu3bu1a9cux8Nut2vy5Ml6//333dcYH+SO/WOz2dSzZ0/l5uY6lfniiy8UGxvr4hb4Nnfsn4qKClVUVMjf3/lPckBAgONsWwBA7dVm7K4SEhKi5s2b64cfftA777yjwYMHO9YZYzRp0iS9++67+uijj9SmTZvL1mXXrl2SpJiYmNo1ppbc1Qc/d+bMGR06dMjRvvj4eAUFBTltNzc3V8eOHbvsdt3B3f2QlpamZs2a6Y477rhsXTx1LNRGTeaal+tXbzsWrlRNPrNaaUyojZr0wc95+5hwpa6kD3x1PPBKnv3uIviilStXmuDgYLNs2TKzb98+M2HCBBMeHm4KCgqMMcaMHDnSPPHEE47yW7ZsMYGBgWbu3Llm//79Zvr06SYoKMjs2bPHGGNMeXm5ufPOO02LFi3Mrl27TH5+vuNRVlbmkTZanav3UXX4VvTac8f+Wb16tQkKCjKvvvqqOXjwoFm4cKEJCAgwmzdvrvP2WZ079k+/fv1MXFycycrKMocPHzZpaWkmJCTELF68uM7bBwC+6ErH7q1bt5p33nnHHDp0yGzatMnceuutpk2bNubUqVOOMg8++KAJCwszGzdudJqffvfdd8YYY/Ly8szMmTPN9u3bzZEjR8zatWtN27ZtTd++feu07VXc0QePPfaY2bhxozly5IjZsmWLSUpKMpGRkaaoqMhRZuLEiaZVq1bmo48+Mtu3bzeJiYkmMTGxztr9c+7oB2N+/BbwVq1amalTp16wTW87Fk6fPm127txpdu7caSSZefPmmZ07d5qjR48aY4x54oknzMiRIx3lDx8+bBo2bGgmT55s9u/fbxYtWmQCAgJMenq6o8zl+tUY7zoW3NEHVhsT3NEHVhsT3NEHxlhrPPAFBJtwi4ULF5pWrVoZm81mevXqZbZu3epY169fPzNq1Cin8qtWrTIdOnQwNpvNxMXFmfXr1zvWHTlyxEiq9pGVlVVHLfI9rtxH1SHY/GXcsX9ee+010759exMSEmK6d+9u1qxZ4+5m+CxX75/8/HwzevRoY7fbTUhIiOnYsaN54YUXTGVlZV00BwDqhSsZuzdu3Gg6d+5sgoODTZMmTczIkSPNV1995fR+F5ufpqWlGWOMOXbsmOnbt6+JiIgwwcHBpn379mby5MmmpKSkLppbLVf3wbBhw0xMTIyx2WymefPmZtiwYSYvL8+pzPfff28eeughc/XVV5uGDRuau+++2+Tn57u1nZfj6n4wxpj333/fSDK5ubkXrPO2YyErK6vaY7eq3aNGjTL9+vW74DU9evQwNpvNtG3b1nGc/9Sl+tUY7zoW3NEHVhsT3NEHVhsT3PW7YKXxwBf4GWOMe84FBQAAAAAAAAD34B6bAAAAAAAAACyHYBMAAAAAAACA5RBsAgAAAAAAALAcgk0AAAAAAAAAlkOwCQAAAAAAAMByCDYBAAAAAAAAWA7BJgAAAAAAAADLIdgEAAAAAAAAYDkEmwAAAAAAAAAsh2ATAAAAAAAAgOUQbAIAAAAAAACwHIJNAD7p9OnTGjFihBo1aqSYmBjNnz9fN998sx599FEdOHBADRs21IoVKxzlV61apQYNGmjfvn2SJD8/vwserVu39lBrAAAAgLpzqbn0zJkzdc0111zwmh49euiPf/yjNm3apKCgIBUUFDitf/TRR9WnT5+6agKAeoJgE4BPSk1N1ZYtW7Ru3TplZGRo8+bN2rFjhySpU6dOmjt3rh566CEdO3ZMx48f18SJEzV79mx16dJFkpSfn+945OXlqX379urbt68nmwQAAADUiUvNpceOHav9+/frs88+c5TfuXOndu/erTFjxqhv375q27at/va3vznWV1RUaPny5Ro7dmydtwWAb/MzxhhPVwIAXOn06dNq0qSJVqxYod/+9reSpJKSEtntdo0fP14vvviiJOk3v/mNSktLZbPZFBAQoPT0dPn5+Tm9lzFGQ4cO1bFjx7R582Y1aNCgrpsDAAAA1JmazKVvv/12tW7dWosXL5Yk/f73v9eePXuUlZUlSZozZ46WLVvmuBpq9erVGjVqlAoKCtSoUSPPNAyAT+KMTQA+5/Dhw6qoqFCvXr0cy8LCwtSxY0enckuXLtXu3bu1Y8cOLVu27IJQU5KefPJJZWdna+3atYSaAAAA8Hk1mUuPHz9ef//733Xu3DmVl5drxYoVTmdjjh49Wnl5edq6daskadmyZbrnnnsINQG4XKCnKwAAnvLvf/9bZ8+elb+/v/Lz8xUTE+O0/s0339T8+fO1ceNGNW/e3EO1BAAAALzLoEGDFBwcrHfffVc2m00VFRWOszslqVmzZho0aJDS0tLUpk0b/fOf/9TGjRs9V2EAPoszNgH4nLZt2yooKMjpvj8lJSX64osvHM9Pnjyp0aNH66mnntLo0aM1YsQIff/994712dnZeuCBB/TKK6+od+/edVp/AAAAwFNqMpcODAzUqFGjlJaWprS0NA0fPvyCq5seeOABvfXWW3r11VfVrl073XjjjXXWBgD1B2dsAvA5jRs31qhRozR58mRFRESoWbNmmj59uvz9/R2Xm0+cOFEtW7bU008/rbKyMl177bV6/PHHtWjRIhUUFOjuu+/W8OHDlZyc7PhGx4CAADVt2tSTTQMAAADcqiZzaenH4LJz586SpC1btlzwPsnJyQoNDdVzzz2nmTNn1ln9AdQvnLEJwCfNmzdPiYmJ+s1vfqOkpCTdeOON6ty5s0JCQvTGG29ow4YN+tvf/qbAwEA1atRIb775pv7617/qn//8pw4cOKDCwkK9/vrriomJcTx69uzp6WYBAAAAbnepuXSVX/3qV7rhhhvUqVMnJSQkXPAe/v7+Gj16tM6fP6/777+/LqsPoB7hW9EB1Atnz55V8+bN9cILL2jcuHGerg4AAABgGdXNpY0x+tWvfqWHHnpIqamp1b5u3Lhx+vrrr7Vu3bq6rC6AeoRL0QH4pJ07d+rAgQPq1auXSkpKHJe/DB482MM1AwAAALzb5ebSX3/9tVauXKmCggKNGTPmgteXlJRoz549WrFiBaEmALci2ATgs+bOnavc3FzZbDbFx8dr8+bNioyM9HS1AAAAAK93qbl0s2bNFBkZqVdffVVXX331Ba8dPHiwtm3bpokTJ+rXv/51XVcdQD3CpegAAAAAAAAALIcvDwIAAAAAAABgOQSbAAAAAAAAACyHYBMAAAAAAACA5RBsAgAAAAAAALAcgk0AAAAAAAAAlkOwCQAAAAAAAMByCDYBAAAAAAAAWE6gpyvgSZWVlTpx4oQaN24sPz8/T1cHAAB4gDFGp0+flt1ul78///MFaoq5NAAAkDw7n67XweaJEyfUsmVLT1cDAAB4gS+//FItWrTwdDUAy2AuDQAAfsoT82mXB5ubNm3S888/r5ycHOXn5+vdd9/VXXfd5Vg/evRovf76606vSU5OVnp6uuP5yZMn9fDDD+sf//iH/P39NXToUC1YsEBXXXWVo8zu3buVkpKizz77TE2bNtXDDz+sKVOmXFFdGzduLOnHjg8NDa1FawEAgNWVlpaqZcuWjnkBgJphLg0AACTPzqddHmyePXtW3bt319ixYzVkyJBqy9x2221KS0tzPA8ODnZaP2LECOXn5ysjI0MVFRUaM2aMJkyYoBUrVkj6scMGDBigpKQkLVmyRHv27NHYsWMVHh6uCRMm1LiuVZfMhIaGMhkDAKCe41JaeAurnCjAXBoAAPyUJ+bTLg82Bw4cqIEDB16yTHBwsKKjo6tdt3//fqWnp+uzzz7T9ddfL0lauHChbr/9ds2dO1d2u13Lly9XeXm5li5dKpvNpri4OO3atUvz5s27omATAAAA8DZWOlEAAADAkzxyj82NGzeqWbNmuvrqq3XrrbfqueeeU5MmTSRJ2dnZCg8Pd4SakpSUlCR/f399+umnuvvuu5Wdna2+ffvKZrM5yiQnJ2v27Nk6deqUrr766mq3W1ZWprKyMsfz0tJSN7UQAAAAqB1OFAAAAKiZOv/qz9tuu01vvPGGMjMzNXv2bH388ccaOHCgzp8/L0kqKChQs2bNnF4TGBioiIgIFRQUOMpERUU5lal6XlWmOrNmzVJYWJjjwc3OAQAAYEVVJwp07NhRDz74oL799lvHusudKFBVproTBXJzc3Xq1Klqt1lWVqbS0lKnBwAAgCfV+Rmbw4cPd/zctWtXdevWTe3atdPGjRvVv39/t2572rRpSk1NdTyvurmpK7V+Yn2Ny/7nz3e4dNsAAADwfbfddpuGDBmiNm3a6NChQ3ryySc1cOBAZWdnKyAgoMYnCrRp08apzE9PFKjuCqhZs2ZpxowZbmoVavo5gs8QAAD8l0cuRf+ptm3bKjIyUnl5eerfv7+io6NVVFTkVOaHH37QyZMnHZfbREdHq7Cw0KlM1fOLXZIj/XjJzs/vP2QFVxKW1pSrJ0QEugAAAHXDUycK1MVJAhIBny+or58N6mu7AcCTPB5sHj9+XN9++61iYmIkSYmJiSouLlZOTo7i4+MlSR999JEqKyuVkJDgKPPUU0+poqJCQUFBkqSMjAx17NjxovfXhDN3hKUAwIdRAKh7dXWigFVPEnCH+vr3js8QAABv4/Jg88yZM8rLy3M8P3LkiHbt2qWIiAhFRERoxowZGjp0qKKjo3Xo0CFNmTJF7du3V3JysiSpc+fOuu222zR+/HgtWbJEFRUVmjRpkoYPHy673S5Juu+++zRjxgyNGzdOU6dO1d69e7VgwQLNnz/f1c2BBfCfUQAAUJ95+kSBa6a/L//ghpcswxwMAAC4g8uDze3bt+uWW25xPK+6XGXUqFF6+eWXtXv3br3++usqLi6W3W7XgAED9Kc//cnpv7/Lly/XpEmT1L9/f/n7+2vo0KF66aWXHOvDwsL0wQcfKCUlRfHx8YqMjNQzzzzj1m9w5L+TgO+qr2ddAAC8EycK1Ax/v1FX+CwIAN7L5cHmzTffLGPMRde///77l32PiIgIrVix4pJlunXrps2bN19x/QAAAABv5qsnCsB7EdwBAKzK4/fY9AY1uXwGAHyNFc50scKXpwGAq/niiQIEZ7gYK8xHAADei2DTg5jgWR+hC+oK95L1blb4UGaFOgIAAO9RX+cO9bXdgFURbAJextf+kPpSewgXfYMvHZMAAO/mjn+C83cMqB1OLHINPhPB2xBswmvxh6d+YZIOWA+/twB8jS/NP32pLQAAXAzBJuoVX/oQ7snL4JkoA97D1b+PnF0EALAyX5uncusrAO7gS/Nzgk2gGr42IQIAAKivmNcBsApfC7J9KTzzJC7/vzSCTdQ5Jpfeywr7xiqDui/9EffkGXyoX6zy+w0AAP6rvs7rmLfAG9XH30eCTQA+ywqDOqEh4F41+X2oLPuuDmoCAMB/MV9DXbHCCRdWuM0a4bT3ItgEAMDL8GEHAAC4C/OMukef1z1P3oeeELRuEWwCAFBHmNQCAADAGzFPhVURbAIAAAAAAAB1iDDZNQg2AQAAAABejQAAAOpeTcdeT96znmATAIBq8AEKAAAAwJXic0TdItgEAACWw4QRAAAAgL+nKwAAAAAAAAAAV4pgEwAAAAAAAIDlEGwCAAAAAAAAsByCTQAAAAAAAACWQ7AJAAAAAAAAwHIINgEAAAAAAABYDsEmAAAAAAAAAMsh2AQAAAAAAABgOQSbAAAAAAAAACyHYBMAAAAAAACA5RBsAgAAAAAAALAcgk0AAAAAAAAAlkOwCQAAAAAAAMByCDYBAAAAAAAAWA7BJgAAAAAAAADLIdgEAAAAAAAAYDkEmwAAAAAAAAAsx+XB5qZNmzRo0CDZ7Xb5+flpzZo1TuuNMXrmmWcUExOjBg0aKCkpSQcPHnQqc/LkSY0YMUKhoaEKDw/XuHHjdObMGacyu3fvVp8+fRQSEqKWLVtqzpw5rm4KAAAAUOeYTwMAANSMy4PNs2fPqnv37lq0aFG16+fMmaOXXnpJS5Ys0aeffqpGjRopOTlZ586dc5QZMWKEPv/8c2VkZOi9997Tpk2bNGHCBMf60tJSDRgwQLGxscrJydHzzz+vZ599Vq+++qqrmwMAAADUKebTAAAANeNnjDFue3M/P7377ru66667JP3432W73a7HHntMjz/+uCSppKREUVFRWrZsmYYPH679+/erS5cu+uyzz3T99ddLktLT03X77bfr+PHjstvtevnll/XUU0+poKBANptNkvTEE09ozZo1OnDgQI3rV1paqrCwMLV8dJX8gxu6tvEAAMASKsu+05cv3qOSkhKFhoZ6ujqAE2+eTzOXBgAAkmfn03V6j80jR46ooKBASUlJjmVhYWFKSEhQdna2JCk7O1vh4eGOSZgkJSUlyd/fX59++qmjTN++fR2TMElKTk5Wbm6uTp06ddHtl5WVqbS01OkBAAAAWIWn59MAAADepE6DzYKCAklSVFSU0/KoqCjHuoKCAjVr1sxpfWBgoCIiIpzKVPceP91GdWbNmqWwsDDHo2XLlr+sQQAAAEAd8uR8mpMEAACAt6lX34o+bdo0lZSUOB5ffvmlp6sEAAAAWAInCQAAAG9Tp8FmdHS0JKmwsNBpeWFhoWNddHS0ioqKnNb/8MMPOnnypFOZ6t7jp9uoTnBwsEJDQ50eAAAAgFV4cj7NSQIAAMDb1Gmw2aZNG0VHRyszM9OxrLS0VJ9++qkSExMlSYmJiSouLlZOTo6jzEcffaTKykolJCQ4ymzatEkVFRWOMhkZGerYsaOuvvrqOmoNAAAAULc8OZ/mJAEAAOBtXB5snjlzRrt27dKuXbsk/XiD8127dunYsWPy8/PTo48+queee07r1q3Tnj17dP/998tutzu+6bFz58667bbbNH78eG3btk1btmzRpEmTNHz4cNntdknSfffdJ5vNpnHjxunzzz/XW2+9pQULFig1NdXVzQEAAADqFPNpAACAmgl09Rtu375dt9xyi+N51eRo1KhRWrZsmaZMmaKzZ89qwoQJKi4u1k033aT09HSFhIQ4XrN8+XJNmjRJ/fv3l7+/v4YOHaqXXnrJsT4sLEwffPCBUlJSFB8fr8jISD3zzDOaMGGCq5sDAAAA1Cnm0wAAADXjZ4wxnq6Ep5SWlv544/NHV8k/uKGnqwMAADygsuw7ffniPSopKeHSWuAKMJcGAACSZ+fT9epb0QEAAAAAAAD4BoJNAAAAAAAAAJZDsAkAAAAAAADAcgg2AQAAAAAAAFgOwSYAAAAAAAAAyyHYBAAAAAAAAGA5BJsAAAAAAAAALIdgEwAAAAAAAIDlEGwCAAAAAAAAsByCTQAAAAAAAACWQ7AJAAAAAAAAwHIINgEAAAAAAABYDsEmAAAAAAAAAMsh2AQAAAAAAABgOQSbAAAAAAAAACyHYBMAAAAAAACA5RBsAgAAAAAAALAcgk0AAAAAAAAAlkOwCQAAAAAAAMByCDYBAAAAAAAAWA7BJgAAAAAAAADLIdgEAAAAAAAAYDkEmwAAAAAAAAAsh2ATAAAAAAAAgOUQbAIAAAAAAACwHIJNAAAAAAAAAJZDsAkAAAAAAADAcgg2AQAAAAAAAFgOwSYAAAAAAAAAyyHYBAAAAAAAAGA5BJsAAAAAAAAALMcjweazzz4rPz8/p0enTp0c68+dO6eUlBQ1adJEV111lYYOHarCwkKn9zh27JjuuOMONWzYUM2aNdPkyZP1ww8/1HVTAAAAgDrFXBoAAOBHgZ7acFxcnD788MP/ViTwv1X5wx/+oPXr1+vtt99WWFiYJk2apCFDhmjLli2SpPPnz+uOO+5QdHS0PvnkE+Xn5+v+++9XUFCQ/u///q/O2wIAAADUJebSAAAAHgw2AwMDFR0dfcHykpISvfbaa1qxYoVuvfVWSVJaWpo6d+6srVu3qnfv3vrggw+0b98+ffjhh4qKilKPHj30pz/9SVOnTtWzzz4rm81W180BAAAA6gxzaQAAAA/eY/PgwYOy2+1q27atRowYoWPHjkmScnJyVFFRoaSkJEfZTp06qVWrVsrOzpYkZWdnq2vXroqKinKUSU5OVmlpqT7//POLbrOsrEylpaVODwAAAMBqmEsDAAB4KNhMSEjQsmXLlJ6erpdffllHjhxRnz59dPr0aRUUFMhmsyk8PNzpNVFRUSooKJAkFRQUOE3EqtZXrbuYWbNmKSwszPFo2bKlaxsGAAAAuBlzaQAAgB955FL0gQMHOn7u1q2bEhISFBsbq1WrVqlBgwZu2+60adOUmprqeF5aWsqEDAAAAJbCXBoAAOBHHrsU/afCw8PVoUMH5eXlKTo6WuXl5SouLnYqU1hY6LiPUHR09AXf7Fj1vLp7DVUJDg5WaGio0wMAAACwMubSAACgvvKKYPPMmTM6dOiQYmJiFB8fr6CgIGVmZjrW5+bm6tixY0pMTJQkJSYmas+ePSoqKnKUycjIUGhoqLp06VLn9QcAAAA8hbk0AACorzxyKfrjjz+uQYMGKTY2VidOnND06dMVEBCge++9V2FhYRo3bpxSU1MVERGh0NBQPfzww0pMTFTv3r0lSQMGDFCXLl00cuRIzZkzRwUFBXr66aeVkpKi4OBgTzQJAAAAqBPMpQEAAH7kkWDz+PHjuvfee/Xtt9+qadOmuummm7R161Y1bdpUkjR//nz5+/tr6NChKisrU3JyshYvXux4fUBAgN577z09+OCDSkxMVKNGjTRq1CjNnDnTE80BAAAA6gxzaQAAgB/5GWOMpyvhKaWlpT9+o+Ojq+Qf3NDT1QEAAB5QWfadvnzxHpWUlHDPQOAKMJcGAACSZ+fTXnGPTQAAAAAAAAC4EgSbAAAAAAAAACyHYBMAAAAAAACA5RBsAgAAAAAAALAcgk0AAAAAAAAAlkOwCQAAAAAAAMByCDYBAAAAAAAAWA7BJgAAAAAAAADLIdgEAAAAAAAAYDkEmwAAAAAAAAAsh2ATAAAAAAAAgOUQbAIAAAAAAACwHIJNAAAAAAAAAJZDsAkAAAAAAADAcgg2AQAAAAAAAFgOwSYAAAAAAAAAyyHYBAAAAAAAAGA5BJsAAAAAAAAALIdgEwAAAAAAAIDlEGwCAAAAAAAAsByCTQAAAAAAAACWQ7AJAAAAAAAAwHIINgEAAAAAAABYDsEmAAAAAAAAAMsh2AQAAAAAAABgOQSbAAAAAAAAACyHYBMAAAAAAACA5RBsAgAAAAAAALAcgk0AAAAAAAAAlkOwCQAAAAAAAMByCDYBAAAAAAAAWI7lg81FixapdevWCgkJUUJCgrZt2+bpKgEAAACWwFwaAABYmaWDzbfeekupqamaPn26duzYoe7duys5OVlFRUWerhoAAADg1ZhLAwAAq7N0sDlv3jyNHz9eY8aMUZcuXbRkyRI1bNhQS5cu9XTVAAAAAK/GXBoAAFhdoKcrUFvl5eXKycnRtGnTHMv8/f2VlJSk7Ozsal9TVlamsrIyx/OSkhJJUmXZd+6tLAAA8FpV8wBjjIdrAtQd5tIAAMBVPDmftmyw+c033+j8+fOKiopyWh4VFaUDBw5U+5pZs2ZpxowZFyz/6uXR7qgiAACwkNOnTyssLMzT1QDqBHNpAADgap6YT1s22KyNadOmKTU11fG8srJSJ0+eVJMmTeTn5+fBmqG0tFQtW7bUl19+qdDQUE9XBz/D/vFu7B/vxz7ybsYYnT59Wna73dNVAbxafZ9LM5bTBxJ9UIV+oA8k+kCiD6p4cj5t2WAzMjJSAQEBKiwsdFpeWFio6Ojoal8THBys4OBgp2Xh4eHuqiJqITQ0tF4PBt6O/ePd2D/ej33kvThTE/UNc+naYyynDyT6oAr9QB9I9IFEH0iem09b9suDbDab4uPjlZmZ6VhWWVmpzMxMJSYmerBmAAAAgHdjLg0AAHyBZc/YlKTU1FSNGjVK119/vXr16qUXX3xRZ8+e1ZgxYzxdNQAAAMCrMZcGAABWZ+lgc9iwYfr666/1zDPPqKCgQD169FB6evoFN0GH9wsODtb06dMvuLwJ3oH9493YP96PfQTAGzGXvjKM5fSBRB9UoR/oA4k+kOgDb+BnPPFd7AAAAAAAAADwC1j2HpsAAAAAAAAA6i+CTQAAAAAAAACWQ7AJAAAAAAAAwHIINgEAAAAAAABYDsEm3GLRokVq3bq1QkJClJCQoG3btl2y/Ntvv61OnTopJCREXbt21YYNGxzrKioqNHXqVHXt2lWNGjWS3W7X/fffrxMnTri7GT7Nlfvo5yZOnCg/Pz+9+OKLLq51/eGO/bN//37deeedCgsLU6NGjdSzZ08dO3bMXU3waa7eP2fOnNGkSZPUokULNWjQQF26dNGSJUvc2QQAqHeuZOyuqKjQzJkz1a5dO4WEhKh79+5KT093KjNr1iz17NlTjRs3VrNmzXTXXXcpNzfXqczNN98sPz8/p8fEiRPd0r6acHUfPPvssxe0r1OnTk5lzp07p5SUFDVp0kRXXXWVhg4dqsLCQre0ryZc3QetW7e+oA/8/PyUkpLiKONNx8GmTZs0aNAg2e12+fn5ac2aNZd9zcaNG3XdddcpODhY7du317Jlyy4oc7l+9bbjwB39YLUxwR19YLUxwR19YLUxwScYwMVWrlxpbDabWbp0qfn888/N+PHjTXh4uCksLKy2/JYtW0xAQICZM2eO2bdvn3n66adNUFCQ2bNnjzHGmOLiYpOUlGTeeustc+DAAZOdnW169epl4uPj67JZPsXV++inVq9ebbp3727sdruZP3++m1vim9yxf/Ly8kxERISZPHmy2bFjh8nLyzNr16696Hvi4tyxf8aPH2/atWtnsrKyzJEjR8wrr7xiAgICzNq1a+uqWQDg06507J4yZYqx2+1m/fr15tChQ2bx4sUmJCTE7Nixw1EmOTnZpKWlmb1795pdu3aZ22+/3bRq1cqcOXPGUaZfv35m/PjxJj8/3/EoKSlxe3ur444+mD59uomLi3Nq39dff+30PhMnTjQtW7Y0mZmZZvv27aZ3797mhhtucGtbL8YdfVBUVOTU/oyMDCPJZGVlOcp403GwYcMG89RTT5nVq1cbSebdd9+9ZPnDhw+bhg0bmtTUVLNv3z6zcOFCExAQYNLT0x1latKv3nQcGOOefrDamOCOPrDamOCOPrDamOALCDbhcr169TIpKSmO5+fPnzd2u93MmjWr2vL33HOPueOOO5yWJSQkmP/93/+96Da2bdtmJJmjR4+6ptL1jLv20fHjx03z5s3N3r17TWxsLMFmLblj/wwbNsz87ne/c0+F6xl37J+4uDgzc+ZMpzLXXXedeeqpp1xYcwCov6507I6JiTF/+ctfnJYNGTLEjBgx4qLbKCoqMpLMxx9/7FjWr18/88gjj/yyyruIO/pg+vTppnv37hfdZnFxsQkKCjJvv/22Y9n+/fuNJJOdnV3LltReXRwHjzzyiGnXrp2prKx0LPOm4+CnahLkTJkyxcTFxTktGzZsmElOTnY8v1y/ettx8HOu6oef8/Yx4adc1QdWGxN+yl3HgZXGBKviUnS4VHl5uXJycpSUlORY5u/vr6SkJGVnZ1f7muzsbKfykpScnHzR8pJUUlIiPz8/hYeHu6Te9Ym79lFlZaVGjhypyZMnKy4uzj2VrwfcsX8qKyu1fv16dejQQcnJyWrWrJkSEhJqdKkFnLnr9+eGG27QunXr9NVXX8kYo6ysLH3xxRcaMGCAexoCAPVIbcbusrIyhYSEOC1r0KCB/vWvf110OyUlJZKkiIgIp+XLly9XZGSkrrnmGk2bNk3fffddbZtSa+7sg4MHD8put6tt27YaMWKE021ucnJyVFFR4bTdTp06qVWrVpec67tDXRwH5eXlevPNNzV27Fj5+fk5rfOG46A2LjePqUm/etNxUFu1/cwqeeeYUBs17QOrjAm1caXHgS+OCd4o0NMVgG/55ptvdP78eUVFRTktj4qK0oEDB6p9TUFBQbXlCwoKqi1/7tw5TZ06Vffee69CQ0NdU/F6xF37aPbs2QoMDNTvf/9711e6HnHH/ikqKtKZM2f05z//Wc8995xmz56t9PR0DRkyRFlZWerXr597GuOD3PX7s3DhQk2YMEEtWrRQYGCg/P399de//lV9+/Z1fSMAoJ6pzdidnJysefPmqW/fvmrXrp0yMzO1evVqnT9/vtrylZWVevTRR3XjjTfqmmuucSy/7777FBsbK7vdrt27d2vq1KnKzc3V6tWrXdfAGnBXHyQkJGjZsmXq2LGj8vPzNWPGDPXp00d79+5V48aNVVBQIJvNdsHJCJea67tLXRwHa9asUXFxsUaPHu203FuOg9q42DymtLRU33//vU6dOnXZfvWm46C2LtcPDRo0cFrn7WNCbdSkD6w0JtTGlR4HvjgmeCOCTVhKRUWF7rnnHhlj9PLLL3u6Ovj/5eTkaMGCBdqxY8cF/4mC51VWVkqSBg8erD/84Q+SpB49euiTTz7RkiVLCDa9wMKFC7V161atW7dOsbGx2rRpk1JSUmS32y/4rzAAwP0WLFig8ePHq1OnTvLz81O7du00ZswYLV26tNryKSkp2rt37wVn8k2YMMHxc9euXRUTE6P+/fvr0KFDateunVvb8EvVpA8GDhzo+Llbt25KSEhQbGysVq1apXHjxnmi2i51pcfBa6+9poEDB8putzstt/JxgNrxxTGhJnx9TLhSjAl1g0vR4VKRkZEKCAi44FvNCgsLFR0dXe1roqOja1S+KtQ8evSoMjIyOFuzltyxjzZv3qyioiK1atVKgYGBCgwM1NGjR/XYY4+pdevWbmmHr3LH/omMjFRgYKC6dOniVKZz5858K/oVcsf++f777/Xkk09q3rx5GjRokLp166ZJkyZp2LBhmjt3rnsaAgD1SG3G7qZNm2rNmjU6e/asjh49qgMHDuiqq65S27ZtLyg7adIkvffee8rKylKLFi0uWZeEhARJUl5eXi1bUzvu7oMq4eHh6tChg6N90dHRKi8vV3FxcY236y7u7oOjR4/qww8/1AMPPHDZunjqOKiNi81jQkND1aBBgxr1qzcdB7V1uX74KSuMCbVxJX1QxZvHhNq4kj7w1THBGxFswqVsNpvi4+OVmZnpWFZZWanMzEwlJiZW+5rExESn8pKUkZHhVL4q1Dx48KA+/PBDNWnSxD0NqAfcsY9Gjhyp3bt3a9euXY6H3W7X5MmT9f7777uvMT7IHfvHZrOpZ8+eys3NdSrzxRdfKDY21sUt8G3u2D8VFRWqqKiQv7/zn+SAgADH2bYAgNqrzdhdJSQkRM2bN9cPP/ygd955R4MHD3asM8Zo0qRJevfdd/XRRx+pTZs2l63Lrl27JEkxMTG1a0wtuasPfu7MmTM6dOiQo33x8fEKCgpy2m5ubq6OHTt22e26mrv7IC0tTc2aNdMdd9xx2bp46jiojZrMMy/Xr950HNRWTT6zWmlMqI2a9MHPefOYUBtX0ge+OiZ4Jc9+dxF80cqVK01wcLBZtmyZ2bdvn5kwYYIJDw83BQUFxhhjRo4caZ544glH+S1btpjAwEAzd+5cs3//fjN9+nQTFBRk9uzZY4wxpry83Nx5552mRYsWZteuXSY/P9/xKCsr80gbrc7V+6g6fCt67blj/6xevdoEBQWZV1991Rw8eNAsXLjQBAQEmM2bN9d5+6zOHfunX79+Ji4uzmRlZZnDhw+btLQ0ExISYhYvXlzn7QMAX3SlY/fWrVvNO++8Yw4dOmQ2bdpkbr31VtOmTRtz6tQpR5kHH3zQhIWFmY0bNzrNT7/77jtjjDF5eXlm5syZZvv27ebIkSNm7dq1pm3btqZv37512vYq7uiDxx57zGzcuNEcOXLEbNmyxSQlJZnIyEhTVFTkKDNx4kTTqlUr89FHH5nt27ebxMREk5iYWGft/il39IExP34LeKtWrczUqVMv2Ka3HQenT582O3fuNDt37jSSzLx588zOnTvN0aNHjTHGPPHEE2bkyJGO8ocPHzYNGzY0kydPNvv37zeLFi0yAQEBJj093VHmcv1qjHcdB8a4px+sNia4ow+sNia4ow+MsdaY4AsINuEWCxcuNK1atTI2m8306tXLbN261bGuX79+ZtSoUU7lV61aZTp06GBsNpuJi4sz69evd6w7cuSIkVTtIysrq45a5HtcuY+qQ7D5y7hj/7z22mumffv2JiQkxHTv3t2sWbPG3c3wWa7eP/n5+Wb06NHGbrebkJAQ07FjR/PCCy+YysrKumgOANQLVzJ2b9y40XTu3NkEBwebJk2amJEjR5qvvvrK6f0uNj9NS0szxhhz7Ngx07dvXxMREWGCg4NN+/btzeTJk01JSUldNLdaru6DYcOGmZiYGGOz2Uzz5s3NsGHDTF5enlOZ77//3jz00EPm6quvNg0bNjR33323yc/Pd2s7L8XVfWCMMe+//76RZHJzcy9Y523HQVZWVrXHbVW7R40aZfr163fBa3r06GFsNptp27at4xj/qUv1qzHedxy4ox+sNia4ow+sNia46/fBSmOCL/Azxhj3nAsKAAAAAAAAAO7BPTYBAAAAAAAAWA7BJgAAAAAAAADLIdgEAAAAAAAAYDkEmwAAAAAAAAAsh2ATAAAAAAAAgOUQbAIAAAAAAACwHIJNAAAAAAAAAJZDsAkAAAAAAADAcgg2AQAAAAAAAFgOwSYAAAAAAAAAyyHYBOCTTp8+rREjRqhRo0aKiYnR/PnzdfPNN+vRRx/VzJkzdc0111zwmh49euiPf/yjJMnPz++CR+vWreu4FQAAAIBnXGo+vXHjxmrny6NHj5YktW7dutr1AOBqBJsAfFJqaqq2bNmidevWKSMjQ5s3b9aOHTskSWPHjtX+/fv12WefOcrv3LlTu3fv1pgxYyRJ+fn5jkdeXp7at2+vvn37eqQtAAAAQF271Hz6hhtucJovf/TRRwoJCXHMlz/77DPHuuPHj6t3797q06ePJ5sDwEcFeroCAOBqp0+f1uuvv64VK1aof//+kqS0tDTZ7XZJUosWLZScnKy0tDT17NnTsb5fv35q27atJCk6OlqSZIzR0KFDFRYWpldeecUDrQEAAADq1uXm0zabzTFf/vbbb/XAAw9o7NixGjt2rCSpadOmjvd65JFHlJ+f73RSAQC4CmdsAvA5hw8fVkVFhXr16uVYFhYWpo4dOzqejx8/Xn//+9917tw5lZeXa8WKFY6J2E89+eSTys7O1tq1a9WgQYM6qT8AAADgSTWZT0tSRUWFhg4dqtjYWC1YsOCC93n11Vf12muvad26dU5hJwC4CmdsAqiXBg0apODgYL377ruy2WyqqKjQb3/7W6cyb775pubPn6+NGzeqefPmHqopAAAA4J0efPBBffnll9q2bZsCA53jhaysLD388MP6+9//rm7dunmohgB8HWdsAvA5bdu2VVBQkNPlLiUlJfriiy8czwMDAzVq1CilpaUpLS1Nw4cPdzojMzs7Ww888IBeeeUV9e7du07rDwAAAHhSTebT8+bN06pVq7R27Vo1adLE6fV5eXn67W9/qyeffFJDhgyps3oDqH84YxOAz2ncuLFGjRqlyZMnKyIiQs2aNdP06dPl7+/v9G2MDzzwgDp37ixJ2rJli2N5QUGB7r77bg0fPlzJyckqKCiQJAUEBHAJDQAAAHze5ebTH374oaZMmaJFixYpMjLSMV9u0KCBbDabBg0apGuvvVYTJkxwrJP+ex97AHAVP2OM8XQlAMDVTp8+rYkTJ2rNmjUKDQ3VlClTtHLlSt16662aNWuWo1zfvn118uRJ7d2717Fs48aNuuWWWy54z9jYWP3nP/+pi+oDAAAAHnWp+XRwcLBmzJhxwWtGjRqlZ599Vm3atKn2PYkfALgawSaAeuHs2bNq3ry5XnjhBY0bN07SjxOrX/3qV3rooYeUmprq4RoCAAAA3qu6+TQAeBqXogPwSTt37tSBAwfUq1cvlZSUaObMmZKkwYMHS5K+/vprrVy5UgUFBRozZownqwoAAAB4ncvNpwHAGxBsAvBZc+fOVW5urmw2m+Lj47V582ZFRkZKkpo1a6bIyEi9+uqruvrqqz1cUwAAAMD7XGo+DQDegEvRAQAAAAAAAFiOv6crAAAAAAAAAABXimATAAAAAAAAgOUQbAIAAAAAAACwHIJNAAAAAAAAAJZDsAkAAAAAAADAcgg2AQAAAAAAAFgOwSYAAAAAAAAAyyHYBAAAAAAAAGA5BJsAAAAAAAAALOf/AwZihZgKFneHAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#%%script echo skipping\n",
        "\n",
        "# Variable names\n",
        "variable_names = [\"D\", \"Sx\", \"Sy\", \"Sz\", \"tau\", \"Bconsx\", \"Bconsy\", \"Bconsz\", \"gxx\", \"gxy\", \"gxz\", \"gyy\", \"gyz\", \"gzz\"]\n",
        "\n",
        "# Plotting histograms of the input variables before z-score normalization\n",
        "plt.figure(figsize=(16, 16))\n",
        "plt.suptitle('Histograms of input variables before (or without at all) z-score normalization', y=1.03)\n",
        "\n",
        "for i in range(N_INPUTS):\n",
        "    plt.subplot(7, 2, i+1)\n",
        "    data = x_train[:, i].cpu().numpy() # Convert tensor to numpy array for percentile calculation\n",
        "    lower_bound, upper_bound = np.percentile(data, [1, 100]) # NOTE: Use this instead to visualize all the data.\n",
        "    # lower_bound, upper_bound = np.percentile(data, [1, 90]) # Calculate 1st and 99th percentile\n",
        "\n",
        "    plt.hist(data, bins=50, range=(lower_bound, upper_bound)) # Set range to the calculated percentile range\n",
        "    plt.xlabel(variable_names[i])\n",
        "    plt.xlim(lower_bound, upper_bound) # Set the x limit to match the range of the histogram\n",
        "\n",
        "plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "plt.show()\n",
        "\n",
        "# Same plotting but for the test data. Adjust the percentile range as needed to zoom in.\n",
        "# plt.figure(figsize=(16, 16))\n",
        "# plt.suptitle('Histograms of input variables before (or without at all) z-score normalization (test data)', y=1.03)\n",
        "\n",
        "# for i in range(14):\n",
        "#     plt.subplot(7, 2, i+1)\n",
        "#     data = x_test[:, i].cpu().numpy() # Convert tensor to numpy array for percentile calculation\n",
        "#     #lower_bound, upper_bound = np.percentile(data, [1, 100]) # NOTE: Use this instead to visualize all the data.\n",
        "#     lower_bound, upper_bound = np.percentile(data, [1, 99]) # Use this to zoom in.\n",
        "\n",
        "#     plt.hist(data, bins=50, range=(lower_bound, upper_bound)) # Set range to the calculated percentile range\n",
        "#     plt.xlabel(variable_names[i])\n",
        "#     plt.xlim(lower_bound, upper_bound) # Set the x limit to match the range of the histogram\n",
        "\n",
        "# plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "# plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C3jSavqlXa9"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plEmGleqyUfJ"
      },
      "source": [
        "### Data normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5ouzqffyUfJ",
        "outputId": "8cd00c6d-77d4-4032-ee96-0dac947f0df5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary statistics of input variables before z-score normalization\n",
            "tensor([[ 1.7178e-05,  1.8661e+03,  2.2990e+00,  1.6672e+00,  7.8869e+00],\n",
            "        [-9.9749e+03,  4.0428e+05,  4.5452e+01,  2.1877e+01,  1.5903e+03],\n",
            "        [-8.9312e+03,  3.6518e+06,  9.3442e+01,  2.2109e+01,  1.3813e+04],\n",
            "        [-1.1200e+04,  2.9022e+05,  4.3849e+01,  2.1879e+01,  1.1846e+03],\n",
            "        [-1.5104e+04,  3.6454e+06,  1.4806e+02,  7.0033e+01,  1.3801e+04],\n",
            "        [-1.1340e+01,  1.1406e+01, -2.0666e-02, -7.4331e-03,  5.7423e+00],\n",
            "        [-1.1255e+01,  1.1218e+01,  9.0668e-03,  4.5898e-03,  5.7402e+00],\n",
            "        [-1.1131e+01,  1.1091e+01,  2.9727e-02,  5.7525e-02,  5.7442e+00],\n",
            "        [ 9.0000e-01,  1.1000e+00,  9.9869e-01,  9.9830e-01,  5.7583e-02],\n",
            "        [ 7.1982e-07,  1.0000e-01,  4.9360e-02,  4.9177e-02,  2.8987e-02],\n",
            "        [ 3.8715e-07,  9.9998e-02,  4.9399e-02,  4.9102e-02,  2.8843e-02],\n",
            "        [ 9.0000e-01,  1.1000e+00,  9.9894e-01,  9.9830e-01,  5.7585e-02],\n",
            "        [ 1.0813e-06,  1.0000e-01,  4.9433e-02,  4.9363e-02,  2.8918e-02],\n",
            "        [ 9.0000e-01,  1.1000e+00,  9.9851e-01,  9.9770e-01,  5.7749e-02]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# Computing summary statistics of the input variables before and after z-score normalization\n",
        "print('Summary statistics of input variables before z-score normalization')\n",
        "print(torch.stack([torch.min(x_train, dim=0).values, torch.max(x_train, dim=0).values, torch.nanmean(x_train, dim=0), torch.median(x_train, dim=0).values, torch.std(x_train, dim=0)], dim=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTEmkR1SUZh7"
      },
      "source": [
        "Perform z-score normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_ifIyZPyUfJ"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPOv6DxhUZh7"
      },
      "outputs": [],
      "source": [
        "if ZSCORE_NORMALIZATION:\n",
        "    \n",
        "    # TODO: Add magnetic field variables to the normalization if I decide to use the old code that is commented out here.\n",
        "    # # Computing the median of each input variable from the training set using torch.nanmedian function\n",
        "    # D_median = torch.nanmedian(x_train[:, 0])\n",
        "    # Sx_median = torch.nanmedian(x_train[:, 1])\n",
        "    # Sy_median = torch.nanmedian(x_train[:, 2])\n",
        "    # Sz_median = torch.nanmedian(x_train[:, 3])\n",
        "    # tau_median = torch.nanmedian(x_train[:, 4])\n",
        "\n",
        "    # # Computing the standard deviation of each input variable from the training set using torch.std function with a boolean mask to ignore nan values\n",
        "    # D_std = torch.std(x_train[~torch.isnan(x_train[:, 0]), 0])\n",
        "    # Sx_std = torch.std(x_train[~torch.isnan(x_train[:, 1]), 1])\n",
        "    # Sy_std = torch.std(x_train[~torch.isnan(x_train[:, 2]), 2])\n",
        "    # Sz_std = torch.std(x_train[~torch.isnan(x_train[:, 3]), 3])\n",
        "    # tau_std = torch.std(x_train[~torch.isnan(x_train[:, 4]), 4])\n",
        "\n",
        "\n",
        "    # # Applying z-score normalization to both train and test sets using the statistics from the training set\n",
        "    # x_train[:, 0] = torch.sub(x_train[:, 0], D_median).div(D_std)\n",
        "    # x_train[:, 1] = torch.sub(x_train[:, 1], Sx_median).div(Sx_std)\n",
        "    # x_train[:, 2] = torch.sub(x_train[:, 2], Sy_median).div(Sy_std)\n",
        "    # x_train[:, 3] = torch.sub(x_train[:, 3], Sz_median).div(Sz_std)\n",
        "    # x_train[:, 4] = torch.sub(x_train[:, 4], tau_median).div(tau_std)\n",
        "\n",
        "    # x_test[:, 0] = torch.sub(x_test[:, 0], D_median).div(D_std)\n",
        "    # x_test[:, 1] = torch.sub(x_test[:, 1], Sx_median).div(Sx_std)\n",
        "    # x_test[:, 2] = torch.sub(x_test[:, 2], Sy_median).div(Sy_std)\n",
        "    # x_test[:, 3] = torch.sub(x_test[:, 3], Sz_median).div(Sz_std)\n",
        "    # x_test[:, 4] = torch.sub(x_test[:, 4], tau_median).div(tau_std)\n",
        "\n",
        "    # Computing the mean and standard deviation of each column\n",
        "    mean = x_train.mean(dim=0)\n",
        "    std = x_train.std(dim=0)\n",
        "\n",
        "    # Applying z-score normalization\n",
        "    x_train = (x_train - mean) / std\n",
        "    # Use the same mean and std from the training data as we don't want test data leakage.\n",
        "    x_test = (x_test - mean) / std\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT2ZWWs_yUfV"
      },
      "source": [
        "Plotting the histograms of the input data after normalization if z-score normalization was performed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tECmchuwyUfV"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"last_expr_or_assign\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlPs0YzAyUfV"
      },
      "outputs": [],
      "source": [
        "if not LOAD_DATA_FROM_CSV:\n",
        "    if ZSCORE_NORMALIZATION:\n",
        "        # Variable names\n",
        "        variable_names = [\"D\", \"Sx\", \"Sy\", \"Sz\", \"tau\", \"Bconsx\", \"Bconsy\", \"Bconsz\", \"gxx\", \"gxy\", \"gxz\", \"gyy\", \"gyz\", \"gzz\"]\n",
        "\n",
        "        # Plotting histograms of the input variables before z-score normalization\n",
        "        plt.figure(figsize=(16, 16))\n",
        "        plt.suptitle('Histograms of input variables before (or without at all) z-score normalization', y=1.03)\n",
        "\n",
        "        for i in range(N_INPUTS):\n",
        "            plt.subplot(7, 2, i+1)\n",
        "            data = x_train[:, i].cpu().numpy() # Convert tensor to numpy array for percentile calculation\n",
        "            lower_bound, upper_bound = np.percentile(data, [1, 100]) # NOTE: Use this instead to visualize all the data.\n",
        "            # lower_bound, upper_bound = np.percentile(data, [1, 90]) # Calculate 1st and 99th percentile\n",
        "\n",
        "            plt.hist(data, bins=50, range=(lower_bound, upper_bound)) # Set range to the calculated percentile range\n",
        "            plt.xlabel(variable_names[i])\n",
        "            plt.xlim(lower_bound, upper_bound) # Set the x limit to match the range of the histogram\n",
        "\n",
        "        plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UR-wo0SyUfW"
      },
      "outputs": [],
      "source": [
        "if ZSCORE_NORMALIZATION:\n",
        "    # Computing summary statistics of the input variables after z-score normalization\n",
        "    print('Summary statistics of input variables after z-score normalization')\n",
        "    print(torch.stack([torch.min(x_train, dim=0).values, torch.max(x_train, dim=0).values, torch.mean(x_train, dim=0), torch.median(x_train, dim=0).values, torch.std(x_train, dim=0)], dim=1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSKTD00gyUfW"
      },
      "source": [
        "### Visualizing input data and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5zmSieOyUfW",
        "outputId": "7c40f866-48ed-42de-a977-1f836ab283c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.0361,  2.7455, 17.4197,  ..., -1.6525,  1.5343, -3.0060],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "x_train\n",
        "y_train\n",
        "x_val\n",
        "y_val\n",
        "x_test\n",
        "y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E96p_MsOUZh9",
        "outputId": "4b95bad0-8f3a-4364-eed1-008e0ce2a5e3"
      },
      "source": [
        "Checking if our output is always positive ~~by plotting a histogram of y_train and y_test tensors~~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXdQF0fdyUfW",
        "outputId": "74f0111f-30ce-4dc6-a3f4-ca8277fac0c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(True)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Create a sample tensor\n",
        "tensor = torch.randn(80000, 14)  # Assuming a tensor of shape [80000, 14]\n",
        "tensor\n",
        "\n",
        "\n",
        "# Check if any element is negative\n",
        "any_negative = torch.any(tensor < 0)\n",
        "\n",
        "# Print the result\n",
        "print(any_negative)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ8DI737yUfX",
        "outputId": "ad420533-268e-4131-b883-e4d096d96402"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "y_train.shape\n",
        "any_negative = torch.any(y_train < 0)\n",
        "any_negative\n",
        "\n",
        "y_test.shape\n",
        "any_negative = torch.any(y_test < 0)\n",
        "any_negative\n",
        "\n",
        "x_train.shape\n",
        "any_negative = torch.any(x_train < 0)\n",
        "any_negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "ljSZWb-cyUfX",
        "outputId": "060d45bd-2081-42dd-b051-8eaba192962e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#%%script echo skipping\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(y_train.cpu().numpy(), bins=100) # must be cpu here.\n",
        "plt.xlabel(\"y_train\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.xscale(\"log\")\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.hist(y_val.cpu().numpy(), bins=100) # must be cpu here\n",
        "plt.xlabel(\"y_val\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.xscale(\"log\")\n",
        "plt.tight_layout()\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(y_test.cpu().numpy(), bins=100) # must be cpu here\n",
        "plt.xlabel(\"y_test\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.xscale(\"log\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEgjk--AUZh9"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2b9GecHUZh9"
      },
      "source": [
        "## Defining the neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4l7rti5iHi_T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv8HA-ZXUZh-"
      },
      "outputs": [],
      "source": [
        "# Defining a class for the network\n",
        "class Net(nn.Module):\n",
        "    \"\"\"A class for creating a network with a\n",
        "    variable number of hidden layers and units.\n",
        "\n",
        "    Attributes:\n",
        "        n_layers (int): The number of hidden layers in the network.\n",
        "        n_units (list): A list of integers representing the number of units in each hidden layer.\n",
        "        hidden_activation (torch.nn.Module): The activation function for the hidden layers.\n",
        "        output_activation (torch.nn.Module): The activation function for the output layer.\n",
        "        layers (torch.nn.ModuleList): A list of linear layers in the network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_layers, n_units, hidden_activation, output_activation, dropout_rate):\n",
        "        \"\"\"Initializes the network with the given hyperparameters.\n",
        "\n",
        "        Args:\n",
        "            n_layers (int): The number of hidden layers in the network.\n",
        "            n_units (list): A list of integers representing the number of units in each hidden layer.\n",
        "            hidden_activation (torch.nn.Module): The activation function for the hidden layers.\n",
        "            output_activation (torch.nn.Module): The activation function for the output layer.\n",
        "            TODO: [ver. Copilot description] dropout_rate (float): The dropout rate to use for all layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.n_units = n_units\n",
        "        self.hidden_activation = hidden_activation\n",
        "        self.output_activation = output_activation\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # Creating a list of linear layers with different numbers of units for each layer\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.dropouts = nn.ModuleList()\n",
        "\n",
        "        self.layers.append(nn.Linear(N_INPUTS, n_units[0]))\n",
        "        self.dropouts.append(nn.Dropout(p=dropout_rate))\n",
        "\n",
        "        for i in range(1, n_layers):\n",
        "            self.layers.append(nn.Linear(n_units[i - 1], n_units[i]))\n",
        "            self.dropouts.append(nn.Dropout(p=dropout_rate))\n",
        "\n",
        "        self.layers.append(nn.Linear(n_units[-1], N_OUTPUTS))\n",
        "\n",
        "        # Adding some assertions to check that the input arguments are valid\n",
        "        assert isinstance(n_layers, int) and n_layers > 0, \"n_layers must be a positive integer\"\n",
        "        assert isinstance(n_units, list) and len(n_units) == n_layers, \"n_units must be a list of length n_layers\"\n",
        "        assert all(isinstance(n, int) and n > 0 for n in n_units), \"n_units must contain positive integers\"\n",
        "        assert isinstance(hidden_activation, nn.Module), \"hidden_activation must be a torch.nn.Module\"\n",
        "        assert isinstance(output_activation, nn.Module), \"output_activation must be a torch.nn.Module\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Performs a forward pass on the input tensor.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input tensor of shape (batch_size, N_INPUTS).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The output tensor of shape (batch_size, N_OUTPUTS).\n",
        "        \"\"\"\n",
        "        # Adding an assertion to check that the input tensor has the expected shape and type\n",
        "        assert isinstance(x, torch.Tensor), \"x must be a torch.Tensor\"\n",
        "        assert x.shape[1] == N_INPUTS, f\"x must have shape (batch_size, {N_INPUTS})\"\n",
        "\n",
        "        for layer, dropout in zip(self.layers[:-1], self.dropouts):\n",
        "            x = dropout(self.hidden_activation(layer(x)))\n",
        "        # Applying the linear transformation and the activation function on the output layer\n",
        "        x = self.output_activation(self.layers[-1](x)) # No dropout at output layer\n",
        "\n",
        "        return x # Returning the output tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVUQAVhzyUfY",
        "outputId": "af00be8c-3b30-4706-e8f6-8534d27612fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "[1,2,3]\n",
        "[1,2,3][:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEBudkK1yUfY",
        "outputId": "2f78e8fc-a58b-4e7b-86ad-52c8c5b0fa01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "for x in [1,2,3][:-1]:\n",
        "    print(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GNvp55PUZh_"
      },
      "source": [
        "## Defining the model and search space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a1opluOUZh_"
      },
      "outputs": [],
      "source": [
        "# Defining a function to create a trial network and optimizer\n",
        "def create_model(trial, optimize):\n",
        "    \"\"\"Creates a trial network and optimizer based on the sampled hyperparameters.\n",
        "\n",
        "    Args:\n",
        "        trial (optuna.trial.Trial): The trial object that contains the hyperparameters.\n",
        "        optimize (boolean): Whether to optimize the hyperparameters or to use predefined values.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple of (net, loss_fn, optimizer, batch_size, n_epochs,\n",
        "            scheduler, loss_name, optimizer_name, scheduler_name,\n",
        "            n_units, n_layers, hidden_activation, output_activation),\n",
        "            where net is the trial network,\n",
        "            loss_fn is the loss function,\n",
        "            optimizer is the optimizer,\n",
        "            batch_size is the batch size,\n",
        "            n_epochs is the number of epochs,\n",
        "            scheduler is the learning rate scheduler,\n",
        "            loss_name is the name of the loss function,\n",
        "            optimizer_name is the name of the optimizer,\n",
        "            scheduler_name is the name of the scheduler,\n",
        "            n_units is a list of integers representing\n",
        "            the number of units in each hidden layer,\n",
        "            n_layers is an integer representing the number of hidden layers in the network,\n",
        "            hidden_activation is a torch.nn.Module representing the activation function for the hidden layers,\n",
        "            output_activation is a torch.nn.Module representing the activation function for the output layer,\n",
        "            lr is the (initial) learning rate.\n",
        "            dropout_rate is the dropout rate.\n",
        "    \"\"\"\n",
        "    # If optimize is True, sample the hyperparameters from the search space\n",
        "    if OPTIMIZE:\n",
        "        n_layers = trial.suggest_int(\"n_layers\", 2, 10)\n",
        "        n_units = [trial.suggest_int(f\"n_units_{i}\", 16, 4096) for i in range(n_layers)]\n",
        "\n",
        "        hidden_activation_name = trial.suggest_categorical(\n",
        "            \"hidden_activation\", [\"ReLU\", \"LeakyReLU\", \"ELU\", \"PReLU\", \"Swish\", \"GELU\", \"SoftPlus\"]\n",
        "        )\n",
        "        output_activation_name = trial.suggest_categorical( \"output_activation\", [\"Linear\"])\n",
        "\n",
        "        loss_name = trial.suggest_categorical( \"loss\", [\"MSE\", \"MAE\", \"Huber\", \"Quantile\"])  # Added Quantile loss\n",
        "\n",
        "        optimizer_name = trial.suggest_categorical( \"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\", \"Adagrad\"] )\n",
        "\n",
        "        lr = trial.suggest_loguniform(\"lr\", 1e-6, 1)\n",
        "\n",
        "        batch_size_list = [32, 64, 128, 256, 512, 1024, 2048]\n",
        "        batch_size = trial.suggest_categorical(\"batch_size\", batch_size_list)\n",
        "        \n",
        "        n_epochs = trial.suggest_int(\"n_epochs\", 50, 150)\n",
        "\n",
        "        # scheduler_name = trial.suggest_categorical(\"scheduler\", [\"CosineAnnealingLR\", \"ReduceLROnPlateau\", \"StepLR\", \"CyclicLR\"])\n",
        "        scheduler_name = trial.suggest_categorical(\"scheduler\", [\"CosineAnnealingLR\", \"ReduceLROnPlateau\", \"StepLR\"])\n",
        "\n",
        "        # Creating the activation functions from their names\n",
        "        if hidden_activation_name == \"ReLU\":\n",
        "            hidden_activation = nn.ReLU()\n",
        "        elif hidden_activation_name == \"LeakyReLU\":\n",
        "            negative_slope = trial.suggest_uniform(\"leakyrelu_slope\", 0.01, 0.3)\n",
        "            hidden_activation = nn.LeakyReLU(negative_slope=negative_slope)\n",
        "        elif hidden_activation_name == \"ELU\":\n",
        "            hidden_activation = nn.ELU() \n",
        "        elif hidden_activation_name == \"PReLU\":\n",
        "            init = trial.suggest_uniform(\"prelu_init\", 0.1, 0.3)\n",
        "            hidden_activation = nn.PReLU(init=init)\n",
        "        elif hidden_activation_name == \"Swish\":\n",
        "            class Swish(nn.Module):\n",
        "                def forward(self, x):\n",
        "                    return x * torch.sigmoid(x)\n",
        "            hidden_activation = Swish()\n",
        "        elif hidden_activation_name == \"GELU\":\n",
        "            hidden_activation = nn.GELU()\n",
        "        elif hidden_activation_name == \"SoftPlus\":\n",
        "            beta = trial.suggest_uniform(\"softplus_beta\", 0.5, 1.5)\n",
        "            hidden_activation = nn.Softplus(beta=beta) # We don't optimize thresshold subparameters, as it's mainly for numerical stability.\n",
        "\n",
        "        dropout_rate = trial.suggest_uniform(\"dropout_rate\", 0.0, 0.5)\n",
        "\n",
        "\n",
        "    # If optimize is False, use the predefined values\n",
        "    else:\n",
        "        # Setting the hyperparameters to the predefined values\n",
        "        n_layers = N_LAYERS_NO_OPT\n",
        "        n_units = N_UNITS_NO_OPT\n",
        "        hidden_activation_name = HIDDEN_ACTIVATION_NAME_NO_OPT\n",
        "        output_activation_name = OUTPUT_ACTIVATION_NAME_NO_OPT\n",
        "        loss_name = LOSS_NAME_NO_OPT\n",
        "        optimizer_name = OPTIMIZER_NAME_NO_OPT\n",
        "        lr = LR_NO_OPT\n",
        "        batch_size = BATCH_SIZE_NO_OPT\n",
        "        n_epochs = N_EPOCHS_NO_OPT\n",
        "        scheduler_name = SCHEDULER_NAME_NO_OPT\n",
        "\n",
        "        # Creating the activation functions from their names\n",
        "        if hidden_activation_name == \"ReLU\":\n",
        "            hidden_activation = nn.ReLU()\n",
        "        elif hidden_activation_name == \"LeakyReLU\":\n",
        "            negative_slope = 0.01\n",
        "            hidden_activation = nn.LeakyReLU(negative_slope=negative_slope)\n",
        "        elif hidden_activation_name == \"ELU\":\n",
        "            hidden_activation = nn.ELU() \n",
        "        elif hidden_activation_name == \"PReLU\":\n",
        "            init = 0.25\n",
        "            hidden_activation = nn.PReLU(init=init)\n",
        "        elif hidden_activation_name == \"Swish\":\n",
        "            class Swish(nn.Module):\n",
        "                def forward(self, x):\n",
        "                    return x * torch.sigmoid(x)\n",
        "            hidden_activation = Swish()\n",
        "        elif hidden_activation_name == \"GELU\":\n",
        "            hidden_activation = nn.GELU()\n",
        "        elif hidden_activation_name == \"SoftPlus\":\n",
        "            beta = 1\n",
        "            hidden_activation = nn.Softplus(beta=beta) # We don't optimize threshold subparameter, as it's mainly for numerical stability.\n",
        "\n",
        "        dropout_rate = DROPOUT_RATE_NO_OPT\n",
        "\n",
        "\n",
        "    # We used to have options here, but since we have a regression problem with continuous output, we only use Linear.\n",
        "    output_activation = nn.Identity()\n",
        "\n",
        "    # Creating the loss function from its name\n",
        "    if loss_name == \"MSE\":\n",
        "        loss_fn = nn.MSELoss()\n",
        "    elif loss_name == \"MAE\":\n",
        "        loss_fn = nn.L1Loss()\n",
        "    elif loss_name == \"Huber\":\n",
        "        loss_fn = nn.SmoothL1Loss() \n",
        "    elif loss_name == \"Quantile\":\n",
        "        def quantile_loss(y_pred, y_true, q=0.5):\n",
        "            e = y_pred - y_true\n",
        "            return torch.mean(torch.max(q*e, (q-1)*e))\n",
        "        loss_fn = quantile_loss\n",
        "    else:\n",
        "        def log_cosh_loss(y_pred, y_true):\n",
        "            return torch.mean(torch.log(torch.cosh(y_pred - y_true)))\n",
        "        loss_fn = log_cosh_loss\n",
        "\n",
        "    # Creating the network with the sampled hyperparameters\n",
        "    net = Net(n_layers, n_units, hidden_activation, output_activation, dropout_rate).to(device)\n",
        "\n",
        "\n",
        "    if OPTIMIZE:\n",
        "        # Creating the optimizer from its name\n",
        "        if optimizer_name == \"SGD\":\n",
        "            weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
        "            momentum = trial.suggest_uniform(\"momentum\", 0.0, 0.99)\n",
        "            optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
        "        elif optimizer_name == \"Adam\":\n",
        "            weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
        "            beta1 = trial.suggest_uniform(\"beta1\", 0.9, 0.999)\n",
        "            beta2 = trial.suggest_uniform(\"beta2\", 0.999, 0.9999)\n",
        "            optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay, betas=(beta1, beta2))\n",
        "        elif optimizer_name == \"RMSprop\":\n",
        "            optimizer = optim.RMSprop(net.parameters(), lr=lr)\n",
        "        else:\n",
        "            optimizer = optim.Adagrad(net.parameters(), lr=lr)\n",
        "\n",
        "        # Creating the learning rate scheduler from its name\n",
        "        if scheduler_name == \"StepLR\":\n",
        "            step_size = trial.suggest_int(\"step_size\", 5, 15)\n",
        "            gamma = trial.suggest_uniform(\"gamma\", 0.1, 0.5)\n",
        "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "        elif scheduler_name == \"ExponentialLR\":\n",
        "            gamma = trial.suggest_uniform(\"gamma\", 0.8, 0.99)\n",
        "            scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
        "        elif scheduler_name == \"CosineAnnealingLR\":\n",
        "            if n_epochs < 150:\n",
        "                t_max_fraction = trial.suggest_uniform('t_max_fraction', 0.1, 0.3)\n",
        "            elif n_epochs > 250:\n",
        "                t_max_fraction = trial.suggest_uniform('t_max_fraction', 0.05, 0.1)\n",
        "            else:\n",
        "                t_max_fraction = trial.suggest_uniform('t_max_fraction', 0.1, 0.2)\n",
        "\n",
        "            T_max = int(n_epochs * t_max_fraction)\n",
        "            eta_min = trial.suggest_loguniform(\"eta_min\", 1e-7, 1e-2)\n",
        "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)\n",
        "        elif scheduler_name == \"ReduceLROnPlateau\":\n",
        "            factor = trial.suggest_uniform(\"factor\", 0.1, 0.5)\n",
        "            patience = trial.suggest_int(\"patience\", 5, 10)\n",
        "            threshold = trial.suggest_loguniform(\"threshold\", 1e-4, 1e-2)\n",
        "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                optimizer, mode=\"min\", factor=factor, patience=patience, threshold=threshold\n",
        "            )\n",
        "        elif scheduler_name == \"CyclicLR\":\n",
        "            base_lr = trial.suggest_loguniform(\"base_lr\", 1e-6, 1e-2)\n",
        "            max_lr = trial.suggest_loguniform(\"max_lr\", 1e-4, 1)\n",
        "            step_size_up = trial.suggest_int(\"step_size_up\", 200, 2000)\n",
        "            scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr, step_size_up=step_size_up)\n",
        "        else:\n",
        "            scheduler = None\n",
        "    else:\n",
        "        # Creating the optimizer from its name\n",
        "        if optimizer_name == \"SGD\":\n",
        "            optimizer = optim.SGD(net.parameters(), lr=lr)\n",
        "        elif optimizer_name == \"Adam\":\n",
        "            optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "        elif optimizer_name == \"RMSprop\":\n",
        "            optimizer = optim.RMSprop(net.parameters(), lr=lr)\n",
        "        else:\n",
        "            optimizer = optim.Adagrad(net.parameters(), lr=lr)\n",
        "\n",
        "        # Creating the learning rate scheduler from its name\n",
        "        if scheduler_name == \"StepLR\":\n",
        "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "        elif scheduler_name == \"ExponentialLR\":\n",
        "            scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "        elif scheduler_name == \"CosineAnnealingLR\":\n",
        "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer)\n",
        "        elif scheduler_name == \"ReduceLROnPlateau\":\n",
        "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                        optimizer, mode=\"min\", factor=0.18979341786654758, patience=11, threshold=0.0017197466122611932 #, min_lr=1e-6\n",
        "                    )\n",
        "        elif scheduler_name == \"CyclicLR\":\n",
        "            # TODO: Change these appropriately.\n",
        "            base_lr = 1e-6\n",
        "            max_lr = 1e-4\n",
        "            step_size_up = 200\n",
        "            scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr, step_size_up=step_size_up)\n",
        "        else:\n",
        "            scheduler = None\n",
        "\n",
        "    # Returning all variables needed for saving and loading\n",
        "    return net, loss_fn, optimizer, batch_size, n_epochs, scheduler, loss_name, optimizer_name, scheduler_name, n_units, n_layers, hidden_activation, output_activation, lr, dropout_rate\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-czA7VvUZiD"
      },
      "source": [
        " ## The training and evaluation loop\n",
        "\n",
        " We first define a couple of functions used in the training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD6FQNmxUZiD"
      },
      "outputs": [],
      "source": [
        "# Defining a function that computes loss and metrics for a given batch\n",
        "def compute_loss_and_metrics(y_pred, y_true, loss_fn):\n",
        "    \"\"\"Computes loss and metrics for a given batch.\n",
        "\n",
        "    Args:\n",
        "        y_pred (torch.Tensor): The predicted pressure tensor of shape (batch_size, 1).\n",
        "        y_true (torch.Tensor): The true pressure tensor of shape (batch_size,).\n",
        "        loss_fn (torch.nn.Module or function): The loss function to use.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple of (loss, l1_norm), where loss is a scalar tensor,\n",
        "            l1_norm is L1 norm for relative error of pressure,\n",
        "            each being a scalar tensor.\n",
        "            linf_norm is Linf norm for relative error of pressure.\n",
        "    \"\"\"\n",
        "    # Reshaping the target tensor to match the input tensor\n",
        "    y_true = y_true.view(-1, 1)\n",
        "\n",
        "    # Computing the loss using the loss function\n",
        "    loss = loss_fn(y_pred, y_true)\n",
        "\n",
        "    # Computing the relative error of pressure\n",
        "    rel_error = torch.abs((y_pred - y_true) / y_true)\n",
        "\n",
        "    # Computing the L1 norm for the relative error of pressure\n",
        "    l1_norm = torch.mean(rel_error) \n",
        "    # Computing the Linf norm for the relative error of pressure\n",
        "    linf_norm = torch.max(rel_error) \n",
        "\n",
        "    # Returning the loss and metrics\n",
        "    return loss, l1_norm, linf_norm\n",
        "\n",
        "\n",
        "# Defining a function that updates the learning rate scheduler with validation loss if applicable\n",
        "def update_scheduler(scheduler, test_loss):\n",
        "    \"\"\"Updates the learning rate scheduler with validation loss if applicable.\n",
        "\n",
        "    Args:\n",
        "        scheduler (torch.optim.lr_scheduler._LRScheduler or None): The learning rate scheduler to use.\n",
        "        test_loss (float): The validation loss to use.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Checking if scheduler is not None\n",
        "    if scheduler is not None:\n",
        "        # Checking if scheduler is ReduceLROnPlateau\n",
        "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
        "            # Updating the scheduler with test_loss\n",
        "            scheduler.step(test_loss)\n",
        "        else:\n",
        "            # Updating the scheduler without test_loss\n",
        "            scheduler.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1nE662UUZiE"
      },
      "source": [
        "Now for the actual training and evaluation loop,"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to train and evaluate a network\n",
        "def train_and_eval(net, loss_fn, optimizer, batch_size, n_epochs, scheduler, train_loader, val_loader, test_loader, trial=None):\n",
        "    # Initializing lists to store the losses and metrics for each epoch\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    test_losses = []\n",
        "    train_metrics = []\n",
        "    val_metrics = []\n",
        "    test_metrics = []\n",
        "\n",
        "\n",
        "    # Creating a SummaryWriter object to log data for tensorboard\n",
        "    writer = tbx.SummaryWriter()\n",
        "\n",
        "    # Looping over the epochs\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # Setting the network to training mode\n",
        "        net.train()\n",
        "\n",
        "        # Initializing variables to store the total loss and metrics for the train set\n",
        "        train_loss = 0.0\n",
        "        train_l1_norm = 0.0\n",
        "        train_linf_norm = 0.0\n",
        "\n",
        "        # Looping over the batches in the train set\n",
        "        for x_batch, y_batch in train_loader:\n",
        "\n",
        "            # Moving the batch tensors to the device\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            # Zeroing the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Performing a forward pass and computing the loss and metrics\n",
        "            y_pred = net(x_batch)\n",
        "            loss, l1_norm, linf_norm = compute_loss_and_metrics(y_pred, y_batch, loss_fn)\n",
        "\n",
        "\n",
        "            # Performing a backward pass and updating the weights\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Updating the total loss and metrics for the train set\n",
        "            train_loss += loss.item() * x_batch.size(0)\n",
        "            train_l1_norm += l1_norm.item() * x_batch.size(0)\n",
        "            train_linf_norm += linf_norm.item() * x_batch.size(0)\n",
        "\n",
        "        # Computing the average loss and metrics for the train set\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_l1_norm /= len(train_loader.dataset)\n",
        "        train_linf_norm /= len(train_loader.dataset)\n",
        "\n",
        "        # Appending the average loss and metrics for the train set to the lists\n",
        "        train_losses.append(train_loss)\n",
        "        train_metrics.append(\n",
        "            {\n",
        "                \"l1_norm\": train_l1_norm,\n",
        "                \"linf_norm\": train_linf_norm,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Logging the average loss and metrics for the train set to tensorboard\n",
        "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
        "        writer.add_scalar(\"L1 norm/train\", train_l1_norm, epoch)\n",
        "        writer.add_scalar(\"Linf norm/train\", train_linf_norm, epoch)\n",
        "\n",
        "        if val_loader is not None:\n",
        "            net.eval()\n",
        "            val_loss = 0.0\n",
        "            val_l1_norm = 0.0\n",
        "            val_linf_norm = 0.0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for x_batch, y_batch in val_loader:\n",
        "                    x_batch = x_batch.to(device)\n",
        "                    y_batch = y_batch.to(device)\n",
        "                    y_pred = net(x_batch)\n",
        "                    loss, l1_norm, linf_norm = compute_loss_and_metrics(y_pred, y_batch, loss_fn)\n",
        "\n",
        "                    val_loss += loss.item() * x_batch.size(0)\n",
        "                    val_l1_norm += l1_norm.item() * x_batch.size(0)\n",
        "                    val_linf_norm += linf_norm.item() * x_batch.size(0)\n",
        "\n",
        "            val_loss /= len(val_loader.dataset)\n",
        "            val_l1_norm /= len(val_loader.dataset)\n",
        "            val_linf_norm /= len(val_loader.dataset)\n",
        "\n",
        "            val_losses.append(val_loss)\n",
        "            val_metrics.append(\n",
        "                {\n",
        "                    \"l1_norm\": val_l1_norm,\n",
        "                    \"linf_norm\": val_linf_norm,\n",
        "                }\n",
        "            )\n",
        "\n",
        "            writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
        "            writer.add_scalar(\"L1 norm/val\", val_l1_norm, epoch)\n",
        "            writer.add_scalar(\"Linf norm/val\", val_linf_norm, epoch)\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs}.. Train loss: {train_loss:.3f}.. Val loss: {val_loss:.3f}.. Train L1 norm: {train_l1_norm:.3f}.. Val L1 norm: {val_l1_norm:.3f}.. Train Linf norm: {train_linf_norm:.3f}.. Val Linf norm: {val_linf_norm:.3f}\")\n",
        "\n",
        "\n",
        "            update_scheduler(scheduler, val_loss)\n",
        "\n",
        "        if test_loader is not None:\n",
        "            net.eval()\n",
        "            test_loss = 0.0\n",
        "            test_l1_norm = 0.0\n",
        "            test_linf_norm = 0.0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for x_batch, y_batch in test_loader:\n",
        "                    x_batch = x_batch.to(device)\n",
        "                    y_batch = y_batch.to(device)\n",
        "                    y_pred = net(x_batch)\n",
        "                    loss, l1_norm, linf_norm = compute_loss_and_metrics(y_pred, y_batch, loss_fn)\n",
        "\n",
        "                    test_loss += loss.item() * x_batch.size(0)\n",
        "                    test_l1_norm += l1_norm.item() * x_batch.size(0)\n",
        "                    test_linf_norm += linf_norm.item() * x_batch.size(0)\n",
        "\n",
        "            test_loss /= len(test_loader.dataset)\n",
        "            test_l1_norm /= len(test_loader.dataset)\n",
        "            test_linf_norm /= len(test_loader.dataset)\n",
        "\n",
        "            test_losses.append(test_loss)\n",
        "            test_metrics.append(\n",
        "                {\n",
        "                    \"l1_norm\": test_l1_norm,\n",
        "                    \"linf_norm\": test_linf_norm,\n",
        "                }\n",
        "            )\n",
        "\n",
        "            writer.add_scalar(\"Loss/test\", test_loss, epoch)\n",
        "            writer.add_scalar(\"L1 norm/test\", test_l1_norm, epoch)\n",
        "            writer.add_scalar(\"Linf norm/test\", test_linf_norm, epoch)\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs}.. Train loss: {train_loss:.3f}.. Test loss: {test_loss:.3f}.. Train L1 norm: {train_l1_norm:.3f}.. Test L1 norm: {test_l1_norm:.3f}.. Train Linf norm: {train_linf_norm:.3f}.. Test Linf norm: {test_linf_norm:.3f}\")\n",
        "\n",
        "\n",
        "        # Reporting the intermediate metric value to Optuna if trial is not None\n",
        "        if trial is not None:\n",
        "            trial.report(val_l1_norm, epoch)\n",
        "\n",
        "            if trial.should_prune():\n",
        "                raise optuna.TrialPruned()\n",
        "\n",
        "    # Closing the SummaryWriter object\n",
        "    writer.close()\n",
        "\n",
        "    # Returning the losses and metrics lists\n",
        "    return train_losses, val_losses, test_losses, train_metrics, val_metrics, test_metrics\n"
      ],
      "metadata": {
        "id": "3DXNF8PPXL4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmRncQPuUZiR"
      },
      "outputs": [],
      "source": [
        "# Defining an objective function for Optuna to minimize\n",
        "def objective(trial):\n",
        "    \"\"\"Defines an objective function for Optuna to minimize.\n",
        "\n",
        "    Args:\n",
        "        trial (optuna.trial.Trial): The trial object that contains the hyperparameters.\n",
        "\n",
        "    Returns:\n",
        "        float: The validation L1 norm to minimize.\n",
        "    \"\"\"\n",
        "    # Creating a trial network and optimizer using the create_model function\n",
        "    net, \\\n",
        "    loss_fn, \\\n",
        "    optimizer, \\\n",
        "    batch_size, \\\n",
        "    n_epochs, \\\n",
        "    scheduler, \\\n",
        "    loss_name, \\\n",
        "    optimizer_name, \\\n",
        "    scheduler_name, \\\n",
        "    n_units, \\\n",
        "    n_layers, \\\n",
        "    hidden_activation, \\\n",
        "    output_activation, \\\n",
        "    lr, \\\n",
        "    dropout_rate = create_model(trial, optimize=True)\n",
        "\n",
        "    # Create separate data loaders for training and validation\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True\n",
        "    )\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.TensorDataset(x_val, y_val), batch_size=batch_size\n",
        "    )\n",
        "    \n",
        "    _, _, _, _, val_metrics, _ = train_and_eval(\n",
        "        net, loss_fn, optimizer, batch_size, n_epochs, scheduler, train_loader, val_loader, None, trial\n",
        "    )\n",
        "\n",
        "    # Returning the last validation L1 norm as the objective value to minimize\n",
        "    return val_metrics[-1][\"l1_norm\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4nRZqALKaf49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2C6OKyfYgacH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmsjYi6sZyrF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cPPcXFbKfEvw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AZ5YQOPwecB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyES4NAyUZiS",
        "outputId": "0117d408-8ab9-4d8e-a397-324b601e38d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 02:36:48,307]\u001b[0m A new study created in memory with name: no-name-381fcc13-8946-48ec-a8cd-d54eee387c84\u001b[0m\n",
            "<ipython-input-77-ddc3c9d46736>:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-6, 1)\n",
            "<ipython-input-77-ddc3c9d46736>:76: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  dropout_rate = trial.suggest_uniform(\"dropout_rate\", 0.0, 0.5)\n",
            "<ipython-input-77-ddc3c9d46736>:178: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  factor = trial.suggest_uniform(\"factor\", 0.1, 0.5)\n",
            "<ipython-input-77-ddc3c9d46736>:180: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  threshold = trial.suggest_loguniform(\"threshold\", 1e-4, 1e-2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/105.. Train loss: 257623969627.328.. Val loss: 3.798.. Train L1 norm: 139930137856.206.. Val L1 norm: 5.813.. Train Linf norm: 1508127832064.914.. Val Linf norm: 123.754\n",
            "Epoch 2/105.. Train loss: 25730.059.. Val loss: 3.804.. Train L1 norm: 7301.163.. Val L1 norm: 4.884.. Train Linf norm: 233573.572.. Val Linf norm: 101.951\n",
            "Epoch 3/105.. Train loss: 6076.437.. Val loss: 3.824.. Train L1 norm: 5348.730.. Val L1 norm: 6.449.. Train Linf norm: 171095.195.. Val Linf norm: 138.457\n",
            "Epoch 4/105.. Train loss: 14614.797.. Val loss: 3.794.. Train L1 norm: 2627.463.. Val L1 norm: 5.553.. Train Linf norm: 84015.930.. Val Linf norm: 117.671\n",
            "Epoch 5/105.. Train loss: 1336.650.. Val loss: 3.984.. Train L1 norm: 407.660.. Val L1 norm: 3.077.. Train Linf norm: 12981.277.. Val Linf norm: 58.074\n",
            "Epoch 6/105.. Train loss: 1733.446.. Val loss: 3.794.. Train L1 norm: 814.258.. Val L1 norm: 5.348.. Train Linf norm: 25994.770.. Val Linf norm: 112.888\n",
            "Epoch 7/105.. Train loss: 1800.736.. Val loss: 3.866.. Train L1 norm: 2703.464.. Val L1 norm: 6.988.. Train Linf norm: 86449.348.. Val Linf norm: 150.830\n",
            "Epoch 8/105.. Train loss: 5.807.. Val loss: 3.823.. Train L1 norm: 51.130.. Val L1 norm: 6.428.. Train Linf norm: 1575.424.. Val Linf norm: 137.980\n",
            "Epoch 9/105.. Train loss: 1451.169.. Val loss: 3.823.. Train L1 norm: 333.878.. Val L1 norm: 4.498.. Train Linf norm: 10623.789.. Val Linf norm: 92.759\n",
            "Epoch 10/105.. Train loss: 1871.958.. Val loss: 3.814.. Train L1 norm: 572.199.. Val L1 norm: 4.669.. Train Linf norm: 18250.214.. Val Linf norm: 96.831\n",
            "Epoch 11/105.. Train loss: 453.911.. Val loss: 3.835.. Train L1 norm: 408.850.. Val L1 norm: 6.614.. Train Linf norm: 13023.100.. Val Linf norm: 142.252\n",
            "Epoch 12/105.. Train loss: 3966.871.. Val loss: 3.807.. Train L1 norm: 1226.382.. Val L1 norm: 6.123.. Train Linf norm: 39183.459.. Val Linf norm: 130.924\n",
            "Epoch 13/105.. Train loss: 3764.147.. Val loss: 3.825.. Train L1 norm: 840.039.. Val L1 norm: 6.460.. Train Linf norm: 26821.447.. Val Linf norm: 138.709\n",
            "Epoch 14/105.. Train loss: 168.715.. Val loss: 3.794.. Train L1 norm: 101.896.. Val L1 norm: 5.359.. Train Linf norm: 3202.419.. Val Linf norm: 113.150\n",
            "Epoch 15/105.. Train loss: 176.275.. Val loss: 3.795.. Train L1 norm: 111.409.. Val L1 norm: 5.636.. Train Linf norm: 3506.416.. Val Linf norm: 119.615\n",
            "Epoch 16/105.. Train loss: 522.948.. Val loss: 3.808.. Train L1 norm: 191.475.. Val L1 norm: 6.133.. Train Linf norm: 6068.782.. Val Linf norm: 131.159\n",
            "Epoch 17/105.. Train loss: 4.562.. Val loss: 3.794.. Train L1 norm: 95.592.. Val L1 norm: 5.515.. Train Linf norm: 2999.509.. Val Linf norm: 116.793\n",
            "Epoch 18/105.. Train loss: 261.392.. Val loss: 3.800.. Train L1 norm: 147.416.. Val L1 norm: 5.898.. Train Linf norm: 4658.569.. Val Linf norm: 125.720\n",
            "Epoch 19/105.. Train loss: 739.652.. Val loss: 3.804.. Train L1 norm: 668.392.. Val L1 norm: 6.026.. Train Linf norm: 21329.539.. Val Linf norm: 128.686\n",
            "Epoch 20/105.. Train loss: 2554.580.. Val loss: 3.797.. Train L1 norm: 257.504.. Val L1 norm: 5.157.. Train Linf norm: 8181.057.. Val Linf norm: 108.398\n",
            "Epoch 21/105.. Train loss: 498.492.. Val loss: 3.802.. Train L1 norm: 181.222.. Val L1 norm: 5.970.. Train Linf norm: 5739.613.. Val Linf norm: 127.382\n",
            "Epoch 22/105.. Train loss: 1452.648.. Val loss: 3.794.. Train L1 norm: 2230.143.. Val L1 norm: 5.368.. Train Linf norm: 71306.392.. Val Linf norm: 113.354\n",
            "Epoch 23/105.. Train loss: 3711.077.. Val loss: 3.811.. Train L1 norm: 271.231.. Val L1 norm: 6.198.. Train Linf norm: 8620.364.. Val Linf norm: 132.675\n",
            "Epoch 24/105.. Train loss: 221.880.. Val loss: 3.800.. Train L1 norm: 186.705.. Val L1 norm: 5.012.. Train Linf norm: 5916.017.. Val Linf norm: 104.974\n",
            "Epoch 25/105.. Train loss: 4.037.. Val loss: 3.794.. Train L1 norm: 53.476.. Val L1 norm: 5.345.. Train Linf norm: 1653.692.. Val Linf norm: 112.810\n",
            "Epoch 26/105.. Train loss: 1298.554.. Val loss: 3.794.. Train L1 norm: 542.962.. Val L1 norm: 5.359.. Train Linf norm: 17315.968.. Val Linf norm: 113.141\n",
            "Epoch 27/105.. Train loss: 1779.458.. Val loss: 3.794.. Train L1 norm: 636.479.. Val L1 norm: 5.601.. Train Linf norm: 20308.986.. Val Linf norm: 118.798\n",
            "Epoch 28/105.. Train loss: 175.127.. Val loss: 3.794.. Train L1 norm: 255.412.. Val L1 norm: 5.438.. Train Linf norm: 8113.717.. Val Linf norm: 114.998\n",
            "Epoch 29/105.. Train loss: 239.169.. Val loss: 3.794.. Train L1 norm: 71.898.. Val L1 norm: 5.400.. Train Linf norm: 2241.259.. Val Linf norm: 114.102\n",
            "Epoch 30/105.. Train loss: 346.647.. Val loss: 3.801.. Train L1 norm: 417.800.. Val L1 norm: 5.949.. Train Linf norm: 13311.171.. Val Linf norm: 126.893\n",
            "Epoch 31/105.. Train loss: 1077.559.. Val loss: 3.796.. Train L1 norm: 637.001.. Val L1 norm: 5.168.. Train Linf norm: 20325.991.. Val Linf norm: 108.655\n",
            "Epoch 32/105.. Train loss: 197.112.. Val loss: 3.794.. Train L1 norm: 112.818.. Val L1 norm: 5.557.. Train Linf norm: 3549.125.. Val Linf norm: 117.783\n",
            "Epoch 33/105.. Train loss: 164.149.. Val loss: 3.794.. Train L1 norm: 511.781.. Val L1 norm: 5.401.. Train Linf norm: 16318.863.. Val Linf norm: 114.115\n",
            "Epoch 34/105.. Train loss: 494.882.. Val loss: 3.795.. Train L1 norm: 213.893.. Val L1 norm: 5.675.. Train Linf norm: 6785.114.. Val Linf norm: 120.525\n",
            "Epoch 35/105.. Train loss: 1787.348.. Val loss: 3.794.. Train L1 norm: 359.965.. Val L1 norm: 5.580.. Train Linf norm: 11459.643.. Val Linf norm: 118.305\n",
            "Epoch 36/105.. Train loss: 2438.055.. Val loss: 3.794.. Train L1 norm: 2068.681.. Val L1 norm: 5.590.. Train Linf norm: 66138.444.. Val Linf norm: 118.536\n",
            "Epoch 37/105.. Train loss: 4.025.. Val loss: 3.794.. Train L1 norm: 33.357.. Val L1 norm: 5.587.. Train Linf norm: 1008.658.. Val Linf norm: 118.477\n",
            "Epoch 38/105.. Train loss: 4.014.. Val loss: 3.795.. Train L1 norm: 57.559.. Val L1 norm: 5.633.. Train Linf norm: 1782.552.. Val Linf norm: 119.541\n",
            "Epoch 39/105.. Train loss: 172.070.. Val loss: 3.794.. Train L1 norm: 126.275.. Val L1 norm: 5.389.. Train Linf norm: 3981.872.. Val Linf norm: 113.846\n",
            "Epoch 40/105.. Train loss: 4.020.. Val loss: 3.794.. Train L1 norm: 83.381.. Val L1 norm: 5.562.. Train Linf norm: 2610.566.. Val Linf norm: 117.884\n",
            "Epoch 41/105.. Train loss: 488.370.. Val loss: 3.794.. Train L1 norm: 488.738.. Val L1 norm: 5.468.. Train Linf norm: 15580.858.. Val Linf norm: 115.702\n",
            "Epoch 42/105.. Train loss: 18.724.. Val loss: 3.794.. Train L1 norm: 111.913.. Val L1 norm: 5.480.. Train Linf norm: 3521.764.. Val Linf norm: 115.984\n",
            "Epoch 43/105.. Train loss: 272.144.. Val loss: 3.794.. Train L1 norm: 103.021.. Val L1 norm: 5.356.. Train Linf norm: 3238.591.. Val Linf norm: 113.066\n",
            "Epoch 44/105.. Train loss: 54.892.. Val loss: 3.794.. Train L1 norm: 54.209.. Val L1 norm: 5.507.. Train Linf norm: 1676.922.. Val Linf norm: 116.618\n",
            "Epoch 45/105.. Train loss: 127.570.. Val loss: 3.794.. Train L1 norm: 104.858.. Val L1 norm: 5.467.. Train Linf norm: 3296.374.. Val Linf norm: 115.681\n",
            "Epoch 46/105.. Train loss: 499.630.. Val loss: 3.794.. Train L1 norm: 157.890.. Val L1 norm: 5.459.. Train Linf norm: 4993.676.. Val Linf norm: 115.480\n",
            "Epoch 47/105.. Train loss: 37.413.. Val loss: 3.794.. Train L1 norm: 17.681.. Val L1 norm: 5.492.. Train Linf norm: 507.882.. Val Linf norm: 116.258\n",
            "Epoch 48/105.. Train loss: 53.833.. Val loss: 3.794.. Train L1 norm: 110.206.. Val L1 norm: 5.485.. Train Linf norm: 3467.951.. Val Linf norm: 116.088\n",
            "Epoch 49/105.. Train loss: 21.379.. Val loss: 3.794.. Train L1 norm: 82.446.. Val L1 norm: 5.513.. Train Linf norm: 2579.391.. Val Linf norm: 116.741\n",
            "Epoch 50/105.. Train loss: 3562.072.. Val loss: 3.794.. Train L1 norm: 1447.550.. Val L1 norm: 5.437.. Train Linf norm: 46262.602.. Val Linf norm: 114.964\n",
            "Epoch 51/105.. Train loss: 5632.804.. Val loss: 3.794.. Train L1 norm: 2647.946.. Val L1 norm: 5.474.. Train Linf norm: 84675.849.. Val Linf norm: 115.842\n",
            "Epoch 52/105.. Train loss: 4.014.. Val loss: 3.794.. Train L1 norm: 31.345.. Val L1 norm: 5.474.. Train Linf norm: 944.634.. Val Linf norm: 115.823\n",
            "Epoch 53/105.. Train loss: 59.441.. Val loss: 3.794.. Train L1 norm: 116.173.. Val L1 norm: 5.469.. Train Linf norm: 3659.988.. Val Linf norm: 115.717\n",
            "Epoch 54/105.. Train loss: 4.016.. Val loss: 3.794.. Train L1 norm: 74.408.. Val L1 norm: 5.486.. Train Linf norm: 2322.771.. Val Linf norm: 116.123\n",
            "Epoch 55/105.. Train loss: 2128.155.. Val loss: 3.794.. Train L1 norm: 991.457.. Val L1 norm: 5.489.. Train Linf norm: 31668.408.. Val Linf norm: 116.181\n",
            "Epoch 56/105.. Train loss: 2450.959.. Val loss: 3.794.. Train L1 norm: 503.045.. Val L1 norm: 5.468.. Train Linf norm: 16039.017.. Val Linf norm: 115.688\n",
            "Epoch 57/105.. Train loss: 1360.535.. Val loss: 3.794.. Train L1 norm: 2898.626.. Val L1 norm: 5.474.. Train Linf norm: 92697.023.. Val Linf norm: 115.828\n",
            "Epoch 58/105.. Train loss: 4.018.. Val loss: 3.794.. Train L1 norm: 21.132.. Val L1 norm: 5.475.. Train Linf norm: 617.274.. Val Linf norm: 115.848\n",
            "Epoch 59/105.. Train loss: 4.022.. Val loss: 3.794.. Train L1 norm: 73.609.. Val L1 norm: 5.472.. Train Linf norm: 2297.197.. Val Linf norm: 115.799\n",
            "Epoch 60/105.. Train loss: 345.264.. Val loss: 3.794.. Train L1 norm: 61.790.. Val L1 norm: 5.484.. Train Linf norm: 1918.257.. Val Linf norm: 116.075\n",
            "Epoch 61/105.. Train loss: 68.396.. Val loss: 3.794.. Train L1 norm: 99.924.. Val L1 norm: 5.470.. Train Linf norm: 3138.650.. Val Linf norm: 115.742\n",
            "Epoch 62/105.. Train loss: 5892.302.. Val loss: 3.794.. Train L1 norm: 1072.778.. Val L1 norm: 5.467.. Train Linf norm: 34271.144.. Val Linf norm: 115.669\n",
            "Epoch 63/105.. Train loss: 1186.742.. Val loss: 3.794.. Train L1 norm: 1409.777.. Val L1 norm: 5.473.. Train Linf norm: 45055.063.. Val Linf norm: 115.820\n",
            "Epoch 64/105.. Train loss: 147.571.. Val loss: 3.794.. Train L1 norm: 65.483.. Val L1 norm: 5.487.. Train Linf norm: 2036.454.. Val Linf norm: 116.138\n",
            "Epoch 65/105.. Train loss: 104.852.. Val loss: 3.794.. Train L1 norm: 133.965.. Val L1 norm: 5.476.. Train Linf norm: 4228.143.. Val Linf norm: 115.881\n",
            "Epoch 66/105.. Train loss: 23.095.. Val loss: 3.794.. Train L1 norm: 129.257.. Val L1 norm: 5.473.. Train Linf norm: 4077.673.. Val Linf norm: 115.817\n",
            "Epoch 67/105.. Train loss: 4.017.. Val loss: 3.794.. Train L1 norm: 48.879.. Val L1 norm: 5.481.. Train Linf norm: 1506.436.. Val Linf norm: 116.002\n",
            "Epoch 68/105.. Train loss: 4.024.. Val loss: 3.794.. Train L1 norm: 69.690.. Val L1 norm: 5.476.. Train Linf norm: 2171.644.. Val Linf norm: 115.874\n",
            "Epoch 69/105.. Train loss: 53.609.. Val loss: 3.794.. Train L1 norm: 57.375.. Val L1 norm: 5.482.. Train Linf norm: 1777.838.. Val Linf norm: 116.017\n",
            "Epoch 70/105.. Train loss: 54.481.. Val loss: 3.794.. Train L1 norm: 35.831.. Val L1 norm: 5.485.. Train Linf norm: 1088.047.. Val Linf norm: 116.081\n",
            "Epoch 71/105.. Train loss: 230.071.. Val loss: 3.794.. Train L1 norm: 95.713.. Val L1 norm: 5.480.. Train Linf norm: 3004.375.. Val Linf norm: 115.973\n",
            "Epoch 72/105.. Train loss: 11.861.. Val loss: 3.794.. Train L1 norm: 34.881.. Val L1 norm: 5.475.. Train Linf norm: 1058.602.. Val Linf norm: 115.855\n",
            "Epoch 73/105.. Train loss: 24.260.. Val loss: 3.794.. Train L1 norm: 29.200.. Val L1 norm: 5.469.. Train Linf norm: 876.199.. Val Linf norm: 115.718\n",
            "Epoch 74/105.. Train loss: 2491.467.. Val loss: 3.794.. Train L1 norm: 10372.420.. Val L1 norm: 5.471.. Train Linf norm: 331858.939.. Val Linf norm: 115.764\n",
            "Epoch 75/105.. Train loss: 549.151.. Val loss: 3.794.. Train L1 norm: 130.079.. Val L1 norm: 5.469.. Train Linf norm: 4103.482.. Val Linf norm: 115.706\n",
            "Epoch 76/105.. Train loss: 4.025.. Val loss: 3.794.. Train L1 norm: 46.311.. Val L1 norm: 5.468.. Train Linf norm: 1424.098.. Val Linf norm: 115.704\n",
            "Epoch 77/105.. Train loss: 4.022.. Val loss: 3.794.. Train L1 norm: 69.055.. Val L1 norm: 5.470.. Train Linf norm: 2150.919.. Val Linf norm: 115.745\n",
            "Epoch 78/105.. Train loss: 86.532.. Val loss: 3.794.. Train L1 norm: 71.087.. Val L1 norm: 5.472.. Train Linf norm: 2217.484.. Val Linf norm: 115.791\n",
            "Epoch 79/105.. Train loss: 4.018.. Val loss: 3.794.. Train L1 norm: 98.028.. Val L1 norm: 5.471.. Train Linf norm: 3078.454.. Val Linf norm: 115.775\n",
            "Epoch 80/105.. Train loss: 149.401.. Val loss: 3.794.. Train L1 norm: 156.116.. Val L1 norm: 5.470.. Train Linf norm: 4936.622.. Val Linf norm: 115.748\n",
            "Epoch 81/105.. Train loss: 12.325.. Val loss: 3.794.. Train L1 norm: 72.872.. Val L1 norm: 5.469.. Train Linf norm: 2273.239.. Val Linf norm: 115.724\n",
            "Epoch 82/105.. Train loss: 16.647.. Val loss: 3.794.. Train L1 norm: 59.941.. Val L1 norm: 5.471.. Train Linf norm: 1859.174.. Val Linf norm: 115.762\n",
            "Epoch 83/105.. Train loss: 628.689.. Val loss: 3.794.. Train L1 norm: 465.792.. Val L1 norm: 5.473.. Train Linf norm: 14846.934.. Val Linf norm: 115.806\n",
            "Epoch 84/105.. Train loss: 111.111.. Val loss: 3.794.. Train L1 norm: 138.840.. Val L1 norm: 5.473.. Train Linf norm: 4384.275.. Val Linf norm: 115.822\n",
            "Epoch 85/105.. Train loss: 4217.523.. Val loss: 3.794.. Train L1 norm: 651.270.. Val L1 norm: 5.476.. Train Linf norm: 20781.870.. Val Linf norm: 115.878\n",
            "Epoch 86/105.. Train loss: 13850.073.. Val loss: 3.794.. Train L1 norm: 2123.642.. Val L1 norm: 5.475.. Train Linf norm: 67898.372.. Val Linf norm: 115.869\n",
            "Epoch 87/105.. Train loss: 172.659.. Val loss: 3.794.. Train L1 norm: 466.349.. Val L1 norm: 5.475.. Train Linf norm: 14864.364.. Val Linf norm: 115.861\n",
            "Epoch 88/105.. Train loss: 2920.455.. Val loss: 3.794.. Train L1 norm: 557.130.. Val L1 norm: 5.476.. Train Linf norm: 17769.401.. Val Linf norm: 115.882\n",
            "Epoch 89/105.. Train loss: 1563.153.. Val loss: 3.794.. Train L1 norm: 404.462.. Val L1 norm: 5.475.. Train Linf norm: 12884.769.. Val Linf norm: 115.867\n",
            "Epoch 90/105.. Train loss: 350.351.. Val loss: 3.794.. Train L1 norm: 68.671.. Val L1 norm: 5.475.. Train Linf norm: 2138.175.. Val Linf norm: 115.857\n",
            "Epoch 91/105.. Train loss: 4.020.. Val loss: 3.794.. Train L1 norm: 16.698.. Val L1 norm: 5.474.. Train Linf norm: 474.817.. Val Linf norm: 115.845\n",
            "Epoch 92/105.. Train loss: 55.114.. Val loss: 3.794.. Train L1 norm: 38.236.. Val L1 norm: 5.475.. Train Linf norm: 1165.043.. Val Linf norm: 115.851\n",
            "Epoch 93/105.. Train loss: 4.025.. Val loss: 3.794.. Train L1 norm: 43.638.. Val L1 norm: 5.475.. Train Linf norm: 1338.205.. Val Linf norm: 115.853\n",
            "Epoch 94/105.. Train loss: 827.661.. Val loss: 3.794.. Train L1 norm: 375.943.. Val L1 norm: 5.475.. Train Linf norm: 11970.933.. Val Linf norm: 115.849\n",
            "Epoch 95/105.. Train loss: 66.094.. Val loss: 3.794.. Train L1 norm: 123.236.. Val L1 norm: 5.475.. Train Linf norm: 3885.031.. Val Linf norm: 115.852\n",
            "Epoch 96/105.. Train loss: 15.011.. Val loss: 3.794.. Train L1 norm: 72.081.. Val L1 norm: 5.475.. Train Linf norm: 2248.310.. Val Linf norm: 115.861\n",
            "Epoch 97/105.. Train loss: 4.016.. Val loss: 3.794.. Train L1 norm: 41.222.. Val L1 norm: 5.475.. Train Linf norm: 1261.040.. Val Linf norm: 115.865\n",
            "Epoch 98/105.. Train loss: 4.012.. Val loss: 3.794.. Train L1 norm: 44.136.. Val L1 norm: 5.476.. Train Linf norm: 1354.315.. Val Linf norm: 115.870\n",
            "Epoch 99/105.. Train loss: 4.014.. Val loss: 3.794.. Train L1 norm: 38.272.. Val L1 norm: 5.476.. Train Linf norm: 1166.206.. Val Linf norm: 115.872\n",
            "Epoch 100/105.. Train loss: 4.020.. Val loss: 3.794.. Train L1 norm: 41.584.. Val L1 norm: 5.476.. Train Linf norm: 1272.025.. Val Linf norm: 115.872\n",
            "Epoch 101/105.. Train loss: 155.865.. Val loss: 3.794.. Train L1 norm: 91.685.. Val L1 norm: 5.476.. Train Linf norm: 2874.596.. Val Linf norm: 115.871\n",
            "Epoch 102/105.. Train loss: 6.800.. Val loss: 3.794.. Train L1 norm: 72.670.. Val L1 norm: 5.476.. Train Linf norm: 2266.750.. Val Linf norm: 115.872\n",
            "Epoch 103/105.. Train loss: 2900.866.. Val loss: 3.794.. Train L1 norm: 16038.523.. Val L1 norm: 5.476.. Train Linf norm: 513174.324.. Val Linf norm: 115.871\n",
            "Epoch 104/105.. Train loss: 1091.305.. Val loss: 3.794.. Train L1 norm: 766.340.. Val L1 norm: 5.476.. Train Linf norm: 24463.958.. Val Linf norm: 115.870\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 03:13:52,750]\u001b[0m Trial 0 finished with value: 5.475563034884135 and parameters: {'n_layers': 5, 'n_units_0': 1810, 'n_units_1': 2471, 'n_units_2': 3863, 'n_units_3': 847, 'n_units_4': 3837, 'hidden_activation': 'Swish', 'output_activation': 'Linear', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.5048353295116449, 'batch_size': 32, 'n_epochs': 105, 'scheduler': 'ReduceLROnPlateau', 'dropout_rate': 0.1488418576403419, 'factor': 0.4948638775131383, 'patience': 5, 'threshold': 0.007132405166355046}. Best is trial 0 with value: 5.475563034884135.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 105/105.. Train loss: 1673.194.. Val loss: 3.794.. Train L1 norm: 600.333.. Val L1 norm: 5.476.. Train Linf norm: 19152.235.. Val Linf norm: 115.871\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-77-ddc3c9d46736>:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-6, 1)\n",
            "<ipython-input-77-ddc3c9d46736>:73: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  beta = trial.suggest_uniform(\"softplus_beta\", 0.5, 1.5)\n",
            "<ipython-input-77-ddc3c9d46736>:76: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  dropout_rate = trial.suggest_uniform(\"dropout_rate\", 0.0, 0.5)\n",
            "<ipython-input-77-ddc3c9d46736>:178: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  factor = trial.suggest_uniform(\"factor\", 0.1, 0.5)\n",
            "<ipython-input-77-ddc3c9d46736>:180: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  threshold = trial.suggest_loguniform(\"threshold\", 1e-4, 1e-2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/99.. Train loss: 5113708.906.. Val loss: 2.057.. Train L1 norm: 20499684.534.. Val L1 norm: 4.433.. Train Linf norm: 1256310831.781.. Val Linf norm: 301.628\n",
            "Epoch 2/99.. Train loss: 2.344.. Val loss: 2.283.. Train L1 norm: 39.399.. Val L1 norm: 4.498.. Train Linf norm: 4735.839.. Val Linf norm: 313.586\n",
            "Epoch 3/99.. Train loss: 3.221.. Val loss: 2.118.. Train L1 norm: 51.418.. Val L1 norm: 4.665.. Train Linf norm: 6164.097.. Val Linf norm: 325.148\n",
            "Epoch 4/99.. Train loss: 2.604.. Val loss: 2.135.. Train L1 norm: 49.712.. Val L1 norm: 5.382.. Train Linf norm: 6001.885.. Val Linf norm: 382.025\n",
            "Epoch 5/99.. Train loss: 2.296.. Val loss: 2.135.. Train L1 norm: 56.312.. Val L1 norm: 5.504.. Train Linf norm: 6880.011.. Val Linf norm: 391.677\n",
            "Epoch 6/99.. Train loss: 2.889.. Val loss: 2.135.. Train L1 norm: 56.180.. Val L1 norm: 5.361.. Train Linf norm: 6824.222.. Val Linf norm: 380.409\n",
            "Epoch 7/99.. Train loss: 2.408.. Val loss: 2.135.. Train L1 norm: 56.628.. Val L1 norm: 5.467.. Train Linf norm: 6915.259.. Val Linf norm: 388.734\n",
            "Epoch 8/99.. Train loss: 2.745.. Val loss: 2.135.. Train L1 norm: 57.149.. Val L1 norm: 5.419.. Train Linf norm: 6926.942.. Val Linf norm: 384.986\n",
            "Epoch 9/99.. Train loss: 2.142.. Val loss: 2.135.. Train L1 norm: 55.988.. Val L1 norm: 5.502.. Train Linf norm: 6853.025.. Val Linf norm: 391.510\n",
            "Epoch 10/99.. Train loss: 2.142.. Val loss: 2.135.. Train L1 norm: 57.102.. Val L1 norm: 5.490.. Train Linf norm: 6996.938.. Val Linf norm: 390.561\n",
            "Epoch 11/99.. Train loss: 2.142.. Val loss: 2.135.. Train L1 norm: 57.102.. Val L1 norm: 5.523.. Train Linf norm: 6995.487.. Val Linf norm: 393.134\n",
            "Epoch 12/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 57.481.. Val L1 norm: 5.507.. Train Linf norm: 7040.472.. Val Linf norm: 391.875\n",
            "Epoch 13/99.. Train loss: 2.142.. Val loss: 2.135.. Train L1 norm: 57.833.. Val L1 norm: 5.480.. Train Linf norm: 7090.870.. Val Linf norm: 389.813\n",
            "Epoch 14/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 57.187.. Val L1 norm: 5.497.. Train Linf norm: 7002.509.. Val Linf norm: 391.120\n",
            "Epoch 15/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.759.. Val L1 norm: 5.460.. Train Linf norm: 6947.590.. Val Linf norm: 388.193\n",
            "Epoch 16/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.762.. Val L1 norm: 5.464.. Train Linf norm: 6941.900.. Val Linf norm: 388.526\n",
            "Epoch 17/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.806.. Val L1 norm: 5.466.. Train Linf norm: 6955.847.. Val Linf norm: 388.700\n",
            "Epoch 18/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 57.007.. Val L1 norm: 5.471.. Train Linf norm: 6977.601.. Val Linf norm: 389.074\n",
            "Epoch 19/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.537.. Val L1 norm: 5.481.. Train Linf norm: 6920.986.. Val Linf norm: 389.888\n",
            "Epoch 20/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.864.. Val L1 norm: 5.474.. Train Linf norm: 6960.388.. Val Linf norm: 389.322\n",
            "Epoch 21/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.892.. Val L1 norm: 5.472.. Train Linf norm: 6969.816.. Val Linf norm: 389.126\n",
            "Epoch 22/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 57.049.. Val L1 norm: 5.462.. Train Linf norm: 6990.970.. Val Linf norm: 388.375\n",
            "Epoch 23/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.747.. Val L1 norm: 5.465.. Train Linf norm: 6951.178.. Val Linf norm: 388.574\n",
            "Epoch 24/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.686.. Val L1 norm: 5.467.. Train Linf norm: 6944.075.. Val Linf norm: 388.755\n",
            "Epoch 25/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.738.. Val L1 norm: 5.469.. Train Linf norm: 6946.720.. Val Linf norm: 388.893\n",
            "Epoch 26/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.822.. Val L1 norm: 5.469.. Train Linf norm: 6960.350.. Val Linf norm: 388.932\n",
            "Epoch 27/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.838.. Val L1 norm: 5.470.. Train Linf norm: 6955.339.. Val Linf norm: 388.999\n",
            "Epoch 28/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.758.. Val L1 norm: 5.472.. Train Linf norm: 6952.540.. Val Linf norm: 389.154\n",
            "Epoch 29/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.821.. Val L1 norm: 5.472.. Train Linf norm: 6961.042.. Val Linf norm: 389.154\n",
            "Epoch 30/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.819.. Val L1 norm: 5.472.. Train Linf norm: 6962.823.. Val Linf norm: 389.155\n",
            "Epoch 31/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.840.. Val L1 norm: 5.472.. Train Linf norm: 6962.921.. Val Linf norm: 389.172\n",
            "Epoch 32/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.472.. Train Linf norm: 6958.585.. Val Linf norm: 389.174\n",
            "Epoch 33/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.819.. Val L1 norm: 5.473.. Train Linf norm: 6953.308.. Val Linf norm: 389.192\n",
            "Epoch 34/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.858.. Val L1 norm: 5.473.. Train Linf norm: 6965.984.. Val Linf norm: 389.193\n",
            "Epoch 35/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6960.501.. Val Linf norm: 389.199\n",
            "Epoch 36/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.811.. Val L1 norm: 5.473.. Train Linf norm: 6959.097.. Val Linf norm: 389.228\n",
            "Epoch 37/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.826.. Val L1 norm: 5.473.. Train Linf norm: 6964.613.. Val Linf norm: 389.228\n",
            "Epoch 38/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.835.. Val L1 norm: 5.473.. Train Linf norm: 6135.049.. Val Linf norm: 389.238\n",
            "Epoch 39/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.838.. Val L1 norm: 5.473.. Train Linf norm: 6953.680.. Val Linf norm: 389.238\n",
            "Epoch 40/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.836.. Val L1 norm: 5.473.. Train Linf norm: 6962.307.. Val Linf norm: 389.238\n",
            "Epoch 41/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.838.. Val L1 norm: 5.473.. Train Linf norm: 6966.377.. Val Linf norm: 389.244\n",
            "Epoch 42/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.837.. Val L1 norm: 5.473.. Train Linf norm: 6957.356.. Val Linf norm: 389.249\n",
            "Epoch 43/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.843.. Val L1 norm: 5.473.. Train Linf norm: 6962.626.. Val Linf norm: 389.252\n",
            "Epoch 44/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.838.. Val L1 norm: 5.473.. Train Linf norm: 6966.661.. Val Linf norm: 389.254\n",
            "Epoch 45/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.838.. Val L1 norm: 5.473.. Train Linf norm: 6961.802.. Val Linf norm: 389.255\n",
            "Epoch 46/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6957.279.. Val Linf norm: 389.255\n",
            "Epoch 47/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6954.879.. Val Linf norm: 389.256\n",
            "Epoch 48/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6967.557.. Val Linf norm: 389.258\n",
            "Epoch 49/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.838.. Val L1 norm: 5.473.. Train Linf norm: 6959.463.. Val Linf norm: 389.259\n",
            "Epoch 50/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.838.. Val L1 norm: 5.473.. Train Linf norm: 6959.924.. Val Linf norm: 389.260\n",
            "Epoch 51/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6960.110.. Val Linf norm: 389.260\n",
            "Epoch 52/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6964.121.. Val Linf norm: 389.260\n",
            "Epoch 53/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.838.. Val L1 norm: 5.473.. Train Linf norm: 6960.839.. Val Linf norm: 389.260\n",
            "Epoch 54/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6961.002.. Val Linf norm: 389.260\n",
            "Epoch 55/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.840.. Val L1 norm: 5.473.. Train Linf norm: 6967.541.. Val Linf norm: 389.260\n",
            "Epoch 56/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6964.668.. Val Linf norm: 389.260\n",
            "Epoch 57/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6956.075.. Val Linf norm: 389.260\n",
            "Epoch 58/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6963.184.. Val Linf norm: 389.261\n",
            "Epoch 59/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6962.266.. Val Linf norm: 389.261\n",
            "Epoch 60/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6963.253.. Val Linf norm: 389.261\n",
            "Epoch 61/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6957.569.. Val Linf norm: 389.261\n",
            "Epoch 62/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6960.723.. Val Linf norm: 389.261\n",
            "Epoch 63/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6959.887.. Val Linf norm: 389.261\n",
            "Epoch 64/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6960.972.. Val Linf norm: 389.261\n",
            "Epoch 65/99.. Train loss: 2.142.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6963.551.. Val Linf norm: 389.261\n",
            "Epoch 66/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6964.840.. Val Linf norm: 389.261\n",
            "Epoch 67/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6960.320.. Val Linf norm: 389.261\n",
            "Epoch 68/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6962.311.. Val Linf norm: 389.261\n",
            "Epoch 69/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6957.195.. Val Linf norm: 389.261\n",
            "Epoch 70/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6963.106.. Val Linf norm: 389.261\n",
            "Epoch 71/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6962.007.. Val Linf norm: 389.261\n",
            "Epoch 72/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6964.704.. Val Linf norm: 389.261\n",
            "Epoch 73/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6963.977.. Val Linf norm: 389.261\n",
            "Epoch 74/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6960.803.. Val Linf norm: 389.261\n",
            "Epoch 75/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6964.693.. Val Linf norm: 389.261\n",
            "Epoch 76/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6965.327.. Val Linf norm: 389.261\n",
            "Epoch 77/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6958.620.. Val Linf norm: 389.261\n",
            "Epoch 78/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6963.489.. Val Linf norm: 389.261\n",
            "Epoch 79/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6960.035.. Val Linf norm: 389.261\n",
            "Epoch 80/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6964.801.. Val Linf norm: 389.261\n",
            "Epoch 81/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6964.497.. Val Linf norm: 389.261\n",
            "Epoch 82/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6964.445.. Val Linf norm: 389.261\n",
            "Epoch 83/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6959.131.. Val Linf norm: 389.261\n",
            "Epoch 84/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6958.875.. Val Linf norm: 389.261\n",
            "Epoch 85/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6964.589.. Val Linf norm: 389.261\n",
            "Epoch 86/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6960.520.. Val Linf norm: 389.261\n",
            "Epoch 87/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6964.346.. Val Linf norm: 389.261\n",
            "Epoch 88/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6960.163.. Val Linf norm: 389.261\n",
            "Epoch 89/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6961.674.. Val Linf norm: 389.261\n",
            "Epoch 90/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6964.464.. Val Linf norm: 389.261\n",
            "Epoch 91/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6962.705.. Val Linf norm: 389.261\n",
            "Epoch 92/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6958.048.. Val Linf norm: 389.261\n",
            "Epoch 93/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6965.160.. Val Linf norm: 389.261\n",
            "Epoch 94/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6959.841.. Val Linf norm: 389.261\n",
            "Epoch 95/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6960.052.. Val Linf norm: 389.261\n",
            "Epoch 96/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6958.510.. Val Linf norm: 389.261\n",
            "Epoch 97/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6963.390.. Val Linf norm: 389.261\n",
            "Epoch 98/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6964.283.. Val Linf norm: 389.261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 03:23:06,772]\u001b[0m Trial 1 finished with value: 5.473422298431396 and parameters: {'n_layers': 4, 'n_units_0': 1461, 'n_units_1': 1257, 'n_units_2': 3865, 'n_units_3': 3100, 'hidden_activation': 'SoftPlus', 'output_activation': 'Linear', 'loss': 'Quantile', 'optimizer': 'RMSprop', 'lr': 0.013350957754286737, 'batch_size': 128, 'n_epochs': 99, 'scheduler': 'ReduceLROnPlateau', 'softplus_beta': 0.7041414244993218, 'dropout_rate': 0.24711369656353527, 'factor': 0.28072566444985225, 'patience': 6, 'threshold': 0.0003643445534342737}. Best is trial 1 with value: 5.473422298431396.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 99/99.. Train loss: 2.141.. Val loss: 2.135.. Train L1 norm: 56.839.. Val L1 norm: 5.473.. Train Linf norm: 6961.839.. Val Linf norm: 389.261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-77-ddc3c9d46736>:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-6, 1)\n",
            "<ipython-input-77-ddc3c9d46736>:58: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  negative_slope = trial.suggest_uniform(\"leakyrelu_slope\", 0.01, 0.3)\n",
            "<ipython-input-77-ddc3c9d46736>:76: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  dropout_rate = trial.suggest_uniform(\"dropout_rate\", 0.0, 0.5)\n",
            "<ipython-input-77-ddc3c9d46736>:145: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
            "<ipython-input-77-ddc3c9d46736>:146: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  momentum = trial.suggest_uniform(\"momentum\", 0.0, 0.99)\n",
            "<ipython-input-77-ddc3c9d46736>:168: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  t_max_fraction = trial.suggest_uniform('t_max_fraction', 0.1, 0.3)\n",
            "<ipython-input-77-ddc3c9d46736>:175: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  eta_min = trial.suggest_loguniform(\"eta_min\", 1e-7, 1e-2)\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 51/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "\u001b[33m[W 2023-05-29 03:28:15,758]\u001b[0m Trial 2 failed with parameters: {'n_layers': 8, 'n_units_0': 3580, 'n_units_1': 1309, 'n_units_2': 44, 'n_units_3': 3153, 'n_units_4': 3302, 'n_units_5': 528, 'n_units_6': 2008, 'n_units_7': 2731, 'hidden_activation': 'LeakyReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'SGD', 'lr': 0.000443614672043173, 'batch_size': 128, 'n_epochs': 52, 'scheduler': 'CosineAnnealingLR', 'leakyrelu_slope': 0.1998660141516426, 'dropout_rate': 0.18536913197790994, 'weight_decay': 0.0004033003754287748, 'momentum': 0.46836209291099223, 't_max_fraction': 0.13849952781161015, 'eta_min': 7.526976524655028e-07} because of the following error: The value nan is not acceptable..\u001b[0m\n",
            "\u001b[33m[W 2023-05-29 03:28:15,759]\u001b[0m Trial 2 failed with value nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 52/52.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-77-ddc3c9d46736>:178: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  factor = trial.suggest_uniform(\"factor\", 0.1, 0.5)\n",
            "<ipython-input-77-ddc3c9d46736>:180: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  threshold = trial.suggest_loguniform(\"threshold\", 1e-4, 1e-2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/111.. Train loss: 1073959011.306.. Val loss: 128.138.. Train L1 norm: 276.488.. Val L1 norm: 2.244.. Train Linf norm: 4557.800.. Val Linf norm: 36.711\n",
            "Epoch 2/111.. Train loss: 81815.299.. Val loss: 45.854.. Train L1 norm: 8.787.. Val L1 norm: 1.892.. Train Linf norm: 242.301.. Val Linf norm: 27.130\n",
            "Epoch 3/111.. Train loss: 310524.864.. Val loss: 52.459.. Train L1 norm: 48.291.. Val L1 norm: 1.399.. Train Linf norm: 1508.157.. Val Linf norm: 13.867\n",
            "Epoch 4/111.. Train loss: 34774.897.. Val loss: 46.565.. Train L1 norm: 23.054.. Val L1 norm: 1.747.. Train Linf norm: 703.349.. Val Linf norm: 23.545\n",
            "Epoch 5/111.. Train loss: 82966.816.. Val loss: 52.891.. Train L1 norm: 26.996.. Val L1 norm: 1.102.. Train Linf norm: 830.361.. Val Linf norm: 4.955\n",
            "Epoch 6/111.. Train loss: 1306.639.. Val loss: 76.768.. Train L1 norm: 25.342.. Val L1 norm: 1.202.. Train Linf norm: 776.957.. Val Linf norm: 8.100\n",
            "Epoch 7/111.. Train loss: 208687.916.. Val loss: 69.229.. Train L1 norm: 15.756.. Val L1 norm: 2.634.. Train Linf norm: 468.356.. Val Linf norm: 45.973\n",
            "Epoch 8/111.. Train loss: 24290.303.. Val loss: 46.217.. Train L1 norm: 4.095.. Val L1 norm: 2.592.. Train Linf norm: 94.551.. Val Linf norm: 44.669\n",
            "Epoch 9/111.. Train loss: 117491.413.. Val loss: 49.373.. Train L1 norm: 60.833.. Val L1 norm: 2.504.. Train Linf norm: 1910.646.. Val Linf norm: 42.848\n",
            "Epoch 10/111.. Train loss: 248122.831.. Val loss: 92.748.. Train L1 norm: 6.912.. Val L1 norm: 2.078.. Train Linf norm: 184.981.. Val Linf norm: 31.904\n",
            "Epoch 11/111.. Train loss: 137741.204.. Val loss: 53.743.. Train L1 norm: 19.695.. Val L1 norm: 1.680.. Train Linf norm: 597.322.. Val Linf norm: 21.282\n",
            "Epoch 12/111.. Train loss: 11880.084.. Val loss: 52.919.. Train L1 norm: 39.797.. Val L1 norm: 1.708.. Train Linf norm: 1239.680.. Val Linf norm: 21.872\n",
            "Epoch 13/111.. Train loss: 1448.886.. Val loss: 54.375.. Train L1 norm: 14.384.. Val L1 norm: 1.635.. Train Linf norm: 426.884.. Val Linf norm: 19.864\n",
            "Epoch 14/111.. Train loss: 14471.927.. Val loss: 57.236.. Train L1 norm: 13.942.. Val L1 norm: 1.603.. Train Linf norm: 413.386.. Val Linf norm: 19.020\n",
            "Epoch 15/111.. Train loss: 30395.018.. Val loss: 63.795.. Train L1 norm: 15.050.. Val L1 norm: 1.430.. Train Linf norm: 449.110.. Val Linf norm: 14.616\n",
            "Epoch 16/111.. Train loss: 1593.662.. Val loss: 58.277.. Train L1 norm: 8.521.. Val L1 norm: 1.467.. Train Linf norm: 240.435.. Val Linf norm: 15.625\n",
            "Epoch 17/111.. Train loss: 2599.427.. Val loss: 58.084.. Train L1 norm: 2.633.. Val L1 norm: 1.497.. Train Linf norm: 52.077.. Val Linf norm: 16.413\n",
            "Epoch 18/111.. Train loss: 467.274.. Val loss: 57.518.. Train L1 norm: 3.415.. Val L1 norm: 1.493.. Train Linf norm: 77.215.. Val Linf norm: 16.112\n",
            "Epoch 19/111.. Train loss: 8534.009.. Val loss: 57.502.. Train L1 norm: 16.161.. Val L1 norm: 1.484.. Train Linf norm: 484.645.. Val Linf norm: 15.890\n",
            "Epoch 20/111.. Train loss: 158.683.. Val loss: 57.385.. Train L1 norm: 9.452.. Val L1 norm: 1.491.. Train Linf norm: 270.071.. Val Linf norm: 16.085\n",
            "Epoch 21/111.. Train loss: 249.056.. Val loss: 57.396.. Train L1 norm: 11.820.. Val L1 norm: 1.492.. Train Linf norm: 345.906.. Val Linf norm: 16.100\n",
            "Epoch 22/111.. Train loss: 1872.761.. Val loss: 57.051.. Train L1 norm: 5.754.. Val L1 norm: 1.491.. Train Linf norm: 151.838.. Val Linf norm: 16.096\n",
            "Epoch 23/111.. Train loss: 41733.070.. Val loss: 57.608.. Train L1 norm: 5.958.. Val L1 norm: 1.480.. Train Linf norm: 158.468.. Val Linf norm: 15.824\n",
            "Epoch 24/111.. Train loss: 132.507.. Val loss: 57.305.. Train L1 norm: 13.573.. Val L1 norm: 1.480.. Train Linf norm: 402.020.. Val Linf norm: 15.798\n",
            "Epoch 25/111.. Train loss: 1451.293.. Val loss: 56.661.. Train L1 norm: 11.464.. Val L1 norm: 1.486.. Train Linf norm: 334.410.. Val Linf norm: 15.951\n",
            "Epoch 26/111.. Train loss: 239.134.. Val loss: 56.685.. Train L1 norm: 3.875.. Val L1 norm: 1.483.. Train Linf norm: 91.724.. Val Linf norm: 15.867\n",
            "Epoch 27/111.. Train loss: 140.010.. Val loss: 56.673.. Train L1 norm: 1.858.. Val L1 norm: 1.483.. Train Linf norm: 27.387.. Val Linf norm: 15.871\n",
            "Epoch 28/111.. Train loss: 713.287.. Val loss: 56.671.. Train L1 norm: 6.259.. Val L1 norm: 1.482.. Train Linf norm: 167.877.. Val Linf norm: 15.858\n",
            "Epoch 29/111.. Train loss: 81.738.. Val loss: 56.635.. Train L1 norm: 7.388.. Val L1 norm: 1.482.. Train Linf norm: 204.137.. Val Linf norm: 15.859\n",
            "Epoch 30/111.. Train loss: 18225.984.. Val loss: 56.682.. Train L1 norm: 4.128.. Val L1 norm: 1.481.. Train Linf norm: 99.919.. Val Linf norm: 15.832\n",
            "Epoch 31/111.. Train loss: 4301.613.. Val loss: 56.669.. Train L1 norm: 6.912.. Val L1 norm: 1.481.. Train Linf norm: 189.035.. Val Linf norm: 15.823\n",
            "Epoch 32/111.. Train loss: 48801.023.. Val loss: 56.748.. Train L1 norm: 6.528.. Val L1 norm: 1.479.. Train Linf norm: 176.538.. Val Linf norm: 15.770\n",
            "Epoch 33/111.. Train loss: 945.640.. Val loss: 56.729.. Train L1 norm: 3.547.. Val L1 norm: 1.479.. Train Linf norm: 81.349.. Val Linf norm: 15.770\n",
            "Epoch 34/111.. Train loss: 17634.134.. Val loss: 56.768.. Train L1 norm: 3.544.. Val L1 norm: 1.479.. Train Linf norm: 81.450.. Val Linf norm: 15.755\n",
            "Epoch 35/111.. Train loss: 17864.842.. Val loss: 56.771.. Train L1 norm: 5.295.. Val L1 norm: 1.478.. Train Linf norm: 137.202.. Val Linf norm: 15.753\n",
            "Epoch 36/111.. Train loss: 1309.939.. Val loss: 56.766.. Train L1 norm: 2.991.. Val L1 norm: 1.479.. Train Linf norm: 63.678.. Val Linf norm: 15.755\n",
            "Epoch 37/111.. Train loss: 159.854.. Val loss: 56.759.. Train L1 norm: 5.130.. Val L1 norm: 1.479.. Train Linf norm: 131.758.. Val Linf norm: 15.755\n",
            "Epoch 38/111.. Train loss: 7878.147.. Val loss: 56.760.. Train L1 norm: 12.330.. Val L1 norm: 1.478.. Train Linf norm: 362.222.. Val Linf norm: 15.754\n",
            "Epoch 39/111.. Train loss: 5566.953.. Val loss: 56.762.. Train L1 norm: 15.526.. Val L1 norm: 1.478.. Train Linf norm: 464.648.. Val Linf norm: 15.753\n",
            "Epoch 40/111.. Train loss: 4723.520.. Val loss: 56.755.. Train L1 norm: 10.665.. Val L1 norm: 1.479.. Train Linf norm: 308.991.. Val Linf norm: 15.756\n",
            "Epoch 41/111.. Train loss: 266.182.. Val loss: 56.755.. Train L1 norm: 2.024.. Val L1 norm: 1.479.. Train Linf norm: 32.670.. Val Linf norm: 15.756\n",
            "Epoch 42/111.. Train loss: 566.311.. Val loss: 56.753.. Train L1 norm: 20.613.. Val L1 norm: 1.479.. Train Linf norm: 627.434.. Val Linf norm: 15.757\n",
            "Epoch 43/111.. Train loss: 42444.914.. Val loss: 56.753.. Train L1 norm: 7.876.. Val L1 norm: 1.479.. Train Linf norm: 219.851.. Val Linf norm: 15.756\n",
            "Epoch 44/111.. Train loss: 871.104.. Val loss: 56.753.. Train L1 norm: 5.034.. Val L1 norm: 1.479.. Train Linf norm: 128.888.. Val Linf norm: 15.756\n",
            "Epoch 45/111.. Train loss: 836.951.. Val loss: 56.753.. Train L1 norm: 13.315.. Val L1 norm: 1.479.. Train Linf norm: 393.943.. Val Linf norm: 15.756\n",
            "Epoch 46/111.. Train loss: 22521.343.. Val loss: 56.753.. Train L1 norm: 4.227.. Val L1 norm: 1.479.. Train Linf norm: 103.227.. Val Linf norm: 15.756\n",
            "Epoch 47/111.. Train loss: 14877.629.. Val loss: 56.753.. Train L1 norm: 4.079.. Val L1 norm: 1.479.. Train Linf norm: 98.359.. Val Linf norm: 15.756\n",
            "Epoch 48/111.. Train loss: 73.922.. Val loss: 56.752.. Train L1 norm: 4.313.. Val L1 norm: 1.479.. Train Linf norm: 105.705.. Val Linf norm: 15.756\n",
            "Epoch 49/111.. Train loss: 8547.233.. Val loss: 56.752.. Train L1 norm: 14.219.. Val L1 norm: 1.479.. Train Linf norm: 422.871.. Val Linf norm: 15.756\n",
            "Epoch 50/111.. Train loss: 140.713.. Val loss: 56.752.. Train L1 norm: 9.177.. Val L1 norm: 1.479.. Train Linf norm: 261.461.. Val Linf norm: 15.756\n",
            "Epoch 51/111.. Train loss: 12356.567.. Val loss: 56.752.. Train L1 norm: 4.751.. Val L1 norm: 1.479.. Train Linf norm: 119.850.. Val Linf norm: 15.756\n",
            "Epoch 52/111.. Train loss: 160.668.. Val loss: 56.752.. Train L1 norm: 2.905.. Val L1 norm: 1.479.. Train Linf norm: 60.821.. Val Linf norm: 15.756\n",
            "Epoch 53/111.. Train loss: 1575.936.. Val loss: 56.752.. Train L1 norm: 6.734.. Val L1 norm: 1.479.. Train Linf norm: 183.474.. Val Linf norm: 15.756\n",
            "Epoch 54/111.. Train loss: 102513.267.. Val loss: 56.752.. Train L1 norm: 11.927.. Val L1 norm: 1.479.. Train Linf norm: 349.589.. Val Linf norm: 15.756\n",
            "Epoch 55/111.. Train loss: 1565.003.. Val loss: 56.752.. Train L1 norm: 2.190.. Val L1 norm: 1.479.. Train Linf norm: 38.007.. Val Linf norm: 15.756\n",
            "Epoch 56/111.. Train loss: 3478.567.. Val loss: 56.752.. Train L1 norm: 8.993.. Val L1 norm: 1.479.. Train Linf norm: 255.630.. Val Linf norm: 15.756\n",
            "Epoch 57/111.. Train loss: 6332.197.. Val loss: 56.752.. Train L1 norm: 7.406.. Val L1 norm: 1.479.. Train Linf norm: 204.727.. Val Linf norm: 15.756\n",
            "Epoch 58/111.. Train loss: 11729.807.. Val loss: 56.752.. Train L1 norm: 9.833.. Val L1 norm: 1.479.. Train Linf norm: 282.438.. Val Linf norm: 15.756\n",
            "Epoch 59/111.. Train loss: 107.784.. Val loss: 56.752.. Train L1 norm: 1.897.. Val L1 norm: 1.479.. Train Linf norm: 28.533.. Val Linf norm: 15.756\n",
            "Epoch 60/111.. Train loss: 262.277.. Val loss: 56.752.. Train L1 norm: 6.874.. Val L1 norm: 1.479.. Train Linf norm: 187.776.. Val Linf norm: 15.756\n",
            "Epoch 61/111.. Train loss: 18954.163.. Val loss: 56.752.. Train L1 norm: 7.994.. Val L1 norm: 1.479.. Train Linf norm: 223.406.. Val Linf norm: 15.756\n",
            "Epoch 62/111.. Train loss: 1820.689.. Val loss: 56.752.. Train L1 norm: 7.900.. Val L1 norm: 1.479.. Train Linf norm: 220.364.. Val Linf norm: 15.756\n",
            "Epoch 63/111.. Train loss: 386.683.. Val loss: 56.752.. Train L1 norm: 6.698.. Val L1 norm: 1.479.. Train Linf norm: 182.035.. Val Linf norm: 15.756\n",
            "Epoch 64/111.. Train loss: 9079.226.. Val loss: 56.752.. Train L1 norm: 6.776.. Val L1 norm: 1.479.. Train Linf norm: 184.747.. Val Linf norm: 15.756\n",
            "Epoch 65/111.. Train loss: 798.108.. Val loss: 56.752.. Train L1 norm: 2.520.. Val L1 norm: 1.479.. Train Linf norm: 48.526.. Val Linf norm: 15.756\n",
            "Epoch 66/111.. Train loss: 574.932.. Val loss: 56.752.. Train L1 norm: 11.224.. Val L1 norm: 1.479.. Train Linf norm: 326.991.. Val Linf norm: 15.756\n",
            "Epoch 67/111.. Train loss: 867.480.. Val loss: 56.752.. Train L1 norm: 7.941.. Val L1 norm: 1.479.. Train Linf norm: 222.081.. Val Linf norm: 15.756\n",
            "Epoch 68/111.. Train loss: 71.181.. Val loss: 56.752.. Train L1 norm: 8.939.. Val L1 norm: 1.479.. Train Linf norm: 253.934.. Val Linf norm: 15.756\n",
            "Epoch 69/111.. Train loss: 48922.902.. Val loss: 56.752.. Train L1 norm: 16.003.. Val L1 norm: 1.479.. Train Linf norm: 479.672.. Val Linf norm: 15.756\n",
            "Epoch 70/111.. Train loss: 2279.461.. Val loss: 56.752.. Train L1 norm: 1.784.. Val L1 norm: 1.479.. Train Linf norm: 24.860.. Val Linf norm: 15.756\n",
            "Epoch 71/111.. Train loss: 874.385.. Val loss: 56.752.. Train L1 norm: 8.132.. Val L1 norm: 1.479.. Train Linf norm: 227.905.. Val Linf norm: 15.756\n",
            "Epoch 72/111.. Train loss: 647.706.. Val loss: 56.752.. Train L1 norm: 5.181.. Val L1 norm: 1.479.. Train Linf norm: 133.631.. Val Linf norm: 15.756\n",
            "Epoch 73/111.. Train loss: 21632.610.. Val loss: 56.752.. Train L1 norm: 5.513.. Val L1 norm: 1.479.. Train Linf norm: 144.267.. Val Linf norm: 15.756\n",
            "Epoch 74/111.. Train loss: 238.048.. Val loss: 56.752.. Train L1 norm: 6.024.. Val L1 norm: 1.479.. Train Linf norm: 160.557.. Val Linf norm: 15.756\n",
            "Epoch 75/111.. Train loss: 174.516.. Val loss: 56.752.. Train L1 norm: 6.942.. Val L1 norm: 1.479.. Train Linf norm: 189.941.. Val Linf norm: 15.756\n",
            "Epoch 76/111.. Train loss: 29520.878.. Val loss: 56.752.. Train L1 norm: 5.005.. Val L1 norm: 1.479.. Train Linf norm: 127.944.. Val Linf norm: 15.756\n",
            "Epoch 77/111.. Train loss: 2332.238.. Val loss: 56.752.. Train L1 norm: 5.376.. Val L1 norm: 1.479.. Train Linf norm: 139.723.. Val Linf norm: 15.756\n",
            "Epoch 78/111.. Train loss: 280.337.. Val loss: 56.752.. Train L1 norm: 4.156.. Val L1 norm: 1.479.. Train Linf norm: 100.740.. Val Linf norm: 15.756\n",
            "Epoch 79/111.. Train loss: 4361.955.. Val loss: 56.752.. Train L1 norm: 4.915.. Val L1 norm: 1.479.. Train Linf norm: 125.155.. Val Linf norm: 15.756\n",
            "Epoch 80/111.. Train loss: 492.346.. Val loss: 56.752.. Train L1 norm: 3.354.. Val L1 norm: 1.479.. Train Linf norm: 75.120.. Val Linf norm: 15.756\n",
            "Epoch 81/111.. Train loss: 1671.934.. Val loss: 56.752.. Train L1 norm: 7.390.. Val L1 norm: 1.479.. Train Linf norm: 204.263.. Val Linf norm: 15.756\n",
            "Epoch 82/111.. Train loss: 134.944.. Val loss: 56.752.. Train L1 norm: 1.818.. Val L1 norm: 1.479.. Train Linf norm: 26.019.. Val Linf norm: 15.756\n",
            "Epoch 83/111.. Train loss: 247.234.. Val loss: 56.752.. Train L1 norm: 1.998.. Val L1 norm: 1.479.. Train Linf norm: 31.697.. Val Linf norm: 15.756\n",
            "Epoch 84/111.. Train loss: 29271.695.. Val loss: 56.752.. Train L1 norm: 14.558.. Val L1 norm: 1.479.. Train Linf norm: 433.847.. Val Linf norm: 15.756\n",
            "Epoch 85/111.. Train loss: 1235.753.. Val loss: 56.752.. Train L1 norm: 3.676.. Val L1 norm: 1.479.. Train Linf norm: 85.474.. Val Linf norm: 15.756\n",
            "Epoch 86/111.. Train loss: 5442.775.. Val loss: 56.752.. Train L1 norm: 5.586.. Val L1 norm: 1.479.. Train Linf norm: 146.606.. Val Linf norm: 15.756\n",
            "Epoch 87/111.. Train loss: 2527.810.. Val loss: 56.752.. Train L1 norm: 3.607.. Val L1 norm: 1.479.. Train Linf norm: 83.466.. Val Linf norm: 15.756\n",
            "Epoch 88/111.. Train loss: 171.509.. Val loss: 56.752.. Train L1 norm: 3.542.. Val L1 norm: 1.479.. Train Linf norm: 81.351.. Val Linf norm: 15.756\n",
            "Epoch 89/111.. Train loss: 2595.859.. Val loss: 56.752.. Train L1 norm: 7.205.. Val L1 norm: 1.479.. Train Linf norm: 198.236.. Val Linf norm: 15.756\n",
            "Epoch 90/111.. Train loss: 630.022.. Val loss: 56.752.. Train L1 norm: 4.813.. Val L1 norm: 1.479.. Train Linf norm: 121.831.. Val Linf norm: 15.756\n",
            "Epoch 91/111.. Train loss: 9445.078.. Val loss: 56.752.. Train L1 norm: 1.909.. Val L1 norm: 1.479.. Train Linf norm: 28.841.. Val Linf norm: 15.756\n",
            "Epoch 92/111.. Train loss: 1009.336.. Val loss: 56.752.. Train L1 norm: 9.155.. Val L1 norm: 1.479.. Train Linf norm: 260.715.. Val Linf norm: 15.756\n",
            "Epoch 93/111.. Train loss: 19720.626.. Val loss: 56.752.. Train L1 norm: 11.586.. Val L1 norm: 1.479.. Train Linf norm: 338.438.. Val Linf norm: 15.756\n",
            "Epoch 94/111.. Train loss: 1992.356.. Val loss: 56.752.. Train L1 norm: 2.243.. Val L1 norm: 1.479.. Train Linf norm: 39.654.. Val Linf norm: 15.756\n",
            "Epoch 95/111.. Train loss: 66570.811.. Val loss: 56.752.. Train L1 norm: 10.059.. Val L1 norm: 1.479.. Train Linf norm: 289.778.. Val Linf norm: 15.756\n",
            "Epoch 96/111.. Train loss: 582.794.. Val loss: 56.752.. Train L1 norm: 4.887.. Val L1 norm: 1.479.. Train Linf norm: 124.219.. Val Linf norm: 15.756\n",
            "Epoch 97/111.. Train loss: 2631.698.. Val loss: 56.752.. Train L1 norm: 4.453.. Val L1 norm: 1.479.. Train Linf norm: 110.307.. Val Linf norm: 15.756\n",
            "Epoch 98/111.. Train loss: 87.693.. Val loss: 56.752.. Train L1 norm: 1.967.. Val L1 norm: 1.479.. Train Linf norm: 30.839.. Val Linf norm: 15.756\n",
            "Epoch 99/111.. Train loss: 593.570.. Val loss: 56.752.. Train L1 norm: 7.985.. Val L1 norm: 1.479.. Train Linf norm: 223.285.. Val Linf norm: 15.756\n",
            "Epoch 100/111.. Train loss: 502.538.. Val loss: 56.752.. Train L1 norm: 2.020.. Val L1 norm: 1.479.. Train Linf norm: 32.537.. Val Linf norm: 15.756\n",
            "Epoch 101/111.. Train loss: 21109.331.. Val loss: 56.752.. Train L1 norm: 1.823.. Val L1 norm: 1.479.. Train Linf norm: 26.192.. Val Linf norm: 15.756\n",
            "Epoch 102/111.. Train loss: 398.074.. Val loss: 56.752.. Train L1 norm: 5.527.. Val L1 norm: 1.479.. Train Linf norm: 144.554.. Val Linf norm: 15.756\n",
            "Epoch 103/111.. Train loss: 202.968.. Val loss: 56.752.. Train L1 norm: 2.907.. Val L1 norm: 1.479.. Train Linf norm: 60.963.. Val Linf norm: 15.756\n",
            "Epoch 104/111.. Train loss: 372852.364.. Val loss: 56.752.. Train L1 norm: 8.942.. Val L1 norm: 1.479.. Train Linf norm: 253.976.. Val Linf norm: 15.756\n",
            "Epoch 105/111.. Train loss: 846.406.. Val loss: 56.752.. Train L1 norm: 4.239.. Val L1 norm: 1.479.. Train Linf norm: 103.542.. Val Linf norm: 15.756\n",
            "Epoch 106/111.. Train loss: 671.388.. Val loss: 56.752.. Train L1 norm: 3.294.. Val L1 norm: 1.479.. Train Linf norm: 73.064.. Val Linf norm: 15.756\n",
            "Epoch 107/111.. Train loss: 112422.633.. Val loss: 56.752.. Train L1 norm: 9.304.. Val L1 norm: 1.479.. Train Linf norm: 265.653.. Val Linf norm: 15.756\n",
            "Epoch 108/111.. Train loss: 26503.983.. Val loss: 56.752.. Train L1 norm: 5.796.. Val L1 norm: 1.479.. Train Linf norm: 153.424.. Val Linf norm: 15.756\n",
            "Epoch 109/111.. Train loss: 5990.451.. Val loss: 56.752.. Train L1 norm: 6.254.. Val L1 norm: 1.479.. Train Linf norm: 167.857.. Val Linf norm: 15.756\n",
            "Epoch 110/111.. Train loss: 151760.815.. Val loss: 56.752.. Train L1 norm: 12.097.. Val L1 norm: 1.479.. Train Linf norm: 354.977.. Val Linf norm: 15.756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 04:23:07,667]\u001b[0m Trial 3 finished with value: 1.478571067237854 and parameters: {'n_layers': 7, 'n_units_0': 2912, 'n_units_1': 3116, 'n_units_2': 2245, 'n_units_3': 488, 'n_units_4': 3078, 'n_units_5': 2852, 'n_units_6': 2983, 'hidden_activation': 'ReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adagrad', 'lr': 0.004781959866774028, 'batch_size': 32, 'n_epochs': 111, 'scheduler': 'ReduceLROnPlateau', 'dropout_rate': 0.12469063486457926, 'factor': 0.12705535727393857, 'patience': 7, 'threshold': 0.004372075888689966}. Best is trial 3 with value: 1.478571067237854.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 111/111.. Train loss: 1041.407.. Val loss: 56.752.. Train L1 norm: 16.380.. Val L1 norm: 1.479.. Train Linf norm: 491.729.. Val Linf norm: 15.756\n",
            "Epoch 1/127.. Train loss: 2431.559.. Val loss: 303.006.. Train L1 norm: 13.627.. Val L1 norm: 6.044.. Train Linf norm: 11947.436.. Val Linf norm: 1790.288\n",
            "Epoch 2/127.. Train loss: 472401.679.. Val loss: 55.036.. Train L1 norm: 47.234.. Val L1 norm: 2.552.. Train Linf norm: 45920.383.. Val Linf norm: 773.171\n",
            "Epoch 3/127.. Train loss: 59994.538.. Val loss: 95.503.. Train L1 norm: 43.976.. Val L1 norm: 2.401.. Train Linf norm: 43213.895.. Val Linf norm: 633.864\n",
            "Epoch 4/127.. Train loss: 86604.997.. Val loss: 51.454.. Train L1 norm: 18.284.. Val L1 norm: 2.650.. Train Linf norm: 16876.323.. Val Linf norm: 796.568\n",
            "Epoch 5/127.. Train loss: 38524.932.. Val loss: 78.464.. Train L1 norm: 53.581.. Val L1 norm: 1.931.. Train Linf norm: 52853.929.. Val Linf norm: 500.041\n",
            "Epoch 6/127.. Train loss: 12585.086.. Val loss: 47.538.. Train L1 norm: 2.386.. Val L1 norm: 1.980.. Train Linf norm: 864.239.. Val Linf norm: 519.851\n",
            "Epoch 7/127.. Train loss: 81827.288.. Val loss: 146.333.. Train L1 norm: 23.073.. Val L1 norm: 3.550.. Train Linf norm: 21932.572.. Val Linf norm: 974.929\n",
            "Epoch 8/127.. Train loss: 311112.207.. Val loss: 50.788.. Train L1 norm: 26.957.. Val L1 norm: 2.106.. Train Linf norm: 25586.309.. Val Linf norm: 576.211\n",
            "Epoch 9/127.. Train loss: 4743.298.. Val loss: 58.331.. Train L1 norm: 36.013.. Val L1 norm: 1.416.. Train Linf norm: 35470.371.. Val Linf norm: 293.346\n",
            "Epoch 10/127.. Train loss: 55170.212.. Val loss: 158.479.. Train L1 norm: 35.985.. Val L1 norm: 3.785.. Train Linf norm: 34886.315.. Val Linf norm: 1036.550\n",
            "Epoch 11/127.. Train loss: 523453.887.. Val loss: 58.857.. Train L1 norm: 22.455.. Val L1 norm: 2.182.. Train Linf norm: 20765.521.. Val Linf norm: 603.280\n",
            "Epoch 12/127.. Train loss: 59837.968.. Val loss: 85.913.. Train L1 norm: 32.793.. Val L1 norm: 1.996.. Train Linf norm: 31926.345.. Val Linf norm: 493.175\n",
            "Epoch 13/127.. Train loss: 77712.958.. Val loss: 60.993.. Train L1 norm: 15.616.. Val L1 norm: 2.634.. Train Linf norm: 14236.562.. Val Linf norm: 791.767\n",
            "Epoch 14/127.. Train loss: 67719.973.. Val loss: 59.785.. Train L1 norm: 37.113.. Val L1 norm: 1.397.. Train Linf norm: 35815.766.. Val Linf norm: 248.465\n",
            "Epoch 15/127.. Train loss: 1817.319.. Val loss: 57.834.. Train L1 norm: 20.447.. Val L1 norm: 1.593.. Train Linf norm: 19543.528.. Val Linf norm: 339.947\n",
            "Epoch 16/127.. Train loss: 25781.751.. Val loss: 74.212.. Train L1 norm: 22.045.. Val L1 norm: 1.557.. Train Linf norm: 21166.649.. Val Linf norm: 326.325\n",
            "Epoch 17/127.. Train loss: 64529.930.. Val loss: 59.172.. Train L1 norm: 20.000.. Val L1 norm: 1.884.. Train Linf norm: 19045.222.. Val Linf norm: 470.821\n",
            "Epoch 18/127.. Train loss: 70316.639.. Val loss: 73.294.. Train L1 norm: 23.051.. Val L1 norm: 1.555.. Train Linf norm: 22046.915.. Val Linf norm: 326.644\n",
            "Epoch 19/127.. Train loss: 100189.665.. Val loss: 59.230.. Train L1 norm: 37.414.. Val L1 norm: 1.833.. Train Linf norm: 36924.184.. Val Linf norm: 448.482\n",
            "Epoch 20/127.. Train loss: 2351.847.. Val loss: 62.077.. Train L1 norm: 23.073.. Val L1 norm: 1.498.. Train Linf norm: 22143.754.. Val Linf norm: 292.152\n",
            "Epoch 21/127.. Train loss: 24027.875.. Val loss: 66.664.. Train L1 norm: 24.803.. Val L1 norm: 1.341.. Train Linf norm: 24085.280.. Val Linf norm: 249.634\n",
            "Epoch 22/127.. Train loss: 365.505.. Val loss: 71.759.. Train L1 norm: 18.145.. Val L1 norm: 1.465.. Train Linf norm: 17358.808.. Val Linf norm: 296.879\n",
            "Epoch 23/127.. Train loss: 2511.371.. Val loss: 66.696.. Train L1 norm: 9.921.. Val L1 norm: 1.345.. Train Linf norm: 8946.231.. Val Linf norm: 251.231\n",
            "Epoch 24/127.. Train loss: 14077.062.. Val loss: 60.320.. Train L1 norm: 3.453.. Val L1 norm: 1.550.. Train Linf norm: 2280.921.. Val Linf norm: 316.516\n",
            "Epoch 25/127.. Train loss: 5276.846.. Val loss: 64.780.. Train L1 norm: 21.073.. Val L1 norm: 1.312.. Train Linf norm: 20258.085.. Val Linf norm: 226.460\n",
            "Epoch 26/127.. Train loss: 4966.429.. Val loss: 71.945.. Train L1 norm: 13.359.. Val L1 norm: 1.464.. Train Linf norm: 12428.416.. Val Linf norm: 296.844\n",
            "Epoch 27/127.. Train loss: 5589.484.. Val loss: 63.307.. Train L1 norm: 8.956.. Val L1 norm: 1.323.. Train Linf norm: 7913.577.. Val Linf norm: 222.668\n",
            "Epoch 28/127.. Train loss: 30514.933.. Val loss: 67.032.. Train L1 norm: 11.473.. Val L1 norm: 1.356.. Train Linf norm: 10530.313.. Val Linf norm: 257.250\n",
            "Epoch 29/127.. Train loss: 192.165.. Val loss: 66.039.. Train L1 norm: 4.722.. Val L1 norm: 1.339.. Train Linf norm: 3632.678.. Val Linf norm: 248.169\n",
            "Epoch 30/127.. Train loss: 3650.474.. Val loss: 63.249.. Train L1 norm: 12.596.. Val L1 norm: 1.314.. Train Linf norm: 11662.554.. Val Linf norm: 220.393\n",
            "Epoch 31/127.. Train loss: 57487.217.. Val loss: 66.809.. Train L1 norm: 6.873.. Val L1 norm: 1.366.. Train Linf norm: 5814.072.. Val Linf norm: 260.131\n",
            "Epoch 32/127.. Train loss: 3158.802.. Val loss: 64.216.. Train L1 norm: 4.930.. Val L1 norm: 1.317.. Train Linf norm: 3838.310.. Val Linf norm: 235.068\n",
            "Epoch 33/127.. Train loss: 1099.244.. Val loss: 66.370.. Train L1 norm: 14.916.. Val L1 norm: 1.354.. Train Linf norm: 14060.423.. Val Linf norm: 254.690\n",
            "Epoch 34/127.. Train loss: 1470.339.. Val loss: 69.721.. Train L1 norm: 26.953.. Val L1 norm: 1.432.. Train Linf norm: 26388.951.. Val Linf norm: 283.642\n",
            "Epoch 35/127.. Train loss: 6004.059.. Val loss: 67.672.. Train L1 norm: 14.457.. Val L1 norm: 1.374.. Train Linf norm: 13561.325.. Val Linf norm: 262.593\n",
            "Epoch 36/127.. Train loss: 11608.886.. Val loss: 65.585.. Train L1 norm: 2.511.. Val L1 norm: 1.332.. Train Linf norm: 1372.272.. Val Linf norm: 243.069\n",
            "Epoch 37/127.. Train loss: 667.058.. Val loss: 64.849.. Train L1 norm: 11.629.. Val L1 norm: 1.321.. Train Linf norm: 10698.113.. Val Linf norm: 237.579\n",
            "Epoch 38/127.. Train loss: 4123.078.. Val loss: 63.336.. Train L1 norm: 14.007.. Val L1 norm: 1.306.. Train Linf norm: 13109.964.. Val Linf norm: 222.594\n",
            "Epoch 39/127.. Train loss: 17362.016.. Val loss: 61.985.. Train L1 norm: 6.178.. Val L1 norm: 1.317.. Train Linf norm: 5077.156.. Val Linf norm: 212.375\n",
            "Epoch 40/127.. Train loss: 37862.521.. Val loss: 63.445.. Train L1 norm: 17.367.. Val L1 norm: 1.306.. Train Linf norm: 16536.517.. Val Linf norm: 223.433\n",
            "Epoch 41/127.. Train loss: 11649.736.. Val loss: 65.322.. Train L1 norm: 4.580.. Val L1 norm: 1.331.. Train Linf norm: 3442.234.. Val Linf norm: 242.235\n",
            "Epoch 42/127.. Train loss: 6733.381.. Val loss: 65.787.. Train L1 norm: 12.042.. Val L1 norm: 1.339.. Train Linf norm: 11083.627.. Val Linf norm: 246.487\n",
            "Epoch 43/127.. Train loss: 5391.556.. Val loss: 66.276.. Train L1 norm: 14.736.. Val L1 norm: 1.348.. Train Linf norm: 13869.788.. Val Linf norm: 250.704\n",
            "Epoch 44/127.. Train loss: 331.382.. Val loss: 66.459.. Train L1 norm: 4.827.. Val L1 norm: 1.351.. Train Linf norm: 3735.486.. Val Linf norm: 251.833\n",
            "Epoch 45/127.. Train loss: 101575.900.. Val loss: 65.518.. Train L1 norm: 3.362.. Val L1 norm: 1.329.. Train Linf norm: 2232.353.. Val Linf norm: 241.203\n",
            "Epoch 46/127.. Train loss: 100.304.. Val loss: 65.526.. Train L1 norm: 13.912.. Val L1 norm: 1.329.. Train Linf norm: 13036.531.. Val Linf norm: 241.254\n",
            "Epoch 47/127.. Train loss: 6450.261.. Val loss: 64.857.. Train L1 norm: 14.555.. Val L1 norm: 1.320.. Train Linf norm: 13697.886.. Val Linf norm: 236.766\n",
            "Epoch 48/127.. Train loss: 2750.925.. Val loss: 64.383.. Train L1 norm: 1.912.. Val L1 norm: 1.314.. Train Linf norm: 751.597.. Val Linf norm: 232.997\n",
            "Epoch 49/127.. Train loss: 3593.834.. Val loss: 64.177.. Train L1 norm: 14.628.. Val L1 norm: 1.311.. Train Linf norm: 13737.739.. Val Linf norm: 230.839\n",
            "Epoch 50/127.. Train loss: 10642.937.. Val loss: 63.853.. Train L1 norm: 2.259.. Val L1 norm: 1.308.. Train Linf norm: 1075.513.. Val Linf norm: 227.603\n",
            "Epoch 51/127.. Train loss: 83.268.. Val loss: 63.865.. Train L1 norm: 27.069.. Val L1 norm: 1.308.. Train Linf norm: 26501.916.. Val Linf norm: 227.629\n",
            "Epoch 52/127.. Train loss: 53899.395.. Val loss: 63.523.. Train L1 norm: 17.754.. Val L1 norm: 1.305.. Train Linf norm: 16958.710.. Val Linf norm: 223.655\n",
            "Epoch 53/127.. Train loss: 33744.432.. Val loss: 63.796.. Train L1 norm: 9.538.. Val L1 norm: 1.306.. Train Linf norm: 8542.731.. Val Linf norm: 226.070\n",
            "Epoch 54/127.. Train loss: 39452.249.. Val loss: 64.063.. Train L1 norm: 2.649.. Val L1 norm: 1.308.. Train Linf norm: 1489.340.. Val Linf norm: 228.613\n",
            "Epoch 55/127.. Train loss: 26231.522.. Val loss: 63.782.. Train L1 norm: 13.022.. Val L1 norm: 1.306.. Train Linf norm: 12104.058.. Val Linf norm: 226.011\n",
            "Epoch 56/127.. Train loss: 55939.108.. Val loss: 63.631.. Train L1 norm: 2.619.. Val L1 norm: 1.305.. Train Linf norm: 1456.275.. Val Linf norm: 224.671\n",
            "Epoch 57/127.. Train loss: 834.607.. Val loss: 63.593.. Train L1 norm: 7.345.. Val L1 norm: 1.305.. Train Linf norm: 6311.427.. Val Linf norm: 224.424\n",
            "Epoch 58/127.. Train loss: 4176.408.. Val loss: 63.516.. Train L1 norm: 6.503.. Val L1 norm: 1.305.. Train Linf norm: 5435.819.. Val Linf norm: 223.840\n",
            "Epoch 59/127.. Train loss: 363.207.. Val loss: 63.471.. Train L1 norm: 1.748.. Val L1 norm: 1.305.. Train Linf norm: 569.532.. Val Linf norm: 223.484\n",
            "Epoch 60/127.. Train loss: 1727.349.. Val loss: 63.565.. Train L1 norm: 15.596.. Val L1 norm: 1.305.. Train Linf norm: 14748.033.. Val Linf norm: 224.269\n",
            "Epoch 61/127.. Train loss: 5944.002.. Val loss: 63.419.. Train L1 norm: 11.546.. Val L1 norm: 1.305.. Train Linf norm: 10586.886.. Val Linf norm: 223.083\n",
            "Epoch 62/127.. Train loss: 184.888.. Val loss: 63.387.. Train L1 norm: 6.960.. Val L1 norm: 1.305.. Train Linf norm: 5900.956.. Val Linf norm: 222.802\n",
            "Epoch 63/127.. Train loss: 4071.462.. Val loss: 63.322.. Train L1 norm: 8.154.. Val L1 norm: 1.305.. Train Linf norm: 7079.683.. Val Linf norm: 222.197\n",
            "Epoch 64/127.. Train loss: 9385.671.. Val loss: 63.251.. Train L1 norm: 4.337.. Val L1 norm: 1.304.. Train Linf norm: 3213.263.. Val Linf norm: 221.527\n",
            "Epoch 65/127.. Train loss: 131.955.. Val loss: 63.247.. Train L1 norm: 27.143.. Val L1 norm: 1.304.. Train Linf norm: 26573.778.. Val Linf norm: 221.500\n",
            "Epoch 66/127.. Train loss: 2122.105.. Val loss: 63.295.. Train L1 norm: 17.451.. Val L1 norm: 1.305.. Train Linf norm: 16626.525.. Val Linf norm: 222.020\n",
            "Epoch 67/127.. Train loss: 13486.723.. Val loss: 63.365.. Train L1 norm: 15.398.. Val L1 norm: 1.305.. Train Linf norm: 14539.723.. Val Linf norm: 222.727\n",
            "Epoch 68/127.. Train loss: 1889.125.. Val loss: 63.405.. Train L1 norm: 7.606.. Val L1 norm: 1.305.. Train Linf norm: 6557.537.. Val Linf norm: 223.076\n",
            "Epoch 69/127.. Train loss: 9749.877.. Val loss: 63.345.. Train L1 norm: 6.388.. Val L1 norm: 1.305.. Train Linf norm: 5315.266.. Val Linf norm: 222.461\n",
            "Epoch 70/127.. Train loss: 12377.692.. Val loss: 63.412.. Train L1 norm: 4.776.. Val L1 norm: 1.305.. Train Linf norm: 3669.185.. Val Linf norm: 223.088\n",
            "Epoch 71/127.. Train loss: 450.545.. Val loss: 63.438.. Train L1 norm: 9.308.. Val L1 norm: 1.305.. Train Linf norm: 8311.687.. Val Linf norm: 223.259\n",
            "Epoch 72/127.. Train loss: 367.749.. Val loss: 63.467.. Train L1 norm: 3.244.. Val L1 norm: 1.305.. Train Linf norm: 2115.474.. Val Linf norm: 223.462\n",
            "Epoch 73/127.. Train loss: 12194.741.. Val loss: 63.403.. Train L1 norm: 20.176.. Val L1 norm: 1.305.. Train Linf norm: 19450.271.. Val Linf norm: 222.767\n",
            "Epoch 74/127.. Train loss: 15380.702.. Val loss: 63.349.. Train L1 norm: 17.174.. Val L1 norm: 1.305.. Train Linf norm: 16373.808.. Val Linf norm: 222.143\n",
            "Epoch 75/127.. Train loss: 1028.799.. Val loss: 63.384.. Train L1 norm: 21.547.. Val L1 norm: 1.305.. Train Linf norm: 20852.741.. Val Linf norm: 222.376\n",
            "Epoch 76/127.. Train loss: 6215.072.. Val loss: 63.434.. Train L1 norm: 2.132.. Val L1 norm: 1.305.. Train Linf norm: 956.082.. Val Linf norm: 222.912\n",
            "Epoch 77/127.. Train loss: 11643.480.. Val loss: 63.489.. Train L1 norm: 28.242.. Val L1 norm: 1.305.. Train Linf norm: 27693.066.. Val Linf norm: 223.474\n",
            "Epoch 78/127.. Train loss: 19645.306.. Val loss: 63.431.. Train L1 norm: 16.445.. Val L1 norm: 1.305.. Train Linf norm: 15628.054.. Val Linf norm: 222.841\n",
            "Epoch 79/127.. Train loss: 4771.631.. Val loss: 63.477.. Train L1 norm: 12.943.. Val L1 norm: 1.305.. Train Linf norm: 12044.963.. Val Linf norm: 223.316\n",
            "Epoch 80/127.. Train loss: 6120.547.. Val loss: 63.432.. Train L1 norm: 22.902.. Val L1 norm: 1.305.. Train Linf norm: 22234.366.. Val Linf norm: 222.849\n",
            "Epoch 81/127.. Train loss: 16060.582.. Val loss: 63.364.. Train L1 norm: 5.344.. Val L1 norm: 1.304.. Train Linf norm: 4246.992.. Val Linf norm: 222.268\n",
            "Epoch 82/127.. Train loss: 5324.410.. Val loss: 63.407.. Train L1 norm: 10.003.. Val L1 norm: 1.305.. Train Linf norm: 9021.932.. Val Linf norm: 222.636\n",
            "Epoch 83/127.. Train loss: 2081.188.. Val loss: 63.365.. Train L1 norm: 15.644.. Val L1 norm: 1.304.. Train Linf norm: 14794.706.. Val Linf norm: 222.323\n",
            "Epoch 84/127.. Train loss: 120.862.. Val loss: 63.376.. Train L1 norm: 19.070.. Val L1 norm: 1.304.. Train Linf norm: 18303.604.. Val Linf norm: 222.397\n",
            "Epoch 85/127.. Train loss: 37698.617.. Val loss: 63.443.. Train L1 norm: 11.055.. Val L1 norm: 1.305.. Train Linf norm: 10077.133.. Val Linf norm: 223.104\n",
            "Epoch 86/127.. Train loss: 11953.138.. Val loss: 63.502.. Train L1 norm: 12.950.. Val L1 norm: 1.305.. Train Linf norm: 12040.237.. Val Linf norm: 223.615\n",
            "Epoch 87/127.. Train loss: 6720.756.. Val loss: 63.553.. Train L1 norm: 19.436.. Val L1 norm: 1.305.. Train Linf norm: 18668.445.. Val Linf norm: 224.029\n",
            "Epoch 88/127.. Train loss: 205.590.. Val loss: 63.541.. Train L1 norm: 24.581.. Val L1 norm: 1.305.. Train Linf norm: 23946.598.. Val Linf norm: 223.924\n",
            "Epoch 89/127.. Train loss: 2018.602.. Val loss: 63.582.. Train L1 norm: 7.262.. Val L1 norm: 1.305.. Train Linf norm: 6209.006.. Val Linf norm: 224.285\n",
            "Epoch 90/127.. Train loss: 2154.440.. Val loss: 63.630.. Train L1 norm: 3.251.. Val L1 norm: 1.305.. Train Linf norm: 2113.588.. Val Linf norm: 224.798\n",
            "Epoch 91/127.. Train loss: 192.034.. Val loss: 63.609.. Train L1 norm: 9.452.. Val L1 norm: 1.305.. Train Linf norm: 8458.642.. Val Linf norm: 224.632\n",
            "Epoch 92/127.. Train loss: 6451.989.. Val loss: 63.672.. Train L1 norm: 6.759.. Val L1 norm: 1.305.. Train Linf norm: 5717.312.. Val Linf norm: 225.197\n",
            "Epoch 93/127.. Train loss: 5002.573.. Val loss: 63.743.. Train L1 norm: 20.533.. Val L1 norm: 1.306.. Train Linf norm: 19815.349.. Val Linf norm: 225.740\n",
            "Epoch 94/127.. Train loss: 10303.114.. Val loss: 63.679.. Train L1 norm: 9.823.. Val L1 norm: 1.305.. Train Linf norm: 8842.645.. Val Linf norm: 225.194\n",
            "Epoch 95/127.. Train loss: 2297.714.. Val loss: 63.724.. Train L1 norm: 14.077.. Val L1 norm: 1.306.. Train Linf norm: 13170.929.. Val Linf norm: 225.606\n",
            "Epoch 96/127.. Train loss: 52173.276.. Val loss: 63.793.. Train L1 norm: 4.100.. Val L1 norm: 1.306.. Train Linf norm: 2980.210.. Val Linf norm: 226.279\n",
            "Epoch 97/127.. Train loss: 59441.986.. Val loss: 63.850.. Train L1 norm: 5.044.. Val L1 norm: 1.307.. Train Linf norm: 3957.755.. Val Linf norm: 226.878\n",
            "Epoch 98/127.. Train loss: 3014.191.. Val loss: 63.806.. Train L1 norm: 25.166.. Val L1 norm: 1.306.. Train Linf norm: 24548.322.. Val Linf norm: 226.575\n",
            "Epoch 99/127.. Train loss: 2174.651.. Val loss: 63.767.. Train L1 norm: 11.336.. Val L1 norm: 1.306.. Train Linf norm: 10374.397.. Val Linf norm: 226.262\n",
            "Epoch 100/127.. Train loss: 808.627.. Val loss: 63.795.. Train L1 norm: 13.362.. Val L1 norm: 1.306.. Train Linf norm: 12460.733.. Val Linf norm: 226.464\n",
            "Epoch 101/127.. Train loss: 20140.788.. Val loss: 63.720.. Train L1 norm: 20.254.. Val L1 norm: 1.306.. Train Linf norm: 19517.192.. Val Linf norm: 225.765\n",
            "Epoch 102/127.. Train loss: 3973.876.. Val loss: 63.672.. Train L1 norm: 10.379.. Val L1 norm: 1.305.. Train Linf norm: 9413.492.. Val Linf norm: 225.327\n",
            "Epoch 103/127.. Train loss: 10417.618.. Val loss: 63.613.. Train L1 norm: 1.809.. Val L1 norm: 1.305.. Train Linf norm: 618.126.. Val Linf norm: 224.704\n",
            "Epoch 104/127.. Train loss: 5080.588.. Val loss: 63.563.. Train L1 norm: 12.036.. Val L1 norm: 1.305.. Train Linf norm: 11099.227.. Val Linf norm: 224.263\n",
            "Epoch 105/127.. Train loss: 678.788.. Val loss: 63.595.. Train L1 norm: 24.237.. Val L1 norm: 1.305.. Train Linf norm: 23605.097.. Val Linf norm: 224.487\n",
            "Epoch 106/127.. Train loss: 4000.453.. Val loss: 63.535.. Train L1 norm: 15.448.. Val L1 norm: 1.305.. Train Linf norm: 14577.706.. Val Linf norm: 223.964\n",
            "Epoch 107/127.. Train loss: 6189.689.. Val loss: 63.595.. Train L1 norm: 2.082.. Val L1 norm: 1.305.. Train Linf norm: 910.501.. Val Linf norm: 224.592\n",
            "Epoch 108/127.. Train loss: 25626.816.. Val loss: 63.528.. Train L1 norm: 13.412.. Val L1 norm: 1.305.. Train Linf norm: 12518.912.. Val Linf norm: 223.861\n",
            "Epoch 109/127.. Train loss: 11662.639.. Val loss: 63.476.. Train L1 norm: 5.479.. Val L1 norm: 1.305.. Train Linf norm: 4378.826.. Val Linf norm: 223.390\n",
            "Epoch 110/127.. Train loss: 9020.124.. Val loss: 63.417.. Train L1 norm: 11.671.. Val L1 norm: 1.304.. Train Linf norm: 10732.974.. Val Linf norm: 222.855\n",
            "Epoch 111/127.. Train loss: 110.983.. Val loss: 63.422.. Train L1 norm: 2.090.. Val L1 norm: 1.304.. Train Linf norm: 916.068.. Val Linf norm: 222.888\n",
            "Epoch 112/127.. Train loss: 13346.849.. Val loss: 63.364.. Train L1 norm: 5.759.. Val L1 norm: 1.304.. Train Linf norm: 4682.567.. Val Linf norm: 222.310\n",
            "Epoch 113/127.. Train loss: 1394.379.. Val loss: 63.405.. Train L1 norm: 2.640.. Val L1 norm: 1.304.. Train Linf norm: 1481.115.. Val Linf norm: 222.616\n",
            "Epoch 114/127.. Train loss: 65662.263.. Val loss: 63.339.. Train L1 norm: 16.640.. Val L1 norm: 1.304.. Train Linf norm: 15823.436.. Val Linf norm: 221.916\n",
            "Epoch 115/127.. Train loss: 25192.309.. Val loss: 63.401.. Train L1 norm: 5.416.. Val L1 norm: 1.304.. Train Linf norm: 4316.703.. Val Linf norm: 222.398\n",
            "Epoch 116/127.. Train loss: 334.457.. Val loss: 63.415.. Train L1 norm: 15.563.. Val L1 norm: 1.304.. Train Linf norm: 14710.883.. Val Linf norm: 222.474\n",
            "Epoch 117/127.. Train loss: 3925.994.. Val loss: 63.369.. Train L1 norm: 8.604.. Val L1 norm: 1.304.. Train Linf norm: 7585.125.. Val Linf norm: 222.008\n",
            "Epoch 118/127.. Train loss: 109500.258.. Val loss: 63.301.. Train L1 norm: 3.953.. Val L1 norm: 1.304.. Train Linf norm: 2827.571.. Val Linf norm: 221.335\n",
            "Epoch 119/127.. Train loss: 2502.231.. Val loss: 63.265.. Train L1 norm: 11.380.. Val L1 norm: 1.304.. Train Linf norm: 10421.359.. Val Linf norm: 221.042\n",
            "Epoch 120/127.. Train loss: 413.530.. Val loss: 63.248.. Train L1 norm: 9.099.. Val L1 norm: 1.304.. Train Linf norm: 8104.328.. Val Linf norm: 220.907\n",
            "Epoch 121/127.. Train loss: 5388.109.. Val loss: 63.191.. Train L1 norm: 18.201.. Val L1 norm: 1.304.. Train Linf norm: 17426.300.. Val Linf norm: 220.459\n",
            "Epoch 122/127.. Train loss: 1078.786.. Val loss: 63.167.. Train L1 norm: 14.728.. Val L1 norm: 1.304.. Train Linf norm: 13855.565.. Val Linf norm: 220.246\n",
            "Epoch 123/127.. Train loss: 4565.379.. Val loss: 63.222.. Train L1 norm: 16.121.. Val L1 norm: 1.304.. Train Linf norm: 15275.652.. Val Linf norm: 220.732\n",
            "Epoch 124/127.. Train loss: 831.446.. Val loss: 63.187.. Train L1 norm: 13.453.. Val L1 norm: 1.304.. Train Linf norm: 12564.641.. Val Linf norm: 220.444\n",
            "Epoch 125/127.. Train loss: 5682.971.. Val loss: 63.250.. Train L1 norm: 16.171.. Val L1 norm: 1.304.. Train Linf norm: 15333.692.. Val Linf norm: 221.006\n",
            "Epoch 126/127.. Train loss: 54545.791.. Val loss: 63.186.. Train L1 norm: 10.341.. Val L1 norm: 1.304.. Train Linf norm: 9383.325.. Val Linf norm: 220.288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 04:30:47,426]\u001b[0m Trial 4 finished with value: 1.30382172832489 and parameters: {'n_layers': 4, 'n_units_0': 313, 'n_units_1': 3265, 'n_units_2': 3831, 'n_units_3': 2412, 'hidden_activation': 'LeakyReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'RMSprop', 'lr': 5.142300685583092e-06, 'batch_size': 1024, 'n_epochs': 127, 'scheduler': 'ReduceLROnPlateau', 'leakyrelu_slope': 0.23154673591607142, 'dropout_rate': 0.10646571932712628, 'factor': 0.44677932125713526, 'patience': 6, 'threshold': 0.0002858329123008351}. Best is trial 4 with value: 1.30382172832489.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 127/127.. Train loss: 1432.051.. Val loss: 63.215.. Train L1 norm: 14.652.. Val L1 norm: 1.304.. Train Linf norm: 13773.091.. Val Linf norm: 220.547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-77-ddc3c9d46736>:73: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  beta = trial.suggest_uniform(\"softplus_beta\", 0.5, 1.5)\n",
            "<ipython-input-77-ddc3c9d46736>:161: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  gamma = trial.suggest_uniform(\"gamma\", 0.1, 0.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/126.. Train loss: 19342.652.. Val loss: 4.505.. Train L1 norm: 8377.501.. Val L1 norm: 6.087.. Train Linf norm: 29754.275.. Val Linf norm: 132.335\n",
            "Epoch 2/126.. Train loss: 4.005.. Val loss: 3.786.. Train L1 norm: 33.480.. Val L1 norm: 5.898.. Train Linf norm: 1016.833.. Val Linf norm: 128.710\n",
            "Epoch 3/126.. Train loss: 3.656.. Val loss: 3.147.. Train L1 norm: 21.576.. Val L1 norm: 3.681.. Train Linf norm: 639.354.. Val Linf norm: 75.362\n",
            "Epoch 4/126.. Train loss: 3.252.. Val loss: 2.870.. Train L1 norm: 20.688.. Val L1 norm: 3.131.. Train Linf norm: 620.932.. Val Linf norm: 68.290\n",
            "Epoch 5/126.. Train loss: 2.890.. Val loss: 2.726.. Train L1 norm: 22.773.. Val L1 norm: 1.768.. Train Linf norm: 691.955.. Val Linf norm: 29.543\n",
            "Epoch 6/126.. Train loss: 2.721.. Val loss: 2.535.. Train L1 norm: 34.278.. Val L1 norm: 2.480.. Train Linf norm: 1062.965.. Val Linf norm: 51.314\n",
            "Epoch 7/126.. Train loss: 2.602.. Val loss: 2.360.. Train L1 norm: 13.526.. Val L1 norm: 2.373.. Train Linf norm: 400.309.. Val Linf norm: 48.652\n",
            "Epoch 8/126.. Train loss: 2.491.. Val loss: 2.373.. Train L1 norm: 24.747.. Val L1 norm: 2.585.. Train Linf norm: 760.405.. Val Linf norm: 56.151\n",
            "Epoch 9/126.. Train loss: 2.436.. Val loss: 2.254.. Train L1 norm: 11.829.. Val L1 norm: 2.528.. Train Linf norm: 347.765.. Val Linf norm: 53.505\n",
            "Epoch 10/126.. Train loss: 2.181.. Val loss: 1.975.. Train L1 norm: 6.057.. Val L1 norm: 2.039.. Train Linf norm: 166.467.. Val Linf norm: 41.719\n",
            "Epoch 11/126.. Train loss: 2.137.. Val loss: 1.952.. Train L1 norm: 7.239.. Val L1 norm: 1.981.. Train Linf norm: 204.449.. Val Linf norm: 39.570\n",
            "Epoch 12/126.. Train loss: 2.572.. Val loss: 1.930.. Train L1 norm: 23.626.. Val L1 norm: 2.132.. Train Linf norm: 729.656.. Val Linf norm: 44.307\n",
            "Epoch 13/126.. Train loss: 2.085.. Val loss: 2.004.. Train L1 norm: 4.707.. Val L1 norm: 2.502.. Train Linf norm: 124.349.. Val Linf norm: 52.204\n",
            "Epoch 14/126.. Train loss: 2.213.. Val loss: 1.946.. Train L1 norm: 7.652.. Val L1 norm: 1.680.. Train Linf norm: 218.725.. Val Linf norm: 31.722\n",
            "Epoch 15/126.. Train loss: 2.102.. Val loss: 1.895.. Train L1 norm: 40.212.. Val L1 norm: 1.614.. Train Linf norm: 1260.924.. Val Linf norm: 30.385\n",
            "Epoch 16/126.. Train loss: 2.075.. Val loss: 1.951.. Train L1 norm: 25.591.. Val L1 norm: 2.268.. Train Linf norm: 792.944.. Val Linf norm: 47.633\n",
            "Epoch 17/126.. Train loss: 2.033.. Val loss: 2.014.. Train L1 norm: 7.958.. Val L1 norm: 1.572.. Train Linf norm: 229.204.. Val Linf norm: 29.284\n",
            "Epoch 18/126.. Train loss: 2.014.. Val loss: 1.909.. Train L1 norm: 29.768.. Val L1 norm: 1.943.. Train Linf norm: 927.449.. Val Linf norm: 37.919\n",
            "Epoch 19/126.. Train loss: 1.953.. Val loss: 1.822.. Train L1 norm: 28.706.. Val L1 norm: 1.699.. Train Linf norm: 893.291.. Val Linf norm: 33.367\n",
            "Epoch 20/126.. Train loss: 1.927.. Val loss: 1.830.. Train L1 norm: 5.809.. Val L1 norm: 1.763.. Train Linf norm: 161.437.. Val Linf norm: 34.692\n",
            "Epoch 21/126.. Train loss: 1.947.. Val loss: 1.826.. Train L1 norm: 16.033.. Val L1 norm: 1.837.. Train Linf norm: 488.681.. Val Linf norm: 36.192\n",
            "Epoch 22/126.. Train loss: 1.910.. Val loss: 1.834.. Train L1 norm: 19.727.. Val L1 norm: 1.791.. Train Linf norm: 606.975.. Val Linf norm: 36.480\n",
            "Epoch 23/126.. Train loss: 1.918.. Val loss: 1.812.. Train L1 norm: 17.367.. Val L1 norm: 1.757.. Train Linf norm: 530.746.. Val Linf norm: 34.374\n",
            "Epoch 24/126.. Train loss: 1.898.. Val loss: 1.823.. Train L1 norm: 14.023.. Val L1 norm: 1.855.. Train Linf norm: 424.374.. Val Linf norm: 37.795\n",
            "Epoch 25/126.. Train loss: 1.906.. Val loss: 1.813.. Train L1 norm: 10.851.. Val L1 norm: 1.793.. Train Linf norm: 323.492.. Val Linf norm: 35.305\n",
            "Epoch 26/126.. Train loss: 1.929.. Val loss: 1.819.. Train L1 norm: 6.779.. Val L1 norm: 1.640.. Train Linf norm: 193.469.. Val Linf norm: 32.472\n",
            "Epoch 27/126.. Train loss: 1.886.. Val loss: 1.800.. Train L1 norm: 22.592.. Val L1 norm: 1.660.. Train Linf norm: 699.324.. Val Linf norm: 32.047\n",
            "Epoch 28/126.. Train loss: 1.913.. Val loss: 1.790.. Train L1 norm: 7.573.. Val L1 norm: 1.644.. Train Linf norm: 218.834.. Val Linf norm: 31.625\n",
            "Epoch 29/126.. Train loss: 1.869.. Val loss: 1.784.. Train L1 norm: 3.752.. Val L1 norm: 1.625.. Train Linf norm: 96.785.. Val Linf norm: 31.558\n",
            "Epoch 30/126.. Train loss: 1.857.. Val loss: 1.791.. Train L1 norm: 18.555.. Val L1 norm: 1.762.. Train Linf norm: 569.311.. Val Linf norm: 35.233\n",
            "Epoch 31/126.. Train loss: 1.888.. Val loss: 1.781.. Train L1 norm: 12.365.. Val L1 norm: 1.681.. Train Linf norm: 372.402.. Val Linf norm: 33.017\n",
            "Epoch 32/126.. Train loss: 1.856.. Val loss: 1.783.. Train L1 norm: 21.045.. Val L1 norm: 1.606.. Train Linf norm: 650.323.. Val Linf norm: 31.180\n",
            "Epoch 33/126.. Train loss: 1.852.. Val loss: 1.775.. Train L1 norm: 25.269.. Val L1 norm: 1.719.. Train Linf norm: 785.449.. Val Linf norm: 34.191\n",
            "Epoch 34/126.. Train loss: 1.853.. Val loss: 1.784.. Train L1 norm: 7.839.. Val L1 norm: 1.717.. Train Linf norm: 227.395.. Val Linf norm: 34.308\n",
            "Epoch 35/126.. Train loss: 1.855.. Val loss: 1.784.. Train L1 norm: 24.593.. Val L1 norm: 1.727.. Train Linf norm: 763.876.. Val Linf norm: 34.607\n",
            "Epoch 36/126.. Train loss: 1.844.. Val loss: 1.781.. Train L1 norm: 27.698.. Val L1 norm: 1.694.. Train Linf norm: 862.788.. Val Linf norm: 33.570\n",
            "Epoch 37/126.. Train loss: 1.843.. Val loss: 1.776.. Train L1 norm: 21.692.. Val L1 norm: 1.690.. Train Linf norm: 670.597.. Val Linf norm: 33.529\n",
            "Epoch 38/126.. Train loss: 1.836.. Val loss: 1.775.. Train L1 norm: 14.745.. Val L1 norm: 1.700.. Train Linf norm: 448.739.. Val Linf norm: 33.804\n",
            "Epoch 39/126.. Train loss: 1.851.. Val loss: 1.775.. Train L1 norm: 3.289.. Val L1 norm: 1.681.. Train Linf norm: 81.862.. Val Linf norm: 33.317\n",
            "Epoch 40/126.. Train loss: 1.840.. Val loss: 1.774.. Train L1 norm: 4.787.. Val L1 norm: 1.667.. Train Linf norm: 130.146.. Val Linf norm: 32.958\n",
            "Epoch 41/126.. Train loss: 1.837.. Val loss: 1.774.. Train L1 norm: 9.839.. Val L1 norm: 1.729.. Train Linf norm: 291.414.. Val Linf norm: 34.572\n",
            "Epoch 42/126.. Train loss: 1.835.. Val loss: 1.772.. Train L1 norm: 10.057.. Val L1 norm: 1.675.. Train Linf norm: 298.839.. Val Linf norm: 33.154\n",
            "Epoch 43/126.. Train loss: 1.907.. Val loss: 1.773.. Train L1 norm: 8.545.. Val L1 norm: 1.676.. Train Linf norm: 250.291.. Val Linf norm: 33.054\n",
            "Epoch 44/126.. Train loss: 1.833.. Val loss: 1.775.. Train L1 norm: 27.165.. Val L1 norm: 1.727.. Train Linf norm: 845.902.. Val Linf norm: 34.480\n",
            "Epoch 45/126.. Train loss: 1.861.. Val loss: 1.774.. Train L1 norm: 32.768.. Val L1 norm: 1.690.. Train Linf norm: 1025.514.. Val Linf norm: 33.541\n",
            "Epoch 46/126.. Train loss: 1.851.. Val loss: 1.774.. Train L1 norm: 5.806.. Val L1 norm: 1.689.. Train Linf norm: 162.839.. Val Linf norm: 33.545\n",
            "Epoch 47/126.. Train loss: 1.845.. Val loss: 1.773.. Train L1 norm: 5.200.. Val L1 norm: 1.706.. Train Linf norm: 143.009.. Val Linf norm: 33.953\n",
            "Epoch 48/126.. Train loss: 1.837.. Val loss: 1.772.. Train L1 norm: 2.862.. Val L1 norm: 1.682.. Train Linf norm: 67.925.. Val Linf norm: 33.335\n",
            "Epoch 49/126.. Train loss: 1.834.. Val loss: 1.772.. Train L1 norm: 15.648.. Val L1 norm: 1.687.. Train Linf norm: 477.421.. Val Linf norm: 33.484\n",
            "Epoch 50/126.. Train loss: 1.839.. Val loss: 1.772.. Train L1 norm: 12.693.. Val L1 norm: 1.697.. Train Linf norm: 383.131.. Val Linf norm: 33.775\n",
            "Epoch 51/126.. Train loss: 1.834.. Val loss: 1.772.. Train L1 norm: 6.083.. Val L1 norm: 1.706.. Train Linf norm: 171.753.. Val Linf norm: 33.982\n",
            "Epoch 52/126.. Train loss: 1.831.. Val loss: 1.771.. Train L1 norm: 4.328.. Val L1 norm: 1.699.. Train Linf norm: 114.593.. Val Linf norm: 33.747\n",
            "Epoch 53/126.. Train loss: 1.833.. Val loss: 1.772.. Train L1 norm: 15.016.. Val L1 norm: 1.699.. Train Linf norm: 457.477.. Val Linf norm: 33.777\n",
            "Epoch 54/126.. Train loss: 1.834.. Val loss: 1.772.. Train L1 norm: 3.661.. Val L1 norm: 1.712.. Train Linf norm: 93.890.. Val Linf norm: 34.123\n",
            "Epoch 55/126.. Train loss: 1.836.. Val loss: 1.771.. Train L1 norm: 30.362.. Val L1 norm: 1.700.. Train Linf norm: 948.761.. Val Linf norm: 33.793\n",
            "Epoch 56/126.. Train loss: 1.868.. Val loss: 1.772.. Train L1 norm: 15.773.. Val L1 norm: 1.702.. Train Linf norm: 481.562.. Val Linf norm: 33.844\n",
            "Epoch 57/126.. Train loss: 1.832.. Val loss: 1.772.. Train L1 norm: 17.964.. Val L1 norm: 1.699.. Train Linf norm: 551.738.. Val Linf norm: 33.780\n",
            "Epoch 58/126.. Train loss: 1.827.. Val loss: 1.772.. Train L1 norm: 5.696.. Val L1 norm: 1.693.. Train Linf norm: 159.272.. Val Linf norm: 33.614\n",
            "Epoch 59/126.. Train loss: 1.829.. Val loss: 1.772.. Train L1 norm: 7.630.. Val L1 norm: 1.699.. Train Linf norm: 220.739.. Val Linf norm: 33.759\n",
            "Epoch 60/126.. Train loss: 1.838.. Val loss: 1.771.. Train L1 norm: 26.763.. Val L1 norm: 1.694.. Train Linf norm: 833.468.. Val Linf norm: 33.647\n",
            "Epoch 61/126.. Train loss: 1.853.. Val loss: 1.772.. Train L1 norm: 19.712.. Val L1 norm: 1.691.. Train Linf norm: 607.410.. Val Linf norm: 33.562\n",
            "Epoch 62/126.. Train loss: 1.843.. Val loss: 1.772.. Train L1 norm: 3.259.. Val L1 norm: 1.689.. Train Linf norm: 81.115.. Val Linf norm: 33.517\n",
            "Epoch 63/126.. Train loss: 1.841.. Val loss: 1.772.. Train L1 norm: 9.005.. Val L1 norm: 1.695.. Train Linf norm: 264.905.. Val Linf norm: 33.653\n",
            "Epoch 64/126.. Train loss: 1.853.. Val loss: 1.772.. Train L1 norm: 17.653.. Val L1 norm: 1.694.. Train Linf norm: 541.710.. Val Linf norm: 33.640\n",
            "Epoch 65/126.. Train loss: 1.839.. Val loss: 1.772.. Train L1 norm: 8.965.. Val L1 norm: 1.694.. Train Linf norm: 263.427.. Val Linf norm: 33.627\n",
            "Epoch 66/126.. Train loss: 1.834.. Val loss: 1.772.. Train L1 norm: 8.977.. Val L1 norm: 1.693.. Train Linf norm: 263.859.. Val Linf norm: 33.617\n",
            "Epoch 67/126.. Train loss: 1.848.. Val loss: 1.772.. Train L1 norm: 39.869.. Val L1 norm: 1.694.. Train Linf norm: 1252.632.. Val Linf norm: 33.641\n",
            "Epoch 68/126.. Train loss: 1.854.. Val loss: 1.772.. Train L1 norm: 5.251.. Val L1 norm: 1.694.. Train Linf norm: 144.032.. Val Linf norm: 33.625\n",
            "Epoch 69/126.. Train loss: 1.835.. Val loss: 1.771.. Train L1 norm: 29.969.. Val L1 norm: 1.694.. Train Linf norm: 935.929.. Val Linf norm: 33.635\n",
            "Epoch 70/126.. Train loss: 1.840.. Val loss: 1.772.. Train L1 norm: 30.938.. Val L1 norm: 1.694.. Train Linf norm: 966.675.. Val Linf norm: 33.631\n",
            "Epoch 71/126.. Train loss: 1.851.. Val loss: 1.771.. Train L1 norm: 4.448.. Val L1 norm: 1.692.. Train Linf norm: 119.601.. Val Linf norm: 33.597\n",
            "Epoch 72/126.. Train loss: 1.834.. Val loss: 1.772.. Train L1 norm: 8.179.. Val L1 norm: 1.692.. Train Linf norm: 238.753.. Val Linf norm: 33.590\n",
            "Epoch 73/126.. Train loss: 1.845.. Val loss: 1.771.. Train L1 norm: 6.059.. Val L1 norm: 1.692.. Train Linf norm: 170.525.. Val Linf norm: 33.590\n",
            "Epoch 74/126.. Train loss: 1.836.. Val loss: 1.772.. Train L1 norm: 34.150.. Val L1 norm: 1.692.. Train Linf norm: 1069.591.. Val Linf norm: 33.586\n",
            "Epoch 75/126.. Train loss: 1.868.. Val loss: 1.772.. Train L1 norm: 29.689.. Val L1 norm: 1.692.. Train Linf norm: 927.043.. Val Linf norm: 33.594\n",
            "Epoch 76/126.. Train loss: 1.849.. Val loss: 1.771.. Train L1 norm: 3.917.. Val L1 norm: 1.693.. Train Linf norm: 102.387.. Val Linf norm: 33.608\n",
            "Epoch 77/126.. Train loss: 1.854.. Val loss: 1.772.. Train L1 norm: 3.264.. Val L1 norm: 1.693.. Train Linf norm: 81.329.. Val Linf norm: 33.610\n",
            "Epoch 78/126.. Train loss: 1.836.. Val loss: 1.772.. Train L1 norm: 20.250.. Val L1 norm: 1.692.. Train Linf norm: 624.778.. Val Linf norm: 33.598\n",
            "Epoch 79/126.. Train loss: 1.831.. Val loss: 1.772.. Train L1 norm: 4.673.. Val L1 norm: 1.693.. Train Linf norm: 125.954.. Val Linf norm: 33.610\n",
            "Epoch 80/126.. Train loss: 1.832.. Val loss: 1.772.. Train L1 norm: 43.364.. Val L1 norm: 1.692.. Train Linf norm: 1364.585.. Val Linf norm: 33.595\n",
            "Epoch 81/126.. Train loss: 1.863.. Val loss: 1.772.. Train L1 norm: 16.378.. Val L1 norm: 1.692.. Train Linf norm: 500.792.. Val Linf norm: 33.587\n",
            "Epoch 82/126.. Train loss: 1.834.. Val loss: 1.772.. Train L1 norm: 6.917.. Val L1 norm: 1.692.. Train Linf norm: 198.279.. Val Linf norm: 33.586\n",
            "Epoch 83/126.. Train loss: 1.847.. Val loss: 1.772.. Train L1 norm: 16.654.. Val L1 norm: 1.692.. Train Linf norm: 509.516.. Val Linf norm: 33.587\n",
            "Epoch 84/126.. Train loss: 1.848.. Val loss: 1.772.. Train L1 norm: 10.858.. Val L1 norm: 1.692.. Train Linf norm: 324.388.. Val Linf norm: 33.586\n",
            "Epoch 85/126.. Train loss: 1.836.. Val loss: 1.772.. Train L1 norm: 18.847.. Val L1 norm: 1.692.. Train Linf norm: 580.058.. Val Linf norm: 33.588\n",
            "Epoch 86/126.. Train loss: 1.832.. Val loss: 1.772.. Train L1 norm: 11.314.. Val L1 norm: 1.692.. Train Linf norm: 338.924.. Val Linf norm: 33.589\n",
            "Epoch 87/126.. Train loss: 1.839.. Val loss: 1.772.. Train L1 norm: 15.794.. Val L1 norm: 1.692.. Train Linf norm: 482.575.. Val Linf norm: 33.588\n",
            "Epoch 88/126.. Train loss: 1.842.. Val loss: 1.772.. Train L1 norm: 9.006.. Val L1 norm: 1.692.. Train Linf norm: 265.152.. Val Linf norm: 33.587\n",
            "Epoch 89/126.. Train loss: 1.831.. Val loss: 1.772.. Train L1 norm: 4.198.. Val L1 norm: 1.692.. Train Linf norm: 111.107.. Val Linf norm: 33.583\n",
            "Epoch 90/126.. Train loss: 1.837.. Val loss: 1.772.. Train L1 norm: 24.993.. Val L1 norm: 1.692.. Train Linf norm: 776.333.. Val Linf norm: 33.584\n",
            "Epoch 91/126.. Train loss: 1.836.. Val loss: 1.772.. Train L1 norm: 4.125.. Val L1 norm: 1.692.. Train Linf norm: 109.010.. Val Linf norm: 33.585\n",
            "Epoch 92/126.. Train loss: 1.829.. Val loss: 1.772.. Train L1 norm: 38.728.. Val L1 norm: 1.692.. Train Linf norm: 1216.268.. Val Linf norm: 33.585\n",
            "Epoch 93/126.. Train loss: 1.912.. Val loss: 1.772.. Train L1 norm: 7.192.. Val L1 norm: 1.692.. Train Linf norm: 207.146.. Val Linf norm: 33.585\n",
            "Epoch 94/126.. Train loss: 1.830.. Val loss: 1.772.. Train L1 norm: 42.136.. Val L1 norm: 1.692.. Train Linf norm: 1325.541.. Val Linf norm: 33.585\n",
            "Epoch 95/126.. Train loss: 1.867.. Val loss: 1.772.. Train L1 norm: 9.842.. Val L1 norm: 1.692.. Train Linf norm: 291.963.. Val Linf norm: 33.585\n",
            "Epoch 96/126.. Train loss: 1.835.. Val loss: 1.772.. Train L1 norm: 6.115.. Val L1 norm: 1.692.. Train Linf norm: 172.420.. Val Linf norm: 33.585\n",
            "Epoch 97/126.. Train loss: 1.826.. Val loss: 1.772.. Train L1 norm: 38.733.. Val L1 norm: 1.692.. Train Linf norm: 1216.170.. Val Linf norm: 33.584\n",
            "Epoch 98/126.. Train loss: 1.849.. Val loss: 1.772.. Train L1 norm: 22.778.. Val L1 norm: 1.692.. Train Linf norm: 705.706.. Val Linf norm: 33.584\n",
            "Epoch 99/126.. Train loss: 1.829.. Val loss: 1.772.. Train L1 norm: 26.010.. Val L1 norm: 1.692.. Train Linf norm: 809.158.. Val Linf norm: 33.584\n",
            "Epoch 100/126.. Train loss: 1.859.. Val loss: 1.772.. Train L1 norm: 4.649.. Val L1 norm: 1.692.. Train Linf norm: 125.398.. Val Linf norm: 33.584\n",
            "Epoch 101/126.. Train loss: 1.830.. Val loss: 1.772.. Train L1 norm: 24.112.. Val L1 norm: 1.692.. Train Linf norm: 748.614.. Val Linf norm: 33.584\n",
            "Epoch 102/126.. Train loss: 1.850.. Val loss: 1.772.. Train L1 norm: 10.605.. Val L1 norm: 1.692.. Train Linf norm: 316.149.. Val Linf norm: 33.584\n",
            "Epoch 103/126.. Train loss: 1.862.. Val loss: 1.772.. Train L1 norm: 11.221.. Val L1 norm: 1.692.. Train Linf norm: 336.052.. Val Linf norm: 33.584\n",
            "Epoch 104/126.. Train loss: 1.835.. Val loss: 1.772.. Train L1 norm: 15.056.. Val L1 norm: 1.692.. Train Linf norm: 458.726.. Val Linf norm: 33.584\n",
            "Epoch 105/126.. Train loss: 1.836.. Val loss: 1.772.. Train L1 norm: 2.541.. Val L1 norm: 1.692.. Train Linf norm: 58.403.. Val Linf norm: 33.584\n",
            "Epoch 106/126.. Train loss: 1.830.. Val loss: 1.772.. Train L1 norm: 6.466.. Val L1 norm: 1.692.. Train Linf norm: 183.929.. Val Linf norm: 33.584\n",
            "Epoch 107/126.. Train loss: 1.841.. Val loss: 1.772.. Train L1 norm: 26.331.. Val L1 norm: 1.692.. Train Linf norm: 819.032.. Val Linf norm: 33.584\n",
            "Epoch 108/126.. Train loss: 1.842.. Val loss: 1.772.. Train L1 norm: 12.868.. Val L1 norm: 1.692.. Train Linf norm: 388.705.. Val Linf norm: 33.584\n",
            "Epoch 109/126.. Train loss: 1.832.. Val loss: 1.772.. Train L1 norm: 13.688.. Val L1 norm: 1.692.. Train Linf norm: 415.037.. Val Linf norm: 33.584\n",
            "Epoch 110/126.. Train loss: 1.828.. Val loss: 1.772.. Train L1 norm: 44.065.. Val L1 norm: 1.692.. Train Linf norm: 1387.028.. Val Linf norm: 33.584\n",
            "Epoch 111/126.. Train loss: 1.848.. Val loss: 1.772.. Train L1 norm: 5.471.. Val L1 norm: 1.692.. Train Linf norm: 151.373.. Val Linf norm: 33.584\n",
            "Epoch 112/126.. Train loss: 1.829.. Val loss: 1.772.. Train L1 norm: 16.546.. Val L1 norm: 1.692.. Train Linf norm: 506.254.. Val Linf norm: 33.584\n",
            "Epoch 113/126.. Train loss: 1.838.. Val loss: 1.772.. Train L1 norm: 13.134.. Val L1 norm: 1.692.. Train Linf norm: 396.982.. Val Linf norm: 33.584\n",
            "Epoch 114/126.. Train loss: 1.834.. Val loss: 1.772.. Train L1 norm: 13.673.. Val L1 norm: 1.692.. Train Linf norm: 414.556.. Val Linf norm: 33.584\n",
            "Epoch 115/126.. Train loss: 1.831.. Val loss: 1.772.. Train L1 norm: 4.434.. Val L1 norm: 1.692.. Train Linf norm: 119.210.. Val Linf norm: 33.584\n",
            "Epoch 116/126.. Train loss: 1.846.. Val loss: 1.772.. Train L1 norm: 18.335.. Val L1 norm: 1.692.. Train Linf norm: 563.656.. Val Linf norm: 33.584\n",
            "Epoch 117/126.. Train loss: 1.835.. Val loss: 1.772.. Train L1 norm: 4.409.. Val L1 norm: 1.692.. Train Linf norm: 118.121.. Val Linf norm: 33.584\n",
            "Epoch 118/126.. Train loss: 1.833.. Val loss: 1.772.. Train L1 norm: 8.282.. Val L1 norm: 1.692.. Train Linf norm: 241.882.. Val Linf norm: 33.584\n",
            "Epoch 119/126.. Train loss: 1.835.. Val loss: 1.772.. Train L1 norm: 11.254.. Val L1 norm: 1.692.. Train Linf norm: 337.107.. Val Linf norm: 33.584\n",
            "Epoch 120/126.. Train loss: 1.834.. Val loss: 1.772.. Train L1 norm: 5.424.. Val L1 norm: 1.692.. Train Linf norm: 150.418.. Val Linf norm: 33.584\n",
            "Epoch 121/126.. Train loss: 1.831.. Val loss: 1.772.. Train L1 norm: 9.792.. Val L1 norm: 1.692.. Train Linf norm: 289.926.. Val Linf norm: 33.584\n",
            "Epoch 122/126.. Train loss: 1.834.. Val loss: 1.772.. Train L1 norm: 29.776.. Val L1 norm: 1.692.. Train Linf norm: 929.774.. Val Linf norm: 33.584\n",
            "Epoch 123/126.. Train loss: 1.863.. Val loss: 1.772.. Train L1 norm: 4.980.. Val L1 norm: 1.692.. Train Linf norm: 136.343.. Val Linf norm: 33.584\n",
            "Epoch 124/126.. Train loss: 1.829.. Val loss: 1.772.. Train L1 norm: 5.546.. Val L1 norm: 1.692.. Train Linf norm: 154.369.. Val Linf norm: 33.584\n",
            "Epoch 125/126.. Train loss: 1.832.. Val loss: 1.772.. Train L1 norm: 9.080.. Val L1 norm: 1.692.. Train Linf norm: 266.948.. Val Linf norm: 33.584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 05:33:51,148]\u001b[0m Trial 5 finished with value: 1.6918909295399984 and parameters: {'n_layers': 10, 'n_units_0': 3397, 'n_units_1': 1563, 'n_units_2': 1854, 'n_units_3': 3052, 'n_units_4': 486, 'n_units_5': 3292, 'n_units_6': 3122, 'n_units_7': 2736, 'n_units_8': 579, 'n_units_9': 1600, 'hidden_activation': 'SoftPlus', 'output_activation': 'Linear', 'loss': 'MAE', 'optimizer': 'RMSprop', 'lr': 0.00048324682346559427, 'batch_size': 32, 'n_epochs': 126, 'scheduler': 'StepLR', 'softplus_beta': 0.7709420236586922, 'dropout_rate': 0.13008975704083042, 'step_size': 9, 'gamma': 0.24815624124188673}. Best is trial 4 with value: 1.30382172832489.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 126/126.. Train loss: 1.842.. Val loss: 1.772.. Train L1 norm: 3.676.. Val L1 norm: 1.692.. Train Linf norm: 94.384.. Val Linf norm: 33.584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 05:34:02,526]\u001b[0m Trial 6 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/93.. Train loss: 9958.053.. Val loss: 43.369.. Train L1 norm: 76.882.. Val L1 norm: 6.172.. Train Linf norm: 2403.202.. Val Linf norm: 133.167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 05:34:40,623]\u001b[0m Trial 7 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/82.. Train loss: 4.978.. Val loss: 4.037.. Train L1 norm: 83.983.. Val L1 norm: 6.195.. Train Linf norm: 2620.200.. Val Linf norm: 133.131\n",
            "Epoch 1/76.. Train loss: 2.696.. Val loss: 2.411.. Train L1 norm: 7.644.. Val L1 norm: 1.262.. Train Linf norm: 13331.965.. Val Linf norm: 280.606\n",
            "Epoch 2/76.. Train loss: 2.638.. Val loss: 2.374.. Train L1 norm: 18.884.. Val L1 norm: 1.454.. Train Linf norm: 36089.662.. Val Linf norm: 475.408\n",
            "Epoch 3/76.. Train loss: 2.662.. Val loss: 2.354.. Train L1 norm: 14.486.. Val L1 norm: 1.571.. Train Linf norm: 26950.096.. Val Linf norm: 591.638\n",
            "Epoch 4/76.. Train loss: 2.712.. Val loss: 2.343.. Train L1 norm: 21.197.. Val L1 norm: 1.648.. Train Linf norm: 40505.180.. Val Linf norm: 669.121\n",
            "Epoch 5/76.. Train loss: 2.611.. Val loss: 2.334.. Train L1 norm: 17.031.. Val L1 norm: 1.709.. Train Linf norm: 32013.459.. Val Linf norm: 730.973\n",
            "Epoch 6/76.. Train loss: 2.666.. Val loss: 2.329.. Train L1 norm: 10.217.. Val L1 norm: 1.755.. Train Linf norm: 17818.076.. Val Linf norm: 777.921\n",
            "Epoch 7/76.. Train loss: 2.710.. Val loss: 2.324.. Train L1 norm: 19.550.. Val L1 norm: 1.794.. Train Linf norm: 36949.992.. Val Linf norm: 817.979\n",
            "Epoch 8/76.. Train loss: 2.488.. Val loss: 2.320.. Train L1 norm: 9.349.. Val L1 norm: 1.830.. Train Linf norm: 16044.408.. Val Linf norm: 855.039\n",
            "Epoch 9/76.. Train loss: 2.711.. Val loss: 2.316.. Train L1 norm: 14.400.. Val L1 norm: 1.860.. Train Linf norm: 26370.348.. Val Linf norm: 886.097\n",
            "Epoch 10/76.. Train loss: 2.614.. Val loss: 2.313.. Train L1 norm: 24.839.. Val L1 norm: 1.886.. Train Linf norm: 47629.742.. Val Linf norm: 912.998\n",
            "Epoch 11/76.. Train loss: 2.625.. Val loss: 2.311.. Train L1 norm: 19.033.. Val L1 norm: 1.909.. Train Linf norm: 35793.752.. Val Linf norm: 936.572\n",
            "Epoch 12/76.. Train loss: 2.587.. Val loss: 2.309.. Train L1 norm: 22.996.. Val L1 norm: 1.930.. Train Linf norm: 43916.893.. Val Linf norm: 958.777\n",
            "Epoch 13/76.. Train loss: 2.616.. Val loss: 2.307.. Train L1 norm: 22.567.. Val L1 norm: 1.950.. Train Linf norm: 43018.249.. Val Linf norm: 979.539\n",
            "Epoch 14/76.. Train loss: 2.650.. Val loss: 2.305.. Train L1 norm: 13.358.. Val L1 norm: 1.968.. Train Linf norm: 24146.999.. Val Linf norm: 998.475\n",
            "Epoch 15/76.. Train loss: 2.599.. Val loss: 2.304.. Train L1 norm: 16.125.. Val L1 norm: 1.975.. Train Linf norm: 29388.913.. Val Linf norm: 1006.087\n",
            "Epoch 16/76.. Train loss: 2.543.. Val loss: 2.304.. Train L1 norm: 21.611.. Val L1 norm: 1.981.. Train Linf norm: 40990.665.. Val Linf norm: 1012.968\n",
            "Epoch 17/76.. Train loss: 2.581.. Val loss: 2.303.. Train L1 norm: 18.020.. Val L1 norm: 1.988.. Train Linf norm: 33629.573.. Val Linf norm: 1019.834\n",
            "Epoch 18/76.. Train loss: 2.613.. Val loss: 2.302.. Train L1 norm: 19.018.. Val L1 norm: 1.994.. Train Linf norm: 35669.002.. Val Linf norm: 1026.441\n",
            "Epoch 19/76.. Train loss: 2.464.. Val loss: 2.302.. Train L1 norm: 24.244.. Val L1 norm: 1.999.. Train Linf norm: 46342.602.. Val Linf norm: 1032.249\n",
            "Epoch 20/76.. Train loss: 2.550.. Val loss: 2.302.. Train L1 norm: 24.454.. Val L1 norm: 2.004.. Train Linf norm: 46769.116.. Val Linf norm: 1037.511\n",
            "Epoch 21/76.. Train loss: 2.633.. Val loss: 2.301.. Train L1 norm: 23.892.. Val L1 norm: 2.009.. Train Linf norm: 45620.822.. Val Linf norm: 1042.661\n",
            "Epoch 22/76.. Train loss: 2.593.. Val loss: 2.301.. Train L1 norm: 25.378.. Val L1 norm: 2.013.. Train Linf norm: 48658.549.. Val Linf norm: 1047.691\n",
            "Epoch 23/76.. Train loss: 2.560.. Val loss: 2.301.. Train L1 norm: 23.055.. Val L1 norm: 2.018.. Train Linf norm: 43933.971.. Val Linf norm: 1052.620\n",
            "Epoch 24/76.. Train loss: 2.617.. Val loss: 2.300.. Train L1 norm: 18.791.. Val L1 norm: 2.022.. Train Linf norm: 35157.747.. Val Linf norm: 1057.391\n",
            "Epoch 25/76.. Train loss: 2.548.. Val loss: 2.300.. Train L1 norm: 19.393.. Val L1 norm: 2.026.. Train Linf norm: 36366.416.. Val Linf norm: 1061.547\n",
            "Epoch 26/76.. Train loss: 2.596.. Val loss: 2.300.. Train L1 norm: 21.476.. Val L1 norm: 2.030.. Train Linf norm: 40626.354.. Val Linf norm: 1065.871\n",
            "Epoch 27/76.. Train loss: 2.651.. Val loss: 2.301.. Train L1 norm: 22.530.. Val L1 norm: 2.026.. Train Linf norm: 42717.816.. Val Linf norm: 1064.314\n",
            "Epoch 28/76.. Train loss: 2.567.. Val loss: 2.301.. Train L1 norm: 28.378.. Val L1 norm: 2.030.. Train Linf norm: 54699.009.. Val Linf norm: 1068.492\n",
            "Epoch 29/76.. Train loss: 2.354.. Val loss: 2.301.. Train L1 norm: 18.023.. Val L1 norm: 2.029.. Train Linf norm: 33581.013.. Val Linf norm: 1067.768\n",
            "Epoch 30/76.. Train loss: 2.553.. Val loss: 2.301.. Train L1 norm: 11.206.. Val L1 norm: 2.030.. Train Linf norm: 19637.681.. Val Linf norm: 1069.614\n",
            "Epoch 31/76.. Train loss: 2.627.. Val loss: 2.301.. Train L1 norm: 21.254.. Val L1 norm: 2.032.. Train Linf norm: 40204.962.. Val Linf norm: 1071.393\n",
            "Epoch 32/76.. Train loss: 2.586.. Val loss: 2.301.. Train L1 norm: 20.803.. Val L1 norm: 2.034.. Train Linf norm: 39157.935.. Val Linf norm: 1073.104\n",
            "Epoch 33/76.. Train loss: 2.540.. Val loss: 2.301.. Train L1 norm: 20.637.. Val L1 norm: 2.035.. Train Linf norm: 38951.651.. Val Linf norm: 1074.687\n",
            "Epoch 34/76.. Train loss: 2.554.. Val loss: 2.301.. Train L1 norm: 24.175.. Val L1 norm: 2.037.. Train Linf norm: 46106.048.. Val Linf norm: 1076.351\n",
            "Epoch 35/76.. Train loss: 2.444.. Val loss: 2.300.. Train L1 norm: 14.744.. Val L1 norm: 2.038.. Train Linf norm: 26847.062.. Val Linf norm: 1077.957\n",
            "Epoch 36/76.. Train loss: 2.571.. Val loss: 2.300.. Train L1 norm: 24.381.. Val L1 norm: 2.040.. Train Linf norm: 46624.888.. Val Linf norm: 1079.622\n",
            "Epoch 37/76.. Train loss: 2.481.. Val loss: 2.300.. Train L1 norm: 19.565.. Val L1 norm: 2.041.. Train Linf norm: 36624.262.. Val Linf norm: 1081.068\n",
            "Epoch 38/76.. Train loss: 2.560.. Val loss: 2.300.. Train L1 norm: 19.787.. Val L1 norm: 2.042.. Train Linf norm: 37162.820.. Val Linf norm: 1082.478\n",
            "Epoch 39/76.. Train loss: 2.525.. Val loss: 2.300.. Train L1 norm: 15.123.. Val L1 norm: 2.043.. Train Linf norm: 27617.616.. Val Linf norm: 1083.860\n",
            "Epoch 40/76.. Train loss: 2.660.. Val loss: 2.300.. Train L1 norm: 15.777.. Val L1 norm: 2.045.. Train Linf norm: 28559.407.. Val Linf norm: 1085.195\n",
            "Epoch 41/76.. Train loss: 2.548.. Val loss: 2.300.. Train L1 norm: 20.308.. Val L1 norm: 2.046.. Train Linf norm: 38242.534.. Val Linf norm: 1086.509\n",
            "Epoch 42/76.. Train loss: 2.558.. Val loss: 2.300.. Train L1 norm: 16.448.. Val L1 norm: 2.047.. Train Linf norm: 30341.178.. Val Linf norm: 1087.721\n",
            "Epoch 43/76.. Train loss: 2.619.. Val loss: 2.300.. Train L1 norm: 20.300.. Val L1 norm: 2.047.. Train Linf norm: 37806.138.. Val Linf norm: 1088.265\n",
            "Epoch 44/76.. Train loss: 2.557.. Val loss: 2.300.. Train L1 norm: 17.283.. Val L1 norm: 2.048.. Train Linf norm: 32046.245.. Val Linf norm: 1088.786\n",
            "Epoch 45/76.. Train loss: 2.441.. Val loss: 2.300.. Train L1 norm: 19.057.. Val L1 norm: 2.048.. Train Linf norm: 35122.677.. Val Linf norm: 1089.307\n",
            "Epoch 46/76.. Train loss: 2.571.. Val loss: 2.300.. Train L1 norm: 18.663.. Val L1 norm: 2.049.. Train Linf norm: 34853.581.. Val Linf norm: 1089.782\n",
            "Epoch 47/76.. Train loss: 2.576.. Val loss: 2.300.. Train L1 norm: 21.610.. Val L1 norm: 2.049.. Train Linf norm: 40824.675.. Val Linf norm: 1090.233\n",
            "Epoch 48/76.. Train loss: 2.512.. Val loss: 2.300.. Train L1 norm: 26.516.. Val L1 norm: 2.049.. Train Linf norm: 50954.542.. Val Linf norm: 1090.677\n",
            "Epoch 49/76.. Train loss: 2.575.. Val loss: 2.300.. Train L1 norm: 20.004.. Val L1 norm: 2.050.. Train Linf norm: 37572.402.. Val Linf norm: 1091.114\n",
            "Epoch 50/76.. Train loss: 2.562.. Val loss: 2.300.. Train L1 norm: 23.983.. Val L1 norm: 2.050.. Train Linf norm: 45761.755.. Val Linf norm: 1091.497\n",
            "Epoch 51/76.. Train loss: 2.589.. Val loss: 2.300.. Train L1 norm: 17.772.. Val L1 norm: 2.050.. Train Linf norm: 33010.803.. Val Linf norm: 1091.887\n",
            "Epoch 52/76.. Train loss: 2.637.. Val loss: 2.300.. Train L1 norm: 22.603.. Val L1 norm: 2.051.. Train Linf norm: 42986.452.. Val Linf norm: 1092.259\n",
            "Epoch 53/76.. Train loss: 2.528.. Val loss: 2.300.. Train L1 norm: 29.048.. Val L1 norm: 2.051.. Train Linf norm: 56119.131.. Val Linf norm: 1092.640\n",
            "Epoch 54/76.. Train loss: 2.416.. Val loss: 2.300.. Train L1 norm: 21.018.. Val L1 norm: 2.051.. Train Linf norm: 39671.570.. Val Linf norm: 1093.040\n",
            "Epoch 55/76.. Train loss: 2.539.. Val loss: 2.300.. Train L1 norm: 21.587.. Val L1 norm: 2.052.. Train Linf norm: 40870.918.. Val Linf norm: 1093.406\n",
            "Epoch 56/76.. Train loss: 2.490.. Val loss: 2.300.. Train L1 norm: 22.544.. Val L1 norm: 2.052.. Train Linf norm: 42854.074.. Val Linf norm: 1093.791\n",
            "Epoch 57/76.. Train loss: 2.640.. Val loss: 2.300.. Train L1 norm: 22.741.. Val L1 norm: 2.052.. Train Linf norm: 43238.074.. Val Linf norm: 1093.873\n",
            "Epoch 58/76.. Train loss: 2.543.. Val loss: 2.300.. Train L1 norm: 25.732.. Val L1 norm: 2.052.. Train Linf norm: 49352.481.. Val Linf norm: 1093.988\n",
            "Epoch 59/76.. Train loss: 2.539.. Val loss: 2.300.. Train L1 norm: 19.355.. Val L1 norm: 2.052.. Train Linf norm: 36270.490.. Val Linf norm: 1094.099\n",
            "Epoch 60/76.. Train loss: 2.553.. Val loss: 2.300.. Train L1 norm: 19.973.. Val L1 norm: 2.052.. Train Linf norm: 37513.633.. Val Linf norm: 1094.206\n",
            "Epoch 61/76.. Train loss: 2.589.. Val loss: 2.300.. Train L1 norm: 19.878.. Val L1 norm: 2.052.. Train Linf norm: 37258.672.. Val Linf norm: 1094.302\n",
            "Epoch 62/76.. Train loss: 2.388.. Val loss: 2.300.. Train L1 norm: 23.477.. Val L1 norm: 2.052.. Train Linf norm: 44601.374.. Val Linf norm: 1094.403\n",
            "Epoch 63/76.. Train loss: 2.563.. Val loss: 2.300.. Train L1 norm: 21.208.. Val L1 norm: 2.052.. Train Linf norm: 40038.355.. Val Linf norm: 1094.495\n",
            "Epoch 64/76.. Train loss: 2.467.. Val loss: 2.300.. Train L1 norm: 16.971.. Val L1 norm: 2.052.. Train Linf norm: 31371.243.. Val Linf norm: 1094.599\n",
            "Epoch 65/76.. Train loss: 2.544.. Val loss: 2.300.. Train L1 norm: 21.344.. Val L1 norm: 2.052.. Train Linf norm: 40345.890.. Val Linf norm: 1094.679\n",
            "Epoch 66/76.. Train loss: 2.429.. Val loss: 2.300.. Train L1 norm: 26.300.. Val L1 norm: 2.052.. Train Linf norm: 50561.908.. Val Linf norm: 1094.788\n",
            "Epoch 67/76.. Train loss: 2.519.. Val loss: 2.300.. Train L1 norm: 25.471.. Val L1 norm: 2.052.. Train Linf norm: 48831.424.. Val Linf norm: 1094.877\n",
            "Epoch 68/76.. Train loss: 2.621.. Val loss: 2.300.. Train L1 norm: 23.705.. Val L1 norm: 2.052.. Train Linf norm: 45203.501.. Val Linf norm: 1094.961\n",
            "Epoch 69/76.. Train loss: 2.521.. Val loss: 2.300.. Train L1 norm: 19.246.. Val L1 norm: 2.053.. Train Linf norm: 36019.919.. Val Linf norm: 1095.040\n",
            "Epoch 70/76.. Train loss: 2.629.. Val loss: 2.300.. Train L1 norm: 16.955.. Val L1 norm: 2.053.. Train Linf norm: 31395.842.. Val Linf norm: 1095.109\n",
            "Epoch 71/76.. Train loss: 2.515.. Val loss: 2.300.. Train L1 norm: 15.745.. Val L1 norm: 2.053.. Train Linf norm: 28891.605.. Val Linf norm: 1095.117\n",
            "Epoch 72/76.. Train loss: 2.484.. Val loss: 2.300.. Train L1 norm: 23.170.. Val L1 norm: 2.053.. Train Linf norm: 44093.225.. Val Linf norm: 1095.141\n",
            "Epoch 73/76.. Train loss: 2.480.. Val loss: 2.300.. Train L1 norm: 11.820.. Val L1 norm: 2.053.. Train Linf norm: 20847.630.. Val Linf norm: 1095.160\n",
            "Epoch 74/76.. Train loss: 2.551.. Val loss: 2.300.. Train L1 norm: 20.621.. Val L1 norm: 2.053.. Train Linf norm: 38888.372.. Val Linf norm: 1095.178\n",
            "Epoch 75/76.. Train loss: 2.567.. Val loss: 2.300.. Train L1 norm: 22.217.. Val L1 norm: 2.053.. Train Linf norm: 42111.353.. Val Linf norm: 1095.196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 05:37:46,157]\u001b[0m Trial 8 finished with value: 2.052531473223368 and parameters: {'n_layers': 5, 'n_units_0': 389, 'n_units_1': 2298, 'n_units_2': 2727, 'n_units_3': 1407, 'n_units_4': 1708, 'hidden_activation': 'SoftPlus', 'output_activation': 'Linear', 'loss': 'Quantile', 'optimizer': 'Adagrad', 'lr': 3.6285475205836346e-06, 'batch_size': 2048, 'n_epochs': 76, 'scheduler': 'StepLR', 'softplus_beta': 1.0680991488570912, 'dropout_rate': 0.14278371333213874, 'step_size': 14, 'gamma': 0.47171326354053933}. Best is trial 4 with value: 1.30382172832489.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 76/76.. Train loss: 2.567.. Val loss: 2.300.. Train L1 norm: 22.206.. Val L1 norm: 2.053.. Train Linf norm: 42064.291.. Val Linf norm: 1095.213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 05:37:47,944]\u001b[0m Trial 9 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/106.. Train loss: 5.401.. Val loss: 3.794.. Train L1 norm: 354.447.. Val L1 norm: 5.436.. Train Linf norm: 89836.231.. Val Linf norm: 705.663\n",
            "Epoch 1/145.. Train loss: 8.379.. Val loss: 4.312.. Train L1 norm: 60.370.. Val L1 norm: 3.062.. Train Linf norm: 29129.410.. Val Linf norm: 574.640\n",
            "Epoch 2/145.. Train loss: 5.066.. Val loss: 3.846.. Train L1 norm: 76.064.. Val L1 norm: 4.746.. Train Linf norm: 37590.781.. Val Linf norm: 1050.389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 05:38:00,488]\u001b[0m Trial 10 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/145.. Train loss: 4.699.. Val loss: 4.021.. Train L1 norm: 78.130.. Val L1 norm: 3.896.. Train Linf norm: 28023.471.. Val Linf norm: 917.885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-77-ddc3c9d46736>:149: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
            "<ipython-input-77-ddc3c9d46736>:150: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  beta1 = trial.suggest_uniform(\"beta1\", 0.9, 0.999)\n",
            "<ipython-input-77-ddc3c9d46736>:151: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  beta2 = trial.suggest_uniform(\"beta2\", 0.999, 0.9999)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55.. Train loss: 372934.830.. Val loss: 937.782.. Train L1 norm: 59.914.. Val L1 norm: 2.411.. Train Linf norm: 59383.138.. Val Linf norm: 609.599\n",
            "Epoch 2/55.. Train loss: 109365.716.. Val loss: 574.757.. Train L1 norm: 34.385.. Val L1 norm: 2.192.. Train Linf norm: 33393.546.. Val Linf norm: 472.424\n",
            "Epoch 3/55.. Train loss: 2786.228.. Val loss: 485.219.. Train L1 norm: 42.871.. Val L1 norm: 2.445.. Train Linf norm: 41974.510.. Val Linf norm: 624.270\n",
            "Epoch 4/55.. Train loss: 4225.760.. Val loss: 432.569.. Train L1 norm: 50.757.. Val L1 norm: 2.642.. Train Linf norm: 49993.455.. Val Linf norm: 728.089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 05:38:07,061]\u001b[0m Trial 11 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/55.. Train loss: 1172.740.. Val loss: 430.780.. Train L1 norm: 41.667.. Val L1 norm: 2.644.. Train Linf norm: 40654.826.. Val Linf norm: 729.283\n",
            "Epoch 1/127.. Train loss: 8655747590464.767.. Val loss: 55.937.. Train L1 norm: 75713.585.. Val L1 norm: 1.091.. Train Linf norm: 5354138.933.. Val Linf norm: 64.305\n",
            "Epoch 2/127.. Train loss: 61.378.. Val loss: 55.937.. Train L1 norm: 2.523.. Val L1 norm: 1.091.. Train Linf norm: 1529.151.. Val Linf norm: 64.340\n",
            "Epoch 3/127.. Train loss: 61.378.. Val loss: 55.937.. Train L1 norm: 2.524.. Val L1 norm: 1.091.. Train Linf norm: 1529.306.. Val Linf norm: 64.338\n",
            "Epoch 4/127.. Train loss: 61.386.. Val loss: 55.937.. Train L1 norm: 2.524.. Val L1 norm: 1.091.. Train Linf norm: 1530.223.. Val Linf norm: 64.336\n",
            "Epoch 5/127.. Train loss: 61.378.. Val loss: 55.937.. Train L1 norm: 2.523.. Val L1 norm: 1.091.. Train Linf norm: 1529.196.. Val Linf norm: 64.333\n",
            "Epoch 6/127.. Train loss: 61.378.. Val loss: 55.937.. Train L1 norm: 2.523.. Val L1 norm: 1.091.. Train Linf norm: 1529.494.. Val Linf norm: 64.330\n",
            "Epoch 7/127.. Train loss: 61.378.. Val loss: 55.937.. Train L1 norm: 2.523.. Val L1 norm: 1.091.. Train Linf norm: 1530.052.. Val Linf norm: 64.326\n",
            "Epoch 8/127.. Train loss: 61.378.. Val loss: 55.937.. Train L1 norm: 2.523.. Val L1 norm: 1.091.. Train Linf norm: 1529.429.. Val Linf norm: 64.323\n",
            "Epoch 9/127.. Train loss: 61.378.. Val loss: 55.937.. Train L1 norm: 2.523.. Val L1 norm: 1.091.. Train Linf norm: 1529.063.. Val Linf norm: 64.320\n",
            "Epoch 10/127.. Train loss: 61.378.. Val loss: 55.937.. Train L1 norm: 2.523.. Val L1 norm: 1.091.. Train Linf norm: 1529.583.. Val Linf norm: 64.317\n",
            "Epoch 11/127.. Train loss: 61.590.. Val loss: 55.937.. Train L1 norm: 2.523.. Val L1 norm: 1.091.. Train Linf norm: 1530.350.. Val Linf norm: 64.313\n",
            "Epoch 12/127.. Train loss: 61.378.. Val loss: 55.937.. Train L1 norm: 2.523.. Val L1 norm: 1.091.. Train Linf norm: 1530.168.. Val Linf norm: 64.310\n",
            "Epoch 13/127.. Train loss: 425.868.. Val loss: 55.937.. Train L1 norm: 3.738.. Val L1 norm: 1.091.. Train Linf norm: 2143.841.. Val Linf norm: 64.306\n",
            "Epoch 14/127.. Train loss: 61.378.. Val loss: 55.937.. Train L1 norm: 2.523.. Val L1 norm: 1.091.. Train Linf norm: 1528.225.. Val Linf norm: 64.302\n",
            "Epoch 15/127.. Train loss: 61.378.. Val loss: 55.937.. Train L1 norm: 2.523.. Val L1 norm: 1.091.. Train Linf norm: 1527.451.. Val Linf norm: 64.299\n",
            "Epoch 16/127.. Train loss: 61.378.. Val loss: 55.937.. Train L1 norm: 2.523.. Val L1 norm: 1.091.. Train Linf norm: 1527.538.. Val Linf norm: 64.295\n",
            "Epoch 17/127.. Train loss: 61.378.. Val loss: 55.937.. Train L1 norm: 2.522.. Val L1 norm: 1.091.. Train Linf norm: 1526.186.. Val Linf norm: 64.291\n",
            "Epoch 18/127.. Train loss: 61.378.. Val loss: 55.937.. Train L1 norm: 2.522.. Val L1 norm: 1.091.. Train Linf norm: 1521.940.. Val Linf norm: 64.287\n",
            "Epoch 19/127.. Train loss: 61.378.. Val loss: 55.937.. Train L1 norm: 2.522.. Val L1 norm: 1.091.. Train Linf norm: 1528.515.. Val Linf norm: 64.282\n",
            "Epoch 20/127.. Train loss: 61.377.. Val loss: 55.937.. Train L1 norm: 2.522.. Val L1 norm: 1.091.. Train Linf norm: 1528.425.. Val Linf norm: 64.277\n",
            "Epoch 21/127.. Train loss: 61.378.. Val loss: 55.937.. Train L1 norm: 2.522.. Val L1 norm: 1.091.. Train Linf norm: 1528.442.. Val Linf norm: 64.272\n",
            "Epoch 22/127.. Train loss: 61.377.. Val loss: 55.937.. Train L1 norm: 2.522.. Val L1 norm: 1.091.. Train Linf norm: 1527.146.. Val Linf norm: 64.266\n",
            "Epoch 23/127.. Train loss: 61.377.. Val loss: 55.936.. Train L1 norm: 2.522.. Val L1 norm: 1.091.. Train Linf norm: 1528.564.. Val Linf norm: 64.260\n",
            "Epoch 24/127.. Train loss: 61.377.. Val loss: 55.936.. Train L1 norm: 2.522.. Val L1 norm: 1.091.. Train Linf norm: 581.527.. Val Linf norm: 64.253\n",
            "Epoch 25/127.. Train loss: 61.377.. Val loss: 55.936.. Train L1 norm: 2.521.. Val L1 norm: 1.091.. Train Linf norm: 1526.246.. Val Linf norm: 64.246\n",
            "Epoch 26/127.. Train loss: 61.377.. Val loss: 55.936.. Train L1 norm: 2.521.. Val L1 norm: 1.091.. Train Linf norm: 1524.930.. Val Linf norm: 64.238\n",
            "Epoch 27/127.. Train loss: 61.377.. Val loss: 55.936.. Train L1 norm: 2.521.. Val L1 norm: 1.091.. Train Linf norm: 1527.354.. Val Linf norm: 64.230\n",
            "Epoch 28/127.. Train loss: 61.377.. Val loss: 55.936.. Train L1 norm: 2.521.. Val L1 norm: 1.091.. Train Linf norm: 1525.870.. Val Linf norm: 64.222\n",
            "Epoch 29/127.. Train loss: 61.377.. Val loss: 55.936.. Train L1 norm: 2.521.. Val L1 norm: 1.091.. Train Linf norm: 1526.467.. Val Linf norm: 64.213\n",
            "Epoch 30/127.. Train loss: 61.377.. Val loss: 55.936.. Train L1 norm: 2.520.. Val L1 norm: 1.091.. Train Linf norm: 1528.044.. Val Linf norm: 64.205\n",
            "Epoch 31/127.. Train loss: 61.377.. Val loss: 55.936.. Train L1 norm: 2.520.. Val L1 norm: 1.091.. Train Linf norm: 1527.412.. Val Linf norm: 64.196\n",
            "Epoch 32/127.. Train loss: 61.376.. Val loss: 55.936.. Train L1 norm: 2.520.. Val L1 norm: 1.091.. Train Linf norm: 1526.442.. Val Linf norm: 64.187\n",
            "Epoch 33/127.. Train loss: 61.376.. Val loss: 55.936.. Train L1 norm: 2.520.. Val L1 norm: 1.091.. Train Linf norm: 1524.771.. Val Linf norm: 64.179\n",
            "Epoch 34/127.. Train loss: 61.376.. Val loss: 55.935.. Train L1 norm: 2.520.. Val L1 norm: 1.091.. Train Linf norm: 1526.850.. Val Linf norm: 64.170\n",
            "Epoch 35/127.. Train loss: 61.376.. Val loss: 55.935.. Train L1 norm: 2.519.. Val L1 norm: 1.091.. Train Linf norm: 1525.278.. Val Linf norm: 64.162\n",
            "Epoch 36/127.. Train loss: 61.376.. Val loss: 55.935.. Train L1 norm: 2.519.. Val L1 norm: 1.090.. Train Linf norm: 1525.466.. Val Linf norm: 64.154\n",
            "Epoch 37/127.. Train loss: 61.376.. Val loss: 55.935.. Train L1 norm: 2.519.. Val L1 norm: 1.090.. Train Linf norm: 1524.703.. Val Linf norm: 64.146\n",
            "Epoch 38/127.. Train loss: 61.376.. Val loss: 55.935.. Train L1 norm: 2.519.. Val L1 norm: 1.090.. Train Linf norm: 1524.582.. Val Linf norm: 64.139\n",
            "Epoch 39/127.. Train loss: 100956.565.. Val loss: 1010400.274.. Train L1 norm: 25.485.. Val L1 norm: 87.921.. Train Linf norm: 3967.364.. Val Linf norm: 30731.532\n",
            "Epoch 40/127.. Train loss: 69439.037.. Val loss: 55.935.. Train L1 norm: 3.236.. Val L1 norm: 1.091.. Train Linf norm: 1596.386.. Val Linf norm: 64.162\n",
            "Epoch 41/127.. Train loss: 61.497.. Val loss: 55.935.. Train L1 norm: 2.519.. Val L1 norm: 1.090.. Train Linf norm: 1526.461.. Val Linf norm: 64.155\n",
            "Epoch 42/127.. Train loss: 61.376.. Val loss: 55.935.. Train L1 norm: 2.519.. Val L1 norm: 1.090.. Train Linf norm: 1525.084.. Val Linf norm: 64.148\n",
            "Epoch 43/127.. Train loss: 1911.228.. Val loss: 55.935.. Train L1 norm: 2.603.. Val L1 norm: 1.090.. Train Linf norm: 1602.982.. Val Linf norm: 64.142\n",
            "Epoch 44/127.. Train loss: 111.746.. Val loss: 55.935.. Train L1 norm: 3.248.. Val L1 norm: 1.090.. Train Linf norm: 2270.028.. Val Linf norm: 64.135\n",
            "Epoch 45/127.. Train loss: 61.661.. Val loss: 55.935.. Train L1 norm: 2.521.. Val L1 norm: 1.090.. Train Linf norm: 1527.079.. Val Linf norm: 64.127\n",
            "Epoch 46/127.. Train loss: 61.954.. Val loss: 55.935.. Train L1 norm: 2.523.. Val L1 norm: 1.090.. Train Linf norm: 1528.556.. Val Linf norm: 64.120\n",
            "Epoch 47/127.. Train loss: 61.376.. Val loss: 55.935.. Train L1 norm: 2.518.. Val L1 norm: 1.090.. Train Linf norm: 1525.569.. Val Linf norm: 64.112\n",
            "Epoch 48/127.. Train loss: 61.401.. Val loss: 55.935.. Train L1 norm: 2.518.. Val L1 norm: 1.090.. Train Linf norm: 1522.900.. Val Linf norm: 64.103\n",
            "Epoch 49/127.. Train loss: 1124.654.. Val loss: 55.935.. Train L1 norm: 2.893.. Val L1 norm: 1.090.. Train Linf norm: 1904.182.. Val Linf norm: 64.093\n",
            "Epoch 50/127.. Train loss: 61.375.. Val loss: 55.934.. Train L1 norm: 2.518.. Val L1 norm: 1.090.. Train Linf norm: 1524.182.. Val Linf norm: 64.083\n",
            "Epoch 51/127.. Train loss: 61.375.. Val loss: 55.934.. Train L1 norm: 2.517.. Val L1 norm: 1.090.. Train Linf norm: 1523.700.. Val Linf norm: 64.073\n",
            "Epoch 52/127.. Train loss: 61.375.. Val loss: 55.934.. Train L1 norm: 2.517.. Val L1 norm: 1.090.. Train Linf norm: 1522.153.. Val Linf norm: 64.061\n",
            "Epoch 53/127.. Train loss: 61.375.. Val loss: 55.934.. Train L1 norm: 2.517.. Val L1 norm: 1.090.. Train Linf norm: 1522.372.. Val Linf norm: 64.050\n",
            "Epoch 54/127.. Train loss: 61.375.. Val loss: 55.934.. Train L1 norm: 2.516.. Val L1 norm: 1.090.. Train Linf norm: 1522.507.. Val Linf norm: 64.037\n",
            "Epoch 55/127.. Train loss: 61.375.. Val loss: 55.934.. Train L1 norm: 2.516.. Val L1 norm: 1.090.. Train Linf norm: 1522.067.. Val Linf norm: 64.024\n",
            "Epoch 56/127.. Train loss: 61.374.. Val loss: 55.934.. Train L1 norm: 2.516.. Val L1 norm: 1.090.. Train Linf norm: 1521.042.. Val Linf norm: 64.011\n",
            "Epoch 57/127.. Train loss: 61.374.. Val loss: 55.933.. Train L1 norm: 2.515.. Val L1 norm: 1.090.. Train Linf norm: 1520.030.. Val Linf norm: 63.998\n",
            "Epoch 58/127.. Train loss: 61.374.. Val loss: 55.933.. Train L1 norm: 2.515.. Val L1 norm: 1.090.. Train Linf norm: 1523.009.. Val Linf norm: 63.984\n",
            "Epoch 59/127.. Train loss: 61.374.. Val loss: 55.933.. Train L1 norm: 2.515.. Val L1 norm: 1.090.. Train Linf norm: 1520.321.. Val Linf norm: 63.971\n",
            "Epoch 60/127.. Train loss: 61.374.. Val loss: 55.933.. Train L1 norm: 2.515.. Val L1 norm: 1.090.. Train Linf norm: 1521.430.. Val Linf norm: 63.957\n",
            "Epoch 61/127.. Train loss: 61.374.. Val loss: 55.933.. Train L1 norm: 2.514.. Val L1 norm: 1.090.. Train Linf norm: 1519.285.. Val Linf norm: 63.944\n",
            "Epoch 62/127.. Train loss: 61.374.. Val loss: 55.933.. Train L1 norm: 2.514.. Val L1 norm: 1.090.. Train Linf norm: 1519.708.. Val Linf norm: 63.931\n",
            "Epoch 63/127.. Train loss: 61.373.. Val loss: 55.933.. Train L1 norm: 2.514.. Val L1 norm: 1.090.. Train Linf norm: 1513.855.. Val Linf norm: 63.919\n",
            "Epoch 64/127.. Train loss: 61.373.. Val loss: 55.932.. Train L1 norm: 2.513.. Val L1 norm: 1.090.. Train Linf norm: 1517.249.. Val Linf norm: 63.907\n",
            "Epoch 65/127.. Train loss: 61.373.. Val loss: 55.932.. Train L1 norm: 2.513.. Val L1 norm: 1.090.. Train Linf norm: 1519.729.. Val Linf norm: 63.895\n",
            "Epoch 66/127.. Train loss: 61.373.. Val loss: 55.932.. Train L1 norm: 2.513.. Val L1 norm: 1.090.. Train Linf norm: 1519.755.. Val Linf norm: 63.884\n",
            "Epoch 67/127.. Train loss: 61.373.. Val loss: 55.932.. Train L1 norm: 2.513.. Val L1 norm: 1.090.. Train Linf norm: 1518.543.. Val Linf norm: 63.874\n",
            "Epoch 68/127.. Train loss: 61.373.. Val loss: 55.932.. Train L1 norm: 2.512.. Val L1 norm: 1.090.. Train Linf norm: 1517.695.. Val Linf norm: 63.864\n",
            "Epoch 69/127.. Train loss: 61.373.. Val loss: 55.932.. Train L1 norm: 2.512.. Val L1 norm: 1.090.. Train Linf norm: 1516.859.. Val Linf norm: 63.853\n",
            "Epoch 70/127.. Train loss: 61.373.. Val loss: 55.932.. Train L1 norm: 2.512.. Val L1 norm: 1.090.. Train Linf norm: 1518.274.. Val Linf norm: 63.843\n",
            "Epoch 71/127.. Train loss: 61.372.. Val loss: 55.932.. Train L1 norm: 2.512.. Val L1 norm: 1.090.. Train Linf norm: 1517.454.. Val Linf norm: 63.833\n",
            "Epoch 72/127.. Train loss: 61.372.. Val loss: 55.931.. Train L1 norm: 2.511.. Val L1 norm: 1.090.. Train Linf norm: 1517.987.. Val Linf norm: 63.823\n",
            "Epoch 73/127.. Train loss: 61.372.. Val loss: 55.931.. Train L1 norm: 2.511.. Val L1 norm: 1.090.. Train Linf norm: 1516.794.. Val Linf norm: 63.813\n",
            "Epoch 74/127.. Train loss: 61.372.. Val loss: 55.931.. Train L1 norm: 2.511.. Val L1 norm: 1.090.. Train Linf norm: 1516.330.. Val Linf norm: 63.802\n",
            "Epoch 75/127.. Train loss: 61.372.. Val loss: 55.931.. Train L1 norm: 2.511.. Val L1 norm: 1.090.. Train Linf norm: 1516.111.. Val Linf norm: 63.790\n",
            "Epoch 76/127.. Train loss: 61.372.. Val loss: 55.931.. Train L1 norm: 2.510.. Val L1 norm: 1.090.. Train Linf norm: 1516.323.. Val Linf norm: 63.778\n",
            "Epoch 77/127.. Train loss: 61.372.. Val loss: 55.931.. Train L1 norm: 2.510.. Val L1 norm: 1.090.. Train Linf norm: 1515.422.. Val Linf norm: 63.764\n",
            "Epoch 78/127.. Train loss: 61.371.. Val loss: 55.931.. Train L1 norm: 2.510.. Val L1 norm: 1.090.. Train Linf norm: 1515.543.. Val Linf norm: 63.750\n",
            "Epoch 79/127.. Train loss: 61.371.. Val loss: 55.930.. Train L1 norm: 2.509.. Val L1 norm: 1.090.. Train Linf norm: 1516.357.. Val Linf norm: 63.735\n",
            "Epoch 80/127.. Train loss: 61.371.. Val loss: 55.930.. Train L1 norm: 2.509.. Val L1 norm: 1.090.. Train Linf norm: 1512.019.. Val Linf norm: 63.720\n",
            "Epoch 81/127.. Train loss: 61.371.. Val loss: 55.930.. Train L1 norm: 2.508.. Val L1 norm: 1.090.. Train Linf norm: 1514.944.. Val Linf norm: 63.703\n",
            "Epoch 82/127.. Train loss: 62.197.. Val loss: 55.930.. Train L1 norm: 2.518.. Val L1 norm: 1.090.. Train Linf norm: 1520.817.. Val Linf norm: 63.686\n",
            "Epoch 83/127.. Train loss: 61.371.. Val loss: 55.930.. Train L1 norm: 2.508.. Val L1 norm: 1.090.. Train Linf norm: 1512.250.. Val Linf norm: 63.668\n",
            "Epoch 84/127.. Train loss: 61.370.. Val loss: 55.929.. Train L1 norm: 2.507.. Val L1 norm: 1.090.. Train Linf norm: 1512.571.. Val Linf norm: 63.650\n",
            "Epoch 85/127.. Train loss: 61.370.. Val loss: 55.929.. Train L1 norm: 2.507.. Val L1 norm: 1.090.. Train Linf norm: 1513.407.. Val Linf norm: 63.631\n",
            "Epoch 86/127.. Train loss: 61.370.. Val loss: 55.929.. Train L1 norm: 2.506.. Val L1 norm: 1.090.. Train Linf norm: 1512.261.. Val Linf norm: 63.613\n",
            "Epoch 87/127.. Train loss: 61.370.. Val loss: 55.929.. Train L1 norm: 2.506.. Val L1 norm: 1.090.. Train Linf norm: 1512.733.. Val Linf norm: 63.594\n",
            "Epoch 88/127.. Train loss: 61.369.. Val loss: 55.929.. Train L1 norm: 2.505.. Val L1 norm: 1.090.. Train Linf norm: 1509.044.. Val Linf norm: 63.576\n",
            "Epoch 89/127.. Train loss: 61.369.. Val loss: 55.928.. Train L1 norm: 2.505.. Val L1 norm: 1.090.. Train Linf norm: 1510.418.. Val Linf norm: 63.558\n",
            "Epoch 90/127.. Train loss: 61.369.. Val loss: 55.928.. Train L1 norm: 2.505.. Val L1 norm: 1.090.. Train Linf norm: 1512.016.. Val Linf norm: 63.540\n",
            "Epoch 91/127.. Train loss: 61.369.. Val loss: 55.928.. Train L1 norm: 2.504.. Val L1 norm: 1.089.. Train Linf norm: 1510.946.. Val Linf norm: 63.523\n",
            "Epoch 92/127.. Train loss: 61.369.. Val loss: 55.928.. Train L1 norm: 2.504.. Val L1 norm: 1.089.. Train Linf norm: 574.076.. Val Linf norm: 63.507\n",
            "Epoch 93/127.. Train loss: 61.368.. Val loss: 55.928.. Train L1 norm: 2.503.. Val L1 norm: 1.089.. Train Linf norm: 1509.166.. Val Linf norm: 63.491\n",
            "Epoch 94/127.. Train loss: 61.368.. Val loss: 55.927.. Train L1 norm: 2.503.. Val L1 norm: 1.089.. Train Linf norm: 1507.088.. Val Linf norm: 63.476\n",
            "Epoch 95/127.. Train loss: 61.368.. Val loss: 55.927.. Train L1 norm: 2.503.. Val L1 norm: 1.089.. Train Linf norm: 1507.717.. Val Linf norm: 63.462\n",
            "Epoch 96/127.. Train loss: 61.368.. Val loss: 55.927.. Train L1 norm: 2.503.. Val L1 norm: 1.089.. Train Linf norm: 1508.627.. Val Linf norm: 63.448\n",
            "Epoch 97/127.. Train loss: 61.368.. Val loss: 55.927.. Train L1 norm: 2.502.. Val L1 norm: 1.089.. Train Linf norm: 1508.687.. Val Linf norm: 63.435\n",
            "Epoch 98/127.. Train loss: 1532.300.. Val loss: 55.927.. Train L1 norm: 2.509.. Val L1 norm: 1.089.. Train Linf norm: 1513.329.. Val Linf norm: 63.422\n",
            "Epoch 99/127.. Train loss: 61.368.. Val loss: 55.927.. Train L1 norm: 2.501.. Val L1 norm: 1.089.. Train Linf norm: 1508.390.. Val Linf norm: 63.408\n",
            "Epoch 100/127.. Train loss: 61.367.. Val loss: 55.927.. Train L1 norm: 2.501.. Val L1 norm: 1.089.. Train Linf norm: 1506.280.. Val Linf norm: 63.395\n",
            "Epoch 101/127.. Train loss: 61.367.. Val loss: 55.926.. Train L1 norm: 2.501.. Val L1 norm: 1.089.. Train Linf norm: 1496.996.. Val Linf norm: 63.381\n",
            "Epoch 102/127.. Train loss: 61.367.. Val loss: 55.926.. Train L1 norm: 2.500.. Val L1 norm: 1.089.. Train Linf norm: 1506.251.. Val Linf norm: 63.366\n",
            "Epoch 103/127.. Train loss: 61.367.. Val loss: 55.926.. Train L1 norm: 2.500.. Val L1 norm: 1.089.. Train Linf norm: 1503.070.. Val Linf norm: 63.350\n",
            "Epoch 104/127.. Train loss: 61.367.. Val loss: 55.926.. Train L1 norm: 2.500.. Val L1 norm: 1.089.. Train Linf norm: 1505.420.. Val Linf norm: 63.334\n",
            "Epoch 105/127.. Train loss: 61.366.. Val loss: 55.926.. Train L1 norm: 2.499.. Val L1 norm: 1.089.. Train Linf norm: 1506.181.. Val Linf norm: 63.317\n",
            "Epoch 106/127.. Train loss: 61.366.. Val loss: 55.925.. Train L1 norm: 2.499.. Val L1 norm: 1.089.. Train Linf norm: 1505.093.. Val Linf norm: 63.298\n",
            "Epoch 107/127.. Train loss: 61.366.. Val loss: 55.925.. Train L1 norm: 2.498.. Val L1 norm: 1.089.. Train Linf norm: 1504.707.. Val Linf norm: 63.278\n",
            "Epoch 108/127.. Train loss: 61.366.. Val loss: 55.925.. Train L1 norm: 2.498.. Val L1 norm: 1.089.. Train Linf norm: 1504.982.. Val Linf norm: 63.257\n",
            "Epoch 109/127.. Train loss: 184417446563.824.. Val loss: 56.364.. Train L1 norm: 6743.338.. Val L1 norm: 1.152.. Train Linf norm: 1320048.396.. Val Linf norm: 101.312\n",
            "Epoch 110/127.. Train loss: 61.808.. Val loss: 56.363.. Train L1 norm: 3.412.. Val L1 norm: 1.152.. Train Linf norm: 2412.506.. Val Linf norm: 101.245\n",
            "Epoch 111/127.. Train loss: 61.808.. Val loss: 56.363.. Train L1 norm: 3.411.. Val L1 norm: 1.152.. Train Linf norm: 2411.343.. Val Linf norm: 101.222\n",
            "Epoch 112/127.. Train loss: 74.717.. Val loss: 56.362.. Train L1 norm: 3.412.. Val L1 norm: 1.152.. Train Linf norm: 2412.338.. Val Linf norm: 101.198\n",
            "Epoch 113/127.. Train loss: 61.807.. Val loss: 56.362.. Train L1 norm: 3.410.. Val L1 norm: 1.152.. Train Linf norm: 2406.974.. Val Linf norm: 101.173\n",
            "Epoch 114/127.. Train loss: 61.807.. Val loss: 56.362.. Train L1 norm: 3.409.. Val L1 norm: 1.152.. Train Linf norm: 2409.597.. Val Linf norm: 101.149\n",
            "Epoch 115/127.. Train loss: 20317038.833.. Val loss: 56.363.. Train L1 norm: 52.002.. Val L1 norm: 1.152.. Train Linf norm: 52168.756.. Val Linf norm: 101.226\n",
            "Epoch 116/127.. Train loss: 61.808.. Val loss: 56.362.. Train L1 norm: 3.411.. Val L1 norm: 1.152.. Train Linf norm: 2413.085.. Val Linf norm: 101.204\n",
            "Epoch 117/127.. Train loss: 61.807.. Val loss: 56.362.. Train L1 norm: 3.410.. Val L1 norm: 1.152.. Train Linf norm: 2408.764.. Val Linf norm: 101.180\n",
            "Epoch 118/127.. Train loss: 61.807.. Val loss: 56.362.. Train L1 norm: 3.410.. Val L1 norm: 1.152.. Train Linf norm: 2410.234.. Val Linf norm: 101.158\n",
            "Epoch 119/127.. Train loss: 168.840.. Val loss: 56.362.. Train L1 norm: 3.450.. Val L1 norm: 1.152.. Train Linf norm: 2436.190.. Val Linf norm: 101.135\n",
            "Epoch 120/127.. Train loss: 61.807.. Val loss: 56.361.. Train L1 norm: 3.409.. Val L1 norm: 1.152.. Train Linf norm: 2408.016.. Val Linf norm: 101.114\n",
            "Epoch 121/127.. Train loss: 61.806.. Val loss: 56.361.. Train L1 norm: 3.408.. Val L1 norm: 1.152.. Train Linf norm: 2408.081.. Val Linf norm: 101.094\n",
            "Epoch 122/127.. Train loss: 61.806.. Val loss: 56.361.. Train L1 norm: 3.408.. Val L1 norm: 1.152.. Train Linf norm: 2406.792.. Val Linf norm: 101.075\n",
            "Epoch 123/127.. Train loss: 61.806.. Val loss: 56.361.. Train L1 norm: 3.407.. Val L1 norm: 1.151.. Train Linf norm: 2409.170.. Val Linf norm: 101.056\n",
            "Epoch 124/127.. Train loss: 61.806.. Val loss: 56.361.. Train L1 norm: 3.407.. Val L1 norm: 1.151.. Train Linf norm: 2403.733.. Val Linf norm: 101.038\n",
            "Epoch 125/127.. Train loss: 61.805.. Val loss: 56.360.. Train L1 norm: 3.406.. Val L1 norm: 1.151.. Train Linf norm: 2404.756.. Val Linf norm: 101.021\n",
            "Epoch 126/127.. Train loss: 61.805.. Val loss: 56.360.. Train L1 norm: 3.406.. Val L1 norm: 1.151.. Train Linf norm: 2406.263.. Val Linf norm: 101.004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 05:48:30,299]\u001b[0m Trial 12 finished with value: 1.1513660175323486 and parameters: {'n_layers': 8, 'n_units_0': 2666, 'n_units_1': 3174, 'n_units_2': 1813, 'n_units_3': 2294, 'n_units_4': 2402, 'n_units_5': 3801, 'n_units_6': 291, 'n_units_7': 81, 'hidden_activation': 'ReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 0.006919811588951734, 'batch_size': 1024, 'n_epochs': 127, 'scheduler': 'CosineAnnealingLR', 'dropout_rate': 0.08585425252313758, 'weight_decay': 0.0003982972185580364, 'beta1': 0.9015749124067319, 'beta2': 0.999784773536315, 't_max_fraction': 0.11504831591926465, 'eta_min': 0.004347985819191817}. Best is trial 12 with value: 1.1513660175323486.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 127/127.. Train loss: 61.805.. Val loss: 56.360.. Train L1 norm: 3.405.. Val L1 norm: 1.151.. Train Linf norm: 2405.049.. Val Linf norm: 100.986\n",
            "Epoch 1/136.. Train loss: 5543.809.. Val loss: 52.775.. Train L1 norm: 4.993.. Val L1 norm: 1.170.. Train Linf norm: 4012.715.. Val Linf norm: 118.708\n",
            "Epoch 2/136.. Train loss: 3197.124.. Val loss: 51.360.. Train L1 norm: 10.409.. Val L1 norm: 1.420.. Train Linf norm: 9372.785.. Val Linf norm: 268.974\n",
            "Epoch 3/136.. Train loss: 895.261.. Val loss: 48.040.. Train L1 norm: 11.876.. Val L1 norm: 2.281.. Train Linf norm: 10843.923.. Val Linf norm: 745.839\n",
            "Epoch 4/136.. Train loss: 51.353.. Val loss: 40.212.. Train L1 norm: 48.631.. Val L1 norm: 6.558.. Train Linf norm: 46940.410.. Val Linf norm: 2797.483\n",
            "Epoch 5/136.. Train loss: 204.528.. Val loss: 40.330.. Train L1 norm: 47.076.. Val L1 norm: 5.938.. Train Linf norm: 44702.058.. Val Linf norm: 2512.216\n",
            "Epoch 6/136.. Train loss: 274.584.. Val loss: 40.311.. Train L1 norm: 64.095.. Val L1 norm: 6.007.. Train Linf norm: 61134.231.. Val Linf norm: 2544.161\n",
            "Epoch 7/136.. Train loss: 45.486.. Val loss: 40.278.. Train L1 norm: 65.878.. Val L1 norm: 6.221.. Train Linf norm: 63626.618.. Val Linf norm: 2642.695\n",
            "Epoch 8/136.. Train loss: 45.501.. Val loss: 40.276.. Train L1 norm: 66.030.. Val L1 norm: 6.254.. Train Linf norm: 63769.348.. Val Linf norm: 2657.850\n",
            "Epoch 9/136.. Train loss: 45.451.. Val loss: 40.285.. Train L1 norm: 67.653.. Val L1 norm: 6.428.. Train Linf norm: 65373.137.. Val Linf norm: 2737.296\n",
            "Epoch 10/136.. Train loss: 226378223.950.. Val loss: 41.212.. Train L1 norm: 1663.959.. Val L1 norm: 7.763.. Train Linf norm: 275938.433.. Val Linf norm: 3342.537\n",
            "Epoch 11/136.. Train loss: 33297939.738.. Val loss: 40.823.. Train L1 norm: 81.649.. Val L1 norm: 7.414.. Train Linf norm: 78356.144.. Val Linf norm: 3185.216\n",
            "Epoch 12/136.. Train loss: 50.519.. Val loss: 40.300.. Train L1 norm: 86.095.. Val L1 norm: 6.521.. Train Linf norm: 83078.584.. Val Linf norm: 2779.704\n",
            "Epoch 13/136.. Train loss: 48.515.. Val loss: 40.282.. Train L1 norm: 108.055.. Val L1 norm: 6.170.. Train Linf norm: 106686.421.. Val Linf norm: 2619.343\n",
            "Epoch 14/136.. Train loss: 47.034.. Val loss: 40.763.. Train L1 norm: 91.705.. Val L1 norm: 5.245.. Train Linf norm: 90010.153.. Val Linf norm: 2190.653\n",
            "Epoch 15/136.. Train loss: 46.867.. Val loss: 40.448.. Train L1 norm: 55.794.. Val L1 norm: 5.666.. Train Linf norm: 53236.785.. Val Linf norm: 2386.597\n",
            "Epoch 16/136.. Train loss: 46.807.. Val loss: 40.326.. Train L1 norm: 38.263.. Val L1 norm: 6.626.. Train Linf norm: 35296.036.. Val Linf norm: 2827.957\n",
            "Epoch 17/136.. Train loss: 19792645.603.. Val loss: 239.024.. Train L1 norm: 144.060.. Val L1 norm: 29.017.. Train Linf norm: 142014.362.. Val Linf norm: 12425.987\n",
            "Epoch 18/136.. Train loss: 92.184.. Val loss: 40.299.. Train L1 norm: 206.736.. Val L1 norm: 6.061.. Train Linf norm: 205280.241.. Val Linf norm: 2568.899\n",
            "Epoch 19/136.. Train loss: 59.584.. Val loss: 40.453.. Train L1 norm: 42.312.. Val L1 norm: 5.657.. Train Linf norm: 38678.866.. Val Linf norm: 2382.507\n",
            "Epoch 20/136.. Train loss: 55.379.. Val loss: 40.280.. Train L1 norm: 22.846.. Val L1 norm: 6.389.. Train Linf norm: 18686.593.. Val Linf norm: 2719.614\n",
            "Epoch 21/136.. Train loss: 54.608.. Val loss: 40.284.. Train L1 norm: 20.223.. Val L1 norm: 6.422.. Train Linf norm: 16250.328.. Val Linf norm: 2734.663\n",
            "Epoch 22/136.. Train loss: 54.163.. Val loss: 40.309.. Train L1 norm: 33.962.. Val L1 norm: 6.563.. Train Linf norm: 30439.288.. Val Linf norm: 2799.288\n",
            "Epoch 23/136.. Train loss: 54.173.. Val loss: 40.310.. Train L1 norm: 101.417.. Val L1 norm: 6.013.. Train Linf norm: 99678.598.. Val Linf norm: 2546.747\n",
            "Epoch 24/136.. Train loss: 53.763.. Val loss: 40.289.. Train L1 norm: 43.508.. Val L1 norm: 6.119.. Train Linf norm: 40461.048.. Val Linf norm: 2595.689\n",
            "Epoch 25/136.. Train loss: 54.033.. Val loss: 40.286.. Train L1 norm: 45.404.. Val L1 norm: 6.138.. Train Linf norm: 42272.429.. Val Linf norm: 2604.218\n",
            "Epoch 26/136.. Train loss: 53.687.. Val loss: 40.282.. Train L1 norm: 110.489.. Val L1 norm: 6.172.. Train Linf norm: 108871.434.. Val Linf norm: 2619.838\n",
            "Epoch 27/136.. Train loss: 53.703.. Val loss: 40.280.. Train L1 norm: 114.619.. Val L1 norm: 6.194.. Train Linf norm: 113072.529.. Val Linf norm: 2630.180\n",
            "Epoch 28/136.. Train loss: 53.573.. Val loss: 40.285.. Train L1 norm: 133.700.. Val L1 norm: 6.145.. Train Linf norm: 132581.229.. Val Linf norm: 2607.798\n",
            "Epoch 29/136.. Train loss: 53.430.. Val loss: 40.277.. Train L1 norm: 42.181.. Val L1 norm: 6.237.. Train Linf norm: 38797.377.. Val Linf norm: 2650.097\n",
            "Epoch 30/136.. Train loss: 53.772.. Val loss: 40.408.. Train L1 norm: 9.654.. Val L1 norm: 5.743.. Train Linf norm: 5362.667.. Val Linf norm: 2422.389\n",
            "Epoch 31/136.. Train loss: 53.470.. Val loss: 40.295.. Train L1 norm: 68.505.. Val L1 norm: 6.497.. Train Linf norm: 65896.734.. Val Linf norm: 2768.924\n",
            "Epoch 32/136.. Train loss: 53.323.. Val loss: 40.420.. Train L1 norm: 68.363.. Val L1 norm: 5.719.. Train Linf norm: 65726.330.. Val Linf norm: 2411.080\n",
            "Epoch 33/136.. Train loss: 53.020.. Val loss: 40.316.. Train L1 norm: 94.399.. Val L1 norm: 5.987.. Train Linf norm: 91981.151.. Val Linf norm: 2534.989\n",
            "Epoch 34/136.. Train loss: 53.060.. Val loss: 40.449.. Train L1 norm: 36.922.. Val L1 norm: 5.665.. Train Linf norm: 33556.841.. Val Linf norm: 2386.030\n",
            "Epoch 35/136.. Train loss: 52.567.. Val loss: 40.278.. Train L1 norm: 9.772.. Val L1 norm: 6.350.. Train Linf norm: 5784.781.. Val Linf norm: 2701.712\n",
            "Epoch 36/136.. Train loss: 52.619.. Val loss: 40.358.. Train L1 norm: 59.303.. Val L1 norm: 5.857.. Train Linf norm: 56550.838.. Val Linf norm: 2474.934\n",
            "Epoch 37/136.. Train loss: 52.077.. Val loss: 40.291.. Train L1 norm: 89.541.. Val L1 norm: 6.472.. Train Linf norm: 87360.547.. Val Linf norm: 2757.383\n",
            "Epoch 38/136.. Train loss: 51.598.. Val loss: 40.314.. Train L1 norm: 48.211.. Val L1 norm: 6.583.. Train Linf norm: 45155.417.. Val Linf norm: 2808.109\n",
            "Epoch 39/136.. Train loss: 51.426.. Val loss: 40.276.. Train L1 norm: 7.022.. Val L1 norm: 6.286.. Train Linf norm: 2975.769.. Val Linf norm: 2672.538\n",
            "Epoch 40/136.. Train loss: 51.340.. Val loss: 40.329.. Train L1 norm: 81.770.. Val L1 norm: 6.636.. Train Linf norm: 79613.608.. Val Linf norm: 2832.396\n",
            "Epoch 41/136.. Train loss: 50.900.. Val loss: 40.455.. Train L1 norm: 65.552.. Val L1 norm: 6.930.. Train Linf norm: 63092.686.. Val Linf norm: 2966.314\n",
            "Epoch 42/136.. Train loss: 50.564.. Val loss: 40.337.. Train L1 norm: 27.566.. Val L1 norm: 6.661.. Train Linf norm: 24222.913.. Val Linf norm: 2843.790\n",
            "Epoch 43/136.. Train loss: 5992831.294.. Val loss: 40.476.. Train L1 norm: 81.854.. Val L1 norm: 6.966.. Train Linf norm: 78920.265.. Val Linf norm: 2982.462\n",
            "Epoch 44/136.. Train loss: 16781345964.228.. Val loss: 286.848.. Train L1 norm: 332.839.. Val L1 norm: 31.649.. Train Linf norm: 287083.729.. Val Linf norm: 13536.169\n",
            "Epoch 45/136.. Train loss: 706.704.. Val loss: 40.551.. Train L1 norm: 266.655.. Val L1 norm: 5.502.. Train Linf norm: 253311.668.. Val Linf norm: 2310.464\n",
            "Epoch 46/136.. Train loss: 578.341.. Val loss: 40.548.. Train L1 norm: 668.250.. Val L1 norm: 7.080.. Train Linf norm: 665961.968.. Val Linf norm: 3034.486\n",
            "Epoch 47/136.. Train loss: 466.090.. Val loss: 41.069.. Train L1 norm: 36.198.. Val L1 norm: 4.960.. Train Linf norm: 20200.928.. Val Linf norm: 2057.365\n",
            "Epoch 48/136.. Train loss: 418.799.. Val loss: 40.753.. Train L1 norm: 406.032.. Val L1 norm: 7.339.. Train Linf norm: 400987.314.. Val Linf norm: 3151.404\n",
            "Epoch 49/136.. Train loss: 394.336.. Val loss: 41.011.. Train L1 norm: 234.766.. Val L1 norm: 7.594.. Train Linf norm: 225852.123.. Val Linf norm: 3266.568\n",
            "Epoch 50/136.. Train loss: 380.058.. Val loss: 40.279.. Train L1 norm: 35.463.. Val L1 norm: 6.199.. Train Linf norm: 22165.931.. Val Linf norm: 2632.459\n",
            "Epoch 51/136.. Train loss: 373.754.. Val loss: 40.278.. Train L1 norm: 180.679.. Val L1 norm: 6.214.. Train Linf norm: 170987.364.. Val Linf norm: 2639.525\n",
            "Epoch 52/136.. Train loss: 374.049.. Val loss: 40.395.. Train L1 norm: 73.813.. Val L1 norm: 6.811.. Train Linf norm: 61785.167.. Val Linf norm: 2911.970\n",
            "Epoch 53/136.. Train loss: 374.236.. Val loss: 40.420.. Train L1 norm: 59.111.. Val L1 norm: 6.864.. Train Linf norm: 46642.259.. Val Linf norm: 2936.283\n",
            "Epoch 54/136.. Train loss: 368.531.. Val loss: 40.437.. Train L1 norm: 135.585.. Val L1 norm: 6.896.. Train Linf norm: 125271.627.. Val Linf norm: 2950.895\n",
            "Epoch 55/136.. Train loss: 368.582.. Val loss: 40.279.. Train L1 norm: 177.568.. Val L1 norm: 6.367.. Train Linf norm: 168148.007.. Val Linf norm: 2709.462\n",
            "Epoch 56/136.. Train loss: 366.725.. Val loss: 40.909.. Train L1 norm: 655.195.. Val L1 norm: 7.499.. Train Linf norm: 656041.336.. Val Linf norm: 3223.641\n",
            "Epoch 57/136.. Train loss: 355.711.. Val loss: 41.167.. Train L1 norm: 324.590.. Val L1 norm: 7.727.. Train Linf norm: 318634.100.. Val Linf norm: 3326.521\n",
            "Epoch 58/136.. Train loss: 346.301.. Val loss: 41.362.. Train L1 norm: 531.716.. Val L1 norm: 7.879.. Train Linf norm: 531521.312.. Val Linf norm: 3394.422\n",
            "Epoch 59/136.. Train loss: 329.247.. Val loss: 40.961.. Train L1 norm: 30.109.. Val L1 norm: 7.549.. Train Linf norm: 17945.916.. Val Linf norm: 3246.016\n",
            "Epoch 60/136.. Train loss: 309.044.. Val loss: 41.451.. Train L1 norm: 34.882.. Val L1 norm: 7.943.. Train Linf norm: 23437.279.. Val Linf norm: 3423.401\n",
            "Epoch 61/136.. Train loss: 290.299.. Val loss: 45.988.. Train L1 norm: 146.900.. Val L1 norm: 9.982.. Train Linf norm: 138319.643.. Val Linf norm: 4326.790\n",
            "Epoch 62/136.. Train loss: 273.193.. Val loss: 41.431.. Train L1 norm: 459.971.. Val L1 norm: 7.929.. Train Linf norm: 459315.535.. Val Linf norm: 3416.881\n",
            "Epoch 63/136.. Train loss: 256.833.. Val loss: 40.303.. Train L1 norm: 48.400.. Val L1 norm: 6.536.. Train Linf norm: 38193.621.. Val Linf norm: 2786.685\n",
            "Epoch 64/136.. Train loss: 242.234.. Val loss: 40.404.. Train L1 norm: 162.164.. Val L1 norm: 5.750.. Train Linf norm: 155345.602.. Val Linf norm: 2425.777\n",
            "Epoch 65/136.. Train loss: 229.192.. Val loss: 45.614.. Train L1 norm: 20.492.. Val L1 norm: 2.927.. Train Linf norm: 9797.694.. Val Linf norm: 1075.103\n",
            "Epoch 66/136.. Train loss: 18361519068052.328.. Val loss: 313.541.. Train L1 norm: 5605.249.. Val L1 norm: 33.007.. Train Linf norm: 5568970.560.. Val Linf norm: 14109.042\n",
            "Epoch 67/136.. Train loss: 189332017920.342.. Val loss: 10372.128.. Train L1 norm: 1874.958.. Val L1 norm: 172.667.. Train Linf norm: 1734501.339.. Val Linf norm: 72989.976\n",
            "Epoch 68/136.. Train loss: 598377839946.564.. Val loss: 123.618.. Train L1 norm: 6988.563.. Val L1 norm: 20.882.. Train Linf norm: 6854042.536.. Val Linf norm: 8988.817\n",
            "Epoch 69/136.. Train loss: 20376.561.. Val loss: 81.926.. Train L1 norm: 932.833.. Val L1 norm: 16.517.. Train Linf norm: 846318.286.. Val Linf norm: 7137.969\n",
            "Epoch 70/136.. Train loss: 12428.043.. Val loss: 65.854.. Train L1 norm: 3828.149.. Val L1 norm: 14.252.. Train Linf norm: 3834124.417.. Val Linf norm: 6172.067\n",
            "Epoch 71/136.. Train loss: 8713.228.. Val loss: 56.092.. Train L1 norm: 1136.807.. Val L1 norm: 12.510.. Train Linf norm: 1093299.209.. Val Linf norm: 5424.597\n",
            "Epoch 72/136.. Train loss: 7762.231.. Val loss: 57.336.. Train L1 norm: 1657.751.. Val L1 norm: 1.298.. Train Linf norm: 1632071.818.. Val Linf norm: 183.867\n",
            "Epoch 73/136.. Train loss: 7148.708.. Val loss: 51.055.. Train L1 norm: 1424.617.. Val L1 norm: 1.640.. Train Linf norm: 1394771.144.. Val Linf norm: 402.157\n",
            "Epoch 74/136.. Train loss: 6914.772.. Val loss: 45.836.. Train L1 norm: 479.223.. Val L1 norm: 2.862.. Train Linf norm: 425843.285.. Val Linf norm: 1042.172\n",
            "Epoch 75/136.. Train loss: 6670.797.. Val loss: 50.991.. Train L1 norm: 1286.921.. Val L1 norm: 1.652.. Train Linf norm: 1258214.978.. Val Linf norm: 408.929\n",
            "Epoch 76/136.. Train loss: 6532.978.. Val loss: 62.218.. Train L1 norm: 1427.845.. Val L1 norm: 2.054.. Train Linf norm: 1400299.418.. Val Linf norm: 567.051\n",
            "Epoch 77/136.. Train loss: 6422.966.. Val loss: 50.417.. Train L1 norm: 503.070.. Val L1 norm: 1.763.. Train Linf norm: 453369.114.. Val Linf norm: 470.421\n",
            "Epoch 78/136.. Train loss: 6402.864.. Val loss: 43.177.. Train L1 norm: 779.271.. Val L1 norm: 3.781.. Train Linf norm: 741311.720.. Val Linf norm: 1495.204\n",
            "Epoch 79/136.. Train loss: 6374.430.. Val loss: 45.248.. Train L1 norm: 1131.700.. Val L1 norm: 3.039.. Train Linf norm: 1099783.201.. Val Linf norm: 1130.884\n",
            "Epoch 80/136.. Train loss: 6385.540.. Val loss: 47.943.. Train L1 norm: 2303.903.. Val L1 norm: 2.304.. Train Linf norm: 2301399.287.. Val Linf norm: 757.886\n",
            "Epoch 81/136.. Train loss: 6379.704.. Val loss: 45.305.. Train L1 norm: 2239.351.. Val L1 norm: 3.021.. Train Linf norm: 2235054.445.. Val Linf norm: 1122.114\n",
            "Epoch 82/136.. Train loss: 6346.799.. Val loss: 42.143.. Train L1 norm: 376.689.. Val L1 norm: 4.265.. Train Linf norm: 325846.732.. Val Linf norm: 1728.102\n",
            "Epoch 83/136.. Train loss: 6234.962.. Val loss: 60.168.. Train L1 norm: 1684.118.. Val L1 norm: 1.737.. Train Linf norm: 1665867.397.. Val Linf norm: 411.942\n",
            "Epoch 84/136.. Train loss: 6163.808.. Val loss: 97.772.. Train L1 norm: 1555.409.. Val L1 norm: 6.586.. Train Linf norm: 1516609.304.. Val Linf norm: 2572.089\n",
            "Epoch 85/136.. Train loss: 5841.841.. Val loss: 40.847.. Train L1 norm: 142.387.. Val L1 norm: 5.159.. Train Linf norm: 90238.863.. Val Linf norm: 2150.521\n",
            "Epoch 86/136.. Train loss: 5576.677.. Val loss: 49.600.. Train L1 norm: 282.592.. Val L1 norm: 1.931.. Train Linf norm: 234607.722.. Val Linf norm: 561.065\n",
            "Epoch 87/136.. Train loss: 5312.686.. Val loss: 42.172.. Train L1 norm: 173.364.. Val L1 norm: 4.250.. Train Linf norm: 124482.024.. Val Linf norm: 1720.772\n",
            "Epoch 88/136.. Train loss: 5008.718.. Val loss: 41.284.. Train L1 norm: 474.551.. Val L1 norm: 4.794.. Train Linf norm: 434116.485.. Val Linf norm: 1978.935\n",
            "Epoch 89/136.. Train loss: 4690.752.. Val loss: 85.482.. Train L1 norm: 790.206.. Val L1 norm: 5.195.. Train Linf norm: 757940.209.. Val Linf norm: 1977.823\n",
            "Epoch 90/136.. Train loss: 4424.053.. Val loss: 48.060.. Train L1 norm: 150.050.. Val L1 norm: 10.614.. Train Linf norm: 105064.024.. Val Linf norm: 4603.413\n",
            "Epoch 91/136.. Train loss: 4127.407.. Val loss: 44.669.. Train L1 norm: 1197.712.. Val L1 norm: 9.519.. Train Linf norm: 1179008.189.. Val Linf norm: 4123.412\n",
            "Epoch 92/136.. Train loss: 3934.853.. Val loss: 42.065.. Train L1 norm: 385.301.. Val L1 norm: 4.307.. Train Linf norm: 348656.865.. Val Linf norm: 1748.055\n",
            "Epoch 93/136.. Train loss: 3693.765.. Val loss: 50.997.. Train L1 norm: 131.932.. Val L1 norm: 11.385.. Train Linf norm: 90445.579.. Val Linf norm: 4938.496\n",
            "Epoch 94/136.. Train loss: 3495.012.. Val loss: 70.071.. Train L1 norm: 398.711.. Val L1 norm: 14.901.. Train Linf norm: 365816.846.. Val Linf norm: 6449.450\n",
            "Epoch 95/136.. Train loss: 3362.012.. Val loss: 48.401.. Train L1 norm: 1449.924.. Val L1 norm: 10.710.. Train Linf norm: 1442456.837.. Val Linf norm: 4645.322\n",
            "Epoch 96/136.. Train loss: 3228.889.. Val loss: 51.824.. Train L1 norm: 296.274.. Val L1 norm: 11.583.. Train Linf norm: 260701.197.. Val Linf norm: 5024.211\n",
            "Epoch 97/136.. Train loss: 3130.888.. Val loss: 65.337.. Train L1 norm: 140.600.. Val L1 norm: 14.169.. Train Linf norm: 102785.411.. Val Linf norm: 6136.533\n",
            "Epoch 98/136.. Train loss: 3027.444.. Val loss: 43.542.. Train L1 norm: 217.365.. Val L1 norm: 9.066.. Train Linf norm: 181993.922.. Val Linf norm: 3923.524\n",
            "Epoch 99/136.. Train loss: 2999.564.. Val loss: 41.369.. Train L1 norm: 371.385.. Val L1 norm: 7.884.. Train Linf norm: 339659.518.. Val Linf norm: 3396.705\n",
            "Epoch 100/136.. Train loss: 2949.813.. Val loss: 43.229.. Train L1 norm: 1014.572.. Val L1 norm: 3.759.. Train Linf norm: 999514.579.. Val Linf norm: 1484.539\n",
            "Epoch 101/136.. Train loss: 2886.853.. Val loss: 45.789.. Train L1 norm: 664.406.. Val L1 norm: 9.915.. Train Linf norm: 641822.665.. Val Linf norm: 4297.750\n",
            "Epoch 102/136.. Train loss: 2882.448.. Val loss: 40.361.. Train L1 norm: 740.740.. Val L1 norm: 5.851.. Train Linf norm: 719607.424.. Val Linf norm: 2472.273\n",
            "Epoch 103/136.. Train loss: 2892.924.. Val loss: 41.288.. Train L1 norm: 263.068.. Val L1 norm: 7.823.. Train Linf norm: 230564.928.. Val Linf norm: 3369.236\n",
            "Epoch 104/136.. Train loss: 2861.079.. Val loss: 41.223.. Train L1 norm: 1074.922.. Val L1 norm: 7.772.. Train Linf norm: 1059641.688.. Val Linf norm: 3346.536\n",
            "Epoch 105/136.. Train loss: 2855.282.. Val loss: 41.171.. Train L1 norm: 312.369.. Val L1 norm: 7.730.. Train Linf norm: 280546.855.. Val Linf norm: 3327.821\n",
            "Epoch 106/136.. Train loss: 2867.180.. Val loss: 40.314.. Train L1 norm: 536.278.. Val L1 norm: 6.583.. Train Linf norm: 507710.072.. Val Linf norm: 2808.120\n",
            "Epoch 107/136.. Train loss: 2872.391.. Val loss: 40.309.. Train L1 norm: 191.294.. Val L1 norm: 6.017.. Train Linf norm: 155471.560.. Val Linf norm: 2548.623\n",
            "Epoch 108/136.. Train loss: 2835.208.. Val loss: 40.292.. Train L1 norm: 611.308.. Val L1 norm: 6.098.. Train Linf norm: 586768.596.. Val Linf norm: 2585.949\n",
            "Epoch 109/136.. Train loss: 2822.753.. Val loss: 41.209.. Train L1 norm: 461.523.. Val L1 norm: 4.849.. Train Linf norm: 433165.472.. Val Linf norm: 2005.273\n",
            "Epoch 110/136.. Train loss: 2797.164.. Val loss: 42.773.. Train L1 norm: 887.416.. Val L1 norm: 3.957.. Train Linf norm: 869466.438.. Val Linf norm: 1580.325\n",
            "Epoch 111/136.. Train loss: 2727.320.. Val loss: 40.278.. Train L1 norm: 240.605.. Val L1 norm: 6.349.. Train Linf norm: 208234.963.. Val Linf norm: 2701.145\n",
            "Epoch 112/136.. Train loss: 2633.603.. Val loss: 48.661.. Train L1 norm: 217.122.. Val L1 norm: 2.136.. Train Linf norm: 184516.940.. Val Linf norm: 670.263\n",
            "Epoch 113/136.. Train loss: 2584.077.. Val loss: 86.332.. Train L1 norm: 290.654.. Val L1 norm: 5.296.. Train Linf norm: 260995.689.. Val Linf norm: 2021.345\n",
            "Epoch 114/136.. Train loss: 2518.630.. Val loss: 40.560.. Train L1 norm: 489.406.. Val L1 norm: 5.489.. Train Linf norm: 464973.155.. Val Linf norm: 2304.756\n",
            "Epoch 115/136.. Train loss: 2416.580.. Val loss: 65.437.. Train L1 norm: 314.458.. Val L1 norm: 2.539.. Train Linf norm: 285386.380.. Val Linf norm: 796.604\n",
            "Epoch 116/136.. Train loss: 2332.184.. Val loss: 46.275.. Train L1 norm: 601.623.. Val L1 norm: 2.736.. Train Linf norm: 577599.043.. Val Linf norm: 978.989\n",
            "Epoch 117/136.. Train loss: 2257.477.. Val loss: 41.706.. Train L1 norm: 1327.688.. Val L1 norm: 4.513.. Train Linf norm: 1323853.225.. Val Linf norm: 1846.176\n",
            "Epoch 118/136.. Train loss: 39004403489734584310956032.000.. Val loss: 2540309033587389310697472.000.. Train L1 norm: 628603515926.594.. Val L1 norm: 571829051125.487.. Train Linf norm: 282073491466466.250.. Val Linf norm: 192071328720678.625\n",
            "Epoch 119/136.. Train loss: 11378814947156729628194766848.000.. Val loss: 3305724709181258891001856.000.. Train L1 norm: 10994498996954.727.. Val L1 norm: 593946806338.628.. Train Linf norm: 10885121541619072.000.. Val Linf norm: 208695833718021.594\n",
            "Epoch 120/136.. Train loss: 61170791718419355304428830720.000.. Val loss: 3179577834421611640389632.000.. Train L1 norm: 688253463181.956.. Val L1 norm: 562215542971.870.. Train Linf norm: 351958611459541.750.. Val Linf norm: 199252609367731.938\n",
            "Epoch 121/136.. Train loss: 21253595329447070646155608064.000.. Val loss: 3074703901725090480390144.000.. Train L1 norm: 1201102978420.385.. Val L1 norm: 541646591448.405.. Train Linf norm: 876989622566833.375.. Val Linf norm: 191535046502173.906\n",
            "Epoch 122/136.. Train loss: 17502288586439393585211113472.000.. Val loss: 2970965479266336431407104.000.. Train L1 norm: 750929987037.711.. Val L1 norm: 524301149090.611.. Train Linf norm: 445954994143876.125.. Val Linf norm: 183976954092357.781\n",
            "Epoch 123/136.. Train loss: 19093317480736888607404457984.000.. Val loss: 2966693255151485288710144.000.. Train L1 norm: 601703096999.702.. Val L1 norm: 518716681582.455.. Train Linf norm: 268303414508920.125.. Val Linf norm: 180646325159862.812\n",
            "Epoch 124/136.. Train loss: 5659795753665819369254420480.000.. Val loss: 2892444215895162794016768.000.. Train L1 norm: 3982830995980.756.. Val L1 norm: 508031688647.202.. Train Linf norm: 3759188147181775.500.. Val Linf norm: 175681294604802.469\n",
            "Epoch 125/136.. Train loss: 63942565125198175354748928.000.. Val loss: 2871605523100251830878208.000.. Train L1 norm: 670068907464.646.. Val L1 norm: 503075394866.381.. Train Linf norm: 368249276753001.062.. Val Linf norm: 173191106428595.938\n",
            "Epoch 126/136.. Train loss: 43262652300246866847923699712.000.. Val loss: 2836286057501371557478400.000.. Train L1 norm: 652862887319.494.. Val L1 norm: 498146994762.547.. Train Linf norm: 317866116603520.125.. Val Linf norm: 170926492027453.438\n",
            "Epoch 127/136.. Train loss: 9164303837233152591363309568.000.. Val loss: 2835102739144434369167360.000.. Train L1 norm: 553593213161.121.. Val L1 norm: 496884012845.193.. Train Linf norm: 252863068285480.844.. Val Linf norm: 170052915035880.375\n",
            "Epoch 128/136.. Train loss: 2129259588584092408177754112.000.. Val loss: 2822945151748673181319168.000.. Train L1 norm: 609581911312.208.. Val L1 norm: 495063210480.981.. Train Linf norm: 332308248903556.438.. Val Linf norm: 169159009616247.750\n",
            "Epoch 129/136.. Train loss: 37523500994811920840071839744.000.. Val loss: 2822856441048976987258880.000.. Train L1 norm: 597235819984.603.. Val L1 norm: 494786160309.043.. Train Linf norm: 313517961450663.812.. Val Linf norm: 168966552279353.469\n",
            "Epoch 130/136.. Train loss: 26839330314789605039128182784.000.. Val loss: 2822834734150334111809536.000.. Train L1 norm: 590366455429.296.. Val L1 norm: 494708698784.290.. Train Linf norm: 302084449170403.188.. Val Linf norm: 168910234528514.062\n",
            "Epoch 131/136.. Train loss: 8453781777170187544889917440.000.. Val loss: 2822831893044301014237184.000.. Train L1 norm: 590431212138.847.. Val L1 norm: 494701270025.284.. Train Linf norm: 279164527911543.031.. Val Linf norm: 168905119702292.875\n",
            "Epoch 132/136.. Train loss: 15763510544547119435551342592.000.. Val loss: 2822799824302065993121792.000.. Train L1 norm: 7304830329384.375.. Val L1 norm: 494628939864.474.. Train Linf norm: 7168958100943802.000.. Val Linf norm: 168853461780695.188\n",
            "Epoch 133/136.. Train loss: 38385138858358857288256585728.000.. Val loss: 2822643306138166460678144.000.. Train L1 norm: 537210639617.697.. Val L1 norm: 494371482107.904.. Train Linf norm: 250395395932141.281.. Val Linf norm: 168673457992280.750\n",
            "Epoch 134/136.. Train loss: 114206607071687003911225344.000.. Val loss: 2822307308840573091184640.000.. Train L1 norm: 14707074307292.248.. Val L1 norm: 493798064180.429.. Train Linf norm: 5487255581415613.000.. Val Linf norm: 168269645810237.438\n",
            "Epoch 135/136.. Train loss: 9563645455389905971669106688.000.. Val loss: 2816353839862363004600320.000.. Train L1 norm: 520778492363.220.. Val L1 norm: 492427592073.216.. Train Linf norm: 231211120502258.312.. Val Linf norm: 167489755176291.531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 05:58:32,336]\u001b[0m Trial 13 finished with value: 491036884983.26184 and parameters: {'n_layers': 9, 'n_units_0': 1244, 'n_units_1': 3230, 'n_units_2': 1218, 'n_units_3': 2303, 'n_units_4': 2450, 'n_units_5': 4095, 'n_units_6': 96, 'n_units_7': 421, 'n_units_8': 3958, 'hidden_activation': 'ReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 1.6190756370319895e-05, 'batch_size': 1024, 'n_epochs': 136, 'scheduler': 'CosineAnnealingLR', 'dropout_rate': 0.07576322122724959, 'weight_decay': 0.00042315897345965406, 'beta1': 0.9002513329092581, 'beta2': 0.9998918652645777, 't_max_fraction': 0.10219960366850916, 'eta_min': 0.009329680637802071}. Best is trial 12 with value: 1.1513660175323486.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 136/136.. Train loss: 11825084847852407669336834048.000.. Val loss: 2815741984419868473032704.000.. Train L1 norm: 732769471722.759.. Val L1 norm: 491036884983.262.. Train Linf norm: 438520978655442.188.. Val Linf norm: 166462811018432.781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-77-ddc3c9d46736>:63: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  init = trial.suggest_uniform(\"prelu_init\", 0.1, 0.3)\n",
            "\u001b[32m[I 2023-05-29 05:58:43,319]\u001b[0m Trial 14 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/124.. Train loss: 584974642.892.. Val loss: 6429299.803.. Train L1 norm: 6034.384.. Val L1 norm: 196.633.. Train Linf norm: 313127.218.. Val Linf norm: 8756.993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/pruners/_percentile.py:21: RuntimeWarning: All-NaN slice encountered\n",
            "  return np.nanmin(values)\n",
            "\u001b[32m[I 2023-05-29 05:58:44,700]\u001b[0m Trial 15 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/149.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 05:58:53,275]\u001b[0m Trial 16 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/123.. Train loss: 148633816774371839804702720.000.. Val loss: 175332376002953891115171840.000.. Train L1 norm: 1796579652714.979.. Val L1 norm: 132660898291.166.. Train Linf norm: 1740822248545208.500.. Val Linf norm: 52558936125507.719\n",
            "Epoch 1/134.. Train loss: 237770.219.. Val loss: 464.190.. Train L1 norm: 69.509.. Val L1 norm: 2.701.. Train Linf norm: 64783.191.. Val Linf norm: 786.548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 05:58:57,174]\u001b[0m Trial 17 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/134.. Train loss: 412595.420.. Val loss: 175.210.. Train L1 norm: 169.684.. Val L1 norm: 3.149.. Train Linf norm: 168330.008.. Val Linf norm: 747.317\n",
            "Epoch 1/117.. Train loss: 5.281.. Val loss: 3.799.. Train L1 norm: 48.852.. Val L1 norm: 2.595.. Train Linf norm: 11992.294.. Val Linf norm: 263.623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 05:59:13,277]\u001b[0m Trial 18 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/117.. Train loss: 5.438.. Val loss: 3.641.. Train L1 norm: 41.178.. Val L1 norm: 3.429.. Train Linf norm: 10036.737.. Val Linf norm: 383.388\n",
            "Epoch 1/137.. Train loss: 2.508.. Val loss: 2.458.. Train L1 norm: 3.213.. Val L1 norm: 1.173.. Train Linf norm: 284.270.. Val Linf norm: 21.905\n",
            "Epoch 2/137.. Train loss: 2.461.. Val loss: 2.402.. Train L1 norm: 6.996.. Val L1 norm: 1.364.. Train Linf norm: 765.651.. Val Linf norm: 41.314\n",
            "Epoch 3/137.. Train loss: 2.407.. Val loss: 2.331.. Train L1 norm: 8.135.. Val L1 norm: 1.635.. Train Linf norm: 905.271.. Val Linf norm: 66.804\n",
            "Epoch 4/137.. Train loss: 2.382.. Val loss: 2.286.. Train L1 norm: 16.714.. Val L1 norm: 1.842.. Train Linf norm: 1994.816.. Val Linf norm: 85.512\n",
            "Epoch 5/137.. Train loss: 2.351.. Val loss: 2.243.. Train L1 norm: 15.623.. Val L1 norm: 2.062.. Train Linf norm: 1850.699.. Val Linf norm: 104.909\n",
            "Epoch 6/137.. Train loss: 2.310.. Val loss: 2.201.. Train L1 norm: 21.568.. Val L1 norm: 2.300.. Train Linf norm: 2601.426.. Val Linf norm: 125.367\n",
            "Epoch 7/137.. Train loss: 2.262.. Val loss: 2.164.. Train L1 norm: 30.978.. Val L1 norm: 2.538.. Train Linf norm: 3798.705.. Val Linf norm: 145.372\n",
            "Epoch 8/137.. Train loss: 2.223.. Val loss: 2.128.. Train L1 norm: 30.317.. Val L1 norm: 2.799.. Train Linf norm: 3703.586.. Val Linf norm: 167.004\n",
            "Epoch 9/137.. Train loss: 2.204.. Val loss: 2.097.. Train L1 norm: 32.027.. Val L1 norm: 3.063.. Train Linf norm: 3911.889.. Val Linf norm: 188.613\n",
            "Epoch 10/137.. Train loss: 2.160.. Val loss: 2.070.. Train L1 norm: 36.830.. Val L1 norm: 3.322.. Train Linf norm: 4519.349.. Val Linf norm: 209.531\n",
            "Epoch 11/137.. Train loss: 2.131.. Val loss: 2.050.. Train L1 norm: 43.438.. Val L1 norm: 3.557.. Train Linf norm: 5352.731.. Val Linf norm: 228.344\n",
            "Epoch 12/137.. Train loss: 2.114.. Val loss: 2.033.. Train L1 norm: 40.122.. Val L1 norm: 3.790.. Train Linf norm: 4910.243.. Val Linf norm: 246.937\n",
            "Epoch 13/137.. Train loss: 2.096.. Val loss: 2.018.. Train L1 norm: 42.220.. Val L1 norm: 4.039.. Train Linf norm: 5174.591.. Val Linf norm: 266.571\n",
            "Epoch 14/137.. Train loss: 2.072.. Val loss: 1.993.. Train L1 norm: 56.930.. Val L1 norm: 4.454.. Train Linf norm: 7039.492.. Val Linf norm: 298.547\n",
            "Epoch 15/137.. Train loss: 2.068.. Val loss: 1.991.. Train L1 norm: 55.930.. Val L1 norm: 4.562.. Train Linf norm: 6905.077.. Val Linf norm: 307.081\n",
            "Epoch 16/137.. Train loss: 2.047.. Val loss: 1.985.. Train L1 norm: 58.532.. Val L1 norm: 4.776.. Train Linf norm: 7229.422.. Val Linf norm: 323.483\n",
            "Epoch 17/137.. Train loss: 2.034.. Val loss: 1.990.. Train L1 norm: 62.100.. Val L1 norm: 4.774.. Train Linf norm: 7690.365.. Val Linf norm: 323.653\n",
            "Epoch 18/137.. Train loss: 2.047.. Val loss: 1.980.. Train L1 norm: 57.361.. Val L1 norm: 5.149.. Train Linf norm: 7068.423.. Val Linf norm: 351.959\n",
            "Epoch 19/137.. Train loss: 2.037.. Val loss: 1.982.. Train L1 norm: 64.685.. Val L1 norm: 5.098.. Train Linf norm: 7996.555.. Val Linf norm: 348.335\n",
            "Epoch 20/137.. Train loss: 2.028.. Val loss: 1.984.. Train L1 norm: 67.290.. Val L1 norm: 5.124.. Train Linf norm: 8330.018.. Val Linf norm: 350.399\n",
            "Epoch 21/137.. Train loss: 2.009.. Val loss: 1.982.. Train L1 norm: 67.689.. Val L1 norm: 5.514.. Train Linf norm: 8382.793.. Val Linf norm: 379.494\n",
            "Epoch 22/137.. Train loss: 2.044.. Val loss: 1.981.. Train L1 norm: 67.323.. Val L1 norm: 5.372.. Train Linf norm: 8324.897.. Val Linf norm: 369.073\n",
            "Epoch 23/137.. Train loss: 2.006.. Val loss: 1.983.. Train L1 norm: 68.290.. Val L1 norm: 5.602.. Train Linf norm: 8436.682.. Val Linf norm: 386.172\n",
            "Epoch 24/137.. Train loss: 2.019.. Val loss: 1.981.. Train L1 norm: 69.686.. Val L1 norm: 5.428.. Train Linf norm: 8618.686.. Val Linf norm: 373.349\n",
            "Epoch 25/137.. Train loss: 2.002.. Val loss: 1.985.. Train L1 norm: 77.933.. Val L1 norm: 5.689.. Train Linf norm: 9675.329.. Val Linf norm: 392.649\n",
            "Epoch 26/137.. Train loss: 2.027.. Val loss: 1.981.. Train L1 norm: 73.331.. Val L1 norm: 5.518.. Train Linf norm: 9088.217.. Val Linf norm: 380.141\n",
            "Epoch 27/137.. Train loss: 1.999.. Val loss: 1.984.. Train L1 norm: 75.380.. Val L1 norm: 5.693.. Train Linf norm: 9335.756.. Val Linf norm: 392.959\n",
            "Epoch 28/137.. Train loss: 2.047.. Val loss: 1.980.. Train L1 norm: 63.968.. Val L1 norm: 5.524.. Train Linf norm: 7890.963.. Val Linf norm: 380.524\n",
            "Epoch 29/137.. Train loss: 2.000.. Val loss: 1.982.. Train L1 norm: 77.453.. Val L1 norm: 5.622.. Train Linf norm: 9608.265.. Val Linf norm: 387.742\n",
            "Epoch 30/137.. Train loss: 2.020.. Val loss: 1.980.. Train L1 norm: 70.314.. Val L1 norm: 5.552.. Train Linf norm: 8697.071.. Val Linf norm: 382.622\n",
            "Epoch 31/137.. Train loss: 2.020.. Val loss: 1.980.. Train L1 norm: 68.062.. Val L1 norm: 5.469.. Train Linf norm: 8406.291.. Val Linf norm: 376.476\n",
            "Epoch 32/137.. Train loss: 2.001.. Val loss: 1.981.. Train L1 norm: 68.876.. Val L1 norm: 5.434.. Train Linf norm: 8521.981.. Val Linf norm: 373.979\n",
            "Epoch 33/137.. Train loss: 2.008.. Val loss: 1.981.. Train L1 norm: 78.294.. Val L1 norm: 5.588.. Train Linf norm: 9724.611.. Val Linf norm: 385.419\n",
            "Epoch 34/137.. Train loss: 1.998.. Val loss: 1.981.. Train L1 norm: 73.057.. Val L1 norm: 5.625.. Train Linf norm: 9050.635.. Val Linf norm: 388.115\n",
            "Epoch 35/137.. Train loss: 2.006.. Val loss: 1.980.. Train L1 norm: 77.763.. Val L1 norm: 5.597.. Train Linf norm: 9650.201.. Val Linf norm: 386.096\n",
            "Epoch 36/137.. Train loss: 1.999.. Val loss: 1.982.. Train L1 norm: 74.222.. Val L1 norm: 5.700.. Train Linf norm: 9195.057.. Val Linf norm: 393.660\n",
            "Epoch 37/137.. Train loss: 2.035.. Val loss: 1.980.. Train L1 norm: 71.028.. Val L1 norm: 5.603.. Train Linf norm: 8789.381.. Val Linf norm: 386.504\n",
            "Epoch 38/137.. Train loss: 2.007.. Val loss: 1.980.. Train L1 norm: 68.138.. Val L1 norm: 5.537.. Train Linf norm: 8416.797.. Val Linf norm: 381.678\n",
            "Epoch 39/137.. Train loss: 2.006.. Val loss: 1.980.. Train L1 norm: 73.864.. Val L1 norm: 5.481.. Train Linf norm: 9155.331.. Val Linf norm: 377.508\n",
            "Epoch 40/137.. Train loss: 2.045.. Val loss: 1.980.. Train L1 norm: 75.309.. Val L1 norm: 5.611.. Train Linf norm: 9341.270.. Val Linf norm: 387.175\n",
            "Epoch 41/137.. Train loss: 2.012.. Val loss: 1.982.. Train L1 norm: 73.478.. Val L1 norm: 5.693.. Train Linf norm: 8006.931.. Val Linf norm: 393.181\n",
            "Epoch 42/137.. Train loss: 2.028.. Val loss: 1.980.. Train L1 norm: 66.882.. Val L1 norm: 5.639.. Train Linf norm: 8255.857.. Val Linf norm: 389.222\n",
            "Epoch 43/137.. Train loss: 2.006.. Val loss: 1.980.. Train L1 norm: 75.825.. Val L1 norm: 5.591.. Train Linf norm: 9394.184.. Val Linf norm: 385.648\n",
            "Epoch 44/137.. Train loss: 2.002.. Val loss: 1.980.. Train L1 norm: 74.603.. Val L1 norm: 5.634.. Train Linf norm: 9248.772.. Val Linf norm: 388.853\n",
            "Epoch 45/137.. Train loss: 2.027.. Val loss: 1.980.. Train L1 norm: 73.694.. Val L1 norm: 5.584.. Train Linf norm: 9130.854.. Val Linf norm: 385.147\n",
            "Epoch 46/137.. Train loss: 2.003.. Val loss: 1.980.. Train L1 norm: 74.838.. Val L1 norm: 5.640.. Train Linf norm: 9277.154.. Val Linf norm: 389.265\n",
            "Epoch 47/137.. Train loss: 2.021.. Val loss: 1.980.. Train L1 norm: 79.724.. Val L1 norm: 5.600.. Train Linf norm: 9906.049.. Val Linf norm: 386.316\n",
            "Epoch 48/137.. Train loss: 2.013.. Val loss: 1.979.. Train L1 norm: 71.331.. Val L1 norm: 5.562.. Train Linf norm: 8831.206.. Val Linf norm: 383.491\n",
            "Epoch 49/137.. Train loss: 2.005.. Val loss: 1.980.. Train L1 norm: 73.741.. Val L1 norm: 5.612.. Train Linf norm: 9131.097.. Val Linf norm: 387.186\n",
            "Epoch 50/137.. Train loss: 2.013.. Val loss: 1.980.. Train L1 norm: 72.357.. Val L1 norm: 5.571.. Train Linf norm: 8960.628.. Val Linf norm: 384.209\n",
            "Epoch 51/137.. Train loss: 1.997.. Val loss: 1.980.. Train L1 norm: 66.561.. Val L1 norm: 5.626.. Train Linf norm: 8212.988.. Val Linf norm: 388.277\n",
            "Epoch 52/137.. Train loss: 2.001.. Val loss: 1.980.. Train L1 norm: 74.972.. Val L1 norm: 5.596.. Train Linf norm: 9293.107.. Val Linf norm: 386.068\n",
            "Epoch 53/137.. Train loss: 1.998.. Val loss: 1.980.. Train L1 norm: 67.503.. Val L1 norm: 5.559.. Train Linf norm: 8342.830.. Val Linf norm: 383.355\n",
            "Epoch 54/137.. Train loss: 2.002.. Val loss: 1.980.. Train L1 norm: 71.924.. Val L1 norm: 5.534.. Train Linf norm: 8907.952.. Val Linf norm: 381.462\n",
            "Epoch 55/137.. Train loss: 2.011.. Val loss: 1.980.. Train L1 norm: 69.979.. Val L1 norm: 5.590.. Train Linf norm: 8646.828.. Val Linf norm: 385.612\n",
            "Epoch 56/137.. Train loss: 2.003.. Val loss: 1.980.. Train L1 norm: 75.647.. Val L1 norm: 5.617.. Train Linf norm: 9384.467.. Val Linf norm: 387.614\n",
            "Epoch 57/137.. Train loss: 1.996.. Val loss: 1.980.. Train L1 norm: 69.599.. Val L1 norm: 5.641.. Train Linf norm: 8606.428.. Val Linf norm: 389.440\n",
            "Epoch 58/137.. Train loss: 1.996.. Val loss: 1.980.. Train L1 norm: 72.529.. Val L1 norm: 5.666.. Train Linf norm: 8965.315.. Val Linf norm: 391.290\n",
            "Epoch 59/137.. Train loss: 1.998.. Val loss: 1.980.. Train L1 norm: 76.399.. Val L1 norm: 5.650.. Train Linf norm: 9468.653.. Val Linf norm: 390.072\n",
            "Epoch 60/137.. Train loss: 2.008.. Val loss: 1.980.. Train L1 norm: 77.714.. Val L1 norm: 5.670.. Train Linf norm: 9646.348.. Val Linf norm: 391.549\n",
            "Epoch 61/137.. Train loss: 2.002.. Val loss: 1.980.. Train L1 norm: 73.227.. Val L1 norm: 5.648.. Train Linf norm: 9059.344.. Val Linf norm: 389.893\n",
            "Epoch 62/137.. Train loss: 1.997.. Val loss: 1.980.. Train L1 norm: 80.627.. Val L1 norm: 5.630.. Train Linf norm: 10015.584.. Val Linf norm: 388.571\n",
            "Epoch 63/137.. Train loss: 2.013.. Val loss: 1.980.. Train L1 norm: 71.312.. Val L1 norm: 5.654.. Train Linf norm: 8821.762.. Val Linf norm: 390.351\n",
            "Epoch 64/137.. Train loss: 2.015.. Val loss: 1.980.. Train L1 norm: 73.123.. Val L1 norm: 5.631.. Train Linf norm: 9053.418.. Val Linf norm: 388.638\n",
            "Epoch 65/137.. Train loss: 2.014.. Val loss: 1.980.. Train L1 norm: 77.495.. Val L1 norm: 5.613.. Train Linf norm: 9618.140.. Val Linf norm: 387.335\n",
            "Epoch 66/137.. Train loss: 2.013.. Val loss: 1.980.. Train L1 norm: 66.866.. Val L1 norm: 5.605.. Train Linf norm: 8259.560.. Val Linf norm: 386.725\n",
            "Epoch 67/137.. Train loss: 2.003.. Val loss: 1.980.. Train L1 norm: 73.641.. Val L1 norm: 5.616.. Train Linf norm: 9122.493.. Val Linf norm: 387.532\n",
            "Epoch 68/137.. Train loss: 2.005.. Val loss: 1.980.. Train L1 norm: 80.594.. Val L1 norm: 5.606.. Train Linf norm: 10014.232.. Val Linf norm: 386.859\n",
            "Epoch 69/137.. Train loss: 1.999.. Val loss: 1.980.. Train L1 norm: 71.955.. Val L1 norm: 5.619.. Train Linf norm: 8908.441.. Val Linf norm: 387.759\n",
            "Epoch 70/137.. Train loss: 2.002.. Val loss: 1.980.. Train L1 norm: 74.518.. Val L1 norm: 5.632.. Train Linf norm: 9239.109.. Val Linf norm: 388.716\n",
            "Epoch 71/137.. Train loss: 2.028.. Val loss: 1.980.. Train L1 norm: 70.664.. Val L1 norm: 5.620.. Train Linf norm: 8740.897.. Val Linf norm: 387.886\n",
            "Epoch 72/137.. Train loss: 2.002.. Val loss: 1.980.. Train L1 norm: 71.550.. Val L1 norm: 5.612.. Train Linf norm: 8850.952.. Val Linf norm: 387.298\n",
            "Epoch 73/137.. Train loss: 2.001.. Val loss: 1.980.. Train L1 norm: 76.476.. Val L1 norm: 5.624.. Train Linf norm: 9483.635.. Val Linf norm: 388.131\n",
            "Epoch 74/137.. Train loss: 2.015.. Val loss: 1.980.. Train L1 norm: 77.168.. Val L1 norm: 5.634.. Train Linf norm: 9575.355.. Val Linf norm: 388.904\n",
            "Epoch 75/137.. Train loss: 2.000.. Val loss: 1.980.. Train L1 norm: 75.823.. Val L1 norm: 5.645.. Train Linf norm: 9403.254.. Val Linf norm: 389.722\n",
            "Epoch 76/137.. Train loss: 2.017.. Val loss: 1.980.. Train L1 norm: 73.429.. Val L1 norm: 5.641.. Train Linf norm: 9092.417.. Val Linf norm: 389.387\n",
            "Epoch 77/137.. Train loss: 2.015.. Val loss: 1.980.. Train L1 norm: 70.561.. Val L1 norm: 5.636.. Train Linf norm: 8725.158.. Val Linf norm: 389.056\n",
            "Epoch 78/137.. Train loss: 2.020.. Val loss: 1.980.. Train L1 norm: 74.383.. Val L1 norm: 5.633.. Train Linf norm: 9216.384.. Val Linf norm: 388.826\n",
            "Epoch 79/137.. Train loss: 2.010.. Val loss: 1.980.. Train L1 norm: 61.413.. Val L1 norm: 5.625.. Train Linf norm: 7555.025.. Val Linf norm: 388.252\n",
            "Epoch 80/137.. Train loss: 2.009.. Val loss: 1.980.. Train L1 norm: 78.776.. Val L1 norm: 5.620.. Train Linf norm: 9782.652.. Val Linf norm: 387.877\n",
            "Epoch 81/137.. Train loss: 1.996.. Val loss: 1.980.. Train L1 norm: 74.330.. Val L1 norm: 5.625.. Train Linf norm: 9212.861.. Val Linf norm: 388.202\n",
            "Epoch 82/137.. Train loss: 2.011.. Val loss: 1.980.. Train L1 norm: 68.089.. Val L1 norm: 5.620.. Train Linf norm: 8410.776.. Val Linf norm: 387.826\n",
            "Epoch 83/137.. Train loss: 1.995.. Val loss: 1.980.. Train L1 norm: 78.410.. Val L1 norm: 5.614.. Train Linf norm: 9736.518.. Val Linf norm: 387.435\n",
            "Epoch 84/137.. Train loss: 2.000.. Val loss: 1.980.. Train L1 norm: 74.290.. Val L1 norm: 5.610.. Train Linf norm: 9207.401.. Val Linf norm: 387.109\n",
            "Epoch 85/137.. Train loss: 2.002.. Val loss: 1.980.. Train L1 norm: 73.622.. Val L1 norm: 5.606.. Train Linf norm: 9115.315.. Val Linf norm: 386.811\n",
            "Epoch 86/137.. Train loss: 1.997.. Val loss: 1.980.. Train L1 norm: 72.219.. Val L1 norm: 5.604.. Train Linf norm: 8943.543.. Val Linf norm: 386.658\n",
            "Epoch 87/137.. Train loss: 1.995.. Val loss: 1.980.. Train L1 norm: 74.159.. Val L1 norm: 5.602.. Train Linf norm: 9189.484.. Val Linf norm: 386.507\n",
            "Epoch 88/137.. Train loss: 1.998.. Val loss: 1.980.. Train L1 norm: 76.524.. Val L1 norm: 5.605.. Train Linf norm: 9490.363.. Val Linf norm: 386.711\n",
            "Epoch 89/137.. Train loss: 1.997.. Val loss: 1.980.. Train L1 norm: 72.594.. Val L1 norm: 5.602.. Train Linf norm: 8990.922.. Val Linf norm: 386.537\n",
            "Epoch 90/137.. Train loss: 2.002.. Val loss: 1.979.. Train L1 norm: 69.202.. Val L1 norm: 5.600.. Train Linf norm: 8554.365.. Val Linf norm: 386.391\n",
            "Epoch 91/137.. Train loss: 2.007.. Val loss: 1.980.. Train L1 norm: 67.355.. Val L1 norm: 5.603.. Train Linf norm: 8318.756.. Val Linf norm: 386.591\n",
            "Epoch 92/137.. Train loss: 2.005.. Val loss: 1.979.. Train L1 norm: 70.911.. Val L1 norm: 5.601.. Train Linf norm: 8773.307.. Val Linf norm: 386.411\n",
            "Epoch 93/137.. Train loss: 2.013.. Val loss: 1.979.. Train L1 norm: 66.277.. Val L1 norm: 5.598.. Train Linf norm: 8179.948.. Val Linf norm: 386.239\n",
            "Epoch 94/137.. Train loss: 2.002.. Val loss: 1.979.. Train L1 norm: 78.454.. Val L1 norm: 5.601.. Train Linf norm: 9741.543.. Val Linf norm: 386.420\n",
            "Epoch 95/137.. Train loss: 1.995.. Val loss: 1.980.. Train L1 norm: 76.922.. Val L1 norm: 5.603.. Train Linf norm: 9547.076.. Val Linf norm: 386.622\n",
            "Epoch 96/137.. Train loss: 2.010.. Val loss: 1.980.. Train L1 norm: 75.746.. Val L1 norm: 5.606.. Train Linf norm: 9395.839.. Val Linf norm: 386.812\n",
            "Epoch 97/137.. Train loss: 2.003.. Val loss: 1.980.. Train L1 norm: 75.432.. Val L1 norm: 5.604.. Train Linf norm: 9352.081.. Val Linf norm: 386.648\n",
            "Epoch 98/137.. Train loss: 1.999.. Val loss: 1.979.. Train L1 norm: 71.227.. Val L1 norm: 5.602.. Train Linf norm: 8810.676.. Val Linf norm: 386.495\n",
            "Epoch 99/137.. Train loss: 1.996.. Val loss: 1.980.. Train L1 norm: 68.057.. Val L1 norm: 5.604.. Train Linf norm: 8406.416.. Val Linf norm: 386.689\n",
            "Epoch 100/137.. Train loss: 2.008.. Val loss: 1.979.. Train L1 norm: 77.063.. Val L1 norm: 5.602.. Train Linf norm: 9564.443.. Val Linf norm: 386.514\n",
            "Epoch 101/137.. Train loss: 2.012.. Val loss: 1.979.. Train L1 norm: 71.107.. Val L1 norm: 5.600.. Train Linf norm: 8795.549.. Val Linf norm: 386.340\n",
            "Epoch 102/137.. Train loss: 1.996.. Val loss: 1.979.. Train L1 norm: 70.100.. Val L1 norm: 5.598.. Train Linf norm: 8669.914.. Val Linf norm: 386.217\n",
            "Epoch 103/137.. Train loss: 1.999.. Val loss: 1.979.. Train L1 norm: 75.535.. Val L1 norm: 5.600.. Train Linf norm: 9370.873.. Val Linf norm: 386.375\n",
            "Epoch 104/137.. Train loss: 2.001.. Val loss: 1.979.. Train L1 norm: 73.418.. Val L1 norm: 5.598.. Train Linf norm: 9094.806.. Val Linf norm: 386.210\n",
            "Epoch 105/137.. Train loss: 2.003.. Val loss: 1.979.. Train L1 norm: 77.908.. Val L1 norm: 5.595.. Train Linf norm: 9673.006.. Val Linf norm: 386.037\n",
            "Epoch 106/137.. Train loss: 2.002.. Val loss: 1.979.. Train L1 norm: 70.995.. Val L1 norm: 5.593.. Train Linf norm: 8787.365.. Val Linf norm: 385.889\n",
            "Epoch 107/137.. Train loss: 2.005.. Val loss: 1.979.. Train L1 norm: 72.563.. Val L1 norm: 5.596.. Train Linf norm: 8987.319.. Val Linf norm: 386.052\n",
            "Epoch 108/137.. Train loss: 1.999.. Val loss: 1.979.. Train L1 norm: 73.508.. Val L1 norm: 5.593.. Train Linf norm: 9102.634.. Val Linf norm: 385.890\n",
            "Epoch 109/137.. Train loss: 2.020.. Val loss: 1.979.. Train L1 norm: 70.275.. Val L1 norm: 5.596.. Train Linf norm: 8692.752.. Val Linf norm: 386.065\n",
            "Epoch 110/137.. Train loss: 1.996.. Val loss: 1.979.. Train L1 norm: 70.219.. Val L1 norm: 5.597.. Train Linf norm: 8683.335.. Val Linf norm: 386.172\n",
            "Epoch 111/137.. Train loss: 2.008.. Val loss: 1.979.. Train L1 norm: 67.795.. Val L1 norm: 5.596.. Train Linf norm: 8375.847.. Val Linf norm: 386.085\n",
            "Epoch 112/137.. Train loss: 1.997.. Val loss: 1.979.. Train L1 norm: 76.028.. Val L1 norm: 5.594.. Train Linf norm: 9429.164.. Val Linf norm: 385.896\n",
            "Epoch 113/137.. Train loss: 2.013.. Val loss: 1.979.. Train L1 norm: 71.545.. Val L1 norm: 5.596.. Train Linf norm: 8850.924.. Val Linf norm: 386.049\n",
            "Epoch 114/137.. Train loss: 1.999.. Val loss: 1.979.. Train L1 norm: 69.249.. Val L1 norm: 5.598.. Train Linf norm: 8564.813.. Val Linf norm: 386.233\n",
            "Epoch 115/137.. Train loss: 2.011.. Val loss: 1.979.. Train L1 norm: 73.689.. Val L1 norm: 5.600.. Train Linf norm: 9127.422.. Val Linf norm: 386.399\n",
            "Epoch 116/137.. Train loss: 2.004.. Val loss: 1.979.. Train L1 norm: 69.209.. Val L1 norm: 5.598.. Train Linf norm: 8558.290.. Val Linf norm: 386.235\n",
            "Epoch 117/137.. Train loss: 2.001.. Val loss: 1.979.. Train L1 norm: 75.258.. Val L1 norm: 5.601.. Train Linf norm: 9332.209.. Val Linf norm: 386.435\n",
            "Epoch 118/137.. Train loss: 2.000.. Val loss: 1.979.. Train L1 norm: 76.949.. Val L1 norm: 5.599.. Train Linf norm: 9551.212.. Val Linf norm: 386.271\n",
            "Epoch 119/137.. Train loss: 2.008.. Val loss: 1.979.. Train L1 norm: 73.580.. Val L1 norm: 5.596.. Train Linf norm: 9117.623.. Val Linf norm: 386.096\n",
            "Epoch 120/137.. Train loss: 2.011.. Val loss: 1.979.. Train L1 norm: 77.894.. Val L1 norm: 5.599.. Train Linf norm: 9669.150.. Val Linf norm: 386.295\n",
            "Epoch 121/137.. Train loss: 2.011.. Val loss: 1.979.. Train L1 norm: 66.300.. Val L1 norm: 5.602.. Train Linf norm: 8185.841.. Val Linf norm: 386.514\n",
            "Epoch 122/137.. Train loss: 1.998.. Val loss: 1.979.. Train L1 norm: 78.283.. Val L1 norm: 5.605.. Train Linf norm: 9719.683.. Val Linf norm: 386.707\n",
            "Epoch 123/137.. Train loss: 2.013.. Val loss: 1.980.. Train L1 norm: 79.002.. Val L1 norm: 5.607.. Train Linf norm: 9811.895.. Val Linf norm: 386.881\n",
            "Epoch 124/137.. Train loss: 1.997.. Val loss: 1.980.. Train L1 norm: 74.197.. Val L1 norm: 5.610.. Train Linf norm: 9199.790.. Val Linf norm: 387.093\n",
            "Epoch 125/137.. Train loss: 2.006.. Val loss: 1.980.. Train L1 norm: 70.930.. Val L1 norm: 5.612.. Train Linf norm: 8774.494.. Val Linf norm: 387.279\n",
            "Epoch 126/137.. Train loss: 1.997.. Val loss: 1.980.. Train L1 norm: 71.572.. Val L1 norm: 5.610.. Train Linf norm: 8855.673.. Val Linf norm: 387.120\n",
            "Epoch 127/137.. Train loss: 2.004.. Val loss: 1.980.. Train L1 norm: 74.710.. Val L1 norm: 5.613.. Train Linf norm: 9261.506.. Val Linf norm: 387.307\n",
            "Epoch 128/137.. Train loss: 2.006.. Val loss: 1.980.. Train L1 norm: 72.854.. Val L1 norm: 5.611.. Train Linf norm: 9025.846.. Val Linf norm: 387.152\n",
            "Epoch 129/137.. Train loss: 2.000.. Val loss: 1.980.. Train L1 norm: 78.625.. Val L1 norm: 5.613.. Train Linf norm: 9739.948.. Val Linf norm: 387.323\n",
            "Epoch 130/137.. Train loss: 1.999.. Val loss: 1.980.. Train L1 norm: 74.278.. Val L1 norm: 5.611.. Train Linf norm: 9204.273.. Val Linf norm: 387.167\n",
            "Epoch 131/137.. Train loss: 2.006.. Val loss: 1.980.. Train L1 norm: 75.554.. Val L1 norm: 5.609.. Train Linf norm: 9364.622.. Val Linf norm: 386.999\n",
            "Epoch 132/137.. Train loss: 1.997.. Val loss: 1.980.. Train L1 norm: 74.376.. Val L1 norm: 5.611.. Train Linf norm: 9214.749.. Val Linf norm: 387.197\n",
            "Epoch 133/137.. Train loss: 1.995.. Val loss: 1.980.. Train L1 norm: 71.246.. Val L1 norm: 5.609.. Train Linf norm: 8814.917.. Val Linf norm: 387.017\n",
            "Epoch 134/137.. Train loss: 2.002.. Val loss: 1.980.. Train L1 norm: 67.475.. Val L1 norm: 5.611.. Train Linf norm: 8328.801.. Val Linf norm: 387.210\n",
            "Epoch 135/137.. Train loss: 1.994.. Val loss: 1.980.. Train L1 norm: 72.046.. Val L1 norm: 5.614.. Train Linf norm: 8923.546.. Val Linf norm: 387.414\n",
            "Epoch 136/137.. Train loss: 1.996.. Val loss: 1.980.. Train L1 norm: 69.560.. Val L1 norm: 5.617.. Train Linf norm: 8599.382.. Val Linf norm: 387.606\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 06:38:13,729]\u001b[0m Trial 19 finished with value: 5.619506869061788 and parameters: {'n_layers': 8, 'n_units_0': 2577, 'n_units_1': 3589, 'n_units_2': 2256, 'n_units_3': 3905, 'n_units_4': 3390, 'n_units_5': 3419, 'n_units_6': 4060, 'n_units_7': 1598, 'hidden_activation': 'ELU', 'output_activation': 'Linear', 'loss': 'Quantile', 'optimizer': 'SGD', 'lr': 1.0136180945341933e-06, 'batch_size': 128, 'n_epochs': 137, 'scheduler': 'ReduceLROnPlateau', 'dropout_rate': 0.18496931985027687, 'weight_decay': 0.001250389560574206, 'momentum': 0.9471395725540319, 'factor': 0.4805499852960079, 'patience': 9, 'threshold': 0.00011696558558783301}. Best is trial 12 with value: 1.1513660175323486.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 137/137.. Train loss: 2.011.. Val loss: 1.980.. Train L1 norm: 71.542.. Val L1 norm: 5.620.. Train Linf norm: 8854.441.. Val Linf norm: 387.810\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 06:38:18,913]\u001b[0m Trial 20 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/88.. Train loss: 241028.875.. Val loss: 4770.420.. Train L1 norm: 765.014.. Val L1 norm: 18.943.. Train Linf norm: 48103.251.. Val Linf norm: 749.348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 06:38:21,221]\u001b[0m Trial 21 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/69.. Train loss: 42287154418.250.. Val loss: 12374275.669.. Train L1 norm: 139776.194.. Val L1 norm: 241.773.. Train Linf norm: 70261389.389.. Val Linf norm: 48737.534\n",
            "Epoch 1/114.. Train loss: 17625556913448404.000.. Val loss: 54.969.. Train L1 norm: 93520.530.. Val L1 norm: 1.697.. Train Linf norm: 78746586.097.. Val Linf norm: 642.891\n",
            "Epoch 2/114.. Train loss: 340.088.. Val loss: 47.445.. Train L1 norm: 3.116.. Val L1 norm: 1.781.. Train Linf norm: 3585.405.. Val Linf norm: 723.806\n",
            "Epoch 3/114.. Train loss: 947.147.. Val loss: 55.724.. Train L1 norm: 6.291.. Val L1 norm: 1.010.. Train Linf norm: 10137.540.. Val Linf norm: 10.782\n",
            "Epoch 4/114.. Train loss: 1625.465.. Val loss: 52.009.. Train L1 norm: 10.157.. Val L1 norm: 1.179.. Train Linf norm: 18058.108.. Val Linf norm: 200.361\n",
            "Epoch 5/114.. Train loss: 2848.112.. Val loss: 68.592.. Train L1 norm: 14.236.. Val L1 norm: 1.374.. Train Linf norm: 26451.637.. Val Linf norm: 294.801\n",
            "Epoch 6/114.. Train loss: 6538.768.. Val loss: 67.721.. Train L1 norm: 10.860.. Val L1 norm: 1.077.. Train Linf norm: 19207.534.. Val Linf norm: 67.833\n",
            "Epoch 7/114.. Train loss: 192.904.. Val loss: 77.143.. Train L1 norm: 8.748.. Val L1 norm: 1.707.. Train Linf norm: 14945.485.. Val Linf norm: 542.785\n",
            "Epoch 8/114.. Train loss: 13418.093.. Val loss: 53.137.. Train L1 norm: 22.121.. Val L1 norm: 1.182.. Train Linf norm: 41783.790.. Val Linf norm: 221.053\n",
            "Epoch 9/114.. Train loss: 7414.958.. Val loss: 65.764.. Train L1 norm: 28.232.. Val L1 norm: 1.339.. Train Linf norm: 54427.272.. Val Linf norm: 260.826\n",
            "Epoch 10/114.. Train loss: 1320.397.. Val loss: 50.484.. Train L1 norm: 6.520.. Val L1 norm: 2.127.. Train Linf norm: 9043.140.. Val Linf norm: 1001.978\n",
            "Epoch 11/114.. Train loss: 12595.282.. Val loss: 53.088.. Train L1 norm: 13.801.. Val L1 norm: 1.127.. Train Linf norm: 25518.851.. Val Linf norm: 158.167\n",
            "Epoch 12/114.. Train loss: 942.917.. Val loss: 53.393.. Train L1 norm: 3.027.. Val L1 norm: 1.117.. Train Linf norm: 3987.810.. Val Linf norm: 147.552\n",
            "Epoch 13/114.. Train loss: 496.694.. Val loss: 54.668.. Train L1 norm: 6.510.. Val L1 norm: 1.062.. Train Linf norm: 11088.398.. Val Linf norm: 87.493\n",
            "Epoch 14/114.. Train loss: 543.975.. Val loss: 55.734.. Train L1 norm: 3.761.. Val L1 norm: 1.027.. Train Linf norm: 5535.062.. Val Linf norm: 44.079\n",
            "Epoch 15/114.. Train loss: 170.854.. Val loss: 56.423.. Train L1 norm: 2.709.. Val L1 norm: 1.014.. Train Linf norm: 3394.727.. Val Linf norm: 20.378\n",
            "Epoch 16/114.. Train loss: 549.583.. Val loss: 57.182.. Train L1 norm: 6.692.. Val L1 norm: 1.023.. Train Linf norm: 11582.673.. Val Linf norm: 20.690\n",
            "Epoch 17/114.. Train loss: 3740.083.. Val loss: 54.320.. Train L1 norm: 1.536.. Val L1 norm: 1.088.. Train Linf norm: 972.672.. Val Linf norm: 120.027\n",
            "Epoch 18/114.. Train loss: 2945.733.. Val loss: 56.807.. Train L1 norm: 1.441.. Val L1 norm: 1.015.. Train Linf norm: 719.042.. Val Linf norm: 12.795\n",
            "Epoch 19/114.. Train loss: 386.695.. Val loss: 56.861.. Train L1 norm: 3.637.. Val L1 norm: 1.015.. Train Linf norm: 5326.982.. Val Linf norm: 11.540\n",
            "Epoch 20/114.. Train loss: 1376.805.. Val loss: 56.715.. Train L1 norm: 1.784.. Val L1 norm: 1.015.. Train Linf norm: 1520.415.. Val Linf norm: 15.159\n",
            "Epoch 21/114.. Train loss: 184.170.. Val loss: 56.660.. Train L1 norm: 2.822.. Val L1 norm: 1.015.. Train Linf norm: 3652.397.. Val Linf norm: 16.495\n",
            "Epoch 22/114.. Train loss: 285.889.. Val loss: 56.706.. Train L1 norm: 4.897.. Val L1 norm: 1.015.. Train Linf norm: 7900.934.. Val Linf norm: 15.288\n",
            "Epoch 23/114.. Train loss: 1053.160.. Val loss: 56.581.. Train L1 norm: 2.470.. Val L1 norm: 1.015.. Train Linf norm: 2920.009.. Val Linf norm: 17.861\n",
            "Epoch 24/114.. Train loss: 114.119.. Val loss: 56.599.. Train L1 norm: 3.584.. Val L1 norm: 1.015.. Train Linf norm: 5209.880.. Val Linf norm: 17.372\n",
            "Epoch 25/114.. Train loss: 1225.885.. Val loss: 56.707.. Train L1 norm: 4.978.. Val L1 norm: 1.015.. Train Linf norm: 8038.395.. Val Linf norm: 14.573\n",
            "Epoch 26/114.. Train loss: 1546.044.. Val loss: 56.582.. Train L1 norm: 4.654.. Val L1 norm: 1.015.. Train Linf norm: 7412.057.. Val Linf norm: 17.129\n",
            "Epoch 27/114.. Train loss: 110.801.. Val loss: 56.584.. Train L1 norm: 3.238.. Val L1 norm: 1.015.. Train Linf norm: 4473.370.. Val Linf norm: 17.073\n",
            "Epoch 28/114.. Train loss: 1523.745.. Val loss: 56.568.. Train L1 norm: 10.220.. Val L1 norm: 1.015.. Train Linf norm: 18806.032.. Val Linf norm: 17.460\n",
            "Epoch 29/114.. Train loss: 1278.327.. Val loss: 56.577.. Train L1 norm: 2.320.. Val L1 norm: 1.015.. Train Linf norm: 2567.450.. Val Linf norm: 17.277\n",
            "Epoch 30/114.. Train loss: 1584.467.. Val loss: 56.581.. Train L1 norm: 2.546.. Val L1 norm: 1.015.. Train Linf norm: 3086.935.. Val Linf norm: 17.164\n",
            "Epoch 31/114.. Train loss: 137.595.. Val loss: 56.577.. Train L1 norm: 2.348.. Val L1 norm: 1.015.. Train Linf norm: 2648.111.. Val Linf norm: 17.276\n",
            "Epoch 32/114.. Train loss: 12823.396.. Val loss: 56.603.. Train L1 norm: 6.866.. Val L1 norm: 1.015.. Train Linf norm: 11934.539.. Val Linf norm: 16.651\n",
            "Epoch 33/114.. Train loss: 1916.816.. Val loss: 56.613.. Train L1 norm: 3.880.. Val L1 norm: 1.015.. Train Linf norm: 5821.837.. Val Linf norm: 16.419\n",
            "Epoch 34/114.. Train loss: 275.565.. Val loss: 56.609.. Train L1 norm: 3.704.. Val L1 norm: 1.015.. Train Linf norm: 5460.626.. Val Linf norm: 16.525\n",
            "Epoch 35/114.. Train loss: 329.177.. Val loss: 56.609.. Train L1 norm: 2.770.. Val L1 norm: 1.015.. Train Linf norm: 3552.469.. Val Linf norm: 16.513\n",
            "Epoch 36/114.. Train loss: 201.645.. Val loss: 56.609.. Train L1 norm: 1.962.. Val L1 norm: 1.015.. Train Linf norm: 1839.058.. Val Linf norm: 16.519\n",
            "Epoch 37/114.. Train loss: 66.399.. Val loss: 56.609.. Train L1 norm: 9.473.. Val L1 norm: 1.015.. Train Linf norm: 17272.388.. Val Linf norm: 16.517\n",
            "Epoch 38/114.. Train loss: 638.887.. Val loss: 56.608.. Train L1 norm: 1.163.. Val L1 norm: 1.015.. Train Linf norm: 246.295.. Val Linf norm: 16.534\n",
            "Epoch 39/114.. Train loss: 549.623.. Val loss: 56.608.. Train L1 norm: 4.446.. Val L1 norm: 1.015.. Train Linf norm: 6987.357.. Val Linf norm: 16.545\n",
            "Epoch 40/114.. Train loss: 120.039.. Val loss: 56.607.. Train L1 norm: 2.057.. Val L1 norm: 1.015.. Train Linf norm: 2094.321.. Val Linf norm: 16.550\n",
            "Epoch 41/114.. Train loss: 139.566.. Val loss: 56.607.. Train L1 norm: 6.417.. Val L1 norm: 1.015.. Train Linf norm: 11015.105.. Val Linf norm: 16.555\n",
            "Epoch 42/114.. Train loss: 975.368.. Val loss: 56.606.. Train L1 norm: 3.163.. Val L1 norm: 1.015.. Train Linf norm: 4358.128.. Val Linf norm: 16.581\n",
            "Epoch 43/114.. Train loss: 117.157.. Val loss: 56.606.. Train L1 norm: 3.327.. Val L1 norm: 1.015.. Train Linf norm: 4687.396.. Val Linf norm: 16.581\n",
            "Epoch 44/114.. Train loss: 96.003.. Val loss: 56.606.. Train L1 norm: 3.018.. Val L1 norm: 1.015.. Train Linf norm: 4048.971.. Val Linf norm: 16.580\n",
            "Epoch 45/114.. Train loss: 1462.993.. Val loss: 56.606.. Train L1 norm: 2.155.. Val L1 norm: 1.015.. Train Linf norm: 2284.301.. Val Linf norm: 16.577\n",
            "Epoch 46/114.. Train loss: 883.741.. Val loss: 56.606.. Train L1 norm: 8.052.. Val L1 norm: 1.015.. Train Linf norm: 14350.161.. Val Linf norm: 16.576\n",
            "Epoch 47/114.. Train loss: 65.475.. Val loss: 56.606.. Train L1 norm: 3.945.. Val L1 norm: 1.015.. Train Linf norm: 5957.824.. Val Linf norm: 16.576\n",
            "Epoch 48/114.. Train loss: 66.750.. Val loss: 56.606.. Train L1 norm: 4.733.. Val L1 norm: 1.015.. Train Linf norm: 7568.381.. Val Linf norm: 16.576\n",
            "Epoch 49/114.. Train loss: 62.045.. Val loss: 56.606.. Train L1 norm: 2.234.. Val L1 norm: 1.015.. Train Linf norm: 2456.266.. Val Linf norm: 16.576\n",
            "Epoch 50/114.. Train loss: 2456.789.. Val loss: 56.606.. Train L1 norm: 3.075.. Val L1 norm: 1.015.. Train Linf norm: 4172.095.. Val Linf norm: 16.574\n",
            "Epoch 51/114.. Train loss: 129.608.. Val loss: 56.606.. Train L1 norm: 2.108.. Val L1 norm: 1.015.. Train Linf norm: 2191.268.. Val Linf norm: 16.574\n",
            "Epoch 52/114.. Train loss: 893.783.. Val loss: 56.606.. Train L1 norm: 1.522.. Val L1 norm: 1.015.. Train Linf norm: 961.675.. Val Linf norm: 16.574\n",
            "Epoch 53/114.. Train loss: 1006.523.. Val loss: 56.606.. Train L1 norm: 2.924.. Val L1 norm: 1.015.. Train Linf norm: 3863.753.. Val Linf norm: 16.573\n",
            "Epoch 54/114.. Train loss: 60.176.. Val loss: 56.606.. Train L1 norm: 1.877.. Val L1 norm: 1.015.. Train Linf norm: 1718.036.. Val Linf norm: 16.573\n",
            "Epoch 55/114.. Train loss: 71.778.. Val loss: 56.606.. Train L1 norm: 3.369.. Val L1 norm: 1.015.. Train Linf norm: 4773.960.. Val Linf norm: 16.573\n",
            "Epoch 56/114.. Train loss: 996.001.. Val loss: 56.606.. Train L1 norm: 5.149.. Val L1 norm: 1.015.. Train Linf norm: 8421.727.. Val Linf norm: 16.573\n",
            "Epoch 57/114.. Train loss: 91.034.. Val loss: 56.606.. Train L1 norm: 1.857.. Val L1 norm: 1.015.. Train Linf norm: 1675.360.. Val Linf norm: 16.573\n",
            "Epoch 58/114.. Train loss: 120.210.. Val loss: 56.606.. Train L1 norm: 3.966.. Val L1 norm: 1.015.. Train Linf norm: 5996.593.. Val Linf norm: 16.573\n",
            "Epoch 59/114.. Train loss: 152.852.. Val loss: 56.606.. Train L1 norm: 2.068.. Val L1 norm: 1.015.. Train Linf norm: 2114.260.. Val Linf norm: 16.573\n",
            "Epoch 60/114.. Train loss: 166.091.. Val loss: 56.606.. Train L1 norm: 2.856.. Val L1 norm: 1.015.. Train Linf norm: 3726.344.. Val Linf norm: 16.573\n",
            "Epoch 61/114.. Train loss: 322.395.. Val loss: 56.606.. Train L1 norm: 2.037.. Val L1 norm: 1.015.. Train Linf norm: 2048.195.. Val Linf norm: 16.573\n",
            "Epoch 62/114.. Train loss: 199.273.. Val loss: 56.606.. Train L1 norm: 2.165.. Val L1 norm: 1.015.. Train Linf norm: 2315.716.. Val Linf norm: 16.573\n",
            "Epoch 63/114.. Train loss: 149.568.. Val loss: 56.606.. Train L1 norm: 5.172.. Val L1 norm: 1.015.. Train Linf norm: 8472.867.. Val Linf norm: 16.573\n",
            "Epoch 64/114.. Train loss: 1493.770.. Val loss: 56.606.. Train L1 norm: 2.535.. Val L1 norm: 1.015.. Train Linf norm: 3057.913.. Val Linf norm: 16.573\n",
            "Epoch 65/114.. Train loss: 3459.786.. Val loss: 56.606.. Train L1 norm: 4.703.. Val L1 norm: 1.015.. Train Linf norm: 7510.394.. Val Linf norm: 16.573\n",
            "Epoch 66/114.. Train loss: 61.851.. Val loss: 56.606.. Train L1 norm: 3.279.. Val L1 norm: 1.015.. Train Linf norm: 4596.731.. Val Linf norm: 16.573\n",
            "Epoch 67/114.. Train loss: 306.964.. Val loss: 56.606.. Train L1 norm: 1.496.. Val L1 norm: 1.015.. Train Linf norm: 931.519.. Val Linf norm: 16.573\n",
            "Epoch 68/114.. Train loss: 644.511.. Val loss: 56.606.. Train L1 norm: 2.016.. Val L1 norm: 1.015.. Train Linf norm: 1996.917.. Val Linf norm: 16.573\n",
            "Epoch 69/114.. Train loss: 1268.931.. Val loss: 56.606.. Train L1 norm: 2.894.. Val L1 norm: 1.015.. Train Linf norm: 3808.056.. Val Linf norm: 16.573\n",
            "Epoch 70/114.. Train loss: 1467.039.. Val loss: 56.606.. Train L1 norm: 1.899.. Val L1 norm: 1.015.. Train Linf norm: 1766.294.. Val Linf norm: 16.573\n",
            "Epoch 71/114.. Train loss: 244.324.. Val loss: 56.606.. Train L1 norm: 6.793.. Val L1 norm: 1.015.. Train Linf norm: 11781.326.. Val Linf norm: 16.573\n",
            "Epoch 72/114.. Train loss: 73.117.. Val loss: 56.606.. Train L1 norm: 2.117.. Val L1 norm: 1.015.. Train Linf norm: 2203.338.. Val Linf norm: 16.573\n",
            "Epoch 73/114.. Train loss: 89.366.. Val loss: 56.606.. Train L1 norm: 6.505.. Val L1 norm: 1.015.. Train Linf norm: 11189.362.. Val Linf norm: 16.573\n",
            "Epoch 74/114.. Train loss: 108.370.. Val loss: 56.606.. Train L1 norm: 3.542.. Val L1 norm: 1.015.. Train Linf norm: 5124.080.. Val Linf norm: 16.573\n",
            "Epoch 75/114.. Train loss: 2620.730.. Val loss: 56.606.. Train L1 norm: 1.887.. Val L1 norm: 1.015.. Train Linf norm: 1739.840.. Val Linf norm: 16.572\n",
            "Epoch 76/114.. Train loss: 113.451.. Val loss: 56.606.. Train L1 norm: 6.021.. Val L1 norm: 1.015.. Train Linf norm: 10206.132.. Val Linf norm: 16.572\n",
            "Epoch 77/114.. Train loss: 3103.619.. Val loss: 56.606.. Train L1 norm: 1.964.. Val L1 norm: 1.015.. Train Linf norm: 1865.985.. Val Linf norm: 16.573\n",
            "Epoch 78/114.. Train loss: 60.280.. Val loss: 56.606.. Train L1 norm: 2.042.. Val L1 norm: 1.015.. Train Linf norm: 2053.172.. Val Linf norm: 16.573\n",
            "Epoch 79/114.. Train loss: 62.644.. Val loss: 56.606.. Train L1 norm: 4.385.. Val L1 norm: 1.015.. Train Linf norm: 6860.050.. Val Linf norm: 16.573\n",
            "Epoch 80/114.. Train loss: 254.573.. Val loss: 56.606.. Train L1 norm: 6.393.. Val L1 norm: 1.015.. Train Linf norm: 10977.329.. Val Linf norm: 16.573\n",
            "Epoch 81/114.. Train loss: 156.975.. Val loss: 56.606.. Train L1 norm: 4.351.. Val L1 norm: 1.015.. Train Linf norm: 6787.254.. Val Linf norm: 16.573\n",
            "Epoch 82/114.. Train loss: 118.931.. Val loss: 56.606.. Train L1 norm: 1.190.. Val L1 norm: 1.015.. Train Linf norm: 307.066.. Val Linf norm: 16.573\n",
            "Epoch 83/114.. Train loss: 321.766.. Val loss: 56.606.. Train L1 norm: 3.981.. Val L1 norm: 1.015.. Train Linf norm: 6024.449.. Val Linf norm: 16.573\n",
            "Epoch 84/114.. Train loss: 180.586.. Val loss: 56.606.. Train L1 norm: 1.764.. Val L1 norm: 1.015.. Train Linf norm: 1488.003.. Val Linf norm: 16.573\n",
            "Epoch 85/114.. Train loss: 739.221.. Val loss: 56.606.. Train L1 norm: 6.354.. Val L1 norm: 1.015.. Train Linf norm: 10892.965.. Val Linf norm: 16.573\n",
            "Epoch 86/114.. Train loss: 263.613.. Val loss: 56.606.. Train L1 norm: 2.703.. Val L1 norm: 1.015.. Train Linf norm: 3403.701.. Val Linf norm: 16.573\n",
            "Epoch 87/114.. Train loss: 659.045.. Val loss: 56.606.. Train L1 norm: 2.927.. Val L1 norm: 1.015.. Train Linf norm: 3855.128.. Val Linf norm: 16.573\n",
            "Epoch 88/114.. Train loss: 115.556.. Val loss: 56.606.. Train L1 norm: 4.636.. Val L1 norm: 1.015.. Train Linf norm: 7368.483.. Val Linf norm: 16.573\n",
            "Epoch 89/114.. Train loss: 68.665.. Val loss: 56.606.. Train L1 norm: 5.361.. Val L1 norm: 1.015.. Train Linf norm: 8848.610.. Val Linf norm: 16.573\n",
            "Epoch 90/114.. Train loss: 120.509.. Val loss: 56.606.. Train L1 norm: 1.628.. Val L1 norm: 1.015.. Train Linf norm: 1206.503.. Val Linf norm: 16.573\n",
            "Epoch 91/114.. Train loss: 582.776.. Val loss: 56.606.. Train L1 norm: 1.819.. Val L1 norm: 1.015.. Train Linf norm: 1609.998.. Val Linf norm: 16.573\n",
            "Epoch 92/114.. Train loss: 104.601.. Val loss: 56.606.. Train L1 norm: 5.127.. Val L1 norm: 1.015.. Train Linf norm: 8317.417.. Val Linf norm: 16.573\n",
            "Epoch 93/114.. Train loss: 449.296.. Val loss: 56.606.. Train L1 norm: 1.209.. Val L1 norm: 1.015.. Train Linf norm: 333.909.. Val Linf norm: 16.573\n",
            "Epoch 94/114.. Train loss: 91.951.. Val loss: 56.606.. Train L1 norm: 2.431.. Val L1 norm: 1.015.. Train Linf norm: 2866.442.. Val Linf norm: 16.573\n",
            "Epoch 95/114.. Train loss: 342.185.. Val loss: 56.606.. Train L1 norm: 4.238.. Val L1 norm: 1.015.. Train Linf norm: 6546.146.. Val Linf norm: 16.572\n",
            "Epoch 96/114.. Train loss: 236.354.. Val loss: 56.606.. Train L1 norm: 1.741.. Val L1 norm: 1.015.. Train Linf norm: 1430.959.. Val Linf norm: 16.572\n",
            "Epoch 97/114.. Train loss: 1072.976.. Val loss: 56.606.. Train L1 norm: 1.106.. Val L1 norm: 1.015.. Train Linf norm: 133.285.. Val Linf norm: 16.572\n",
            "Epoch 98/114.. Train loss: 113.897.. Val loss: 56.606.. Train L1 norm: 2.379.. Val L1 norm: 1.015.. Train Linf norm: 2742.212.. Val Linf norm: 16.572\n",
            "Epoch 99/114.. Train loss: 847.337.. Val loss: 56.606.. Train L1 norm: 3.622.. Val L1 norm: 1.015.. Train Linf norm: 5286.653.. Val Linf norm: 16.572\n",
            "Epoch 100/114.. Train loss: 2049.561.. Val loss: 56.607.. Train L1 norm: 3.419.. Val L1 norm: 1.015.. Train Linf norm: 4870.146.. Val Linf norm: 16.572\n",
            "Epoch 101/114.. Train loss: 729.880.. Val loss: 56.607.. Train L1 norm: 9.007.. Val L1 norm: 1.015.. Train Linf norm: 16317.449.. Val Linf norm: 16.571\n",
            "Epoch 102/114.. Train loss: 570.403.. Val loss: 56.607.. Train L1 norm: 2.358.. Val L1 norm: 1.015.. Train Linf norm: 2695.286.. Val Linf norm: 16.571\n",
            "Epoch 103/114.. Train loss: 149.809.. Val loss: 56.607.. Train L1 norm: 2.034.. Val L1 norm: 1.015.. Train Linf norm: 2047.110.. Val Linf norm: 16.571\n",
            "Epoch 104/114.. Train loss: 1341.312.. Val loss: 56.607.. Train L1 norm: 6.466.. Val L1 norm: 1.015.. Train Linf norm: 11115.455.. Val Linf norm: 16.571\n",
            "Epoch 105/114.. Train loss: 66.833.. Val loss: 56.607.. Train L1 norm: 1.343.. Val L1 norm: 1.015.. Train Linf norm: 625.551.. Val Linf norm: 16.571\n",
            "Epoch 106/114.. Train loss: 1635.017.. Val loss: 56.607.. Train L1 norm: 1.516.. Val L1 norm: 1.015.. Train Linf norm: 984.187.. Val Linf norm: 16.572\n",
            "Epoch 107/114.. Train loss: 73.323.. Val loss: 56.607.. Train L1 norm: 2.160.. Val L1 norm: 1.015.. Train Linf norm: 2293.705.. Val Linf norm: 16.572\n",
            "Epoch 108/114.. Train loss: 4035.692.. Val loss: 56.607.. Train L1 norm: 3.460.. Val L1 norm: 1.015.. Train Linf norm: 4954.781.. Val Linf norm: 16.571\n",
            "Epoch 109/114.. Train loss: 61.407.. Val loss: 56.607.. Train L1 norm: 1.108.. Val L1 norm: 1.015.. Train Linf norm: 152.876.. Val Linf norm: 16.571\n",
            "Epoch 110/114.. Train loss: 1166.958.. Val loss: 56.607.. Train L1 norm: 1.775.. Val L1 norm: 1.015.. Train Linf norm: 1506.052.. Val Linf norm: 16.571\n",
            "Epoch 111/114.. Train loss: 99.202.. Val loss: 56.607.. Train L1 norm: 2.143.. Val L1 norm: 1.015.. Train Linf norm: 2266.031.. Val Linf norm: 16.571\n",
            "Epoch 112/114.. Train loss: 213.700.. Val loss: 56.607.. Train L1 norm: 1.962.. Val L1 norm: 1.015.. Train Linf norm: 1889.551.. Val Linf norm: 16.571\n",
            "Epoch 113/114.. Train loss: 160.754.. Val loss: 56.607.. Train L1 norm: 1.150.. Val L1 norm: 1.015.. Train Linf norm: 215.519.. Val Linf norm: 16.571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 06:48:02,172]\u001b[0m Trial 22 finished with value: 1.0150012904485066 and parameters: {'n_layers': 7, 'n_units_0': 2882, 'n_units_1': 3130, 'n_units_2': 2203, 'n_units_3': 1211, 'n_units_4': 2905, 'n_units_5': 2730, 'n_units_6': 2542, 'hidden_activation': 'ReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adagrad', 'lr': 0.0037796163474596747, 'batch_size': 2048, 'n_epochs': 114, 'scheduler': 'ReduceLROnPlateau', 'dropout_rate': 0.11318047708667109, 'factor': 0.1158770269366258, 'patience': 7, 'threshold': 0.0017827349270975305}. Best is trial 22 with value: 1.0150012904485066.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 114/114.. Train loss: 327.102.. Val loss: 56.607.. Train L1 norm: 2.499.. Val L1 norm: 1.015.. Train Linf norm: 2987.325.. Val Linf norm: 16.571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 06:48:05,899]\u001b[0m Trial 23 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/114.. Train loss: 8636339.788.. Val loss: 40.548.. Train L1 norm: 171.571.. Val L1 norm: 7.080.. Train Linf norm: 182438.008.. Val Linf norm: 5502.771\n",
            "Epoch 1/129.. Train loss: 445947174981508288.000.. Val loss: 50.128.. Train L1 norm: 2946235.265.. Val L1 norm: 1.360.. Train Linf norm: 334473620.250.. Val Linf norm: 379.157\n",
            "Epoch 2/129.. Train loss: 97.686.. Val loss: 54.611.. Train L1 norm: 4.146.. Val L1 norm: 1.066.. Train Linf norm: 5993.504.. Val Linf norm: 98.020\n",
            "Epoch 3/129.. Train loss: 65.203.. Val loss: 54.582.. Train L1 norm: 2.300.. Val L1 norm: 1.069.. Train Linf norm: 2575.895.. Val Linf norm: 102.734\n",
            "Epoch 4/129.. Train loss: 61.481.. Val loss: 54.557.. Train L1 norm: 3.722.. Val L1 norm: 1.073.. Train Linf norm: 5434.227.. Val Linf norm: 106.825\n",
            "Epoch 5/129.. Train loss: 68.177.. Val loss: 54.535.. Train L1 norm: 8.743.. Val L1 norm: 1.076.. Train Linf norm: 15571.420.. Val Linf norm: 110.502\n",
            "Epoch 6/129.. Train loss: 109.398.. Val loss: 54.523.. Train L1 norm: 2.473.. Val L1 norm: 1.077.. Train Linf norm: 2741.465.. Val Linf norm: 112.535\n",
            "Epoch 7/129.. Train loss: 60.381.. Val loss: 54.514.. Train L1 norm: 2.541.. Val L1 norm: 1.078.. Train Linf norm: 3100.958.. Val Linf norm: 114.018\n",
            "Epoch 8/129.. Train loss: 60.553.. Val loss: 54.513.. Train L1 norm: 2.673.. Val L1 norm: 1.078.. Train Linf norm: 3370.522.. Val Linf norm: 114.224\n",
            "Epoch 9/129.. Train loss: 59.974.. Val loss: 54.512.. Train L1 norm: 2.705.. Val L1 norm: 1.079.. Train Linf norm: 3436.909.. Val Linf norm: 114.430\n",
            "Epoch 10/129.. Train loss: 60.081.. Val loss: 54.510.. Train L1 norm: 2.459.. Val L1 norm: 1.079.. Train Linf norm: 2937.803.. Val Linf norm: 114.636\n",
            "Epoch 11/129.. Train loss: 59.952.. Val loss: 54.509.. Train L1 norm: 2.546.. Val L1 norm: 1.079.. Train Linf norm: 3099.701.. Val Linf norm: 114.841\n",
            "Epoch 12/129.. Train loss: 59.967.. Val loss: 54.508.. Train L1 norm: 2.461.. Val L1 norm: 1.079.. Train Linf norm: 2943.896.. Val Linf norm: 115.047\n",
            "Epoch 13/129.. Train loss: 59.982.. Val loss: 54.507.. Train L1 norm: 2.788.. Val L1 norm: 1.079.. Train Linf norm: 3609.140.. Val Linf norm: 115.253\n",
            "Epoch 14/129.. Train loss: 59.958.. Val loss: 54.506.. Train L1 norm: 2.476.. Val L1 norm: 1.079.. Train Linf norm: 2928.326.. Val Linf norm: 115.295\n",
            "Epoch 15/129.. Train loss: 59.950.. Val loss: 54.506.. Train L1 norm: 2.426.. Val L1 norm: 1.079.. Train Linf norm: 2865.802.. Val Linf norm: 115.337\n",
            "Epoch 16/129.. Train loss: 59.958.. Val loss: 54.506.. Train L1 norm: 2.037.. Val L1 norm: 1.079.. Train Linf norm: 2061.650.. Val Linf norm: 115.380\n",
            "Epoch 17/129.. Train loss: 59.960.. Val loss: 54.506.. Train L1 norm: 18.273.. Val L1 norm: 1.079.. Train Linf norm: 35323.474.. Val Linf norm: 115.422\n",
            "Epoch 18/129.. Train loss: 59.988.. Val loss: 54.505.. Train L1 norm: 2.573.. Val L1 norm: 1.079.. Train Linf norm: 3166.911.. Val Linf norm: 115.465\n",
            "Epoch 19/129.. Train loss: 59.990.. Val loss: 54.505.. Train L1 norm: 2.574.. Val L1 norm: 1.079.. Train Linf norm: 3171.513.. Val Linf norm: 115.507\n",
            "Epoch 20/129.. Train loss: 59.833.. Val loss: 54.505.. Train L1 norm: 16.680.. Val L1 norm: 1.079.. Train Linf norm: 32060.096.. Val Linf norm: 115.516\n",
            "Epoch 21/129.. Train loss: 59.948.. Val loss: 54.505.. Train L1 norm: 2.737.. Val L1 norm: 1.079.. Train Linf norm: 3506.572.. Val Linf norm: 115.525\n",
            "Epoch 22/129.. Train loss: 60.463.. Val loss: 54.505.. Train L1 norm: 2.782.. Val L1 norm: 1.079.. Train Linf norm: 3590.882.. Val Linf norm: 115.533\n",
            "Epoch 23/129.. Train loss: 59.958.. Val loss: 54.505.. Train L1 norm: 2.431.. Val L1 norm: 1.079.. Train Linf norm: 2877.002.. Val Linf norm: 115.542\n",
            "Epoch 24/129.. Train loss: 59.946.. Val loss: 54.505.. Train L1 norm: 2.610.. Val L1 norm: 1.079.. Train Linf norm: 3244.942.. Val Linf norm: 115.551\n",
            "Epoch 25/129.. Train loss: 59.997.. Val loss: 54.505.. Train L1 norm: 2.083.. Val L1 norm: 1.079.. Train Linf norm: 2163.286.. Val Linf norm: 115.559\n",
            "Epoch 26/129.. Train loss: 60.208.. Val loss: 54.505.. Train L1 norm: 2.559.. Val L1 norm: 1.079.. Train Linf norm: 3133.421.. Val Linf norm: 115.561\n",
            "Epoch 27/129.. Train loss: 59.949.. Val loss: 54.505.. Train L1 norm: 2.409.. Val L1 norm: 1.079.. Train Linf norm: 2790.029.. Val Linf norm: 115.563\n",
            "Epoch 28/129.. Train loss: 59.939.. Val loss: 54.505.. Train L1 norm: 2.682.. Val L1 norm: 1.079.. Train Linf norm: 3388.616.. Val Linf norm: 115.565\n",
            "Epoch 29/129.. Train loss: 59.962.. Val loss: 54.505.. Train L1 norm: 2.621.. Val L1 norm: 1.080.. Train Linf norm: 3264.849.. Val Linf norm: 115.567\n",
            "Epoch 30/129.. Train loss: 59.995.. Val loss: 54.505.. Train L1 norm: 2.591.. Val L1 norm: 1.080.. Train Linf norm: 3207.168.. Val Linf norm: 115.568\n",
            "Epoch 31/129.. Train loss: 59.956.. Val loss: 54.505.. Train L1 norm: 2.481.. Val L1 norm: 1.080.. Train Linf norm: 2963.620.. Val Linf norm: 115.570\n",
            "Epoch 32/129.. Train loss: 60.050.. Val loss: 54.505.. Train L1 norm: 2.801.. Val L1 norm: 1.080.. Train Linf norm: 3638.572.. Val Linf norm: 115.571\n",
            "Epoch 33/129.. Train loss: 59.961.. Val loss: 54.505.. Train L1 norm: 2.813.. Val L1 norm: 1.080.. Train Linf norm: 3653.215.. Val Linf norm: 115.571\n",
            "Epoch 34/129.. Train loss: 59.954.. Val loss: 54.505.. Train L1 norm: 2.459.. Val L1 norm: 1.080.. Train Linf norm: 2935.431.. Val Linf norm: 115.571\n",
            "Epoch 35/129.. Train loss: 59.949.. Val loss: 54.505.. Train L1 norm: 2.601.. Val L1 norm: 1.080.. Train Linf norm: 3226.240.. Val Linf norm: 115.572\n",
            "Epoch 36/129.. Train loss: 59.951.. Val loss: 54.505.. Train L1 norm: 2.553.. Val L1 norm: 1.080.. Train Linf norm: 3127.700.. Val Linf norm: 115.572\n",
            "Epoch 37/129.. Train loss: 59.959.. Val loss: 54.505.. Train L1 norm: 2.460.. Val L1 norm: 1.080.. Train Linf norm: 2932.996.. Val Linf norm: 115.572\n",
            "Epoch 38/129.. Train loss: 59.970.. Val loss: 54.505.. Train L1 norm: 2.585.. Val L1 norm: 1.080.. Train Linf norm: 3189.160.. Val Linf norm: 115.572\n",
            "Epoch 39/129.. Train loss: 59.974.. Val loss: 54.505.. Train L1 norm: 2.663.. Val L1 norm: 1.080.. Train Linf norm: 3351.820.. Val Linf norm: 115.572\n",
            "Epoch 40/129.. Train loss: 60.045.. Val loss: 54.505.. Train L1 norm: 2.740.. Val L1 norm: 1.080.. Train Linf norm: 3510.051.. Val Linf norm: 115.573\n",
            "Epoch 41/129.. Train loss: 59.966.. Val loss: 54.505.. Train L1 norm: 2.780.. Val L1 norm: 1.080.. Train Linf norm: 3593.941.. Val Linf norm: 115.573\n",
            "Epoch 42/129.. Train loss: 59.946.. Val loss: 54.505.. Train L1 norm: 2.545.. Val L1 norm: 1.080.. Train Linf norm: 3110.560.. Val Linf norm: 115.573\n",
            "Epoch 43/129.. Train loss: 59.952.. Val loss: 54.505.. Train L1 norm: 2.678.. Val L1 norm: 1.080.. Train Linf norm: 3381.572.. Val Linf norm: 115.573\n",
            "Epoch 44/129.. Train loss: 59.959.. Val loss: 54.505.. Train L1 norm: 2.543.. Val L1 norm: 1.080.. Train Linf norm: 3105.475.. Val Linf norm: 115.573\n",
            "Epoch 45/129.. Train loss: 59.959.. Val loss: 54.505.. Train L1 norm: 2.148.. Val L1 norm: 1.080.. Train Linf norm: 2265.076.. Val Linf norm: 115.573\n",
            "Epoch 46/129.. Train loss: 59.961.. Val loss: 54.505.. Train L1 norm: 2.411.. Val L1 norm: 1.080.. Train Linf norm: 2838.583.. Val Linf norm: 115.573\n",
            "Epoch 47/129.. Train loss: 59.960.. Val loss: 54.505.. Train L1 norm: 2.572.. Val L1 norm: 1.080.. Train Linf norm: 3165.091.. Val Linf norm: 115.573\n",
            "Epoch 48/129.. Train loss: 60.041.. Val loss: 54.505.. Train L1 norm: 2.385.. Val L1 norm: 1.080.. Train Linf norm: 2776.729.. Val Linf norm: 115.573\n",
            "Epoch 49/129.. Train loss: 59.944.. Val loss: 54.505.. Train L1 norm: 2.173.. Val L1 norm: 1.080.. Train Linf norm: 2343.090.. Val Linf norm: 115.573\n",
            "Epoch 50/129.. Train loss: 59.973.. Val loss: 54.505.. Train L1 norm: 2.905.. Val L1 norm: 1.080.. Train Linf norm: 3848.337.. Val Linf norm: 115.573\n",
            "Epoch 51/129.. Train loss: 60.208.. Val loss: 54.505.. Train L1 norm: 2.424.. Val L1 norm: 1.080.. Train Linf norm: 2737.474.. Val Linf norm: 115.573\n",
            "Epoch 52/129.. Train loss: 59.954.. Val loss: 54.505.. Train L1 norm: 2.691.. Val L1 norm: 1.080.. Train Linf norm: 3410.179.. Val Linf norm: 115.573\n",
            "Epoch 53/129.. Train loss: 60.020.. Val loss: 54.505.. Train L1 norm: 2.388.. Val L1 norm: 1.080.. Train Linf norm: 2787.785.. Val Linf norm: 115.573\n",
            "Epoch 54/129.. Train loss: 59.948.. Val loss: 54.505.. Train L1 norm: 2.592.. Val L1 norm: 1.080.. Train Linf norm: 3210.429.. Val Linf norm: 115.573\n",
            "Epoch 55/129.. Train loss: 60.010.. Val loss: 54.505.. Train L1 norm: 2.531.. Val L1 norm: 1.080.. Train Linf norm: 3080.009.. Val Linf norm: 115.573\n",
            "Epoch 56/129.. Train loss: 59.996.. Val loss: 54.505.. Train L1 norm: 2.407.. Val L1 norm: 1.080.. Train Linf norm: 2826.795.. Val Linf norm: 115.573\n",
            "Epoch 57/129.. Train loss: 60.076.. Val loss: 54.505.. Train L1 norm: 2.475.. Val L1 norm: 1.080.. Train Linf norm: 2967.769.. Val Linf norm: 115.573\n",
            "Epoch 58/129.. Train loss: 59.961.. Val loss: 54.505.. Train L1 norm: 2.400.. Val L1 norm: 1.080.. Train Linf norm: 2807.102.. Val Linf norm: 115.573\n",
            "Epoch 59/129.. Train loss: 59.965.. Val loss: 54.505.. Train L1 norm: 2.578.. Val L1 norm: 1.080.. Train Linf norm: 3177.787.. Val Linf norm: 115.573\n",
            "Epoch 60/129.. Train loss: 59.972.. Val loss: 54.505.. Train L1 norm: 2.331.. Val L1 norm: 1.080.. Train Linf norm: 2672.667.. Val Linf norm: 115.573\n",
            "Epoch 61/129.. Train loss: 60.018.. Val loss: 54.505.. Train L1 norm: 2.245.. Val L1 norm: 1.080.. Train Linf norm: 2497.279.. Val Linf norm: 115.573\n",
            "Epoch 62/129.. Train loss: 59.935.. Val loss: 54.505.. Train L1 norm: 2.352.. Val L1 norm: 1.080.. Train Linf norm: 2715.585.. Val Linf norm: 115.573\n",
            "Epoch 63/129.. Train loss: 59.989.. Val loss: 54.505.. Train L1 norm: 2.772.. Val L1 norm: 1.080.. Train Linf norm: 3572.763.. Val Linf norm: 115.573\n",
            "Epoch 64/129.. Train loss: 59.956.. Val loss: 54.505.. Train L1 norm: 2.414.. Val L1 norm: 1.080.. Train Linf norm: 2843.695.. Val Linf norm: 115.573\n",
            "Epoch 65/129.. Train loss: 59.951.. Val loss: 54.505.. Train L1 norm: 2.450.. Val L1 norm: 1.080.. Train Linf norm: 2914.819.. Val Linf norm: 115.573\n",
            "Epoch 66/129.. Train loss: 59.961.. Val loss: 54.505.. Train L1 norm: 2.722.. Val L1 norm: 1.080.. Train Linf norm: 3472.706.. Val Linf norm: 115.573\n",
            "Epoch 67/129.. Train loss: 59.954.. Val loss: 54.505.. Train L1 norm: 2.204.. Val L1 norm: 1.080.. Train Linf norm: 2415.592.. Val Linf norm: 115.573\n",
            "Epoch 68/129.. Train loss: 59.963.. Val loss: 54.505.. Train L1 norm: 2.851.. Val L1 norm: 1.080.. Train Linf norm: 3738.692.. Val Linf norm: 115.573\n",
            "Epoch 69/129.. Train loss: 59.965.. Val loss: 54.505.. Train L1 norm: 2.573.. Val L1 norm: 1.080.. Train Linf norm: 3168.426.. Val Linf norm: 115.573\n",
            "Epoch 70/129.. Train loss: 60.701.. Val loss: 54.505.. Train L1 norm: 2.312.. Val L1 norm: 1.080.. Train Linf norm: 2627.910.. Val Linf norm: 115.573\n",
            "Epoch 71/129.. Train loss: 59.951.. Val loss: 54.505.. Train L1 norm: 2.288.. Val L1 norm: 1.080.. Train Linf norm: 2582.139.. Val Linf norm: 115.573\n",
            "Epoch 72/129.. Train loss: 61.861.. Val loss: 54.505.. Train L1 norm: 2.990.. Val L1 norm: 1.080.. Train Linf norm: 4018.651.. Val Linf norm: 115.573\n",
            "Epoch 73/129.. Train loss: 59.968.. Val loss: 54.505.. Train L1 norm: 2.600.. Val L1 norm: 1.080.. Train Linf norm: 3221.353.. Val Linf norm: 115.573\n",
            "Epoch 74/129.. Train loss: 59.993.. Val loss: 54.505.. Train L1 norm: 2.642.. Val L1 norm: 1.080.. Train Linf norm: 3310.449.. Val Linf norm: 115.573\n",
            "Epoch 75/129.. Train loss: 59.940.. Val loss: 54.505.. Train L1 norm: 2.757.. Val L1 norm: 1.080.. Train Linf norm: 3545.961.. Val Linf norm: 115.573\n",
            "Epoch 76/129.. Train loss: 59.946.. Val loss: 54.505.. Train L1 norm: 2.807.. Val L1 norm: 1.080.. Train Linf norm: 3648.267.. Val Linf norm: 115.573\n",
            "Epoch 77/129.. Train loss: 60.488.. Val loss: 54.505.. Train L1 norm: 2.486.. Val L1 norm: 1.080.. Train Linf norm: 2991.311.. Val Linf norm: 115.573\n",
            "Epoch 78/129.. Train loss: 60.015.. Val loss: 54.505.. Train L1 norm: 2.534.. Val L1 norm: 1.080.. Train Linf norm: 3088.002.. Val Linf norm: 115.573\n",
            "Epoch 79/129.. Train loss: 59.962.. Val loss: 54.505.. Train L1 norm: 2.521.. Val L1 norm: 1.080.. Train Linf norm: 3063.378.. Val Linf norm: 115.573\n",
            "Epoch 80/129.. Train loss: 59.938.. Val loss: 54.505.. Train L1 norm: 2.681.. Val L1 norm: 1.080.. Train Linf norm: 3388.016.. Val Linf norm: 115.573\n",
            "Epoch 81/129.. Train loss: 60.039.. Val loss: 54.505.. Train L1 norm: 2.575.. Val L1 norm: 1.080.. Train Linf norm: 3172.073.. Val Linf norm: 115.573\n",
            "Epoch 82/129.. Train loss: 59.949.. Val loss: 54.505.. Train L1 norm: 2.535.. Val L1 norm: 1.080.. Train Linf norm: 3090.603.. Val Linf norm: 115.573\n",
            "Epoch 83/129.. Train loss: 60.055.. Val loss: 54.505.. Train L1 norm: 2.466.. Val L1 norm: 1.080.. Train Linf norm: 2942.891.. Val Linf norm: 115.573\n",
            "Epoch 84/129.. Train loss: 59.956.. Val loss: 54.505.. Train L1 norm: 2.623.. Val L1 norm: 1.080.. Train Linf norm: 3269.492.. Val Linf norm: 115.573\n",
            "Epoch 85/129.. Train loss: 60.150.. Val loss: 54.505.. Train L1 norm: 2.224.. Val L1 norm: 1.080.. Train Linf norm: 2452.927.. Val Linf norm: 115.573\n",
            "Epoch 86/129.. Train loss: 60.427.. Val loss: 54.505.. Train L1 norm: 2.690.. Val L1 norm: 1.080.. Train Linf norm: 3408.155.. Val Linf norm: 115.573\n",
            "Epoch 87/129.. Train loss: 59.959.. Val loss: 54.505.. Train L1 norm: 2.576.. Val L1 norm: 1.080.. Train Linf norm: 3175.447.. Val Linf norm: 115.573\n",
            "Epoch 88/129.. Train loss: 59.940.. Val loss: 54.505.. Train L1 norm: 2.707.. Val L1 norm: 1.080.. Train Linf norm: 3439.605.. Val Linf norm: 115.573\n",
            "Epoch 89/129.. Train loss: 60.019.. Val loss: 54.505.. Train L1 norm: 2.234.. Val L1 norm: 1.080.. Train Linf norm: 2465.683.. Val Linf norm: 115.573\n",
            "Epoch 90/129.. Train loss: 59.942.. Val loss: 54.505.. Train L1 norm: 2.503.. Val L1 norm: 1.080.. Train Linf norm: 3024.426.. Val Linf norm: 115.573\n",
            "Epoch 91/129.. Train loss: 59.961.. Val loss: 54.505.. Train L1 norm: 2.605.. Val L1 norm: 1.080.. Train Linf norm: 3232.595.. Val Linf norm: 115.573\n",
            "Epoch 92/129.. Train loss: 60.149.. Val loss: 54.505.. Train L1 norm: 2.887.. Val L1 norm: 1.080.. Train Linf norm: 3812.842.. Val Linf norm: 115.573\n",
            "Epoch 93/129.. Train loss: 59.911.. Val loss: 54.505.. Train L1 norm: 2.450.. Val L1 norm: 1.080.. Train Linf norm: 2920.495.. Val Linf norm: 115.573\n",
            "Epoch 94/129.. Train loss: 60.015.. Val loss: 54.505.. Train L1 norm: 2.579.. Val L1 norm: 1.080.. Train Linf norm: 3178.334.. Val Linf norm: 115.573\n",
            "Epoch 95/129.. Train loss: 59.970.. Val loss: 54.505.. Train L1 norm: 2.777.. Val L1 norm: 1.080.. Train Linf norm: 3587.353.. Val Linf norm: 115.573\n",
            "Epoch 96/129.. Train loss: 60.662.. Val loss: 54.505.. Train L1 norm: 2.641.. Val L1 norm: 1.080.. Train Linf norm: 3307.161.. Val Linf norm: 115.573\n",
            "Epoch 97/129.. Train loss: 59.949.. Val loss: 54.505.. Train L1 norm: 2.660.. Val L1 norm: 1.080.. Train Linf norm: 3339.305.. Val Linf norm: 115.573\n",
            "Epoch 98/129.. Train loss: 59.957.. Val loss: 54.505.. Train L1 norm: 2.576.. Val L1 norm: 1.080.. Train Linf norm: 3174.282.. Val Linf norm: 115.573\n",
            "Epoch 99/129.. Train loss: 59.967.. Val loss: 54.505.. Train L1 norm: 2.694.. Val L1 norm: 1.080.. Train Linf norm: 3406.776.. Val Linf norm: 115.573\n",
            "Epoch 100/129.. Train loss: 59.977.. Val loss: 54.505.. Train L1 norm: 2.661.. Val L1 norm: 1.080.. Train Linf norm: 3350.305.. Val Linf norm: 115.573\n",
            "Epoch 101/129.. Train loss: 60.103.. Val loss: 54.505.. Train L1 norm: 2.571.. Val L1 norm: 1.080.. Train Linf norm: 3164.103.. Val Linf norm: 115.573\n",
            "Epoch 102/129.. Train loss: 59.978.. Val loss: 54.505.. Train L1 norm: 2.331.. Val L1 norm: 1.080.. Train Linf norm: 2671.770.. Val Linf norm: 115.573\n",
            "Epoch 103/129.. Train loss: 59.960.. Val loss: 54.505.. Train L1 norm: 2.658.. Val L1 norm: 1.080.. Train Linf norm: 3321.939.. Val Linf norm: 115.573\n",
            "Epoch 104/129.. Train loss: 59.948.. Val loss: 54.505.. Train L1 norm: 2.714.. Val L1 norm: 1.080.. Train Linf norm: 3454.339.. Val Linf norm: 115.573\n",
            "Epoch 105/129.. Train loss: 60.021.. Val loss: 54.505.. Train L1 norm: 2.589.. Val L1 norm: 1.080.. Train Linf norm: 3199.910.. Val Linf norm: 115.573\n",
            "Epoch 106/129.. Train loss: 59.993.. Val loss: 54.505.. Train L1 norm: 2.547.. Val L1 norm: 1.080.. Train Linf norm: 3105.279.. Val Linf norm: 115.573\n",
            "Epoch 107/129.. Train loss: 59.984.. Val loss: 54.505.. Train L1 norm: 2.509.. Val L1 norm: 1.080.. Train Linf norm: 3031.535.. Val Linf norm: 115.573\n",
            "Epoch 108/129.. Train loss: 430.452.. Val loss: 54.505.. Train L1 norm: 2.884.. Val L1 norm: 1.080.. Train Linf norm: 3803.827.. Val Linf norm: 115.573\n",
            "Epoch 109/129.. Train loss: 59.964.. Val loss: 54.505.. Train L1 norm: 2.495.. Val L1 norm: 1.080.. Train Linf norm: 3010.393.. Val Linf norm: 115.573\n",
            "Epoch 110/129.. Train loss: 59.952.. Val loss: 54.505.. Train L1 norm: 2.319.. Val L1 norm: 1.080.. Train Linf norm: 2647.133.. Val Linf norm: 115.573\n",
            "Epoch 111/129.. Train loss: 59.967.. Val loss: 54.505.. Train L1 norm: 2.713.. Val L1 norm: 1.080.. Train Linf norm: 3454.215.. Val Linf norm: 115.573\n",
            "Epoch 112/129.. Train loss: 59.975.. Val loss: 54.505.. Train L1 norm: 3.085.. Val L1 norm: 1.080.. Train Linf norm: 4220.370.. Val Linf norm: 115.573\n",
            "Epoch 113/129.. Train loss: 59.991.. Val loss: 54.505.. Train L1 norm: 2.322.. Val L1 norm: 1.080.. Train Linf norm: 2650.729.. Val Linf norm: 115.573\n",
            "Epoch 114/129.. Train loss: 60.601.. Val loss: 54.505.. Train L1 norm: 2.517.. Val L1 norm: 1.080.. Train Linf norm: 3052.128.. Val Linf norm: 115.573\n",
            "Epoch 115/129.. Train loss: 59.996.. Val loss: 54.505.. Train L1 norm: 2.629.. Val L1 norm: 1.080.. Train Linf norm: 3284.059.. Val Linf norm: 115.573\n",
            "Epoch 116/129.. Train loss: 59.959.. Val loss: 54.505.. Train L1 norm: 2.564.. Val L1 norm: 1.080.. Train Linf norm: 3148.726.. Val Linf norm: 115.573\n",
            "Epoch 117/129.. Train loss: 59.959.. Val loss: 54.505.. Train L1 norm: 2.270.. Val L1 norm: 1.080.. Train Linf norm: 2551.712.. Val Linf norm: 115.573\n",
            "Epoch 118/129.. Train loss: 59.953.. Val loss: 54.505.. Train L1 norm: 2.508.. Val L1 norm: 1.080.. Train Linf norm: 3037.585.. Val Linf norm: 115.573\n",
            "Epoch 119/129.. Train loss: 59.931.. Val loss: 54.505.. Train L1 norm: 2.466.. Val L1 norm: 1.080.. Train Linf norm: 2947.767.. Val Linf norm: 115.573\n",
            "Epoch 120/129.. Train loss: 59.952.. Val loss: 54.505.. Train L1 norm: 2.400.. Val L1 norm: 1.080.. Train Linf norm: 2815.385.. Val Linf norm: 115.573\n",
            "Epoch 121/129.. Train loss: 59.957.. Val loss: 54.505.. Train L1 norm: 2.762.. Val L1 norm: 1.080.. Train Linf norm: 3555.638.. Val Linf norm: 115.573\n",
            "Epoch 122/129.. Train loss: 59.977.. Val loss: 54.505.. Train L1 norm: 2.374.. Val L1 norm: 1.080.. Train Linf norm: 2758.315.. Val Linf norm: 115.573\n",
            "Epoch 123/129.. Train loss: 62.440.. Val loss: 54.505.. Train L1 norm: 2.457.. Val L1 norm: 1.080.. Train Linf norm: 2928.928.. Val Linf norm: 115.573\n",
            "Epoch 124/129.. Train loss: 61.293.. Val loss: 54.505.. Train L1 norm: 2.792.. Val L1 norm: 1.080.. Train Linf norm: 3615.734.. Val Linf norm: 115.573\n",
            "Epoch 125/129.. Train loss: 59.972.. Val loss: 54.505.. Train L1 norm: 2.624.. Val L1 norm: 1.080.. Train Linf norm: 3270.754.. Val Linf norm: 115.573\n",
            "Epoch 126/129.. Train loss: 59.971.. Val loss: 54.505.. Train L1 norm: 2.537.. Val L1 norm: 1.080.. Train Linf norm: 3094.696.. Val Linf norm: 115.573\n",
            "Epoch 127/129.. Train loss: 59.975.. Val loss: 54.505.. Train L1 norm: 2.709.. Val L1 norm: 1.080.. Train Linf norm: 3449.901.. Val Linf norm: 115.573\n",
            "Epoch 128/129.. Train loss: 59.947.. Val loss: 54.505.. Train L1 norm: 2.648.. Val L1 norm: 1.080.. Train Linf norm: 3312.228.. Val Linf norm: 115.573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 06:58:53,078]\u001b[0m Trial 24 finished with value: 1.0795055260976156 and parameters: {'n_layers': 9, 'n_units_0': 2839, 'n_units_1': 3181, 'n_units_2': 1957, 'n_units_3': 1863, 'n_units_4': 3049, 'n_units_5': 3340, 'n_units_6': 1139, 'n_units_7': 48, 'n_units_8': 145, 'hidden_activation': 'ReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adagrad', 'lr': 0.011081463667925381, 'batch_size': 2048, 'n_epochs': 129, 'scheduler': 'ReduceLROnPlateau', 'dropout_rate': 0.10522947336460009, 'factor': 0.2067170774063598, 'patience': 5, 'threshold': 0.0013844568766163486}. Best is trial 22 with value: 1.0150012904485066.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 129/129.. Train loss: 62.828.. Val loss: 54.505.. Train L1 norm: 2.416.. Val L1 norm: 1.080.. Train Linf norm: 2845.763.. Val Linf norm: 115.573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 06:58:58,028]\u001b[0m Trial 25 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/143.. Train loss: 1078536935408491.375.. Val loss: 156.078.. Train L1 norm: 3001548.718.. Val L1 norm: 2.542.. Train Linf norm: 1954700899.762.. Val Linf norm: 1289.735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 06:59:04,107]\u001b[0m Trial 26 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/119.. Train loss: 22989105486084920489709840367616.000.. Val loss: 13922296044594773565942792192.000.. Train L1 norm: 515630849530288.062.. Val L1 norm: 32391606800814.352.. Train Linf norm: 1009025964039756672.000.. Val Linf norm: 18625408053446864.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 06:59:09,229]\u001b[0m Trial 27 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/130.. Train loss: 122523.498.. Val loss: 4.310.. Train L1 norm: 191826.466.. Val L1 norm: 3.298.. Train Linf norm: 137295742.381.. Val Linf norm: 1780.104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 06:59:15,470]\u001b[0m Trial 28 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/111.. Train loss: 7981462687494.353.. Val loss: 4051762.892.. Train L1 norm: 5781100973547.317.. Val L1 norm: 8002185.813.. Train Linf norm: 6407204773579551.000.. Val Linf norm: 4627737576.277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 06:59:18,244]\u001b[0m Trial 29 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/98.. Train loss: 286349480413504.750.. Val loss: 4.044.. Train L1 norm: 173977106812354.250.. Val L1 norm: 2.738.. Train Linf norm: 13603733520037278.000.. Val Linf norm: 1776.636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 06:59:23,375]\u001b[0m Trial 30 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/140.. Train loss: 5977401152565221737365504.000.. Val loss: 1378767641945474993225728.000.. Train L1 norm: 66273633170747787147476992.000.. Val L1 norm: 1249580189706835657228288.000.. Train Linf norm: 127606177761096235211647090688.000.. Val Linf norm: 590938047160725413606457344.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 06:59:29,297]\u001b[0m Trial 31 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/106.. Train loss: 711012904074596846671201398947840.000.. Val loss: 376481318875853230060058181632.000.. Train L1 norm: 3142102497978662.500.. Val L1 norm: 185772547914561.406.. Train Linf norm: 1549058092042970368.000.. Val Linf norm: 38069306765565944.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 06:59:36,303]\u001b[0m Trial 32 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/130.. Train loss: 211222969903301684619116544.000.. Val loss: 10924871943816678100434944.000.. Train L1 norm: 958044575497.318.. Val L1 norm: 81293002095.548.. Train Linf norm: 914322767186201.375.. Val Linf norm: 27531564393326.184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "\u001b[32m[I 2023-05-29 06:59:39,360]\u001b[0m Trial 33 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/119.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "\u001b[32m[I 2023-05-29 06:59:53,138]\u001b[0m Trial 34 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/130.. Train loss: inf.. Val loss: 2268979159247821342256104734720.000.. Train L1 norm: 7442267178170476.000.. Val L1 norm: 487632496618233.312.. Train Linf norm: 921884951247499008.000.. Val Linf norm: 31936087801851876.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 06:59:57,721]\u001b[0m Trial 35 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/123.. Train loss: 382701.211.. Val loss: 4035.538.. Train L1 norm: 718.733.. Val L1 norm: 20.118.. Train Linf norm: 701563.580.. Val Linf norm: 6713.891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 07:00:05,457]\u001b[0m Trial 36 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/107.. Train loss: 440.016.. Val loss: 34.853.. Train L1 norm: 80.223.. Val L1 norm: 5.541.. Train Linf norm: 19816.138.. Val Linf norm: 687.399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 07:00:21,283]\u001b[0m Trial 37 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/101.. Train loss: 43687777300.120.. Val loss: 1586328.883.. Train L1 norm: 162205755135.166.. Val L1 norm: 2623296.098.. Train Linf norm: 7280468565751.652.. Val Linf norm: 101686018.745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 07:01:09,743]\u001b[0m Trial 38 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/114.. Train loss: 9047.901.. Val loss: 4.271.. Train L1 norm: 27024.843.. Val L1 norm: 5.614.. Train Linf norm: 759401.595.. Val Linf norm: 119.115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 07:01:17,319]\u001b[0m Trial 39 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/127.. Train loss: 2503092522632554.500.. Val loss: 10137252.042.. Train L1 norm: 1183731.587.. Val L1 norm: 520.969.. Train Linf norm: 70108369.744.. Val Linf norm: 33899.579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 07:01:20,802]\u001b[0m Trial 40 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/133.. Train loss: 430954278003709760.000.. Val loss: 8146826616097682.000.. Train L1 norm: 100474358.115.. Val L1 norm: 5591039.212.. Train Linf norm: 169068288053.642.. Val Linf norm: 4022779372.058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 07:01:29,971]\u001b[0m Trial 41 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/149.. Train loss: 516121.870.. Val loss: 3.723.. Train L1 norm: 327394.041.. Val L1 norm: 5.237.. Train Linf norm: 17452385.443.. Val Linf norm: 2142.566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 07:01:59,723]\u001b[0m Trial 42 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/111.. Train loss: 420.041.. Val loss: 40.040.. Train L1 norm: 21.678.. Val L1 norm: 5.206.. Train Linf norm: 643.790.. Val Linf norm: 109.796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 07:02:36,220]\u001b[0m Trial 43 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/96.. Train loss: 2151631151.411.. Val loss: 51.509.. Train L1 norm: 626.643.. Val L1 norm: 2.915.. Train Linf norm: 8969.318.. Val Linf norm: 53.140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 07:02:59,152]\u001b[0m Trial 44 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/121.. Train loss: 6616.815.. Val loss: 4.126.. Train L1 norm: 2197.869.. Val L1 norm: 3.893.. Train Linf norm: 9998.076.. Val Linf norm: 77.425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "\u001b[32m[I 2023-05-29 07:03:35,592]\u001b[0m Trial 45 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/103.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 07:03:41,254]\u001b[0m Trial 46 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/126.. Train loss: 5888978348963444.000.. Val loss: 22207.634.. Train L1 norm: 3922401.934.. Val L1 norm: 14.073.. Train Linf norm: 273900742.057.. Val Linf norm: 5037.207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 07:03:49,823]\u001b[0m Trial 47 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/110.. Train loss: 106104307.040.. Val loss: 1248654.159.. Train L1 norm: 368821730.218.. Val L1 norm: 1842857.599.. Train Linf norm: 64629904605.689.. Val Linf norm: 381929311.944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 07:03:52,772]\u001b[0m Trial 48 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/116.. Train loss: 1488672.461.. Val loss: 48.611.. Train L1 norm: 142.156.. Val L1 norm: 6.644.. Train Linf norm: 270308.602.. Val Linf norm: 5376.698\n",
            "Epoch 1/139.. Train loss: 154.162.. Val loss: 56.386.. Train L1 norm: 2.162.. Val L1 norm: 1.051.. Train Linf norm: 1171.150.. Val Linf norm: 33.537\n",
            "Epoch 2/139.. Train loss: 597.090.. Val loss: 53.980.. Train L1 norm: 1.428.. Val L1 norm: 1.040.. Train Linf norm: 427.480.. Val Linf norm: 32.443\n",
            "Epoch 3/139.. Train loss: 561.705.. Val loss: 55.372.. Train L1 norm: 2.633.. Val L1 norm: 1.011.. Train Linf norm: 1658.553.. Val Linf norm: 9.260\n",
            "Epoch 4/139.. Train loss: 392.470.. Val loss: 53.965.. Train L1 norm: 2.439.. Val L1 norm: 1.048.. Train Linf norm: 1465.120.. Val Linf norm: 38.314\n",
            "Epoch 5/139.. Train loss: 192.312.. Val loss: 55.098.. Train L1 norm: 1.792.. Val L1 norm: 1.012.. Train Linf norm: 802.395.. Val Linf norm: 12.288\n",
            "Epoch 6/139.. Train loss: 118.092.. Val loss: 54.639.. Train L1 norm: 1.614.. Val L1 norm: 1.030.. Train Linf norm: 627.393.. Val Linf norm: 26.079\n",
            "Epoch 7/139.. Train loss: 84.513.. Val loss: 54.892.. Train L1 norm: 2.174.. Val L1 norm: 1.025.. Train Linf norm: 1196.916.. Val Linf norm: 22.313\n",
            "Epoch 8/139.. Train loss: 82.283.. Val loss: 54.439.. Train L1 norm: 2.303.. Val L1 norm: 1.043.. Train Linf norm: 1326.700.. Val Linf norm: 36.103\n",
            "Epoch 9/139.. Train loss: 59.999.. Val loss: 54.338.. Train L1 norm: 2.615.. Val L1 norm: 1.050.. Train Linf norm: 1646.269.. Val Linf norm: 41.191\n",
            "Epoch 10/139.. Train loss: 63.776.. Val loss: 54.361.. Train L1 norm: 2.744.. Val L1 norm: 1.050.. Train Linf norm: 1777.168.. Val Linf norm: 40.961\n",
            "Epoch 11/139.. Train loss: 59.787.. Val loss: 54.360.. Train L1 norm: 2.522.. Val L1 norm: 1.050.. Train Linf norm: 1549.085.. Val Linf norm: 41.362\n",
            "Epoch 12/139.. Train loss: 60.391.. Val loss: 54.368.. Train L1 norm: 2.445.. Val L1 norm: 1.051.. Train Linf norm: 1470.189.. Val Linf norm: 41.536\n",
            "Epoch 13/139.. Train loss: 61.546.. Val loss: 54.352.. Train L1 norm: 2.706.. Val L1 norm: 1.052.. Train Linf norm: 1736.638.. Val Linf norm: 42.296\n",
            "Epoch 14/139.. Train loss: 61.958.. Val loss: 54.360.. Train L1 norm: 2.515.. Val L1 norm: 1.052.. Train Linf norm: 1540.693.. Val Linf norm: 42.488\n",
            "Epoch 15/139.. Train loss: 59.665.. Val loss: 54.349.. Train L1 norm: 2.823.. Val L1 norm: 1.053.. Train Linf norm: 1859.098.. Val Linf norm: 43.143\n",
            "Epoch 16/139.. Train loss: 60.034.. Val loss: 54.318.. Train L1 norm: 2.847.. Val L1 norm: 1.055.. Train Linf norm: 1882.073.. Val Linf norm: 44.269\n",
            "Epoch 17/139.. Train loss: 59.963.. Val loss: 54.319.. Train L1 norm: 2.912.. Val L1 norm: 1.055.. Train Linf norm: 1947.126.. Val Linf norm: 44.301\n",
            "Epoch 18/139.. Train loss: 60.299.. Val loss: 54.314.. Train L1 norm: 2.908.. Val L1 norm: 1.055.. Train Linf norm: 1942.889.. Val Linf norm: 44.481\n",
            "Epoch 19/139.. Train loss: 59.635.. Val loss: 54.313.. Train L1 norm: 2.544.. Val L1 norm: 1.055.. Train Linf norm: 1564.583.. Val Linf norm: 44.561\n",
            "Epoch 20/139.. Train loss: 60.132.. Val loss: 54.315.. Train L1 norm: 2.821.. Val L1 norm: 1.055.. Train Linf norm: 1854.870.. Val Linf norm: 44.567\n",
            "Epoch 21/139.. Train loss: 59.689.. Val loss: 54.314.. Train L1 norm: 2.737.. Val L1 norm: 1.055.. Train Linf norm: 1768.204.. Val Linf norm: 44.636\n",
            "Epoch 22/139.. Train loss: 59.654.. Val loss: 54.314.. Train L1 norm: 2.665.. Val L1 norm: 1.055.. Train Linf norm: 1694.685.. Val Linf norm: 44.706\n",
            "Epoch 23/139.. Train loss: 59.691.. Val loss: 54.313.. Train L1 norm: 2.803.. Val L1 norm: 1.055.. Train Linf norm: 1831.705.. Val Linf norm: 44.763\n",
            "Epoch 24/139.. Train loss: 61.448.. Val loss: 54.314.. Train L1 norm: 2.672.. Val L1 norm: 1.055.. Train Linf norm: 1691.906.. Val Linf norm: 44.755\n",
            "Epoch 25/139.. Train loss: 62.480.. Val loss: 54.316.. Train L1 norm: 3.098.. Val L1 norm: 1.055.. Train Linf norm: 2136.897.. Val Linf norm: 44.725\n",
            "Epoch 26/139.. Train loss: 59.701.. Val loss: 54.315.. Train L1 norm: 2.929.. Val L1 norm: 1.055.. Train Linf norm: 1965.695.. Val Linf norm: 44.739\n",
            "Epoch 27/139.. Train loss: 60.498.. Val loss: 54.316.. Train L1 norm: 2.807.. Val L1 norm: 1.055.. Train Linf norm: 1838.788.. Val Linf norm: 44.736\n",
            "Epoch 28/139.. Train loss: 61.456.. Val loss: 54.317.. Train L1 norm: 3.000.. Val L1 norm: 1.055.. Train Linf norm: 2036.570.. Val Linf norm: 44.723\n",
            "Epoch 29/139.. Train loss: 59.624.. Val loss: 54.316.. Train L1 norm: 2.788.. Val L1 norm: 1.055.. Train Linf norm: 1820.050.. Val Linf norm: 44.734\n",
            "Epoch 30/139.. Train loss: 61.338.. Val loss: 54.317.. Train L1 norm: 2.817.. Val L1 norm: 1.055.. Train Linf norm: 1850.145.. Val Linf norm: 44.732\n",
            "Epoch 31/139.. Train loss: 60.056.. Val loss: 54.317.. Train L1 norm: 2.975.. Val L1 norm: 1.055.. Train Linf norm: 2012.356.. Val Linf norm: 44.741\n",
            "Epoch 32/139.. Train loss: 60.231.. Val loss: 54.317.. Train L1 norm: 2.797.. Val L1 norm: 1.055.. Train Linf norm: 1829.468.. Val Linf norm: 44.738\n",
            "Epoch 33/139.. Train loss: 59.675.. Val loss: 54.317.. Train L1 norm: 2.804.. Val L1 norm: 1.055.. Train Linf norm: 1837.251.. Val Linf norm: 44.749\n",
            "Epoch 34/139.. Train loss: 59.652.. Val loss: 54.317.. Train L1 norm: 2.812.. Val L1 norm: 1.055.. Train Linf norm: 1845.434.. Val Linf norm: 44.760\n",
            "Epoch 35/139.. Train loss: 59.618.. Val loss: 54.317.. Train L1 norm: 2.859.. Val L1 norm: 1.055.. Train Linf norm: 1892.309.. Val Linf norm: 44.773\n",
            "Epoch 36/139.. Train loss: 59.778.. Val loss: 54.317.. Train L1 norm: 2.866.. Val L1 norm: 1.055.. Train Linf norm: 1901.189.. Val Linf norm: 44.779\n",
            "Epoch 37/139.. Train loss: 59.780.. Val loss: 54.317.. Train L1 norm: 2.764.. Val L1 norm: 1.055.. Train Linf norm: 1793.754.. Val Linf norm: 44.786\n",
            "Epoch 38/139.. Train loss: 59.850.. Val loss: 54.316.. Train L1 norm: 2.878.. Val L1 norm: 1.055.. Train Linf norm: 1911.645.. Val Linf norm: 44.808\n",
            "Epoch 39/139.. Train loss: 59.731.. Val loss: 54.316.. Train L1 norm: 2.841.. Val L1 norm: 1.055.. Train Linf norm: 1875.401.. Val Linf norm: 44.817\n",
            "Epoch 40/139.. Train loss: 62.077.. Val loss: 54.318.. Train L1 norm: 2.984.. Val L1 norm: 1.055.. Train Linf norm: 2020.350.. Val Linf norm: 44.779\n",
            "Epoch 41/139.. Train loss: 59.628.. Val loss: 54.318.. Train L1 norm: 2.858.. Val L1 norm: 1.055.. Train Linf norm: 1892.722.. Val Linf norm: 44.792\n",
            "Epoch 42/139.. Train loss: 61.197.. Val loss: 54.319.. Train L1 norm: 2.742.. Val L1 norm: 1.055.. Train Linf norm: 1772.786.. Val Linf norm: 44.770\n",
            "Epoch 43/139.. Train loss: 59.640.. Val loss: 54.319.. Train L1 norm: 2.803.. Val L1 norm: 1.055.. Train Linf norm: 1834.623.. Val Linf norm: 44.783\n",
            "Epoch 44/139.. Train loss: 60.388.. Val loss: 54.320.. Train L1 norm: 2.836.. Val L1 norm: 1.055.. Train Linf norm: 1868.268.. Val Linf norm: 44.769\n",
            "Epoch 45/139.. Train loss: 59.703.. Val loss: 54.320.. Train L1 norm: 2.738.. Val L1 norm: 1.055.. Train Linf norm: 1769.165.. Val Linf norm: 44.777\n",
            "Epoch 46/139.. Train loss: 60.865.. Val loss: 54.321.. Train L1 norm: 2.853.. Val L1 norm: 1.055.. Train Linf norm: 1886.089.. Val Linf norm: 44.748\n",
            "Epoch 47/139.. Train loss: 59.664.. Val loss: 54.321.. Train L1 norm: 2.901.. Val L1 norm: 1.055.. Train Linf norm: 1934.929.. Val Linf norm: 44.761\n",
            "Epoch 48/139.. Train loss: 60.429.. Val loss: 54.322.. Train L1 norm: 2.730.. Val L1 norm: 1.055.. Train Linf norm: 1761.315.. Val Linf norm: 44.740\n",
            "Epoch 49/139.. Train loss: 60.668.. Val loss: 54.324.. Train L1 norm: 2.930.. Val L1 norm: 1.055.. Train Linf norm: 1966.771.. Val Linf norm: 44.709\n",
            "Epoch 50/139.. Train loss: 59.791.. Val loss: 54.324.. Train L1 norm: 2.817.. Val L1 norm: 1.055.. Train Linf norm: 1849.732.. Val Linf norm: 44.709\n",
            "Epoch 51/139.. Train loss: 60.810.. Val loss: 54.326.. Train L1 norm: 2.596.. Val L1 norm: 1.055.. Train Linf norm: 1622.428.. Val Linf norm: 44.676\n",
            "Epoch 52/139.. Train loss: 59.651.. Val loss: 54.326.. Train L1 norm: 2.868.. Val L1 norm: 1.055.. Train Linf norm: 1902.587.. Val Linf norm: 44.692\n",
            "Epoch 53/139.. Train loss: 60.677.. Val loss: 54.325.. Train L1 norm: 2.707.. Val L1 norm: 1.055.. Train Linf norm: 1738.220.. Val Linf norm: 44.726\n",
            "Epoch 54/139.. Train loss: 59.640.. Val loss: 54.323.. Train L1 norm: 2.764.. Val L1 norm: 1.055.. Train Linf norm: 1796.167.. Val Linf norm: 44.773\n",
            "Epoch 55/139.. Train loss: 87.906.. Val loss: 54.330.. Train L1 norm: 2.831.. Val L1 norm: 1.055.. Train Linf norm: 1864.430.. Val Linf norm: 44.600\n",
            "Epoch 56/139.. Train loss: 60.253.. Val loss: 54.331.. Train L1 norm: 2.780.. Val L1 norm: 1.055.. Train Linf norm: 1812.118.. Val Linf norm: 44.583\n",
            "Epoch 57/139.. Train loss: 61.352.. Val loss: 54.328.. Train L1 norm: 2.801.. Val L1 norm: 1.055.. Train Linf norm: 1833.298.. Val Linf norm: 44.660\n",
            "Epoch 58/139.. Train loss: 60.444.. Val loss: 54.330.. Train L1 norm: 2.901.. Val L1 norm: 1.055.. Train Linf norm: 1936.061.. Val Linf norm: 44.635\n",
            "Epoch 59/139.. Train loss: 59.689.. Val loss: 54.329.. Train L1 norm: 2.877.. Val L1 norm: 1.055.. Train Linf norm: 1911.619.. Val Linf norm: 44.659\n",
            "Epoch 60/139.. Train loss: 59.792.. Val loss: 54.328.. Train L1 norm: 2.822.. Val L1 norm: 1.055.. Train Linf norm: 1854.462.. Val Linf norm: 44.695\n",
            "Epoch 61/139.. Train loss: 60.040.. Val loss: 54.326.. Train L1 norm: 2.856.. Val L1 norm: 1.055.. Train Linf norm: 1887.667.. Val Linf norm: 44.742\n",
            "Epoch 62/139.. Train loss: 59.629.. Val loss: 54.326.. Train L1 norm: 2.787.. Val L1 norm: 1.055.. Train Linf norm: 1817.392.. Val Linf norm: 44.760\n",
            "Epoch 63/139.. Train loss: 60.503.. Val loss: 54.323.. Train L1 norm: 2.859.. Val L1 norm: 1.055.. Train Linf norm: 1893.515.. Val Linf norm: 44.831\n",
            "Epoch 64/139.. Train loss: 63.583.. Val loss: 54.328.. Train L1 norm: 2.792.. Val L1 norm: 1.055.. Train Linf norm: 1824.257.. Val Linf norm: 44.729\n",
            "Epoch 65/139.. Train loss: 59.722.. Val loss: 54.328.. Train L1 norm: 2.871.. Val L1 norm: 1.055.. Train Linf norm: 1906.201.. Val Linf norm: 44.738\n",
            "Epoch 66/139.. Train loss: 59.843.. Val loss: 54.327.. Train L1 norm: 2.883.. Val L1 norm: 1.055.. Train Linf norm: 1916.888.. Val Linf norm: 44.767\n",
            "Epoch 67/139.. Train loss: 59.803.. Val loss: 54.327.. Train L1 norm: 2.932.. Val L1 norm: 1.055.. Train Linf norm: 1967.222.. Val Linf norm: 44.784\n",
            "Epoch 68/139.. Train loss: 59.649.. Val loss: 54.326.. Train L1 norm: 2.859.. Val L1 norm: 1.055.. Train Linf norm: 1880.838.. Val Linf norm: 44.808\n",
            "Epoch 69/139.. Train loss: 59.930.. Val loss: 54.324.. Train L1 norm: 2.817.. Val L1 norm: 1.055.. Train Linf norm: 1851.560.. Val Linf norm: 44.867\n",
            "Epoch 70/139.. Train loss: 59.785.. Val loss: 54.323.. Train L1 norm: 2.704.. Val L1 norm: 1.055.. Train Linf norm: 1733.497.. Val Linf norm: 44.911\n",
            "Epoch 71/139.. Train loss: 60.027.. Val loss: 54.320.. Train L1 norm: 2.907.. Val L1 norm: 1.055.. Train Linf norm: 1941.383.. Val Linf norm: 44.975\n",
            "Epoch 72/139.. Train loss: 61.478.. Val loss: 54.318.. Train L1 norm: 2.851.. Val L1 norm: 1.056.. Train Linf norm: 1884.651.. Val Linf norm: 45.042\n",
            "Epoch 73/139.. Train loss: 60.543.. Val loss: 54.314.. Train L1 norm: 2.719.. Val L1 norm: 1.056.. Train Linf norm: 1749.712.. Val Linf norm: 45.144\n",
            "Epoch 74/139.. Train loss: 63.838.. Val loss: 54.318.. Train L1 norm: 2.865.. Val L1 norm: 1.056.. Train Linf norm: 1899.255.. Val Linf norm: 45.064\n",
            "Epoch 75/139.. Train loss: 61.440.. Val loss: 54.321.. Train L1 norm: 2.805.. Val L1 norm: 1.055.. Train Linf norm: 1838.065.. Val Linf norm: 44.998\n",
            "Epoch 76/139.. Train loss: 59.625.. Val loss: 54.321.. Train L1 norm: 2.720.. Val L1 norm: 1.055.. Train Linf norm: 1737.214.. Val Linf norm: 45.000\n",
            "Epoch 77/139.. Train loss: 59.728.. Val loss: 54.320.. Train L1 norm: 2.704.. Val L1 norm: 1.056.. Train Linf norm: 1734.372.. Val Linf norm: 45.039\n",
            "Epoch 78/139.. Train loss: 60.291.. Val loss: 54.322.. Train L1 norm: 2.850.. Val L1 norm: 1.055.. Train Linf norm: 1885.166.. Val Linf norm: 45.007\n",
            "Epoch 79/139.. Train loss: 60.369.. Val loss: 54.318.. Train L1 norm: 2.866.. Val L1 norm: 1.056.. Train Linf norm: 1899.335.. Val Linf norm: 45.098\n",
            "Epoch 80/139.. Train loss: 59.653.. Val loss: 54.317.. Train L1 norm: 2.753.. Val L1 norm: 1.056.. Train Linf norm: 1784.086.. Val Linf norm: 45.129\n",
            "Epoch 81/139.. Train loss: 61.131.. Val loss: 54.321.. Train L1 norm: 2.844.. Val L1 norm: 1.056.. Train Linf norm: 1876.245.. Val Linf norm: 45.052\n",
            "Epoch 82/139.. Train loss: 61.343.. Val loss: 54.316.. Train L1 norm: 2.829.. Val L1 norm: 1.056.. Train Linf norm: 1861.751.. Val Linf norm: 45.179\n",
            "Epoch 83/139.. Train loss: 59.617.. Val loss: 54.315.. Train L1 norm: 2.837.. Val L1 norm: 1.056.. Train Linf norm: 1866.399.. Val Linf norm: 45.209\n",
            "Epoch 84/139.. Train loss: 60.378.. Val loss: 54.312.. Train L1 norm: 2.841.. Val L1 norm: 1.056.. Train Linf norm: 1873.886.. Val Linf norm: 45.300\n",
            "Epoch 85/139.. Train loss: 59.815.. Val loss: 54.312.. Train L1 norm: 2.666.. Val L1 norm: 1.056.. Train Linf norm: 1694.883.. Val Linf norm: 45.299\n",
            "Epoch 86/139.. Train loss: 60.865.. Val loss: 54.316.. Train L1 norm: 2.763.. Val L1 norm: 1.056.. Train Linf norm: 1794.982.. Val Linf norm: 45.224\n",
            "Epoch 87/139.. Train loss: 59.709.. Val loss: 54.315.. Train L1 norm: 2.806.. Val L1 norm: 1.056.. Train Linf norm: 1839.071.. Val Linf norm: 45.257\n",
            "Epoch 88/139.. Train loss: 59.644.. Val loss: 54.313.. Train L1 norm: 2.696.. Val L1 norm: 1.056.. Train Linf norm: 1726.563.. Val Linf norm: 45.313\n",
            "Epoch 89/139.. Train loss: 59.937.. Val loss: 54.310.. Train L1 norm: 2.904.. Val L1 norm: 1.056.. Train Linf norm: 1938.989.. Val Linf norm: 45.389\n",
            "Epoch 90/139.. Train loss: 60.464.. Val loss: 54.313.. Train L1 norm: 2.806.. Val L1 norm: 1.056.. Train Linf norm: 1833.729.. Val Linf norm: 45.346\n",
            "Epoch 91/139.. Train loss: 59.631.. Val loss: 54.312.. Train L1 norm: 2.726.. Val L1 norm: 1.056.. Train Linf norm: 1755.502.. Val Linf norm: 45.372\n",
            "Epoch 92/139.. Train loss: 59.997.. Val loss: 54.314.. Train L1 norm: 2.782.. Val L1 norm: 1.056.. Train Linf norm: 1815.035.. Val Linf norm: 45.338\n",
            "Epoch 93/139.. Train loss: 59.909.. Val loss: 54.311.. Train L1 norm: 2.878.. Val L1 norm: 1.056.. Train Linf norm: 1907.353.. Val Linf norm: 45.419\n",
            "Epoch 94/139.. Train loss: 59.736.. Val loss: 54.311.. Train L1 norm: 3.034.. Val L1 norm: 1.056.. Train Linf norm: 2071.256.. Val Linf norm: 45.430\n",
            "Epoch 95/139.. Train loss: 62.675.. Val loss: 54.305.. Train L1 norm: 2.901.. Val L1 norm: 1.056.. Train Linf norm: 1936.187.. Val Linf norm: 45.591\n",
            "Epoch 96/139.. Train loss: 59.673.. Val loss: 54.304.. Train L1 norm: 3.049.. Val L1 norm: 1.056.. Train Linf norm: 2088.693.. Val Linf norm: 45.645\n",
            "Epoch 97/139.. Train loss: 59.610.. Val loss: 54.303.. Train L1 norm: 2.894.. Val L1 norm: 1.056.. Train Linf norm: 1928.820.. Val Linf norm: 45.671\n",
            "Epoch 98/139.. Train loss: 59.682.. Val loss: 54.301.. Train L1 norm: 2.886.. Val L1 norm: 1.057.. Train Linf norm: 1920.224.. Val Linf norm: 45.727\n",
            "Epoch 99/139.. Train loss: 60.005.. Val loss: 54.297.. Train L1 norm: 2.861.. Val L1 norm: 1.057.. Train Linf norm: 1893.852.. Val Linf norm: 45.836\n",
            "Epoch 100/139.. Train loss: 59.670.. Val loss: 54.295.. Train L1 norm: 2.889.. Val L1 norm: 1.057.. Train Linf norm: 1923.429.. Val Linf norm: 45.896\n",
            "Epoch 101/139.. Train loss: 62.511.. Val loss: 54.287.. Train L1 norm: 2.973.. Val L1 norm: 1.057.. Train Linf norm: 2007.128.. Val Linf norm: 46.128\n",
            "Epoch 102/139.. Train loss: 61.208.. Val loss: 54.292.. Train L1 norm: 2.916.. Val L1 norm: 1.057.. Train Linf norm: 1950.523.. Val Linf norm: 46.024\n",
            "Epoch 103/139.. Train loss: 59.751.. Val loss: 54.293.. Train L1 norm: 2.990.. Val L1 norm: 1.057.. Train Linf norm: 2026.468.. Val Linf norm: 46.010\n",
            "Epoch 104/139.. Train loss: 59.673.. Val loss: 54.291.. Train L1 norm: 2.929.. Val L1 norm: 1.057.. Train Linf norm: 1963.798.. Val Linf norm: 46.065\n",
            "Epoch 105/139.. Train loss: 59.811.. Val loss: 54.288.. Train L1 norm: 2.978.. Val L1 norm: 1.057.. Train Linf norm: 2014.822.. Val Linf norm: 46.151\n",
            "Epoch 106/139.. Train loss: 60.848.. Val loss: 54.293.. Train L1 norm: 2.914.. Val L1 norm: 1.057.. Train Linf norm: 1948.969.. Val Linf norm: 46.051\n",
            "Epoch 107/139.. Train loss: 59.694.. Val loss: 54.290.. Train L1 norm: 2.944.. Val L1 norm: 1.057.. Train Linf norm: 1980.176.. Val Linf norm: 46.121\n",
            "Epoch 108/139.. Train loss: 60.059.. Val loss: 54.286.. Train L1 norm: 2.781.. Val L1 norm: 1.057.. Train Linf norm: 1812.494.. Val Linf norm: 46.244\n",
            "Epoch 109/139.. Train loss: 59.754.. Val loss: 54.286.. Train L1 norm: 3.036.. Val L1 norm: 1.057.. Train Linf norm: 2073.072.. Val Linf norm: 46.245\n",
            "Epoch 110/139.. Train loss: 59.991.. Val loss: 54.283.. Train L1 norm: 3.083.. Val L1 norm: 1.057.. Train Linf norm: 2120.818.. Val Linf norm: 46.334\n",
            "Epoch 111/139.. Train loss: 61.418.. Val loss: 54.287.. Train L1 norm: 2.884.. Val L1 norm: 1.057.. Train Linf norm: 1918.333.. Val Linf norm: 46.261\n",
            "Epoch 112/139.. Train loss: 59.684.. Val loss: 54.286.. Train L1 norm: 2.854.. Val L1 norm: 1.057.. Train Linf norm: 1883.306.. Val Linf norm: 46.276\n",
            "Epoch 113/139.. Train loss: 61.235.. Val loss: 54.287.. Train L1 norm: 2.802.. Val L1 norm: 1.057.. Train Linf norm: 1833.851.. Val Linf norm: 46.282\n",
            "Epoch 114/139.. Train loss: 61.269.. Val loss: 54.291.. Train L1 norm: 2.605.. Val L1 norm: 1.057.. Train Linf norm: 1630.874.. Val Linf norm: 46.184\n",
            "Epoch 115/139.. Train loss: 59.595.. Val loss: 54.291.. Train L1 norm: 2.866.. Val L1 norm: 1.057.. Train Linf norm: 1899.411.. Val Linf norm: 46.208\n",
            "Epoch 116/139.. Train loss: 61.809.. Val loss: 54.295.. Train L1 norm: 2.914.. Val L1 norm: 1.057.. Train Linf norm: 1947.925.. Val Linf norm: 46.109\n",
            "Epoch 117/139.. Train loss: 59.619.. Val loss: 54.295.. Train L1 norm: 2.862.. Val L1 norm: 1.057.. Train Linf norm: 1887.528.. Val Linf norm: 46.129\n",
            "Epoch 118/139.. Train loss: 62.489.. Val loss: 54.301.. Train L1 norm: 2.706.. Val L1 norm: 1.057.. Train Linf norm: 1735.527.. Val Linf norm: 46.001\n",
            "Epoch 119/139.. Train loss: 59.612.. Val loss: 54.300.. Train L1 norm: 2.905.. Val L1 norm: 1.057.. Train Linf norm: 1940.751.. Val Linf norm: 46.028\n",
            "Epoch 120/139.. Train loss: 61.927.. Val loss: 54.293.. Train L1 norm: 2.931.. Val L1 norm: 1.057.. Train Linf norm: 1965.188.. Val Linf norm: 46.200\n",
            "Epoch 121/139.. Train loss: 60.192.. Val loss: 54.293.. Train L1 norm: 3.107.. Val L1 norm: 1.057.. Train Linf norm: 2146.654.. Val Linf norm: 46.211\n",
            "Epoch 122/139.. Train loss: 60.614.. Val loss: 54.291.. Train L1 norm: 2.781.. Val L1 norm: 1.057.. Train Linf norm: 1812.883.. Val Linf norm: 46.293\n",
            "Epoch 123/139.. Train loss: 60.600.. Val loss: 54.285.. Train L1 norm: 2.859.. Val L1 norm: 1.058.. Train Linf norm: 1892.757.. Val Linf norm: 46.436\n",
            "Epoch 124/139.. Train loss: 59.942.. Val loss: 54.287.. Train L1 norm: 2.888.. Val L1 norm: 1.057.. Train Linf norm: 1922.609.. Val Linf norm: 46.415\n",
            "Epoch 125/139.. Train loss: 59.857.. Val loss: 54.288.. Train L1 norm: 2.892.. Val L1 norm: 1.057.. Train Linf norm: 1927.308.. Val Linf norm: 46.395\n",
            "Epoch 126/139.. Train loss: 60.145.. Val loss: 54.290.. Train L1 norm: 3.022.. Val L1 norm: 1.057.. Train Linf norm: 2059.702.. Val Linf norm: 46.361\n",
            "Epoch 127/139.. Train loss: 59.596.. Val loss: 54.290.. Train L1 norm: 2.929.. Val L1 norm: 1.057.. Train Linf norm: 1963.929.. Val Linf norm: 46.377\n",
            "Epoch 128/139.. Train loss: 59.965.. Val loss: 54.286.. Train L1 norm: 2.902.. Val L1 norm: 1.058.. Train Linf norm: 1937.073.. Val Linf norm: 46.478\n",
            "Epoch 129/139.. Train loss: 59.652.. Val loss: 54.284.. Train L1 norm: 3.013.. Val L1 norm: 1.058.. Train Linf norm: 2049.567.. Val Linf norm: 46.541\n",
            "Epoch 130/139.. Train loss: 60.318.. Val loss: 54.287.. Train L1 norm: 2.880.. Val L1 norm: 1.058.. Train Linf norm: 1914.782.. Val Linf norm: 46.469\n",
            "Epoch 131/139.. Train loss: 59.615.. Val loss: 54.287.. Train L1 norm: 2.934.. Val L1 norm: 1.058.. Train Linf norm: 1969.858.. Val Linf norm: 46.486\n",
            "Epoch 132/139.. Train loss: 59.703.. Val loss: 54.287.. Train L1 norm: 3.099.. Val L1 norm: 1.058.. Train Linf norm: 2138.277.. Val Linf norm: 46.483\n",
            "Epoch 133/139.. Train loss: 59.606.. Val loss: 54.286.. Train L1 norm: 2.819.. Val L1 norm: 1.058.. Train Linf norm: 1851.969.. Val Linf norm: 46.526\n",
            "Epoch 134/139.. Train loss: 59.790.. Val loss: 54.287.. Train L1 norm: 3.048.. Val L1 norm: 1.058.. Train Linf norm: 2085.086.. Val Linf norm: 46.524\n",
            "Epoch 135/139.. Train loss: 59.940.. Val loss: 54.290.. Train L1 norm: 3.024.. Val L1 norm: 1.058.. Train Linf norm: 2062.451.. Val Linf norm: 46.467\n",
            "Epoch 136/139.. Train loss: 60.059.. Val loss: 54.285.. Train L1 norm: 2.960.. Val L1 norm: 1.058.. Train Linf norm: 1992.379.. Val Linf norm: 46.602\n",
            "Epoch 137/139.. Train loss: 59.862.. Val loss: 54.287.. Train L1 norm: 2.862.. Val L1 norm: 1.058.. Train Linf norm: 1895.455.. Val Linf norm: 46.573\n",
            "Epoch 138/139.. Train loss: 59.880.. Val loss: 54.283.. Train L1 norm: 2.723.. Val L1 norm: 1.058.. Train Linf norm: 1747.004.. Val Linf norm: 46.685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 07:14:46,377]\u001b[0m Trial 49 finished with value: 1.057984852854411 and parameters: {'n_layers': 7, 'n_units_0': 3045, 'n_units_1': 2702, 'n_units_2': 2550, 'n_units_3': 284, 'n_units_4': 1717, 'n_units_5': 2183, 'n_units_6': 3460, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 2.9382078873281185e-06, 'batch_size': 1024, 'n_epochs': 139, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.22286999887217426, 'dropout_rate': 0.007220609953757867, 'weight_decay': 0.00022964965491176597, 'beta1': 0.9217100970024541, 'beta2': 0.9990206915073904, 'factor': 0.14148484797468677, 'patience': 6, 'threshold': 0.002322738048207631}. Best is trial 22 with value: 1.0150012904485066.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 139/139.. Train loss: 59.693.. Val loss: 54.280.. Train L1 norm: 2.987.. Val L1 norm: 1.058.. Train Linf norm: 2023.511.. Val Linf norm: 46.764\n",
            "Epoch 1/139.. Train loss: 1825.008.. Val loss: 54.493.. Train L1 norm: 3.058.. Val L1 norm: 1.025.. Train Linf norm: 2097.407.. Val Linf norm: 22.837\n",
            "Epoch 2/139.. Train loss: 71.749.. Val loss: 54.591.. Train L1 norm: 2.218.. Val L1 norm: 1.026.. Train Linf norm: 1244.015.. Val Linf norm: 23.934\n",
            "Epoch 3/139.. Train loss: 73.599.. Val loss: 54.276.. Train L1 norm: 2.162.. Val L1 norm: 1.044.. Train Linf norm: 1183.005.. Val Linf norm: 36.651\n",
            "Epoch 4/139.. Train loss: 86.038.. Val loss: 54.630.. Train L1 norm: 2.581.. Val L1 norm: 1.035.. Train Linf norm: 1612.215.. Val Linf norm: 30.512\n",
            "Epoch 5/139.. Train loss: 84.279.. Val loss: 54.079.. Train L1 norm: 2.443.. Val L1 norm: 1.064.. Train Linf norm: 1467.578.. Val Linf norm: 50.473\n",
            "Epoch 6/139.. Train loss: 114.642.. Val loss: 54.658.. Train L1 norm: 2.948.. Val L1 norm: 1.043.. Train Linf norm: 1978.063.. Val Linf norm: 37.233\n",
            "Epoch 7/139.. Train loss: 94.206.. Val loss: 53.970.. Train L1 norm: 2.718.. Val L1 norm: 1.078.. Train Linf norm: 1747.285.. Val Linf norm: 60.141\n",
            "Epoch 8/139.. Train loss: 138.977.. Val loss: 54.171.. Train L1 norm: 3.254.. Val L1 norm: 1.075.. Train Linf norm: 2286.890.. Val Linf norm: 58.796\n",
            "Epoch 9/139.. Train loss: 166.131.. Val loss: 53.967.. Train L1 norm: 2.931.. Val L1 norm: 1.087.. Train Linf norm: 1947.974.. Val Linf norm: 67.006\n",
            "Epoch 10/139.. Train loss: 150.734.. Val loss: 53.964.. Train L1 norm: 3.530.. Val L1 norm: 1.095.. Train Linf norm: 2562.362.. Val Linf norm: 72.470\n",
            "Epoch 11/139.. Train loss: 130.484.. Val loss: 53.720.. Train L1 norm: 2.538.. Val L1 norm: 1.110.. Train Linf norm: 1554.166.. Val Linf norm: 82.533\n",
            "Epoch 12/139.. Train loss: 189.310.. Val loss: 54.722.. Train L1 norm: 3.722.. Val L1 norm: 1.072.. Train Linf norm: 2751.900.. Val Linf norm: 58.457\n",
            "Epoch 13/139.. Train loss: 141.495.. Val loss: 53.845.. Train L1 norm: 3.616.. Val L1 norm: 1.115.. Train Linf norm: 2652.107.. Val Linf norm: 86.315\n",
            "Epoch 14/139.. Train loss: 80.550.. Val loss: 54.193.. Train L1 norm: 3.610.. Val L1 norm: 1.106.. Train Linf norm: 2641.304.. Val Linf norm: 81.150\n",
            "Epoch 15/139.. Train loss: 75.368.. Val loss: 53.895.. Train L1 norm: 3.382.. Val L1 norm: 1.125.. Train Linf norm: 2411.017.. Val Linf norm: 92.987\n",
            "Epoch 16/139.. Train loss: 74.846.. Val loss: 53.967.. Train L1 norm: 3.949.. Val L1 norm: 1.129.. Train Linf norm: 2968.959.. Val Linf norm: 96.097\n",
            "Epoch 17/139.. Train loss: 63.038.. Val loss: 53.746.. Train L1 norm: 4.063.. Val L1 norm: 1.145.. Train Linf norm: 3098.148.. Val Linf norm: 106.243\n",
            "Epoch 18/139.. Train loss: 60.521.. Val loss: 53.734.. Train L1 norm: 4.095.. Val L1 norm: 1.146.. Train Linf norm: 3126.088.. Val Linf norm: 107.230\n",
            "Epoch 19/139.. Train loss: 59.643.. Val loss: 53.733.. Train L1 norm: 4.105.. Val L1 norm: 1.148.. Train Linf norm: 3134.880.. Val Linf norm: 107.955\n",
            "Epoch 20/139.. Train loss: 59.617.. Val loss: 53.730.. Train L1 norm: 4.168.. Val L1 norm: 1.149.. Train Linf norm: 3199.934.. Val Linf norm: 108.708\n",
            "Epoch 21/139.. Train loss: 59.455.. Val loss: 53.722.. Train L1 norm: 4.185.. Val L1 norm: 1.150.. Train Linf norm: 3217.378.. Val Linf norm: 109.588\n",
            "Epoch 22/139.. Train loss: 59.294.. Val loss: 53.714.. Train L1 norm: 4.199.. Val L1 norm: 1.151.. Train Linf norm: 3232.261.. Val Linf norm: 110.491\n",
            "Epoch 23/139.. Train loss: 59.100.. Val loss: 53.702.. Train L1 norm: 4.272.. Val L1 norm: 1.153.. Train Linf norm: 3294.267.. Val Linf norm: 111.488\n",
            "Epoch 24/139.. Train loss: 59.042.. Val loss: 53.700.. Train L1 norm: 4.258.. Val L1 norm: 1.153.. Train Linf norm: 3293.622.. Val Linf norm: 111.647\n",
            "Epoch 25/139.. Train loss: 59.043.. Val loss: 53.698.. Train L1 norm: 4.242.. Val L1 norm: 1.153.. Train Linf norm: 3274.809.. Val Linf norm: 111.807\n",
            "Epoch 26/139.. Train loss: 59.041.. Val loss: 53.696.. Train L1 norm: 4.248.. Val L1 norm: 1.154.. Train Linf norm: 3280.164.. Val Linf norm: 111.968\n",
            "Epoch 27/139.. Train loss: 59.071.. Val loss: 53.695.. Train L1 norm: 4.268.. Val L1 norm: 1.154.. Train Linf norm: 3300.933.. Val Linf norm: 112.124\n",
            "Epoch 28/139.. Train loss: 59.193.. Val loss: 53.693.. Train L1 norm: 4.288.. Val L1 norm: 1.154.. Train Linf norm: 3320.834.. Val Linf norm: 112.268\n",
            "Epoch 29/139.. Train loss: 59.054.. Val loss: 53.691.. Train L1 norm: 4.290.. Val L1 norm: 1.154.. Train Linf norm: 3324.108.. Val Linf norm: 112.428\n",
            "Epoch 30/139.. Train loss: 59.034.. Val loss: 53.691.. Train L1 norm: 4.275.. Val L1 norm: 1.154.. Train Linf norm: 3307.588.. Val Linf norm: 112.452\n",
            "Epoch 31/139.. Train loss: 59.092.. Val loss: 53.691.. Train L1 norm: 4.290.. Val L1 norm: 1.154.. Train Linf norm: 3322.485.. Val Linf norm: 112.476\n",
            "Epoch 32/139.. Train loss: 59.048.. Val loss: 53.690.. Train L1 norm: 4.277.. Val L1 norm: 1.154.. Train Linf norm: 3306.578.. Val Linf norm: 112.501\n",
            "Epoch 33/139.. Train loss: 59.021.. Val loss: 53.690.. Train L1 norm: 4.303.. Val L1 norm: 1.155.. Train Linf norm: 3333.108.. Val Linf norm: 112.527\n",
            "Epoch 34/139.. Train loss: 59.030.. Val loss: 53.690.. Train L1 norm: 4.293.. Val L1 norm: 1.155.. Train Linf norm: 3324.988.. Val Linf norm: 112.552\n",
            "Epoch 35/139.. Train loss: 59.043.. Val loss: 53.689.. Train L1 norm: 4.262.. Val L1 norm: 1.155.. Train Linf norm: 3295.766.. Val Linf norm: 112.577\n",
            "Epoch 36/139.. Train loss: 59.062.. Val loss: 53.689.. Train L1 norm: 4.274.. Val L1 norm: 1.155.. Train Linf norm: 3304.963.. Val Linf norm: 112.581\n",
            "Epoch 37/139.. Train loss: 59.054.. Val loss: 53.689.. Train L1 norm: 4.296.. Val L1 norm: 1.155.. Train Linf norm: 3329.208.. Val Linf norm: 112.585\n",
            "Epoch 38/139.. Train loss: 59.012.. Val loss: 53.689.. Train L1 norm: 4.268.. Val L1 norm: 1.155.. Train Linf norm: 3300.980.. Val Linf norm: 112.589\n",
            "Epoch 39/139.. Train loss: 59.020.. Val loss: 53.689.. Train L1 norm: 4.305.. Val L1 norm: 1.155.. Train Linf norm: 3337.315.. Val Linf norm: 112.593\n",
            "Epoch 40/139.. Train loss: 59.082.. Val loss: 53.689.. Train L1 norm: 4.273.. Val L1 norm: 1.155.. Train Linf norm: 3306.307.. Val Linf norm: 112.597\n",
            "Epoch 41/139.. Train loss: 59.044.. Val loss: 53.689.. Train L1 norm: 4.265.. Val L1 norm: 1.155.. Train Linf norm: 3297.062.. Val Linf norm: 112.601\n",
            "Epoch 42/139.. Train loss: 59.050.. Val loss: 53.689.. Train L1 norm: 4.287.. Val L1 norm: 1.155.. Train Linf norm: 3317.204.. Val Linf norm: 112.605\n",
            "Epoch 43/139.. Train loss: 59.055.. Val loss: 53.689.. Train L1 norm: 4.292.. Val L1 norm: 1.155.. Train Linf norm: 3320.800.. Val Linf norm: 112.609\n",
            "Epoch 44/139.. Train loss: 59.054.. Val loss: 53.689.. Train L1 norm: 4.274.. Val L1 norm: 1.155.. Train Linf norm: 3304.182.. Val Linf norm: 112.613\n",
            "Epoch 45/139.. Train loss: 59.030.. Val loss: 53.689.. Train L1 norm: 4.300.. Val L1 norm: 1.155.. Train Linf norm: 3329.853.. Val Linf norm: 112.617\n",
            "Epoch 46/139.. Train loss: 59.033.. Val loss: 53.689.. Train L1 norm: 4.270.. Val L1 norm: 1.155.. Train Linf norm: 3300.478.. Val Linf norm: 112.621\n",
            "Epoch 47/139.. Train loss: 59.042.. Val loss: 53.689.. Train L1 norm: 4.269.. Val L1 norm: 1.155.. Train Linf norm: 3301.090.. Val Linf norm: 112.625\n",
            "Epoch 48/139.. Train loss: 59.059.. Val loss: 53.689.. Train L1 norm: 4.286.. Val L1 norm: 1.155.. Train Linf norm: 3316.454.. Val Linf norm: 112.629\n",
            "Epoch 49/139.. Train loss: 59.036.. Val loss: 53.689.. Train L1 norm: 4.285.. Val L1 norm: 1.155.. Train Linf norm: 3313.392.. Val Linf norm: 112.633\n",
            "Epoch 50/139.. Train loss: 59.053.. Val loss: 53.689.. Train L1 norm: 4.270.. Val L1 norm: 1.155.. Train Linf norm: 3299.930.. Val Linf norm: 112.637\n",
            "Epoch 51/139.. Train loss: 59.034.. Val loss: 53.689.. Train L1 norm: 4.259.. Val L1 norm: 1.155.. Train Linf norm: 3289.396.. Val Linf norm: 112.641\n",
            "Epoch 52/139.. Train loss: 59.012.. Val loss: 53.689.. Train L1 norm: 4.296.. Val L1 norm: 1.155.. Train Linf norm: 3324.432.. Val Linf norm: 112.646\n",
            "Epoch 53/139.. Train loss: 59.015.. Val loss: 53.689.. Train L1 norm: 4.265.. Val L1 norm: 1.155.. Train Linf norm: 3298.002.. Val Linf norm: 112.650\n",
            "Epoch 54/139.. Train loss: 59.049.. Val loss: 53.688.. Train L1 norm: 4.283.. Val L1 norm: 1.155.. Train Linf norm: 3311.019.. Val Linf norm: 112.654\n",
            "Epoch 55/139.. Train loss: 59.027.. Val loss: 53.688.. Train L1 norm: 4.293.. Val L1 norm: 1.155.. Train Linf norm: 3312.164.. Val Linf norm: 112.658\n",
            "Epoch 56/139.. Train loss: 59.008.. Val loss: 53.688.. Train L1 norm: 4.279.. Val L1 norm: 1.155.. Train Linf norm: 3311.824.. Val Linf norm: 112.663\n",
            "Epoch 57/139.. Train loss: 59.034.. Val loss: 53.688.. Train L1 norm: 4.287.. Val L1 norm: 1.155.. Train Linf norm: 3316.968.. Val Linf norm: 112.667\n",
            "Epoch 58/139.. Train loss: 59.041.. Val loss: 53.688.. Train L1 norm: 4.309.. Val L1 norm: 1.155.. Train Linf norm: 3341.183.. Val Linf norm: 112.671\n",
            "Epoch 59/139.. Train loss: 59.020.. Val loss: 53.688.. Train L1 norm: 4.285.. Val L1 norm: 1.155.. Train Linf norm: 3317.851.. Val Linf norm: 112.675\n",
            "Epoch 60/139.. Train loss: 59.023.. Val loss: 53.688.. Train L1 norm: 4.283.. Val L1 norm: 1.155.. Train Linf norm: 3312.026.. Val Linf norm: 112.680\n",
            "Epoch 61/139.. Train loss: 59.029.. Val loss: 53.688.. Train L1 norm: 4.261.. Val L1 norm: 1.155.. Train Linf norm: 3290.853.. Val Linf norm: 112.684\n",
            "Epoch 62/139.. Train loss: 59.019.. Val loss: 53.688.. Train L1 norm: 4.293.. Val L1 norm: 1.155.. Train Linf norm: 3323.879.. Val Linf norm: 112.689\n",
            "Epoch 63/139.. Train loss: 59.026.. Val loss: 53.688.. Train L1 norm: 4.287.. Val L1 norm: 1.155.. Train Linf norm: 3308.471.. Val Linf norm: 112.693\n",
            "Epoch 64/139.. Train loss: 59.019.. Val loss: 53.688.. Train L1 norm: 4.281.. Val L1 norm: 1.155.. Train Linf norm: 3315.765.. Val Linf norm: 112.697\n",
            "Epoch 65/139.. Train loss: 59.130.. Val loss: 53.688.. Train L1 norm: 4.254.. Val L1 norm: 1.155.. Train Linf norm: 3285.348.. Val Linf norm: 112.701\n",
            "Epoch 66/139.. Train loss: 59.027.. Val loss: 53.688.. Train L1 norm: 4.267.. Val L1 norm: 1.155.. Train Linf norm: 3297.426.. Val Linf norm: 112.706\n",
            "Epoch 67/139.. Train loss: 59.052.. Val loss: 53.688.. Train L1 norm: 4.300.. Val L1 norm: 1.155.. Train Linf norm: 3332.437.. Val Linf norm: 112.710\n",
            "Epoch 68/139.. Train loss: 59.028.. Val loss: 53.688.. Train L1 norm: 4.295.. Val L1 norm: 1.155.. Train Linf norm: 3327.353.. Val Linf norm: 112.715\n",
            "Epoch 69/139.. Train loss: 59.039.. Val loss: 53.688.. Train L1 norm: 4.263.. Val L1 norm: 1.155.. Train Linf norm: 3293.761.. Val Linf norm: 112.719\n",
            "Epoch 70/139.. Train loss: 59.021.. Val loss: 53.688.. Train L1 norm: 4.286.. Val L1 norm: 1.155.. Train Linf norm: 3309.055.. Val Linf norm: 112.724\n",
            "Epoch 71/139.. Train loss: 59.011.. Val loss: 53.687.. Train L1 norm: 4.279.. Val L1 norm: 1.155.. Train Linf norm: 3303.797.. Val Linf norm: 112.728\n",
            "Epoch 72/139.. Train loss: 59.052.. Val loss: 53.687.. Train L1 norm: 4.304.. Val L1 norm: 1.155.. Train Linf norm: 3335.969.. Val Linf norm: 112.733\n",
            "Epoch 73/139.. Train loss: 59.057.. Val loss: 53.687.. Train L1 norm: 4.283.. Val L1 norm: 1.155.. Train Linf norm: 3317.077.. Val Linf norm: 112.737\n",
            "Epoch 74/139.. Train loss: 59.010.. Val loss: 53.687.. Train L1 norm: 4.287.. Val L1 norm: 1.155.. Train Linf norm: 3321.298.. Val Linf norm: 112.742\n",
            "Epoch 75/139.. Train loss: 59.022.. Val loss: 53.687.. Train L1 norm: 4.280.. Val L1 norm: 1.155.. Train Linf norm: 3312.583.. Val Linf norm: 112.746\n",
            "Epoch 76/139.. Train loss: 59.028.. Val loss: 53.687.. Train L1 norm: 4.276.. Val L1 norm: 1.155.. Train Linf norm: 3306.744.. Val Linf norm: 112.751\n",
            "Epoch 77/139.. Train loss: 59.011.. Val loss: 53.687.. Train L1 norm: 4.285.. Val L1 norm: 1.155.. Train Linf norm: 3316.941.. Val Linf norm: 112.756\n",
            "Epoch 78/139.. Train loss: 59.041.. Val loss: 53.687.. Train L1 norm: 4.251.. Val L1 norm: 1.155.. Train Linf norm: 3282.828.. Val Linf norm: 112.760\n",
            "Epoch 79/139.. Train loss: 59.021.. Val loss: 53.687.. Train L1 norm: 4.308.. Val L1 norm: 1.155.. Train Linf norm: 3339.322.. Val Linf norm: 112.765\n",
            "Epoch 80/139.. Train loss: 59.073.. Val loss: 53.687.. Train L1 norm: 4.294.. Val L1 norm: 1.155.. Train Linf norm: 3323.469.. Val Linf norm: 112.770\n",
            "Epoch 81/139.. Train loss: 59.034.. Val loss: 53.687.. Train L1 norm: 4.240.. Val L1 norm: 1.155.. Train Linf norm: 3272.733.. Val Linf norm: 112.775\n",
            "Epoch 82/139.. Train loss: 59.057.. Val loss: 53.687.. Train L1 norm: 4.282.. Val L1 norm: 1.155.. Train Linf norm: 3311.071.. Val Linf norm: 112.779\n",
            "Epoch 83/139.. Train loss: 59.008.. Val loss: 53.687.. Train L1 norm: 4.285.. Val L1 norm: 1.155.. Train Linf norm: 3315.432.. Val Linf norm: 112.784\n",
            "Epoch 84/139.. Train loss: 59.050.. Val loss: 53.687.. Train L1 norm: 4.289.. Val L1 norm: 1.155.. Train Linf norm: 3318.286.. Val Linf norm: 112.789\n",
            "Epoch 85/139.. Train loss: 59.042.. Val loss: 53.687.. Train L1 norm: 4.276.. Val L1 norm: 1.155.. Train Linf norm: 3308.583.. Val Linf norm: 112.794\n",
            "Epoch 86/139.. Train loss: 59.037.. Val loss: 53.687.. Train L1 norm: 4.292.. Val L1 norm: 1.155.. Train Linf norm: 3325.254.. Val Linf norm: 112.799\n",
            "Epoch 87/139.. Train loss: 59.048.. Val loss: 53.686.. Train L1 norm: 4.302.. Val L1 norm: 1.155.. Train Linf norm: 3334.639.. Val Linf norm: 112.803\n",
            "Epoch 88/139.. Train loss: 59.065.. Val loss: 53.686.. Train L1 norm: 4.293.. Val L1 norm: 1.155.. Train Linf norm: 3325.918.. Val Linf norm: 112.808\n",
            "Epoch 89/139.. Train loss: 59.012.. Val loss: 53.686.. Train L1 norm: 4.306.. Val L1 norm: 1.155.. Train Linf norm: 3337.981.. Val Linf norm: 112.813\n",
            "Epoch 90/139.. Train loss: 59.065.. Val loss: 53.686.. Train L1 norm: 4.302.. Val L1 norm: 1.155.. Train Linf norm: 3331.568.. Val Linf norm: 112.819\n",
            "Epoch 91/139.. Train loss: 59.009.. Val loss: 53.686.. Train L1 norm: 4.303.. Val L1 norm: 1.155.. Train Linf norm: 3331.862.. Val Linf norm: 112.823\n",
            "Epoch 92/139.. Train loss: 59.034.. Val loss: 53.686.. Train L1 norm: 4.305.. Val L1 norm: 1.155.. Train Linf norm: 3336.808.. Val Linf norm: 112.829\n",
            "Epoch 93/139.. Train loss: 59.024.. Val loss: 53.686.. Train L1 norm: 4.293.. Val L1 norm: 1.155.. Train Linf norm: 3322.597.. Val Linf norm: 112.834\n",
            "Epoch 94/139.. Train loss: 59.026.. Val loss: 53.686.. Train L1 norm: 4.303.. Val L1 norm: 1.155.. Train Linf norm: 3333.858.. Val Linf norm: 112.840\n",
            "Epoch 95/139.. Train loss: 59.011.. Val loss: 53.686.. Train L1 norm: 4.263.. Val L1 norm: 1.155.. Train Linf norm: 3293.907.. Val Linf norm: 112.846\n",
            "Epoch 96/139.. Train loss: 59.005.. Val loss: 53.686.. Train L1 norm: 4.289.. Val L1 norm: 1.155.. Train Linf norm: 3320.592.. Val Linf norm: 112.852\n",
            "Epoch 97/139.. Train loss: 59.006.. Val loss: 53.686.. Train L1 norm: 4.251.. Val L1 norm: 1.155.. Train Linf norm: 3282.664.. Val Linf norm: 112.859\n",
            "Epoch 98/139.. Train loss: 59.004.. Val loss: 53.685.. Train L1 norm: 4.237.. Val L1 norm: 1.155.. Train Linf norm: 3266.279.. Val Linf norm: 112.865\n",
            "Epoch 99/139.. Train loss: 59.010.. Val loss: 53.685.. Train L1 norm: 4.303.. Val L1 norm: 1.155.. Train Linf norm: 3334.498.. Val Linf norm: 112.871\n",
            "Epoch 100/139.. Train loss: 59.021.. Val loss: 53.685.. Train L1 norm: 4.303.. Val L1 norm: 1.155.. Train Linf norm: 3325.581.. Val Linf norm: 112.878\n",
            "Epoch 101/139.. Train loss: 59.013.. Val loss: 53.685.. Train L1 norm: 4.307.. Val L1 norm: 1.155.. Train Linf norm: 3338.331.. Val Linf norm: 112.884\n",
            "Epoch 102/139.. Train loss: 59.030.. Val loss: 53.685.. Train L1 norm: 4.280.. Val L1 norm: 1.155.. Train Linf norm: 3311.812.. Val Linf norm: 112.890\n",
            "Epoch 103/139.. Train loss: 59.023.. Val loss: 53.685.. Train L1 norm: 4.304.. Val L1 norm: 1.155.. Train Linf norm: 3337.311.. Val Linf norm: 112.896\n",
            "Epoch 104/139.. Train loss: 59.016.. Val loss: 53.685.. Train L1 norm: 4.304.. Val L1 norm: 1.155.. Train Linf norm: 3336.178.. Val Linf norm: 112.903\n",
            "Epoch 105/139.. Train loss: 59.007.. Val loss: 53.685.. Train L1 norm: 4.291.. Val L1 norm: 1.155.. Train Linf norm: 3321.943.. Val Linf norm: 112.910\n",
            "Epoch 106/139.. Train loss: 59.013.. Val loss: 53.684.. Train L1 norm: 4.309.. Val L1 norm: 1.155.. Train Linf norm: 3341.870.. Val Linf norm: 112.917\n",
            "Epoch 107/139.. Train loss: 59.019.. Val loss: 53.684.. Train L1 norm: 4.270.. Val L1 norm: 1.155.. Train Linf norm: 3300.248.. Val Linf norm: 112.924\n",
            "Epoch 108/139.. Train loss: 59.015.. Val loss: 53.684.. Train L1 norm: 4.271.. Val L1 norm: 1.155.. Train Linf norm: 3295.292.. Val Linf norm: 112.931\n",
            "Epoch 109/139.. Train loss: 59.001.. Val loss: 53.684.. Train L1 norm: 4.305.. Val L1 norm: 1.155.. Train Linf norm: 3303.086.. Val Linf norm: 112.939\n",
            "Epoch 110/139.. Train loss: 59.021.. Val loss: 53.684.. Train L1 norm: 4.284.. Val L1 norm: 1.155.. Train Linf norm: 3317.168.. Val Linf norm: 112.946\n",
            "Epoch 111/139.. Train loss: 59.013.. Val loss: 53.684.. Train L1 norm: 4.296.. Val L1 norm: 1.155.. Train Linf norm: 3329.513.. Val Linf norm: 112.954\n",
            "Epoch 112/139.. Train loss: 59.016.. Val loss: 53.684.. Train L1 norm: 4.306.. Val L1 norm: 1.155.. Train Linf norm: 3337.024.. Val Linf norm: 112.960\n",
            "Epoch 113/139.. Train loss: 59.008.. Val loss: 53.683.. Train L1 norm: 4.307.. Val L1 norm: 1.155.. Train Linf norm: 3338.035.. Val Linf norm: 112.969\n",
            "Epoch 114/139.. Train loss: 59.028.. Val loss: 53.683.. Train L1 norm: 4.260.. Val L1 norm: 1.155.. Train Linf norm: 3293.382.. Val Linf norm: 112.977\n",
            "Epoch 115/139.. Train loss: 59.004.. Val loss: 53.683.. Train L1 norm: 4.281.. Val L1 norm: 1.155.. Train Linf norm: 3312.560.. Val Linf norm: 112.984\n",
            "Epoch 116/139.. Train loss: 59.008.. Val loss: 53.683.. Train L1 norm: 4.300.. Val L1 norm: 1.155.. Train Linf norm: 3329.651.. Val Linf norm: 112.993\n",
            "Epoch 117/139.. Train loss: 59.009.. Val loss: 53.683.. Train L1 norm: 4.310.. Val L1 norm: 1.155.. Train Linf norm: 3342.003.. Val Linf norm: 113.002\n",
            "Epoch 118/139.. Train loss: 59.001.. Val loss: 53.683.. Train L1 norm: 4.329.. Val L1 norm: 1.155.. Train Linf norm: 3363.450.. Val Linf norm: 113.011\n",
            "Epoch 119/139.. Train loss: 59.020.. Val loss: 53.682.. Train L1 norm: 4.316.. Val L1 norm: 1.155.. Train Linf norm: 3345.262.. Val Linf norm: 113.019\n",
            "Epoch 120/139.. Train loss: 59.001.. Val loss: 53.682.. Train L1 norm: 4.326.. Val L1 norm: 1.155.. Train Linf norm: 3357.147.. Val Linf norm: 113.029\n",
            "Epoch 121/139.. Train loss: 59.025.. Val loss: 53.682.. Train L1 norm: 4.292.. Val L1 norm: 1.155.. Train Linf norm: 3321.619.. Val Linf norm: 113.038\n",
            "Epoch 122/139.. Train loss: 59.026.. Val loss: 53.682.. Train L1 norm: 4.280.. Val L1 norm: 1.155.. Train Linf norm: 3299.180.. Val Linf norm: 113.046\n",
            "Epoch 123/139.. Train loss: 59.005.. Val loss: 53.682.. Train L1 norm: 4.291.. Val L1 norm: 1.155.. Train Linf norm: 3320.367.. Val Linf norm: 113.057\n",
            "Epoch 124/139.. Train loss: 59.016.. Val loss: 53.681.. Train L1 norm: 4.303.. Val L1 norm: 1.155.. Train Linf norm: 3334.202.. Val Linf norm: 113.067\n",
            "Epoch 125/139.. Train loss: 59.025.. Val loss: 53.681.. Train L1 norm: 4.310.. Val L1 norm: 1.155.. Train Linf norm: 3342.395.. Val Linf norm: 113.075\n",
            "Epoch 126/139.. Train loss: 59.058.. Val loss: 53.681.. Train L1 norm: 4.290.. Val L1 norm: 1.155.. Train Linf norm: 3321.673.. Val Linf norm: 113.081\n",
            "Epoch 127/139.. Train loss: 59.007.. Val loss: 53.681.. Train L1 norm: 4.315.. Val L1 norm: 1.155.. Train Linf norm: 3345.439.. Val Linf norm: 113.092\n",
            "Epoch 128/139.. Train loss: 59.012.. Val loss: 53.681.. Train L1 norm: 4.286.. Val L1 norm: 1.155.. Train Linf norm: 3314.327.. Val Linf norm: 113.104\n",
            "Epoch 129/139.. Train loss: 59.029.. Val loss: 53.680.. Train L1 norm: 4.289.. Val L1 norm: 1.155.. Train Linf norm: 1253.353.. Val Linf norm: 113.113\n",
            "Epoch 130/139.. Train loss: 59.004.. Val loss: 53.680.. Train L1 norm: 4.343.. Val L1 norm: 1.155.. Train Linf norm: 3378.494.. Val Linf norm: 113.124\n",
            "Epoch 131/139.. Train loss: 59.002.. Val loss: 53.680.. Train L1 norm: 4.314.. Val L1 norm: 1.155.. Train Linf norm: 3348.524.. Val Linf norm: 113.142\n",
            "Epoch 132/139.. Train loss: 59.004.. Val loss: 53.679.. Train L1 norm: 4.317.. Val L1 norm: 1.156.. Train Linf norm: 3350.042.. Val Linf norm: 113.155\n",
            "Epoch 133/139.. Train loss: 59.003.. Val loss: 53.679.. Train L1 norm: 4.326.. Val L1 norm: 1.156.. Train Linf norm: 3345.079.. Val Linf norm: 113.173\n",
            "Epoch 134/139.. Train loss: 59.005.. Val loss: 53.679.. Train L1 norm: 4.287.. Val L1 norm: 1.156.. Train Linf norm: 3319.210.. Val Linf norm: 113.186\n",
            "Epoch 135/139.. Train loss: 59.007.. Val loss: 53.678.. Train L1 norm: 4.313.. Val L1 norm: 1.156.. Train Linf norm: 3343.951.. Val Linf norm: 113.199\n",
            "Epoch 136/139.. Train loss: 58.995.. Val loss: 53.678.. Train L1 norm: 4.313.. Val L1 norm: 1.156.. Train Linf norm: 3345.068.. Val Linf norm: 113.214\n",
            "Epoch 137/139.. Train loss: 58.998.. Val loss: 53.678.. Train L1 norm: 4.326.. Val L1 norm: 1.156.. Train Linf norm: 3359.466.. Val Linf norm: 113.227\n",
            "Epoch 138/139.. Train loss: 59.016.. Val loss: 53.677.. Train L1 norm: 4.330.. Val L1 norm: 1.156.. Train Linf norm: 3360.862.. Val Linf norm: 113.240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 07:30:03,611]\u001b[0m Trial 50 finished with value: 1.1556805559158325 and parameters: {'n_layers': 8, 'n_units_0': 3050, 'n_units_1': 2619, 'n_units_2': 2560, 'n_units_3': 297, 'n_units_4': 1537, 'n_units_5': 2160, 'n_units_6': 3858, 'n_units_7': 3444, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 4.578700174734198e-06, 'batch_size': 1024, 'n_epochs': 139, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.22743859932231933, 'dropout_rate': 0.0010345598361282751, 'weight_decay': 0.00020486181635549585, 'beta1': 0.9171956431597448, 'beta2': 0.9990249550265704, 'factor': 0.15747567946572688, 'patience': 5, 'threshold': 0.0016005385086804828}. Best is trial 22 with value: 1.0150012904485066.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 139/139.. Train loss: 58.996.. Val loss: 53.677.. Train L1 norm: 4.284.. Val L1 norm: 1.156.. Train Linf norm: 3316.714.. Val Linf norm: 113.257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 07:30:10,394]\u001b[0m Trial 51 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/141.. Train loss: 70838.273.. Val loss: 743.274.. Train L1 norm: 2.209.. Val L1 norm: 1.390.. Train Linf norm: 760.702.. Val Linf norm: 233.217\n",
            "Epoch 1/138.. Train loss: 284.260.. Val loss: 56.336.. Train L1 norm: 3.054.. Val L1 norm: 1.053.. Train Linf norm: 2088.699.. Val Linf norm: 40.607\n",
            "Epoch 2/138.. Train loss: 1334.045.. Val loss: 53.914.. Train L1 norm: 1.717.. Val L1 norm: 1.059.. Train Linf norm: 723.254.. Val Linf norm: 47.039\n",
            "Epoch 3/138.. Train loss: 258.293.. Val loss: 55.131.. Train L1 norm: 2.871.. Val L1 norm: 1.022.. Train Linf norm: 1911.770.. Val Linf norm: 20.391\n",
            "Epoch 4/138.. Train loss: 131.621.. Val loss: 54.251.. Train L1 norm: 1.805.. Val L1 norm: 1.050.. Train Linf norm: 822.356.. Val Linf norm: 41.291\n",
            "Epoch 5/138.. Train loss: 100.009.. Val loss: 54.727.. Train L1 norm: 2.226.. Val L1 norm: 1.036.. Train Linf norm: 1251.147.. Val Linf norm: 32.182\n",
            "Epoch 6/138.. Train loss: 73.945.. Val loss: 54.324.. Train L1 norm: 2.215.. Val L1 norm: 1.054.. Train Linf norm: 1240.699.. Val Linf norm: 44.794\n",
            "Epoch 7/138.. Train loss: 76.544.. Val loss: 54.646.. Train L1 norm: 2.389.. Val L1 norm: 1.045.. Train Linf norm: 1413.869.. Val Linf norm: 38.539\n",
            "Epoch 8/138.. Train loss: 86.864.. Val loss: 53.959.. Train L1 norm: 3.285.. Val L1 norm: 1.074.. Train Linf norm: 2332.488.. Val Linf norm: 58.895\n",
            "Epoch 9/138.. Train loss: 108.535.. Val loss: 54.110.. Train L1 norm: 3.054.. Val L1 norm: 1.068.. Train Linf norm: 2087.262.. Val Linf norm: 54.866\n",
            "Epoch 10/138.. Train loss: 71.849.. Val loss: 54.198.. Train L1 norm: 3.180.. Val L1 norm: 1.065.. Train Linf norm: 2218.129.. Val Linf norm: 52.747\n",
            "Epoch 11/138.. Train loss: 63.541.. Val loss: 54.253.. Train L1 norm: 2.868.. Val L1 norm: 1.063.. Train Linf norm: 1899.355.. Val Linf norm: 51.602\n",
            "Epoch 12/138.. Train loss: 59.539.. Val loss: 54.243.. Train L1 norm: 3.057.. Val L1 norm: 1.064.. Train Linf norm: 2093.249.. Val Linf norm: 52.285\n",
            "Epoch 13/138.. Train loss: 61.225.. Val loss: 54.257.. Train L1 norm: 3.083.. Val L1 norm: 1.064.. Train Linf norm: 2119.866.. Val Linf norm: 52.311\n",
            "Epoch 14/138.. Train loss: 59.998.. Val loss: 54.242.. Train L1 norm: 3.103.. Val L1 norm: 1.066.. Train Linf norm: 2142.094.. Val Linf norm: 53.149\n",
            "Epoch 15/138.. Train loss: 59.999.. Val loss: 54.242.. Train L1 norm: 3.015.. Val L1 norm: 1.066.. Train Linf norm: 2049.820.. Val Linf norm: 53.215\n",
            "Epoch 16/138.. Train loss: 61.404.. Val loss: 54.250.. Train L1 norm: 3.069.. Val L1 norm: 1.066.. Train Linf norm: 2103.119.. Val Linf norm: 53.076\n",
            "Epoch 17/138.. Train loss: 59.724.. Val loss: 54.250.. Train L1 norm: 3.067.. Val L1 norm: 1.066.. Train Linf norm: 2103.105.. Val Linf norm: 53.150\n",
            "Epoch 18/138.. Train loss: 60.035.. Val loss: 54.244.. Train L1 norm: 3.084.. Val L1 norm: 1.066.. Train Linf norm: 2120.422.. Val Linf norm: 53.403\n",
            "Epoch 19/138.. Train loss: 59.604.. Val loss: 54.238.. Train L1 norm: 2.952.. Val L1 norm: 1.066.. Train Linf norm: 1981.112.. Val Linf norm: 53.622\n",
            "Epoch 20/138.. Train loss: 60.079.. Val loss: 54.231.. Train L1 norm: 2.971.. Val L1 norm: 1.067.. Train Linf norm: 2004.315.. Val Linf norm: 53.920\n",
            "Epoch 21/138.. Train loss: 60.107.. Val loss: 54.229.. Train L1 norm: 3.024.. Val L1 norm: 1.067.. Train Linf norm: 2060.079.. Val Linf norm: 53.966\n",
            "Epoch 22/138.. Train loss: 60.996.. Val loss: 54.230.. Train L1 norm: 3.012.. Val L1 norm: 1.067.. Train Linf norm: 2045.233.. Val Linf norm: 53.959\n",
            "Epoch 23/138.. Train loss: 59.656.. Val loss: 54.230.. Train L1 norm: 3.156.. Val L1 norm: 1.067.. Train Linf norm: 2193.513.. Val Linf norm: 53.969\n",
            "Epoch 24/138.. Train loss: 59.585.. Val loss: 54.230.. Train L1 norm: 2.998.. Val L1 norm: 1.067.. Train Linf norm: 2028.525.. Val Linf norm: 53.984\n",
            "Epoch 25/138.. Train loss: 59.597.. Val loss: 54.230.. Train L1 norm: 3.089.. Val L1 norm: 1.067.. Train Linf norm: 2126.225.. Val Linf norm: 54.002\n",
            "Epoch 26/138.. Train loss: 60.922.. Val loss: 54.231.. Train L1 norm: 3.129.. Val L1 norm: 1.067.. Train Linf norm: 2166.216.. Val Linf norm: 53.976\n",
            "Epoch 27/138.. Train loss: 59.698.. Val loss: 54.231.. Train L1 norm: 3.124.. Val L1 norm: 1.067.. Train Linf norm: 2159.048.. Val Linf norm: 53.979\n",
            "Epoch 28/138.. Train loss: 60.225.. Val loss: 54.231.. Train L1 norm: 3.195.. Val L1 norm: 1.067.. Train Linf norm: 2233.432.. Val Linf norm: 53.980\n",
            "Epoch 29/138.. Train loss: 60.253.. Val loss: 54.231.. Train L1 norm: 3.107.. Val L1 norm: 1.067.. Train Linf norm: 2144.262.. Val Linf norm: 53.981\n",
            "Epoch 30/138.. Train loss: 60.891.. Val loss: 54.231.. Train L1 norm: 3.154.. Val L1 norm: 1.067.. Train Linf norm: 2191.903.. Val Linf norm: 53.988\n",
            "Epoch 31/138.. Train loss: 60.578.. Val loss: 54.231.. Train L1 norm: 3.163.. Val L1 norm: 1.067.. Train Linf norm: 2201.561.. Val Linf norm: 53.991\n",
            "Epoch 32/138.. Train loss: 59.631.. Val loss: 54.231.. Train L1 norm: 3.204.. Val L1 norm: 1.067.. Train Linf norm: 2238.982.. Val Linf norm: 53.994\n",
            "Epoch 33/138.. Train loss: 59.595.. Val loss: 54.231.. Train L1 norm: 2.991.. Val L1 norm: 1.067.. Train Linf norm: 2025.296.. Val Linf norm: 53.997\n",
            "Epoch 34/138.. Train loss: 64.301.. Val loss: 54.232.. Train L1 norm: 3.064.. Val L1 norm: 1.067.. Train Linf norm: 2083.475.. Val Linf norm: 53.984\n",
            "Epoch 35/138.. Train loss: 60.143.. Val loss: 54.232.. Train L1 norm: 3.100.. Val L1 norm: 1.067.. Train Linf norm: 2137.049.. Val Linf norm: 53.987\n",
            "Epoch 36/138.. Train loss: 60.995.. Val loss: 54.231.. Train L1 norm: 3.069.. Val L1 norm: 1.067.. Train Linf norm: 2103.878.. Val Linf norm: 53.997\n",
            "Epoch 37/138.. Train loss: 59.669.. Val loss: 54.231.. Train L1 norm: 3.202.. Val L1 norm: 1.067.. Train Linf norm: 2240.669.. Val Linf norm: 54.003\n",
            "Epoch 38/138.. Train loss: 60.204.. Val loss: 54.231.. Train L1 norm: 3.033.. Val L1 norm: 1.067.. Train Linf norm: 2067.591.. Val Linf norm: 54.011\n",
            "Epoch 39/138.. Train loss: 59.735.. Val loss: 54.231.. Train L1 norm: 3.124.. Val L1 norm: 1.067.. Train Linf norm: 2162.140.. Val Linf norm: 54.014\n",
            "Epoch 40/138.. Train loss: 59.601.. Val loss: 54.231.. Train L1 norm: 3.043.. Val L1 norm: 1.067.. Train Linf norm: 2078.886.. Val Linf norm: 54.017\n",
            "Epoch 41/138.. Train loss: 59.555.. Val loss: 54.231.. Train L1 norm: 2.983.. Val L1 norm: 1.067.. Train Linf norm: 2016.594.. Val Linf norm: 54.022\n",
            "Epoch 42/138.. Train loss: 60.762.. Val loss: 54.231.. Train L1 norm: 3.038.. Val L1 norm: 1.067.. Train Linf norm: 2075.166.. Val Linf norm: 54.017\n",
            "Epoch 43/138.. Train loss: 59.899.. Val loss: 54.231.. Train L1 norm: 3.077.. Val L1 norm: 1.067.. Train Linf norm: 2113.315.. Val Linf norm: 54.017\n",
            "Epoch 44/138.. Train loss: 60.059.. Val loss: 54.231.. Train L1 norm: 3.179.. Val L1 norm: 1.067.. Train Linf norm: 2217.514.. Val Linf norm: 54.027\n",
            "Epoch 45/138.. Train loss: 59.666.. Val loss: 54.231.. Train L1 norm: 3.214.. Val L1 norm: 1.067.. Train Linf norm: 2253.698.. Val Linf norm: 54.030\n",
            "Epoch 46/138.. Train loss: 60.356.. Val loss: 54.231.. Train L1 norm: 2.934.. Val L1 norm: 1.067.. Train Linf norm: 1965.829.. Val Linf norm: 54.027\n",
            "Epoch 47/138.. Train loss: 59.699.. Val loss: 54.231.. Train L1 norm: 2.998.. Val L1 norm: 1.067.. Train Linf norm: 2031.873.. Val Linf norm: 54.028\n",
            "Epoch 48/138.. Train loss: 59.783.. Val loss: 54.231.. Train L1 norm: 3.126.. Val L1 norm: 1.067.. Train Linf norm: 2163.868.. Val Linf norm: 54.030\n",
            "Epoch 49/138.. Train loss: 59.968.. Val loss: 54.231.. Train L1 norm: 3.001.. Val L1 norm: 1.067.. Train Linf norm: 2035.322.. Val Linf norm: 54.029\n",
            "Epoch 50/138.. Train loss: 63.385.. Val loss: 54.232.. Train L1 norm: 3.132.. Val L1 norm: 1.067.. Train Linf norm: 2169.478.. Val Linf norm: 54.010\n",
            "Epoch 51/138.. Train loss: 59.601.. Val loss: 54.232.. Train L1 norm: 3.225.. Val L1 norm: 1.067.. Train Linf norm: 2265.305.. Val Linf norm: 54.016\n",
            "Epoch 52/138.. Train loss: 59.535.. Val loss: 54.232.. Train L1 norm: 3.149.. Val L1 norm: 1.067.. Train Linf norm: 2186.140.. Val Linf norm: 54.021\n",
            "Epoch 53/138.. Train loss: 60.381.. Val loss: 54.232.. Train L1 norm: 2.925.. Val L1 norm: 1.067.. Train Linf norm: 1956.993.. Val Linf norm: 54.032\n",
            "Epoch 54/138.. Train loss: 60.165.. Val loss: 54.231.. Train L1 norm: 3.020.. Val L1 norm: 1.067.. Train Linf norm: 2049.306.. Val Linf norm: 54.037\n",
            "Epoch 55/138.. Train loss: 64.166.. Val loss: 54.233.. Train L1 norm: 3.181.. Val L1 norm: 1.067.. Train Linf norm: 2218.824.. Val Linf norm: 54.002\n",
            "Epoch 56/138.. Train loss: 59.589.. Val loss: 54.233.. Train L1 norm: 3.043.. Val L1 norm: 1.067.. Train Linf norm: 2079.050.. Val Linf norm: 54.007\n",
            "Epoch 57/138.. Train loss: 59.538.. Val loss: 54.233.. Train L1 norm: 3.010.. Val L1 norm: 1.067.. Train Linf norm: 2045.335.. Val Linf norm: 54.013\n",
            "Epoch 58/138.. Train loss: 59.813.. Val loss: 54.232.. Train L1 norm: 3.140.. Val L1 norm: 1.067.. Train Linf norm: 2177.358.. Val Linf norm: 54.026\n",
            "Epoch 59/138.. Train loss: 59.528.. Val loss: 54.232.. Train L1 norm: 2.855.. Val L1 norm: 1.067.. Train Linf norm: 1886.024.. Val Linf norm: 54.031\n",
            "Epoch 60/138.. Train loss: 59.548.. Val loss: 54.232.. Train L1 norm: 3.011.. Val L1 norm: 1.067.. Train Linf norm: 2045.541.. Val Linf norm: 54.036\n",
            "Epoch 61/138.. Train loss: 59.722.. Val loss: 54.232.. Train L1 norm: 3.139.. Val L1 norm: 1.067.. Train Linf norm: 2177.563.. Val Linf norm: 54.048\n",
            "Epoch 62/138.. Train loss: 59.544.. Val loss: 54.232.. Train L1 norm: 3.003.. Val L1 norm: 1.067.. Train Linf norm: 2033.026.. Val Linf norm: 54.053\n",
            "Epoch 63/138.. Train loss: 60.111.. Val loss: 54.232.. Train L1 norm: 3.215.. Val L1 norm: 1.067.. Train Linf norm: 2254.634.. Val Linf norm: 54.053\n",
            "Epoch 64/138.. Train loss: 61.529.. Val loss: 54.231.. Train L1 norm: 2.910.. Val L1 norm: 1.067.. Train Linf norm: 1937.993.. Val Linf norm: 54.083\n",
            "Epoch 65/138.. Train loss: 59.648.. Val loss: 54.230.. Train L1 norm: 2.896.. Val L1 norm: 1.067.. Train Linf norm: 1928.562.. Val Linf norm: 54.093\n",
            "Epoch 66/138.. Train loss: 60.243.. Val loss: 54.231.. Train L1 norm: 3.098.. Val L1 norm: 1.067.. Train Linf norm: 2135.059.. Val Linf norm: 54.083\n",
            "Epoch 67/138.. Train loss: 59.534.. Val loss: 54.231.. Train L1 norm: 3.221.. Val L1 norm: 1.067.. Train Linf norm: 2262.814.. Val Linf norm: 54.086\n",
            "Epoch 68/138.. Train loss: 59.790.. Val loss: 54.231.. Train L1 norm: 3.137.. Val L1 norm: 1.067.. Train Linf norm: 2174.188.. Val Linf norm: 54.084\n",
            "Epoch 69/138.. Train loss: 60.254.. Val loss: 54.231.. Train L1 norm: 3.028.. Val L1 norm: 1.067.. Train Linf norm: 2063.273.. Val Linf norm: 54.072\n",
            "Epoch 70/138.. Train loss: 59.912.. Val loss: 54.232.. Train L1 norm: 2.966.. Val L1 norm: 1.067.. Train Linf norm: 1999.228.. Val Linf norm: 54.065\n",
            "Epoch 71/138.. Train loss: 59.522.. Val loss: 54.232.. Train L1 norm: 2.863.. Val L1 norm: 1.067.. Train Linf norm: 1894.571.. Val Linf norm: 54.071\n",
            "Epoch 72/138.. Train loss: 59.538.. Val loss: 54.232.. Train L1 norm: 3.195.. Val L1 norm: 1.067.. Train Linf norm: 2234.001.. Val Linf norm: 54.079\n",
            "Epoch 73/138.. Train loss: 59.952.. Val loss: 54.231.. Train L1 norm: 2.973.. Val L1 norm: 1.067.. Train Linf norm: 2005.910.. Val Linf norm: 54.101\n",
            "Epoch 74/138.. Train loss: 59.744.. Val loss: 54.231.. Train L1 norm: 3.093.. Val L1 norm: 1.067.. Train Linf norm: 2129.860.. Val Linf norm: 54.106\n",
            "Epoch 75/138.. Train loss: 59.682.. Val loss: 54.231.. Train L1 norm: 3.178.. Val L1 norm: 1.067.. Train Linf norm: 2215.672.. Val Linf norm: 54.094\n",
            "Epoch 76/138.. Train loss: 59.581.. Val loss: 54.231.. Train L1 norm: 3.138.. Val L1 norm: 1.067.. Train Linf norm: 2170.158.. Val Linf norm: 54.104\n",
            "Epoch 77/138.. Train loss: 59.888.. Val loss: 54.231.. Train L1 norm: 3.106.. Val L1 norm: 1.067.. Train Linf norm: 2140.719.. Val Linf norm: 54.115\n",
            "Epoch 78/138.. Train loss: 59.839.. Val loss: 54.229.. Train L1 norm: 3.146.. Val L1 norm: 1.067.. Train Linf norm: 2184.364.. Val Linf norm: 54.154\n",
            "Epoch 79/138.. Train loss: 59.586.. Val loss: 54.229.. Train L1 norm: 3.165.. Val L1 norm: 1.067.. Train Linf norm: 2193.522.. Val Linf norm: 54.156\n",
            "Epoch 80/138.. Train loss: 59.574.. Val loss: 54.229.. Train L1 norm: 3.047.. Val L1 norm: 1.067.. Train Linf norm: 2083.346.. Val Linf norm: 54.159\n",
            "Epoch 81/138.. Train loss: 59.522.. Val loss: 54.229.. Train L1 norm: 3.124.. Val L1 norm: 1.067.. Train Linf norm: 2161.366.. Val Linf norm: 54.165\n",
            "Epoch 82/138.. Train loss: 59.521.. Val loss: 54.229.. Train L1 norm: 3.170.. Val L1 norm: 1.067.. Train Linf norm: 2208.429.. Val Linf norm: 54.173\n",
            "Epoch 83/138.. Train loss: 59.785.. Val loss: 54.228.. Train L1 norm: 2.956.. Val L1 norm: 1.067.. Train Linf norm: 1989.795.. Val Linf norm: 54.198\n",
            "Epoch 84/138.. Train loss: 60.143.. Val loss: 54.229.. Train L1 norm: 3.063.. Val L1 norm: 1.067.. Train Linf norm: 2098.136.. Val Linf norm: 54.177\n",
            "Epoch 85/138.. Train loss: 59.518.. Val loss: 54.229.. Train L1 norm: 3.334.. Val L1 norm: 1.067.. Train Linf norm: 2375.061.. Val Linf norm: 54.184\n",
            "Epoch 86/138.. Train loss: 60.182.. Val loss: 54.228.. Train L1 norm: 3.020.. Val L1 norm: 1.067.. Train Linf norm: 2055.818.. Val Linf norm: 54.223\n",
            "Epoch 87/138.. Train loss: 59.629.. Val loss: 54.228.. Train L1 norm: 2.962.. Val L1 norm: 1.067.. Train Linf norm: 1996.267.. Val Linf norm: 54.225\n",
            "Epoch 88/138.. Train loss: 59.624.. Val loss: 54.228.. Train L1 norm: 3.310.. Val L1 norm: 1.067.. Train Linf norm: 2351.433.. Val Linf norm: 54.225\n",
            "Epoch 89/138.. Train loss: 60.827.. Val loss: 54.226.. Train L1 norm: 3.197.. Val L1 norm: 1.067.. Train Linf norm: 2234.633.. Val Linf norm: 54.280\n",
            "Epoch 90/138.. Train loss: 60.681.. Val loss: 54.225.. Train L1 norm: 3.081.. Val L1 norm: 1.067.. Train Linf norm: 2116.022.. Val Linf norm: 54.327\n",
            "Epoch 91/138.. Train loss: 59.678.. Val loss: 54.223.. Train L1 norm: 3.062.. Val L1 norm: 1.067.. Train Linf norm: 2097.726.. Val Linf norm: 54.363\n",
            "Epoch 92/138.. Train loss: 59.717.. Val loss: 54.224.. Train L1 norm: 3.209.. Val L1 norm: 1.067.. Train Linf norm: 2248.117.. Val Linf norm: 54.358\n",
            "Epoch 93/138.. Train loss: 60.644.. Val loss: 54.225.. Train L1 norm: 3.031.. Val L1 norm: 1.067.. Train Linf norm: 771.464.. Val Linf norm: 54.322\n",
            "Epoch 94/138.. Train loss: 59.744.. Val loss: 54.224.. Train L1 norm: 2.959.. Val L1 norm: 1.067.. Train Linf norm: 1992.246.. Val Linf norm: 54.350\n",
            "Epoch 95/138.. Train loss: 59.916.. Val loss: 54.223.. Train L1 norm: 3.029.. Val L1 norm: 1.067.. Train Linf norm: 2064.311.. Val Linf norm: 54.386\n",
            "Epoch 96/138.. Train loss: 59.839.. Val loss: 54.224.. Train L1 norm: 3.137.. Val L1 norm: 1.067.. Train Linf norm: 2174.686.. Val Linf norm: 54.374\n",
            "Epoch 97/138.. Train loss: 60.354.. Val loss: 54.224.. Train L1 norm: 2.742.. Val L1 norm: 1.067.. Train Linf norm: 1768.837.. Val Linf norm: 54.366\n",
            "Epoch 98/138.. Train loss: 60.411.. Val loss: 54.227.. Train L1 norm: 3.034.. Val L1 norm: 1.067.. Train Linf norm: 2068.487.. Val Linf norm: 54.293\n",
            "Epoch 99/138.. Train loss: 60.274.. Val loss: 54.228.. Train L1 norm: 2.987.. Val L1 norm: 1.067.. Train Linf norm: 2020.667.. Val Linf norm: 54.258\n",
            "Epoch 100/138.. Train loss: 60.552.. Val loss: 54.229.. Train L1 norm: 3.016.. Val L1 norm: 1.067.. Train Linf norm: 2050.255.. Val Linf norm: 54.223\n",
            "Epoch 101/138.. Train loss: 59.827.. Val loss: 54.229.. Train L1 norm: 3.191.. Val L1 norm: 1.067.. Train Linf norm: 2230.540.. Val Linf norm: 54.246\n",
            "Epoch 102/138.. Train loss: 59.522.. Val loss: 54.228.. Train L1 norm: 3.085.. Val L1 norm: 1.067.. Train Linf norm: 2120.461.. Val Linf norm: 54.265\n",
            "Epoch 103/138.. Train loss: 60.118.. Val loss: 54.229.. Train L1 norm: 3.059.. Val L1 norm: 1.067.. Train Linf norm: 2093.932.. Val Linf norm: 54.237\n",
            "Epoch 104/138.. Train loss: 59.554.. Val loss: 54.229.. Train L1 norm: 3.139.. Val L1 norm: 1.067.. Train Linf norm: 2177.983.. Val Linf norm: 54.249\n",
            "Epoch 105/138.. Train loss: 60.880.. Val loss: 54.227.. Train L1 norm: 3.208.. Val L1 norm: 1.067.. Train Linf norm: 2246.491.. Val Linf norm: 54.302\n",
            "Epoch 106/138.. Train loss: 59.621.. Val loss: 54.226.. Train L1 norm: 3.087.. Val L1 norm: 1.067.. Train Linf norm: 2124.621.. Val Linf norm: 54.340\n",
            "Epoch 107/138.. Train loss: 59.577.. Val loss: 54.226.. Train L1 norm: 3.040.. Val L1 norm: 1.067.. Train Linf norm: 2075.589.. Val Linf norm: 54.341\n",
            "Epoch 108/138.. Train loss: 59.715.. Val loss: 54.225.. Train L1 norm: 3.175.. Val L1 norm: 1.067.. Train Linf norm: 2213.249.. Val Linf norm: 54.372\n",
            "Epoch 109/138.. Train loss: 60.250.. Val loss: 54.223.. Train L1 norm: 3.206.. Val L1 norm: 1.067.. Train Linf norm: 2245.103.. Val Linf norm: 54.426\n",
            "Epoch 110/138.. Train loss: 59.846.. Val loss: 54.222.. Train L1 norm: 3.128.. Val L1 norm: 1.067.. Train Linf norm: 2165.202.. Val Linf norm: 54.451\n",
            "Epoch 111/138.. Train loss: 59.570.. Val loss: 54.222.. Train L1 norm: 3.024.. Val L1 norm: 1.068.. Train Linf norm: 2057.719.. Val Linf norm: 54.473\n",
            "Epoch 112/138.. Train loss: 60.296.. Val loss: 54.221.. Train L1 norm: 2.889.. Val L1 norm: 1.068.. Train Linf norm: 1920.256.. Val Linf norm: 54.489\n",
            "Epoch 113/138.. Train loss: 59.555.. Val loss: 54.217.. Train L1 norm: 3.059.. Val L1 norm: 1.068.. Train Linf norm: 2095.035.. Val Linf norm: 54.600\n",
            "Epoch 114/138.. Train loss: 60.789.. Val loss: 54.219.. Train L1 norm: 2.848.. Val L1 norm: 1.068.. Train Linf norm: 1878.932.. Val Linf norm: 54.560\n",
            "Epoch 115/138.. Train loss: 59.812.. Val loss: 54.218.. Train L1 norm: 3.013.. Val L1 norm: 1.068.. Train Linf norm: 2047.846.. Val Linf norm: 54.595\n",
            "Epoch 116/138.. Train loss: 59.998.. Val loss: 54.218.. Train L1 norm: 3.067.. Val L1 norm: 1.068.. Train Linf norm: 2103.386.. Val Linf norm: 54.592\n",
            "Epoch 117/138.. Train loss: 59.663.. Val loss: 54.219.. Train L1 norm: 3.113.. Val L1 norm: 1.068.. Train Linf norm: 2150.790.. Val Linf norm: 54.566\n",
            "Epoch 118/138.. Train loss: 61.702.. Val loss: 54.221.. Train L1 norm: 3.174.. Val L1 norm: 1.068.. Train Linf norm: 2212.478.. Val Linf norm: 54.510\n",
            "Epoch 119/138.. Train loss: 59.528.. Val loss: 54.221.. Train L1 norm: 3.172.. Val L1 norm: 1.068.. Train Linf norm: 2208.277.. Val Linf norm: 54.516\n",
            "Epoch 120/138.. Train loss: 61.097.. Val loss: 54.222.. Train L1 norm: 3.157.. Val L1 norm: 1.068.. Train Linf norm: 2194.326.. Val Linf norm: 54.482\n",
            "Epoch 121/138.. Train loss: 59.604.. Val loss: 54.223.. Train L1 norm: 2.790.. Val L1 norm: 1.067.. Train Linf norm: 1819.330.. Val Linf norm: 54.461\n",
            "Epoch 122/138.. Train loss: 59.713.. Val loss: 54.223.. Train L1 norm: 2.996.. Val L1 norm: 1.067.. Train Linf norm: 2029.679.. Val Linf norm: 54.455\n",
            "Epoch 123/138.. Train loss: 59.618.. Val loss: 54.224.. Train L1 norm: 3.136.. Val L1 norm: 1.067.. Train Linf norm: 2173.077.. Val Linf norm: 54.456\n",
            "Epoch 124/138.. Train loss: 59.527.. Val loss: 54.223.. Train L1 norm: 3.192.. Val L1 norm: 1.068.. Train Linf norm: 2232.977.. Val Linf norm: 54.469\n",
            "Epoch 125/138.. Train loss: 60.080.. Val loss: 54.224.. Train L1 norm: 3.236.. Val L1 norm: 1.067.. Train Linf norm: 2275.607.. Val Linf norm: 54.451\n",
            "Epoch 126/138.. Train loss: 62.138.. Val loss: 54.227.. Train L1 norm: 3.114.. Val L1 norm: 1.067.. Train Linf norm: 2151.715.. Val Linf norm: 54.371\n",
            "Epoch 127/138.. Train loss: 59.769.. Val loss: 54.226.. Train L1 norm: 3.230.. Val L1 norm: 1.067.. Train Linf norm: 2265.540.. Val Linf norm: 54.398\n",
            "Epoch 128/138.. Train loss: 59.724.. Val loss: 54.225.. Train L1 norm: 3.040.. Val L1 norm: 1.067.. Train Linf norm: 2075.321.. Val Linf norm: 54.432\n",
            "Epoch 129/138.. Train loss: 60.284.. Val loss: 54.226.. Train L1 norm: 3.197.. Val L1 norm: 1.067.. Train Linf norm: 2236.545.. Val Linf norm: 54.399\n",
            "Epoch 130/138.. Train loss: 60.460.. Val loss: 54.227.. Train L1 norm: 2.975.. Val L1 norm: 1.067.. Train Linf norm: 2008.805.. Val Linf norm: 54.368\n",
            "Epoch 131/138.. Train loss: 60.157.. Val loss: 54.228.. Train L1 norm: 3.076.. Val L1 norm: 1.067.. Train Linf norm: 2111.750.. Val Linf norm: 54.338\n",
            "Epoch 132/138.. Train loss: 61.579.. Val loss: 54.227.. Train L1 norm: 3.063.. Val L1 norm: 1.067.. Train Linf norm: 2098.428.. Val Linf norm: 54.390\n",
            "Epoch 133/138.. Train loss: 59.874.. Val loss: 54.226.. Train L1 norm: 3.135.. Val L1 norm: 1.067.. Train Linf norm: 2172.633.. Val Linf norm: 54.402\n",
            "Epoch 134/138.. Train loss: 59.567.. Val loss: 54.226.. Train L1 norm: 2.993.. Val L1 norm: 1.067.. Train Linf norm: 2023.262.. Val Linf norm: 54.415\n",
            "Epoch 135/138.. Train loss: 60.327.. Val loss: 54.227.. Train L1 norm: 3.156.. Val L1 norm: 1.067.. Train Linf norm: 2193.988.. Val Linf norm: 54.388\n",
            "Epoch 136/138.. Train loss: 59.662.. Val loss: 54.227.. Train L1 norm: 3.011.. Val L1 norm: 1.067.. Train Linf norm: 2035.981.. Val Linf norm: 54.386\n",
            "Epoch 137/138.. Train loss: 59.587.. Val loss: 54.227.. Train L1 norm: 3.117.. Val L1 norm: 1.067.. Train Linf norm: 2154.506.. Val Linf norm: 54.389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 07:41:58,587]\u001b[0m Trial 52 finished with value: 1.067402313899994 and parameters: {'n_layers': 7, 'n_units_0': 3577, 'n_units_1': 2907, 'n_units_2': 3173, 'n_units_3': 304, 'n_units_4': 1762, 'n_units_5': 1759, 'n_units_6': 3636, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 2.8078458774708172e-06, 'batch_size': 1024, 'n_epochs': 138, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.22191154112061756, 'dropout_rate': 0.006301888044792111, 'weight_decay': 0.00019846731258881073, 'beta1': 0.9161504581676174, 'beta2': 0.9990020048873883, 'factor': 0.17728561207041818, 'patience': 5, 'threshold': 0.0011586984428089211}. Best is trial 22 with value: 1.0150012904485066.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 138/138.. Train loss: 59.525.. Val loss: 54.227.. Train L1 norm: 3.113.. Val L1 norm: 1.067.. Train Linf norm: 2149.474.. Val Linf norm: 54.399\n",
            "Epoch 1/138.. Train loss: 1826.126.. Val loss: 53.625.. Train L1 norm: 5.746.. Val L1 norm: 1.081.. Train Linf norm: 4818.298.. Val Linf norm: 59.494\n",
            "Epoch 2/138.. Train loss: 91.126.. Val loss: 54.272.. Train L1 norm: 3.357.. Val L1 norm: 1.058.. Train Linf norm: 2396.770.. Val Linf norm: 46.274\n",
            "Epoch 3/138.. Train loss: 102.170.. Val loss: 53.506.. Train L1 norm: 4.139.. Val L1 norm: 1.091.. Train Linf norm: 3193.347.. Val Linf norm: 66.248\n",
            "Epoch 4/138.. Train loss: 141.990.. Val loss: 54.413.. Train L1 norm: 4.219.. Val L1 norm: 1.060.. Train Linf norm: 3271.358.. Val Linf norm: 47.687\n",
            "Epoch 5/138.. Train loss: 131.279.. Val loss: 53.417.. Train L1 norm: 4.188.. Val L1 norm: 1.102.. Train Linf norm: 3242.557.. Val Linf norm: 73.457\n",
            "Epoch 6/138.. Train loss: 190.193.. Val loss: 54.291.. Train L1 norm: 4.541.. Val L1 norm: 1.068.. Train Linf norm: 3596.095.. Val Linf norm: 53.482\n",
            "Epoch 7/138.. Train loss: 90.964.. Val loss: 53.887.. Train L1 norm: 3.815.. Val L1 norm: 1.087.. Train Linf norm: 2862.104.. Val Linf norm: 64.618\n",
            "Epoch 8/138.. Train loss: 67.961.. Val loss: 54.318.. Train L1 norm: 3.451.. Val L1 norm: 1.073.. Train Linf norm: 2492.654.. Val Linf norm: 56.551\n",
            "Epoch 9/138.. Train loss: 81.660.. Val loss: 53.517.. Train L1 norm: 4.267.. Val L1 norm: 1.108.. Train Linf norm: 3312.766.. Val Linf norm: 78.412\n",
            "Epoch 10/138.. Train loss: 112.038.. Val loss: 54.645.. Train L1 norm: 3.811.. Val L1 norm: 1.068.. Train Linf norm: 2855.595.. Val Linf norm: 53.840\n",
            "Epoch 11/138.. Train loss: 150.488.. Val loss: 54.311.. Train L1 norm: 3.401.. Val L1 norm: 1.083.. Train Linf norm: 2444.301.. Val Linf norm: 63.357\n",
            "Epoch 12/138.. Train loss: 59.703.. Val loss: 54.029.. Train L1 norm: 3.955.. Val L1 norm: 1.094.. Train Linf norm: 3003.637.. Val Linf norm: 70.228\n",
            "Epoch 13/138.. Train loss: 60.330.. Val loss: 53.979.. Train L1 norm: 4.049.. Val L1 norm: 1.096.. Train Linf norm: 3091.158.. Val Linf norm: 71.809\n",
            "Epoch 14/138.. Train loss: 59.156.. Val loss: 53.972.. Train L1 norm: 4.096.. Val L1 norm: 1.097.. Train Linf norm: 3144.674.. Val Linf norm: 72.400\n",
            "Epoch 15/138.. Train loss: 59.163.. Val loss: 53.955.. Train L1 norm: 4.125.. Val L1 norm: 1.099.. Train Linf norm: 3174.791.. Val Linf norm: 73.200\n",
            "Epoch 16/138.. Train loss: 59.195.. Val loss: 53.938.. Train L1 norm: 4.167.. Val L1 norm: 1.100.. Train Linf norm: 3215.708.. Val Linf norm: 74.035\n",
            "Epoch 17/138.. Train loss: 59.573.. Val loss: 53.947.. Train L1 norm: 4.148.. Val L1 norm: 1.100.. Train Linf norm: 3196.185.. Val Linf norm: 74.227\n",
            "Epoch 18/138.. Train loss: 59.271.. Val loss: 53.943.. Train L1 norm: 4.164.. Val L1 norm: 1.100.. Train Linf norm: 3215.367.. Val Linf norm: 74.400\n",
            "Epoch 19/138.. Train loss: 59.259.. Val loss: 53.939.. Train L1 norm: 4.143.. Val L1 norm: 1.101.. Train Linf norm: 3190.776.. Val Linf norm: 74.583\n",
            "Epoch 20/138.. Train loss: 59.431.. Val loss: 53.932.. Train L1 norm: 4.182.. Val L1 norm: 1.101.. Train Linf norm: 3231.742.. Val Linf norm: 74.806\n",
            "Epoch 21/138.. Train loss: 59.200.. Val loss: 53.932.. Train L1 norm: 4.207.. Val L1 norm: 1.101.. Train Linf norm: 3254.588.. Val Linf norm: 74.889\n",
            "Epoch 22/138.. Train loss: 59.114.. Val loss: 53.929.. Train L1 norm: 4.182.. Val L1 norm: 1.101.. Train Linf norm: 3230.929.. Val Linf norm: 75.036\n",
            "Epoch 23/138.. Train loss: 59.267.. Val loss: 53.930.. Train L1 norm: 4.162.. Val L1 norm: 1.101.. Train Linf norm: 3211.095.. Val Linf norm: 75.086\n",
            "Epoch 24/138.. Train loss: 59.318.. Val loss: 53.930.. Train L1 norm: 4.219.. Val L1 norm: 1.101.. Train Linf norm: 3270.325.. Val Linf norm: 75.097\n",
            "Epoch 25/138.. Train loss: 59.291.. Val loss: 53.930.. Train L1 norm: 4.148.. Val L1 norm: 1.102.. Train Linf norm: 3194.189.. Val Linf norm: 75.110\n",
            "Epoch 26/138.. Train loss: 59.139.. Val loss: 53.930.. Train L1 norm: 4.218.. Val L1 norm: 1.102.. Train Linf norm: 3269.005.. Val Linf norm: 75.127\n",
            "Epoch 27/138.. Train loss: 59.102.. Val loss: 53.930.. Train L1 norm: 4.231.. Val L1 norm: 1.102.. Train Linf norm: 3283.150.. Val Linf norm: 75.148\n",
            "Epoch 28/138.. Train loss: 59.090.. Val loss: 53.929.. Train L1 norm: 4.202.. Val L1 norm: 1.102.. Train Linf norm: 3250.774.. Val Linf norm: 75.170\n",
            "Epoch 29/138.. Train loss: 59.389.. Val loss: 53.928.. Train L1 norm: 4.156.. Val L1 norm: 1.102.. Train Linf norm: 3206.163.. Val Linf norm: 75.210\n",
            "Epoch 30/138.. Train loss: 59.089.. Val loss: 53.928.. Train L1 norm: 4.086.. Val L1 norm: 1.102.. Train Linf norm: 3133.341.. Val Linf norm: 75.213\n",
            "Epoch 31/138.. Train loss: 59.124.. Val loss: 53.928.. Train L1 norm: 4.163.. Val L1 norm: 1.102.. Train Linf norm: 3211.640.. Val Linf norm: 75.216\n",
            "Epoch 32/138.. Train loss: 59.229.. Val loss: 53.928.. Train L1 norm: 4.231.. Val L1 norm: 1.102.. Train Linf norm: 3282.271.. Val Linf norm: 75.219\n",
            "Epoch 33/138.. Train loss: 59.126.. Val loss: 53.928.. Train L1 norm: 4.178.. Val L1 norm: 1.102.. Train Linf norm: 3228.704.. Val Linf norm: 75.223\n",
            "Epoch 34/138.. Train loss: 59.099.. Val loss: 53.928.. Train L1 norm: 4.177.. Val L1 norm: 1.102.. Train Linf norm: 3227.545.. Val Linf norm: 75.226\n",
            "Epoch 35/138.. Train loss: 59.133.. Val loss: 53.928.. Train L1 norm: 4.180.. Val L1 norm: 1.102.. Train Linf norm: 3228.653.. Val Linf norm: 75.230\n",
            "Epoch 36/138.. Train loss: 59.118.. Val loss: 53.928.. Train L1 norm: 4.192.. Val L1 norm: 1.102.. Train Linf norm: 3241.036.. Val Linf norm: 75.233\n",
            "Epoch 37/138.. Train loss: 59.882.. Val loss: 53.928.. Train L1 norm: 4.162.. Val L1 norm: 1.102.. Train Linf norm: 3209.921.. Val Linf norm: 75.240\n",
            "Epoch 38/138.. Train loss: 59.185.. Val loss: 53.928.. Train L1 norm: 4.231.. Val L1 norm: 1.102.. Train Linf norm: 3281.412.. Val Linf norm: 75.243\n",
            "Epoch 39/138.. Train loss: 59.093.. Val loss: 53.928.. Train L1 norm: 4.136.. Val L1 norm: 1.102.. Train Linf norm: 3183.910.. Val Linf norm: 75.247\n",
            "Epoch 40/138.. Train loss: 59.105.. Val loss: 53.928.. Train L1 norm: 4.245.. Val L1 norm: 1.102.. Train Linf norm: 3295.174.. Val Linf norm: 75.250\n",
            "Epoch 41/138.. Train loss: 59.139.. Val loss: 53.928.. Train L1 norm: 4.102.. Val L1 norm: 1.102.. Train Linf norm: 3151.089.. Val Linf norm: 75.254\n",
            "Epoch 42/138.. Train loss: 59.147.. Val loss: 53.928.. Train L1 norm: 4.206.. Val L1 norm: 1.102.. Train Linf norm: 3255.808.. Val Linf norm: 75.258\n",
            "Epoch 43/138.. Train loss: 59.098.. Val loss: 53.927.. Train L1 norm: 4.060.. Val L1 norm: 1.102.. Train Linf norm: 3106.710.. Val Linf norm: 75.262\n",
            "Epoch 44/138.. Train loss: 59.149.. Val loss: 53.927.. Train L1 norm: 4.176.. Val L1 norm: 1.102.. Train Linf norm: 3225.801.. Val Linf norm: 75.265\n",
            "Epoch 45/138.. Train loss: 59.102.. Val loss: 53.927.. Train L1 norm: 4.149.. Val L1 norm: 1.102.. Train Linf norm: 3197.248.. Val Linf norm: 75.269\n",
            "Epoch 46/138.. Train loss: 59.124.. Val loss: 53.927.. Train L1 norm: 4.122.. Val L1 norm: 1.102.. Train Linf norm: 3169.617.. Val Linf norm: 75.273\n",
            "Epoch 47/138.. Train loss: 59.222.. Val loss: 53.927.. Train L1 norm: 4.192.. Val L1 norm: 1.102.. Train Linf norm: 3242.804.. Val Linf norm: 75.276\n",
            "Epoch 48/138.. Train loss: 59.602.. Val loss: 53.927.. Train L1 norm: 4.047.. Val L1 norm: 1.102.. Train Linf norm: 3092.062.. Val Linf norm: 75.281\n",
            "Epoch 49/138.. Train loss: 59.115.. Val loss: 53.927.. Train L1 norm: 4.209.. Val L1 norm: 1.102.. Train Linf norm: 3258.611.. Val Linf norm: 75.288\n",
            "Epoch 50/138.. Train loss: 59.094.. Val loss: 53.927.. Train L1 norm: 4.137.. Val L1 norm: 1.102.. Train Linf norm: 3183.250.. Val Linf norm: 75.292\n",
            "Epoch 51/138.. Train loss: 59.095.. Val loss: 53.927.. Train L1 norm: 4.215.. Val L1 norm: 1.102.. Train Linf norm: 3257.804.. Val Linf norm: 75.296\n",
            "Epoch 52/138.. Train loss: 59.461.. Val loss: 53.927.. Train L1 norm: 4.158.. Val L1 norm: 1.102.. Train Linf norm: 3206.878.. Val Linf norm: 75.296\n",
            "Epoch 53/138.. Train loss: 59.113.. Val loss: 53.927.. Train L1 norm: 4.227.. Val L1 norm: 1.102.. Train Linf norm: 3277.278.. Val Linf norm: 75.300\n",
            "Epoch 54/138.. Train loss: 59.093.. Val loss: 53.927.. Train L1 norm: 4.250.. Val L1 norm: 1.102.. Train Linf norm: 3298.505.. Val Linf norm: 75.305\n",
            "Epoch 55/138.. Train loss: 59.090.. Val loss: 53.927.. Train L1 norm: 4.181.. Val L1 norm: 1.102.. Train Linf norm: 3230.232.. Val Linf norm: 75.309\n",
            "Epoch 56/138.. Train loss: 59.162.. Val loss: 53.927.. Train L1 norm: 4.141.. Val L1 norm: 1.102.. Train Linf norm: 3186.949.. Val Linf norm: 75.314\n",
            "Epoch 57/138.. Train loss: 59.096.. Val loss: 53.927.. Train L1 norm: 4.121.. Val L1 norm: 1.102.. Train Linf norm: 3169.900.. Val Linf norm: 75.318\n",
            "Epoch 58/138.. Train loss: 59.104.. Val loss: 53.926.. Train L1 norm: 4.112.. Val L1 norm: 1.102.. Train Linf norm: 3159.336.. Val Linf norm: 75.323\n",
            "Epoch 59/138.. Train loss: 59.129.. Val loss: 53.926.. Train L1 norm: 4.172.. Val L1 norm: 1.102.. Train Linf norm: 3218.134.. Val Linf norm: 75.327\n",
            "Epoch 60/138.. Train loss: 59.094.. Val loss: 53.926.. Train L1 norm: 4.181.. Val L1 norm: 1.102.. Train Linf norm: 3228.643.. Val Linf norm: 75.332\n",
            "Epoch 61/138.. Train loss: 59.143.. Val loss: 53.926.. Train L1 norm: 4.156.. Val L1 norm: 1.102.. Train Linf norm: 3202.360.. Val Linf norm: 75.336\n",
            "Epoch 62/138.. Train loss: 59.170.. Val loss: 53.926.. Train L1 norm: 4.184.. Val L1 norm: 1.102.. Train Linf norm: 3232.286.. Val Linf norm: 75.338\n",
            "Epoch 63/138.. Train loss: 59.121.. Val loss: 53.926.. Train L1 norm: 4.158.. Val L1 norm: 1.102.. Train Linf norm: 3204.836.. Val Linf norm: 75.343\n",
            "Epoch 64/138.. Train loss: 59.132.. Val loss: 53.926.. Train L1 norm: 4.107.. Val L1 norm: 1.102.. Train Linf norm: 3154.736.. Val Linf norm: 75.348\n",
            "Epoch 65/138.. Train loss: 59.156.. Val loss: 53.926.. Train L1 norm: 4.186.. Val L1 norm: 1.102.. Train Linf norm: 3234.658.. Val Linf norm: 75.354\n",
            "Epoch 66/138.. Train loss: 59.078.. Val loss: 53.926.. Train L1 norm: 4.207.. Val L1 norm: 1.102.. Train Linf norm: 3256.845.. Val Linf norm: 75.358\n",
            "Epoch 67/138.. Train loss: 59.313.. Val loss: 53.925.. Train L1 norm: 4.218.. Val L1 norm: 1.102.. Train Linf norm: 3268.222.. Val Linf norm: 75.371\n",
            "Epoch 68/138.. Train loss: 59.109.. Val loss: 53.925.. Train L1 norm: 4.180.. Val L1 norm: 1.102.. Train Linf norm: 3229.725.. Val Linf norm: 75.376\n",
            "Epoch 69/138.. Train loss: 59.235.. Val loss: 53.925.. Train L1 norm: 4.182.. Val L1 norm: 1.102.. Train Linf norm: 3233.769.. Val Linf norm: 75.377\n",
            "Epoch 70/138.. Train loss: 59.136.. Val loss: 53.925.. Train L1 norm: 4.217.. Val L1 norm: 1.102.. Train Linf norm: 3267.398.. Val Linf norm: 75.383\n",
            "Epoch 71/138.. Train loss: 59.143.. Val loss: 53.925.. Train L1 norm: 4.220.. Val L1 norm: 1.102.. Train Linf norm: 3271.843.. Val Linf norm: 75.386\n",
            "Epoch 72/138.. Train loss: 59.531.. Val loss: 53.924.. Train L1 norm: 4.182.. Val L1 norm: 1.102.. Train Linf norm: 3233.104.. Val Linf norm: 75.408\n",
            "Epoch 73/138.. Train loss: 59.400.. Val loss: 53.924.. Train L1 norm: 4.088.. Val L1 norm: 1.102.. Train Linf norm: 3133.779.. Val Linf norm: 75.426\n",
            "Epoch 74/138.. Train loss: 59.102.. Val loss: 53.924.. Train L1 norm: 4.204.. Val L1 norm: 1.102.. Train Linf norm: 3254.799.. Val Linf norm: 75.432\n",
            "Epoch 75/138.. Train loss: 59.114.. Val loss: 53.923.. Train L1 norm: 4.183.. Val L1 norm: 1.102.. Train Linf norm: 3233.983.. Val Linf norm: 75.438\n",
            "Epoch 76/138.. Train loss: 59.165.. Val loss: 53.923.. Train L1 norm: 4.139.. Val L1 norm: 1.102.. Train Linf norm: 3187.281.. Val Linf norm: 75.447\n",
            "Epoch 77/138.. Train loss: 59.551.. Val loss: 53.923.. Train L1 norm: 4.197.. Val L1 norm: 1.102.. Train Linf norm: 3244.824.. Val Linf norm: 75.456\n",
            "Epoch 78/138.. Train loss: 59.091.. Val loss: 53.922.. Train L1 norm: 4.233.. Val L1 norm: 1.102.. Train Linf norm: 3284.282.. Val Linf norm: 75.478\n",
            "Epoch 79/138.. Train loss: 59.098.. Val loss: 53.922.. Train L1 norm: 4.207.. Val L1 norm: 1.102.. Train Linf norm: 3257.105.. Val Linf norm: 75.482\n",
            "Epoch 80/138.. Train loss: 59.141.. Val loss: 53.922.. Train L1 norm: 4.230.. Val L1 norm: 1.102.. Train Linf norm: 3281.097.. Val Linf norm: 75.494\n",
            "Epoch 81/138.. Train loss: 59.254.. Val loss: 53.921.. Train L1 norm: 4.105.. Val L1 norm: 1.102.. Train Linf norm: 3152.443.. Val Linf norm: 75.511\n",
            "Epoch 82/138.. Train loss: 59.109.. Val loss: 53.921.. Train L1 norm: 4.227.. Val L1 norm: 1.102.. Train Linf norm: 3278.287.. Val Linf norm: 75.516\n",
            "Epoch 83/138.. Train loss: 59.859.. Val loss: 53.920.. Train L1 norm: 4.192.. Val L1 norm: 1.102.. Train Linf norm: 3231.678.. Val Linf norm: 75.535\n",
            "Epoch 84/138.. Train loss: 59.082.. Val loss: 53.919.. Train L1 norm: 4.171.. Val L1 norm: 1.102.. Train Linf norm: 3221.860.. Val Linf norm: 75.563\n",
            "Epoch 85/138.. Train loss: 59.372.. Val loss: 53.920.. Train L1 norm: 4.256.. Val L1 norm: 1.102.. Train Linf norm: 3305.242.. Val Linf norm: 75.551\n",
            "Epoch 86/138.. Train loss: 59.093.. Val loss: 53.920.. Train L1 norm: 4.199.. Val L1 norm: 1.102.. Train Linf norm: 3251.036.. Val Linf norm: 75.557\n",
            "Epoch 87/138.. Train loss: 59.118.. Val loss: 53.920.. Train L1 norm: 4.102.. Val L1 norm: 1.102.. Train Linf norm: 3149.336.. Val Linf norm: 75.561\n",
            "Epoch 88/138.. Train loss: 59.200.. Val loss: 53.919.. Train L1 norm: 4.241.. Val L1 norm: 1.102.. Train Linf norm: 3293.258.. Val Linf norm: 75.577\n",
            "Epoch 89/138.. Train loss: 59.100.. Val loss: 53.919.. Train L1 norm: 4.198.. Val L1 norm: 1.102.. Train Linf norm: 3240.463.. Val Linf norm: 75.584\n",
            "Epoch 90/138.. Train loss: 59.097.. Val loss: 53.919.. Train L1 norm: 4.281.. Val L1 norm: 1.102.. Train Linf norm: 3332.152.. Val Linf norm: 75.588\n",
            "Epoch 91/138.. Train loss: 59.158.. Val loss: 53.919.. Train L1 norm: 4.074.. Val L1 norm: 1.102.. Train Linf norm: 3120.147.. Val Linf norm: 75.603\n",
            "Epoch 92/138.. Train loss: 59.365.. Val loss: 53.917.. Train L1 norm: 4.201.. Val L1 norm: 1.102.. Train Linf norm: 3249.506.. Val Linf norm: 75.638\n",
            "Epoch 93/138.. Train loss: 59.137.. Val loss: 53.917.. Train L1 norm: 4.218.. Val L1 norm: 1.102.. Train Linf norm: 3260.776.. Val Linf norm: 75.648\n",
            "Epoch 94/138.. Train loss: 59.078.. Val loss: 53.917.. Train L1 norm: 4.211.. Val L1 norm: 1.102.. Train Linf norm: 3261.317.. Val Linf norm: 75.655\n",
            "Epoch 95/138.. Train loss: 59.107.. Val loss: 53.916.. Train L1 norm: 4.204.. Val L1 norm: 1.102.. Train Linf norm: 3254.017.. Val Linf norm: 75.667\n",
            "Epoch 96/138.. Train loss: 59.095.. Val loss: 53.916.. Train L1 norm: 4.155.. Val L1 norm: 1.102.. Train Linf norm: 3203.428.. Val Linf norm: 75.672\n",
            "Epoch 97/138.. Train loss: 59.151.. Val loss: 53.916.. Train L1 norm: 4.084.. Val L1 norm: 1.102.. Train Linf norm: 3129.863.. Val Linf norm: 75.672\n",
            "Epoch 98/138.. Train loss: 59.088.. Val loss: 53.916.. Train L1 norm: 4.235.. Val L1 norm: 1.102.. Train Linf norm: 3276.753.. Val Linf norm: 75.678\n",
            "Epoch 99/138.. Train loss: 59.090.. Val loss: 53.916.. Train L1 norm: 4.184.. Val L1 norm: 1.102.. Train Linf norm: 3236.581.. Val Linf norm: 75.685\n",
            "Epoch 100/138.. Train loss: 59.179.. Val loss: 53.915.. Train L1 norm: 4.178.. Val L1 norm: 1.102.. Train Linf norm: 3228.136.. Val Linf norm: 75.709\n",
            "Epoch 101/138.. Train loss: 59.091.. Val loss: 53.915.. Train L1 norm: 4.218.. Val L1 norm: 1.102.. Train Linf norm: 3270.008.. Val Linf norm: 75.717\n",
            "Epoch 102/138.. Train loss: 59.272.. Val loss: 53.916.. Train L1 norm: 4.187.. Val L1 norm: 1.102.. Train Linf norm: 3236.289.. Val Linf norm: 75.706\n",
            "Epoch 103/138.. Train loss: 59.134.. Val loss: 53.915.. Train L1 norm: 4.207.. Val L1 norm: 1.102.. Train Linf norm: 3255.807.. Val Linf norm: 75.717\n",
            "Epoch 104/138.. Train loss: 59.190.. Val loss: 53.916.. Train L1 norm: 4.203.. Val L1 norm: 1.102.. Train Linf norm: 3252.264.. Val Linf norm: 75.711\n",
            "Epoch 105/138.. Train loss: 59.096.. Val loss: 53.916.. Train L1 norm: 4.160.. Val L1 norm: 1.102.. Train Linf norm: 3208.105.. Val Linf norm: 75.718\n",
            "Epoch 106/138.. Train loss: 59.097.. Val loss: 53.915.. Train L1 norm: 4.207.. Val L1 norm: 1.102.. Train Linf norm: 3257.506.. Val Linf norm: 75.725\n",
            "Epoch 107/138.. Train loss: 59.086.. Val loss: 53.915.. Train L1 norm: 4.179.. Val L1 norm: 1.102.. Train Linf norm: 3227.998.. Val Linf norm: 75.732\n",
            "Epoch 108/138.. Train loss: 59.104.. Val loss: 53.915.. Train L1 norm: 4.201.. Val L1 norm: 1.102.. Train Linf norm: 3251.384.. Val Linf norm: 75.748\n",
            "Epoch 109/138.. Train loss: 59.100.. Val loss: 53.915.. Train L1 norm: 4.219.. Val L1 norm: 1.103.. Train Linf norm: 3269.077.. Val Linf norm: 75.756\n",
            "Epoch 110/138.. Train loss: 59.201.. Val loss: 53.913.. Train L1 norm: 4.214.. Val L1 norm: 1.103.. Train Linf norm: 3263.682.. Val Linf norm: 75.792\n",
            "Epoch 111/138.. Train loss: 59.097.. Val loss: 53.912.. Train L1 norm: 4.277.. Val L1 norm: 1.103.. Train Linf norm: 3326.944.. Val Linf norm: 75.816\n",
            "Epoch 112/138.. Train loss: 59.339.. Val loss: 53.914.. Train L1 norm: 4.263.. Val L1 norm: 1.103.. Train Linf norm: 3307.348.. Val Linf norm: 75.777\n",
            "Epoch 113/138.. Train loss: 59.096.. Val loss: 53.914.. Train L1 norm: 4.247.. Val L1 norm: 1.103.. Train Linf norm: 3298.805.. Val Linf norm: 75.785\n",
            "Epoch 114/138.. Train loss: 59.338.. Val loss: 53.912.. Train L1 norm: 4.196.. Val L1 norm: 1.103.. Train Linf norm: 3246.238.. Val Linf norm: 75.836\n",
            "Epoch 115/138.. Train loss: 59.217.. Val loss: 53.910.. Train L1 norm: 4.213.. Val L1 norm: 1.103.. Train Linf norm: 3260.766.. Val Linf norm: 75.887\n",
            "Epoch 116/138.. Train loss: 59.103.. Val loss: 53.910.. Train L1 norm: 4.157.. Val L1 norm: 1.103.. Train Linf norm: 3205.821.. Val Linf norm: 75.891\n",
            "Epoch 117/138.. Train loss: 59.155.. Val loss: 53.909.. Train L1 norm: 4.249.. Val L1 norm: 1.103.. Train Linf norm: 3300.687.. Val Linf norm: 75.927\n",
            "Epoch 118/138.. Train loss: 59.087.. Val loss: 53.909.. Train L1 norm: 4.218.. Val L1 norm: 1.103.. Train Linf norm: 3266.790.. Val Linf norm: 75.935\n",
            "Epoch 119/138.. Train loss: 59.091.. Val loss: 53.908.. Train L1 norm: 4.224.. Val L1 norm: 1.103.. Train Linf norm: 3275.043.. Val Linf norm: 75.944\n",
            "Epoch 120/138.. Train loss: 59.111.. Val loss: 53.908.. Train L1 norm: 4.032.. Val L1 norm: 1.103.. Train Linf norm: 3077.032.. Val Linf norm: 75.965\n",
            "Epoch 121/138.. Train loss: 59.089.. Val loss: 53.907.. Train L1 norm: 4.173.. Val L1 norm: 1.103.. Train Linf norm: 3221.319.. Val Linf norm: 75.973\n",
            "Epoch 122/138.. Train loss: 59.142.. Val loss: 53.908.. Train L1 norm: 4.278.. Val L1 norm: 1.103.. Train Linf norm: 3330.474.. Val Linf norm: 75.964\n",
            "Epoch 123/138.. Train loss: 59.127.. Val loss: 53.907.. Train L1 norm: 4.215.. Val L1 norm: 1.103.. Train Linf norm: 3264.880.. Val Linf norm: 75.994\n",
            "Epoch 124/138.. Train loss: 59.096.. Val loss: 53.906.. Train L1 norm: 4.237.. Val L1 norm: 1.103.. Train Linf norm: 3285.496.. Val Linf norm: 76.012\n",
            "Epoch 125/138.. Train loss: 59.082.. Val loss: 53.906.. Train L1 norm: 4.298.. Val L1 norm: 1.103.. Train Linf norm: 3349.934.. Val Linf norm: 76.019\n",
            "Epoch 126/138.. Train loss: 59.162.. Val loss: 53.904.. Train L1 norm: 4.246.. Val L1 norm: 1.103.. Train Linf norm: 3296.654.. Val Linf norm: 76.064\n",
            "Epoch 127/138.. Train loss: 59.373.. Val loss: 53.901.. Train L1 norm: 4.165.. Val L1 norm: 1.103.. Train Linf norm: 3212.199.. Val Linf norm: 76.143\n",
            "Epoch 128/138.. Train loss: 59.115.. Val loss: 53.902.. Train L1 norm: 4.245.. Val L1 norm: 1.103.. Train Linf norm: 3297.383.. Val Linf norm: 76.144\n",
            "Epoch 129/138.. Train loss: 59.072.. Val loss: 53.901.. Train L1 norm: 4.129.. Val L1 norm: 1.103.. Train Linf norm: 3176.737.. Val Linf norm: 76.160\n",
            "Epoch 130/138.. Train loss: 59.080.. Val loss: 53.901.. Train L1 norm: 4.229.. Val L1 norm: 1.103.. Train Linf norm: 3270.341.. Val Linf norm: 76.173\n",
            "Epoch 131/138.. Train loss: 59.087.. Val loss: 53.901.. Train L1 norm: 4.233.. Val L1 norm: 1.103.. Train Linf norm: 3283.168.. Val Linf norm: 76.178\n",
            "Epoch 132/138.. Train loss: 59.089.. Val loss: 53.900.. Train L1 norm: 4.238.. Val L1 norm: 1.103.. Train Linf norm: 3285.890.. Val Linf norm: 76.197\n",
            "Epoch 133/138.. Train loss: 59.078.. Val loss: 53.900.. Train L1 norm: 4.219.. Val L1 norm: 1.103.. Train Linf norm: 3268.142.. Val Linf norm: 76.209\n",
            "Epoch 134/138.. Train loss: 59.086.. Val loss: 53.900.. Train L1 norm: 4.215.. Val L1 norm: 1.103.. Train Linf norm: 3264.657.. Val Linf norm: 76.217\n",
            "Epoch 135/138.. Train loss: 59.126.. Val loss: 53.900.. Train L1 norm: 4.198.. Val L1 norm: 1.103.. Train Linf norm: 3248.439.. Val Linf norm: 76.209\n",
            "Epoch 136/138.. Train loss: 59.074.. Val loss: 53.900.. Train L1 norm: 4.244.. Val L1 norm: 1.103.. Train Linf norm: 3294.641.. Val Linf norm: 76.223\n",
            "Epoch 137/138.. Train loss: 59.071.. Val loss: 53.899.. Train L1 norm: 4.253.. Val L1 norm: 1.103.. Train Linf norm: 3303.038.. Val Linf norm: 76.241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 07:52:49,309]\u001b[0m Trial 53 finished with value: 1.1032618911743164 and parameters: {'n_layers': 7, 'n_units_0': 3820, 'n_units_1': 2516, 'n_units_2': 3213, 'n_units_3': 256, 'n_units_4': 1668, 'n_units_5': 1681, 'n_units_6': 3968, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 2.355415135743467e-06, 'batch_size': 1024, 'n_epochs': 138, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.22358239637671426, 'dropout_rate': 0.0007411445484086232, 'weight_decay': 0.00021408957629636725, 'beta1': 0.9161929206887539, 'beta2': 0.9990082773529612, 'factor': 0.17943631301356686, 'patience': 5, 'threshold': 0.0011445872221554699}. Best is trial 22 with value: 1.0150012904485066.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 138/138.. Train loss: 59.103.. Val loss: 53.900.. Train L1 norm: 4.207.. Val L1 norm: 1.103.. Train Linf norm: 3256.327.. Val Linf norm: 76.234\n",
            "Epoch 1/134.. Train loss: 941.932.. Val loss: 53.778.. Train L1 norm: 2.747.. Val L1 norm: 1.045.. Train Linf norm: 1778.475.. Val Linf norm: 34.791\n",
            "Epoch 2/134.. Train loss: 194.621.. Val loss: 54.606.. Train L1 norm: 2.972.. Val L1 norm: 1.017.. Train Linf norm: 2012.688.. Val Linf norm: 14.784\n",
            "Epoch 3/134.. Train loss: 147.663.. Val loss: 53.909.. Train L1 norm: 2.465.. Val L1 norm: 1.044.. Train Linf norm: 1498.117.. Val Linf norm: 35.180\n",
            "Epoch 4/134.. Train loss: 151.968.. Val loss: 54.749.. Train L1 norm: 1.930.. Val L1 norm: 1.016.. Train Linf norm: 949.220.. Val Linf norm: 14.302\n",
            "Epoch 5/134.. Train loss: 162.416.. Val loss: 53.698.. Train L1 norm: 2.013.. Val L1 norm: 1.058.. Train Linf norm: 1030.341.. Val Linf norm: 44.075\n",
            "Epoch 6/134.. Train loss: 205.188.. Val loss: 54.800.. Train L1 norm: 3.021.. Val L1 norm: 1.018.. Train Linf norm: 2058.738.. Val Linf norm: 16.026\n",
            "Epoch 7/134.. Train loss: 132.916.. Val loss: 54.027.. Train L1 norm: 2.273.. Val L1 norm: 1.048.. Train Linf norm: 1296.800.. Val Linf norm: 37.441\n",
            "Epoch 8/134.. Train loss: 93.031.. Val loss: 54.705.. Train L1 norm: 2.640.. Val L1 norm: 1.025.. Train Linf norm: 1673.518.. Val Linf norm: 21.308\n",
            "Epoch 9/134.. Train loss: 190.906.. Val loss: 53.627.. Train L1 norm: 2.192.. Val L1 norm: 1.072.. Train Linf norm: 1214.110.. Val Linf norm: 53.683\n",
            "Epoch 10/134.. Train loss: 255.101.. Val loss: 54.970.. Train L1 norm: 2.127.. Val L1 norm: 1.023.. Train Linf norm: 1140.939.. Val Linf norm: 21.223\n",
            "Epoch 11/134.. Train loss: 119.185.. Val loss: 54.181.. Train L1 norm: 2.592.. Val L1 norm: 1.053.. Train Linf norm: 1626.550.. Val Linf norm: 42.637\n",
            "Epoch 12/134.. Train loss: 103.773.. Val loss: 54.791.. Train L1 norm: 2.902.. Val L1 norm: 1.033.. Train Linf norm: 1938.503.. Val Linf norm: 28.744\n",
            "Epoch 13/134.. Train loss: 130.886.. Val loss: 53.876.. Train L1 norm: 2.212.. Val L1 norm: 1.069.. Train Linf norm: 1230.921.. Val Linf norm: 53.257\n",
            "Epoch 14/134.. Train loss: 110.160.. Val loss: 54.379.. Train L1 norm: 3.221.. Val L1 norm: 1.050.. Train Linf norm: 2260.927.. Val Linf norm: 40.696\n",
            "Epoch 15/134.. Train loss: 74.037.. Val loss: 54.095.. Train L1 norm: 2.683.. Val L1 norm: 1.063.. Train Linf norm: 1713.725.. Val Linf norm: 49.495\n",
            "Epoch 16/134.. Train loss: 68.533.. Val loss: 54.138.. Train L1 norm: 3.021.. Val L1 norm: 1.061.. Train Linf norm: 2055.812.. Val Linf norm: 48.625\n",
            "Epoch 17/134.. Train loss: 68.604.. Val loss: 54.203.. Train L1 norm: 2.986.. Val L1 norm: 1.059.. Train Linf norm: 2021.598.. Val Linf norm: 47.089\n",
            "Epoch 18/134.. Train loss: 63.487.. Val loss: 54.237.. Train L1 norm: 2.988.. Val L1 norm: 1.058.. Train Linf norm: 2022.139.. Val Linf norm: 46.472\n",
            "Epoch 19/134.. Train loss: 62.664.. Val loss: 54.273.. Train L1 norm: 2.875.. Val L1 norm: 1.057.. Train Linf norm: 1908.875.. Val Linf norm: 45.792\n",
            "Epoch 20/134.. Train loss: 61.629.. Val loss: 54.231.. Train L1 norm: 3.005.. Val L1 norm: 1.059.. Train Linf norm: 2040.866.. Val Linf norm: 47.231\n",
            "Epoch 21/134.. Train loss: 59.597.. Val loss: 54.224.. Train L1 norm: 3.114.. Val L1 norm: 1.060.. Train Linf norm: 2151.342.. Val Linf norm: 47.719\n",
            "Epoch 22/134.. Train loss: 60.233.. Val loss: 54.219.. Train L1 norm: 3.094.. Val L1 norm: 1.060.. Train Linf norm: 2126.807.. Val Linf norm: 47.914\n",
            "Epoch 23/134.. Train loss: 59.561.. Val loss: 54.217.. Train L1 norm: 2.901.. Val L1 norm: 1.060.. Train Linf norm: 1934.021.. Val Linf norm: 48.022\n",
            "Epoch 24/134.. Train loss: 59.592.. Val loss: 54.215.. Train L1 norm: 3.132.. Val L1 norm: 1.060.. Train Linf norm: 2172.074.. Val Linf norm: 48.128\n",
            "Epoch 25/134.. Train loss: 60.081.. Val loss: 54.213.. Train L1 norm: 2.678.. Val L1 norm: 1.061.. Train Linf norm: 1704.623.. Val Linf norm: 48.249\n",
            "Epoch 26/134.. Train loss: 59.544.. Val loss: 54.208.. Train L1 norm: 2.836.. Val L1 norm: 1.061.. Train Linf norm: 1865.923.. Val Linf norm: 48.435\n",
            "Epoch 27/134.. Train loss: 59.739.. Val loss: 54.204.. Train L1 norm: 2.841.. Val L1 norm: 1.061.. Train Linf norm: 1870.813.. Val Linf norm: 48.602\n",
            "Epoch 28/134.. Train loss: 64.647.. Val loss: 54.207.. Train L1 norm: 2.889.. Val L1 norm: 1.061.. Train Linf norm: 1920.846.. Val Linf norm: 48.548\n",
            "Epoch 29/134.. Train loss: 71.715.. Val loss: 54.203.. Train L1 norm: 2.894.. Val L1 norm: 1.061.. Train Linf norm: 1926.362.. Val Linf norm: 48.644\n",
            "Epoch 30/134.. Train loss: 64.747.. Val loss: 54.202.. Train L1 norm: 3.002.. Val L1 norm: 1.061.. Train Linf norm: 2036.262.. Val Linf norm: 48.699\n",
            "Epoch 31/134.. Train loss: 59.833.. Val loss: 54.194.. Train L1 norm: 2.936.. Val L1 norm: 1.062.. Train Linf norm: 1969.171.. Val Linf norm: 48.905\n",
            "Epoch 32/134.. Train loss: 60.006.. Val loss: 54.195.. Train L1 norm: 3.124.. Val L1 norm: 1.062.. Train Linf norm: 2162.017.. Val Linf norm: 48.913\n",
            "Epoch 33/134.. Train loss: 60.089.. Val loss: 54.194.. Train L1 norm: 2.904.. Val L1 norm: 1.062.. Train Linf norm: 1936.464.. Val Linf norm: 48.938\n",
            "Epoch 34/134.. Train loss: 59.532.. Val loss: 54.194.. Train L1 norm: 2.927.. Val L1 norm: 1.062.. Train Linf norm: 1960.605.. Val Linf norm: 48.956\n",
            "Epoch 35/134.. Train loss: 59.530.. Val loss: 54.193.. Train L1 norm: 2.885.. Val L1 norm: 1.062.. Train Linf norm: 1914.072.. Val Linf norm: 48.974\n",
            "Epoch 36/134.. Train loss: 59.560.. Val loss: 54.193.. Train L1 norm: 2.909.. Val L1 norm: 1.062.. Train Linf norm: 1942.683.. Val Linf norm: 48.993\n",
            "Epoch 37/134.. Train loss: 59.869.. Val loss: 54.192.. Train L1 norm: 3.069.. Val L1 norm: 1.062.. Train Linf norm: 2104.877.. Val Linf norm: 49.039\n",
            "Epoch 38/134.. Train loss: 59.534.. Val loss: 54.191.. Train L1 norm: 2.878.. Val L1 norm: 1.062.. Train Linf norm: 1910.280.. Val Linf norm: 49.057\n",
            "Epoch 39/134.. Train loss: 60.371.. Val loss: 54.192.. Train L1 norm: 2.962.. Val L1 norm: 1.062.. Train Linf norm: 1995.404.. Val Linf norm: 49.040\n",
            "Epoch 40/134.. Train loss: 59.552.. Val loss: 54.192.. Train L1 norm: 2.869.. Val L1 norm: 1.062.. Train Linf norm: 1900.653.. Val Linf norm: 49.059\n",
            "Epoch 41/134.. Train loss: 60.371.. Val loss: 54.193.. Train L1 norm: 2.908.. Val L1 norm: 1.062.. Train Linf norm: 1929.210.. Val Linf norm: 49.039\n",
            "Epoch 42/134.. Train loss: 60.510.. Val loss: 54.192.. Train L1 norm: 3.107.. Val L1 norm: 1.062.. Train Linf norm: 2144.280.. Val Linf norm: 49.090\n",
            "Epoch 43/134.. Train loss: 61.674.. Val loss: 54.193.. Train L1 norm: 2.861.. Val L1 norm: 1.062.. Train Linf norm: 1892.071.. Val Linf norm: 49.076\n",
            "Epoch 44/134.. Train loss: 59.559.. Val loss: 54.193.. Train L1 norm: 2.881.. Val L1 norm: 1.062.. Train Linf norm: 1914.081.. Val Linf norm: 49.072\n",
            "Epoch 45/134.. Train loss: 59.978.. Val loss: 54.194.. Train L1 norm: 3.147.. Val L1 norm: 1.062.. Train Linf norm: 2185.824.. Val Linf norm: 49.058\n",
            "Epoch 46/134.. Train loss: 62.258.. Val loss: 54.191.. Train L1 norm: 2.972.. Val L1 norm: 1.062.. Train Linf norm: 2005.859.. Val Linf norm: 49.135\n",
            "Epoch 47/134.. Train loss: 63.968.. Val loss: 54.187.. Train L1 norm: 3.123.. Val L1 norm: 1.062.. Train Linf norm: 2161.328.. Val Linf norm: 49.258\n",
            "Epoch 48/134.. Train loss: 61.176.. Val loss: 54.183.. Train L1 norm: 2.833.. Val L1 norm: 1.062.. Train Linf norm: 1864.400.. Val Linf norm: 49.383\n",
            "Epoch 49/134.. Train loss: 60.157.. Val loss: 54.182.. Train L1 norm: 2.849.. Val L1 norm: 1.062.. Train Linf norm: 1879.648.. Val Linf norm: 49.447\n",
            "Epoch 50/134.. Train loss: 60.980.. Val loss: 54.179.. Train L1 norm: 2.629.. Val L1 norm: 1.063.. Train Linf norm: 1654.306.. Val Linf norm: 49.534\n",
            "Epoch 51/134.. Train loss: 59.803.. Val loss: 54.177.. Train L1 norm: 2.992.. Val L1 norm: 1.063.. Train Linf norm: 2026.229.. Val Linf norm: 49.583\n",
            "Epoch 52/134.. Train loss: 63.132.. Val loss: 54.181.. Train L1 norm: 2.908.. Val L1 norm: 1.062.. Train Linf norm: 1940.396.. Val Linf norm: 49.485\n",
            "Epoch 53/134.. Train loss: 59.533.. Val loss: 54.181.. Train L1 norm: 2.783.. Val L1 norm: 1.062.. Train Linf norm: 1804.127.. Val Linf norm: 49.504\n",
            "Epoch 54/134.. Train loss: 59.704.. Val loss: 54.181.. Train L1 norm: 3.068.. Val L1 norm: 1.062.. Train Linf norm: 2104.984.. Val Linf norm: 49.506\n",
            "Epoch 55/134.. Train loss: 65.741.. Val loss: 54.176.. Train L1 norm: 2.979.. Val L1 norm: 1.063.. Train Linf norm: 2012.944.. Val Linf norm: 49.650\n",
            "Epoch 56/134.. Train loss: 62.965.. Val loss: 54.171.. Train L1 norm: 3.093.. Val L1 norm: 1.063.. Train Linf norm: 2130.170.. Val Linf norm: 49.806\n",
            "Epoch 57/134.. Train loss: 59.547.. Val loss: 54.171.. Train L1 norm: 2.706.. Val L1 norm: 1.063.. Train Linf norm: 1733.277.. Val Linf norm: 49.817\n",
            "Epoch 58/134.. Train loss: 59.605.. Val loss: 54.170.. Train L1 norm: 2.784.. Val L1 norm: 1.063.. Train Linf norm: 1813.117.. Val Linf norm: 49.847\n",
            "Epoch 59/134.. Train loss: 64.635.. Val loss: 54.175.. Train L1 norm: 3.095.. Val L1 norm: 1.063.. Train Linf norm: 2130.727.. Val Linf norm: 49.723\n",
            "Epoch 60/134.. Train loss: 61.966.. Val loss: 54.176.. Train L1 norm: 2.797.. Val L1 norm: 1.063.. Train Linf norm: 1825.590.. Val Linf norm: 49.708\n",
            "Epoch 61/134.. Train loss: 71.833.. Val loss: 54.169.. Train L1 norm: 2.930.. Val L1 norm: 1.063.. Train Linf norm: 1962.363.. Val Linf norm: 49.892\n",
            "Epoch 62/134.. Train loss: 59.525.. Val loss: 54.169.. Train L1 norm: 3.008.. Val L1 norm: 1.063.. Train Linf norm: 2043.488.. Val Linf norm: 49.917\n",
            "Epoch 63/134.. Train loss: 60.667.. Val loss: 54.166.. Train L1 norm: 2.966.. Val L1 norm: 1.063.. Train Linf norm: 1999.694.. Val Linf norm: 50.006\n",
            "Epoch 64/134.. Train loss: 60.365.. Val loss: 54.168.. Train L1 norm: 2.701.. Val L1 norm: 1.063.. Train Linf norm: 1727.994.. Val Linf norm: 49.965\n",
            "Epoch 65/134.. Train loss: 61.338.. Val loss: 54.172.. Train L1 norm: 3.085.. Val L1 norm: 1.063.. Train Linf norm: 2122.537.. Val Linf norm: 49.875\n",
            "Epoch 66/134.. Train loss: 62.403.. Val loss: 54.167.. Train L1 norm: 2.979.. Val L1 norm: 1.063.. Train Linf norm: 2013.235.. Val Linf norm: 50.026\n",
            "Epoch 67/134.. Train loss: 63.226.. Val loss: 54.172.. Train L1 norm: 2.988.. Val L1 norm: 1.063.. Train Linf norm: 2023.089.. Val Linf norm: 49.895\n",
            "Epoch 68/134.. Train loss: 60.293.. Val loss: 54.171.. Train L1 norm: 2.927.. Val L1 norm: 1.063.. Train Linf norm: 1960.188.. Val Linf norm: 49.934\n",
            "Epoch 69/134.. Train loss: 59.510.. Val loss: 54.169.. Train L1 norm: 2.913.. Val L1 norm: 1.063.. Train Linf norm: 1946.392.. Val Linf norm: 50.007\n",
            "Epoch 70/134.. Train loss: 62.195.. Val loss: 54.164.. Train L1 norm: 2.956.. Val L1 norm: 1.063.. Train Linf norm: 1982.959.. Val Linf norm: 50.153\n",
            "Epoch 71/134.. Train loss: 59.594.. Val loss: 54.163.. Train L1 norm: 3.044.. Val L1 norm: 1.063.. Train Linf norm: 777.100.. Val Linf norm: 50.175\n",
            "Epoch 72/134.. Train loss: 62.194.. Val loss: 54.158.. Train L1 norm: 2.999.. Val L1 norm: 1.064.. Train Linf norm: 2032.515.. Val Linf norm: 50.335\n",
            "Epoch 73/134.. Train loss: 61.122.. Val loss: 54.161.. Train L1 norm: 3.131.. Val L1 norm: 1.064.. Train Linf norm: 2167.548.. Val Linf norm: 50.262\n",
            "Epoch 74/134.. Train loss: 60.848.. Val loss: 54.165.. Train L1 norm: 3.109.. Val L1 norm: 1.063.. Train Linf norm: 2140.875.. Val Linf norm: 50.174\n",
            "Epoch 75/134.. Train loss: 59.462.. Val loss: 54.165.. Train L1 norm: 3.143.. Val L1 norm: 1.063.. Train Linf norm: 2180.618.. Val Linf norm: 50.200\n",
            "Epoch 76/134.. Train loss: 68.387.. Val loss: 54.154.. Train L1 norm: 3.309.. Val L1 norm: 1.064.. Train Linf norm: 2351.265.. Val Linf norm: 50.492\n",
            "Epoch 77/134.. Train loss: 65.171.. Val loss: 54.160.. Train L1 norm: 2.938.. Val L1 norm: 1.064.. Train Linf norm: 1970.975.. Val Linf norm: 50.328\n",
            "Epoch 78/134.. Train loss: 60.403.. Val loss: 54.162.. Train L1 norm: 2.855.. Val L1 norm: 1.064.. Train Linf norm: 1884.655.. Val Linf norm: 50.294\n",
            "Epoch 79/134.. Train loss: 59.884.. Val loss: 54.165.. Train L1 norm: 2.937.. Val L1 norm: 1.064.. Train Linf norm: 1971.303.. Val Linf norm: 50.224\n",
            "Epoch 80/134.. Train loss: 59.951.. Val loss: 54.164.. Train L1 norm: 3.103.. Val L1 norm: 1.064.. Train Linf norm: 2139.907.. Val Linf norm: 50.258\n",
            "Epoch 81/134.. Train loss: 64.848.. Val loss: 54.168.. Train L1 norm: 3.199.. Val L1 norm: 1.063.. Train Linf norm: 2237.833.. Val Linf norm: 50.182\n",
            "Epoch 82/134.. Train loss: 59.692.. Val loss: 54.171.. Train L1 norm: 2.763.. Val L1 norm: 1.063.. Train Linf norm: 1791.756.. Val Linf norm: 50.089\n",
            "Epoch 83/134.. Train loss: 61.878.. Val loss: 54.165.. Train L1 norm: 2.835.. Val L1 norm: 1.064.. Train Linf norm: 1866.278.. Val Linf norm: 50.271\n",
            "Epoch 84/134.. Train loss: 59.762.. Val loss: 54.166.. Train L1 norm: 2.655.. Val L1 norm: 1.064.. Train Linf norm: 1681.794.. Val Linf norm: 50.253\n",
            "Epoch 85/134.. Train loss: 59.519.. Val loss: 54.166.. Train L1 norm: 2.919.. Val L1 norm: 1.064.. Train Linf norm: 1950.076.. Val Linf norm: 50.278\n",
            "Epoch 86/134.. Train loss: 63.094.. Val loss: 54.173.. Train L1 norm: 2.949.. Val L1 norm: 1.063.. Train Linf norm: 1981.884.. Val Linf norm: 50.102\n",
            "Epoch 87/134.. Train loss: 60.004.. Val loss: 54.170.. Train L1 norm: 3.178.. Val L1 norm: 1.063.. Train Linf norm: 2216.267.. Val Linf norm: 50.200\n",
            "Epoch 88/134.. Train loss: 62.662.. Val loss: 54.176.. Train L1 norm: 3.001.. Val L1 norm: 1.063.. Train Linf norm: 2035.073.. Val Linf norm: 50.029\n",
            "Epoch 89/134.. Train loss: 62.972.. Val loss: 54.173.. Train L1 norm: 2.786.. Val L1 norm: 1.063.. Train Linf norm: 1815.518.. Val Linf norm: 50.129\n",
            "Epoch 90/134.. Train loss: 59.893.. Val loss: 54.165.. Train L1 norm: 2.771.. Val L1 norm: 1.064.. Train Linf norm: 1799.571.. Val Linf norm: 50.358\n",
            "Epoch 91/134.. Train loss: 60.057.. Val loss: 54.161.. Train L1 norm: 3.040.. Val L1 norm: 1.064.. Train Linf norm: 2074.295.. Val Linf norm: 50.480\n",
            "Epoch 92/134.. Train loss: 65.473.. Val loss: 54.169.. Train L1 norm: 2.935.. Val L1 norm: 1.064.. Train Linf norm: 1967.531.. Val Linf norm: 50.267\n",
            "Epoch 93/134.. Train loss: 64.577.. Val loss: 54.178.. Train L1 norm: 2.733.. Val L1 norm: 1.063.. Train Linf norm: 1762.006.. Val Linf norm: 50.042\n",
            "Epoch 94/134.. Train loss: 62.325.. Val loss: 54.173.. Train L1 norm: 3.267.. Val L1 norm: 1.063.. Train Linf norm: 2309.491.. Val Linf norm: 50.195\n",
            "Epoch 95/134.. Train loss: 59.971.. Val loss: 54.168.. Train L1 norm: 3.085.. Val L1 norm: 1.064.. Train Linf norm: 2121.731.. Val Linf norm: 50.342\n",
            "Epoch 96/134.. Train loss: 61.922.. Val loss: 54.162.. Train L1 norm: 3.075.. Val L1 norm: 1.064.. Train Linf norm: 2110.264.. Val Linf norm: 50.523\n",
            "Epoch 97/134.. Train loss: 59.601.. Val loss: 54.160.. Train L1 norm: 3.026.. Val L1 norm: 1.064.. Train Linf norm: 2057.249.. Val Linf norm: 50.588\n",
            "Epoch 98/134.. Train loss: 59.512.. Val loss: 54.159.. Train L1 norm: 3.071.. Val L1 norm: 1.064.. Train Linf norm: 2106.276.. Val Linf norm: 50.621\n",
            "Epoch 99/134.. Train loss: 60.998.. Val loss: 54.153.. Train L1 norm: 3.021.. Val L1 norm: 1.064.. Train Linf norm: 2056.044.. Val Linf norm: 50.801\n",
            "Epoch 100/134.. Train loss: 61.035.. Val loss: 54.147.. Train L1 norm: 3.090.. Val L1 norm: 1.065.. Train Linf norm: 2126.040.. Val Linf norm: 50.977\n",
            "Epoch 101/134.. Train loss: 64.827.. Val loss: 54.137.. Train L1 norm: 3.180.. Val L1 norm: 1.065.. Train Linf norm: 2217.222.. Val Linf norm: 51.264\n",
            "Epoch 102/134.. Train loss: 70.619.. Val loss: 54.126.. Train L1 norm: 2.808.. Val L1 norm: 1.065.. Train Linf norm: 1823.616.. Val Linf norm: 51.588\n",
            "Epoch 103/134.. Train loss: 74.172.. Val loss: 54.136.. Train L1 norm: 3.115.. Val L1 norm: 1.065.. Train Linf norm: 2151.158.. Val Linf norm: 51.316\n",
            "Epoch 104/134.. Train loss: 61.461.. Val loss: 54.143.. Train L1 norm: 3.082.. Val L1 norm: 1.065.. Train Linf norm: 2118.597.. Val Linf norm: 51.154\n",
            "Epoch 105/134.. Train loss: 59.479.. Val loss: 54.142.. Train L1 norm: 2.953.. Val L1 norm: 1.065.. Train Linf norm: 1984.622.. Val Linf norm: 51.181\n",
            "Epoch 106/134.. Train loss: 59.559.. Val loss: 54.142.. Train L1 norm: 3.239.. Val L1 norm: 1.065.. Train Linf norm: 2279.184.. Val Linf norm: 51.187\n",
            "Epoch 107/134.. Train loss: 60.023.. Val loss: 54.145.. Train L1 norm: 3.174.. Val L1 norm: 1.065.. Train Linf norm: 2212.045.. Val Linf norm: 51.138\n",
            "Epoch 108/134.. Train loss: 62.239.. Val loss: 54.150.. Train L1 norm: 2.907.. Val L1 norm: 1.065.. Train Linf norm: 1934.609.. Val Linf norm: 51.021\n",
            "Epoch 109/134.. Train loss: 60.269.. Val loss: 54.153.. Train L1 norm: 3.110.. Val L1 norm: 1.065.. Train Linf norm: 2146.847.. Val Linf norm: 50.958\n",
            "Epoch 110/134.. Train loss: 61.467.. Val loss: 54.147.. Train L1 norm: 2.780.. Val L1 norm: 1.065.. Train Linf norm: 1807.913.. Val Linf norm: 51.112\n",
            "Epoch 111/134.. Train loss: 61.827.. Val loss: 54.153.. Train L1 norm: 3.131.. Val L1 norm: 1.065.. Train Linf norm: 2167.961.. Val Linf norm: 50.979\n",
            "Epoch 112/134.. Train loss: 62.458.. Val loss: 54.159.. Train L1 norm: 2.967.. Val L1 norm: 1.064.. Train Linf norm: 2001.023.. Val Linf norm: 50.830\n",
            "Epoch 113/134.. Train loss: 59.566.. Val loss: 54.160.. Train L1 norm: 3.030.. Val L1 norm: 1.064.. Train Linf norm: 2066.081.. Val Linf norm: 50.819\n",
            "Epoch 114/134.. Train loss: 59.674.. Val loss: 54.161.. Train L1 norm: 3.031.. Val L1 norm: 1.064.. Train Linf norm: 2065.036.. Val Linf norm: 50.804\n",
            "Epoch 115/134.. Train loss: 60.124.. Val loss: 54.163.. Train L1 norm: 2.895.. Val L1 norm: 1.064.. Train Linf norm: 1927.253.. Val Linf norm: 50.747\n",
            "Epoch 116/134.. Train loss: 60.701.. Val loss: 54.165.. Train L1 norm: 2.711.. Val L1 norm: 1.064.. Train Linf norm: 1738.907.. Val Linf norm: 50.726\n",
            "Epoch 117/134.. Train loss: 60.602.. Val loss: 54.163.. Train L1 norm: 3.104.. Val L1 norm: 1.064.. Train Linf norm: 2140.607.. Val Linf norm: 50.774\n",
            "Epoch 118/134.. Train loss: 59.924.. Val loss: 54.159.. Train L1 norm: 3.091.. Val L1 norm: 1.064.. Train Linf norm: 2127.129.. Val Linf norm: 50.887\n",
            "Epoch 119/134.. Train loss: 59.683.. Val loss: 54.158.. Train L1 norm: 2.941.. Val L1 norm: 1.064.. Train Linf norm: 1974.615.. Val Linf norm: 50.933\n",
            "Epoch 120/134.. Train loss: 63.350.. Val loss: 54.165.. Train L1 norm: 2.940.. Val L1 norm: 1.064.. Train Linf norm: 1966.651.. Val Linf norm: 50.758\n",
            "Epoch 121/134.. Train loss: 72.236.. Val loss: 54.152.. Train L1 norm: 3.054.. Val L1 norm: 1.065.. Train Linf norm: 2088.342.. Val Linf norm: 51.134\n",
            "Epoch 122/134.. Train loss: 59.990.. Val loss: 54.154.. Train L1 norm: 3.102.. Val L1 norm: 1.065.. Train Linf norm: 2122.808.. Val Linf norm: 51.090\n",
            "Epoch 123/134.. Train loss: 60.182.. Val loss: 54.153.. Train L1 norm: 2.977.. Val L1 norm: 1.065.. Train Linf norm: 2009.719.. Val Linf norm: 51.137\n",
            "Epoch 124/134.. Train loss: 59.685.. Val loss: 54.150.. Train L1 norm: 3.065.. Val L1 norm: 1.065.. Train Linf norm: 2100.327.. Val Linf norm: 51.222\n",
            "Epoch 125/134.. Train loss: 60.728.. Val loss: 54.153.. Train L1 norm: 3.155.. Val L1 norm: 1.065.. Train Linf norm: 2192.615.. Val Linf norm: 51.152\n",
            "Epoch 126/134.. Train loss: 60.077.. Val loss: 54.151.. Train L1 norm: 3.221.. Val L1 norm: 1.065.. Train Linf norm: 2260.997.. Val Linf norm: 51.219\n",
            "Epoch 127/134.. Train loss: 63.989.. Val loss: 54.145.. Train L1 norm: 2.678.. Val L1 norm: 1.065.. Train Linf norm: 1703.075.. Val Linf norm: 51.388\n",
            "Epoch 128/134.. Train loss: 59.877.. Val loss: 54.138.. Train L1 norm: 2.939.. Val L1 norm: 1.065.. Train Linf norm: 1969.973.. Val Linf norm: 51.613\n",
            "Epoch 129/134.. Train loss: 62.886.. Val loss: 54.144.. Train L1 norm: 3.121.. Val L1 norm: 1.065.. Train Linf norm: 2159.634.. Val Linf norm: 51.453\n",
            "Epoch 130/134.. Train loss: 59.505.. Val loss: 54.143.. Train L1 norm: 3.212.. Val L1 norm: 1.065.. Train Linf norm: 2249.711.. Val Linf norm: 51.473\n",
            "Epoch 131/134.. Train loss: 63.072.. Val loss: 54.151.. Train L1 norm: 2.964.. Val L1 norm: 1.065.. Train Linf norm: 1990.804.. Val Linf norm: 51.277\n",
            "Epoch 132/134.. Train loss: 59.700.. Val loss: 54.148.. Train L1 norm: 2.785.. Val L1 norm: 1.065.. Train Linf norm: 1813.729.. Val Linf norm: 51.360\n",
            "Epoch 133/134.. Train loss: 63.386.. Val loss: 54.140.. Train L1 norm: 3.089.. Val L1 norm: 1.065.. Train Linf norm: 2125.015.. Val Linf norm: 51.608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 08:03:02,314]\u001b[0m Trial 54 finished with value: 1.0651369391123453 and parameters: {'n_layers': 7, 'n_units_0': 3758, 'n_units_1': 2233, 'n_units_2': 3158, 'n_units_3': 341, 'n_units_4': 1669, 'n_units_5': 1677, 'n_units_6': 3599, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 1.959813886855388e-06, 'batch_size': 1024, 'n_epochs': 134, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.21559918916892523, 'dropout_rate': 0.013776592130666936, 'weight_decay': 0.0004422528144714954, 'beta1': 0.9110499759413703, 'beta2': 0.9990063712163181, 'factor': 0.1826825688459291, 'patience': 5, 'threshold': 0.0010529772610224418}. Best is trial 22 with value: 1.0150012904485066.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 134/134.. Train loss: 66.884.. Val loss: 54.148.. Train L1 norm: 3.112.. Val L1 norm: 1.065.. Train Linf norm: 2148.757.. Val Linf norm: 51.398\n",
            "Epoch 1/145.. Train loss: 798.321.. Val loss: 54.403.. Train L1 norm: 5.868.. Val L1 norm: 1.023.. Train Linf norm: 4952.726.. Val Linf norm: 21.163\n",
            "Epoch 2/145.. Train loss: 60.775.. Val loss: 54.714.. Train L1 norm: 2.246.. Val L1 norm: 1.022.. Train Linf norm: 1273.405.. Val Linf norm: 21.636\n",
            "Epoch 3/145.. Train loss: 63.702.. Val loss: 54.414.. Train L1 norm: 2.217.. Val L1 norm: 1.027.. Train Linf norm: 1245.483.. Val Linf norm: 23.777\n",
            "Epoch 4/145.. Train loss: 68.957.. Val loss: 54.737.. Train L1 norm: 2.377.. Val L1 norm: 1.025.. Train Linf norm: 1406.966.. Val Linf norm: 23.084\n",
            "Epoch 5/145.. Train loss: 130.206.. Val loss: 53.551.. Train L1 norm: 2.421.. Val L1 norm: 1.067.. Train Linf norm: 1451.782.. Val Linf norm: 51.631\n",
            "Epoch 6/145.. Train loss: 285.448.. Val loss: 54.566.. Train L1 norm: 3.430.. Val L1 norm: 1.028.. Train Linf norm: 2470.449.. Val Linf norm: 25.431\n",
            "Epoch 7/145.. Train loss: 167.906.. Val loss: 54.542.. Train L1 norm: 1.957.. Val L1 norm: 1.031.. Train Linf norm: 977.194.. Val Linf norm: 27.622\n",
            "Epoch 8/145.. Train loss: 129.792.. Val loss: 54.841.. Train L1 norm: 3.042.. Val L1 norm: 1.025.. Train Linf norm: 2087.519.. Val Linf norm: 22.888\n",
            "Epoch 9/145.. Train loss: 131.496.. Val loss: 53.825.. Train L1 norm: 2.302.. Val L1 norm: 1.063.. Train Linf norm: 1325.057.. Val Linf norm: 50.128\n",
            "Epoch 10/145.. Train loss: 155.978.. Val loss: 54.386.. Train L1 norm: 3.344.. Val L1 norm: 1.044.. Train Linf norm: 2385.655.. Val Linf norm: 37.210\n",
            "Epoch 11/145.. Train loss: 104.296.. Val loss: 54.118.. Train L1 norm: 2.814.. Val L1 norm: 1.057.. Train Linf norm: 1846.734.. Val Linf norm: 46.656\n",
            "Epoch 12/145.. Train loss: 84.271.. Val loss: 54.125.. Train L1 norm: 2.874.. Val L1 norm: 1.057.. Train Linf norm: 1907.889.. Val Linf norm: 46.772\n",
            "Epoch 13/145.. Train loss: 62.236.. Val loss: 54.232.. Train L1 norm: 3.184.. Val L1 norm: 1.053.. Train Linf norm: 2227.921.. Val Linf norm: 44.243\n",
            "Epoch 14/145.. Train loss: 74.096.. Val loss: 54.303.. Train L1 norm: 3.042.. Val L1 norm: 1.051.. Train Linf norm: 2081.368.. Val Linf norm: 42.553\n",
            "Epoch 15/145.. Train loss: 59.675.. Val loss: 54.294.. Train L1 norm: 2.868.. Val L1 norm: 1.052.. Train Linf norm: 1903.338.. Val Linf norm: 43.076\n",
            "Epoch 16/145.. Train loss: 59.700.. Val loss: 54.275.. Train L1 norm: 3.065.. Val L1 norm: 1.053.. Train Linf norm: 2106.250.. Val Linf norm: 43.881\n",
            "Epoch 17/145.. Train loss: 61.956.. Val loss: 54.303.. Train L1 norm: 2.942.. Val L1 norm: 1.052.. Train Linf norm: 1980.985.. Val Linf norm: 43.435\n",
            "Epoch 18/145.. Train loss: 59.628.. Val loss: 54.302.. Train L1 norm: 3.012.. Val L1 norm: 1.052.. Train Linf norm: 2051.552.. Val Linf norm: 43.522\n",
            "Epoch 19/145.. Train loss: 59.633.. Val loss: 54.300.. Train L1 norm: 3.056.. Val L1 norm: 1.052.. Train Linf norm: 2096.584.. Val Linf norm: 43.612\n",
            "Epoch 20/145.. Train loss: 59.994.. Val loss: 54.301.. Train L1 norm: 3.160.. Val L1 norm: 1.052.. Train Linf norm: 2197.221.. Val Linf norm: 43.643\n",
            "Epoch 21/145.. Train loss: 59.608.. Val loss: 54.300.. Train L1 norm: 2.867.. Val L1 norm: 1.052.. Train Linf norm: 1903.933.. Val Linf norm: 43.746\n",
            "Epoch 22/145.. Train loss: 59.649.. Val loss: 54.297.. Train L1 norm: 3.119.. Val L1 norm: 1.053.. Train Linf norm: 2161.670.. Val Linf norm: 43.869\n",
            "Epoch 23/145.. Train loss: 59.888.. Val loss: 54.298.. Train L1 norm: 3.008.. Val L1 norm: 1.053.. Train Linf norm: 2035.055.. Val Linf norm: 43.902\n",
            "Epoch 24/145.. Train loss: 60.541.. Val loss: 54.297.. Train L1 norm: 2.920.. Val L1 norm: 1.053.. Train Linf norm: 1956.590.. Val Linf norm: 43.945\n",
            "Epoch 25/145.. Train loss: 60.394.. Val loss: 54.297.. Train L1 norm: 2.928.. Val L1 norm: 1.053.. Train Linf norm: 1960.776.. Val Linf norm: 43.946\n",
            "Epoch 26/145.. Train loss: 59.704.. Val loss: 54.296.. Train L1 norm: 2.911.. Val L1 norm: 1.053.. Train Linf norm: 1947.598.. Val Linf norm: 43.970\n",
            "Epoch 27/145.. Train loss: 59.623.. Val loss: 54.296.. Train L1 norm: 3.135.. Val L1 norm: 1.053.. Train Linf norm: 2174.957.. Val Linf norm: 43.988\n",
            "Epoch 28/145.. Train loss: 59.726.. Val loss: 54.296.. Train L1 norm: 2.911.. Val L1 norm: 1.053.. Train Linf norm: 1948.326.. Val Linf norm: 44.012\n",
            "Epoch 29/145.. Train loss: 60.913.. Val loss: 54.294.. Train L1 norm: 2.876.. Val L1 norm: 1.053.. Train Linf norm: 1911.316.. Val Linf norm: 44.054\n",
            "Epoch 30/145.. Train loss: 59.803.. Val loss: 54.293.. Train L1 norm: 2.733.. Val L1 norm: 1.053.. Train Linf norm: 1765.792.. Val Linf norm: 44.106\n",
            "Epoch 31/145.. Train loss: 59.855.. Val loss: 54.292.. Train L1 norm: 2.991.. Val L1 norm: 1.053.. Train Linf norm: 2030.410.. Val Linf norm: 44.140\n",
            "Epoch 32/145.. Train loss: 59.795.. Val loss: 54.291.. Train L1 norm: 3.157.. Val L1 norm: 1.053.. Train Linf norm: 2200.904.. Val Linf norm: 44.164\n",
            "Epoch 33/145.. Train loss: 62.052.. Val loss: 54.288.. Train L1 norm: 3.167.. Val L1 norm: 1.053.. Train Linf norm: 2210.028.. Val Linf norm: 44.257\n",
            "Epoch 34/145.. Train loss: 60.017.. Val loss: 54.287.. Train L1 norm: 3.092.. Val L1 norm: 1.053.. Train Linf norm: 2131.438.. Val Linf norm: 44.300\n",
            "Epoch 35/145.. Train loss: 60.069.. Val loss: 54.288.. Train L1 norm: 2.923.. Val L1 norm: 1.053.. Train Linf norm: 1960.362.. Val Linf norm: 44.296\n",
            "Epoch 36/145.. Train loss: 64.596.. Val loss: 54.291.. Train L1 norm: 2.880.. Val L1 norm: 1.053.. Train Linf norm: 1916.028.. Val Linf norm: 44.230\n",
            "Epoch 37/145.. Train loss: 60.274.. Val loss: 54.289.. Train L1 norm: 3.057.. Val L1 norm: 1.053.. Train Linf norm: 2097.608.. Val Linf norm: 44.277\n",
            "Epoch 38/145.. Train loss: 59.735.. Val loss: 54.289.. Train L1 norm: 2.969.. Val L1 norm: 1.053.. Train Linf norm: 2007.899.. Val Linf norm: 44.290\n",
            "Epoch 39/145.. Train loss: 60.833.. Val loss: 54.291.. Train L1 norm: 2.859.. Val L1 norm: 1.053.. Train Linf norm: 1895.301.. Val Linf norm: 44.263\n",
            "Epoch 40/145.. Train loss: 59.817.. Val loss: 54.291.. Train L1 norm: 2.963.. Val L1 norm: 1.053.. Train Linf norm: 2000.828.. Val Linf norm: 44.275\n",
            "Epoch 41/145.. Train loss: 59.659.. Val loss: 54.290.. Train L1 norm: 3.026.. Val L1 norm: 1.053.. Train Linf norm: 2065.345.. Val Linf norm: 44.301\n",
            "Epoch 42/145.. Train loss: 59.903.. Val loss: 54.289.. Train L1 norm: 3.040.. Val L1 norm: 1.053.. Train Linf norm: 2080.703.. Val Linf norm: 44.347\n",
            "Epoch 43/145.. Train loss: 60.294.. Val loss: 54.287.. Train L1 norm: 3.052.. Val L1 norm: 1.053.. Train Linf norm: 2091.896.. Val Linf norm: 44.413\n",
            "Epoch 44/145.. Train loss: 60.875.. Val loss: 54.284.. Train L1 norm: 3.146.. Val L1 norm: 1.054.. Train Linf norm: 2187.234.. Val Linf norm: 44.504\n",
            "Epoch 45/145.. Train loss: 59.712.. Val loss: 54.283.. Train L1 norm: 3.111.. Val L1 norm: 1.054.. Train Linf norm: 2152.772.. Val Linf norm: 44.518\n",
            "Epoch 46/145.. Train loss: 59.611.. Val loss: 54.283.. Train L1 norm: 2.837.. Val L1 norm: 1.054.. Train Linf norm: 1870.630.. Val Linf norm: 44.538\n",
            "Epoch 47/145.. Train loss: 60.176.. Val loss: 54.281.. Train L1 norm: 2.969.. Val L1 norm: 1.054.. Train Linf norm: 2006.211.. Val Linf norm: 44.608\n",
            "Epoch 48/145.. Train loss: 61.503.. Val loss: 54.284.. Train L1 norm: 2.990.. Val L1 norm: 1.054.. Train Linf norm: 2028.111.. Val Linf norm: 44.547\n",
            "Epoch 49/145.. Train loss: 63.431.. Val loss: 54.288.. Train L1 norm: 2.899.. Val L1 norm: 1.053.. Train Linf norm: 1934.077.. Val Linf norm: 44.445\n",
            "Epoch 50/145.. Train loss: 60.023.. Val loss: 54.289.. Train L1 norm: 3.158.. Val L1 norm: 1.053.. Train Linf norm: 2200.900.. Val Linf norm: 44.424\n",
            "Epoch 51/145.. Train loss: 62.589.. Val loss: 54.285.. Train L1 norm: 2.998.. Val L1 norm: 1.054.. Train Linf norm: 2033.254.. Val Linf norm: 44.550\n",
            "Epoch 52/145.. Train loss: 59.708.. Val loss: 54.283.. Train L1 norm: 2.919.. Val L1 norm: 1.054.. Train Linf norm: 1956.776.. Val Linf norm: 44.611\n",
            "Epoch 53/145.. Train loss: 60.788.. Val loss: 54.279.. Train L1 norm: 3.059.. Val L1 norm: 1.054.. Train Linf norm: 2098.129.. Val Linf norm: 44.738\n",
            "Epoch 54/145.. Train loss: 59.997.. Val loss: 54.278.. Train L1 norm: 2.943.. Val L1 norm: 1.054.. Train Linf norm: 1980.067.. Val Linf norm: 44.783\n",
            "Epoch 55/145.. Train loss: 59.598.. Val loss: 54.276.. Train L1 norm: 3.016.. Val L1 norm: 1.054.. Train Linf norm: 2054.489.. Val Linf norm: 44.842\n",
            "Epoch 56/145.. Train loss: 60.059.. Val loss: 54.273.. Train L1 norm: 3.045.. Val L1 norm: 1.054.. Train Linf norm: 2085.952.. Val Linf norm: 44.918\n",
            "Epoch 57/145.. Train loss: 59.635.. Val loss: 54.273.. Train L1 norm: 3.153.. Val L1 norm: 1.054.. Train Linf norm: 2194.543.. Val Linf norm: 44.937\n",
            "Epoch 58/145.. Train loss: 60.458.. Val loss: 54.269.. Train L1 norm: 3.146.. Val L1 norm: 1.054.. Train Linf norm: 2188.522.. Val Linf norm: 45.046\n",
            "Epoch 59/145.. Train loss: 60.277.. Val loss: 54.266.. Train L1 norm: 2.884.. Val L1 norm: 1.054.. Train Linf norm: 1919.132.. Val Linf norm: 45.149\n",
            "Epoch 60/145.. Train loss: 59.957.. Val loss: 54.263.. Train L1 norm: 2.927.. Val L1 norm: 1.055.. Train Linf norm: 1963.878.. Val Linf norm: 45.236\n",
            "Epoch 61/145.. Train loss: 60.293.. Val loss: 54.262.. Train L1 norm: 2.988.. Val L1 norm: 1.055.. Train Linf norm: 2027.732.. Val Linf norm: 45.291\n",
            "Epoch 62/145.. Train loss: 66.745.. Val loss: 54.266.. Train L1 norm: 3.134.. Val L1 norm: 1.054.. Train Linf norm: 2175.784.. Val Linf norm: 45.174\n",
            "Epoch 63/145.. Train loss: 60.213.. Val loss: 54.264.. Train L1 norm: 3.131.. Val L1 norm: 1.055.. Train Linf norm: 2171.300.. Val Linf norm: 45.251\n",
            "Epoch 64/145.. Train loss: 59.769.. Val loss: 54.264.. Train L1 norm: 3.029.. Val L1 norm: 1.055.. Train Linf norm: 2069.099.. Val Linf norm: 45.257\n",
            "Epoch 65/145.. Train loss: 59.603.. Val loss: 54.263.. Train L1 norm: 3.216.. Val L1 norm: 1.055.. Train Linf norm: 2254.602.. Val Linf norm: 45.296\n",
            "Epoch 66/145.. Train loss: 61.258.. Val loss: 54.258.. Train L1 norm: 3.026.. Val L1 norm: 1.055.. Train Linf norm: 2064.343.. Val Linf norm: 45.463\n",
            "Epoch 67/145.. Train loss: 59.765.. Val loss: 54.255.. Train L1 norm: 3.034.. Val L1 norm: 1.055.. Train Linf norm: 2074.317.. Val Linf norm: 45.540\n",
            "Epoch 68/145.. Train loss: 62.237.. Val loss: 54.260.. Train L1 norm: 3.011.. Val L1 norm: 1.055.. Train Linf norm: 2049.996.. Val Linf norm: 45.417\n",
            "Epoch 69/145.. Train loss: 60.132.. Val loss: 54.257.. Train L1 norm: 2.950.. Val L1 norm: 1.055.. Train Linf norm: 1987.347.. Val Linf norm: 45.519\n",
            "Epoch 70/145.. Train loss: 59.626.. Val loss: 54.256.. Train L1 norm: 2.998.. Val L1 norm: 1.055.. Train Linf norm: 2035.899.. Val Linf norm: 45.546\n",
            "Epoch 71/145.. Train loss: 60.586.. Val loss: 54.259.. Train L1 norm: 2.864.. Val L1 norm: 1.055.. Train Linf norm: 1899.219.. Val Linf norm: 45.474\n",
            "Epoch 72/145.. Train loss: 59.590.. Val loss: 54.259.. Train L1 norm: 3.132.. Val L1 norm: 1.055.. Train Linf norm: 2173.734.. Val Linf norm: 45.499\n",
            "Epoch 73/145.. Train loss: 59.978.. Val loss: 54.260.. Train L1 norm: 3.226.. Val L1 norm: 1.055.. Train Linf norm: 2270.136.. Val Linf norm: 45.472\n",
            "Epoch 74/145.. Train loss: 59.624.. Val loss: 54.259.. Train L1 norm: 2.911.. Val L1 norm: 1.055.. Train Linf norm: 1947.927.. Val Linf norm: 45.514\n",
            "Epoch 75/145.. Train loss: 59.657.. Val loss: 54.259.. Train L1 norm: 2.911.. Val L1 norm: 1.055.. Train Linf norm: 1943.098.. Val Linf norm: 45.520\n",
            "Epoch 76/145.. Train loss: 59.939.. Val loss: 54.261.. Train L1 norm: 2.935.. Val L1 norm: 1.055.. Train Linf norm: 1973.290.. Val Linf norm: 45.480\n",
            "Epoch 77/145.. Train loss: 59.903.. Val loss: 54.258.. Train L1 norm: 3.067.. Val L1 norm: 1.055.. Train Linf norm: 2107.354.. Val Linf norm: 45.568\n",
            "Epoch 78/145.. Train loss: 66.400.. Val loss: 54.263.. Train L1 norm: 3.014.. Val L1 norm: 1.055.. Train Linf norm: 2039.199.. Val Linf norm: 45.471\n",
            "Epoch 79/145.. Train loss: 60.959.. Val loss: 54.272.. Train L1 norm: 3.019.. Val L1 norm: 1.055.. Train Linf norm: 2057.590.. Val Linf norm: 45.242\n",
            "Epoch 80/145.. Train loss: 59.688.. Val loss: 54.272.. Train L1 norm: 3.129.. Val L1 norm: 1.055.. Train Linf norm: 2170.698.. Val Linf norm: 45.243\n",
            "Epoch 81/145.. Train loss: 60.427.. Val loss: 54.267.. Train L1 norm: 2.932.. Val L1 norm: 1.055.. Train Linf norm: 1968.785.. Val Linf norm: 45.394\n",
            "Epoch 82/145.. Train loss: 61.425.. Val loss: 54.273.. Train L1 norm: 3.181.. Val L1 norm: 1.055.. Train Linf norm: 2220.961.. Val Linf norm: 45.269\n",
            "Epoch 83/145.. Train loss: 61.617.. Val loss: 54.265.. Train L1 norm: 3.045.. Val L1 norm: 1.055.. Train Linf norm: 2083.749.. Val Linf norm: 45.487\n",
            "Epoch 84/145.. Train loss: 59.600.. Val loss: 54.263.. Train L1 norm: 3.055.. Val L1 norm: 1.055.. Train Linf norm: 2095.294.. Val Linf norm: 45.550\n",
            "Epoch 85/145.. Train loss: 66.313.. Val loss: 54.251.. Train L1 norm: 3.119.. Val L1 norm: 1.055.. Train Linf norm: 2159.476.. Val Linf norm: 45.898\n",
            "Epoch 86/145.. Train loss: 60.658.. Val loss: 54.252.. Train L1 norm: 3.118.. Val L1 norm: 1.055.. Train Linf norm: 2156.316.. Val Linf norm: 45.870\n",
            "Epoch 87/145.. Train loss: 59.564.. Val loss: 54.253.. Train L1 norm: 3.167.. Val L1 norm: 1.055.. Train Linf norm: 2210.150.. Val Linf norm: 45.857\n",
            "Epoch 88/145.. Train loss: 60.671.. Val loss: 54.247.. Train L1 norm: 2.844.. Val L1 norm: 1.056.. Train Linf norm: 1879.191.. Val Linf norm: 46.038\n",
            "Epoch 89/145.. Train loss: 60.110.. Val loss: 54.242.. Train L1 norm: 3.043.. Val L1 norm: 1.056.. Train Linf norm: 2083.198.. Val Linf norm: 46.183\n",
            "Epoch 90/145.. Train loss: 61.753.. Val loss: 54.248.. Train L1 norm: 3.015.. Val L1 norm: 1.056.. Train Linf norm: 2053.342.. Val Linf norm: 46.036\n",
            "Epoch 91/145.. Train loss: 60.166.. Val loss: 54.251.. Train L1 norm: 3.126.. Val L1 norm: 1.056.. Train Linf norm: 2165.815.. Val Linf norm: 45.977\n",
            "Epoch 92/145.. Train loss: 62.049.. Val loss: 54.243.. Train L1 norm: 3.071.. Val L1 norm: 1.056.. Train Linf norm: 2110.771.. Val Linf norm: 46.215\n",
            "Epoch 93/145.. Train loss: 60.221.. Val loss: 54.245.. Train L1 norm: 2.944.. Val L1 norm: 1.056.. Train Linf norm: 1980.679.. Val Linf norm: 46.155\n",
            "Epoch 94/145.. Train loss: 60.241.. Val loss: 54.242.. Train L1 norm: 3.235.. Val L1 norm: 1.056.. Train Linf norm: 2278.704.. Val Linf norm: 46.251\n",
            "Epoch 95/145.. Train loss: 59.940.. Val loss: 54.241.. Train L1 norm: 3.081.. Val L1 norm: 1.056.. Train Linf norm: 2117.614.. Val Linf norm: 46.310\n",
            "Epoch 96/145.. Train loss: 59.561.. Val loss: 54.241.. Train L1 norm: 3.223.. Val L1 norm: 1.056.. Train Linf norm: 2263.510.. Val Linf norm: 46.312\n",
            "Epoch 97/145.. Train loss: 59.572.. Val loss: 54.240.. Train L1 norm: 3.042.. Val L1 norm: 1.056.. Train Linf norm: 2075.394.. Val Linf norm: 46.343\n",
            "Epoch 98/145.. Train loss: 59.883.. Val loss: 54.242.. Train L1 norm: 3.314.. Val L1 norm: 1.056.. Train Linf norm: 2360.148.. Val Linf norm: 46.308\n",
            "Epoch 99/145.. Train loss: 59.557.. Val loss: 54.241.. Train L1 norm: 3.105.. Val L1 norm: 1.056.. Train Linf norm: 2145.498.. Val Linf norm: 46.350\n",
            "Epoch 100/145.. Train loss: 60.236.. Val loss: 54.235.. Train L1 norm: 3.165.. Val L1 norm: 1.056.. Train Linf norm: 2206.455.. Val Linf norm: 46.550\n",
            "Epoch 101/145.. Train loss: 59.921.. Val loss: 54.236.. Train L1 norm: 3.177.. Val L1 norm: 1.056.. Train Linf norm: 2218.348.. Val Linf norm: 46.523\n",
            "Epoch 102/145.. Train loss: 59.613.. Val loss: 54.237.. Train L1 norm: 3.006.. Val L1 norm: 1.056.. Train Linf norm: 2043.998.. Val Linf norm: 46.501\n",
            "Epoch 103/145.. Train loss: 60.093.. Val loss: 54.241.. Train L1 norm: 3.133.. Val L1 norm: 1.056.. Train Linf norm: 2173.170.. Val Linf norm: 46.416\n",
            "Epoch 104/145.. Train loss: 61.223.. Val loss: 54.244.. Train L1 norm: 3.091.. Val L1 norm: 1.056.. Train Linf norm: 2131.982.. Val Linf norm: 46.339\n",
            "Epoch 105/145.. Train loss: 59.682.. Val loss: 54.246.. Train L1 norm: 3.091.. Val L1 norm: 1.056.. Train Linf norm: 2132.310.. Val Linf norm: 46.301\n",
            "Epoch 106/145.. Train loss: 59.879.. Val loss: 54.239.. Train L1 norm: 3.148.. Val L1 norm: 1.056.. Train Linf norm: 2189.118.. Val Linf norm: 46.508\n",
            "Epoch 107/145.. Train loss: 59.812.. Val loss: 54.235.. Train L1 norm: 3.224.. Val L1 norm: 1.056.. Train Linf norm: 2268.254.. Val Linf norm: 46.623\n",
            "Epoch 108/145.. Train loss: 59.586.. Val loss: 54.231.. Train L1 norm: 3.205.. Val L1 norm: 1.057.. Train Linf norm: 2247.403.. Val Linf norm: 46.752\n",
            "Epoch 109/145.. Train loss: 60.122.. Val loss: 54.234.. Train L1 norm: 3.056.. Val L1 norm: 1.057.. Train Linf norm: 2095.767.. Val Linf norm: 46.682\n",
            "Epoch 110/145.. Train loss: 59.600.. Val loss: 54.231.. Train L1 norm: 3.120.. Val L1 norm: 1.057.. Train Linf norm: 2161.615.. Val Linf norm: 46.780\n",
            "Epoch 111/145.. Train loss: 59.592.. Val loss: 54.228.. Train L1 norm: 3.241.. Val L1 norm: 1.057.. Train Linf norm: 2284.506.. Val Linf norm: 46.876\n",
            "Epoch 112/145.. Train loss: 60.588.. Val loss: 54.233.. Train L1 norm: 3.303.. Val L1 norm: 1.057.. Train Linf norm: 2349.069.. Val Linf norm: 46.768\n",
            "Epoch 113/145.. Train loss: 60.576.. Val loss: 54.226.. Train L1 norm: 2.972.. Val L1 norm: 1.057.. Train Linf norm: 2008.364.. Val Linf norm: 46.947\n",
            "Epoch 114/145.. Train loss: 59.590.. Val loss: 54.225.. Train L1 norm: 3.264.. Val L1 norm: 1.057.. Train Linf norm: 2308.833.. Val Linf norm: 47.003\n",
            "Epoch 115/145.. Train loss: 60.475.. Val loss: 54.217.. Train L1 norm: 3.074.. Val L1 norm: 1.057.. Train Linf norm: 2112.166.. Val Linf norm: 47.224\n",
            "Epoch 116/145.. Train loss: 60.484.. Val loss: 54.221.. Train L1 norm: 2.979.. Val L1 norm: 1.057.. Train Linf norm: 2015.777.. Val Linf norm: 47.131\n",
            "Epoch 117/145.. Train loss: 60.506.. Val loss: 54.213.. Train L1 norm: 3.194.. Val L1 norm: 1.058.. Train Linf norm: 2232.415.. Val Linf norm: 47.383\n",
            "Epoch 118/145.. Train loss: 61.830.. Val loss: 54.201.. Train L1 norm: 3.154.. Val L1 norm: 1.058.. Train Linf norm: 2194.178.. Val Linf norm: 47.719\n",
            "Epoch 119/145.. Train loss: 59.636.. Val loss: 54.200.. Train L1 norm: 3.139.. Val L1 norm: 1.058.. Train Linf norm: 2179.372.. Val Linf norm: 47.772\n",
            "Epoch 120/145.. Train loss: 59.615.. Val loss: 54.206.. Train L1 norm: 3.392.. Val L1 norm: 1.058.. Train Linf norm: 2438.995.. Val Linf norm: 47.603\n",
            "Epoch 121/145.. Train loss: 60.635.. Val loss: 54.212.. Train L1 norm: 2.949.. Val L1 norm: 1.058.. Train Linf norm: 1980.141.. Val Linf norm: 47.451\n",
            "Epoch 122/145.. Train loss: 61.415.. Val loss: 54.220.. Train L1 norm: 3.143.. Val L1 norm: 1.057.. Train Linf norm: 2182.078.. Val Linf norm: 47.258\n",
            "Epoch 123/145.. Train loss: 59.811.. Val loss: 54.222.. Train L1 norm: 3.100.. Val L1 norm: 1.057.. Train Linf norm: 2140.941.. Val Linf norm: 47.208\n",
            "Epoch 124/145.. Train loss: 59.947.. Val loss: 54.217.. Train L1 norm: 3.062.. Val L1 norm: 1.058.. Train Linf norm: 2100.673.. Val Linf norm: 47.359\n",
            "Epoch 125/145.. Train loss: 59.554.. Val loss: 54.216.. Train L1 norm: 3.051.. Val L1 norm: 1.058.. Train Linf norm: 2089.488.. Val Linf norm: 47.414\n",
            "Epoch 126/145.. Train loss: 61.824.. Val loss: 54.225.. Train L1 norm: 2.823.. Val L1 norm: 1.057.. Train Linf norm: 1856.016.. Val Linf norm: 47.169\n",
            "Epoch 127/145.. Train loss: 60.513.. Val loss: 54.231.. Train L1 norm: 2.969.. Val L1 norm: 1.057.. Train Linf norm: 2005.625.. Val Linf norm: 47.031\n",
            "Epoch 128/145.. Train loss: 62.847.. Val loss: 54.227.. Train L1 norm: 3.151.. Val L1 norm: 1.057.. Train Linf norm: 2191.778.. Val Linf norm: 47.153\n",
            "Epoch 129/145.. Train loss: 62.261.. Val loss: 54.206.. Train L1 norm: 3.128.. Val L1 norm: 1.058.. Train Linf norm: 2167.651.. Val Linf norm: 47.733\n",
            "Epoch 130/145.. Train loss: 60.219.. Val loss: 54.199.. Train L1 norm: 3.319.. Val L1 norm: 1.058.. Train Linf norm: 2363.671.. Val Linf norm: 47.919\n",
            "Epoch 131/145.. Train loss: 59.710.. Val loss: 54.200.. Train L1 norm: 3.265.. Val L1 norm: 1.058.. Train Linf norm: 2309.529.. Val Linf norm: 47.905\n",
            "Epoch 132/145.. Train loss: 59.562.. Val loss: 54.199.. Train L1 norm: 3.116.. Val L1 norm: 1.058.. Train Linf norm: 2156.307.. Val Linf norm: 47.956\n",
            "Epoch 133/145.. Train loss: 60.154.. Val loss: 54.191.. Train L1 norm: 3.099.. Val L1 norm: 1.059.. Train Linf norm: 2137.838.. Val Linf norm: 48.191\n",
            "Epoch 134/145.. Train loss: 60.270.. Val loss: 54.195.. Train L1 norm: 3.143.. Val L1 norm: 1.059.. Train Linf norm: 2183.775.. Val Linf norm: 48.083\n",
            "Epoch 135/145.. Train loss: 61.310.. Val loss: 54.186.. Train L1 norm: 3.192.. Val L1 norm: 1.059.. Train Linf norm: 2234.131.. Val Linf norm: 48.361\n",
            "Epoch 136/145.. Train loss: 62.597.. Val loss: 54.193.. Train L1 norm: 3.253.. Val L1 norm: 1.059.. Train Linf norm: 2294.684.. Val Linf norm: 48.159\n",
            "Epoch 137/145.. Train loss: 59.868.. Val loss: 54.188.. Train L1 norm: 3.231.. Val L1 norm: 1.059.. Train Linf norm: 2272.985.. Val Linf norm: 48.312\n",
            "Epoch 138/145.. Train loss: 59.766.. Val loss: 54.183.. Train L1 norm: 3.079.. Val L1 norm: 1.059.. Train Linf norm: 2117.828.. Val Linf norm: 48.453\n",
            "Epoch 139/145.. Train loss: 60.301.. Val loss: 54.187.. Train L1 norm: 3.248.. Val L1 norm: 1.059.. Train Linf norm: 2291.314.. Val Linf norm: 48.360\n",
            "Epoch 140/145.. Train loss: 65.194.. Val loss: 54.173.. Train L1 norm: 3.216.. Val L1 norm: 1.060.. Train Linf norm: 2256.822.. Val Linf norm: 48.755\n",
            "Epoch 141/145.. Train loss: 60.843.. Val loss: 54.178.. Train L1 norm: 3.194.. Val L1 norm: 1.059.. Train Linf norm: 2234.699.. Val Linf norm: 48.632\n",
            "Epoch 142/145.. Train loss: 61.649.. Val loss: 54.178.. Train L1 norm: 3.328.. Val L1 norm: 1.059.. Train Linf norm: 2373.301.. Val Linf norm: 48.639\n",
            "Epoch 143/145.. Train loss: 60.836.. Val loss: 54.178.. Train L1 norm: 3.318.. Val L1 norm: 1.059.. Train Linf norm: 2361.900.. Val Linf norm: 48.657\n",
            "Epoch 144/145.. Train loss: 59.501.. Val loss: 54.177.. Train L1 norm: 3.138.. Val L1 norm: 1.059.. Train Linf norm: 2178.643.. Val Linf norm: 48.720\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 08:13:31,200]\u001b[0m Trial 55 finished with value: 1.0596554427782694 and parameters: {'n_layers': 7, 'n_units_0': 3786, 'n_units_1': 2213, 'n_units_2': 3199, 'n_units_3': 359, 'n_units_4': 1748, 'n_units_5': 1401, 'n_units_6': 3629, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 1.803380718220668e-06, 'batch_size': 1024, 'n_epochs': 145, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.2129324023069778, 'dropout_rate': 0.006061456203508761, 'weight_decay': 0.00017926265997443623, 'beta1': 0.9168510263520014, 'beta2': 0.9990046663126676, 'factor': 0.18290106025864714, 'patience': 5, 'threshold': 0.0011033724083006806}. Best is trial 22 with value: 1.0150012904485066.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 145/145.. Train loss: 59.665.. Val loss: 54.173.. Train L1 norm: 3.300.. Val L1 norm: 1.060.. Train Linf norm: 2342.316.. Val Linf norm: 48.832\n",
            "Epoch 1/146.. Train loss: 221.830.. Val loss: 55.491.. Train L1 norm: 2.447.. Val L1 norm: 1.038.. Train Linf norm: 1444.916.. Val Linf norm: 32.068\n",
            "Epoch 2/146.. Train loss: 302.065.. Val loss: 53.769.. Train L1 norm: 1.448.. Val L1 norm: 1.048.. Train Linf norm: 451.564.. Val Linf norm: 36.532\n",
            "Epoch 3/146.. Train loss: 203.653.. Val loss: 54.798.. Train L1 norm: 2.890.. Val L1 norm: 1.029.. Train Linf norm: 1924.583.. Val Linf norm: 26.745\n",
            "Epoch 4/146.. Train loss: 144.922.. Val loss: 54.004.. Train L1 norm: 1.435.. Val L1 norm: 1.036.. Train Linf norm: 443.395.. Val Linf norm: 27.937\n",
            "Epoch 5/146.. Train loss: 235.164.. Val loss: 55.137.. Train L1 norm: 2.133.. Val L1 norm: 1.028.. Train Linf norm: 1156.379.. Val Linf norm: 25.761\n",
            "Epoch 6/146.. Train loss: 118.879.. Val loss: 54.547.. Train L1 norm: 1.398.. Val L1 norm: 1.027.. Train Linf norm: 404.991.. Val Linf norm: 24.621\n",
            "Epoch 7/146.. Train loss: 101.464.. Val loss: 54.934.. Train L1 norm: 2.285.. Val L1 norm: 1.026.. Train Linf norm: 1312.152.. Val Linf norm: 23.949\n",
            "Epoch 8/146.. Train loss: 129.137.. Val loss: 53.925.. Train L1 norm: 1.682.. Val L1 norm: 1.044.. Train Linf norm: 693.768.. Val Linf norm: 33.882\n",
            "Epoch 9/146.. Train loss: 275.125.. Val loss: 54.095.. Train L1 norm: 2.981.. Val L1 norm: 1.037.. Train Linf norm: 2015.907.. Val Linf norm: 30.055\n",
            "Epoch 10/146.. Train loss: 157.149.. Val loss: 54.377.. Train L1 norm: 2.746.. Val L1 norm: 1.027.. Train Linf norm: 1782.958.. Val Linf norm: 23.666\n",
            "Epoch 11/146.. Train loss: 93.006.. Val loss: 54.617.. Train L1 norm: 1.875.. Val L1 norm: 1.024.. Train Linf norm: 893.930.. Val Linf norm: 22.261\n",
            "Epoch 12/146.. Train loss: 72.772.. Val loss: 54.660.. Train L1 norm: 1.962.. Val L1 norm: 1.024.. Train Linf norm: 364.478.. Val Linf norm: 22.367\n",
            "Epoch 13/146.. Train loss: 60.440.. Val loss: 54.678.. Train L1 norm: 1.770.. Val L1 norm: 1.024.. Train Linf norm: 787.192.. Val Linf norm: 22.308\n",
            "Epoch 14/146.. Train loss: 88.412.. Val loss: 54.581.. Train L1 norm: 2.132.. Val L1 norm: 1.025.. Train Linf norm: 1157.687.. Val Linf norm: 22.254\n",
            "Epoch 15/146.. Train loss: 105.779.. Val loss: 54.582.. Train L1 norm: 2.417.. Val L1 norm: 1.025.. Train Linf norm: 1448.123.. Val Linf norm: 22.252\n",
            "Epoch 16/146.. Train loss: 64.137.. Val loss: 54.613.. Train L1 norm: 2.518.. Val L1 norm: 1.025.. Train Linf norm: 1552.662.. Val Linf norm: 22.278\n",
            "Epoch 17/146.. Train loss: 61.909.. Val loss: 54.610.. Train L1 norm: 2.196.. Val L1 norm: 1.025.. Train Linf norm: 1223.270.. Val Linf norm: 22.295\n",
            "Epoch 18/146.. Train loss: 75.916.. Val loss: 54.623.. Train L1 norm: 2.303.. Val L1 norm: 1.025.. Train Linf norm: 1331.710.. Val Linf norm: 22.298\n",
            "Epoch 19/146.. Train loss: 70.676.. Val loss: 54.645.. Train L1 norm: 2.631.. Val L1 norm: 1.024.. Train Linf norm: 1667.814.. Val Linf norm: 22.281\n",
            "Epoch 20/146.. Train loss: 63.789.. Val loss: 54.646.. Train L1 norm: 2.052.. Val L1 norm: 1.024.. Train Linf norm: 1075.255.. Val Linf norm: 22.286\n",
            "Epoch 21/146.. Train loss: 60.032.. Val loss: 54.647.. Train L1 norm: 2.431.. Val L1 norm: 1.024.. Train Linf norm: 1459.253.. Val Linf norm: 22.291\n",
            "Epoch 22/146.. Train loss: 60.601.. Val loss: 54.646.. Train L1 norm: 2.022.. Val L1 norm: 1.024.. Train Linf norm: 1044.973.. Val Linf norm: 22.290\n",
            "Epoch 23/146.. Train loss: 84.866.. Val loss: 54.652.. Train L1 norm: 2.481.. Val L1 norm: 1.024.. Train Linf norm: 1514.378.. Val Linf norm: 22.297\n",
            "Epoch 24/146.. Train loss: 71.569.. Val loss: 54.657.. Train L1 norm: 2.278.. Val L1 norm: 1.024.. Train Linf norm: 1306.669.. Val Linf norm: 22.303\n",
            "Epoch 25/146.. Train loss: 71.215.. Val loss: 54.661.. Train L1 norm: 2.253.. Val L1 norm: 1.024.. Train Linf norm: 1280.880.. Val Linf norm: 22.307\n",
            "Epoch 26/146.. Train loss: 63.446.. Val loss: 54.659.. Train L1 norm: 2.419.. Val L1 norm: 1.024.. Train Linf norm: 1450.894.. Val Linf norm: 22.305\n",
            "Epoch 27/146.. Train loss: 76.775.. Val loss: 54.660.. Train L1 norm: 1.995.. Val L1 norm: 1.024.. Train Linf norm: 1017.627.. Val Linf norm: 22.307\n",
            "Epoch 28/146.. Train loss: 80.330.. Val loss: 54.658.. Train L1 norm: 1.992.. Val L1 norm: 1.024.. Train Linf norm: 1013.441.. Val Linf norm: 22.301\n",
            "Epoch 29/146.. Train loss: 87.386.. Val loss: 54.659.. Train L1 norm: 1.943.. Val L1 norm: 1.024.. Train Linf norm: 962.747.. Val Linf norm: 22.302\n",
            "Epoch 30/146.. Train loss: 63.364.. Val loss: 54.659.. Train L1 norm: 1.842.. Val L1 norm: 1.024.. Train Linf norm: 859.401.. Val Linf norm: 22.302\n",
            "Epoch 31/146.. Train loss: 94.083.. Val loss: 54.660.. Train L1 norm: 2.008.. Val L1 norm: 1.024.. Train Linf norm: 1029.839.. Val Linf norm: 22.302\n",
            "Epoch 32/146.. Train loss: 64.013.. Val loss: 54.660.. Train L1 norm: 1.715.. Val L1 norm: 1.024.. Train Linf norm: 729.428.. Val Linf norm: 22.306\n",
            "Epoch 33/146.. Train loss: 63.918.. Val loss: 54.660.. Train L1 norm: 2.522.. Val L1 norm: 1.024.. Train Linf norm: 1557.544.. Val Linf norm: 22.305\n",
            "Epoch 34/146.. Train loss: 62.575.. Val loss: 54.660.. Train L1 norm: 2.278.. Val L1 norm: 1.024.. Train Linf norm: 1307.028.. Val Linf norm: 22.303\n",
            "Epoch 35/146.. Train loss: 60.089.. Val loss: 54.660.. Train L1 norm: 2.385.. Val L1 norm: 1.024.. Train Linf norm: 1416.690.. Val Linf norm: 22.302\n",
            "Epoch 36/146.. Train loss: 63.738.. Val loss: 54.661.. Train L1 norm: 2.169.. Val L1 norm: 1.024.. Train Linf norm: 1194.565.. Val Linf norm: 22.301\n",
            "Epoch 37/146.. Train loss: 65.631.. Val loss: 54.661.. Train L1 norm: 2.097.. Val L1 norm: 1.024.. Train Linf norm: 1120.979.. Val Linf norm: 22.303\n",
            "Epoch 38/146.. Train loss: 60.928.. Val loss: 54.661.. Train L1 norm: 2.454.. Val L1 norm: 1.024.. Train Linf norm: 1487.098.. Val Linf norm: 22.303\n",
            "Epoch 39/146.. Train loss: 60.173.. Val loss: 54.661.. Train L1 norm: 2.225.. Val L1 norm: 1.024.. Train Linf norm: 1251.507.. Val Linf norm: 22.302\n",
            "Epoch 40/146.. Train loss: 99.731.. Val loss: 54.658.. Train L1 norm: 1.819.. Val L1 norm: 1.024.. Train Linf norm: 836.740.. Val Linf norm: 22.294\n",
            "Epoch 41/146.. Train loss: 60.156.. Val loss: 54.658.. Train L1 norm: 2.189.. Val L1 norm: 1.024.. Train Linf norm: 1215.141.. Val Linf norm: 22.295\n",
            "Epoch 42/146.. Train loss: 61.246.. Val loss: 54.658.. Train L1 norm: 1.980.. Val L1 norm: 1.024.. Train Linf norm: 1002.444.. Val Linf norm: 22.294\n",
            "Epoch 43/146.. Train loss: 86.093.. Val loss: 54.660.. Train L1 norm: 1.914.. Val L1 norm: 1.024.. Train Linf norm: 932.569.. Val Linf norm: 22.298\n",
            "Epoch 44/146.. Train loss: 64.119.. Val loss: 54.660.. Train L1 norm: 1.800.. Val L1 norm: 1.024.. Train Linf norm: 816.686.. Val Linf norm: 22.298\n",
            "Epoch 45/146.. Train loss: 73.082.. Val loss: 54.662.. Train L1 norm: 1.735.. Val L1 norm: 1.024.. Train Linf norm: 749.540.. Val Linf norm: 22.297\n",
            "Epoch 46/146.. Train loss: 60.188.. Val loss: 54.662.. Train L1 norm: 2.342.. Val L1 norm: 1.024.. Train Linf norm: 1372.779.. Val Linf norm: 22.298\n",
            "Epoch 47/146.. Train loss: 60.047.. Val loss: 54.662.. Train L1 norm: 2.926.. Val L1 norm: 1.024.. Train Linf norm: 1970.226.. Val Linf norm: 22.298\n",
            "Epoch 48/146.. Train loss: 72.436.. Val loss: 54.662.. Train L1 norm: 2.712.. Val L1 norm: 1.024.. Train Linf norm: 1752.365.. Val Linf norm: 22.298\n",
            "Epoch 49/146.. Train loss: 66.719.. Val loss: 54.660.. Train L1 norm: 2.318.. Val L1 norm: 1.024.. Train Linf norm: 1346.926.. Val Linf norm: 22.298\n",
            "Epoch 50/146.. Train loss: 64.781.. Val loss: 54.661.. Train L1 norm: 2.044.. Val L1 norm: 1.024.. Train Linf norm: 1067.282.. Val Linf norm: 22.301\n",
            "Epoch 51/146.. Train loss: 130.013.. Val loss: 54.658.. Train L1 norm: 2.499.. Val L1 norm: 1.024.. Train Linf norm: 1534.016.. Val Linf norm: 22.291\n",
            "Epoch 52/146.. Train loss: 61.897.. Val loss: 54.656.. Train L1 norm: 2.548.. Val L1 norm: 1.024.. Train Linf norm: 1583.216.. Val Linf norm: 22.287\n",
            "Epoch 53/146.. Train loss: 60.873.. Val loss: 54.656.. Train L1 norm: 2.461.. Val L1 norm: 1.024.. Train Linf norm: 1493.880.. Val Linf norm: 22.286\n",
            "Epoch 54/146.. Train loss: 60.148.. Val loss: 54.656.. Train L1 norm: 2.833.. Val L1 norm: 1.024.. Train Linf norm: 1875.439.. Val Linf norm: 22.286\n",
            "Epoch 55/146.. Train loss: 62.765.. Val loss: 54.655.. Train L1 norm: 1.222.. Val L1 norm: 1.024.. Train Linf norm: 226.149.. Val Linf norm: 22.287\n",
            "Epoch 56/146.. Train loss: 60.842.. Val loss: 54.655.. Train L1 norm: 2.527.. Val L1 norm: 1.024.. Train Linf norm: 1561.348.. Val Linf norm: 22.289\n",
            "Epoch 57/146.. Train loss: 60.781.. Val loss: 54.655.. Train L1 norm: 1.939.. Val L1 norm: 1.024.. Train Linf norm: 958.205.. Val Linf norm: 22.289\n",
            "Epoch 58/146.. Train loss: 60.389.. Val loss: 54.655.. Train L1 norm: 2.674.. Val L1 norm: 1.024.. Train Linf norm: 1712.893.. Val Linf norm: 22.289\n",
            "Epoch 59/146.. Train loss: 89.378.. Val loss: 54.652.. Train L1 norm: 1.853.. Val L1 norm: 1.024.. Train Linf norm: 871.202.. Val Linf norm: 22.283\n",
            "Epoch 60/146.. Train loss: 64.561.. Val loss: 54.649.. Train L1 norm: 2.157.. Val L1 norm: 1.024.. Train Linf norm: 1182.988.. Val Linf norm: 22.278\n",
            "Epoch 61/146.. Train loss: 60.252.. Val loss: 54.649.. Train L1 norm: 2.161.. Val L1 norm: 1.024.. Train Linf norm: 1187.009.. Val Linf norm: 22.279\n",
            "Epoch 62/146.. Train loss: 65.766.. Val loss: 54.650.. Train L1 norm: 2.554.. Val L1 norm: 1.024.. Train Linf norm: 1588.242.. Val Linf norm: 22.283\n",
            "Epoch 63/146.. Train loss: 64.284.. Val loss: 54.651.. Train L1 norm: 2.480.. Val L1 norm: 1.024.. Train Linf norm: 1513.485.. Val Linf norm: 22.283\n",
            "Epoch 64/146.. Train loss: 63.954.. Val loss: 54.651.. Train L1 norm: 1.885.. Val L1 norm: 1.024.. Train Linf norm: 903.813.. Val Linf norm: 22.284\n",
            "Epoch 65/146.. Train loss: 60.619.. Val loss: 54.651.. Train L1 norm: 2.172.. Val L1 norm: 1.024.. Train Linf norm: 1197.363.. Val Linf norm: 22.288\n",
            "Epoch 66/146.. Train loss: 61.708.. Val loss: 54.652.. Train L1 norm: 2.636.. Val L1 norm: 1.024.. Train Linf norm: 1672.948.. Val Linf norm: 22.287\n",
            "Epoch 67/146.. Train loss: 62.892.. Val loss: 54.652.. Train L1 norm: 2.198.. Val L1 norm: 1.024.. Train Linf norm: 1225.183.. Val Linf norm: 22.288\n",
            "Epoch 68/146.. Train loss: 62.692.. Val loss: 54.652.. Train L1 norm: 2.483.. Val L1 norm: 1.024.. Train Linf norm: 1516.540.. Val Linf norm: 22.291\n",
            "Epoch 69/146.. Train loss: 61.660.. Val loss: 54.651.. Train L1 norm: 2.402.. Val L1 norm: 1.024.. Train Linf norm: 1435.421.. Val Linf norm: 22.291\n",
            "Epoch 70/146.. Train loss: 77.505.. Val loss: 54.649.. Train L1 norm: 2.266.. Val L1 norm: 1.024.. Train Linf norm: 1290.757.. Val Linf norm: 22.288\n",
            "Epoch 71/146.. Train loss: 72.126.. Val loss: 54.645.. Train L1 norm: 2.135.. Val L1 norm: 1.025.. Train Linf norm: 1160.018.. Val Linf norm: 22.284\n",
            "Epoch 72/146.. Train loss: 61.570.. Val loss: 54.646.. Train L1 norm: 2.399.. Val L1 norm: 1.024.. Train Linf norm: 1427.127.. Val Linf norm: 22.285\n",
            "Epoch 73/146.. Train loss: 61.652.. Val loss: 54.647.. Train L1 norm: 1.816.. Val L1 norm: 1.024.. Train Linf norm: 832.895.. Val Linf norm: 22.285\n",
            "Epoch 74/146.. Train loss: 60.707.. Val loss: 54.646.. Train L1 norm: 2.364.. Val L1 norm: 1.025.. Train Linf norm: 1394.321.. Val Linf norm: 22.286\n",
            "Epoch 75/146.. Train loss: 65.897.. Val loss: 54.647.. Train L1 norm: 2.244.. Val L1 norm: 1.024.. Train Linf norm: 1269.116.. Val Linf norm: 22.289\n",
            "Epoch 76/146.. Train loss: 61.818.. Val loss: 54.647.. Train L1 norm: 1.888.. Val L1 norm: 1.025.. Train Linf norm: 906.475.. Val Linf norm: 22.290\n",
            "Epoch 77/146.. Train loss: 61.036.. Val loss: 54.646.. Train L1 norm: 1.946.. Val L1 norm: 1.025.. Train Linf norm: 967.469.. Val Linf norm: 22.287\n",
            "Epoch 78/146.. Train loss: 63.130.. Val loss: 54.645.. Train L1 norm: 1.830.. Val L1 norm: 1.025.. Train Linf norm: 847.179.. Val Linf norm: 22.286\n",
            "Epoch 79/146.. Train loss: 86.774.. Val loss: 54.647.. Train L1 norm: 2.467.. Val L1 norm: 1.025.. Train Linf norm: 1499.011.. Val Linf norm: 22.297\n",
            "Epoch 80/146.. Train loss: 61.284.. Val loss: 54.648.. Train L1 norm: 1.900.. Val L1 norm: 1.025.. Train Linf norm: 917.927.. Val Linf norm: 22.300\n",
            "Epoch 81/146.. Train loss: 84.815.. Val loss: 54.644.. Train L1 norm: 1.813.. Val L1 norm: 1.025.. Train Linf norm: 829.640.. Val Linf norm: 22.293\n",
            "Epoch 82/146.. Train loss: 64.778.. Val loss: 54.642.. Train L1 norm: 2.574.. Val L1 norm: 1.025.. Train Linf norm: 1607.934.. Val Linf norm: 22.288\n",
            "Epoch 83/146.. Train loss: 71.563.. Val loss: 54.645.. Train L1 norm: 2.822.. Val L1 norm: 1.025.. Train Linf norm: 1863.216.. Val Linf norm: 22.297\n",
            "Epoch 84/146.. Train loss: 62.464.. Val loss: 54.644.. Train L1 norm: 2.275.. Val L1 norm: 1.025.. Train Linf norm: 1303.743.. Val Linf norm: 22.300\n",
            "Epoch 85/146.. Train loss: 64.338.. Val loss: 54.645.. Train L1 norm: 2.004.. Val L1 norm: 1.025.. Train Linf norm: 1026.167.. Val Linf norm: 22.305\n",
            "Epoch 86/146.. Train loss: 70.147.. Val loss: 54.643.. Train L1 norm: 2.403.. Val L1 norm: 1.025.. Train Linf norm: 1433.723.. Val Linf norm: 22.308\n",
            "Epoch 87/146.. Train loss: 167.224.. Val loss: 54.637.. Train L1 norm: 2.418.. Val L1 norm: 1.025.. Train Linf norm: 1444.559.. Val Linf norm: 22.292\n",
            "Epoch 88/146.. Train loss: 63.744.. Val loss: 54.638.. Train L1 norm: 2.531.. Val L1 norm: 1.025.. Train Linf norm: 1565.555.. Val Linf norm: 22.294\n",
            "Epoch 89/146.. Train loss: 129.587.. Val loss: 54.639.. Train L1 norm: 2.676.. Val L1 norm: 1.025.. Train Linf norm: 1713.433.. Val Linf norm: 22.297\n",
            "Epoch 90/146.. Train loss: 61.550.. Val loss: 54.641.. Train L1 norm: 2.486.. Val L1 norm: 1.025.. Train Linf norm: 1518.282.. Val Linf norm: 22.309\n",
            "Epoch 91/146.. Train loss: 73.958.. Val loss: 54.642.. Train L1 norm: 1.714.. Val L1 norm: 1.025.. Train Linf norm: 728.926.. Val Linf norm: 22.312\n",
            "Epoch 92/146.. Train loss: 59.958.. Val loss: 54.643.. Train L1 norm: 2.650.. Val L1 norm: 1.025.. Train Linf norm: 1688.297.. Val Linf norm: 22.314\n",
            "Epoch 93/146.. Train loss: 63.583.. Val loss: 54.643.. Train L1 norm: 2.009.. Val L1 norm: 1.025.. Train Linf norm: 1032.315.. Val Linf norm: 22.315\n",
            "Epoch 94/146.. Train loss: 63.009.. Val loss: 54.644.. Train L1 norm: 1.726.. Val L1 norm: 1.025.. Train Linf norm: 741.887.. Val Linf norm: 22.316\n",
            "Epoch 95/146.. Train loss: 64.498.. Val loss: 54.645.. Train L1 norm: 2.514.. Val L1 norm: 1.025.. Train Linf norm: 1543.090.. Val Linf norm: 22.320\n",
            "Epoch 96/146.. Train loss: 107.010.. Val loss: 54.649.. Train L1 norm: 1.882.. Val L1 norm: 1.025.. Train Linf norm: 901.140.. Val Linf norm: 22.331\n",
            "Epoch 97/146.. Train loss: 69.018.. Val loss: 54.650.. Train L1 norm: 1.940.. Val L1 norm: 1.025.. Train Linf norm: 960.074.. Val Linf norm: 22.335\n",
            "Epoch 98/146.. Train loss: 61.247.. Val loss: 54.650.. Train L1 norm: 2.642.. Val L1 norm: 1.025.. Train Linf norm: 1680.501.. Val Linf norm: 22.337\n",
            "Epoch 99/146.. Train loss: 62.073.. Val loss: 54.650.. Train L1 norm: 2.442.. Val L1 norm: 1.025.. Train Linf norm: 1475.711.. Val Linf norm: 22.335\n",
            "Epoch 100/146.. Train loss: 101.227.. Val loss: 54.653.. Train L1 norm: 1.865.. Val L1 norm: 1.025.. Train Linf norm: 883.111.. Val Linf norm: 22.344\n",
            "Epoch 101/146.. Train loss: 78.316.. Val loss: 54.651.. Train L1 norm: 2.987.. Val L1 norm: 1.025.. Train Linf norm: 2028.028.. Val Linf norm: 22.350\n",
            "Epoch 102/146.. Train loss: 89.471.. Val loss: 54.651.. Train L1 norm: 2.335.. Val L1 norm: 1.025.. Train Linf norm: 1364.330.. Val Linf norm: 22.352\n",
            "Epoch 103/146.. Train loss: 103.324.. Val loss: 54.654.. Train L1 norm: 1.633.. Val L1 norm: 1.025.. Train Linf norm: 644.088.. Val Linf norm: 22.348\n",
            "Epoch 104/146.. Train loss: 60.222.. Val loss: 54.655.. Train L1 norm: 2.271.. Val L1 norm: 1.025.. Train Linf norm: 1299.117.. Val Linf norm: 22.349\n",
            "Epoch 105/146.. Train loss: 178.529.. Val loss: 54.658.. Train L1 norm: 2.776.. Val L1 norm: 1.025.. Train Linf norm: 1816.725.. Val Linf norm: 22.344\n",
            "Epoch 106/146.. Train loss: 64.477.. Val loss: 54.659.. Train L1 norm: 2.633.. Val L1 norm: 1.025.. Train Linf norm: 1669.646.. Val Linf norm: 22.344\n",
            "Epoch 107/146.. Train loss: 157.678.. Val loss: 54.656.. Train L1 norm: 2.370.. Val L1 norm: 1.025.. Train Linf norm: 1398.303.. Val Linf norm: 22.333\n",
            "Epoch 108/146.. Train loss: 60.542.. Val loss: 54.655.. Train L1 norm: 2.735.. Val L1 norm: 1.025.. Train Linf norm: 1774.674.. Val Linf norm: 22.333\n",
            "Epoch 109/146.. Train loss: 61.137.. Val loss: 54.655.. Train L1 norm: 2.712.. Val L1 norm: 1.025.. Train Linf norm: 1750.445.. Val Linf norm: 22.334\n",
            "Epoch 110/146.. Train loss: 60.180.. Val loss: 54.655.. Train L1 norm: 2.421.. Val L1 norm: 1.025.. Train Linf norm: 1453.004.. Val Linf norm: 22.335\n",
            "Epoch 111/146.. Train loss: 60.530.. Val loss: 54.655.. Train L1 norm: 2.524.. Val L1 norm: 1.025.. Train Linf norm: 1557.865.. Val Linf norm: 22.335\n",
            "Epoch 112/146.. Train loss: 61.499.. Val loss: 54.654.. Train L1 norm: 2.049.. Val L1 norm: 1.025.. Train Linf norm: 1072.904.. Val Linf norm: 22.335\n",
            "Epoch 113/146.. Train loss: 60.131.. Val loss: 54.654.. Train L1 norm: 1.612.. Val L1 norm: 1.025.. Train Linf norm: 625.011.. Val Linf norm: 22.335\n",
            "Epoch 114/146.. Train loss: 63.884.. Val loss: 54.653.. Train L1 norm: 1.938.. Val L1 norm: 1.025.. Train Linf norm: 958.195.. Val Linf norm: 22.338\n",
            "Epoch 115/146.. Train loss: 60.286.. Val loss: 54.653.. Train L1 norm: 2.441.. Val L1 norm: 1.025.. Train Linf norm: 1474.095.. Val Linf norm: 22.340\n",
            "Epoch 116/146.. Train loss: 73.953.. Val loss: 54.651.. Train L1 norm: 2.309.. Val L1 norm: 1.025.. Train Linf norm: 1339.179.. Val Linf norm: 22.340\n",
            "Epoch 117/146.. Train loss: 60.145.. Val loss: 54.650.. Train L1 norm: 1.862.. Val L1 norm: 1.025.. Train Linf norm: 876.198.. Val Linf norm: 22.341\n",
            "Epoch 118/146.. Train loss: 61.028.. Val loss: 54.650.. Train L1 norm: 2.649.. Val L1 norm: 1.025.. Train Linf norm: 1685.337.. Val Linf norm: 22.340\n",
            "Epoch 119/146.. Train loss: 87.629.. Val loss: 54.653.. Train L1 norm: 2.588.. Val L1 norm: 1.025.. Train Linf norm: 1623.901.. Val Linf norm: 22.343\n",
            "Epoch 120/146.. Train loss: 60.143.. Val loss: 54.653.. Train L1 norm: 2.495.. Val L1 norm: 1.025.. Train Linf norm: 1526.479.. Val Linf norm: 22.344\n",
            "Epoch 121/146.. Train loss: 70.454.. Val loss: 54.652.. Train L1 norm: 1.933.. Val L1 norm: 1.025.. Train Linf norm: 954.647.. Val Linf norm: 22.341\n",
            "Epoch 122/146.. Train loss: 69.765.. Val loss: 54.649.. Train L1 norm: 2.100.. Val L1 norm: 1.025.. Train Linf norm: 1124.395.. Val Linf norm: 22.333\n",
            "Epoch 123/146.. Train loss: 60.377.. Val loss: 54.649.. Train L1 norm: 2.138.. Val L1 norm: 1.025.. Train Linf norm: 1162.947.. Val Linf norm: 22.334\n",
            "Epoch 124/146.. Train loss: 91.616.. Val loss: 54.645.. Train L1 norm: 2.579.. Val L1 norm: 1.025.. Train Linf norm: 1613.058.. Val Linf norm: 22.320\n",
            "Epoch 125/146.. Train loss: 60.168.. Val loss: 54.645.. Train L1 norm: 1.572.. Val L1 norm: 1.025.. Train Linf norm: 580.924.. Val Linf norm: 22.319\n",
            "Epoch 126/146.. Train loss: 68.553.. Val loss: 54.647.. Train L1 norm: 1.733.. Val L1 norm: 1.025.. Train Linf norm: 747.583.. Val Linf norm: 22.321\n",
            "Epoch 127/146.. Train loss: 60.872.. Val loss: 54.646.. Train L1 norm: 2.186.. Val L1 norm: 1.025.. Train Linf norm: 1211.817.. Val Linf norm: 22.322\n",
            "Epoch 128/146.. Train loss: 63.786.. Val loss: 54.647.. Train L1 norm: 1.904.. Val L1 norm: 1.025.. Train Linf norm: 922.339.. Val Linf norm: 22.322\n",
            "Epoch 129/146.. Train loss: 61.157.. Val loss: 54.647.. Train L1 norm: 1.717.. Val L1 norm: 1.025.. Train Linf norm: 731.890.. Val Linf norm: 22.322\n",
            "Epoch 130/146.. Train loss: 91.898.. Val loss: 54.649.. Train L1 norm: 2.505.. Val L1 norm: 1.025.. Train Linf norm: 1538.015.. Val Linf norm: 22.325\n",
            "Epoch 131/146.. Train loss: 64.593.. Val loss: 54.650.. Train L1 norm: 2.857.. Val L1 norm: 1.025.. Train Linf norm: 1898.956.. Val Linf norm: 22.327\n",
            "Epoch 132/146.. Train loss: 74.751.. Val loss: 54.649.. Train L1 norm: 1.793.. Val L1 norm: 1.025.. Train Linf norm: 809.155.. Val Linf norm: 22.326\n",
            "Epoch 133/146.. Train loss: 76.156.. Val loss: 54.651.. Train L1 norm: 2.675.. Val L1 norm: 1.025.. Train Linf norm: 1712.006.. Val Linf norm: 22.339\n",
            "Epoch 134/146.. Train loss: 60.599.. Val loss: 54.651.. Train L1 norm: 2.770.. Val L1 norm: 1.025.. Train Linf norm: 1810.844.. Val Linf norm: 22.341\n",
            "Epoch 135/146.. Train loss: 66.891.. Val loss: 54.653.. Train L1 norm: 2.208.. Val L1 norm: 1.025.. Train Linf norm: 1233.594.. Val Linf norm: 22.344\n",
            "Epoch 136/146.. Train loss: 68.594.. Val loss: 54.655.. Train L1 norm: 2.280.. Val L1 norm: 1.025.. Train Linf norm: 1308.999.. Val Linf norm: 22.350\n",
            "Epoch 137/146.. Train loss: 64.305.. Val loss: 54.654.. Train L1 norm: 1.779.. Val L1 norm: 1.025.. Train Linf norm: 795.051.. Val Linf norm: 22.349\n",
            "Epoch 138/146.. Train loss: 68.823.. Val loss: 54.655.. Train L1 norm: 2.227.. Val L1 norm: 1.025.. Train Linf norm: 1253.452.. Val Linf norm: 22.350\n",
            "Epoch 139/146.. Train loss: 60.279.. Val loss: 54.655.. Train L1 norm: 1.907.. Val L1 norm: 1.025.. Train Linf norm: 926.975.. Val Linf norm: 22.350\n",
            "Epoch 140/146.. Train loss: 64.461.. Val loss: 54.656.. Train L1 norm: 2.073.. Val L1 norm: 1.025.. Train Linf norm: 1097.644.. Val Linf norm: 22.351\n",
            "Epoch 141/146.. Train loss: 60.587.. Val loss: 54.656.. Train L1 norm: 2.694.. Val L1 norm: 1.025.. Train Linf norm: 1732.963.. Val Linf norm: 22.353\n",
            "Epoch 142/146.. Train loss: 66.071.. Val loss: 54.654.. Train L1 norm: 1.982.. Val L1 norm: 1.025.. Train Linf norm: 1003.019.. Val Linf norm: 22.347\n",
            "Epoch 143/146.. Train loss: 70.252.. Val loss: 54.653.. Train L1 norm: 2.636.. Val L1 norm: 1.025.. Train Linf norm: 1672.769.. Val Linf norm: 22.351\n",
            "Epoch 144/146.. Train loss: 61.973.. Val loss: 54.653.. Train L1 norm: 1.671.. Val L1 norm: 1.025.. Train Linf norm: 685.224.. Val Linf norm: 22.354\n",
            "Epoch 145/146.. Train loss: 69.344.. Val loss: 54.655.. Train L1 norm: 2.082.. Val L1 norm: 1.025.. Train Linf norm: 1105.498.. Val Linf norm: 22.356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 08:21:25,563]\u001b[0m Trial 56 finished with value: 1.0245833877563477 and parameters: {'n_layers': 6, 'n_units_0': 3694, 'n_units_1': 2063, 'n_units_2': 3537, 'n_units_3': 692, 'n_units_4': 50, 'n_units_5': 1240, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 1.7654921277220958e-06, 'batch_size': 1024, 'n_epochs': 146, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.204972083853437, 'dropout_rate': 0.019571147023664463, 'weight_decay': 0.0006560836954250863, 'beta1': 0.9280963077150483, 'beta2': 0.999144362338129, 'factor': 0.2154685113654854, 'patience': 5, 'threshold': 0.0009941885041754484}. Best is trial 22 with value: 1.0150012904485066.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 146/146.. Train loss: 61.247.. Val loss: 54.655.. Train L1 norm: 1.893.. Val L1 norm: 1.025.. Train Linf norm: 912.543.. Val Linf norm: 22.357\n",
            "Epoch 1/146.. Train loss: 1849.111.. Val loss: 54.871.. Train L1 norm: 2.261.. Val L1 norm: 1.017.. Train Linf norm: 1280.490.. Val Linf norm: 15.349\n",
            "Epoch 2/146.. Train loss: 73.734.. Val loss: 53.900.. Train L1 norm: 2.528.. Val L1 norm: 1.043.. Train Linf norm: 1559.416.. Val Linf norm: 34.032\n",
            "Epoch 3/146.. Train loss: 257.944.. Val loss: 55.293.. Train L1 norm: 2.498.. Val L1 norm: 1.031.. Train Linf norm: 1528.943.. Val Linf norm: 27.288\n",
            "Epoch 4/146.. Train loss: 242.042.. Val loss: 54.523.. Train L1 norm: 1.669.. Val L1 norm: 1.025.. Train Linf norm: 679.953.. Val Linf norm: 21.277\n",
            "Epoch 5/146.. Train loss: 108.825.. Val loss: 54.887.. Train L1 norm: 2.574.. Val L1 norm: 1.027.. Train Linf norm: 1609.114.. Val Linf norm: 25.933\n",
            "Epoch 6/146.. Train loss: 83.443.. Val loss: 54.159.. Train L1 norm: 2.135.. Val L1 norm: 1.032.. Train Linf norm: 1158.837.. Val Linf norm: 26.024\n",
            "Epoch 7/146.. Train loss: 78.705.. Val loss: 54.875.. Train L1 norm: 2.303.. Val L1 norm: 1.026.. Train Linf norm: 1331.305.. Val Linf norm: 24.737\n",
            "Epoch 8/146.. Train loss: 128.760.. Val loss: 54.442.. Train L1 norm: 2.454.. Val L1 norm: 1.028.. Train Linf norm: 1486.982.. Val Linf norm: 23.532\n",
            "Epoch 9/146.. Train loss: 130.399.. Val loss: 54.385.. Train L1 norm: 2.636.. Val L1 norm: 1.029.. Train Linf norm: 1671.280.. Val Linf norm: 24.428\n",
            "Epoch 10/146.. Train loss: 73.419.. Val loss: 54.346.. Train L1 norm: 2.971.. Val L1 norm: 1.031.. Train Linf norm: 2016.440.. Val Linf norm: 25.322\n",
            "Epoch 11/146.. Train loss: 205.943.. Val loss: 54.620.. Train L1 norm: 2.498.. Val L1 norm: 1.027.. Train Linf norm: 1529.485.. Val Linf norm: 24.662\n",
            "Epoch 12/146.. Train loss: 60.776.. Val loss: 54.631.. Train L1 norm: 2.586.. Val L1 norm: 1.027.. Train Linf norm: 1621.910.. Val Linf norm: 24.774\n",
            "Epoch 13/146.. Train loss: 96.593.. Val loss: 54.524.. Train L1 norm: 2.215.. Val L1 norm: 1.028.. Train Linf norm: 1241.211.. Val Linf norm: 24.029\n",
            "Epoch 14/146.. Train loss: 60.233.. Val loss: 54.502.. Train L1 norm: 3.250.. Val L1 norm: 1.028.. Train Linf norm: 2301.466.. Val Linf norm: 24.223\n",
            "Epoch 15/146.. Train loss: 63.079.. Val loss: 54.497.. Train L1 norm: 2.552.. Val L1 norm: 1.028.. Train Linf norm: 1587.062.. Val Linf norm: 24.275\n",
            "Epoch 16/146.. Train loss: 61.641.. Val loss: 54.491.. Train L1 norm: 2.777.. Val L1 norm: 1.028.. Train Linf norm: 1816.670.. Val Linf norm: 24.342\n",
            "Epoch 17/146.. Train loss: 63.557.. Val loss: 54.496.. Train L1 norm: 2.093.. Val L1 norm: 1.028.. Train Linf norm: 1109.767.. Val Linf norm: 24.313\n",
            "Epoch 18/146.. Train loss: 108.132.. Val loss: 54.471.. Train L1 norm: 2.190.. Val L1 norm: 1.029.. Train Linf norm: 1215.487.. Val Linf norm: 24.618\n",
            "Epoch 19/146.. Train loss: 88.910.. Val loss: 54.448.. Train L1 norm: 2.874.. Val L1 norm: 1.030.. Train Linf norm: 1914.779.. Val Linf norm: 25.118\n",
            "Epoch 20/146.. Train loss: 88.967.. Val loss: 54.454.. Train L1 norm: 2.735.. Val L1 norm: 1.030.. Train Linf norm: 1773.128.. Val Linf norm: 25.014\n",
            "Epoch 21/146.. Train loss: 74.548.. Val loss: 54.457.. Train L1 norm: 3.159.. Val L1 norm: 1.029.. Train Linf norm: 2207.811.. Val Linf norm: 24.948\n",
            "Epoch 22/146.. Train loss: 70.965.. Val loss: 54.456.. Train L1 norm: 2.914.. Val L1 norm: 1.030.. Train Linf norm: 1956.428.. Val Linf norm: 24.986\n",
            "Epoch 23/146.. Train loss: 59.937.. Val loss: 54.456.. Train L1 norm: 2.311.. Val L1 norm: 1.030.. Train Linf norm: 1334.997.. Val Linf norm: 24.991\n",
            "Epoch 24/146.. Train loss: 81.569.. Val loss: 54.457.. Train L1 norm: 2.393.. Val L1 norm: 1.030.. Train Linf norm: 1421.969.. Val Linf norm: 24.966\n",
            "Epoch 25/146.. Train loss: 62.501.. Val loss: 54.459.. Train L1 norm: 2.338.. Val L1 norm: 1.029.. Train Linf norm: 1364.759.. Val Linf norm: 24.948\n",
            "Epoch 26/146.. Train loss: 73.724.. Val loss: 54.460.. Train L1 norm: 1.905.. Val L1 norm: 1.029.. Train Linf norm: 922.246.. Val Linf norm: 24.919\n",
            "Epoch 27/146.. Train loss: 82.599.. Val loss: 54.463.. Train L1 norm: 2.568.. Val L1 norm: 1.029.. Train Linf norm: 1602.201.. Val Linf norm: 24.872\n",
            "Epoch 28/146.. Train loss: 115.137.. Val loss: 54.467.. Train L1 norm: 2.495.. Val L1 norm: 1.029.. Train Linf norm: 1524.209.. Val Linf norm: 24.780\n",
            "Epoch 29/146.. Train loss: 65.263.. Val loss: 54.468.. Train L1 norm: 2.527.. Val L1 norm: 1.029.. Train Linf norm: 1560.509.. Val Linf norm: 24.759\n",
            "Epoch 30/146.. Train loss: 203.284.. Val loss: 54.473.. Train L1 norm: 2.658.. Val L1 norm: 1.029.. Train Linf norm: 1695.004.. Val Linf norm: 24.648\n",
            "Epoch 31/146.. Train loss: 65.350.. Val loss: 54.473.. Train L1 norm: 2.698.. Val L1 norm: 1.029.. Train Linf norm: 1728.560.. Val Linf norm: 24.662\n",
            "Epoch 32/146.. Train loss: 75.616.. Val loss: 54.474.. Train L1 norm: 2.852.. Val L1 norm: 1.029.. Train Linf norm: 1892.714.. Val Linf norm: 24.639\n",
            "Epoch 33/146.. Train loss: 63.582.. Val loss: 54.474.. Train L1 norm: 2.608.. Val L1 norm: 1.029.. Train Linf norm: 1642.663.. Val Linf norm: 24.649\n",
            "Epoch 34/146.. Train loss: 70.482.. Val loss: 54.470.. Train L1 norm: 2.437.. Val L1 norm: 1.029.. Train Linf norm: 1466.338.. Val Linf norm: 24.721\n",
            "Epoch 35/146.. Train loss: 75.593.. Val loss: 54.472.. Train L1 norm: 2.473.. Val L1 norm: 1.029.. Train Linf norm: 1505.305.. Val Linf norm: 24.676\n",
            "Epoch 36/146.. Train loss: 68.095.. Val loss: 54.474.. Train L1 norm: 2.385.. Val L1 norm: 1.029.. Train Linf norm: 1413.512.. Val Linf norm: 24.653\n",
            "Epoch 37/146.. Train loss: 78.018.. Val loss: 54.477.. Train L1 norm: 2.503.. Val L1 norm: 1.029.. Train Linf norm: 1535.631.. Val Linf norm: 24.589\n",
            "Epoch 38/146.. Train loss: 61.505.. Val loss: 54.477.. Train L1 norm: 2.871.. Val L1 norm: 1.029.. Train Linf norm: 1912.926.. Val Linf norm: 24.598\n",
            "Epoch 39/146.. Train loss: 60.421.. Val loss: 54.476.. Train L1 norm: 2.301.. Val L1 norm: 1.029.. Train Linf norm: 1328.305.. Val Linf norm: 24.613\n",
            "Epoch 40/146.. Train loss: 61.778.. Val loss: 54.477.. Train L1 norm: 2.357.. Val L1 norm: 1.029.. Train Linf norm: 1386.521.. Val Linf norm: 24.604\n",
            "Epoch 41/146.. Train loss: 126.974.. Val loss: 54.479.. Train L1 norm: 2.580.. Val L1 norm: 1.029.. Train Linf norm: 1613.759.. Val Linf norm: 24.574\n",
            "Epoch 42/146.. Train loss: 75.643.. Val loss: 54.486.. Train L1 norm: 3.199.. Val L1 norm: 1.029.. Train Linf norm: 2248.982.. Val Linf norm: 24.469\n",
            "Epoch 43/146.. Train loss: 60.024.. Val loss: 54.486.. Train L1 norm: 2.533.. Val L1 norm: 1.029.. Train Linf norm: 1566.243.. Val Linf norm: 24.474\n",
            "Epoch 44/146.. Train loss: 73.552.. Val loss: 54.483.. Train L1 norm: 2.436.. Val L1 norm: 1.029.. Train Linf norm: 1467.861.. Val Linf norm: 24.496\n",
            "Epoch 45/146.. Train loss: 61.144.. Val loss: 54.483.. Train L1 norm: 2.393.. Val L1 norm: 1.029.. Train Linf norm: 1423.344.. Val Linf norm: 24.498\n",
            "Epoch 46/146.. Train loss: 115.190.. Val loss: 54.486.. Train L1 norm: 2.895.. Val L1 norm: 1.029.. Train Linf norm: 1935.706.. Val Linf norm: 24.476\n",
            "Epoch 47/146.. Train loss: 61.902.. Val loss: 54.491.. Train L1 norm: 2.736.. Val L1 norm: 1.029.. Train Linf norm: 1772.524.. Val Linf norm: 24.434\n",
            "Epoch 48/146.. Train loss: 62.731.. Val loss: 54.492.. Train L1 norm: 3.013.. Val L1 norm: 1.029.. Train Linf norm: 2055.835.. Val Linf norm: 24.430\n",
            "Epoch 49/146.. Train loss: 69.159.. Val loss: 54.494.. Train L1 norm: 2.476.. Val L1 norm: 1.028.. Train Linf norm: 1504.027.. Val Linf norm: 24.415\n",
            "Epoch 50/146.. Train loss: 96.242.. Val loss: 54.499.. Train L1 norm: 2.897.. Val L1 norm: 1.028.. Train Linf norm: 1937.399.. Val Linf norm: 24.386\n",
            "Epoch 51/146.. Train loss: 60.696.. Val loss: 54.499.. Train L1 norm: 1.611.. Val L1 norm: 1.028.. Train Linf norm: 622.017.. Val Linf norm: 24.387\n",
            "Epoch 52/146.. Train loss: 69.736.. Val loss: 54.500.. Train L1 norm: 2.778.. Val L1 norm: 1.028.. Train Linf norm: 1811.446.. Val Linf norm: 24.386\n",
            "Epoch 53/146.. Train loss: 82.406.. Val loss: 54.502.. Train L1 norm: 2.586.. Val L1 norm: 1.028.. Train Linf norm: 1619.228.. Val Linf norm: 24.365\n",
            "Epoch 54/146.. Train loss: 89.055.. Val loss: 54.505.. Train L1 norm: 2.575.. Val L1 norm: 1.028.. Train Linf norm: 1609.219.. Val Linf norm: 24.343\n",
            "Epoch 55/146.. Train loss: 70.261.. Val loss: 54.508.. Train L1 norm: 2.979.. Val L1 norm: 1.028.. Train Linf norm: 2022.527.. Val Linf norm: 24.323\n",
            "Epoch 56/146.. Train loss: 65.058.. Val loss: 54.510.. Train L1 norm: 1.829.. Val L1 norm: 1.028.. Train Linf norm: 845.843.. Val Linf norm: 24.311\n",
            "Epoch 57/146.. Train loss: 62.093.. Val loss: 54.511.. Train L1 norm: 2.769.. Val L1 norm: 1.028.. Train Linf norm: 1808.587.. Val Linf norm: 24.307\n",
            "Epoch 58/146.. Train loss: 90.133.. Val loss: 54.515.. Train L1 norm: 3.042.. Val L1 norm: 1.028.. Train Linf norm: 2086.618.. Val Linf norm: 24.275\n",
            "Epoch 59/146.. Train loss: 64.742.. Val loss: 54.515.. Train L1 norm: 2.763.. Val L1 norm: 1.028.. Train Linf norm: 1801.660.. Val Linf norm: 24.280\n",
            "Epoch 60/146.. Train loss: 75.467.. Val loss: 54.512.. Train L1 norm: 2.797.. Val L1 norm: 1.028.. Train Linf norm: 1836.168.. Val Linf norm: 24.307\n",
            "Epoch 61/146.. Train loss: 85.609.. Val loss: 54.516.. Train L1 norm: 2.398.. Val L1 norm: 1.028.. Train Linf norm: 1428.332.. Val Linf norm: 24.286\n",
            "Epoch 62/146.. Train loss: 64.208.. Val loss: 54.517.. Train L1 norm: 3.012.. Val L1 norm: 1.028.. Train Linf norm: 2055.245.. Val Linf norm: 24.281\n",
            "Epoch 63/146.. Train loss: 90.015.. Val loss: 54.512.. Train L1 norm: 2.029.. Val L1 norm: 1.028.. Train Linf norm: 1049.465.. Val Linf norm: 24.326\n",
            "Epoch 64/146.. Train loss: 76.230.. Val loss: 54.509.. Train L1 norm: 1.774.. Val L1 norm: 1.028.. Train Linf norm: 788.986.. Val Linf norm: 24.354\n",
            "Epoch 65/146.. Train loss: 60.438.. Val loss: 54.508.. Train L1 norm: 2.072.. Val L1 norm: 1.028.. Train Linf norm: 1092.292.. Val Linf norm: 24.365\n",
            "Epoch 66/146.. Train loss: 64.808.. Val loss: 54.510.. Train L1 norm: 2.322.. Val L1 norm: 1.028.. Train Linf norm: 1348.856.. Val Linf norm: 24.354\n",
            "Epoch 67/146.. Train loss: 88.919.. Val loss: 54.515.. Train L1 norm: 2.494.. Val L1 norm: 1.028.. Train Linf norm: 1525.688.. Val Linf norm: 24.314\n",
            "Epoch 68/146.. Train loss: 68.545.. Val loss: 54.518.. Train L1 norm: 2.789.. Val L1 norm: 1.028.. Train Linf norm: 1828.757.. Val Linf norm: 24.293\n",
            "Epoch 69/146.. Train loss: 64.944.. Val loss: 54.515.. Train L1 norm: 2.449.. Val L1 norm: 1.028.. Train Linf norm: 1479.326.. Val Linf norm: 24.319\n",
            "Epoch 70/146.. Train loss: 69.910.. Val loss: 54.518.. Train L1 norm: 2.363.. Val L1 norm: 1.028.. Train Linf norm: 515.447.. Val Linf norm: 24.295\n",
            "Epoch 71/146.. Train loss: 64.823.. Val loss: 54.517.. Train L1 norm: 1.969.. Val L1 norm: 1.028.. Train Linf norm: 987.820.. Val Linf norm: 24.312\n",
            "Epoch 72/146.. Train loss: 59.995.. Val loss: 54.516.. Train L1 norm: 3.230.. Val L1 norm: 1.028.. Train Linf norm: 2279.843.. Val Linf norm: 24.322\n",
            "Epoch 73/146.. Train loss: 69.673.. Val loss: 54.519.. Train L1 norm: 1.901.. Val L1 norm: 1.028.. Train Linf norm: 919.322.. Val Linf norm: 24.304\n",
            "Epoch 74/146.. Train loss: 60.057.. Val loss: 54.518.. Train L1 norm: 2.534.. Val L1 norm: 1.028.. Train Linf norm: 1567.137.. Val Linf norm: 24.309\n",
            "Epoch 75/146.. Train loss: 61.377.. Val loss: 54.519.. Train L1 norm: 2.600.. Val L1 norm: 1.028.. Train Linf norm: 1634.695.. Val Linf norm: 24.306\n",
            "Epoch 76/146.. Train loss: 101.659.. Val loss: 54.513.. Train L1 norm: 2.469.. Val L1 norm: 1.028.. Train Linf norm: 1500.352.. Val Linf norm: 24.358\n",
            "Epoch 77/146.. Train loss: 70.238.. Val loss: 54.511.. Train L1 norm: 2.119.. Val L1 norm: 1.028.. Train Linf norm: 1143.654.. Val Linf norm: 24.380\n",
            "Epoch 78/146.. Train loss: 84.258.. Val loss: 54.510.. Train L1 norm: 1.086.. Val L1 norm: 1.028.. Train Linf norm: 78.130.. Val Linf norm: 24.390\n",
            "Epoch 79/146.. Train loss: 64.332.. Val loss: 54.512.. Train L1 norm: 2.927.. Val L1 norm: 1.028.. Train Linf norm: 1969.816.. Val Linf norm: 24.385\n",
            "Epoch 80/146.. Train loss: 69.161.. Val loss: 54.515.. Train L1 norm: 2.534.. Val L1 norm: 1.028.. Train Linf norm: 1563.336.. Val Linf norm: 24.366\n",
            "Epoch 81/146.. Train loss: 62.859.. Val loss: 54.517.. Train L1 norm: 2.595.. Val L1 norm: 1.028.. Train Linf norm: 1630.360.. Val Linf norm: 24.356\n",
            "Epoch 82/146.. Train loss: 74.603.. Val loss: 54.522.. Train L1 norm: 2.373.. Val L1 norm: 1.028.. Train Linf norm: 1402.044.. Val Linf norm: 24.323\n",
            "Epoch 83/146.. Train loss: 60.959.. Val loss: 54.523.. Train L1 norm: 2.760.. Val L1 norm: 1.028.. Train Linf norm: 1799.303.. Val Linf norm: 24.318\n",
            "Epoch 84/146.. Train loss: 141.562.. Val loss: 54.531.. Train L1 norm: 3.179.. Val L1 norm: 1.028.. Train Linf norm: 2227.643.. Val Linf norm: 24.264\n",
            "Epoch 85/146.. Train loss: 60.317.. Val loss: 54.531.. Train L1 norm: 2.019.. Val L1 norm: 1.028.. Train Linf norm: 1040.086.. Val Linf norm: 24.266\n",
            "Epoch 86/146.. Train loss: 176.229.. Val loss: 54.522.. Train L1 norm: 2.944.. Val L1 norm: 1.028.. Train Linf norm: 1986.647.. Val Linf norm: 24.338\n",
            "Epoch 87/146.. Train loss: 101.499.. Val loss: 54.527.. Train L1 norm: 2.154.. Val L1 norm: 1.028.. Train Linf norm: 1178.085.. Val Linf norm: 24.301\n",
            "Epoch 88/146.. Train loss: 65.671.. Val loss: 54.525.. Train L1 norm: 2.809.. Val L1 norm: 1.028.. Train Linf norm: 1838.698.. Val Linf norm: 24.327\n",
            "Epoch 89/146.. Train loss: 60.097.. Val loss: 54.524.. Train L1 norm: 2.555.. Val L1 norm: 1.028.. Train Linf norm: 1588.190.. Val Linf norm: 24.337\n",
            "Epoch 90/146.. Train loss: 181.546.. Val loss: 54.533.. Train L1 norm: 2.549.. Val L1 norm: 1.028.. Train Linf norm: 1582.895.. Val Linf norm: 24.299\n",
            "Epoch 91/146.. Train loss: 60.475.. Val loss: 54.533.. Train L1 norm: 2.314.. Val L1 norm: 1.028.. Train Linf norm: 1341.370.. Val Linf norm: 24.299\n",
            "Epoch 92/146.. Train loss: 70.326.. Val loss: 54.535.. Train L1 norm: 2.591.. Val L1 norm: 1.028.. Train Linf norm: 1626.932.. Val Linf norm: 24.314\n",
            "Epoch 93/146.. Train loss: 60.770.. Val loss: 54.535.. Train L1 norm: 2.987.. Val L1 norm: 1.028.. Train Linf norm: 2031.156.. Val Linf norm: 24.313\n",
            "Epoch 94/146.. Train loss: 62.871.. Val loss: 54.533.. Train L1 norm: 2.435.. Val L1 norm: 1.028.. Train Linf norm: 1465.058.. Val Linf norm: 24.307\n",
            "Epoch 95/146.. Train loss: 83.625.. Val loss: 54.528.. Train L1 norm: 2.553.. Val L1 norm: 1.028.. Train Linf norm: 1588.083.. Val Linf norm: 24.312\n",
            "Epoch 96/146.. Train loss: 62.121.. Val loss: 54.528.. Train L1 norm: 2.651.. Val L1 norm: 1.028.. Train Linf norm: 1686.300.. Val Linf norm: 24.320\n",
            "Epoch 97/146.. Train loss: 73.983.. Val loss: 54.530.. Train L1 norm: 2.347.. Val L1 norm: 1.028.. Train Linf norm: 1375.961.. Val Linf norm: 24.309\n",
            "Epoch 98/146.. Train loss: 70.530.. Val loss: 54.534.. Train L1 norm: 3.307.. Val L1 norm: 1.028.. Train Linf norm: 2359.287.. Val Linf norm: 24.336\n",
            "Epoch 99/146.. Train loss: 63.086.. Val loss: 54.535.. Train L1 norm: 2.522.. Val L1 norm: 1.028.. Train Linf norm: 1555.082.. Val Linf norm: 24.343\n",
            "Epoch 100/146.. Train loss: 98.273.. Val loss: 54.529.. Train L1 norm: 2.436.. Val L1 norm: 1.028.. Train Linf norm: 1466.729.. Val Linf norm: 24.325\n",
            "Epoch 101/146.. Train loss: 69.693.. Val loss: 54.531.. Train L1 norm: 2.216.. Val L1 norm: 1.028.. Train Linf norm: 1241.320.. Val Linf norm: 24.317\n",
            "Epoch 102/146.. Train loss: 66.470.. Val loss: 54.532.. Train L1 norm: 1.555.. Val L1 norm: 1.028.. Train Linf norm: 565.675.. Val Linf norm: 24.326\n",
            "Epoch 103/146.. Train loss: 75.822.. Val loss: 54.530.. Train L1 norm: 2.843.. Val L1 norm: 1.028.. Train Linf norm: 1883.467.. Val Linf norm: 24.317\n",
            "Epoch 104/146.. Train loss: 89.667.. Val loss: 54.534.. Train L1 norm: 2.720.. Val L1 norm: 1.028.. Train Linf norm: 1757.405.. Val Linf norm: 24.335\n",
            "Epoch 105/146.. Train loss: 64.909.. Val loss: 54.532.. Train L1 norm: 2.362.. Val L1 norm: 1.028.. Train Linf norm: 1383.512.. Val Linf norm: 24.337\n",
            "Epoch 106/146.. Train loss: 79.093.. Val loss: 54.535.. Train L1 norm: 2.857.. Val L1 norm: 1.028.. Train Linf norm: 1896.185.. Val Linf norm: 24.354\n",
            "Epoch 107/146.. Train loss: 90.032.. Val loss: 54.540.. Train L1 norm: 3.155.. Val L1 norm: 1.028.. Train Linf norm: 2202.592.. Val Linf norm: 24.389\n",
            "Epoch 108/146.. Train loss: 118.804.. Val loss: 54.549.. Train L1 norm: 2.737.. Val L1 norm: 1.028.. Train Linf norm: 1774.287.. Val Linf norm: 24.441\n",
            "Epoch 109/146.. Train loss: 61.148.. Val loss: 54.548.. Train L1 norm: 2.109.. Val L1 norm: 1.028.. Train Linf norm: 1131.430.. Val Linf norm: 24.443\n",
            "Epoch 110/146.. Train loss: 67.482.. Val loss: 54.546.. Train L1 norm: 2.313.. Val L1 norm: 1.028.. Train Linf norm: 1341.364.. Val Linf norm: 24.439\n",
            "Epoch 111/146.. Train loss: 64.452.. Val loss: 54.543.. Train L1 norm: 1.203.. Val L1 norm: 1.028.. Train Linf norm: 203.719.. Val Linf norm: 24.432\n",
            "Epoch 112/146.. Train loss: 86.100.. Val loss: 54.549.. Train L1 norm: 2.633.. Val L1 norm: 1.028.. Train Linf norm: 1666.783.. Val Linf norm: 24.479\n",
            "Epoch 113/146.. Train loss: 72.874.. Val loss: 54.545.. Train L1 norm: 2.466.. Val L1 norm: 1.028.. Train Linf norm: 1496.522.. Val Linf norm: 24.465\n",
            "Epoch 114/146.. Train loss: 61.757.. Val loss: 54.546.. Train L1 norm: 2.094.. Val L1 norm: 1.028.. Train Linf norm: 1116.027.. Val Linf norm: 24.471\n",
            "Epoch 115/146.. Train loss: 68.913.. Val loss: 54.548.. Train L1 norm: 2.375.. Val L1 norm: 1.028.. Train Linf norm: 1404.069.. Val Linf norm: 24.491\n",
            "Epoch 116/146.. Train loss: 71.932.. Val loss: 54.552.. Train L1 norm: 2.449.. Val L1 norm: 1.028.. Train Linf norm: 1480.279.. Val Linf norm: 24.513\n",
            "Epoch 117/146.. Train loss: 61.010.. Val loss: 54.553.. Train L1 norm: 2.730.. Val L1 norm: 1.028.. Train Linf norm: 1769.116.. Val Linf norm: 24.521\n",
            "Epoch 118/146.. Train loss: 60.431.. Val loss: 54.553.. Train L1 norm: 2.035.. Val L1 norm: 1.028.. Train Linf norm: 1057.140.. Val Linf norm: 24.522\n",
            "Epoch 119/146.. Train loss: 61.306.. Val loss: 54.553.. Train L1 norm: 2.428.. Val L1 norm: 1.028.. Train Linf norm: 1458.796.. Val Linf norm: 24.530\n",
            "Epoch 120/146.. Train loss: 60.151.. Val loss: 54.553.. Train L1 norm: 2.647.. Val L1 norm: 1.028.. Train Linf norm: 1682.746.. Val Linf norm: 24.534\n",
            "Epoch 121/146.. Train loss: 60.001.. Val loss: 54.553.. Train L1 norm: 2.688.. Val L1 norm: 1.028.. Train Linf norm: 1725.825.. Val Linf norm: 24.534\n",
            "Epoch 122/146.. Train loss: 75.977.. Val loss: 54.548.. Train L1 norm: 3.204.. Val L1 norm: 1.028.. Train Linf norm: 2254.385.. Val Linf norm: 24.520\n",
            "Epoch 123/146.. Train loss: 70.533.. Val loss: 54.551.. Train L1 norm: 2.674.. Val L1 norm: 1.028.. Train Linf norm: 1710.934.. Val Linf norm: 24.528\n",
            "Epoch 124/146.. Train loss: 81.250.. Val loss: 54.546.. Train L1 norm: 2.478.. Val L1 norm: 1.028.. Train Linf norm: 1510.011.. Val Linf norm: 24.513\n",
            "Epoch 125/146.. Train loss: 67.820.. Val loss: 54.549.. Train L1 norm: 2.645.. Val L1 norm: 1.028.. Train Linf norm: 1680.662.. Val Linf norm: 24.528\n",
            "Epoch 126/146.. Train loss: 60.427.. Val loss: 54.549.. Train L1 norm: 2.115.. Val L1 norm: 1.028.. Train Linf norm: 1138.150.. Val Linf norm: 24.531\n",
            "Epoch 127/146.. Train loss: 108.649.. Val loss: 54.542.. Train L1 norm: 2.829.. Val L1 norm: 1.028.. Train Linf norm: 1870.491.. Val Linf norm: 24.490\n",
            "Epoch 128/146.. Train loss: 104.098.. Val loss: 54.546.. Train L1 norm: 2.296.. Val L1 norm: 1.028.. Train Linf norm: 1323.543.. Val Linf norm: 24.507\n",
            "Epoch 129/146.. Train loss: 60.281.. Val loss: 54.550.. Train L1 norm: 2.308.. Val L1 norm: 1.028.. Train Linf norm: 1336.504.. Val Linf norm: 24.529\n",
            "Epoch 130/146.. Train loss: 95.465.. Val loss: 54.543.. Train L1 norm: 2.251.. Val L1 norm: 1.028.. Train Linf norm: 1276.024.. Val Linf norm: 24.494\n",
            "Epoch 131/146.. Train loss: 60.131.. Val loss: 54.543.. Train L1 norm: 2.263.. Val L1 norm: 1.028.. Train Linf norm: 1289.832.. Val Linf norm: 24.496\n",
            "Epoch 132/146.. Train loss: 61.162.. Val loss: 54.544.. Train L1 norm: 2.182.. Val L1 norm: 1.028.. Train Linf norm: 1207.213.. Val Linf norm: 24.501\n",
            "Epoch 133/146.. Train loss: 69.074.. Val loss: 54.548.. Train L1 norm: 3.107.. Val L1 norm: 1.028.. Train Linf norm: 2154.129.. Val Linf norm: 24.534\n",
            "Epoch 134/146.. Train loss: 60.190.. Val loss: 54.548.. Train L1 norm: 2.538.. Val L1 norm: 1.028.. Train Linf norm: 1571.991.. Val Linf norm: 24.538\n",
            "Epoch 135/146.. Train loss: 76.796.. Val loss: 54.549.. Train L1 norm: 2.701.. Val L1 norm: 1.028.. Train Linf norm: 1738.750.. Val Linf norm: 24.545\n",
            "Epoch 136/146.. Train loss: 68.957.. Val loss: 54.557.. Train L1 norm: 2.426.. Val L1 norm: 1.028.. Train Linf norm: 1457.202.. Val Linf norm: 24.570\n",
            "Epoch 137/146.. Train loss: 73.013.. Val loss: 54.552.. Train L1 norm: 2.925.. Val L1 norm: 1.028.. Train Linf norm: 1966.932.. Val Linf norm: 24.549\n",
            "Epoch 138/146.. Train loss: 86.995.. Val loss: 54.546.. Train L1 norm: 2.250.. Val L1 norm: 1.028.. Train Linf norm: 1276.749.. Val Linf norm: 24.521\n",
            "Epoch 139/146.. Train loss: 73.612.. Val loss: 54.550.. Train L1 norm: 2.532.. Val L1 norm: 1.028.. Train Linf norm: 1566.488.. Val Linf norm: 24.538\n",
            "Epoch 140/146.. Train loss: 60.048.. Val loss: 54.550.. Train L1 norm: 2.758.. Val L1 norm: 1.028.. Train Linf norm: 1797.452.. Val Linf norm: 24.541\n",
            "Epoch 141/146.. Train loss: 63.670.. Val loss: 54.550.. Train L1 norm: 1.682.. Val L1 norm: 1.028.. Train Linf norm: 692.948.. Val Linf norm: 24.549\n",
            "Epoch 142/146.. Train loss: 66.907.. Val loss: 54.555.. Train L1 norm: 2.496.. Val L1 norm: 1.028.. Train Linf norm: 1528.894.. Val Linf norm: 24.583\n",
            "Epoch 143/146.. Train loss: 60.205.. Val loss: 54.555.. Train L1 norm: 2.615.. Val L1 norm: 1.028.. Train Linf norm: 1649.650.. Val Linf norm: 24.588\n",
            "Epoch 144/146.. Train loss: 154.354.. Val loss: 54.561.. Train L1 norm: 3.986.. Val L1 norm: 1.028.. Train Linf norm: 3054.189.. Val Linf norm: 24.614\n",
            "Epoch 145/146.. Train loss: 60.933.. Val loss: 54.560.. Train L1 norm: 2.752.. Val L1 norm: 1.028.. Train Linf norm: 1790.731.. Val Linf norm: 24.616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 08:29:13,776]\u001b[0m Trial 57 finished with value: 1.0281715446790058 and parameters: {'n_layers': 6, 'n_units_0': 3623, 'n_units_1': 2179, 'n_units_2': 3520, 'n_units_3': 395, 'n_units_4': 53, 'n_units_5': 1227, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 1.968439577968984e-06, 'batch_size': 1024, 'n_epochs': 146, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.20705955598218942, 'dropout_rate': 0.019665980427407977, 'weight_decay': 0.0005480983879349226, 'beta1': 0.9262194746712544, 'beta2': 0.9991264750138273, 'factor': 0.1356701004094824, 'patience': 5, 'threshold': 0.0009385330115639749}. Best is trial 22 with value: 1.0150012904485066.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 146/146.. Train loss: 63.409.. Val loss: 54.562.. Train L1 norm: 2.767.. Val L1 norm: 1.028.. Train Linf norm: 1805.005.. Val Linf norm: 24.625\n",
            "Epoch 1/146.. Train loss: 1045.521.. Val loss: 55.534.. Train L1 norm: 1.632.. Val L1 norm: 1.035.. Train Linf norm: 640.775.. Val Linf norm: 30.630\n",
            "Epoch 2/146.. Train loss: 65.175.. Val loss: 54.728.. Train L1 norm: 2.373.. Val L1 norm: 1.031.. Train Linf norm: 1401.385.. Val Linf norm: 26.156\n",
            "Epoch 3/146.. Train loss: 61.770.. Val loss: 54.102.. Train L1 norm: 3.158.. Val L1 norm: 1.049.. Train Linf norm: 2203.243.. Val Linf norm: 37.091\n",
            "Epoch 4/146.. Train loss: 226.044.. Val loss: 56.600.. Train L1 norm: 3.046.. Val L1 norm: 1.070.. Train Linf norm: 2081.554.. Val Linf norm: 53.990\n",
            "Epoch 5/146.. Train loss: 1249.273.. Val loss: 52.571.. Train L1 norm: 2.280.. Val L1 norm: 1.120.. Train Linf norm: 1286.771.. Val Linf norm: 82.769\n",
            "Epoch 6/146.. Train loss: 820.075.. Val loss: 55.248.. Train L1 norm: 2.817.. Val L1 norm: 1.024.. Train Linf norm: 1841.929.. Val Linf norm: 21.071\n",
            "Epoch 7/146.. Train loss: 66.861.. Val loss: 54.998.. Train L1 norm: 2.339.. Val L1 norm: 1.030.. Train Linf norm: 1367.812.. Val Linf norm: 26.650\n",
            "Epoch 8/146.. Train loss: 114.821.. Val loss: 56.004.. Train L1 norm: 1.257.. Val L1 norm: 1.037.. Train Linf norm: 255.706.. Val Linf norm: 30.137\n",
            "Epoch 9/146.. Train loss: 727.009.. Val loss: 54.777.. Train L1 norm: 1.172.. Val L1 norm: 1.039.. Train Linf norm: 168.416.. Val Linf norm: 33.367\n",
            "Epoch 10/146.. Train loss: 1080.025.. Val loss: 54.502.. Train L1 norm: 3.225.. Val L1 norm: 1.050.. Train Linf norm: 2244.786.. Val Linf norm: 40.454\n",
            "Epoch 11/146.. Train loss: 208.003.. Val loss: 54.729.. Train L1 norm: 2.208.. Val L1 norm: 1.047.. Train Linf norm: 1232.405.. Val Linf norm: 38.557\n",
            "Epoch 12/146.. Train loss: 164.631.. Val loss: 55.218.. Train L1 norm: 3.413.. Val L1 norm: 1.035.. Train Linf norm: 2459.881.. Val Linf norm: 30.196\n",
            "Epoch 13/146.. Train loss: 215.913.. Val loss: 55.045.. Train L1 norm: 2.282.. Val L1 norm: 1.040.. Train Linf norm: 1306.157.. Val Linf norm: 33.994\n",
            "Epoch 14/146.. Train loss: 95.593.. Val loss: 54.981.. Train L1 norm: 2.615.. Val L1 norm: 1.042.. Train Linf norm: 1645.067.. Val Linf norm: 35.459\n",
            "Epoch 15/146.. Train loss: 221.794.. Val loss: 55.001.. Train L1 norm: 2.665.. Val L1 norm: 1.041.. Train Linf norm: 1698.607.. Val Linf norm: 35.029\n",
            "Epoch 16/146.. Train loss: 68.677.. Val loss: 55.086.. Train L1 norm: 2.055.. Val L1 norm: 1.039.. Train Linf norm: 1072.944.. Val Linf norm: 33.144\n",
            "Epoch 17/146.. Train loss: 89.456.. Val loss: 55.156.. Train L1 norm: 1.118.. Val L1 norm: 1.037.. Train Linf norm: 114.585.. Val Linf norm: 31.860\n",
            "Epoch 18/146.. Train loss: 242.806.. Val loss: 54.961.. Train L1 norm: 2.232.. Val L1 norm: 1.042.. Train Linf norm: 1254.969.. Val Linf norm: 35.814\n",
            "Epoch 19/146.. Train loss: 119.198.. Val loss: 55.049.. Train L1 norm: 2.365.. Val L1 norm: 1.040.. Train Linf norm: 1391.165.. Val Linf norm: 34.145\n",
            "Epoch 20/146.. Train loss: 181.086.. Val loss: 55.025.. Train L1 norm: 2.229.. Val L1 norm: 1.041.. Train Linf norm: 1251.392.. Val Linf norm: 34.674\n",
            "Epoch 21/146.. Train loss: 159.790.. Val loss: 55.003.. Train L1 norm: 1.701.. Val L1 norm: 1.041.. Train Linf norm: 712.503.. Val Linf norm: 35.160\n",
            "Epoch 22/146.. Train loss: 103.727.. Val loss: 54.996.. Train L1 norm: 2.172.. Val L1 norm: 1.042.. Train Linf norm: 1191.894.. Val Linf norm: 35.335\n",
            "Epoch 23/146.. Train loss: 59.962.. Val loss: 54.986.. Train L1 norm: 2.220.. Val L1 norm: 1.042.. Train Linf norm: 1241.658.. Val Linf norm: 35.575\n",
            "Epoch 24/146.. Train loss: 91.807.. Val loss: 54.970.. Train L1 norm: 2.177.. Val L1 norm: 1.043.. Train Linf norm: 1198.509.. Val Linf norm: 35.929\n",
            "Epoch 25/146.. Train loss: 95.559.. Val loss: 54.955.. Train L1 norm: 2.913.. Val L1 norm: 1.043.. Train Linf norm: 1951.383.. Val Linf norm: 36.280\n",
            "Epoch 26/146.. Train loss: 90.931.. Val loss: 54.938.. Train L1 norm: 2.649.. Val L1 norm: 1.044.. Train Linf norm: 1681.241.. Val Linf norm: 36.642\n",
            "Epoch 27/146.. Train loss: 67.073.. Val loss: 54.938.. Train L1 norm: 2.963.. Val L1 norm: 1.044.. Train Linf norm: 2003.275.. Val Linf norm: 36.653\n",
            "Epoch 28/146.. Train loss: 60.156.. Val loss: 54.938.. Train L1 norm: 3.165.. Val L1 norm: 1.044.. Train Linf norm: 2209.871.. Val Linf norm: 36.659\n",
            "Epoch 29/146.. Train loss: 209.558.. Val loss: 54.934.. Train L1 norm: 1.772.. Val L1 norm: 1.044.. Train Linf norm: 777.756.. Val Linf norm: 36.738\n",
            "Epoch 30/146.. Train loss: 60.257.. Val loss: 54.934.. Train L1 norm: 3.450.. Val L1 norm: 1.044.. Train Linf norm: 2500.728.. Val Linf norm: 36.742\n",
            "Epoch 31/146.. Train loss: 64.871.. Val loss: 54.934.. Train L1 norm: 2.860.. Val L1 norm: 1.044.. Train Linf norm: 1896.499.. Val Linf norm: 36.740\n",
            "Epoch 32/146.. Train loss: 64.165.. Val loss: 54.934.. Train L1 norm: 2.678.. Val L1 norm: 1.044.. Train Linf norm: 1708.662.. Val Linf norm: 36.746\n",
            "Epoch 33/146.. Train loss: 96.617.. Val loss: 54.932.. Train L1 norm: 3.356.. Val L1 norm: 1.044.. Train Linf norm: 2405.639.. Val Linf norm: 36.785\n",
            "Epoch 34/146.. Train loss: 60.359.. Val loss: 54.932.. Train L1 norm: 3.111.. Val L1 norm: 1.044.. Train Linf norm: 2154.369.. Val Linf norm: 36.790\n",
            "Epoch 35/146.. Train loss: 129.216.. Val loss: 54.932.. Train L1 norm: 2.598.. Val L1 norm: 1.044.. Train Linf norm: 1629.239.. Val Linf norm: 36.808\n",
            "Epoch 36/146.. Train loss: 66.800.. Val loss: 54.929.. Train L1 norm: 2.864.. Val L1 norm: 1.044.. Train Linf norm: 1900.789.. Val Linf norm: 36.859\n",
            "Epoch 37/146.. Train loss: 61.865.. Val loss: 54.929.. Train L1 norm: 2.601.. Val L1 norm: 1.044.. Train Linf norm: 1631.111.. Val Linf norm: 36.859\n",
            "Epoch 38/146.. Train loss: 144.833.. Val loss: 54.926.. Train L1 norm: 2.144.. Val L1 norm: 1.044.. Train Linf norm: 1164.682.. Val Linf norm: 36.932\n",
            "Epoch 39/146.. Train loss: 71.137.. Val loss: 54.927.. Train L1 norm: 2.596.. Val L1 norm: 1.044.. Train Linf norm: 1626.191.. Val Linf norm: 36.921\n",
            "Epoch 40/146.. Train loss: 144.573.. Val loss: 54.923.. Train L1 norm: 2.595.. Val L1 norm: 1.044.. Train Linf norm: 1626.689.. Val Linf norm: 36.990\n",
            "Epoch 41/146.. Train loss: 64.501.. Val loss: 54.923.. Train L1 norm: 1.832.. Val L1 norm: 1.044.. Train Linf norm: 844.910.. Val Linf norm: 37.004\n",
            "Epoch 42/146.. Train loss: 66.267.. Val loss: 54.923.. Train L1 norm: 2.835.. Val L1 norm: 1.044.. Train Linf norm: 1870.472.. Val Linf norm: 36.992\n",
            "Epoch 43/146.. Train loss: 131.555.. Val loss: 54.923.. Train L1 norm: 2.160.. Val L1 norm: 1.044.. Train Linf norm: 1179.188.. Val Linf norm: 37.015\n",
            "Epoch 44/146.. Train loss: 83.251.. Val loss: 54.922.. Train L1 norm: 2.455.. Val L1 norm: 1.044.. Train Linf norm: 1481.875.. Val Linf norm: 37.032\n",
            "Epoch 45/146.. Train loss: 80.118.. Val loss: 54.923.. Train L1 norm: 2.931.. Val L1 norm: 1.044.. Train Linf norm: 1968.839.. Val Linf norm: 37.009\n",
            "Epoch 46/146.. Train loss: 185.648.. Val loss: 54.927.. Train L1 norm: 2.476.. Val L1 norm: 1.044.. Train Linf norm: 1504.633.. Val Linf norm: 36.924\n",
            "Epoch 47/146.. Train loss: 63.003.. Val loss: 54.927.. Train L1 norm: 3.297.. Val L1 norm: 1.044.. Train Linf norm: 2344.991.. Val Linf norm: 36.935\n",
            "Epoch 48/146.. Train loss: 113.391.. Val loss: 54.930.. Train L1 norm: 2.554.. Val L1 norm: 1.044.. Train Linf norm: 1583.100.. Val Linf norm: 36.881\n",
            "Epoch 49/146.. Train loss: 79.507.. Val loss: 54.929.. Train L1 norm: 2.579.. Val L1 norm: 1.044.. Train Linf norm: 1609.543.. Val Linf norm: 36.901\n",
            "Epoch 50/146.. Train loss: 83.254.. Val loss: 54.931.. Train L1 norm: 2.665.. Val L1 norm: 1.044.. Train Linf norm: 1696.012.. Val Linf norm: 36.864\n",
            "Epoch 51/146.. Train loss: 186.158.. Val loss: 54.934.. Train L1 norm: 2.320.. Val L1 norm: 1.044.. Train Linf norm: 1344.291.. Val Linf norm: 36.797\n",
            "Epoch 52/146.. Train loss: 60.605.. Val loss: 54.936.. Train L1 norm: 1.485.. Val L1 norm: 1.044.. Train Linf norm: 490.505.. Val Linf norm: 36.758\n",
            "Epoch 53/146.. Train loss: 59.996.. Val loss: 54.935.. Train L1 norm: 3.133.. Val L1 norm: 1.044.. Train Linf norm: 2176.899.. Val Linf norm: 36.763\n",
            "Epoch 54/146.. Train loss: 67.741.. Val loss: 54.935.. Train L1 norm: 2.305.. Val L1 norm: 1.044.. Train Linf norm: 1329.404.. Val Linf norm: 36.779\n",
            "Epoch 55/146.. Train loss: 112.429.. Val loss: 54.930.. Train L1 norm: 2.103.. Val L1 norm: 1.044.. Train Linf norm: 1122.835.. Val Linf norm: 36.880\n",
            "Epoch 56/146.. Train loss: 103.285.. Val loss: 54.929.. Train L1 norm: 2.800.. Val L1 norm: 1.044.. Train Linf norm: 1834.370.. Val Linf norm: 36.912\n",
            "Epoch 57/146.. Train loss: 92.145.. Val loss: 54.925.. Train L1 norm: 2.663.. Val L1 norm: 1.044.. Train Linf norm: 1694.854.. Val Linf norm: 37.001\n",
            "Epoch 58/146.. Train loss: 60.065.. Val loss: 54.924.. Train L1 norm: 1.761.. Val L1 norm: 1.044.. Train Linf norm: 296.920.. Val Linf norm: 37.010\n",
            "Epoch 59/146.. Train loss: 69.365.. Val loss: 54.923.. Train L1 norm: 2.633.. Val L1 norm: 1.044.. Train Linf norm: 1665.094.. Val Linf norm: 37.041\n",
            "Epoch 60/146.. Train loss: 74.646.. Val loss: 54.921.. Train L1 norm: 2.465.. Val L1 norm: 1.044.. Train Linf norm: 1492.380.. Val Linf norm: 37.083\n",
            "Epoch 61/146.. Train loss: 108.882.. Val loss: 54.924.. Train L1 norm: 2.876.. Val L1 norm: 1.044.. Train Linf norm: 1913.229.. Val Linf norm: 37.024\n",
            "Epoch 62/146.. Train loss: 62.284.. Val loss: 54.924.. Train L1 norm: 1.824.. Val L1 norm: 1.044.. Train Linf norm: 837.287.. Val Linf norm: 37.019\n",
            "Epoch 63/146.. Train loss: 83.305.. Val loss: 54.927.. Train L1 norm: 2.718.. Val L1 norm: 1.044.. Train Linf norm: 1751.545.. Val Linf norm: 36.970\n",
            "Epoch 64/146.. Train loss: 67.287.. Val loss: 54.925.. Train L1 norm: 3.090.. Val L1 norm: 1.044.. Train Linf norm: 2132.433.. Val Linf norm: 36.996\n",
            "Epoch 65/146.. Train loss: 214.761.. Val loss: 54.919.. Train L1 norm: 2.754.. Val L1 norm: 1.044.. Train Linf norm: 1787.214.. Val Linf norm: 37.135\n",
            "Epoch 66/146.. Train loss: 62.733.. Val loss: 54.919.. Train L1 norm: 2.175.. Val L1 norm: 1.044.. Train Linf norm: 1194.862.. Val Linf norm: 37.131\n",
            "Epoch 67/146.. Train loss: 71.071.. Val loss: 54.920.. Train L1 norm: 2.007.. Val L1 norm: 1.044.. Train Linf norm: 1024.224.. Val Linf norm: 37.124\n",
            "Epoch 68/146.. Train loss: 65.697.. Val loss: 54.922.. Train L1 norm: 3.022.. Val L1 norm: 1.044.. Train Linf norm: 2061.539.. Val Linf norm: 37.081\n",
            "Epoch 69/146.. Train loss: 60.033.. Val loss: 54.922.. Train L1 norm: 2.108.. Val L1 norm: 1.044.. Train Linf norm: 1126.817.. Val Linf norm: 37.086\n",
            "Epoch 70/146.. Train loss: 76.811.. Val loss: 54.924.. Train L1 norm: 2.633.. Val L1 norm: 1.044.. Train Linf norm: 1663.726.. Val Linf norm: 37.048\n",
            "Epoch 71/146.. Train loss: 65.268.. Val loss: 54.925.. Train L1 norm: 2.669.. Val L1 norm: 1.044.. Train Linf norm: 1700.401.. Val Linf norm: 37.030\n",
            "Epoch 72/146.. Train loss: 66.919.. Val loss: 54.923.. Train L1 norm: 2.503.. Val L1 norm: 1.044.. Train Linf norm: 1529.420.. Val Linf norm: 37.066\n",
            "Epoch 73/146.. Train loss: 160.386.. Val loss: 54.927.. Train L1 norm: 3.941.. Val L1 norm: 1.044.. Train Linf norm: 3003.746.. Val Linf norm: 36.980\n",
            "Epoch 74/146.. Train loss: 74.407.. Val loss: 54.931.. Train L1 norm: 2.599.. Val L1 norm: 1.044.. Train Linf norm: 1629.543.. Val Linf norm: 36.894\n",
            "Epoch 75/146.. Train loss: 74.712.. Val loss: 54.929.. Train L1 norm: 2.996.. Val L1 norm: 1.044.. Train Linf norm: 2032.001.. Val Linf norm: 36.935\n",
            "Epoch 76/146.. Train loss: 61.175.. Val loss: 54.930.. Train L1 norm: 3.464.. Val L1 norm: 1.044.. Train Linf norm: 2515.645.. Val Linf norm: 36.934\n",
            "Epoch 77/146.. Train loss: 74.130.. Val loss: 54.927.. Train L1 norm: 1.764.. Val L1 norm: 1.044.. Train Linf norm: 775.772.. Val Linf norm: 36.984\n",
            "Epoch 78/146.. Train loss: 170.045.. Val loss: 54.921.. Train L1 norm: 2.318.. Val L1 norm: 1.044.. Train Linf norm: 1342.883.. Val Linf norm: 37.111\n",
            "Epoch 79/146.. Train loss: 75.639.. Val loss: 54.919.. Train L1 norm: 1.958.. Val L1 norm: 1.044.. Train Linf norm: 973.533.. Val Linf norm: 37.167\n",
            "Epoch 80/146.. Train loss: 97.330.. Val loss: 54.915.. Train L1 norm: 1.812.. Val L1 norm: 1.044.. Train Linf norm: 823.820.. Val Linf norm: 37.248\n",
            "Epoch 81/146.. Train loss: 97.751.. Val loss: 54.911.. Train L1 norm: 1.973.. Val L1 norm: 1.045.. Train Linf norm: 989.808.. Val Linf norm: 37.332\n",
            "Epoch 82/146.. Train loss: 66.803.. Val loss: 54.910.. Train L1 norm: 2.444.. Val L1 norm: 1.045.. Train Linf norm: 543.787.. Val Linf norm: 37.366\n",
            "Epoch 83/146.. Train loss: 82.021.. Val loss: 54.906.. Train L1 norm: 2.530.. Val L1 norm: 1.045.. Train Linf norm: 1557.901.. Val Linf norm: 37.440\n",
            "Epoch 84/146.. Train loss: 67.391.. Val loss: 54.907.. Train L1 norm: 2.839.. Val L1 norm: 1.045.. Train Linf norm: 1875.415.. Val Linf norm: 37.415\n",
            "Epoch 85/146.. Train loss: 60.034.. Val loss: 54.907.. Train L1 norm: 1.692.. Val L1 norm: 1.045.. Train Linf norm: 700.839.. Val Linf norm: 37.419\n",
            "Epoch 86/146.. Train loss: 59.975.. Val loss: 54.907.. Train L1 norm: 2.520.. Val L1 norm: 1.045.. Train Linf norm: 1548.263.. Val Linf norm: 37.424\n",
            "Epoch 87/146.. Train loss: 70.223.. Val loss: 54.909.. Train L1 norm: 1.709.. Val L1 norm: 1.045.. Train Linf norm: 717.389.. Val Linf norm: 37.389\n",
            "Epoch 88/146.. Train loss: 166.621.. Val loss: 54.902.. Train L1 norm: 3.061.. Val L1 norm: 1.045.. Train Linf norm: 2102.725.. Val Linf norm: 37.532\n",
            "Epoch 89/146.. Train loss: 61.475.. Val loss: 54.902.. Train L1 norm: 1.894.. Val L1 norm: 1.045.. Train Linf norm: 908.300.. Val Linf norm: 37.545\n",
            "Epoch 90/146.. Train loss: 69.848.. Val loss: 54.903.. Train L1 norm: 2.624.. Val L1 norm: 1.045.. Train Linf norm: 1651.886.. Val Linf norm: 37.515\n",
            "Epoch 91/146.. Train loss: 63.302.. Val loss: 54.903.. Train L1 norm: 1.936.. Val L1 norm: 1.045.. Train Linf norm: 950.114.. Val Linf norm: 37.526\n",
            "Epoch 92/146.. Train loss: 68.320.. Val loss: 54.904.. Train L1 norm: 3.242.. Val L1 norm: 1.045.. Train Linf norm: 2287.063.. Val Linf norm: 37.498\n",
            "Epoch 93/146.. Train loss: 73.521.. Val loss: 54.903.. Train L1 norm: 2.646.. Val L1 norm: 1.045.. Train Linf norm: 1676.160.. Val Linf norm: 37.534\n",
            "Epoch 94/146.. Train loss: 62.374.. Val loss: 54.903.. Train L1 norm: 2.269.. Val L1 norm: 1.045.. Train Linf norm: 1291.504.. Val Linf norm: 37.543\n",
            "Epoch 95/146.. Train loss: 169.776.. Val loss: 54.895.. Train L1 norm: 3.090.. Val L1 norm: 1.045.. Train Linf norm: 2131.727.. Val Linf norm: 37.697\n",
            "Epoch 96/146.. Train loss: 107.271.. Val loss: 54.900.. Train L1 norm: 2.378.. Val L1 norm: 1.045.. Train Linf norm: 1404.257.. Val Linf norm: 37.599\n",
            "Epoch 97/146.. Train loss: 115.855.. Val loss: 54.904.. Train L1 norm: 2.421.. Val L1 norm: 1.045.. Train Linf norm: 1447.867.. Val Linf norm: 37.513\n",
            "Epoch 98/146.. Train loss: 172.478.. Val loss: 54.897.. Train L1 norm: 2.468.. Val L1 norm: 1.045.. Train Linf norm: 1495.734.. Val Linf norm: 37.650\n",
            "Epoch 99/146.. Train loss: 61.613.. Val loss: 54.897.. Train L1 norm: 3.087.. Val L1 norm: 1.045.. Train Linf norm: 2128.474.. Val Linf norm: 37.665\n",
            "Epoch 100/146.. Train loss: 65.562.. Val loss: 54.898.. Train L1 norm: 3.279.. Val L1 norm: 1.045.. Train Linf norm: 2326.048.. Val Linf norm: 37.649\n",
            "Epoch 101/146.. Train loss: 60.568.. Val loss: 54.898.. Train L1 norm: 2.064.. Val L1 norm: 1.045.. Train Linf norm: 1079.636.. Val Linf norm: 37.653\n",
            "Epoch 102/146.. Train loss: 114.490.. Val loss: 54.902.. Train L1 norm: 2.680.. Val L1 norm: 1.045.. Train Linf norm: 1713.350.. Val Linf norm: 37.570\n",
            "Epoch 103/146.. Train loss: 61.338.. Val loss: 54.902.. Train L1 norm: 2.455.. Val L1 norm: 1.045.. Train Linf norm: 1476.370.. Val Linf norm: 37.569\n",
            "Epoch 104/146.. Train loss: 64.275.. Val loss: 54.902.. Train L1 norm: 1.782.. Val L1 norm: 1.045.. Train Linf norm: 789.433.. Val Linf norm: 37.554\n",
            "Epoch 105/146.. Train loss: 265.691.. Val loss: 54.911.. Train L1 norm: 3.918.. Val L1 norm: 1.045.. Train Linf norm: 2980.029.. Val Linf norm: 37.381\n",
            "Epoch 106/146.. Train loss: 79.179.. Val loss: 54.909.. Train L1 norm: 1.782.. Val L1 norm: 1.045.. Train Linf norm: 788.492.. Val Linf norm: 37.434\n",
            "Epoch 107/146.. Train loss: 81.557.. Val loss: 54.911.. Train L1 norm: 2.848.. Val L1 norm: 1.045.. Train Linf norm: 1884.941.. Val Linf norm: 37.391\n",
            "Epoch 108/146.. Train loss: 158.921.. Val loss: 54.905.. Train L1 norm: 2.034.. Val L1 norm: 1.045.. Train Linf norm: 1050.929.. Val Linf norm: 37.521\n",
            "Epoch 109/146.. Train loss: 120.288.. Val loss: 54.900.. Train L1 norm: 1.239.. Val L1 norm: 1.045.. Train Linf norm: 235.103.. Val Linf norm: 37.623\n",
            "Epoch 110/146.. Train loss: 222.996.. Val loss: 54.892.. Train L1 norm: 2.422.. Val L1 norm: 1.045.. Train Linf norm: 1447.200.. Val Linf norm: 37.791\n",
            "Epoch 111/146.. Train loss: 62.403.. Val loss: 54.893.. Train L1 norm: 1.966.. Val L1 norm: 1.045.. Train Linf norm: 980.976.. Val Linf norm: 37.788\n",
            "Epoch 112/146.. Train loss: 63.406.. Val loss: 54.893.. Train L1 norm: 2.389.. Val L1 norm: 1.045.. Train Linf norm: 1414.351.. Val Linf norm: 37.777\n",
            "Epoch 113/146.. Train loss: 69.905.. Val loss: 54.894.. Train L1 norm: 3.166.. Val L1 norm: 1.045.. Train Linf norm: 2210.128.. Val Linf norm: 37.757\n",
            "Epoch 114/146.. Train loss: 64.608.. Val loss: 54.893.. Train L1 norm: 2.050.. Val L1 norm: 1.045.. Train Linf norm: 1068.219.. Val Linf norm: 37.776\n",
            "Epoch 115/146.. Train loss: 158.717.. Val loss: 54.887.. Train L1 norm: 3.008.. Val L1 norm: 1.045.. Train Linf norm: 2044.292.. Val Linf norm: 37.906\n",
            "Epoch 116/146.. Train loss: 149.374.. Val loss: 54.882.. Train L1 norm: 2.025.. Val L1 norm: 1.046.. Train Linf norm: 1041.991.. Val Linf norm: 38.011\n",
            "Epoch 117/146.. Train loss: 59.911.. Val loss: 54.881.. Train L1 norm: 2.658.. Val L1 norm: 1.046.. Train Linf norm: 1690.538.. Val Linf norm: 38.031\n",
            "Epoch 118/146.. Train loss: 76.977.. Val loss: 54.883.. Train L1 norm: 1.161.. Val L1 norm: 1.046.. Train Linf norm: 155.904.. Val Linf norm: 37.990\n",
            "Epoch 119/146.. Train loss: 59.989.. Val loss: 54.883.. Train L1 norm: 2.258.. Val L1 norm: 1.046.. Train Linf norm: 1279.598.. Val Linf norm: 37.995\n",
            "Epoch 120/146.. Train loss: 60.132.. Val loss: 54.883.. Train L1 norm: 2.094.. Val L1 norm: 1.046.. Train Linf norm: 1112.051.. Val Linf norm: 38.001\n",
            "Epoch 121/146.. Train loss: 135.308.. Val loss: 54.884.. Train L1 norm: 3.222.. Val L1 norm: 1.046.. Train Linf norm: 2267.798.. Val Linf norm: 37.983\n",
            "Epoch 122/146.. Train loss: 60.152.. Val loss: 54.888.. Train L1 norm: 2.982.. Val L1 norm: 1.045.. Train Linf norm: 2021.601.. Val Linf norm: 37.900\n",
            "Epoch 123/146.. Train loss: 106.901.. Val loss: 54.883.. Train L1 norm: 2.574.. Val L1 norm: 1.046.. Train Linf norm: 1603.898.. Val Linf norm: 38.002\n",
            "Epoch 124/146.. Train loss: 62.466.. Val loss: 54.883.. Train L1 norm: 1.496.. Val L1 norm: 1.046.. Train Linf norm: 496.407.. Val Linf norm: 38.016\n",
            "Epoch 125/146.. Train loss: 102.075.. Val loss: 54.880.. Train L1 norm: 1.846.. Val L1 norm: 1.046.. Train Linf norm: 856.627.. Val Linf norm: 38.070\n",
            "Epoch 126/146.. Train loss: 114.225.. Val loss: 54.882.. Train L1 norm: 2.389.. Val L1 norm: 1.046.. Train Linf norm: 1413.416.. Val Linf norm: 38.032\n",
            "Epoch 127/146.. Train loss: 82.523.. Val loss: 54.879.. Train L1 norm: 2.736.. Val L1 norm: 1.046.. Train Linf norm: 1767.795.. Val Linf norm: 38.098\n",
            "Epoch 128/146.. Train loss: 63.255.. Val loss: 54.879.. Train L1 norm: 3.038.. Val L1 norm: 1.046.. Train Linf norm: 2078.532.. Val Linf norm: 38.092\n",
            "Epoch 129/146.. Train loss: 88.720.. Val loss: 54.882.. Train L1 norm: 2.797.. Val L1 norm: 1.046.. Train Linf norm: 1831.945.. Val Linf norm: 38.027\n",
            "Epoch 130/146.. Train loss: 280.195.. Val loss: 54.890.. Train L1 norm: 2.893.. Val L1 norm: 1.045.. Train Linf norm: 1930.786.. Val Linf norm: 37.864\n",
            "Epoch 131/146.. Train loss: 76.102.. Val loss: 54.892.. Train L1 norm: 2.229.. Val L1 norm: 1.045.. Train Linf norm: 1250.477.. Val Linf norm: 37.824\n",
            "Epoch 132/146.. Train loss: 69.379.. Val loss: 54.891.. Train L1 norm: 3.265.. Val L1 norm: 1.045.. Train Linf norm: 2312.400.. Val Linf norm: 37.859\n",
            "Epoch 133/146.. Train loss: 62.719.. Val loss: 54.890.. Train L1 norm: 2.820.. Val L1 norm: 1.045.. Train Linf norm: 1855.819.. Val Linf norm: 37.877\n",
            "Epoch 134/146.. Train loss: 111.541.. Val loss: 54.886.. Train L1 norm: 1.809.. Val L1 norm: 1.046.. Train Linf norm: 820.600.. Val Linf norm: 37.959\n",
            "Epoch 135/146.. Train loss: 64.923.. Val loss: 54.884.. Train L1 norm: 2.104.. Val L1 norm: 1.046.. Train Linf norm: 1123.061.. Val Linf norm: 38.009\n",
            "Epoch 136/146.. Train loss: 59.998.. Val loss: 54.884.. Train L1 norm: 3.381.. Val L1 norm: 1.046.. Train Linf norm: 2430.517.. Val Linf norm: 38.013\n",
            "Epoch 137/146.. Train loss: 170.622.. Val loss: 54.878.. Train L1 norm: 2.486.. Val L1 norm: 1.046.. Train Linf norm: 1512.646.. Val Linf norm: 38.150\n",
            "Epoch 138/146.. Train loss: 82.381.. Val loss: 54.875.. Train L1 norm: 1.371.. Val L1 norm: 1.046.. Train Linf norm: 371.554.. Val Linf norm: 38.215\n",
            "Epoch 139/146.. Train loss: 63.322.. Val loss: 54.875.. Train L1 norm: 2.926.. Val L1 norm: 1.046.. Train Linf norm: 1964.598.. Val Linf norm: 38.212\n",
            "Epoch 140/146.. Train loss: 169.473.. Val loss: 54.881.. Train L1 norm: 2.941.. Val L1 norm: 1.046.. Train Linf norm: 1979.373.. Val Linf norm: 38.087\n",
            "Epoch 141/146.. Train loss: 108.982.. Val loss: 54.878.. Train L1 norm: 2.279.. Val L1 norm: 1.046.. Train Linf norm: 1302.207.. Val Linf norm: 38.169\n",
            "Epoch 142/146.. Train loss: 60.211.. Val loss: 54.877.. Train L1 norm: 2.084.. Val L1 norm: 1.046.. Train Linf norm: 1102.228.. Val Linf norm: 38.175\n",
            "Epoch 143/146.. Train loss: 112.537.. Val loss: 54.875.. Train L1 norm: 2.536.. Val L1 norm: 1.046.. Train Linf norm: 1564.545.. Val Linf norm: 38.233\n",
            "Epoch 144/146.. Train loss: 157.228.. Val loss: 54.879.. Train L1 norm: 2.457.. Val L1 norm: 1.046.. Train Linf norm: 1483.371.. Val Linf norm: 38.146\n",
            "Epoch 145/146.. Train loss: 64.210.. Val loss: 54.878.. Train L1 norm: 2.448.. Val L1 norm: 1.046.. Train Linf norm: 1475.102.. Val Linf norm: 38.165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 08:36:45,312]\u001b[0m Trial 58 finished with value: 1.0458819308916727 and parameters: {'n_layers': 6, 'n_units_0': 4051, 'n_units_1': 1722, 'n_units_2': 3558, 'n_units_3': 706, 'n_units_4': 249, 'n_units_5': 1137, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 1.8857686483308225e-06, 'batch_size': 1024, 'n_epochs': 146, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.1969538144237576, 'dropout_rate': 0.023252813255143626, 'weight_decay': 0.0005832862939087129, 'beta1': 0.9301783023702841, 'beta2': 0.9991245550803523, 'factor': 0.12249852467984962, 'patience': 6, 'threshold': 0.000916896173486486}. Best is trial 22 with value: 1.0150012904485066.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 146/146.. Train loss: 61.688.. Val loss: 54.877.. Train L1 norm: 2.194.. Val L1 norm: 1.046.. Train Linf norm: 1215.581.. Val Linf norm: 38.183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 08:36:48,649]\u001b[0m Trial 59 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/147.. Train loss: 5.133.. Val loss: 4.193.. Train L1 norm: 14.404.. Val L1 norm: 3.031.. Train Linf norm: 13295.102.. Val Linf norm: 976.735\n",
            "Epoch 1/145.. Train loss: 2468.309.. Val loss: 54.721.. Train L1 norm: 3.871.. Val L1 norm: 1.029.. Train Linf norm: 2913.396.. Val Linf norm: 23.538\n",
            "Epoch 2/145.. Train loss: 91.998.. Val loss: 55.572.. Train L1 norm: 1.242.. Val L1 norm: 1.026.. Train Linf norm: 242.827.. Val Linf norm: 24.032\n",
            "Epoch 3/145.. Train loss: 61.171.. Val loss: 55.604.. Train L1 norm: 1.754.. Val L1 norm: 1.028.. Train Linf norm: 768.122.. Val Linf norm: 24.783\n",
            "Epoch 4/145.. Train loss: 82.767.. Val loss: 56.118.. Train L1 norm: 1.216.. Val L1 norm: 1.036.. Train Linf norm: 215.119.. Val Linf norm: 30.672\n",
            "Epoch 5/145.. Train loss: 314.084.. Val loss: 54.993.. Train L1 norm: 1.029.. Val L1 norm: 1.025.. Train Linf norm: 22.633.. Val Linf norm: 22.234\n",
            "Epoch 6/145.. Train loss: 436.336.. Val loss: 54.976.. Train L1 norm: 1.081.. Val L1 norm: 1.025.. Train Linf norm: 75.925.. Val Linf norm: 22.031\n",
            "Epoch 7/145.. Train loss: 334.571.. Val loss: 55.474.. Train L1 norm: 1.319.. Val L1 norm: 1.024.. Train Linf norm: 319.663.. Val Linf norm: 21.909\n",
            "Epoch 8/145.. Train loss: 61.426.. Val loss: 54.920.. Train L1 norm: 1.062.. Val L1 norm: 1.030.. Train Linf norm: 60.084.. Val Linf norm: 25.331\n",
            "Epoch 9/145.. Train loss: 66.022.. Val loss: 54.935.. Train L1 norm: 1.291.. Val L1 norm: 1.030.. Train Linf norm: 287.532.. Val Linf norm: 25.077\n",
            "Epoch 10/145.. Train loss: 157.555.. Val loss: 55.086.. Train L1 norm: 1.144.. Val L1 norm: 1.025.. Train Linf norm: 142.453.. Val Linf norm: 21.718\n",
            "Epoch 11/145.. Train loss: 72.815.. Val loss: 55.128.. Train L1 norm: 1.963.. Val L1 norm: 1.023.. Train Linf norm: 981.354.. Val Linf norm: 20.904\n",
            "Epoch 12/145.. Train loss: 64.080.. Val loss: 55.150.. Train L1 norm: 2.032.. Val L1 norm: 1.023.. Train Linf norm: 1053.674.. Val Linf norm: 20.485\n",
            "Epoch 13/145.. Train loss: 109.974.. Val loss: 55.271.. Train L1 norm: 2.014.. Val L1 norm: 1.022.. Train Linf norm: 1034.239.. Val Linf norm: 20.308\n",
            "Epoch 14/145.. Train loss: 103.614.. Val loss: 55.185.. Train L1 norm: 1.121.. Val L1 norm: 1.022.. Train Linf norm: 120.184.. Val Linf norm: 20.199\n",
            "Epoch 15/145.. Train loss: 61.385.. Val loss: 55.146.. Train L1 norm: 2.209.. Val L1 norm: 1.023.. Train Linf norm: 1234.457.. Val Linf norm: 20.721\n",
            "Epoch 16/145.. Train loss: 382.824.. Val loss: 55.185.. Train L1 norm: 1.755.. Val L1 norm: 1.022.. Train Linf norm: 768.859.. Val Linf norm: 20.263\n",
            "Epoch 17/145.. Train loss: 68.497.. Val loss: 55.178.. Train L1 norm: 1.888.. Val L1 norm: 1.022.. Train Linf norm: 905.430.. Val Linf norm: 20.337\n",
            "Epoch 18/145.. Train loss: 120.400.. Val loss: 55.180.. Train L1 norm: 2.605.. Val L1 norm: 1.022.. Train Linf norm: 1639.649.. Val Linf norm: 20.325\n",
            "Epoch 19/145.. Train loss: 61.551.. Val loss: 55.196.. Train L1 norm: 1.965.. Val L1 norm: 1.022.. Train Linf norm: 984.881.. Val Linf norm: 20.183\n",
            "Epoch 20/145.. Train loss: 131.524.. Val loss: 55.210.. Train L1 norm: 2.326.. Val L1 norm: 1.022.. Train Linf norm: 1355.047.. Val Linf norm: 20.129\n",
            "Epoch 21/145.. Train loss: 85.968.. Val loss: 55.218.. Train L1 norm: 1.769.. Val L1 norm: 1.022.. Train Linf norm: 784.657.. Val Linf norm: 20.161\n",
            "Epoch 22/145.. Train loss: 81.307.. Val loss: 55.216.. Train L1 norm: 1.608.. Val L1 norm: 1.022.. Train Linf norm: 618.416.. Val Linf norm: 20.182\n",
            "Epoch 23/145.. Train loss: 124.559.. Val loss: 55.219.. Train L1 norm: 1.117.. Val L1 norm: 1.022.. Train Linf norm: 116.088.. Val Linf norm: 20.191\n",
            "Epoch 24/145.. Train loss: 62.380.. Val loss: 55.219.. Train L1 norm: 2.162.. Val L1 norm: 1.022.. Train Linf norm: 1185.146.. Val Linf norm: 20.190\n",
            "Epoch 25/145.. Train loss: 100.672.. Val loss: 55.216.. Train L1 norm: 1.537.. Val L1 norm: 1.022.. Train Linf norm: 545.524.. Val Linf norm: 20.184\n",
            "Epoch 26/145.. Train loss: 72.411.. Val loss: 55.217.. Train L1 norm: 1.211.. Val L1 norm: 1.022.. Train Linf norm: 212.834.. Val Linf norm: 20.188\n",
            "Epoch 27/145.. Train loss: 200.728.. Val loss: 55.222.. Train L1 norm: 2.254.. Val L1 norm: 1.022.. Train Linf norm: 1281.153.. Val Linf norm: 20.198\n",
            "Epoch 28/145.. Train loss: 103.236.. Val loss: 55.224.. Train L1 norm: 1.610.. Val L1 norm: 1.022.. Train Linf norm: 620.796.. Val Linf norm: 20.206\n",
            "Epoch 29/145.. Train loss: 77.935.. Val loss: 55.225.. Train L1 norm: 2.771.. Val L1 norm: 1.022.. Train Linf norm: 1806.814.. Val Linf norm: 20.214\n",
            "Epoch 30/145.. Train loss: 60.664.. Val loss: 55.225.. Train L1 norm: 1.918.. Val L1 norm: 1.022.. Train Linf norm: 936.180.. Val Linf norm: 20.213\n",
            "Epoch 31/145.. Train loss: 143.891.. Val loss: 55.221.. Train L1 norm: 1.795.. Val L1 norm: 1.022.. Train Linf norm: 810.447.. Val Linf norm: 20.205\n",
            "Epoch 32/145.. Train loss: 83.385.. Val loss: 55.222.. Train L1 norm: 1.103.. Val L1 norm: 1.022.. Train Linf norm: 101.824.. Val Linf norm: 20.208\n",
            "Epoch 33/145.. Train loss: 210.689.. Val loss: 55.227.. Train L1 norm: 1.917.. Val L1 norm: 1.022.. Train Linf norm: 932.810.. Val Linf norm: 20.222\n",
            "Epoch 34/145.. Train loss: 64.557.. Val loss: 55.227.. Train L1 norm: 1.940.. Val L1 norm: 1.022.. Train Linf norm: 959.723.. Val Linf norm: 20.220\n",
            "Epoch 35/145.. Train loss: 61.379.. Val loss: 55.227.. Train L1 norm: 1.611.. Val L1 norm: 1.022.. Train Linf norm: 622.099.. Val Linf norm: 20.221\n",
            "Epoch 36/145.. Train loss: 70.631.. Val loss: 55.226.. Train L1 norm: 1.737.. Val L1 norm: 1.022.. Train Linf norm: 751.931.. Val Linf norm: 20.219\n",
            "Epoch 37/145.. Train loss: 64.965.. Val loss: 55.225.. Train L1 norm: 2.221.. Val L1 norm: 1.022.. Train Linf norm: 1245.996.. Val Linf norm: 20.215\n",
            "Epoch 38/145.. Train loss: 60.667.. Val loss: 55.224.. Train L1 norm: 1.579.. Val L1 norm: 1.022.. Train Linf norm: 589.785.. Val Linf norm: 20.214\n",
            "Epoch 39/145.. Train loss: 61.382.. Val loss: 55.224.. Train L1 norm: 2.364.. Val L1 norm: 1.022.. Train Linf norm: 1394.374.. Val Linf norm: 20.215\n",
            "Epoch 40/145.. Train loss: 61.724.. Val loss: 55.225.. Train L1 norm: 2.121.. Val L1 norm: 1.022.. Train Linf norm: 1143.802.. Val Linf norm: 20.215\n",
            "Epoch 41/145.. Train loss: 77.912.. Val loss: 55.226.. Train L1 norm: 1.438.. Val L1 norm: 1.022.. Train Linf norm: 445.090.. Val Linf norm: 20.221\n",
            "Epoch 42/145.. Train loss: 98.052.. Val loss: 55.223.. Train L1 norm: 1.698.. Val L1 norm: 1.022.. Train Linf norm: 711.445.. Val Linf norm: 20.214\n",
            "Epoch 43/145.. Train loss: 123.805.. Val loss: 55.222.. Train L1 norm: 1.948.. Val L1 norm: 1.022.. Train Linf norm: 967.819.. Val Linf norm: 20.209\n",
            "Epoch 44/145.. Train loss: 61.623.. Val loss: 55.219.. Train L1 norm: 2.704.. Val L1 norm: 1.022.. Train Linf norm: 1736.294.. Val Linf norm: 20.204\n",
            "Epoch 45/145.. Train loss: 68.232.. Val loss: 55.220.. Train L1 norm: 1.968.. Val L1 norm: 1.022.. Train Linf norm: 988.574.. Val Linf norm: 20.205\n",
            "Epoch 46/145.. Train loss: 87.670.. Val loss: 55.219.. Train L1 norm: 1.245.. Val L1 norm: 1.022.. Train Linf norm: 244.763.. Val Linf norm: 20.202\n",
            "Epoch 47/145.. Train loss: 214.012.. Val loss: 55.222.. Train L1 norm: 1.680.. Val L1 norm: 1.022.. Train Linf norm: 693.519.. Val Linf norm: 20.219\n",
            "Epoch 48/145.. Train loss: 72.462.. Val loss: 55.221.. Train L1 norm: 1.704.. Val L1 norm: 1.022.. Train Linf norm: 716.804.. Val Linf norm: 20.216\n",
            "Epoch 49/145.. Train loss: 65.577.. Val loss: 55.221.. Train L1 norm: 1.945.. Val L1 norm: 1.022.. Train Linf norm: 963.889.. Val Linf norm: 20.217\n",
            "Epoch 50/145.. Train loss: 226.249.. Val loss: 55.226.. Train L1 norm: 1.325.. Val L1 norm: 1.022.. Train Linf norm: 328.870.. Val Linf norm: 20.242\n",
            "Epoch 51/145.. Train loss: 83.183.. Val loss: 55.229.. Train L1 norm: 1.132.. Val L1 norm: 1.022.. Train Linf norm: 131.255.. Val Linf norm: 20.248\n",
            "Epoch 52/145.. Train loss: 145.132.. Val loss: 55.232.. Train L1 norm: 1.118.. Val L1 norm: 1.022.. Train Linf norm: 117.273.. Val Linf norm: 20.256\n",
            "Epoch 53/145.. Train loss: 128.139.. Val loss: 55.236.. Train L1 norm: 2.137.. Val L1 norm: 1.022.. Train Linf norm: 1161.486.. Val Linf norm: 20.264\n",
            "Epoch 54/145.. Train loss: 84.811.. Val loss: 55.237.. Train L1 norm: 1.458.. Val L1 norm: 1.022.. Train Linf norm: 466.377.. Val Linf norm: 20.266\n",
            "Epoch 55/145.. Train loss: 67.945.. Val loss: 55.237.. Train L1 norm: 2.409.. Val L1 norm: 1.022.. Train Linf norm: 1439.238.. Val Linf norm: 20.271\n",
            "Epoch 56/145.. Train loss: 98.179.. Val loss: 55.240.. Train L1 norm: 2.043.. Val L1 norm: 1.022.. Train Linf norm: 1064.194.. Val Linf norm: 20.281\n",
            "Epoch 57/145.. Train loss: 61.019.. Val loss: 55.239.. Train L1 norm: 2.308.. Val L1 norm: 1.022.. Train Linf norm: 1337.456.. Val Linf norm: 20.281\n",
            "Epoch 58/145.. Train loss: 60.162.. Val loss: 55.239.. Train L1 norm: 1.474.. Val L1 norm: 1.022.. Train Linf norm: 480.620.. Val Linf norm: 20.281\n",
            "Epoch 59/145.. Train loss: 103.224.. Val loss: 55.235.. Train L1 norm: 1.107.. Val L1 norm: 1.022.. Train Linf norm: 105.976.. Val Linf norm: 20.269\n",
            "Epoch 60/145.. Train loss: 74.309.. Val loss: 55.233.. Train L1 norm: 2.942.. Val L1 norm: 1.022.. Train Linf norm: 1986.121.. Val Linf norm: 20.267\n",
            "Epoch 61/145.. Train loss: 102.677.. Val loss: 55.237.. Train L1 norm: 2.347.. Val L1 norm: 1.022.. Train Linf norm: 1376.505.. Val Linf norm: 20.282\n",
            "Epoch 62/145.. Train loss: 89.800.. Val loss: 55.236.. Train L1 norm: 2.682.. Val L1 norm: 1.022.. Train Linf norm: 1719.102.. Val Linf norm: 20.280\n",
            "Epoch 63/145.. Train loss: 81.260.. Val loss: 55.236.. Train L1 norm: 3.372.. Val L1 norm: 1.022.. Train Linf norm: 2425.674.. Val Linf norm: 20.283\n",
            "Epoch 64/145.. Train loss: 246.104.. Val loss: 55.243.. Train L1 norm: 1.084.. Val L1 norm: 1.022.. Train Linf norm: 76.984.. Val Linf norm: 20.299\n",
            "Epoch 65/145.. Train loss: 114.317.. Val loss: 55.246.. Train L1 norm: 1.121.. Val L1 norm: 1.022.. Train Linf norm: 120.542.. Val Linf norm: 20.310\n",
            "Epoch 66/145.. Train loss: 69.235.. Val loss: 55.248.. Train L1 norm: 1.402.. Val L1 norm: 1.022.. Train Linf norm: 408.508.. Val Linf norm: 20.319\n",
            "Epoch 67/145.. Train loss: 60.761.. Val loss: 55.249.. Train L1 norm: 1.733.. Val L1 norm: 1.022.. Train Linf norm: 744.667.. Val Linf norm: 20.321\n",
            "Epoch 68/145.. Train loss: 83.645.. Val loss: 55.246.. Train L1 norm: 1.489.. Val L1 norm: 1.022.. Train Linf norm: 497.182.. Val Linf norm: 20.313\n",
            "Epoch 69/145.. Train loss: 68.413.. Val loss: 55.245.. Train L1 norm: 1.606.. Val L1 norm: 1.022.. Train Linf norm: 616.574.. Val Linf norm: 20.308\n",
            "Epoch 70/145.. Train loss: 100.438.. Val loss: 55.248.. Train L1 norm: 1.669.. Val L1 norm: 1.022.. Train Linf norm: 680.505.. Val Linf norm: 20.319\n",
            "Epoch 71/145.. Train loss: 61.796.. Val loss: 55.248.. Train L1 norm: 1.426.. Val L1 norm: 1.022.. Train Linf norm: 433.878.. Val Linf norm: 20.321\n",
            "Epoch 72/145.. Train loss: 94.224.. Val loss: 55.246.. Train L1 norm: 1.415.. Val L1 norm: 1.022.. Train Linf norm: 420.090.. Val Linf norm: 20.314\n",
            "Epoch 73/145.. Train loss: 71.047.. Val loss: 55.243.. Train L1 norm: 1.452.. Val L1 norm: 1.022.. Train Linf norm: 459.070.. Val Linf norm: 20.304\n",
            "Epoch 74/145.. Train loss: 85.628.. Val loss: 55.246.. Train L1 norm: 1.669.. Val L1 norm: 1.022.. Train Linf norm: 681.486.. Val Linf norm: 20.314\n",
            "Epoch 75/145.. Train loss: 75.447.. Val loss: 55.247.. Train L1 norm: 1.415.. Val L1 norm: 1.022.. Train Linf norm: 421.602.. Val Linf norm: 20.317\n",
            "Epoch 76/145.. Train loss: 85.554.. Val loss: 55.246.. Train L1 norm: 1.687.. Val L1 norm: 1.022.. Train Linf norm: 699.738.. Val Linf norm: 20.316\n",
            "Epoch 77/145.. Train loss: 66.796.. Val loss: 55.247.. Train L1 norm: 1.632.. Val L1 norm: 1.022.. Train Linf norm: 643.525.. Val Linf norm: 20.321\n",
            "Epoch 78/145.. Train loss: 69.684.. Val loss: 55.246.. Train L1 norm: 1.480.. Val L1 norm: 1.022.. Train Linf norm: 487.464.. Val Linf norm: 20.319\n",
            "Epoch 79/145.. Train loss: 157.417.. Val loss: 55.247.. Train L1 norm: 1.126.. Val L1 norm: 1.022.. Train Linf norm: 125.793.. Val Linf norm: 20.322\n",
            "Epoch 80/145.. Train loss: 105.032.. Val loss: 55.251.. Train L1 norm: 1.512.. Val L1 norm: 1.022.. Train Linf norm: 520.321.. Val Linf norm: 20.336\n",
            "Epoch 81/145.. Train loss: 61.090.. Val loss: 55.251.. Train L1 norm: 2.105.. Val L1 norm: 1.022.. Train Linf norm: 1128.061.. Val Linf norm: 20.338\n",
            "Epoch 82/145.. Train loss: 78.858.. Val loss: 55.253.. Train L1 norm: 1.614.. Val L1 norm: 1.022.. Train Linf norm: 619.826.. Val Linf norm: 20.345\n",
            "Epoch 83/145.. Train loss: 63.782.. Val loss: 55.253.. Train L1 norm: 2.144.. Val L1 norm: 1.022.. Train Linf norm: 1168.232.. Val Linf norm: 20.345\n",
            "Epoch 84/145.. Train loss: 210.260.. Val loss: 55.258.. Train L1 norm: 2.081.. Val L1 norm: 1.022.. Train Linf norm: 1103.884.. Val Linf norm: 20.365\n",
            "Epoch 85/145.. Train loss: 60.851.. Val loss: 55.258.. Train L1 norm: 1.550.. Val L1 norm: 1.022.. Train Linf norm: 560.526.. Val Linf norm: 20.365\n",
            "Epoch 86/145.. Train loss: 60.535.. Val loss: 55.258.. Train L1 norm: 1.103.. Val L1 norm: 1.022.. Train Linf norm: 101.667.. Val Linf norm: 20.366\n",
            "Epoch 87/145.. Train loss: 61.433.. Val loss: 55.259.. Train L1 norm: 1.560.. Val L1 norm: 1.022.. Train Linf norm: 569.361.. Val Linf norm: 20.368\n",
            "Epoch 88/145.. Train loss: 60.250.. Val loss: 55.258.. Train L1 norm: 1.230.. Val L1 norm: 1.022.. Train Linf norm: 231.813.. Val Linf norm: 20.368\n",
            "Epoch 89/145.. Train loss: 61.688.. Val loss: 55.259.. Train L1 norm: 2.572.. Val L1 norm: 1.022.. Train Linf norm: 1605.810.. Val Linf norm: 20.370\n",
            "Epoch 90/145.. Train loss: 80.447.. Val loss: 55.258.. Train L1 norm: 1.242.. Val L1 norm: 1.022.. Train Linf norm: 244.301.. Val Linf norm: 20.368\n",
            "Epoch 91/145.. Train loss: 122.354.. Val loss: 55.261.. Train L1 norm: 2.216.. Val L1 norm: 1.022.. Train Linf norm: 1241.555.. Val Linf norm: 20.381\n",
            "Epoch 92/145.. Train loss: 97.107.. Val loss: 55.264.. Train L1 norm: 2.437.. Val L1 norm: 1.022.. Train Linf norm: 1468.673.. Val Linf norm: 20.390\n",
            "Epoch 93/145.. Train loss: 158.116.. Val loss: 55.270.. Train L1 norm: 2.116.. Val L1 norm: 1.022.. Train Linf norm: 1139.065.. Val Linf norm: 20.414\n",
            "Epoch 94/145.. Train loss: 70.108.. Val loss: 55.271.. Train L1 norm: 1.100.. Val L1 norm: 1.022.. Train Linf norm: 98.264.. Val Linf norm: 20.419\n",
            "Epoch 95/145.. Train loss: 91.757.. Val loss: 55.268.. Train L1 norm: 1.395.. Val L1 norm: 1.022.. Train Linf norm: 401.810.. Val Linf norm: 20.415\n",
            "Epoch 96/145.. Train loss: 63.972.. Val loss: 55.269.. Train L1 norm: 2.020.. Val L1 norm: 1.022.. Train Linf norm: 1040.815.. Val Linf norm: 20.420\n",
            "Epoch 97/145.. Train loss: 62.633.. Val loss: 55.268.. Train L1 norm: 2.092.. Val L1 norm: 1.022.. Train Linf norm: 1109.739.. Val Linf norm: 20.419\n",
            "Epoch 98/145.. Train loss: 109.733.. Val loss: 55.273.. Train L1 norm: 2.509.. Val L1 norm: 1.022.. Train Linf norm: 1542.426.. Val Linf norm: 20.438\n",
            "Epoch 99/145.. Train loss: 67.824.. Val loss: 55.274.. Train L1 norm: 2.259.. Val L1 norm: 1.022.. Train Linf norm: 1274.614.. Val Linf norm: 20.440\n",
            "Epoch 100/145.. Train loss: 216.120.. Val loss: 55.281.. Train L1 norm: 1.079.. Val L1 norm: 1.022.. Train Linf norm: 73.798.. Val Linf norm: 20.463\n",
            "Epoch 101/145.. Train loss: 128.273.. Val loss: 55.277.. Train L1 norm: 1.351.. Val L1 norm: 1.022.. Train Linf norm: 355.262.. Val Linf norm: 20.450\n",
            "Epoch 102/145.. Train loss: 90.406.. Val loss: 55.280.. Train L1 norm: 2.075.. Val L1 norm: 1.022.. Train Linf norm: 1097.313.. Val Linf norm: 20.453\n",
            "Epoch 103/145.. Train loss: 71.930.. Val loss: 55.279.. Train L1 norm: 1.532.. Val L1 norm: 1.022.. Train Linf norm: 541.972.. Val Linf norm: 20.448\n",
            "Epoch 104/145.. Train loss: 64.242.. Val loss: 55.280.. Train L1 norm: 2.246.. Val L1 norm: 1.022.. Train Linf norm: 1271.643.. Val Linf norm: 20.452\n",
            "Epoch 105/145.. Train loss: 142.886.. Val loss: 55.275.. Train L1 norm: 1.146.. Val L1 norm: 1.022.. Train Linf norm: 146.704.. Val Linf norm: 20.436\n",
            "Epoch 106/145.. Train loss: 63.332.. Val loss: 55.275.. Train L1 norm: 1.119.. Val L1 norm: 1.022.. Train Linf norm: 119.583.. Val Linf norm: 20.438\n",
            "Epoch 107/145.. Train loss: 151.385.. Val loss: 55.281.. Train L1 norm: 1.656.. Val L1 norm: 1.022.. Train Linf norm: 667.992.. Val Linf norm: 20.457\n",
            "Epoch 108/145.. Train loss: 60.450.. Val loss: 55.281.. Train L1 norm: 1.408.. Val L1 norm: 1.022.. Train Linf norm: 413.865.. Val Linf norm: 20.457\n",
            "Epoch 109/145.. Train loss: 80.944.. Val loss: 55.283.. Train L1 norm: 1.593.. Val L1 norm: 1.022.. Train Linf norm: 603.385.. Val Linf norm: 20.464\n",
            "Epoch 110/145.. Train loss: 88.206.. Val loss: 55.287.. Train L1 norm: 1.988.. Val L1 norm: 1.022.. Train Linf norm: 1008.776.. Val Linf norm: 20.478\n",
            "Epoch 111/145.. Train loss: 80.951.. Val loss: 55.284.. Train L1 norm: 2.439.. Val L1 norm: 1.022.. Train Linf norm: 1470.906.. Val Linf norm: 20.473\n",
            "Epoch 112/145.. Train loss: 61.340.. Val loss: 55.284.. Train L1 norm: 1.788.. Val L1 norm: 1.022.. Train Linf norm: 802.914.. Val Linf norm: 20.472\n",
            "Epoch 113/145.. Train loss: 85.076.. Val loss: 55.287.. Train L1 norm: 1.197.. Val L1 norm: 1.022.. Train Linf norm: 187.301.. Val Linf norm: 20.476\n",
            "Epoch 114/145.. Train loss: 79.826.. Val loss: 55.284.. Train L1 norm: 1.179.. Val L1 norm: 1.022.. Train Linf norm: 180.219.. Val Linf norm: 20.465\n",
            "Epoch 115/145.. Train loss: 73.782.. Val loss: 55.286.. Train L1 norm: 1.714.. Val L1 norm: 1.022.. Train Linf norm: 726.383.. Val Linf norm: 20.470\n",
            "Epoch 116/145.. Train loss: 185.843.. Val loss: 55.290.. Train L1 norm: 1.509.. Val L1 norm: 1.022.. Train Linf norm: 515.771.. Val Linf norm: 20.489\n",
            "Epoch 117/145.. Train loss: 95.370.. Val loss: 55.296.. Train L1 norm: 1.761.. Val L1 norm: 1.022.. Train Linf norm: 776.361.. Val Linf norm: 20.512\n",
            "Epoch 118/145.. Train loss: 64.917.. Val loss: 55.297.. Train L1 norm: 1.791.. Val L1 norm: 1.022.. Train Linf norm: 807.760.. Val Linf norm: 20.517\n",
            "Epoch 119/145.. Train loss: 67.708.. Val loss: 55.299.. Train L1 norm: 1.058.. Val L1 norm: 1.022.. Train Linf norm: 55.303.. Val Linf norm: 20.524\n",
            "Epoch 120/145.. Train loss: 445.470.. Val loss: 55.309.. Train L1 norm: 1.117.. Val L1 norm: 1.022.. Train Linf norm: 112.674.. Val Linf norm: 20.541\n",
            "Epoch 121/145.. Train loss: 61.303.. Val loss: 55.309.. Train L1 norm: 1.941.. Val L1 norm: 1.022.. Train Linf norm: 955.714.. Val Linf norm: 20.542\n",
            "Epoch 122/145.. Train loss: 60.485.. Val loss: 55.309.. Train L1 norm: 2.106.. Val L1 norm: 1.022.. Train Linf norm: 1129.792.. Val Linf norm: 20.542\n",
            "Epoch 123/145.. Train loss: 107.441.. Val loss: 55.311.. Train L1 norm: 2.184.. Val L1 norm: 1.022.. Train Linf norm: 1209.295.. Val Linf norm: 20.549\n",
            "Epoch 124/145.. Train loss: 73.160.. Val loss: 55.313.. Train L1 norm: 2.008.. Val L1 norm: 1.022.. Train Linf norm: 1028.639.. Val Linf norm: 20.557\n",
            "Epoch 125/145.. Train loss: 64.303.. Val loss: 55.313.. Train L1 norm: 1.037.. Val L1 norm: 1.022.. Train Linf norm: 35.419.. Val Linf norm: 20.558\n",
            "Epoch 126/145.. Train loss: 60.434.. Val loss: 55.312.. Train L1 norm: 1.632.. Val L1 norm: 1.022.. Train Linf norm: 643.144.. Val Linf norm: 20.558\n",
            "Epoch 127/145.. Train loss: 64.492.. Val loss: 55.313.. Train L1 norm: 2.363.. Val L1 norm: 1.022.. Train Linf norm: 1392.882.. Val Linf norm: 20.560\n",
            "Epoch 128/145.. Train loss: 69.118.. Val loss: 55.312.. Train L1 norm: 2.051.. Val L1 norm: 1.022.. Train Linf norm: 1073.416.. Val Linf norm: 20.554\n",
            "Epoch 129/145.. Train loss: 68.467.. Val loss: 55.310.. Train L1 norm: 1.123.. Val L1 norm: 1.022.. Train Linf norm: 121.782.. Val Linf norm: 20.550\n",
            "Epoch 130/145.. Train loss: 182.128.. Val loss: 55.315.. Train L1 norm: 1.401.. Val L1 norm: 1.022.. Train Linf norm: 406.792.. Val Linf norm: 20.571\n",
            "Epoch 131/145.. Train loss: 71.731.. Val loss: 55.317.. Train L1 norm: 1.405.. Val L1 norm: 1.022.. Train Linf norm: 412.047.. Val Linf norm: 20.580\n",
            "Epoch 132/145.. Train loss: 60.758.. Val loss: 55.317.. Train L1 norm: 1.924.. Val L1 norm: 1.022.. Train Linf norm: 942.785.. Val Linf norm: 20.582\n",
            "Epoch 133/145.. Train loss: 71.974.. Val loss: 55.318.. Train L1 norm: 1.640.. Val L1 norm: 1.022.. Train Linf norm: 651.830.. Val Linf norm: 20.585\n",
            "Epoch 134/145.. Train loss: 60.754.. Val loss: 55.319.. Train L1 norm: 1.635.. Val L1 norm: 1.022.. Train Linf norm: 647.165.. Val Linf norm: 20.590\n",
            "Epoch 135/145.. Train loss: 70.250.. Val loss: 55.317.. Train L1 norm: 1.254.. Val L1 norm: 1.022.. Train Linf norm: 257.732.. Val Linf norm: 20.585\n",
            "Epoch 136/145.. Train loss: 73.368.. Val loss: 55.315.. Train L1 norm: 1.379.. Val L1 norm: 1.022.. Train Linf norm: 383.660.. Val Linf norm: 20.577\n",
            "Epoch 137/145.. Train loss: 138.654.. Val loss: 55.320.. Train L1 norm: 1.804.. Val L1 norm: 1.022.. Train Linf norm: 820.640.. Val Linf norm: 20.587\n",
            "Epoch 138/145.. Train loss: 67.925.. Val loss: 55.322.. Train L1 norm: 2.065.. Val L1 norm: 1.022.. Train Linf norm: 1086.161.. Val Linf norm: 20.594\n",
            "Epoch 139/145.. Train loss: 122.720.. Val loss: 55.327.. Train L1 norm: 1.546.. Val L1 norm: 1.022.. Train Linf norm: 555.727.. Val Linf norm: 20.612\n",
            "Epoch 140/145.. Train loss: 82.874.. Val loss: 55.325.. Train L1 norm: 1.238.. Val L1 norm: 1.022.. Train Linf norm: 240.144.. Val Linf norm: 20.603\n",
            "Epoch 141/145.. Train loss: 81.781.. Val loss: 55.322.. Train L1 norm: 2.116.. Val L1 norm: 1.022.. Train Linf norm: 1139.453.. Val Linf norm: 20.601\n",
            "Epoch 142/145.. Train loss: 96.385.. Val loss: 55.317.. Train L1 norm: 1.257.. Val L1 norm: 1.022.. Train Linf norm: 259.793.. Val Linf norm: 20.591\n",
            "Epoch 143/145.. Train loss: 76.818.. Val loss: 55.314.. Train L1 norm: 1.930.. Val L1 norm: 1.022.. Train Linf norm: 948.933.. Val Linf norm: 20.581\n",
            "Epoch 144/145.. Train loss: 65.123.. Val loss: 55.315.. Train L1 norm: 2.163.. Val L1 norm: 1.022.. Train Linf norm: 1186.918.. Val Linf norm: 20.582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 08:42:31,663]\u001b[0m Trial 60 finished with value: 1.0222859530766806 and parameters: {'n_layers': 6, 'n_units_0': 4019, 'n_units_1': 1078, 'n_units_2': 3519, 'n_units_3': 447, 'n_units_4': 607, 'n_units_5': 1092, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 1.4536265452254893e-06, 'batch_size': 1024, 'n_epochs': 145, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.19282901476470884, 'dropout_rate': 0.03044840444068328, 'weight_decay': 0.0007774682407695617, 'beta1': 0.9290882277433042, 'beta2': 0.9991434179994843, 'factor': 0.13851610188849037, 'patience': 6, 'threshold': 0.002174289166996554}. Best is trial 22 with value: 1.0150012904485066.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 145/145.. Train loss: 61.663.. Val loss: 55.315.. Train L1 norm: 1.323.. Val L1 norm: 1.022.. Train Linf norm: 327.875.. Val Linf norm: 20.585\n",
            "Epoch 1/145.. Train loss: 3438.335.. Val loss: 52.929.. Train L1 norm: 3.320.. Val L1 norm: 1.089.. Train Linf norm: 1174.742.. Val Linf norm: 38.898\n",
            "Epoch 2/145.. Train loss: 824.882.. Val loss: 54.979.. Train L1 norm: 2.474.. Val L1 norm: 1.024.. Train Linf norm: 752.587.. Val Linf norm: 12.654\n",
            "Epoch 3/145.. Train loss: 60.865.. Val loss: 54.774.. Train L1 norm: 2.076.. Val L1 norm: 1.023.. Train Linf norm: 551.486.. Val Linf norm: 12.136\n",
            "Epoch 4/145.. Train loss: 60.222.. Val loss: 54.642.. Train L1 norm: 2.510.. Val L1 norm: 1.027.. Train Linf norm: 773.554.. Val Linf norm: 13.987\n",
            "Epoch 5/145.. Train loss: 137.470.. Val loss: 53.200.. Train L1 norm: 3.577.. Val L1 norm: 1.085.. Train Linf norm: 1316.797.. Val Linf norm: 37.771\n",
            "Epoch 6/145.. Train loss: 210.232.. Val loss: 55.311.. Train L1 norm: 2.985.. Val L1 norm: 1.032.. Train Linf norm: 1011.648.. Val Linf norm: 15.483\n",
            "Epoch 7/145.. Train loss: 457.889.. Val loss: 52.716.. Train L1 norm: 4.710.. Val L1 norm: 1.112.. Train Linf norm: 1894.801.. Val Linf norm: 47.522\n",
            "Epoch 8/145.. Train loss: 706.748.. Val loss: 55.576.. Train L1 norm: 5.068.. Val L1 norm: 1.030.. Train Linf norm: 2074.963.. Val Linf norm: 14.084\n",
            "Epoch 9/145.. Train loss: 766.243.. Val loss: 53.217.. Train L1 norm: 1.809.. Val L1 norm: 1.092.. Train Linf norm: 408.529.. Val Linf norm: 40.977\n",
            "Epoch 10/145.. Train loss: 316.696.. Val loss: 55.075.. Train L1 norm: 1.927.. Val L1 norm: 1.027.. Train Linf norm: 474.190.. Val Linf norm: 13.598\n",
            "Epoch 11/145.. Train loss: 144.045.. Val loss: 53.730.. Train L1 norm: 3.846.. Val L1 norm: 1.073.. Train Linf norm: 1456.145.. Val Linf norm: 33.599\n",
            "Epoch 12/145.. Train loss: 75.796.. Val loss: 54.225.. Train L1 norm: 3.694.. Val L1 norm: 1.056.. Train Linf norm: 1377.240.. Val Linf norm: 26.721\n",
            "Epoch 13/145.. Train loss: 104.985.. Val loss: 54.991.. Train L1 norm: 2.606.. Val L1 norm: 1.034.. Train Linf norm: 820.532.. Val Linf norm: 17.037\n",
            "Epoch 14/145.. Train loss: 411.543.. Val loss: 53.063.. Train L1 norm: 1.370.. Val L1 norm: 1.106.. Train Linf norm: 187.175.. Val Linf norm: 46.135\n",
            "Epoch 15/145.. Train loss: 542.424.. Val loss: 53.320.. Train L1 norm: 3.737.. Val L1 norm: 1.094.. Train Linf norm: 1392.098.. Val Linf norm: 41.712\n",
            "Epoch 16/145.. Train loss: 159.586.. Val loss: 53.363.. Train L1 norm: 3.311.. Val L1 norm: 1.093.. Train Linf norm: 1175.446.. Val Linf norm: 41.071\n",
            "Epoch 17/145.. Train loss: 147.489.. Val loss: 53.630.. Train L1 norm: 3.659.. Val L1 norm: 1.081.. Train Linf norm: 1355.854.. Val Linf norm: 36.688\n",
            "Epoch 18/145.. Train loss: 236.592.. Val loss: 53.850.. Train L1 norm: 4.505.. Val L1 norm: 1.072.. Train Linf norm: 1788.629.. Val Linf norm: 33.263\n",
            "Epoch 19/145.. Train loss: 59.554.. Val loss: 53.842.. Train L1 norm: 3.550.. Val L1 norm: 1.073.. Train Linf norm: 1299.855.. Val Linf norm: 33.487\n",
            "Epoch 20/145.. Train loss: 320.084.. Val loss: 54.107.. Train L1 norm: 2.357.. Val L1 norm: 1.062.. Train Linf norm: 692.356.. Val Linf norm: 29.273\n",
            "Epoch 21/145.. Train loss: 145.057.. Val loss: 54.286.. Train L1 norm: 3.177.. Val L1 norm: 1.055.. Train Linf norm: 1109.317.. Val Linf norm: 26.632\n",
            "Epoch 22/145.. Train loss: 103.023.. Val loss: 54.269.. Train L1 norm: 3.931.. Val L1 norm: 1.056.. Train Linf norm: 1498.847.. Val Linf norm: 26.904\n",
            "Epoch 23/145.. Train loss: 132.933.. Val loss: 54.290.. Train L1 norm: 2.986.. Val L1 norm: 1.055.. Train Linf norm: 1015.113.. Val Linf norm: 26.598\n",
            "Epoch 24/145.. Train loss: 59.827.. Val loss: 54.289.. Train L1 norm: 2.482.. Val L1 norm: 1.055.. Train Linf norm: 756.580.. Val Linf norm: 26.630\n",
            "Epoch 25/145.. Train loss: 185.410.. Val loss: 54.259.. Train L1 norm: 3.887.. Val L1 norm: 1.057.. Train Linf norm: 1476.351.. Val Linf norm: 27.102\n",
            "Epoch 26/145.. Train loss: 135.815.. Val loss: 54.281.. Train L1 norm: 2.471.. Val L1 norm: 1.056.. Train Linf norm: 750.676.. Val Linf norm: 26.777\n",
            "Epoch 27/145.. Train loss: 89.672.. Val loss: 54.264.. Train L1 norm: 3.483.. Val L1 norm: 1.056.. Train Linf norm: 1269.133.. Val Linf norm: 27.043\n",
            "Epoch 28/145.. Train loss: 86.669.. Val loss: 54.249.. Train L1 norm: 4.130.. Val L1 norm: 1.057.. Train Linf norm: 1599.966.. Val Linf norm: 27.294\n",
            "Epoch 29/145.. Train loss: 82.222.. Val loss: 54.247.. Train L1 norm: 3.605.. Val L1 norm: 1.057.. Train Linf norm: 1331.795.. Val Linf norm: 27.320\n",
            "Epoch 30/145.. Train loss: 89.989.. Val loss: 54.245.. Train L1 norm: 3.555.. Val L1 norm: 1.057.. Train Linf norm: 1306.477.. Val Linf norm: 27.351\n",
            "Epoch 31/145.. Train loss: 64.152.. Val loss: 54.244.. Train L1 norm: 2.376.. Val L1 norm: 1.057.. Train Linf norm: 702.622.. Val Linf norm: 27.363\n",
            "Epoch 32/145.. Train loss: 106.339.. Val loss: 54.242.. Train L1 norm: 3.099.. Val L1 norm: 1.057.. Train Linf norm: 1072.730.. Val Linf norm: 27.404\n",
            "Epoch 33/145.. Train loss: 113.589.. Val loss: 54.240.. Train L1 norm: 2.507.. Val L1 norm: 1.057.. Train Linf norm: 769.081.. Val Linf norm: 27.434\n",
            "Epoch 34/145.. Train loss: 99.849.. Val loss: 54.236.. Train L1 norm: 2.981.. Val L1 norm: 1.058.. Train Linf norm: 1012.501.. Val Linf norm: 27.500\n",
            "Epoch 35/145.. Train loss: 64.748.. Val loss: 54.235.. Train L1 norm: 2.630.. Val L1 norm: 1.058.. Train Linf norm: 833.035.. Val Linf norm: 27.514\n",
            "Epoch 36/145.. Train loss: 70.503.. Val loss: 54.234.. Train L1 norm: 1.959.. Val L1 norm: 1.058.. Train Linf norm: 487.757.. Val Linf norm: 27.535\n",
            "Epoch 37/145.. Train loss: 63.649.. Val loss: 54.233.. Train L1 norm: 3.620.. Val L1 norm: 1.058.. Train Linf norm: 1339.398.. Val Linf norm: 27.548\n",
            "Epoch 38/145.. Train loss: 63.075.. Val loss: 54.233.. Train L1 norm: 4.318.. Val L1 norm: 1.058.. Train Linf norm: 1697.040.. Val Linf norm: 27.543\n",
            "Epoch 39/145.. Train loss: 279.357.. Val loss: 54.227.. Train L1 norm: 3.395.. Val L1 norm: 1.058.. Train Linf norm: 1224.357.. Val Linf norm: 27.642\n",
            "Epoch 40/145.. Train loss: 59.685.. Val loss: 54.227.. Train L1 norm: 3.019.. Val L1 norm: 1.058.. Train Linf norm: 1031.765.. Val Linf norm: 27.645\n",
            "Epoch 41/145.. Train loss: 165.649.. Val loss: 54.221.. Train L1 norm: 2.945.. Val L1 norm: 1.058.. Train Linf norm: 994.074.. Val Linf norm: 27.727\n",
            "Epoch 42/145.. Train loss: 85.224.. Val loss: 54.219.. Train L1 norm: 3.370.. Val L1 norm: 1.058.. Train Linf norm: 1210.834.. Val Linf norm: 27.766\n",
            "Epoch 43/145.. Train loss: 121.815.. Val loss: 54.215.. Train L1 norm: 2.302.. Val L1 norm: 1.058.. Train Linf norm: 664.069.. Val Linf norm: 27.825\n",
            "Epoch 44/145.. Train loss: 79.863.. Val loss: 54.217.. Train L1 norm: 3.047.. Val L1 norm: 1.058.. Train Linf norm: 1046.352.. Val Linf norm: 27.795\n",
            "Epoch 45/145.. Train loss: 65.916.. Val loss: 54.218.. Train L1 norm: 2.381.. Val L1 norm: 1.058.. Train Linf norm: 704.990.. Val Linf norm: 27.783\n",
            "Epoch 46/145.. Train loss: 117.823.. Val loss: 54.222.. Train L1 norm: 3.829.. Val L1 norm: 1.058.. Train Linf norm: 1446.696.. Val Linf norm: 27.727\n",
            "Epoch 47/145.. Train loss: 92.260.. Val loss: 54.219.. Train L1 norm: 2.634.. Val L1 norm: 1.058.. Train Linf norm: 834.920.. Val Linf norm: 27.769\n",
            "Epoch 48/145.. Train loss: 60.663.. Val loss: 54.220.. Train L1 norm: 3.538.. Val L1 norm: 1.058.. Train Linf norm: 1297.066.. Val Linf norm: 27.768\n",
            "Epoch 49/145.. Train loss: 85.980.. Val loss: 54.217.. Train L1 norm: 2.733.. Val L1 norm: 1.058.. Train Linf norm: 883.470.. Val Linf norm: 27.813\n",
            "Epoch 50/145.. Train loss: 78.806.. Val loss: 54.214.. Train L1 norm: 2.536.. Val L1 norm: 1.058.. Train Linf norm: 783.886.. Val Linf norm: 27.862\n",
            "Epoch 51/145.. Train loss: 144.630.. Val loss: 54.208.. Train L1 norm: 3.736.. Val L1 norm: 1.059.. Train Linf norm: 1399.033.. Val Linf norm: 27.942\n",
            "Epoch 52/145.. Train loss: 72.236.. Val loss: 54.210.. Train L1 norm: 3.104.. Val L1 norm: 1.059.. Train Linf norm: 1074.873.. Val Linf norm: 27.921\n",
            "Epoch 53/145.. Train loss: 122.107.. Val loss: 54.215.. Train L1 norm: 2.652.. Val L1 norm: 1.058.. Train Linf norm: 843.940.. Val Linf norm: 27.845\n",
            "Epoch 54/145.. Train loss: 323.273.. Val loss: 54.208.. Train L1 norm: 3.043.. Val L1 norm: 1.059.. Train Linf norm: 1043.886.. Val Linf norm: 27.956\n",
            "Epoch 55/145.. Train loss: 113.285.. Val loss: 54.204.. Train L1 norm: 2.378.. Val L1 norm: 1.059.. Train Linf norm: 703.493.. Val Linf norm: 28.015\n",
            "Epoch 56/145.. Train loss: 120.068.. Val loss: 54.200.. Train L1 norm: 2.555.. Val L1 norm: 1.059.. Train Linf norm: 792.002.. Val Linf norm: 28.078\n",
            "Epoch 57/145.. Train loss: 59.647.. Val loss: 54.199.. Train L1 norm: 2.598.. Val L1 norm: 1.059.. Train Linf norm: 816.538.. Val Linf norm: 28.090\n",
            "Epoch 58/145.. Train loss: 59.604.. Val loss: 54.199.. Train L1 norm: 3.216.. Val L1 norm: 1.059.. Train Linf norm: 1131.883.. Val Linf norm: 28.093\n",
            "Epoch 59/145.. Train loss: 70.010.. Val loss: 54.197.. Train L1 norm: 3.505.. Val L1 norm: 1.059.. Train Linf norm: 1280.429.. Val Linf norm: 28.128\n",
            "Epoch 60/145.. Train loss: 64.094.. Val loss: 54.196.. Train L1 norm: 3.277.. Val L1 norm: 1.059.. Train Linf norm: 1161.743.. Val Linf norm: 28.146\n",
            "Epoch 61/145.. Train loss: 103.779.. Val loss: 54.192.. Train L1 norm: 3.398.. Val L1 norm: 1.059.. Train Linf norm: 1225.910.. Val Linf norm: 28.219\n",
            "Epoch 62/145.. Train loss: 82.423.. Val loss: 54.189.. Train L1 norm: 3.977.. Val L1 norm: 1.059.. Train Linf norm: 1521.464.. Val Linf norm: 28.270\n",
            "Epoch 63/145.. Train loss: 61.420.. Val loss: 54.189.. Train L1 norm: 2.395.. Val L1 norm: 1.059.. Train Linf norm: 711.822.. Val Linf norm: 28.265\n",
            "Epoch 64/145.. Train loss: 262.580.. Val loss: 54.181.. Train L1 norm: 2.256.. Val L1 norm: 1.060.. Train Linf norm: 638.772.. Val Linf norm: 28.392\n",
            "Epoch 65/145.. Train loss: 74.615.. Val loss: 54.179.. Train L1 norm: 3.576.. Val L1 norm: 1.060.. Train Linf norm: 1317.500.. Val Linf norm: 28.428\n",
            "Epoch 66/145.. Train loss: 74.451.. Val loss: 54.181.. Train L1 norm: 2.008.. Val L1 norm: 1.060.. Train Linf norm: 513.001.. Val Linf norm: 28.400\n",
            "Epoch 67/145.. Train loss: 75.314.. Val loss: 54.178.. Train L1 norm: 4.411.. Val L1 norm: 1.060.. Train Linf norm: 1744.200.. Val Linf norm: 28.439\n",
            "Epoch 68/145.. Train loss: 61.735.. Val loss: 54.178.. Train L1 norm: 3.415.. Val L1 norm: 1.060.. Train Linf norm: 1234.290.. Val Linf norm: 28.453\n",
            "Epoch 69/145.. Train loss: 63.226.. Val loss: 54.179.. Train L1 norm: 3.694.. Val L1 norm: 1.060.. Train Linf norm: 1376.996.. Val Linf norm: 28.442\n",
            "Epoch 70/145.. Train loss: 96.806.. Val loss: 54.179.. Train L1 norm: 2.871.. Val L1 norm: 1.060.. Train Linf norm: 954.830.. Val Linf norm: 28.431\n",
            "Epoch 71/145.. Train loss: 113.230.. Val loss: 54.178.. Train L1 norm: 2.616.. Val L1 norm: 1.060.. Train Linf norm: 825.404.. Val Linf norm: 28.455\n",
            "Epoch 72/145.. Train loss: 59.845.. Val loss: 54.178.. Train L1 norm: 3.143.. Val L1 norm: 1.060.. Train Linf norm: 1095.179.. Val Linf norm: 28.459\n",
            "Epoch 73/145.. Train loss: 105.052.. Val loss: 54.182.. Train L1 norm: 1.113.. Val L1 norm: 1.060.. Train Linf norm: 55.159.. Val Linf norm: 28.391\n",
            "Epoch 74/145.. Train loss: 63.741.. Val loss: 54.183.. Train L1 norm: 2.922.. Val L1 norm: 1.060.. Train Linf norm: 982.257.. Val Linf norm: 28.379\n",
            "Epoch 75/145.. Train loss: 61.368.. Val loss: 54.184.. Train L1 norm: 3.576.. Val L1 norm: 1.060.. Train Linf norm: 1316.964.. Val Linf norm: 28.373\n",
            "Epoch 76/145.. Train loss: 77.564.. Val loss: 54.187.. Train L1 norm: 2.991.. Val L1 norm: 1.060.. Train Linf norm: 1016.444.. Val Linf norm: 28.325\n",
            "Epoch 77/145.. Train loss: 60.451.. Val loss: 54.187.. Train L1 norm: 3.192.. Val L1 norm: 1.060.. Train Linf norm: 1120.038.. Val Linf norm: 28.327\n",
            "Epoch 78/145.. Train loss: 95.237.. Val loss: 54.192.. Train L1 norm: 2.937.. Val L1 norm: 1.059.. Train Linf norm: 989.796.. Val Linf norm: 28.247\n",
            "Epoch 79/145.. Train loss: 70.636.. Val loss: 54.189.. Train L1 norm: 2.149.. Val L1 norm: 1.059.. Train Linf norm: 585.445.. Val Linf norm: 28.292\n",
            "Epoch 80/145.. Train loss: 90.670.. Val loss: 54.194.. Train L1 norm: 2.778.. Val L1 norm: 1.059.. Train Linf norm: 907.413.. Val Linf norm: 28.225\n",
            "Epoch 81/145.. Train loss: 64.072.. Val loss: 54.192.. Train L1 norm: 2.416.. Val L1 norm: 1.059.. Train Linf norm: 722.967.. Val Linf norm: 28.253\n",
            "Epoch 82/145.. Train loss: 79.617.. Val loss: 54.188.. Train L1 norm: 3.086.. Val L1 norm: 1.060.. Train Linf norm: 1066.370.. Val Linf norm: 28.316\n",
            "Epoch 83/145.. Train loss: 166.302.. Val loss: 54.180.. Train L1 norm: 3.434.. Val L1 norm: 1.060.. Train Linf norm: 1244.048.. Val Linf norm: 28.444\n",
            "Epoch 84/145.. Train loss: 119.708.. Val loss: 54.175.. Train L1 norm: 3.904.. Val L1 norm: 1.060.. Train Linf norm: 1483.471.. Val Linf norm: 28.535\n",
            "Epoch 85/145.. Train loss: 74.140.. Val loss: 54.177.. Train L1 norm: 2.990.. Val L1 norm: 1.060.. Train Linf norm: 1016.862.. Val Linf norm: 28.498\n",
            "Epoch 86/145.. Train loss: 86.597.. Val loss: 54.173.. Train L1 norm: 2.203.. Val L1 norm: 1.060.. Train Linf norm: 613.853.. Val Linf norm: 28.562\n",
            "Epoch 87/145.. Train loss: 65.019.. Val loss: 54.172.. Train L1 norm: 4.183.. Val L1 norm: 1.060.. Train Linf norm: 1627.620.. Val Linf norm: 28.589\n",
            "Epoch 88/145.. Train loss: 59.820.. Val loss: 54.171.. Train L1 norm: 3.281.. Val L1 norm: 1.060.. Train Linf norm: 1165.446.. Val Linf norm: 28.593\n",
            "Epoch 89/145.. Train loss: 74.132.. Val loss: 54.168.. Train L1 norm: 4.229.. Val L1 norm: 1.060.. Train Linf norm: 1651.626.. Val Linf norm: 28.650\n",
            "Epoch 90/145.. Train loss: 80.345.. Val loss: 54.164.. Train L1 norm: 2.993.. Val L1 norm: 1.060.. Train Linf norm: 1017.807.. Val Linf norm: 28.710\n",
            "Epoch 91/145.. Train loss: 88.176.. Val loss: 54.160.. Train L1 norm: 2.630.. Val L1 norm: 1.061.. Train Linf norm: 832.283.. Val Linf norm: 28.775\n",
            "Epoch 92/145.. Train loss: 133.167.. Val loss: 54.165.. Train L1 norm: 2.433.. Val L1 norm: 1.060.. Train Linf norm: 731.265.. Val Linf norm: 28.695\n",
            "Epoch 93/145.. Train loss: 87.186.. Val loss: 54.162.. Train L1 norm: 2.805.. Val L1 norm: 1.061.. Train Linf norm: 921.331.. Val Linf norm: 28.746\n",
            "Epoch 94/145.. Train loss: 65.345.. Val loss: 54.163.. Train L1 norm: 3.718.. Val L1 norm: 1.061.. Train Linf norm: 1389.430.. Val Linf norm: 28.727\n",
            "Epoch 95/145.. Train loss: 68.909.. Val loss: 54.161.. Train L1 norm: 3.204.. Val L1 norm: 1.061.. Train Linf norm: 1126.457.. Val Linf norm: 28.765\n",
            "Epoch 96/145.. Train loss: 60.296.. Val loss: 54.161.. Train L1 norm: 3.632.. Val L1 norm: 1.061.. Train Linf norm: 1345.137.. Val Linf norm: 28.763\n",
            "Epoch 97/145.. Train loss: 62.308.. Val loss: 54.162.. Train L1 norm: 2.797.. Val L1 norm: 1.061.. Train Linf norm: 917.171.. Val Linf norm: 28.751\n",
            "Epoch 98/145.. Train loss: 60.045.. Val loss: 54.162.. Train L1 norm: 2.536.. Val L1 norm: 1.061.. Train Linf norm: 783.136.. Val Linf norm: 28.758\n",
            "Epoch 99/145.. Train loss: 86.509.. Val loss: 54.166.. Train L1 norm: 3.341.. Val L1 norm: 1.060.. Train Linf norm: 1196.668.. Val Linf norm: 28.689\n",
            "Epoch 100/145.. Train loss: 60.258.. Val loss: 54.167.. Train L1 norm: 2.524.. Val L1 norm: 1.060.. Train Linf norm: 778.474.. Val Linf norm: 28.686\n",
            "Epoch 101/145.. Train loss: 81.923.. Val loss: 54.162.. Train L1 norm: 2.289.. Val L1 norm: 1.061.. Train Linf norm: 650.702.. Val Linf norm: 28.762\n",
            "Epoch 102/145.. Train loss: 65.582.. Val loss: 54.164.. Train L1 norm: 3.356.. Val L1 norm: 1.061.. Train Linf norm: 1202.919.. Val Linf norm: 28.729\n",
            "Epoch 103/145.. Train loss: 61.877.. Val loss: 54.163.. Train L1 norm: 2.173.. Val L1 norm: 1.061.. Train Linf norm: 598.502.. Val Linf norm: 28.750\n",
            "Epoch 104/145.. Train loss: 60.900.. Val loss: 54.162.. Train L1 norm: 3.462.. Val L1 norm: 1.061.. Train Linf norm: 1258.525.. Val Linf norm: 28.772\n",
            "Epoch 105/145.. Train loss: 68.346.. Val loss: 54.165.. Train L1 norm: 3.202.. Val L1 norm: 1.061.. Train Linf norm: 1123.105.. Val Linf norm: 28.726\n",
            "Epoch 106/145.. Train loss: 264.654.. Val loss: 54.165.. Train L1 norm: 3.594.. Val L1 norm: 1.060.. Train Linf norm: 1325.889.. Val Linf norm: 28.718\n",
            "Epoch 107/145.. Train loss: 61.172.. Val loss: 54.174.. Train L1 norm: 3.014.. Val L1 norm: 1.060.. Train Linf norm: 1028.932.. Val Linf norm: 28.582\n",
            "Epoch 108/145.. Train loss: 103.860.. Val loss: 54.179.. Train L1 norm: 3.495.. Val L1 norm: 1.060.. Train Linf norm: 1274.527.. Val Linf norm: 28.515\n",
            "Epoch 109/145.. Train loss: 69.359.. Val loss: 54.177.. Train L1 norm: 3.436.. Val L1 norm: 1.060.. Train Linf norm: 1244.762.. Val Linf norm: 28.550\n",
            "Epoch 110/145.. Train loss: 84.614.. Val loss: 54.173.. Train L1 norm: 3.062.. Val L1 norm: 1.060.. Train Linf norm: 1052.643.. Val Linf norm: 28.612\n",
            "Epoch 111/145.. Train loss: 59.583.. Val loss: 54.173.. Train L1 norm: 3.753.. Val L1 norm: 1.060.. Train Linf norm: 1407.780.. Val Linf norm: 28.615\n",
            "Epoch 112/145.. Train loss: 121.863.. Val loss: 54.167.. Train L1 norm: 3.381.. Val L1 norm: 1.060.. Train Linf norm: 1216.877.. Val Linf norm: 28.704\n",
            "Epoch 113/145.. Train loss: 60.340.. Val loss: 54.167.. Train L1 norm: 3.003.. Val L1 norm: 1.060.. Train Linf norm: 1023.436.. Val Linf norm: 28.711\n",
            "Epoch 114/145.. Train loss: 103.320.. Val loss: 54.162.. Train L1 norm: 3.783.. Val L1 norm: 1.061.. Train Linf norm: 1422.070.. Val Linf norm: 28.783\n",
            "Epoch 115/145.. Train loss: 64.914.. Val loss: 54.160.. Train L1 norm: 2.551.. Val L1 norm: 1.061.. Train Linf norm: 791.500.. Val Linf norm: 28.817\n",
            "Epoch 116/145.. Train loss: 93.160.. Val loss: 54.164.. Train L1 norm: 2.311.. Val L1 norm: 1.061.. Train Linf norm: 668.910.. Val Linf norm: 28.760\n",
            "Epoch 117/145.. Train loss: 61.016.. Val loss: 54.165.. Train L1 norm: 3.920.. Val L1 norm: 1.061.. Train Linf norm: 1492.685.. Val Linf norm: 28.756\n",
            "Epoch 118/145.. Train loss: 95.339.. Val loss: 54.159.. Train L1 norm: 3.268.. Val L1 norm: 1.061.. Train Linf norm: 1159.066.. Val Linf norm: 28.837\n",
            "Epoch 119/145.. Train loss: 77.803.. Val loss: 54.156.. Train L1 norm: 3.857.. Val L1 norm: 1.061.. Train Linf norm: 1460.673.. Val Linf norm: 28.890\n",
            "Epoch 120/145.. Train loss: 110.676.. Val loss: 54.161.. Train L1 norm: 2.967.. Val L1 norm: 1.061.. Train Linf norm: 1004.470.. Val Linf norm: 28.811\n",
            "Epoch 121/145.. Train loss: 63.751.. Val loss: 54.163.. Train L1 norm: 3.665.. Val L1 norm: 1.061.. Train Linf norm: 1360.900.. Val Linf norm: 28.793\n",
            "Epoch 122/145.. Train loss: 59.641.. Val loss: 54.162.. Train L1 norm: 3.756.. Val L1 norm: 1.061.. Train Linf norm: 1408.524.. Val Linf norm: 28.797\n",
            "Epoch 123/145.. Train loss: 123.196.. Val loss: 54.169.. Train L1 norm: 2.641.. Val L1 norm: 1.060.. Train Linf norm: 837.461.. Val Linf norm: 28.702\n",
            "Epoch 124/145.. Train loss: 103.387.. Val loss: 54.163.. Train L1 norm: 3.236.. Val L1 norm: 1.061.. Train Linf norm: 1142.465.. Val Linf norm: 28.784\n",
            "Epoch 125/145.. Train loss: 73.548.. Val loss: 54.166.. Train L1 norm: 3.478.. Val L1 norm: 1.061.. Train Linf norm: 1266.685.. Val Linf norm: 28.741\n",
            "Epoch 126/145.. Train loss: 59.962.. Val loss: 54.166.. Train L1 norm: 2.742.. Val L1 norm: 1.061.. Train Linf norm: 890.361.. Val Linf norm: 28.742\n",
            "Epoch 127/145.. Train loss: 69.794.. Val loss: 54.166.. Train L1 norm: 2.726.. Val L1 norm: 1.061.. Train Linf norm: 881.213.. Val Linf norm: 28.752\n",
            "Epoch 128/145.. Train loss: 101.757.. Val loss: 54.168.. Train L1 norm: 2.464.. Val L1 norm: 1.061.. Train Linf norm: 747.842.. Val Linf norm: 28.726\n",
            "Epoch 129/145.. Train loss: 71.650.. Val loss: 54.165.. Train L1 norm: 2.166.. Val L1 norm: 1.061.. Train Linf norm: 593.175.. Val Linf norm: 28.767\n",
            "Epoch 130/145.. Train loss: 59.789.. Val loss: 54.165.. Train L1 norm: 2.669.. Val L1 norm: 1.061.. Train Linf norm: 851.391.. Val Linf norm: 28.775\n",
            "Epoch 131/145.. Train loss: 62.271.. Val loss: 54.166.. Train L1 norm: 4.064.. Val L1 norm: 1.061.. Train Linf norm: 1566.244.. Val Linf norm: 28.765\n",
            "Epoch 132/145.. Train loss: 60.812.. Val loss: 54.165.. Train L1 norm: 3.671.. Val L1 norm: 1.061.. Train Linf norm: 1365.304.. Val Linf norm: 28.774\n",
            "Epoch 133/145.. Train loss: 61.898.. Val loss: 54.166.. Train L1 norm: 3.086.. Val L1 norm: 1.061.. Train Linf norm: 1065.281.. Val Linf norm: 28.763\n",
            "Epoch 134/145.. Train loss: 81.278.. Val loss: 54.161.. Train L1 norm: 3.293.. Val L1 norm: 1.061.. Train Linf norm: 1171.167.. Val Linf norm: 28.836\n",
            "Epoch 135/145.. Train loss: 64.905.. Val loss: 54.163.. Train L1 norm: 2.589.. Val L1 norm: 1.061.. Train Linf norm: 810.654.. Val Linf norm: 28.806\n",
            "Epoch 136/145.. Train loss: 135.498.. Val loss: 54.170.. Train L1 norm: 3.592.. Val L1 norm: 1.060.. Train Linf norm: 1325.326.. Val Linf norm: 28.705\n",
            "Epoch 137/145.. Train loss: 73.756.. Val loss: 54.167.. Train L1 norm: 4.130.. Val L1 norm: 1.061.. Train Linf norm: 1600.648.. Val Linf norm: 28.761\n",
            "Epoch 138/145.. Train loss: 155.381.. Val loss: 54.174.. Train L1 norm: 3.167.. Val L1 norm: 1.060.. Train Linf norm: 1107.776.. Val Linf norm: 28.651\n",
            "Epoch 139/145.. Train loss: 60.408.. Val loss: 54.174.. Train L1 norm: 3.211.. Val L1 norm: 1.060.. Train Linf norm: 1129.504.. Val Linf norm: 28.649\n",
            "Epoch 140/145.. Train loss: 59.913.. Val loss: 54.174.. Train L1 norm: 2.193.. Val L1 norm: 1.060.. Train Linf norm: 608.859.. Val Linf norm: 28.654\n",
            "Epoch 141/145.. Train loss: 60.206.. Val loss: 54.174.. Train L1 norm: 2.951.. Val L1 norm: 1.060.. Train Linf norm: 996.461.. Val Linf norm: 28.654\n",
            "Epoch 142/145.. Train loss: 65.665.. Val loss: 54.172.. Train L1 norm: 3.385.. Val L1 norm: 1.060.. Train Linf norm: 1218.626.. Val Linf norm: 28.693\n",
            "Epoch 143/145.. Train loss: 81.192.. Val loss: 54.176.. Train L1 norm: 2.316.. Val L1 norm: 1.060.. Train Linf norm: 671.263.. Val Linf norm: 28.626\n",
            "Epoch 144/145.. Train loss: 68.733.. Val loss: 54.173.. Train L1 norm: 3.021.. Val L1 norm: 1.060.. Train Linf norm: 1032.452.. Val Linf norm: 28.673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 08:49:45,884]\u001b[0m Trial 61 finished with value: 1.0601067168235778 and parameters: {'n_layers': 6, 'n_units_0': 3992, 'n_units_1': 1110, 'n_units_2': 3963, 'n_units_3': 743, 'n_units_4': 566, 'n_units_5': 932, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 1.5046735543510803e-06, 'batch_size': 512, 'n_epochs': 145, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.18492154483800977, 'dropout_rate': 0.026236830914579592, 'weight_decay': 0.0007573289792433787, 'beta1': 0.9306228492303049, 'beta2': 0.9991526623387109, 'factor': 0.12449754916781351, 'patience': 6, 'threshold': 0.0024404796057482933}. Best is trial 22 with value: 1.0150012904485066.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 145/145.. Train loss: 127.007.. Val loss: 54.180.. Train L1 norm: 2.260.. Val L1 norm: 1.060.. Train Linf norm: 642.820.. Val Linf norm: 28.562\n",
            "Epoch 1/150.. Train loss: 994.927.. Val loss: 55.080.. Train L1 norm: 1.708.. Val L1 norm: 1.021.. Train Linf norm: 700.898.. Val Linf norm: 18.039\n",
            "Epoch 2/150.. Train loss: 332.052.. Val loss: 54.559.. Train L1 norm: 2.549.. Val L1 norm: 1.042.. Train Linf norm: 1579.519.. Val Linf norm: 31.751\n",
            "Epoch 3/150.. Train loss: 123.483.. Val loss: 54.354.. Train L1 norm: 3.318.. Val L1 norm: 1.051.. Train Linf norm: 2358.323.. Val Linf norm: 37.882\n",
            "Epoch 4/150.. Train loss: 147.545.. Val loss: 54.620.. Train L1 norm: 1.986.. Val L1 norm: 1.040.. Train Linf norm: 999.436.. Val Linf norm: 30.308\n",
            "Epoch 5/150.. Train loss: 131.867.. Val loss: 54.591.. Train L1 norm: 2.100.. Val L1 norm: 1.042.. Train Linf norm: 1116.701.. Val Linf norm: 31.953\n",
            "Epoch 6/150.. Train loss: 186.175.. Val loss: 54.040.. Train L1 norm: 3.337.. Val L1 norm: 1.069.. Train Linf norm: 2383.626.. Val Linf norm: 49.798\n",
            "Epoch 7/150.. Train loss: 178.900.. Val loss: 53.533.. Train L1 norm: 3.830.. Val L1 norm: 1.103.. Train Linf norm: 2874.245.. Val Linf norm: 70.761\n",
            "Epoch 8/150.. Train loss: 940.337.. Val loss: 54.253.. Train L1 norm: 2.733.. Val L1 norm: 1.066.. Train Linf norm: 1740.178.. Val Linf norm: 47.742\n",
            "Epoch 9/150.. Train loss: 61.686.. Val loss: 54.223.. Train L1 norm: 3.803.. Val L1 norm: 1.068.. Train Linf norm: 2850.697.. Val Linf norm: 49.475\n",
            "Epoch 10/150.. Train loss: 250.789.. Val loss: 54.349.. Train L1 norm: 3.836.. Val L1 norm: 1.062.. Train Linf norm: 2888.672.. Val Linf norm: 45.198\n",
            "Epoch 11/150.. Train loss: 308.391.. Val loss: 54.517.. Train L1 norm: 2.272.. Val L1 norm: 1.056.. Train Linf norm: 1293.798.. Val Linf norm: 41.632\n",
            "Epoch 12/150.. Train loss: 104.617.. Val loss: 54.683.. Train L1 norm: 3.332.. Val L1 norm: 1.048.. Train Linf norm: 2373.408.. Val Linf norm: 36.770\n",
            "Epoch 13/150.. Train loss: 85.325.. Val loss: 54.501.. Train L1 norm: 2.996.. Val L1 norm: 1.058.. Train Linf norm: 2031.474.. Val Linf norm: 43.182\n",
            "Epoch 14/150.. Train loss: 78.375.. Val loss: 54.639.. Train L1 norm: 2.762.. Val L1 norm: 1.051.. Train Linf norm: 1791.270.. Val Linf norm: 38.789\n",
            "Epoch 15/150.. Train loss: 188.211.. Val loss: 54.652.. Train L1 norm: 3.083.. Val L1 norm: 1.050.. Train Linf norm: 2120.177.. Val Linf norm: 38.422\n",
            "Epoch 16/150.. Train loss: 104.101.. Val loss: 54.621.. Train L1 norm: 3.390.. Val L1 norm: 1.052.. Train Linf norm: 2433.373.. Val Linf norm: 39.571\n",
            "Epoch 17/150.. Train loss: 65.871.. Val loss: 54.598.. Train L1 norm: 2.602.. Val L1 norm: 1.053.. Train Linf norm: 1626.954.. Val Linf norm: 40.453\n",
            "Epoch 18/150.. Train loss: 362.287.. Val loss: 54.706.. Train L1 norm: 3.207.. Val L1 norm: 1.048.. Train Linf norm: 2245.486.. Val Linf norm: 36.759\n",
            "Epoch 19/150.. Train loss: 69.283.. Val loss: 54.694.. Train L1 norm: 3.264.. Val L1 norm: 1.048.. Train Linf norm: 2306.447.. Val Linf norm: 37.208\n",
            "Epoch 20/150.. Train loss: 67.841.. Val loss: 54.671.. Train L1 norm: 2.353.. Val L1 norm: 1.050.. Train Linf norm: 1376.004.. Val Linf norm: 38.033\n",
            "Epoch 21/150.. Train loss: 68.946.. Val loss: 54.690.. Train L1 norm: 1.871.. Val L1 norm: 1.049.. Train Linf norm: 881.299.. Val Linf norm: 37.432\n",
            "Epoch 22/150.. Train loss: 60.123.. Val loss: 54.691.. Train L1 norm: 2.637.. Val L1 norm: 1.049.. Train Linf norm: 1668.090.. Val Linf norm: 37.428\n",
            "Epoch 23/150.. Train loss: 59.749.. Val loss: 54.690.. Train L1 norm: 2.489.. Val L1 norm: 1.049.. Train Linf norm: 1517.261.. Val Linf norm: 37.443\n",
            "Epoch 24/150.. Train loss: 84.237.. Val loss: 54.689.. Train L1 norm: 3.515.. Val L1 norm: 1.049.. Train Linf norm: 2564.383.. Val Linf norm: 37.512\n",
            "Epoch 25/150.. Train loss: 95.654.. Val loss: 54.686.. Train L1 norm: 1.891.. Val L1 norm: 1.049.. Train Linf norm: 901.870.. Val Linf norm: 37.628\n",
            "Epoch 26/150.. Train loss: 169.219.. Val loss: 54.693.. Train L1 norm: 1.855.. Val L1 norm: 1.049.. Train Linf norm: 865.423.. Val Linf norm: 37.408\n",
            "Epoch 27/150.. Train loss: 67.017.. Val loss: 54.691.. Train L1 norm: 2.774.. Val L1 norm: 1.049.. Train Linf norm: 1805.037.. Val Linf norm: 37.513\n",
            "Epoch 28/150.. Train loss: 240.932.. Val loss: 54.695.. Train L1 norm: 3.331.. Val L1 norm: 1.049.. Train Linf norm: 2375.550.. Val Linf norm: 37.366\n",
            "Epoch 29/150.. Train loss: 61.553.. Val loss: 54.697.. Train L1 norm: 3.370.. Val L1 norm: 1.048.. Train Linf norm: 2416.385.. Val Linf norm: 37.321\n",
            "Epoch 30/150.. Train loss: 155.532.. Val loss: 54.696.. Train L1 norm: 2.405.. Val L1 norm: 1.049.. Train Linf norm: 1428.335.. Val Linf norm: 37.356\n",
            "Epoch 31/150.. Train loss: 104.277.. Val loss: 54.695.. Train L1 norm: 2.415.. Val L1 norm: 1.049.. Train Linf norm: 1439.425.. Val Linf norm: 37.383\n",
            "Epoch 32/150.. Train loss: 75.388.. Val loss: 54.695.. Train L1 norm: 3.016.. Val L1 norm: 1.049.. Train Linf norm: 2054.071.. Val Linf norm: 37.373\n",
            "Epoch 33/150.. Train loss: 125.351.. Val loss: 54.695.. Train L1 norm: 2.806.. Val L1 norm: 1.049.. Train Linf norm: 1837.827.. Val Linf norm: 37.393\n",
            "Epoch 34/150.. Train loss: 168.395.. Val loss: 54.693.. Train L1 norm: 3.252.. Val L1 norm: 1.049.. Train Linf norm: 2296.636.. Val Linf norm: 37.478\n",
            "Epoch 35/150.. Train loss: 105.315.. Val loss: 54.692.. Train L1 norm: 3.764.. Val L1 norm: 1.049.. Train Linf norm: 2819.699.. Val Linf norm: 37.512\n",
            "Epoch 36/150.. Train loss: 60.284.. Val loss: 54.692.. Train L1 norm: 1.682.. Val L1 norm: 1.049.. Train Linf norm: 685.879.. Val Linf norm: 37.518\n",
            "Epoch 37/150.. Train loss: 71.445.. Val loss: 54.692.. Train L1 norm: 3.661.. Val L1 norm: 1.049.. Train Linf norm: 2713.107.. Val Linf norm: 37.512\n",
            "Epoch 38/150.. Train loss: 60.972.. Val loss: 54.692.. Train L1 norm: 2.876.. Val L1 norm: 1.049.. Train Linf norm: 1910.844.. Val Linf norm: 37.509\n",
            "Epoch 39/150.. Train loss: 91.161.. Val loss: 54.692.. Train L1 norm: 3.021.. Val L1 norm: 1.049.. Train Linf norm: 2056.456.. Val Linf norm: 37.492\n",
            "Epoch 40/150.. Train loss: 59.718.. Val loss: 54.692.. Train L1 norm: 1.996.. Val L1 norm: 1.049.. Train Linf norm: 1010.470.. Val Linf norm: 37.493\n",
            "Epoch 41/150.. Train loss: 60.124.. Val loss: 54.692.. Train L1 norm: 3.254.. Val L1 norm: 1.049.. Train Linf norm: 2299.418.. Val Linf norm: 37.494\n",
            "Epoch 42/150.. Train loss: 132.465.. Val loss: 54.693.. Train L1 norm: 2.681.. Val L1 norm: 1.049.. Train Linf norm: 1711.419.. Val Linf norm: 37.485\n",
            "Epoch 43/150.. Train loss: 60.526.. Val loss: 54.693.. Train L1 norm: 3.179.. Val L1 norm: 1.049.. Train Linf norm: 2217.636.. Val Linf norm: 37.488\n",
            "Epoch 44/150.. Train loss: 68.903.. Val loss: 54.693.. Train L1 norm: 1.950.. Val L1 norm: 1.049.. Train Linf norm: 962.823.. Val Linf norm: 37.478\n",
            "Epoch 45/150.. Train loss: 62.338.. Val loss: 54.693.. Train L1 norm: 2.023.. Val L1 norm: 1.049.. Train Linf norm: 1038.834.. Val Linf norm: 37.471\n",
            "Epoch 46/150.. Train loss: 65.831.. Val loss: 54.694.. Train L1 norm: 2.496.. Val L1 norm: 1.049.. Train Linf norm: 1522.187.. Val Linf norm: 37.465\n",
            "Epoch 47/150.. Train loss: 150.091.. Val loss: 54.694.. Train L1 norm: 2.283.. Val L1 norm: 1.049.. Train Linf norm: 1303.554.. Val Linf norm: 37.452\n",
            "Epoch 48/150.. Train loss: 68.775.. Val loss: 54.694.. Train L1 norm: 3.364.. Val L1 norm: 1.049.. Train Linf norm: 2410.645.. Val Linf norm: 37.436\n",
            "Epoch 49/150.. Train loss: 122.313.. Val loss: 54.693.. Train L1 norm: 2.867.. Val L1 norm: 1.049.. Train Linf norm: 1899.227.. Val Linf norm: 37.493\n",
            "Epoch 50/150.. Train loss: 127.731.. Val loss: 54.691.. Train L1 norm: 3.322.. Val L1 norm: 1.049.. Train Linf norm: 2368.126.. Val Linf norm: 37.550\n",
            "Epoch 51/150.. Train loss: 207.917.. Val loss: 54.689.. Train L1 norm: 3.800.. Val L1 norm: 1.049.. Train Linf norm: 2854.828.. Val Linf norm: 37.631\n",
            "Epoch 52/150.. Train loss: 64.261.. Val loss: 54.689.. Train L1 norm: 2.825.. Val L1 norm: 1.049.. Train Linf norm: 1858.880.. Val Linf norm: 37.623\n",
            "Epoch 53/150.. Train loss: 203.871.. Val loss: 54.692.. Train L1 norm: 3.471.. Val L1 norm: 1.049.. Train Linf norm: 2518.464.. Val Linf norm: 37.545\n",
            "Epoch 54/150.. Train loss: 278.141.. Val loss: 54.690.. Train L1 norm: 3.416.. Val L1 norm: 1.049.. Train Linf norm: 2464.393.. Val Linf norm: 37.637\n",
            "Epoch 55/150.. Train loss: 118.670.. Val loss: 54.691.. Train L1 norm: 2.885.. Val L1 norm: 1.049.. Train Linf norm: 1917.714.. Val Linf norm: 37.602\n",
            "Epoch 56/150.. Train loss: 70.949.. Val loss: 54.691.. Train L1 norm: 2.996.. Val L1 norm: 1.049.. Train Linf norm: 2033.419.. Val Linf norm: 37.605\n",
            "Epoch 57/150.. Train loss: 132.447.. Val loss: 54.692.. Train L1 norm: 2.474.. Val L1 norm: 1.049.. Train Linf norm: 1498.524.. Val Linf norm: 37.555\n",
            "Epoch 58/150.. Train loss: 64.604.. Val loss: 54.692.. Train L1 norm: 2.092.. Val L1 norm: 1.049.. Train Linf norm: 1107.223.. Val Linf norm: 37.547\n",
            "Epoch 59/150.. Train loss: 95.167.. Val loss: 54.692.. Train L1 norm: 2.106.. Val L1 norm: 1.049.. Train Linf norm: 1124.601.. Val Linf norm: 37.576\n",
            "Epoch 60/150.. Train loss: 64.765.. Val loss: 54.691.. Train L1 norm: 2.126.. Val L1 norm: 1.049.. Train Linf norm: 1142.249.. Val Linf norm: 37.587\n",
            "Epoch 61/150.. Train loss: 73.557.. Val loss: 54.692.. Train L1 norm: 1.904.. Val L1 norm: 1.049.. Train Linf norm: 916.360.. Val Linf norm: 37.570\n",
            "Epoch 62/150.. Train loss: 89.729.. Val loss: 54.691.. Train L1 norm: 3.764.. Val L1 norm: 1.049.. Train Linf norm: 2820.572.. Val Linf norm: 37.604\n",
            "Epoch 63/150.. Train loss: 61.959.. Val loss: 54.691.. Train L1 norm: 4.099.. Val L1 norm: 1.049.. Train Linf norm: 3162.024.. Val Linf norm: 37.610\n",
            "Epoch 64/150.. Train loss: 334.742.. Val loss: 54.691.. Train L1 norm: 2.761.. Val L1 norm: 1.049.. Train Linf norm: 1791.795.. Val Linf norm: 37.596\n",
            "Epoch 65/150.. Train loss: 143.256.. Val loss: 54.694.. Train L1 norm: 3.538.. Val L1 norm: 1.049.. Train Linf norm: 2588.226.. Val Linf norm: 37.501\n",
            "Epoch 66/150.. Train loss: 61.203.. Val loss: 54.694.. Train L1 norm: 1.986.. Val L1 norm: 1.049.. Train Linf norm: 999.594.. Val Linf norm: 37.505\n",
            "Epoch 67/150.. Train loss: 303.906.. Val loss: 54.692.. Train L1 norm: 2.784.. Val L1 norm: 1.049.. Train Linf norm: 1817.353.. Val Linf norm: 37.589\n",
            "Epoch 68/150.. Train loss: 65.311.. Val loss: 54.692.. Train L1 norm: 2.114.. Val L1 norm: 1.049.. Train Linf norm: 1129.781.. Val Linf norm: 37.589\n",
            "Epoch 69/150.. Train loss: 89.418.. Val loss: 54.691.. Train L1 norm: 3.579.. Val L1 norm: 1.049.. Train Linf norm: 2629.876.. Val Linf norm: 37.601\n",
            "Epoch 70/150.. Train loss: 60.973.. Val loss: 54.691.. Train L1 norm: 3.642.. Val L1 norm: 1.049.. Train Linf norm: 2692.033.. Val Linf norm: 37.610\n",
            "Epoch 71/150.. Train loss: 75.385.. Val loss: 54.690.. Train L1 norm: 3.199.. Val L1 norm: 1.049.. Train Linf norm: 2239.904.. Val Linf norm: 37.639\n",
            "Epoch 72/150.. Train loss: 65.212.. Val loss: 54.690.. Train L1 norm: 2.895.. Val L1 norm: 1.049.. Train Linf norm: 1927.714.. Val Linf norm: 37.652\n",
            "Epoch 73/150.. Train loss: 76.668.. Val loss: 54.690.. Train L1 norm: 3.830.. Val L1 norm: 1.049.. Train Linf norm: 2887.645.. Val Linf norm: 37.632\n",
            "Epoch 74/150.. Train loss: 127.269.. Val loss: 54.691.. Train L1 norm: 3.250.. Val L1 norm: 1.049.. Train Linf norm: 2292.527.. Val Linf norm: 37.618\n",
            "Epoch 75/150.. Train loss: 105.340.. Val loss: 54.692.. Train L1 norm: 2.877.. Val L1 norm: 1.049.. Train Linf norm: 1913.652.. Val Linf norm: 37.587\n",
            "Epoch 76/150.. Train loss: 69.129.. Val loss: 54.692.. Train L1 norm: 2.653.. Val L1 norm: 1.049.. Train Linf norm: 1682.511.. Val Linf norm: 37.586\n",
            "Epoch 77/150.. Train loss: 73.535.. Val loss: 54.693.. Train L1 norm: 3.510.. Val L1 norm: 1.049.. Train Linf norm: 2557.583.. Val Linf norm: 37.571\n",
            "Epoch 78/150.. Train loss: 61.068.. Val loss: 54.693.. Train L1 norm: 2.216.. Val L1 norm: 1.049.. Train Linf norm: 1234.394.. Val Linf norm: 37.568\n",
            "Epoch 79/150.. Train loss: 94.515.. Val loss: 54.693.. Train L1 norm: 2.777.. Val L1 norm: 1.049.. Train Linf norm: 1810.625.. Val Linf norm: 37.557\n",
            "Epoch 80/150.. Train loss: 60.093.. Val loss: 54.693.. Train L1 norm: 3.250.. Val L1 norm: 1.049.. Train Linf norm: 2293.844.. Val Linf norm: 37.560\n",
            "Epoch 81/150.. Train loss: 156.614.. Val loss: 54.695.. Train L1 norm: 3.014.. Val L1 norm: 1.049.. Train Linf norm: 2052.504.. Val Linf norm: 37.499\n",
            "Epoch 82/150.. Train loss: 63.517.. Val loss: 54.695.. Train L1 norm: 3.761.. Val L1 norm: 1.049.. Train Linf norm: 2817.174.. Val Linf norm: 37.509\n",
            "Epoch 83/150.. Train loss: 76.893.. Val loss: 54.694.. Train L1 norm: 2.458.. Val L1 norm: 1.049.. Train Linf norm: 1482.805.. Val Linf norm: 37.540\n",
            "Epoch 84/150.. Train loss: 84.462.. Val loss: 54.693.. Train L1 norm: 2.439.. Val L1 norm: 1.049.. Train Linf norm: 1464.408.. Val Linf norm: 37.588\n",
            "Epoch 85/150.. Train loss: 88.588.. Val loss: 54.694.. Train L1 norm: 2.744.. Val L1 norm: 1.049.. Train Linf norm: 1777.131.. Val Linf norm: 37.538\n",
            "Epoch 86/150.. Train loss: 304.299.. Val loss: 54.691.. Train L1 norm: 3.359.. Val L1 norm: 1.049.. Train Linf norm: 2401.335.. Val Linf norm: 37.639\n",
            "Epoch 87/150.. Train loss: 64.011.. Val loss: 54.691.. Train L1 norm: 1.375.. Val L1 norm: 1.049.. Train Linf norm: 373.109.. Val Linf norm: 37.641\n",
            "Epoch 88/150.. Train loss: 60.073.. Val loss: 54.692.. Train L1 norm: 2.662.. Val L1 norm: 1.049.. Train Linf norm: 1690.738.. Val Linf norm: 37.638\n",
            "Epoch 89/150.. Train loss: 130.489.. Val loss: 54.690.. Train L1 norm: 2.916.. Val L1 norm: 1.049.. Train Linf norm: 1951.240.. Val Linf norm: 37.703\n",
            "Epoch 90/150.. Train loss: 119.514.. Val loss: 54.688.. Train L1 norm: 1.182.. Val L1 norm: 1.049.. Train Linf norm: 175.144.. Val Linf norm: 37.768\n",
            "Epoch 91/150.. Train loss: 163.153.. Val loss: 54.686.. Train L1 norm: 1.807.. Val L1 norm: 1.049.. Train Linf norm: 815.741.. Val Linf norm: 37.846\n",
            "Epoch 92/150.. Train loss: 67.080.. Val loss: 54.685.. Train L1 norm: 4.067.. Val L1 norm: 1.049.. Train Linf norm: 3129.620.. Val Linf norm: 37.858\n",
            "Epoch 93/150.. Train loss: 68.026.. Val loss: 54.685.. Train L1 norm: 3.098.. Val L1 norm: 1.049.. Train Linf norm: 2139.034.. Val Linf norm: 37.875\n",
            "Epoch 94/150.. Train loss: 81.389.. Val loss: 54.686.. Train L1 norm: 2.834.. Val L1 norm: 1.049.. Train Linf norm: 1865.148.. Val Linf norm: 37.852\n",
            "Epoch 95/150.. Train loss: 127.638.. Val loss: 54.684.. Train L1 norm: 2.889.. Val L1 norm: 1.049.. Train Linf norm: 1922.082.. Val Linf norm: 37.913\n",
            "Epoch 96/150.. Train loss: 97.128.. Val loss: 54.685.. Train L1 norm: 3.719.. Val L1 norm: 1.049.. Train Linf norm: 2772.946.. Val Linf norm: 37.874\n",
            "Epoch 97/150.. Train loss: 63.946.. Val loss: 54.685.. Train L1 norm: 1.699.. Val L1 norm: 1.049.. Train Linf norm: 705.710.. Val Linf norm: 37.881\n",
            "Epoch 98/150.. Train loss: 71.481.. Val loss: 54.684.. Train L1 norm: 1.736.. Val L1 norm: 1.049.. Train Linf norm: 742.805.. Val Linf norm: 37.911\n",
            "Epoch 99/150.. Train loss: 179.195.. Val loss: 54.683.. Train L1 norm: 2.334.. Val L1 norm: 1.049.. Train Linf norm: 1355.480.. Val Linf norm: 37.967\n",
            "Epoch 100/150.. Train loss: 84.766.. Val loss: 54.683.. Train L1 norm: 2.014.. Val L1 norm: 1.049.. Train Linf norm: 1028.422.. Val Linf norm: 37.957\n",
            "Epoch 101/150.. Train loss: 59.878.. Val loss: 54.684.. Train L1 norm: 1.809.. Val L1 norm: 1.049.. Train Linf norm: 817.371.. Val Linf norm: 37.928\n",
            "Epoch 102/150.. Train loss: 67.608.. Val loss: 54.683.. Train L1 norm: 3.311.. Val L1 norm: 1.049.. Train Linf norm: 2355.901.. Val Linf norm: 37.944\n",
            "Epoch 103/150.. Train loss: 86.395.. Val loss: 54.684.. Train L1 norm: 3.270.. Val L1 norm: 1.049.. Train Linf norm: 2313.374.. Val Linf norm: 37.929\n",
            "Epoch 104/150.. Train loss: 115.426.. Val loss: 54.682.. Train L1 norm: 3.012.. Val L1 norm: 1.049.. Train Linf norm: 2047.185.. Val Linf norm: 37.988\n",
            "Epoch 105/150.. Train loss: 87.132.. Val loss: 54.683.. Train L1 norm: 2.116.. Val L1 norm: 1.049.. Train Linf norm: 1133.705.. Val Linf norm: 37.950\n",
            "Epoch 106/150.. Train loss: 76.097.. Val loss: 54.683.. Train L1 norm: 4.074.. Val L1 norm: 1.049.. Train Linf norm: 3137.681.. Val Linf norm: 37.973\n",
            "Epoch 107/150.. Train loss: 62.998.. Val loss: 54.682.. Train L1 norm: 3.199.. Val L1 norm: 1.049.. Train Linf norm: 2237.343.. Val Linf norm: 37.988\n",
            "Epoch 108/150.. Train loss: 60.025.. Val loss: 54.682.. Train L1 norm: 3.164.. Val L1 norm: 1.049.. Train Linf norm: 2205.266.. Val Linf norm: 37.991\n",
            "Epoch 109/150.. Train loss: 61.624.. Val loss: 54.682.. Train L1 norm: 3.022.. Val L1 norm: 1.049.. Train Linf norm: 2058.700.. Val Linf norm: 37.997\n",
            "Epoch 110/150.. Train loss: 71.238.. Val loss: 54.682.. Train L1 norm: 2.568.. Val L1 norm: 1.049.. Train Linf norm: 1593.689.. Val Linf norm: 38.005\n",
            "Epoch 111/150.. Train loss: 138.861.. Val loss: 54.679.. Train L1 norm: 2.660.. Val L1 norm: 1.050.. Train Linf norm: 1688.227.. Val Linf norm: 38.104\n",
            "Epoch 112/150.. Train loss: 75.297.. Val loss: 54.679.. Train L1 norm: 2.932.. Val L1 norm: 1.050.. Train Linf norm: 1966.905.. Val Linf norm: 38.112\n",
            "Epoch 113/150.. Train loss: 70.126.. Val loss: 54.679.. Train L1 norm: 3.478.. Val L1 norm: 1.050.. Train Linf norm: 2526.411.. Val Linf norm: 38.116\n",
            "Epoch 114/150.. Train loss: 81.283.. Val loss: 54.678.. Train L1 norm: 3.083.. Val L1 norm: 1.050.. Train Linf norm: 2121.379.. Val Linf norm: 38.160\n",
            "Epoch 115/150.. Train loss: 114.145.. Val loss: 54.678.. Train L1 norm: 2.256.. Val L1 norm: 1.050.. Train Linf norm: 1269.761.. Val Linf norm: 38.146\n",
            "Epoch 116/150.. Train loss: 79.270.. Val loss: 54.677.. Train L1 norm: 3.266.. Val L1 norm: 1.050.. Train Linf norm: 2309.647.. Val Linf norm: 38.176\n",
            "Epoch 117/150.. Train loss: 72.839.. Val loss: 54.678.. Train L1 norm: 2.797.. Val L1 norm: 1.050.. Train Linf norm: 1828.729.. Val Linf norm: 38.156\n",
            "Epoch 118/150.. Train loss: 59.867.. Val loss: 54.678.. Train L1 norm: 2.420.. Val L1 norm: 1.050.. Train Linf norm: 1445.043.. Val Linf norm: 38.159\n",
            "Epoch 119/150.. Train loss: 61.075.. Val loss: 54.678.. Train L1 norm: 2.480.. Val L1 norm: 1.050.. Train Linf norm: 1505.138.. Val Linf norm: 38.164\n",
            "Epoch 120/150.. Train loss: 87.848.. Val loss: 54.679.. Train L1 norm: 2.205.. Val L1 norm: 1.050.. Train Linf norm: 1224.241.. Val Linf norm: 38.113\n",
            "Epoch 121/150.. Train loss: 69.597.. Val loss: 54.678.. Train L1 norm: 2.379.. Val L1 norm: 1.050.. Train Linf norm: 1403.116.. Val Linf norm: 38.148\n",
            "Epoch 122/150.. Train loss: 154.994.. Val loss: 54.676.. Train L1 norm: 2.228.. Val L1 norm: 1.050.. Train Linf norm: 1245.235.. Val Linf norm: 38.247\n",
            "Epoch 123/150.. Train loss: 101.602.. Val loss: 54.677.. Train L1 norm: 4.025.. Val L1 norm: 1.050.. Train Linf norm: 3086.021.. Val Linf norm: 38.210\n",
            "Epoch 124/150.. Train loss: 59.880.. Val loss: 54.677.. Train L1 norm: 2.584.. Val L1 norm: 1.050.. Train Linf norm: 1612.443.. Val Linf norm: 38.209\n",
            "Epoch 125/150.. Train loss: 107.703.. Val loss: 54.676.. Train L1 norm: 3.539.. Val L1 norm: 1.050.. Train Linf norm: 2579.384.. Val Linf norm: 38.267\n",
            "Epoch 126/150.. Train loss: 63.161.. Val loss: 54.675.. Train L1 norm: 2.280.. Val L1 norm: 1.050.. Train Linf norm: 1299.692.. Val Linf norm: 38.285\n",
            "Epoch 127/150.. Train loss: 79.538.. Val loss: 54.674.. Train L1 norm: 3.970.. Val L1 norm: 1.050.. Train Linf norm: 3030.249.. Val Linf norm: 38.337\n",
            "Epoch 128/150.. Train loss: 87.412.. Val loss: 54.673.. Train L1 norm: 2.999.. Val L1 norm: 1.050.. Train Linf norm: 2036.402.. Val Linf norm: 38.375\n",
            "Epoch 129/150.. Train loss: 63.971.. Val loss: 54.673.. Train L1 norm: 2.990.. Val L1 norm: 1.050.. Train Linf norm: 2026.089.. Val Linf norm: 38.362\n",
            "Epoch 130/150.. Train loss: 59.931.. Val loss: 54.673.. Train L1 norm: 2.656.. Val L1 norm: 1.050.. Train Linf norm: 1687.033.. Val Linf norm: 38.362\n",
            "Epoch 131/150.. Train loss: 88.630.. Val loss: 54.672.. Train L1 norm: 3.085.. Val L1 norm: 1.050.. Train Linf norm: 2126.079.. Val Linf norm: 38.413\n",
            "Epoch 132/150.. Train loss: 59.679.. Val loss: 54.671.. Train L1 norm: 1.701.. Val L1 norm: 1.050.. Train Linf norm: 708.589.. Val Linf norm: 38.432\n",
            "Epoch 133/150.. Train loss: 79.169.. Val loss: 54.670.. Train L1 norm: 1.701.. Val L1 norm: 1.050.. Train Linf norm: 707.750.. Val Linf norm: 38.479\n",
            "Epoch 134/150.. Train loss: 140.919.. Val loss: 54.668.. Train L1 norm: 2.353.. Val L1 norm: 1.050.. Train Linf norm: 1370.197.. Val Linf norm: 38.543\n",
            "Epoch 135/150.. Train loss: 72.006.. Val loss: 54.667.. Train L1 norm: 2.918.. Val L1 norm: 1.050.. Train Linf norm: 1946.743.. Val Linf norm: 38.570\n",
            "Epoch 136/150.. Train loss: 59.905.. Val loss: 54.667.. Train L1 norm: 2.727.. Val L1 norm: 1.050.. Train Linf norm: 1758.502.. Val Linf norm: 38.576\n",
            "Epoch 137/150.. Train loss: 84.385.. Val loss: 54.666.. Train L1 norm: 2.292.. Val L1 norm: 1.050.. Train Linf norm: 1310.991.. Val Linf norm: 38.614\n",
            "Epoch 138/150.. Train loss: 60.248.. Val loss: 54.666.. Train L1 norm: 2.978.. Val L1 norm: 1.050.. Train Linf norm: 2015.095.. Val Linf norm: 38.618\n",
            "Epoch 139/150.. Train loss: 134.653.. Val loss: 54.664.. Train L1 norm: 3.353.. Val L1 norm: 1.050.. Train Linf norm: 2401.072.. Val Linf norm: 38.700\n",
            "Epoch 140/150.. Train loss: 108.964.. Val loss: 54.664.. Train L1 norm: 2.562.. Val L1 norm: 1.050.. Train Linf norm: 1589.912.. Val Linf norm: 38.689\n",
            "Epoch 141/150.. Train loss: 100.423.. Val loss: 54.666.. Train L1 norm: 2.858.. Val L1 norm: 1.050.. Train Linf norm: 1889.413.. Val Linf norm: 38.626\n",
            "Epoch 142/150.. Train loss: 144.524.. Val loss: 54.665.. Train L1 norm: 2.616.. Val L1 norm: 1.050.. Train Linf norm: 1644.262.. Val Linf norm: 38.676\n",
            "Epoch 143/150.. Train loss: 92.567.. Val loss: 54.663.. Train L1 norm: 2.631.. Val L1 norm: 1.051.. Train Linf norm: 1659.099.. Val Linf norm: 38.731\n",
            "Epoch 144/150.. Train loss: 141.354.. Val loss: 54.665.. Train L1 norm: 3.350.. Val L1 norm: 1.050.. Train Linf norm: 2394.717.. Val Linf norm: 38.657\n",
            "Epoch 145/150.. Train loss: 329.400.. Val loss: 54.662.. Train L1 norm: 3.167.. Val L1 norm: 1.051.. Train Linf norm: 2207.349.. Val Linf norm: 38.773\n",
            "Epoch 146/150.. Train loss: 65.273.. Val loss: 54.663.. Train L1 norm: 3.073.. Val L1 norm: 1.051.. Train Linf norm: 2111.065.. Val Linf norm: 38.760\n",
            "Epoch 147/150.. Train loss: 62.833.. Val loss: 54.663.. Train L1 norm: 2.424.. Val L1 norm: 1.051.. Train Linf norm: 1446.994.. Val Linf norm: 38.753\n",
            "Epoch 148/150.. Train loss: 61.118.. Val loss: 54.663.. Train L1 norm: 3.239.. Val L1 norm: 1.051.. Train Linf norm: 2281.615.. Val Linf norm: 38.759\n",
            "Epoch 149/150.. Train loss: 217.960.. Val loss: 54.662.. Train L1 norm: 2.818.. Val L1 norm: 1.051.. Train Linf norm: 1849.752.. Val Linf norm: 38.797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 08:54:10,276]\u001b[0m Trial 62 finished with value: 1.050685984992981 and parameters: {'n_layers': 6, 'n_units_0': 3882, 'n_units_1': 718, 'n_units_2': 3391, 'n_units_3': 410, 'n_units_4': 19, 'n_units_5': 1233, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 1.003343760920705e-06, 'batch_size': 1024, 'n_epochs': 150, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.19827068038349666, 'dropout_rate': 0.0353731949133511, 'weight_decay': 0.0006311443859081231, 'beta1': 0.9257649529035097, 'beta2': 0.9991215626145623, 'factor': 0.143232733853902, 'patience': 6, 'threshold': 0.0019699734832802104}. Best is trial 22 with value: 1.0150012904485066.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 150/150.. Train loss: 67.742.. Val loss: 54.661.. Train L1 norm: 3.172.. Val L1 norm: 1.051.. Train Linf norm: 2212.137.. Val Linf norm: 38.846\n",
            "Epoch 1/150.. Train loss: 266.432.. Val loss: 54.100.. Train L1 norm: 2.688.. Val L1 norm: 1.074.. Train Linf norm: 1708.486.. Val Linf norm: 57.531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 08:54:14,269]\u001b[0m Trial 63 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/150.. Train loss: 62.183.. Val loss: 54.305.. Train L1 norm: 1.193.. Val L1 norm: 1.069.. Train Linf norm: 177.746.. Val Linf norm: 54.858\n",
            "Epoch 1/143.. Train loss: 1570.453.. Val loss: 53.628.. Train L1 norm: 3.321.. Val L1 norm: 1.066.. Train Linf norm: 2359.686.. Val Linf norm: 51.626\n",
            "Epoch 2/143.. Train loss: 117.960.. Val loss: 54.177.. Train L1 norm: 2.955.. Val L1 norm: 1.044.. Train Linf norm: 1990.590.. Val Linf norm: 37.798\n",
            "Epoch 3/143.. Train loss: 109.555.. Val loss: 54.710.. Train L1 norm: 2.170.. Val L1 norm: 1.028.. Train Linf norm: 1187.514.. Val Linf norm: 25.509\n",
            "Epoch 4/143.. Train loss: 116.563.. Val loss: 54.555.. Train L1 norm: 3.991.. Val L1 norm: 1.034.. Train Linf norm: 3056.861.. Val Linf norm: 29.830\n",
            "Epoch 5/143.. Train loss: 61.359.. Val loss: 54.358.. Train L1 norm: 2.318.. Val L1 norm: 1.041.. Train Linf norm: 1341.000.. Val Linf norm: 35.516\n",
            "Epoch 6/143.. Train loss: 135.024.. Val loss: 54.191.. Train L1 norm: 3.449.. Val L1 norm: 1.048.. Train Linf norm: 2499.330.. Val Linf norm: 40.497\n",
            "Epoch 7/143.. Train loss: 61.505.. Val loss: 53.764.. Train L1 norm: 3.651.. Val L1 norm: 1.066.. Train Linf norm: 2701.823.. Val Linf norm: 52.376\n",
            "Epoch 8/143.. Train loss: 61.553.. Val loss: 53.861.. Train L1 norm: 2.871.. Val L1 norm: 1.062.. Train Linf norm: 1898.396.. Val Linf norm: 50.168\n",
            "Epoch 9/143.. Train loss: 169.230.. Val loss: 53.887.. Train L1 norm: 2.936.. Val L1 norm: 1.061.. Train Linf norm: 1965.527.. Val Linf norm: 49.522\n",
            "Epoch 10/143.. Train loss: 60.796.. Val loss: 53.940.. Train L1 norm: 4.071.. Val L1 norm: 1.059.. Train Linf norm: 3132.035.. Val Linf norm: 48.125\n",
            "Epoch 11/143.. Train loss: 266.035.. Val loss: 53.966.. Train L1 norm: 1.876.. Val L1 norm: 1.058.. Train Linf norm: 886.148.. Val Linf norm: 47.500\n",
            "Epoch 12/143.. Train loss: 80.220.. Val loss: 53.995.. Train L1 norm: 5.188.. Val L1 norm: 1.057.. Train Linf norm: 4266.773.. Val Linf norm: 46.764\n",
            "Epoch 13/143.. Train loss: 181.937.. Val loss: 54.060.. Train L1 norm: 4.472.. Val L1 norm: 1.055.. Train Linf norm: 3543.668.. Val Linf norm: 45.099\n",
            "Epoch 14/143.. Train loss: 280.849.. Val loss: 54.082.. Train L1 norm: 2.233.. Val L1 norm: 1.054.. Train Linf norm: 1248.246.. Val Linf norm: 44.576\n",
            "Epoch 15/143.. Train loss: 166.617.. Val loss: 54.100.. Train L1 norm: 4.161.. Val L1 norm: 1.053.. Train Linf norm: 3226.226.. Val Linf norm: 44.039\n",
            "Epoch 16/143.. Train loss: 101.487.. Val loss: 54.105.. Train L1 norm: 3.471.. Val L1 norm: 1.053.. Train Linf norm: 2518.043.. Val Linf norm: 43.915\n",
            "Epoch 17/143.. Train loss: 248.421.. Val loss: 54.116.. Train L1 norm: 2.635.. Val L1 norm: 1.053.. Train Linf norm: 1662.584.. Val Linf norm: 43.632\n",
            "Epoch 18/143.. Train loss: 169.372.. Val loss: 54.123.. Train L1 norm: 4.004.. Val L1 norm: 1.052.. Train Linf norm: 3062.496.. Val Linf norm: 43.465\n",
            "Epoch 19/143.. Train loss: 120.628.. Val loss: 54.128.. Train L1 norm: 4.144.. Val L1 norm: 1.052.. Train Linf norm: 3207.297.. Val Linf norm: 43.337\n",
            "Epoch 20/143.. Train loss: 89.358.. Val loss: 54.134.. Train L1 norm: 3.331.. Val L1 norm: 1.052.. Train Linf norm: 2374.572.. Val Linf norm: 43.192\n",
            "Epoch 21/143.. Train loss: 101.079.. Val loss: 54.139.. Train L1 norm: 3.154.. Val L1 norm: 1.052.. Train Linf norm: 2191.701.. Val Linf norm: 43.059\n",
            "Epoch 22/143.. Train loss: 613.337.. Val loss: 54.153.. Train L1 norm: 1.897.. Val L1 norm: 1.051.. Train Linf norm: 909.868.. Val Linf norm: 42.685\n",
            "Epoch 23/143.. Train loss: 74.827.. Val loss: 54.154.. Train L1 norm: 2.206.. Val L1 norm: 1.051.. Train Linf norm: 1224.568.. Val Linf norm: 42.683\n",
            "Epoch 24/143.. Train loss: 158.569.. Val loss: 54.154.. Train L1 norm: 2.909.. Val L1 norm: 1.051.. Train Linf norm: 1942.535.. Val Linf norm: 42.669\n",
            "Epoch 25/143.. Train loss: 61.479.. Val loss: 54.154.. Train L1 norm: 2.717.. Val L1 norm: 1.051.. Train Linf norm: 1745.750.. Val Linf norm: 42.670\n",
            "Epoch 26/143.. Train loss: 291.225.. Val loss: 54.155.. Train L1 norm: 3.274.. Val L1 norm: 1.051.. Train Linf norm: 2315.256.. Val Linf norm: 42.641\n",
            "Epoch 27/143.. Train loss: 76.387.. Val loss: 54.155.. Train L1 norm: 5.454.. Val L1 norm: 1.051.. Train Linf norm: 4548.843.. Val Linf norm: 42.647\n",
            "Epoch 28/143.. Train loss: 208.879.. Val loss: 54.154.. Train L1 norm: 2.824.. Val L1 norm: 1.051.. Train Linf norm: 1856.563.. Val Linf norm: 42.669\n",
            "Epoch 29/143.. Train loss: 59.974.. Val loss: 54.154.. Train L1 norm: 1.699.. Val L1 norm: 1.051.. Train Linf norm: 705.902.. Val Linf norm: 42.670\n",
            "Epoch 30/143.. Train loss: 80.593.. Val loss: 54.154.. Train L1 norm: 2.438.. Val L1 norm: 1.051.. Train Linf norm: 1455.030.. Val Linf norm: 42.678\n",
            "Epoch 31/143.. Train loss: 179.300.. Val loss: 54.153.. Train L1 norm: 1.216.. Val L1 norm: 1.051.. Train Linf norm: 208.311.. Val Linf norm: 42.699\n",
            "Epoch 32/143.. Train loss: 61.288.. Val loss: 54.153.. Train L1 norm: 3.083.. Val L1 norm: 1.051.. Train Linf norm: 2121.011.. Val Linf norm: 42.700\n",
            "Epoch 33/143.. Train loss: 340.230.. Val loss: 54.152.. Train L1 norm: 3.291.. Val L1 norm: 1.051.. Train Linf norm: 2334.916.. Val Linf norm: 42.728\n",
            "Epoch 34/143.. Train loss: 269.275.. Val loss: 54.151.. Train L1 norm: 3.589.. Val L1 norm: 1.051.. Train Linf norm: 2639.688.. Val Linf norm: 42.759\n",
            "Epoch 35/143.. Train loss: 128.064.. Val loss: 54.151.. Train L1 norm: 3.826.. Val L1 norm: 1.051.. Train Linf norm: 2883.438.. Val Linf norm: 42.771\n",
            "Epoch 36/143.. Train loss: 104.655.. Val loss: 54.151.. Train L1 norm: 2.473.. Val L1 norm: 1.051.. Train Linf norm: 1497.806.. Val Linf norm: 42.765\n",
            "Epoch 37/143.. Train loss: 315.298.. Val loss: 54.152.. Train L1 norm: 3.806.. Val L1 norm: 1.051.. Train Linf norm: 2861.515.. Val Linf norm: 42.734\n",
            "Epoch 38/143.. Train loss: 91.940.. Val loss: 54.152.. Train L1 norm: 2.679.. Val L1 norm: 1.051.. Train Linf norm: 1706.991.. Val Linf norm: 42.739\n",
            "Epoch 39/143.. Train loss: 340.342.. Val loss: 54.152.. Train L1 norm: 4.258.. Val L1 norm: 1.051.. Train Linf norm: 3326.508.. Val Linf norm: 42.723\n",
            "Epoch 40/143.. Train loss: 297.423.. Val loss: 54.154.. Train L1 norm: 2.800.. Val L1 norm: 1.051.. Train Linf norm: 1830.812.. Val Linf norm: 42.693\n",
            "Epoch 41/143.. Train loss: 533.418.. Val loss: 54.155.. Train L1 norm: 3.762.. Val L1 norm: 1.051.. Train Linf norm: 2820.123.. Val Linf norm: 42.664\n",
            "Epoch 42/143.. Train loss: 81.181.. Val loss: 54.155.. Train L1 norm: 2.963.. Val L1 norm: 1.051.. Train Linf norm: 1999.211.. Val Linf norm: 42.654\n",
            "Epoch 43/143.. Train loss: 394.017.. Val loss: 54.154.. Train L1 norm: 3.842.. Val L1 norm: 1.051.. Train Linf norm: 2898.315.. Val Linf norm: 42.684\n",
            "Epoch 44/143.. Train loss: 183.799.. Val loss: 54.154.. Train L1 norm: 2.633.. Val L1 norm: 1.051.. Train Linf norm: 1661.134.. Val Linf norm: 42.673\n",
            "Epoch 45/143.. Train loss: 272.131.. Val loss: 54.155.. Train L1 norm: 5.391.. Val L1 norm: 1.051.. Train Linf norm: 4487.081.. Val Linf norm: 42.667\n",
            "Epoch 46/143.. Train loss: 84.515.. Val loss: 54.155.. Train L1 norm: 4.722.. Val L1 norm: 1.051.. Train Linf norm: 3800.869.. Val Linf norm: 42.652\n",
            "Epoch 47/143.. Train loss: 103.001.. Val loss: 54.155.. Train L1 norm: 3.579.. Val L1 norm: 1.051.. Train Linf norm: 2629.885.. Val Linf norm: 42.661\n",
            "Epoch 48/143.. Train loss: 126.440.. Val loss: 54.155.. Train L1 norm: 2.374.. Val L1 norm: 1.051.. Train Linf norm: 1393.451.. Val Linf norm: 42.651\n",
            "Epoch 49/143.. Train loss: 573.706.. Val loss: 54.157.. Train L1 norm: 1.748.. Val L1 norm: 1.051.. Train Linf norm: 749.998.. Val Linf norm: 42.615\n",
            "Epoch 50/143.. Train loss: 60.132.. Val loss: 54.157.. Train L1 norm: 2.760.. Val L1 norm: 1.051.. Train Linf norm: 1786.538.. Val Linf norm: 42.613\n",
            "Epoch 51/143.. Train loss: 90.057.. Val loss: 54.157.. Train L1 norm: 3.377.. Val L1 norm: 1.051.. Train Linf norm: 2424.644.. Val Linf norm: 42.609\n",
            "Epoch 52/143.. Train loss: 88.081.. Val loss: 54.157.. Train L1 norm: 3.400.. Val L1 norm: 1.051.. Train Linf norm: 2443.473.. Val Linf norm: 42.616\n",
            "Epoch 53/143.. Train loss: 729.809.. Val loss: 54.158.. Train L1 norm: 2.123.. Val L1 norm: 1.051.. Train Linf norm: 1137.551.. Val Linf norm: 42.578\n",
            "Epoch 54/143.. Train loss: 67.727.. Val loss: 54.158.. Train L1 norm: 2.929.. Val L1 norm: 1.051.. Train Linf norm: 1965.246.. Val Linf norm: 42.573\n",
            "Epoch 55/143.. Train loss: 62.914.. Val loss: 54.158.. Train L1 norm: 2.623.. Val L1 norm: 1.051.. Train Linf norm: 1648.479.. Val Linf norm: 42.574\n",
            "Epoch 56/143.. Train loss: 155.601.. Val loss: 54.159.. Train L1 norm: 4.376.. Val L1 norm: 1.051.. Train Linf norm: 3443.336.. Val Linf norm: 42.562\n",
            "Epoch 57/143.. Train loss: 69.225.. Val loss: 54.159.. Train L1 norm: 3.078.. Val L1 norm: 1.051.. Train Linf norm: 2115.773.. Val Linf norm: 42.561\n",
            "Epoch 58/143.. Train loss: 119.701.. Val loss: 54.159.. Train L1 norm: 3.161.. Val L1 norm: 1.051.. Train Linf norm: 2199.151.. Val Linf norm: 42.551\n",
            "Epoch 59/143.. Train loss: 60.840.. Val loss: 54.159.. Train L1 norm: 1.555.. Val L1 norm: 1.051.. Train Linf norm: 556.682.. Val Linf norm: 42.552\n",
            "Epoch 60/143.. Train loss: 135.276.. Val loss: 54.160.. Train L1 norm: 4.217.. Val L1 norm: 1.051.. Train Linf norm: 3282.082.. Val Linf norm: 42.541\n",
            "Epoch 61/143.. Train loss: 146.416.. Val loss: 54.160.. Train L1 norm: 2.451.. Val L1 norm: 1.051.. Train Linf norm: 1475.437.. Val Linf norm: 42.526\n",
            "Epoch 62/143.. Train loss: 101.732.. Val loss: 54.160.. Train L1 norm: 2.188.. Val L1 norm: 1.051.. Train Linf norm: 1205.540.. Val Linf norm: 42.536\n",
            "Epoch 63/143.. Train loss: 86.004.. Val loss: 54.160.. Train L1 norm: 4.853.. Val L1 norm: 1.051.. Train Linf norm: 3934.019.. Val Linf norm: 42.532\n",
            "Epoch 64/143.. Train loss: 665.411.. Val loss: 54.159.. Train L1 norm: 5.180.. Val L1 norm: 1.051.. Train Linf norm: 4270.256.. Val Linf norm: 42.562\n",
            "Epoch 65/143.. Train loss: 163.977.. Val loss: 54.159.. Train L1 norm: 1.614.. Val L1 norm: 1.051.. Train Linf norm: 616.941.. Val Linf norm: 42.565\n",
            "Epoch 66/143.. Train loss: 120.524.. Val loss: 54.159.. Train L1 norm: 4.013.. Val L1 norm: 1.051.. Train Linf norm: 3073.023.. Val Linf norm: 42.564\n",
            "Epoch 67/143.. Train loss: 123.609.. Val loss: 54.159.. Train L1 norm: 2.258.. Val L1 norm: 1.051.. Train Linf norm: 1276.420.. Val Linf norm: 42.572\n",
            "Epoch 68/143.. Train loss: 83.045.. Val loss: 54.158.. Train L1 norm: 2.983.. Val L1 norm: 1.051.. Train Linf norm: 2019.299.. Val Linf norm: 42.586\n",
            "Epoch 69/143.. Train loss: 460.458.. Val loss: 54.160.. Train L1 norm: 3.276.. Val L1 norm: 1.051.. Train Linf norm: 2318.343.. Val Linf norm: 42.550\n",
            "Epoch 70/143.. Train loss: 167.461.. Val loss: 54.159.. Train L1 norm: 4.777.. Val L1 norm: 1.051.. Train Linf norm: 3852.578.. Val Linf norm: 42.564\n",
            "Epoch 71/143.. Train loss: 77.188.. Val loss: 54.159.. Train L1 norm: 1.423.. Val L1 norm: 1.051.. Train Linf norm: 421.957.. Val Linf norm: 42.561\n",
            "Epoch 72/143.. Train loss: 262.245.. Val loss: 54.159.. Train L1 norm: 2.290.. Val L1 norm: 1.051.. Train Linf norm: 1310.418.. Val Linf norm: 42.578\n",
            "Epoch 73/143.. Train loss: 234.259.. Val loss: 54.159.. Train L1 norm: 3.758.. Val L1 norm: 1.051.. Train Linf norm: 2813.830.. Val Linf norm: 42.575\n",
            "Epoch 74/143.. Train loss: 518.290.. Val loss: 54.158.. Train L1 norm: 1.828.. Val L1 norm: 1.051.. Train Linf norm: 836.711.. Val Linf norm: 42.603\n",
            "Epoch 75/143.. Train loss: 155.884.. Val loss: 54.158.. Train L1 norm: 2.960.. Val L1 norm: 1.051.. Train Linf norm: 1996.997.. Val Linf norm: 42.596\n",
            "Epoch 76/143.. Train loss: 76.128.. Val loss: 54.158.. Train L1 norm: 1.077.. Val L1 norm: 1.051.. Train Linf norm: 67.576.. Val Linf norm: 42.600\n",
            "Epoch 77/143.. Train loss: 288.220.. Val loss: 54.157.. Train L1 norm: 3.535.. Val L1 norm: 1.051.. Train Linf norm: 2585.404.. Val Linf norm: 42.623\n",
            "Epoch 78/143.. Train loss: 65.242.. Val loss: 54.157.. Train L1 norm: 3.059.. Val L1 norm: 1.051.. Train Linf norm: 2098.671.. Val Linf norm: 42.633\n",
            "Epoch 79/143.. Train loss: 87.629.. Val loss: 54.157.. Train L1 norm: 5.348.. Val L1 norm: 1.051.. Train Linf norm: 4442.440.. Val Linf norm: 42.627\n",
            "Epoch 80/143.. Train loss: 60.039.. Val loss: 54.157.. Train L1 norm: 4.444.. Val L1 norm: 1.051.. Train Linf norm: 3514.053.. Val Linf norm: 42.628\n",
            "Epoch 81/143.. Train loss: 66.181.. Val loss: 54.157.. Train L1 norm: 3.369.. Val L1 norm: 1.051.. Train Linf norm: 2416.667.. Val Linf norm: 42.630\n",
            "Epoch 82/143.. Train loss: 540.450.. Val loss: 54.156.. Train L1 norm: 4.407.. Val L1 norm: 1.051.. Train Linf norm: 3476.883.. Val Linf norm: 42.644\n",
            "Epoch 83/143.. Train loss: 136.488.. Val loss: 54.155.. Train L1 norm: 4.244.. Val L1 norm: 1.051.. Train Linf norm: 3307.667.. Val Linf norm: 42.677\n",
            "Epoch 84/143.. Train loss: 66.198.. Val loss: 54.155.. Train L1 norm: 3.343.. Val L1 norm: 1.051.. Train Linf norm: 2390.487.. Val Linf norm: 42.681\n",
            "Epoch 85/143.. Train loss: 78.508.. Val loss: 54.155.. Train L1 norm: 2.178.. Val L1 norm: 1.051.. Train Linf norm: 1195.022.. Val Linf norm: 42.686\n",
            "Epoch 86/143.. Train loss: 75.214.. Val loss: 54.154.. Train L1 norm: 2.191.. Val L1 norm: 1.051.. Train Linf norm: 1209.723.. Val Linf norm: 42.691\n",
            "Epoch 87/143.. Train loss: 115.366.. Val loss: 54.154.. Train L1 norm: 2.910.. Val L1 norm: 1.051.. Train Linf norm: 1946.130.. Val Linf norm: 42.703\n",
            "Epoch 88/143.. Train loss: 499.650.. Val loss: 54.154.. Train L1 norm: 4.173.. Val L1 norm: 1.051.. Train Linf norm: 3237.421.. Val Linf norm: 42.712\n",
            "Epoch 89/143.. Train loss: 124.983.. Val loss: 54.153.. Train L1 norm: 4.763.. Val L1 norm: 1.051.. Train Linf norm: 3843.827.. Val Linf norm: 42.733\n",
            "Epoch 90/143.. Train loss: 198.361.. Val loss: 54.154.. Train L1 norm: 2.780.. Val L1 norm: 1.051.. Train Linf norm: 1811.436.. Val Linf norm: 42.715\n",
            "Epoch 91/143.. Train loss: 146.008.. Val loss: 54.154.. Train L1 norm: 3.787.. Val L1 norm: 1.051.. Train Linf norm: 2838.825.. Val Linf norm: 42.703\n",
            "Epoch 92/143.. Train loss: 113.743.. Val loss: 54.154.. Train L1 norm: 3.058.. Val L1 norm: 1.051.. Train Linf norm: 2095.346.. Val Linf norm: 42.701\n",
            "Epoch 93/143.. Train loss: 765.687.. Val loss: 54.155.. Train L1 norm: 1.354.. Val L1 norm: 1.051.. Train Linf norm: 350.354.. Val Linf norm: 42.669\n",
            "Epoch 94/143.. Train loss: 150.169.. Val loss: 54.156.. Train L1 norm: 5.164.. Val L1 norm: 1.051.. Train Linf norm: 4254.413.. Val Linf norm: 42.655\n",
            "Epoch 95/143.. Train loss: 79.697.. Val loss: 54.156.. Train L1 norm: 2.307.. Val L1 norm: 1.051.. Train Linf norm: 1317.426.. Val Linf norm: 42.653\n",
            "Epoch 96/143.. Train loss: 561.700.. Val loss: 54.157.. Train L1 norm: 2.636.. Val L1 norm: 1.051.. Train Linf norm: 1664.510.. Val Linf norm: 42.624\n",
            "Epoch 97/143.. Train loss: 63.656.. Val loss: 54.157.. Train L1 norm: 4.608.. Val L1 norm: 1.051.. Train Linf norm: 3681.113.. Val Linf norm: 42.621\n",
            "Epoch 98/143.. Train loss: 73.383.. Val loss: 54.157.. Train L1 norm: 3.348.. Val L1 norm: 1.051.. Train Linf norm: 2394.251.. Val Linf norm: 42.618\n",
            "Epoch 99/143.. Train loss: 83.643.. Val loss: 54.157.. Train L1 norm: 4.004.. Val L1 norm: 1.051.. Train Linf norm: 3063.622.. Val Linf norm: 42.616\n",
            "Epoch 100/143.. Train loss: 607.766.. Val loss: 54.159.. Train L1 norm: 3.527.. Val L1 norm: 1.051.. Train Linf norm: 2571.629.. Val Linf norm: 42.573\n",
            "Epoch 101/143.. Train loss: 76.024.. Val loss: 54.159.. Train L1 norm: 4.810.. Val L1 norm: 1.051.. Train Linf norm: 3890.057.. Val Linf norm: 42.569\n",
            "Epoch 102/143.. Train loss: 66.526.. Val loss: 54.159.. Train L1 norm: 4.093.. Val L1 norm: 1.051.. Train Linf norm: 3153.833.. Val Linf norm: 42.572\n",
            "Epoch 103/143.. Train loss: 68.548.. Val loss: 54.159.. Train L1 norm: 3.269.. Val L1 norm: 1.051.. Train Linf norm: 2312.752.. Val Linf norm: 42.574\n",
            "Epoch 104/143.. Train loss: 763.107.. Val loss: 54.161.. Train L1 norm: 1.969.. Val L1 norm: 1.051.. Train Linf norm: 981.280.. Val Linf norm: 42.530\n",
            "Epoch 105/143.. Train loss: 95.406.. Val loss: 54.161.. Train L1 norm: 2.553.. Val L1 norm: 1.051.. Train Linf norm: 1576.663.. Val Linf norm: 42.521\n",
            "Epoch 106/143.. Train loss: 129.788.. Val loss: 54.161.. Train L1 norm: 1.129.. Val L1 norm: 1.051.. Train Linf norm: 120.484.. Val Linf norm: 42.528\n",
            "Epoch 107/143.. Train loss: 60.134.. Val loss: 54.161.. Train L1 norm: 4.224.. Val L1 norm: 1.051.. Train Linf norm: 3290.993.. Val Linf norm: 42.535\n",
            "Epoch 108/143.. Train loss: 168.195.. Val loss: 54.161.. Train L1 norm: 3.517.. Val L1 norm: 1.051.. Train Linf norm: 2564.523.. Val Linf norm: 42.522\n",
            "Epoch 109/143.. Train loss: 95.427.. Val loss: 54.161.. Train L1 norm: 5.693.. Val L1 norm: 1.051.. Train Linf norm: 4794.248.. Val Linf norm: 42.525\n",
            "Epoch 110/143.. Train loss: 111.431.. Val loss: 54.161.. Train L1 norm: 1.093.. Val L1 norm: 1.051.. Train Linf norm: 83.016.. Val Linf norm: 42.522\n",
            "Epoch 111/143.. Train loss: 162.708.. Val loss: 54.162.. Train L1 norm: 3.810.. Val L1 norm: 1.051.. Train Linf norm: 2867.256.. Val Linf norm: 42.509\n",
            "Epoch 112/143.. Train loss: 116.832.. Val loss: 54.161.. Train L1 norm: 3.512.. Val L1 norm: 1.051.. Train Linf norm: 2558.840.. Val Linf norm: 42.515\n",
            "Epoch 113/143.. Train loss: 60.858.. Val loss: 54.161.. Train L1 norm: 2.025.. Val L1 norm: 1.051.. Train Linf norm: 1038.263.. Val Linf norm: 42.516\n",
            "Epoch 114/143.. Train loss: 91.004.. Val loss: 54.162.. Train L1 norm: 3.236.. Val L1 norm: 1.051.. Train Linf norm: 2268.644.. Val Linf norm: 42.510\n",
            "Epoch 115/143.. Train loss: 134.963.. Val loss: 54.162.. Train L1 norm: 5.271.. Val L1 norm: 1.051.. Train Linf norm: 4355.953.. Val Linf norm: 42.495\n",
            "Epoch 116/143.. Train loss: 396.486.. Val loss: 54.161.. Train L1 norm: 4.634.. Val L1 norm: 1.051.. Train Linf norm: 3709.911.. Val Linf norm: 42.532\n",
            "Epoch 117/143.. Train loss: 189.564.. Val loss: 54.161.. Train L1 norm: 2.754.. Val L1 norm: 1.051.. Train Linf norm: 1782.954.. Val Linf norm: 42.516\n",
            "Epoch 118/143.. Train loss: 76.222.. Val loss: 54.161.. Train L1 norm: 4.351.. Val L1 norm: 1.051.. Train Linf norm: 3421.399.. Val Linf norm: 42.521\n",
            "Epoch 119/143.. Train loss: 190.811.. Val loss: 54.162.. Train L1 norm: 3.642.. Val L1 norm: 1.051.. Train Linf norm: 2683.456.. Val Linf norm: 42.504\n",
            "Epoch 120/143.. Train loss: 546.941.. Val loss: 54.160.. Train L1 norm: 5.010.. Val L1 norm: 1.051.. Train Linf norm: 4091.637.. Val Linf norm: 42.543\n",
            "Epoch 121/143.. Train loss: 60.013.. Val loss: 54.160.. Train L1 norm: 4.697.. Val L1 norm: 1.051.. Train Linf norm: 3774.620.. Val Linf norm: 42.544\n",
            "Epoch 122/143.. Train loss: 108.342.. Val loss: 54.160.. Train L1 norm: 2.933.. Val L1 norm: 1.051.. Train Linf norm: 1967.092.. Val Linf norm: 42.551\n",
            "Epoch 123/143.. Train loss: 86.347.. Val loss: 54.160.. Train L1 norm: 1.649.. Val L1 norm: 1.051.. Train Linf norm: 654.175.. Val Linf norm: 42.550\n",
            "Epoch 124/143.. Train loss: 61.204.. Val loss: 54.160.. Train L1 norm: 3.999.. Val L1 norm: 1.051.. Train Linf norm: 3061.499.. Val Linf norm: 42.551\n",
            "Epoch 125/143.. Train loss: 64.658.. Val loss: 54.160.. Train L1 norm: 3.877.. Val L1 norm: 1.051.. Train Linf norm: 2935.548.. Val Linf norm: 42.550\n",
            "Epoch 126/143.. Train loss: 102.941.. Val loss: 54.161.. Train L1 norm: 3.647.. Val L1 norm: 1.051.. Train Linf norm: 2699.542.. Val Linf norm: 42.543\n",
            "Epoch 127/143.. Train loss: 138.826.. Val loss: 54.161.. Train L1 norm: 2.220.. Val L1 norm: 1.051.. Train Linf norm: 1238.837.. Val Linf norm: 42.531\n",
            "Epoch 128/143.. Train loss: 69.444.. Val loss: 54.161.. Train L1 norm: 2.237.. Val L1 norm: 1.051.. Train Linf norm: 1254.501.. Val Linf norm: 42.532\n",
            "Epoch 129/143.. Train loss: 248.225.. Val loss: 54.160.. Train L1 norm: 3.180.. Val L1 norm: 1.051.. Train Linf norm: 2222.163.. Val Linf norm: 42.560\n",
            "Epoch 130/143.. Train loss: 210.517.. Val loss: 54.161.. Train L1 norm: 3.660.. Val L1 norm: 1.051.. Train Linf norm: 2712.837.. Val Linf norm: 42.544\n",
            "Epoch 131/143.. Train loss: 182.440.. Val loss: 54.161.. Train L1 norm: 3.507.. Val L1 norm: 1.051.. Train Linf norm: 2557.441.. Val Linf norm: 42.526\n",
            "Epoch 132/143.. Train loss: 65.453.. Val loss: 54.161.. Train L1 norm: 4.021.. Val L1 norm: 1.051.. Train Linf norm: 3083.336.. Val Linf norm: 42.528\n",
            "Epoch 133/143.. Train loss: 61.172.. Val loss: 54.161.. Train L1 norm: 3.084.. Val L1 norm: 1.051.. Train Linf norm: 2121.411.. Val Linf norm: 42.528\n",
            "Epoch 134/143.. Train loss: 75.276.. Val loss: 54.161.. Train L1 norm: 3.794.. Val L1 norm: 1.051.. Train Linf norm: 2850.854.. Val Linf norm: 42.535\n",
            "Epoch 135/143.. Train loss: 108.798.. Val loss: 54.161.. Train L1 norm: 4.529.. Val L1 norm: 1.051.. Train Linf norm: 3603.660.. Val Linf norm: 42.531\n",
            "Epoch 136/143.. Train loss: 163.292.. Val loss: 54.163.. Train L1 norm: 2.365.. Val L1 norm: 1.051.. Train Linf norm: 1386.830.. Val Linf norm: 42.496\n",
            "Epoch 137/143.. Train loss: 94.358.. Val loss: 54.162.. Train L1 norm: 4.565.. Val L1 norm: 1.051.. Train Linf norm: 3639.602.. Val Linf norm: 42.506\n",
            "Epoch 138/143.. Train loss: 192.491.. Val loss: 54.163.. Train L1 norm: 2.511.. Val L1 norm: 1.051.. Train Linf norm: 1535.601.. Val Linf norm: 42.492\n",
            "Epoch 139/143.. Train loss: 128.997.. Val loss: 54.163.. Train L1 norm: 2.594.. Val L1 norm: 1.051.. Train Linf norm: 1622.843.. Val Linf norm: 42.502\n",
            "Epoch 140/143.. Train loss: 547.010.. Val loss: 54.164.. Train L1 norm: 2.113.. Val L1 norm: 1.051.. Train Linf norm: 1127.817.. Val Linf norm: 42.461\n",
            "Epoch 141/143.. Train loss: 63.110.. Val loss: 54.164.. Train L1 norm: 3.325.. Val L1 norm: 1.051.. Train Linf norm: 2368.443.. Val Linf norm: 42.463\n",
            "Epoch 142/143.. Train loss: 101.776.. Val loss: 54.164.. Train L1 norm: 4.063.. Val L1 norm: 1.051.. Train Linf norm: 3127.064.. Val Linf norm: 42.459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 08:57:39,932]\u001b[0m Trial 64 finished with value: 1.0508889041264853 and parameters: {'n_layers': 6, 'n_units_0': 3591, 'n_units_1': 169, 'n_units_2': 3833, 'n_units_3': 461, 'n_units_4': 299, 'n_units_5': 1306, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 1.0407197691924618e-06, 'batch_size': 1024, 'n_epochs': 143, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.20205870135037282, 'dropout_rate': 0.06566332598964499, 'weight_decay': 0.0009078290537371376, 'beta1': 0.9387565752480174, 'beta2': 0.9992190520293398, 'factor': 0.11666917737155699, 'patience': 6, 'threshold': 0.0021476599039349513}. Best is trial 22 with value: 1.0150012904485066.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 143/143.. Train loss: 127.931.. Val loss: 54.164.. Train L1 norm: 2.849.. Val L1 norm: 1.051.. Train Linf norm: 1883.892.. Val Linf norm: 42.469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 08:57:42,105]\u001b[0m Trial 65 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/143.. Train loss: 4.694.. Val loss: 4.324.. Train L1 norm: 6.118.. Val L1 norm: 1.246.. Train Linf norm: 5152.485.. Val Linf norm: 154.276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 08:57:44,038]\u001b[0m Trial 66 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/148.. Train loss: 11989.408.. Val loss: 61.197.. Train L1 norm: 5.573.. Val L1 norm: 1.165.. Train Linf norm: 4611.415.. Val Linf norm: 120.305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 08:57:53,208]\u001b[0m Trial 67 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/143.. Train loss: 3.139.. Val loss: 2.101.. Train L1 norm: 30.419.. Val L1 norm: 2.872.. Train Linf norm: 1856.698.. Val Linf norm: 94.631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 08:57:55,491]\u001b[0m Trial 68 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/53.. Train loss: 4707.815.. Val loss: 51.999.. Train L1 norm: 8.856.. Val L1 norm: 1.169.. Train Linf norm: 7910.358.. Val Linf norm: 112.727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 08:57:57,528]\u001b[0m Trial 69 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150.. Train loss: 8861.500.. Val loss: 52.312.. Train L1 norm: 2.044.. Val L1 norm: 1.128.. Train Linf norm: 979.422.. Val Linf norm: 85.209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 08:58:02,541]\u001b[0m Trial 70 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/65.. Train loss: 6000.061.. Val loss: 74.856.. Train L1 norm: 19.206.. Val L1 norm: 1.438.. Train Linf norm: 2303.446.. Val Linf norm: 44.942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 08:58:05,737]\u001b[0m Trial 71 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/146.. Train loss: 4750.608.. Val loss: 51.990.. Train L1 norm: 7.283.. Val L1 norm: 1.140.. Train Linf norm: 6380.689.. Val Linf norm: 93.049\n",
            "Epoch 1/142.. Train loss: 491.032.. Val loss: 55.853.. Train L1 norm: 1.892.. Val L1 norm: 1.032.. Train Linf norm: 895.468.. Val Linf norm: 26.513\n",
            "Epoch 2/142.. Train loss: 113.349.. Val loss: 54.464.. Train L1 norm: 1.452.. Val L1 norm: 1.026.. Train Linf norm: 456.065.. Val Linf norm: 22.796\n",
            "Epoch 3/142.. Train loss: 140.390.. Val loss: 56.110.. Train L1 norm: 1.307.. Val L1 norm: 1.041.. Train Linf norm: 307.817.. Val Linf norm: 31.441\n",
            "Epoch 4/142.. Train loss: 567.999.. Val loss: 52.875.. Train L1 norm: 4.526.. Val L1 norm: 1.103.. Train Linf norm: 3590.953.. Val Linf norm: 72.754\n",
            "Epoch 5/142.. Train loss: 1278.130.. Val loss: 56.358.. Train L1 norm: 2.399.. Val L1 norm: 1.046.. Train Linf norm: 1412.491.. Val Linf norm: 33.781\n",
            "Epoch 6/142.. Train loss: 279.813.. Val loss: 54.558.. Train L1 norm: 1.713.. Val L1 norm: 1.030.. Train Linf norm: 724.092.. Val Linf norm: 26.554\n",
            "Epoch 7/142.. Train loss: 332.435.. Val loss: 55.919.. Train L1 norm: 2.462.. Val L1 norm: 1.028.. Train Linf norm: 1494.278.. Val Linf norm: 23.211\n",
            "Epoch 8/142.. Train loss: 245.495.. Val loss: 54.824.. Train L1 norm: 1.807.. Val L1 norm: 1.024.. Train Linf norm: 823.065.. Val Linf norm: 22.157\n",
            "Epoch 9/142.. Train loss: 81.253.. Val loss: 55.444.. Train L1 norm: 1.525.. Val L1 norm: 1.016.. Train Linf norm: 535.863.. Val Linf norm: 15.470\n",
            "Epoch 10/142.. Train loss: 61.312.. Val loss: 55.342.. Train L1 norm: 1.788.. Val L1 norm: 1.016.. Train Linf norm: 803.978.. Val Linf norm: 14.630\n",
            "Epoch 11/142.. Train loss: 310.354.. Val loss: 54.342.. Train L1 norm: 1.546.. Val L1 norm: 1.049.. Train Linf norm: 557.174.. Val Linf norm: 40.405\n",
            "Epoch 12/142.. Train loss: 283.051.. Val loss: 54.440.. Train L1 norm: 3.177.. Val L1 norm: 1.045.. Train Linf norm: 2218.792.. Val Linf norm: 37.748\n",
            "Epoch 13/142.. Train loss: 155.461.. Val loss: 54.651.. Train L1 norm: 2.265.. Val L1 norm: 1.037.. Train Linf norm: 1288.028.. Val Linf norm: 32.285\n",
            "Epoch 14/142.. Train loss: 235.781.. Val loss: 54.905.. Train L1 norm: 2.780.. Val L1 norm: 1.028.. Train Linf norm: 1817.883.. Val Linf norm: 25.978\n",
            "Epoch 15/142.. Train loss: 60.509.. Val loss: 54.916.. Train L1 norm: 2.182.. Val L1 norm: 1.028.. Train Linf norm: 1207.522.. Val Linf norm: 26.014\n",
            "Epoch 16/142.. Train loss: 63.943.. Val loss: 54.947.. Train L1 norm: 1.142.. Val L1 norm: 1.027.. Train Linf norm: 141.283.. Val Linf norm: 25.505\n",
            "Epoch 17/142.. Train loss: 60.663.. Val loss: 54.934.. Train L1 norm: 2.041.. Val L1 norm: 1.028.. Train Linf norm: 1062.296.. Val Linf norm: 26.050\n",
            "Epoch 18/142.. Train loss: 72.346.. Val loss: 54.965.. Train L1 norm: 2.450.. Val L1 norm: 1.027.. Train Linf norm: 1480.758.. Val Linf norm: 25.531\n",
            "Epoch 19/142.. Train loss: 74.309.. Val loss: 54.985.. Train L1 norm: 2.234.. Val L1 norm: 1.027.. Train Linf norm: 1259.746.. Val Linf norm: 25.100\n",
            "Epoch 20/142.. Train loss: 68.311.. Val loss: 54.994.. Train L1 norm: 1.909.. Val L1 norm: 1.027.. Train Linf norm: 928.314.. Val Linf norm: 24.925\n",
            "Epoch 21/142.. Train loss: 60.383.. Val loss: 54.994.. Train L1 norm: 2.547.. Val L1 norm: 1.027.. Train Linf norm: 1582.020.. Val Linf norm: 24.961\n",
            "Epoch 22/142.. Train loss: 86.985.. Val loss: 54.983.. Train L1 norm: 2.126.. Val L1 norm: 1.027.. Train Linf norm: 1148.072.. Val Linf norm: 25.243\n",
            "Epoch 23/142.. Train loss: 73.454.. Val loss: 54.987.. Train L1 norm: 2.003.. Val L1 norm: 1.027.. Train Linf norm: 1023.899.. Val Linf norm: 25.186\n",
            "Epoch 24/142.. Train loss: 106.357.. Val loss: 54.982.. Train L1 norm: 1.617.. Val L1 norm: 1.027.. Train Linf norm: 628.191.. Val Linf norm: 25.347\n",
            "Epoch 25/142.. Train loss: 133.728.. Val loss: 54.920.. Train L1 norm: 1.948.. Val L1 norm: 1.029.. Train Linf norm: 967.101.. Val Linf norm: 26.796\n",
            "Epoch 26/142.. Train loss: 62.216.. Val loss: 54.920.. Train L1 norm: 2.131.. Val L1 norm: 1.029.. Train Linf norm: 1153.137.. Val Linf norm: 26.810\n",
            "Epoch 27/142.. Train loss: 93.878.. Val loss: 54.922.. Train L1 norm: 1.519.. Val L1 norm: 1.029.. Train Linf norm: 527.813.. Val Linf norm: 26.760\n",
            "Epoch 28/142.. Train loss: 60.085.. Val loss: 54.923.. Train L1 norm: 2.246.. Val L1 norm: 1.029.. Train Linf norm: 1273.280.. Val Linf norm: 26.751\n",
            "Epoch 29/142.. Train loss: 77.645.. Val loss: 54.920.. Train L1 norm: 1.879.. Val L1 norm: 1.029.. Train Linf norm: 896.382.. Val Linf norm: 26.818\n",
            "Epoch 30/142.. Train loss: 78.416.. Val loss: 54.922.. Train L1 norm: 1.626.. Val L1 norm: 1.029.. Train Linf norm: 636.676.. Val Linf norm: 26.776\n",
            "Epoch 31/142.. Train loss: 60.203.. Val loss: 54.922.. Train L1 norm: 1.878.. Val L1 norm: 1.029.. Train Linf norm: 896.446.. Val Linf norm: 26.784\n",
            "Epoch 32/142.. Train loss: 60.125.. Val loss: 54.922.. Train L1 norm: 2.594.. Val L1 norm: 1.029.. Train Linf norm: 1629.059.. Val Linf norm: 26.792\n",
            "Epoch 33/142.. Train loss: 61.900.. Val loss: 54.923.. Train L1 norm: 2.395.. Val L1 norm: 1.029.. Train Linf norm: 1423.944.. Val Linf norm: 26.787\n",
            "Epoch 34/142.. Train loss: 72.123.. Val loss: 54.925.. Train L1 norm: 2.660.. Val L1 norm: 1.029.. Train Linf norm: 1696.985.. Val Linf norm: 26.746\n",
            "Epoch 35/142.. Train loss: 86.722.. Val loss: 54.929.. Train L1 norm: 2.050.. Val L1 norm: 1.029.. Train Linf norm: 1069.317.. Val Linf norm: 26.652\n",
            "Epoch 36/142.. Train loss: 63.615.. Val loss: 54.927.. Train L1 norm: 1.717.. Val L1 norm: 1.029.. Train Linf norm: 730.470.. Val Linf norm: 26.684\n",
            "Epoch 37/142.. Train loss: 60.428.. Val loss: 54.927.. Train L1 norm: 1.517.. Val L1 norm: 1.029.. Train Linf norm: 526.789.. Val Linf norm: 26.691\n",
            "Epoch 38/142.. Train loss: 73.336.. Val loss: 54.930.. Train L1 norm: 1.787.. Val L1 norm: 1.029.. Train Linf norm: 803.565.. Val Linf norm: 26.634\n",
            "Epoch 39/142.. Train loss: 60.352.. Val loss: 54.930.. Train L1 norm: 2.303.. Val L1 norm: 1.029.. Train Linf norm: 1332.357.. Val Linf norm: 26.640\n",
            "Epoch 40/142.. Train loss: 71.302.. Val loss: 54.930.. Train L1 norm: 2.101.. Val L1 norm: 1.029.. Train Linf norm: 1123.081.. Val Linf norm: 26.659\n",
            "Epoch 41/142.. Train loss: 73.222.. Val loss: 54.928.. Train L1 norm: 1.325.. Val L1 norm: 1.029.. Train Linf norm: 329.776.. Val Linf norm: 26.702\n",
            "Epoch 42/142.. Train loss: 62.636.. Val loss: 54.929.. Train L1 norm: 2.202.. Val L1 norm: 1.029.. Train Linf norm: 1226.859.. Val Linf norm: 26.675\n",
            "Epoch 43/142.. Train loss: 172.072.. Val loss: 54.937.. Train L1 norm: 2.213.. Val L1 norm: 1.029.. Train Linf norm: 1239.100.. Val Linf norm: 26.496\n",
            "Epoch 44/142.. Train loss: 235.933.. Val loss: 54.948.. Train L1 norm: 2.621.. Val L1 norm: 1.028.. Train Linf norm: 1657.637.. Val Linf norm: 26.248\n",
            "Epoch 45/142.. Train loss: 68.890.. Val loss: 54.946.. Train L1 norm: 2.381.. Val L1 norm: 1.028.. Train Linf norm: 1409.967.. Val Linf norm: 26.304\n",
            "Epoch 46/142.. Train loss: 64.002.. Val loss: 54.944.. Train L1 norm: 1.684.. Val L1 norm: 1.028.. Train Linf norm: 697.076.. Val Linf norm: 26.360\n",
            "Epoch 47/142.. Train loss: 66.567.. Val loss: 54.942.. Train L1 norm: 1.723.. Val L1 norm: 1.028.. Train Linf norm: 737.135.. Val Linf norm: 26.407\n",
            "Epoch 48/142.. Train loss: 83.322.. Val loss: 54.937.. Train L1 norm: 2.195.. Val L1 norm: 1.029.. Train Linf norm: 1220.383.. Val Linf norm: 26.529\n",
            "Epoch 49/142.. Train loss: 80.054.. Val loss: 54.934.. Train L1 norm: 1.720.. Val L1 norm: 1.029.. Train Linf norm: 733.456.. Val Linf norm: 26.604\n",
            "Epoch 50/142.. Train loss: 61.057.. Val loss: 54.931.. Train L1 norm: 2.232.. Val L1 norm: 1.029.. Train Linf norm: 1257.777.. Val Linf norm: 26.670\n",
            "Epoch 51/142.. Train loss: 63.954.. Val loss: 54.933.. Train L1 norm: 2.256.. Val L1 norm: 1.029.. Train Linf norm: 1281.741.. Val Linf norm: 26.639\n",
            "Epoch 52/142.. Train loss: 60.364.. Val loss: 54.932.. Train L1 norm: 2.258.. Val L1 norm: 1.029.. Train Linf norm: 1285.399.. Val Linf norm: 26.655\n",
            "Epoch 53/142.. Train loss: 61.296.. Val loss: 54.932.. Train L1 norm: 2.643.. Val L1 norm: 1.029.. Train Linf norm: 1679.066.. Val Linf norm: 26.674\n",
            "Epoch 54/142.. Train loss: 61.251.. Val loss: 54.930.. Train L1 norm: 2.178.. Val L1 norm: 1.029.. Train Linf norm: 1202.914.. Val Linf norm: 26.714\n",
            "Epoch 55/142.. Train loss: 88.543.. Val loss: 54.924.. Train L1 norm: 2.976.. Val L1 norm: 1.029.. Train Linf norm: 2020.568.. Val Linf norm: 26.870\n",
            "Epoch 56/142.. Train loss: 81.810.. Val loss: 54.919.. Train L1 norm: 2.085.. Val L1 norm: 1.029.. Train Linf norm: 1108.444.. Val Linf norm: 27.001\n",
            "Epoch 57/142.. Train loss: 63.090.. Val loss: 54.917.. Train L1 norm: 1.434.. Val L1 norm: 1.029.. Train Linf norm: 440.628.. Val Linf norm: 27.055\n",
            "Epoch 58/142.. Train loss: 60.501.. Val loss: 54.916.. Train L1 norm: 2.471.. Val L1 norm: 1.029.. Train Linf norm: 1502.908.. Val Linf norm: 27.062\n",
            "Epoch 59/142.. Train loss: 60.125.. Val loss: 54.917.. Train L1 norm: 2.702.. Val L1 norm: 1.029.. Train Linf norm: 1739.229.. Val Linf norm: 27.067\n",
            "Epoch 60/142.. Train loss: 80.846.. Val loss: 54.922.. Train L1 norm: 2.031.. Val L1 norm: 1.029.. Train Linf norm: 1053.061.. Val Linf norm: 26.952\n",
            "Epoch 61/142.. Train loss: 68.141.. Val loss: 54.919.. Train L1 norm: 1.920.. Val L1 norm: 1.029.. Train Linf norm: 938.715.. Val Linf norm: 27.027\n",
            "Epoch 62/142.. Train loss: 62.055.. Val loss: 54.917.. Train L1 norm: 2.778.. Val L1 norm: 1.029.. Train Linf norm: 1816.182.. Val Linf norm: 27.089\n",
            "Epoch 63/142.. Train loss: 60.111.. Val loss: 54.916.. Train L1 norm: 1.905.. Val L1 norm: 1.029.. Train Linf norm: 922.638.. Val Linf norm: 27.107\n",
            "Epoch 64/142.. Train loss: 83.150.. Val loss: 54.909.. Train L1 norm: 2.576.. Val L1 norm: 1.030.. Train Linf norm: 1606.425.. Val Linf norm: 27.269\n",
            "Epoch 65/142.. Train loss: 62.330.. Val loss: 54.910.. Train L1 norm: 1.982.. Val L1 norm: 1.030.. Train Linf norm: 1002.572.. Val Linf norm: 27.259\n",
            "Epoch 66/142.. Train loss: 61.517.. Val loss: 54.912.. Train L1 norm: 2.234.. Val L1 norm: 1.030.. Train Linf norm: 1259.766.. Val Linf norm: 27.219\n",
            "Epoch 67/142.. Train loss: 60.233.. Val loss: 54.912.. Train L1 norm: 2.284.. Val L1 norm: 1.030.. Train Linf norm: 1312.385.. Val Linf norm: 27.232\n",
            "Epoch 68/142.. Train loss: 60.180.. Val loss: 54.912.. Train L1 norm: 2.087.. Val L1 norm: 1.030.. Train Linf norm: 1109.785.. Val Linf norm: 27.239\n",
            "Epoch 69/142.. Train loss: 63.649.. Val loss: 54.909.. Train L1 norm: 1.412.. Val L1 norm: 1.030.. Train Linf norm: 418.540.. Val Linf norm: 27.308\n",
            "Epoch 70/142.. Train loss: 76.304.. Val loss: 54.914.. Train L1 norm: 2.260.. Val L1 norm: 1.030.. Train Linf norm: 1287.425.. Val Linf norm: 27.203\n",
            "Epoch 71/142.. Train loss: 64.960.. Val loss: 54.918.. Train L1 norm: 2.014.. Val L1 norm: 1.029.. Train Linf norm: 1035.153.. Val Linf norm: 27.118\n",
            "Epoch 72/142.. Train loss: 63.757.. Val loss: 54.916.. Train L1 norm: 2.088.. Val L1 norm: 1.030.. Train Linf norm: 1109.927.. Val Linf norm: 27.188\n",
            "Epoch 73/142.. Train loss: 61.324.. Val loss: 54.916.. Train L1 norm: 1.353.. Val L1 norm: 1.029.. Train Linf norm: 359.061.. Val Linf norm: 27.174\n",
            "Epoch 74/142.. Train loss: 112.358.. Val loss: 54.906.. Train L1 norm: 2.844.. Val L1 norm: 1.030.. Train Linf norm: 1884.128.. Val Linf norm: 27.401\n",
            "Epoch 75/142.. Train loss: 76.622.. Val loss: 54.901.. Train L1 norm: 1.924.. Val L1 norm: 1.030.. Train Linf norm: 943.278.. Val Linf norm: 27.545\n",
            "Epoch 76/142.. Train loss: 60.159.. Val loss: 54.900.. Train L1 norm: 2.237.. Val L1 norm: 1.030.. Train Linf norm: 1263.106.. Val Linf norm: 27.566\n",
            "Epoch 77/142.. Train loss: 60.250.. Val loss: 54.899.. Train L1 norm: 2.631.. Val L1 norm: 1.030.. Train Linf norm: 1666.532.. Val Linf norm: 27.585\n",
            "Epoch 78/142.. Train loss: 62.199.. Val loss: 54.901.. Train L1 norm: 2.053.. Val L1 norm: 1.030.. Train Linf norm: 1074.852.. Val Linf norm: 27.566\n",
            "Epoch 79/142.. Train loss: 74.005.. Val loss: 54.907.. Train L1 norm: 2.637.. Val L1 norm: 1.030.. Train Linf norm: 1668.477.. Val Linf norm: 27.419\n",
            "Epoch 80/142.. Train loss: 60.300.. Val loss: 54.907.. Train L1 norm: 2.493.. Val L1 norm: 1.030.. Train Linf norm: 1519.370.. Val Linf norm: 27.437\n",
            "Epoch 81/142.. Train loss: 73.593.. Val loss: 54.900.. Train L1 norm: 2.001.. Val L1 norm: 1.030.. Train Linf norm: 1011.329.. Val Linf norm: 27.605\n",
            "Epoch 82/142.. Train loss: 62.827.. Val loss: 54.896.. Train L1 norm: 1.980.. Val L1 norm: 1.030.. Train Linf norm: 1000.205.. Val Linf norm: 27.690\n",
            "Epoch 83/142.. Train loss: 61.497.. Val loss: 54.898.. Train L1 norm: 1.597.. Val L1 norm: 1.030.. Train Linf norm: 607.830.. Val Linf norm: 27.663\n",
            "Epoch 84/142.. Train loss: 95.613.. Val loss: 54.888.. Train L1 norm: 2.402.. Val L1 norm: 1.030.. Train Linf norm: 1431.979.. Val Linf norm: 27.906\n",
            "Epoch 85/142.. Train loss: 69.366.. Val loss: 54.893.. Train L1 norm: 3.143.. Val L1 norm: 1.030.. Train Linf norm: 2190.539.. Val Linf norm: 27.789\n",
            "Epoch 86/142.. Train loss: 62.669.. Val loss: 54.890.. Train L1 norm: 1.898.. Val L1 norm: 1.030.. Train Linf norm: 916.408.. Val Linf norm: 27.860\n",
            "Epoch 87/142.. Train loss: 60.261.. Val loss: 54.890.. Train L1 norm: 2.215.. Val L1 norm: 1.030.. Train Linf norm: 1240.366.. Val Linf norm: 27.871\n",
            "Epoch 88/142.. Train loss: 61.783.. Val loss: 54.892.. Train L1 norm: 1.979.. Val L1 norm: 1.030.. Train Linf norm: 1000.746.. Val Linf norm: 27.831\n",
            "Epoch 89/142.. Train loss: 63.079.. Val loss: 54.895.. Train L1 norm: 1.791.. Val L1 norm: 1.030.. Train Linf norm: 805.196.. Val Linf norm: 27.764\n",
            "Epoch 90/142.. Train loss: 60.053.. Val loss: 54.895.. Train L1 norm: 2.034.. Val L1 norm: 1.030.. Train Linf norm: 1054.930.. Val Linf norm: 27.775\n",
            "Epoch 91/142.. Train loss: 74.531.. Val loss: 54.897.. Train L1 norm: 1.830.. Val L1 norm: 1.030.. Train Linf norm: 846.581.. Val Linf norm: 27.730\n",
            "Epoch 92/142.. Train loss: 60.122.. Val loss: 54.902.. Train L1 norm: 2.584.. Val L1 norm: 1.030.. Train Linf norm: 1618.609.. Val Linf norm: 27.606\n",
            "Epoch 93/142.. Train loss: 99.470.. Val loss: 54.913.. Train L1 norm: 1.788.. Val L1 norm: 1.030.. Train Linf norm: 803.071.. Val Linf norm: 27.356\n",
            "Epoch 94/142.. Train loss: 187.120.. Val loss: 54.897.. Train L1 norm: 1.300.. Val L1 norm: 1.030.. Train Linf norm: 303.096.. Val Linf norm: 27.749\n",
            "Epoch 95/142.. Train loss: 69.043.. Val loss: 54.901.. Train L1 norm: 2.007.. Val L1 norm: 1.030.. Train Linf norm: 1027.353.. Val Linf norm: 27.661\n",
            "Epoch 96/142.. Train loss: 106.763.. Val loss: 54.898.. Train L1 norm: 2.237.. Val L1 norm: 1.030.. Train Linf norm: 1263.658.. Val Linf norm: 27.752\n",
            "Epoch 97/142.. Train loss: 93.677.. Val loss: 54.898.. Train L1 norm: 2.632.. Val L1 norm: 1.030.. Train Linf norm: 1668.364.. Val Linf norm: 27.736\n",
            "Epoch 98/142.. Train loss: 181.527.. Val loss: 54.912.. Train L1 norm: 1.769.. Val L1 norm: 1.030.. Train Linf norm: 782.997.. Val Linf norm: 27.421\n",
            "Epoch 99/142.. Train loss: 66.542.. Val loss: 54.915.. Train L1 norm: 2.873.. Val L1 norm: 1.030.. Train Linf norm: 1912.935.. Val Linf norm: 27.361\n",
            "Epoch 100/142.. Train loss: 68.757.. Val loss: 54.914.. Train L1 norm: 2.630.. Val L1 norm: 1.030.. Train Linf norm: 1665.862.. Val Linf norm: 27.393\n",
            "Epoch 101/142.. Train loss: 110.848.. Val loss: 54.919.. Train L1 norm: 2.771.. Val L1 norm: 1.030.. Train Linf norm: 1808.672.. Val Linf norm: 27.273\n",
            "Epoch 102/142.. Train loss: 106.209.. Val loss: 54.917.. Train L1 norm: 2.637.. Val L1 norm: 1.030.. Train Linf norm: 1672.431.. Val Linf norm: 27.324\n",
            "Epoch 103/142.. Train loss: 88.332.. Val loss: 54.905.. Train L1 norm: 2.340.. Val L1 norm: 1.030.. Train Linf norm: 1366.075.. Val Linf norm: 27.619\n",
            "Epoch 104/142.. Train loss: 94.142.. Val loss: 54.901.. Train L1 norm: 2.139.. Val L1 norm: 1.030.. Train Linf norm: 1162.425.. Val Linf norm: 27.719\n",
            "Epoch 105/142.. Train loss: 60.221.. Val loss: 54.897.. Train L1 norm: 2.494.. Val L1 norm: 1.030.. Train Linf norm: 1526.016.. Val Linf norm: 27.823\n",
            "Epoch 106/142.. Train loss: 76.877.. Val loss: 54.891.. Train L1 norm: 2.009.. Val L1 norm: 1.031.. Train Linf norm: 1029.684.. Val Linf norm: 27.959\n",
            "Epoch 107/142.. Train loss: 82.414.. Val loss: 54.885.. Train L1 norm: 2.368.. Val L1 norm: 1.031.. Train Linf norm: 1397.294.. Val Linf norm: 28.118\n",
            "Epoch 108/142.. Train loss: 85.230.. Val loss: 54.879.. Train L1 norm: 2.142.. Val L1 norm: 1.031.. Train Linf norm: 1166.408.. Val Linf norm: 28.289\n",
            "Epoch 109/142.. Train loss: 77.546.. Val loss: 54.883.. Train L1 norm: 2.309.. Val L1 norm: 1.031.. Train Linf norm: 1335.685.. Val Linf norm: 28.179\n",
            "Epoch 110/142.. Train loss: 68.459.. Val loss: 54.880.. Train L1 norm: 2.123.. Val L1 norm: 1.031.. Train Linf norm: 1146.160.. Val Linf norm: 28.271\n",
            "Epoch 111/142.. Train loss: 62.068.. Val loss: 54.881.. Train L1 norm: 1.871.. Val L1 norm: 1.031.. Train Linf norm: 887.863.. Val Linf norm: 28.248\n",
            "Epoch 112/142.. Train loss: 114.372.. Val loss: 54.873.. Train L1 norm: 2.461.. Val L1 norm: 1.031.. Train Linf norm: 1491.840.. Val Linf norm: 28.437\n",
            "Epoch 113/142.. Train loss: 60.032.. Val loss: 54.871.. Train L1 norm: 2.035.. Val L1 norm: 1.031.. Train Linf norm: 1055.015.. Val Linf norm: 28.492\n",
            "Epoch 114/142.. Train loss: 61.771.. Val loss: 54.870.. Train L1 norm: 1.998.. Val L1 norm: 1.031.. Train Linf norm: 1016.689.. Val Linf norm: 28.539\n",
            "Epoch 115/142.. Train loss: 80.134.. Val loss: 54.865.. Train L1 norm: 2.480.. Val L1 norm: 1.031.. Train Linf norm: 1511.500.. Val Linf norm: 28.655\n",
            "Epoch 116/142.. Train loss: 60.051.. Val loss: 54.864.. Train L1 norm: 1.990.. Val L1 norm: 1.032.. Train Linf norm: 1011.305.. Val Linf norm: 28.694\n",
            "Epoch 117/142.. Train loss: 113.936.. Val loss: 54.861.. Train L1 norm: 2.011.. Val L1 norm: 1.032.. Train Linf norm: 1031.888.. Val Linf norm: 28.756\n",
            "Epoch 118/142.. Train loss: 78.160.. Val loss: 54.858.. Train L1 norm: 1.702.. Val L1 norm: 1.032.. Train Linf norm: 715.427.. Val Linf norm: 28.850\n",
            "Epoch 119/142.. Train loss: 75.328.. Val loss: 54.857.. Train L1 norm: 1.900.. Val L1 norm: 1.032.. Train Linf norm: 913.112.. Val Linf norm: 28.867\n",
            "Epoch 120/142.. Train loss: 66.890.. Val loss: 54.857.. Train L1 norm: 1.795.. Val L1 norm: 1.032.. Train Linf norm: 809.428.. Val Linf norm: 28.892\n",
            "Epoch 121/142.. Train loss: 82.782.. Val loss: 54.863.. Train L1 norm: 1.984.. Val L1 norm: 1.032.. Train Linf norm: 1003.522.. Val Linf norm: 28.751\n",
            "Epoch 122/142.. Train loss: 148.748.. Val loss: 54.852.. Train L1 norm: 1.777.. Val L1 norm: 1.032.. Train Linf norm: 788.679.. Val Linf norm: 29.014\n",
            "Epoch 123/142.. Train loss: 166.943.. Val loss: 54.838.. Train L1 norm: 2.956.. Val L1 norm: 1.032.. Train Linf norm: 1997.103.. Val Linf norm: 29.357\n",
            "Epoch 124/142.. Train loss: 73.090.. Val loss: 54.839.. Train L1 norm: 2.767.. Val L1 norm: 1.032.. Train Linf norm: 1803.516.. Val Linf norm: 29.351\n",
            "Epoch 125/142.. Train loss: 73.635.. Val loss: 54.847.. Train L1 norm: 2.874.. Val L1 norm: 1.032.. Train Linf norm: 1912.114.. Val Linf norm: 29.165\n",
            "Epoch 126/142.. Train loss: 68.232.. Val loss: 54.850.. Train L1 norm: 2.132.. Val L1 norm: 1.032.. Train Linf norm: 1154.809.. Val Linf norm: 29.100\n",
            "Epoch 127/142.. Train loss: 76.238.. Val loss: 54.845.. Train L1 norm: 2.018.. Val L1 norm: 1.032.. Train Linf norm: 1038.833.. Val Linf norm: 29.214\n",
            "Epoch 128/142.. Train loss: 82.494.. Val loss: 54.839.. Train L1 norm: 2.032.. Val L1 norm: 1.032.. Train Linf norm: 1052.418.. Val Linf norm: 29.372\n",
            "Epoch 129/142.. Train loss: 70.703.. Val loss: 54.842.. Train L1 norm: 1.877.. Val L1 norm: 1.032.. Train Linf norm: 894.924.. Val Linf norm: 29.291\n",
            "Epoch 130/142.. Train loss: 76.972.. Val loss: 54.838.. Train L1 norm: 2.881.. Val L1 norm: 1.033.. Train Linf norm: 1922.511.. Val Linf norm: 29.400\n",
            "Epoch 131/142.. Train loss: 61.442.. Val loss: 54.835.. Train L1 norm: 1.095.. Val L1 norm: 1.033.. Train Linf norm: 93.265.. Val Linf norm: 29.475\n",
            "Epoch 132/142.. Train loss: 60.118.. Val loss: 54.835.. Train L1 norm: 2.795.. Val L1 norm: 1.033.. Train Linf norm: 1834.572.. Val Linf norm: 29.491\n",
            "Epoch 133/142.. Train loss: 76.724.. Val loss: 54.829.. Train L1 norm: 2.902.. Val L1 norm: 1.033.. Train Linf norm: 1944.370.. Val Linf norm: 29.641\n",
            "Epoch 134/142.. Train loss: 96.910.. Val loss: 54.821.. Train L1 norm: 2.099.. Val L1 norm: 1.033.. Train Linf norm: 1121.983.. Val Linf norm: 29.845\n",
            "Epoch 135/142.. Train loss: 95.169.. Val loss: 54.828.. Train L1 norm: 2.582.. Val L1 norm: 1.033.. Train Linf norm: 1615.978.. Val Linf norm: 29.668\n",
            "Epoch 136/142.. Train loss: 65.076.. Val loss: 54.825.. Train L1 norm: 1.608.. Val L1 norm: 1.033.. Train Linf norm: 618.926.. Val Linf norm: 29.751\n",
            "Epoch 137/142.. Train loss: 79.914.. Val loss: 54.824.. Train L1 norm: 2.437.. Val L1 norm: 1.033.. Train Linf norm: 1466.170.. Val Linf norm: 29.789\n",
            "Epoch 138/142.. Train loss: 65.928.. Val loss: 54.808.. Train L1 norm: 2.225.. Val L1 norm: 1.034.. Train Linf norm: 1250.552.. Val Linf norm: 30.194\n",
            "Epoch 139/142.. Train loss: 62.018.. Val loss: 54.809.. Train L1 norm: 1.497.. Val L1 norm: 1.034.. Train Linf norm: 502.824.. Val Linf norm: 30.172\n",
            "Epoch 140/142.. Train loss: 83.034.. Val loss: 54.804.. Train L1 norm: 2.529.. Val L1 norm: 1.034.. Train Linf norm: 1560.772.. Val Linf norm: 30.314\n",
            "Epoch 141/142.. Train loss: 67.486.. Val loss: 54.806.. Train L1 norm: 2.335.. Val L1 norm: 1.034.. Train Linf norm: 1360.671.. Val Linf norm: 30.261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:02:08,365]\u001b[0m Trial 72 finished with value: 1.0335612177213034 and parameters: {'n_layers': 6, 'n_units_0': 3316, 'n_units_1': 501, 'n_units_2': 2967, 'n_units_3': 410, 'n_units_4': 810, 'n_units_5': 1374, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 2.9983846898843848e-06, 'batch_size': 1024, 'n_epochs': 142, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.21176869298129422, 'dropout_rate': 0.018478154113782765, 'weight_decay': 0.0005102452985009764, 'beta1': 0.923074657510643, 'beta2': 0.9990837029809938, 'factor': 0.14277257901667464, 'patience': 6, 'threshold': 0.002207316328010768}. Best is trial 22 with value: 1.0150012904485066.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 142/142.. Train loss: 80.732.. Val loss: 54.811.. Train L1 norm: 2.235.. Val L1 norm: 1.034.. Train Linf norm: 1259.158.. Val Linf norm: 30.143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:02:10,216]\u001b[0m Trial 73 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/88.. Train loss: 401.766.. Val loss: 48.721.. Train L1 norm: 7.323.. Val L1 norm: 1.455.. Train Linf norm: 6336.342.. Val Linf norm: 251.456\n",
            "Epoch 1/142.. Train loss: 69.028.. Val loss: 54.653.. Train L1 norm: 1.606.. Val L1 norm: 1.016.. Train Linf norm: 615.689.. Val Linf norm: 15.428\n",
            "Epoch 2/142.. Train loss: 221.727.. Val loss: 55.507.. Train L1 norm: 2.024.. Val L1 norm: 1.022.. Train Linf norm: 1045.323.. Val Linf norm: 18.926\n",
            "Epoch 3/142.. Train loss: 289.818.. Val loss: 54.394.. Train L1 norm: 1.251.. Val L1 norm: 1.025.. Train Linf norm: 252.101.. Val Linf norm: 22.444\n",
            "Epoch 4/142.. Train loss: 68.448.. Val loss: 54.658.. Train L1 norm: 1.639.. Val L1 norm: 1.017.. Train Linf norm: 650.800.. Val Linf norm: 16.067\n",
            "Epoch 5/142.. Train loss: 169.936.. Val loss: 54.405.. Train L1 norm: 1.877.. Val L1 norm: 1.026.. Train Linf norm: 895.975.. Val Linf norm: 23.235\n",
            "Epoch 6/142.. Train loss: 368.637.. Val loss: 54.252.. Train L1 norm: 2.554.. Val L1 norm: 1.033.. Train Linf norm: 1583.426.. Val Linf norm: 28.338\n",
            "Epoch 7/142.. Train loss: 101.368.. Val loss: 54.377.. Train L1 norm: 1.062.. Val L1 norm: 1.029.. Train Linf norm: 60.208.. Val Linf norm: 25.431\n",
            "Epoch 8/142.. Train loss: 131.141.. Val loss: 55.034.. Train L1 norm: 2.311.. Val L1 norm: 1.014.. Train Linf norm: 1340.213.. Val Linf norm: 13.968\n",
            "Epoch 9/142.. Train loss: 143.069.. Val loss: 54.698.. Train L1 norm: 1.901.. Val L1 norm: 1.020.. Train Linf norm: 918.609.. Val Linf norm: 18.637\n",
            "Epoch 10/142.. Train loss: 176.275.. Val loss: 54.944.. Train L1 norm: 2.848.. Val L1 norm: 1.013.. Train Linf norm: 1887.714.. Val Linf norm: 13.121\n",
            "Epoch 11/142.. Train loss: 139.625.. Val loss: 54.762.. Train L1 norm: 1.696.. Val L1 norm: 1.020.. Train Linf norm: 709.476.. Val Linf norm: 18.182\n",
            "Epoch 12/142.. Train loss: 68.278.. Val loss: 54.603.. Train L1 norm: 2.297.. Val L1 norm: 1.026.. Train Linf norm: 1320.485.. Val Linf norm: 22.880\n",
            "Epoch 13/142.. Train loss: 73.314.. Val loss: 54.240.. Train L1 norm: 2.477.. Val L1 norm: 1.039.. Train Linf norm: 1508.742.. Val Linf norm: 32.747\n",
            "Epoch 14/142.. Train loss: 77.799.. Val loss: 54.624.. Train L1 norm: 3.374.. Val L1 norm: 1.026.. Train Linf norm: 2426.395.. Val Linf norm: 22.966\n",
            "Epoch 15/142.. Train loss: 280.984.. Val loss: 54.569.. Train L1 norm: 2.402.. Val L1 norm: 1.028.. Train Linf norm: 1433.403.. Val Linf norm: 24.387\n",
            "Epoch 16/142.. Train loss: 133.855.. Val loss: 54.349.. Train L1 norm: 2.072.. Val L1 norm: 1.035.. Train Linf norm: 1093.179.. Val Linf norm: 30.162\n",
            "Epoch 17/142.. Train loss: 519.048.. Val loss: 54.266.. Train L1 norm: 3.561.. Val L1 norm: 1.039.. Train Linf norm: 2617.302.. Val Linf norm: 32.606\n",
            "Epoch 18/142.. Train loss: 74.480.. Val loss: 54.091.. Train L1 norm: 1.732.. Val L1 norm: 1.046.. Train Linf norm: 742.819.. Val Linf norm: 37.641\n",
            "Epoch 19/142.. Train loss: 75.925.. Val loss: 54.042.. Train L1 norm: 2.294.. Val L1 norm: 1.048.. Train Linf norm: 1317.450.. Val Linf norm: 39.147\n",
            "Epoch 20/142.. Train loss: 148.831.. Val loss: 54.127.. Train L1 norm: 2.887.. Val L1 norm: 1.044.. Train Linf norm: 1924.133.. Val Linf norm: 36.773\n",
            "Epoch 21/142.. Train loss: 171.577.. Val loss: 54.039.. Train L1 norm: 2.550.. Val L1 norm: 1.048.. Train Linf norm: 1580.577.. Val Linf norm: 39.299\n",
            "Epoch 22/142.. Train loss: 60.522.. Val loss: 54.005.. Train L1 norm: 2.220.. Val L1 norm: 1.050.. Train Linf norm: 1241.582.. Val Linf norm: 40.337\n",
            "Epoch 23/142.. Train loss: 72.139.. Val loss: 54.031.. Train L1 norm: 3.335.. Val L1 norm: 1.049.. Train Linf norm: 2381.286.. Val Linf norm: 39.749\n",
            "Epoch 24/142.. Train loss: 189.638.. Val loss: 54.127.. Train L1 norm: 2.160.. Val L1 norm: 1.045.. Train Linf norm: 1179.980.. Val Linf norm: 37.015\n",
            "Epoch 25/142.. Train loss: 211.583.. Val loss: 54.120.. Train L1 norm: 2.878.. Val L1 norm: 1.045.. Train Linf norm: 1912.577.. Val Linf norm: 37.263\n",
            "Epoch 26/142.. Train loss: 468.059.. Val loss: 53.959.. Train L1 norm: 1.126.. Val L1 norm: 1.052.. Train Linf norm: 119.840.. Val Linf norm: 41.703\n",
            "Epoch 27/142.. Train loss: 68.966.. Val loss: 54.058.. Train L1 norm: 2.715.. Val L1 norm: 1.048.. Train Linf norm: 1748.674.. Val Linf norm: 38.945\n",
            "Epoch 28/142.. Train loss: 67.372.. Val loss: 54.080.. Train L1 norm: 2.661.. Val L1 norm: 1.047.. Train Linf norm: 1683.283.. Val Linf norm: 38.420\n",
            "Epoch 29/142.. Train loss: 205.010.. Val loss: 54.183.. Train L1 norm: 2.564.. Val L1 norm: 1.042.. Train Linf norm: 1595.632.. Val Linf norm: 35.490\n",
            "Epoch 30/142.. Train loss: 59.929.. Val loss: 54.187.. Train L1 norm: 3.087.. Val L1 norm: 1.042.. Train Linf norm: 2130.706.. Val Linf norm: 35.442\n",
            "Epoch 31/142.. Train loss: 247.741.. Val loss: 54.312.. Train L1 norm: 1.708.. Val L1 norm: 1.038.. Train Linf norm: 713.689.. Val Linf norm: 32.132\n",
            "Epoch 32/142.. Train loss: 63.149.. Val loss: 54.304.. Train L1 norm: 2.988.. Val L1 norm: 1.038.. Train Linf norm: 2027.706.. Val Linf norm: 32.453\n",
            "Epoch 33/142.. Train loss: 94.629.. Val loss: 54.365.. Train L1 norm: 2.908.. Val L1 norm: 1.036.. Train Linf norm: 1948.206.. Val Linf norm: 30.757\n",
            "Epoch 34/142.. Train loss: 69.970.. Val loss: 54.404.. Train L1 norm: 3.279.. Val L1 norm: 1.035.. Train Linf norm: 2328.652.. Val Linf norm: 29.697\n",
            "Epoch 35/142.. Train loss: 105.326.. Val loss: 54.397.. Train L1 norm: 2.851.. Val L1 norm: 1.035.. Train Linf norm: 1889.260.. Val Linf norm: 29.904\n",
            "Epoch 36/142.. Train loss: 101.373.. Val loss: 54.399.. Train L1 norm: 2.699.. Val L1 norm: 1.035.. Train Linf norm: 1731.977.. Val Linf norm: 29.853\n",
            "Epoch 37/142.. Train loss: 100.610.. Val loss: 54.410.. Train L1 norm: 1.694.. Val L1 norm: 1.034.. Train Linf norm: 706.242.. Val Linf norm: 29.534\n",
            "Epoch 38/142.. Train loss: 323.216.. Val loss: 54.389.. Train L1 norm: 3.501.. Val L1 norm: 1.035.. Train Linf norm: 2556.261.. Val Linf norm: 30.133\n",
            "Epoch 39/142.. Train loss: 67.827.. Val loss: 54.385.. Train L1 norm: 1.900.. Val L1 norm: 1.035.. Train Linf norm: 917.958.. Val Linf norm: 30.259\n",
            "Epoch 40/142.. Train loss: 78.865.. Val loss: 54.379.. Train L1 norm: 1.963.. Val L1 norm: 1.036.. Train Linf norm: 981.370.. Val Linf norm: 30.443\n",
            "Epoch 41/142.. Train loss: 60.343.. Val loss: 54.378.. Train L1 norm: 1.345.. Val L1 norm: 1.036.. Train Linf norm: 347.949.. Val Linf norm: 30.459\n",
            "Epoch 42/142.. Train loss: 76.594.. Val loss: 54.383.. Train L1 norm: 2.740.. Val L1 norm: 1.035.. Train Linf norm: 1775.213.. Val Linf norm: 30.346\n",
            "Epoch 43/142.. Train loss: 63.591.. Val loss: 54.382.. Train L1 norm: 2.875.. Val L1 norm: 1.035.. Train Linf norm: 1916.391.. Val Linf norm: 30.351\n",
            "Epoch 44/142.. Train loss: 160.110.. Val loss: 54.381.. Train L1 norm: 3.033.. Val L1 norm: 1.036.. Train Linf norm: 2074.516.. Val Linf norm: 30.393\n",
            "Epoch 45/142.. Train loss: 128.993.. Val loss: 54.379.. Train L1 norm: 2.543.. Val L1 norm: 1.036.. Train Linf norm: 1575.261.. Val Linf norm: 30.439\n",
            "Epoch 46/142.. Train loss: 63.508.. Val loss: 54.380.. Train L1 norm: 2.431.. Val L1 norm: 1.036.. Train Linf norm: 1458.805.. Val Linf norm: 30.440\n",
            "Epoch 47/142.. Train loss: 61.886.. Val loss: 54.380.. Train L1 norm: 1.549.. Val L1 norm: 1.036.. Train Linf norm: 557.900.. Val Linf norm: 30.437\n",
            "Epoch 48/142.. Train loss: 81.124.. Val loss: 54.379.. Train L1 norm: 2.050.. Val L1 norm: 1.036.. Train Linf norm: 1070.868.. Val Linf norm: 30.456\n",
            "Epoch 49/142.. Train loss: 75.530.. Val loss: 54.380.. Train L1 norm: 2.101.. Val L1 norm: 1.036.. Train Linf norm: 1123.823.. Val Linf norm: 30.444\n",
            "Epoch 50/142.. Train loss: 84.307.. Val loss: 54.379.. Train L1 norm: 2.027.. Val L1 norm: 1.036.. Train Linf norm: 1046.072.. Val Linf norm: 30.451\n",
            "Epoch 51/142.. Train loss: 223.510.. Val loss: 54.377.. Train L1 norm: 2.422.. Val L1 norm: 1.036.. Train Linf norm: 1450.993.. Val Linf norm: 30.526\n",
            "Epoch 52/142.. Train loss: 74.146.. Val loss: 54.377.. Train L1 norm: 1.696.. Val L1 norm: 1.036.. Train Linf norm: 706.417.. Val Linf norm: 30.530\n",
            "Epoch 53/142.. Train loss: 109.848.. Val loss: 54.378.. Train L1 norm: 2.177.. Val L1 norm: 1.036.. Train Linf norm: 1195.887.. Val Linf norm: 30.497\n",
            "Epoch 54/142.. Train loss: 270.634.. Val loss: 54.377.. Train L1 norm: 2.566.. Val L1 norm: 1.036.. Train Linf norm: 1597.927.. Val Linf norm: 30.520\n",
            "Epoch 55/142.. Train loss: 85.636.. Val loss: 54.374.. Train L1 norm: 2.929.. Val L1 norm: 1.036.. Train Linf norm: 1971.369.. Val Linf norm: 30.601\n",
            "Epoch 56/142.. Train loss: 211.107.. Val loss: 54.372.. Train L1 norm: 2.331.. Val L1 norm: 1.036.. Train Linf norm: 1353.010.. Val Linf norm: 30.660\n",
            "Epoch 57/142.. Train loss: 178.329.. Val loss: 54.370.. Train L1 norm: 2.188.. Val L1 norm: 1.036.. Train Linf norm: 1211.866.. Val Linf norm: 30.717\n",
            "Epoch 58/142.. Train loss: 373.511.. Val loss: 54.367.. Train L1 norm: 2.534.. Val L1 norm: 1.036.. Train Linf norm: 1564.480.. Val Linf norm: 30.799\n",
            "Epoch 59/142.. Train loss: 201.333.. Val loss: 54.365.. Train L1 norm: 3.066.. Val L1 norm: 1.036.. Train Linf norm: 2110.318.. Val Linf norm: 30.858\n",
            "Epoch 60/142.. Train loss: 59.981.. Val loss: 54.365.. Train L1 norm: 2.720.. Val L1 norm: 1.036.. Train Linf norm: 1757.345.. Val Linf norm: 30.861\n",
            "Epoch 61/142.. Train loss: 63.797.. Val loss: 54.365.. Train L1 norm: 2.309.. Val L1 norm: 1.036.. Train Linf norm: 1334.656.. Val Linf norm: 30.868\n",
            "Epoch 62/142.. Train loss: 154.945.. Val loss: 54.363.. Train L1 norm: 2.653.. Val L1 norm: 1.036.. Train Linf norm: 1688.755.. Val Linf norm: 30.918\n",
            "Epoch 63/142.. Train loss: 61.414.. Val loss: 54.363.. Train L1 norm: 2.801.. Val L1 norm: 1.036.. Train Linf norm: 1838.217.. Val Linf norm: 30.918\n",
            "Epoch 64/142.. Train loss: 119.657.. Val loss: 54.362.. Train L1 norm: 1.516.. Val L1 norm: 1.036.. Train Linf norm: 522.154.. Val Linf norm: 30.955\n",
            "Epoch 65/142.. Train loss: 60.285.. Val loss: 54.362.. Train L1 norm: 3.427.. Val L1 norm: 1.036.. Train Linf norm: 2478.551.. Val Linf norm: 30.956\n",
            "Epoch 66/142.. Train loss: 64.267.. Val loss: 54.362.. Train L1 norm: 2.526.. Val L1 norm: 1.036.. Train Linf norm: 577.947.. Val Linf norm: 30.964\n",
            "Epoch 67/142.. Train loss: 66.072.. Val loss: 54.362.. Train L1 norm: 3.285.. Val L1 norm: 1.036.. Train Linf norm: 2332.959.. Val Linf norm: 30.958\n",
            "Epoch 68/142.. Train loss: 69.531.. Val loss: 54.362.. Train L1 norm: 2.557.. Val L1 norm: 1.036.. Train Linf norm: 1589.175.. Val Linf norm: 30.972\n",
            "Epoch 69/142.. Train loss: 79.600.. Val loss: 54.361.. Train L1 norm: 2.821.. Val L1 norm: 1.036.. Train Linf norm: 1860.837.. Val Linf norm: 30.993\n",
            "Epoch 70/142.. Train loss: 85.871.. Val loss: 54.362.. Train L1 norm: 2.085.. Val L1 norm: 1.036.. Train Linf norm: 1106.574.. Val Linf norm: 30.970\n",
            "Epoch 71/142.. Train loss: 76.118.. Val loss: 54.363.. Train L1 norm: 1.702.. Val L1 norm: 1.036.. Train Linf norm: 700.843.. Val Linf norm: 30.954\n",
            "Epoch 72/142.. Train loss: 63.493.. Val loss: 54.362.. Train L1 norm: 2.873.. Val L1 norm: 1.036.. Train Linf norm: 1913.180.. Val Linf norm: 30.959\n",
            "Epoch 73/142.. Train loss: 85.520.. Val loss: 54.361.. Train L1 norm: 2.981.. Val L1 norm: 1.036.. Train Linf norm: 2022.981.. Val Linf norm: 30.989\n",
            "Epoch 74/142.. Train loss: 263.096.. Val loss: 54.360.. Train L1 norm: 2.041.. Val L1 norm: 1.036.. Train Linf norm: 1057.976.. Val Linf norm: 31.037\n",
            "Epoch 75/142.. Train loss: 97.146.. Val loss: 54.359.. Train L1 norm: 3.215.. Val L1 norm: 1.036.. Train Linf norm: 2262.721.. Val Linf norm: 31.050\n",
            "Epoch 76/142.. Train loss: 61.408.. Val loss: 54.359.. Train L1 norm: 2.533.. Val L1 norm: 1.036.. Train Linf norm: 1564.582.. Val Linf norm: 31.050\n",
            "Epoch 77/142.. Train loss: 88.183.. Val loss: 54.360.. Train L1 norm: 2.086.. Val L1 norm: 1.036.. Train Linf norm: 1107.999.. Val Linf norm: 31.037\n",
            "Epoch 78/142.. Train loss: 80.259.. Val loss: 54.359.. Train L1 norm: 2.270.. Val L1 norm: 1.036.. Train Linf norm: 1295.750.. Val Linf norm: 31.047\n",
            "Epoch 79/142.. Train loss: 60.237.. Val loss: 54.359.. Train L1 norm: 1.773.. Val L1 norm: 1.036.. Train Linf norm: 787.381.. Val Linf norm: 31.049\n",
            "Epoch 80/142.. Train loss: 74.727.. Val loss: 54.359.. Train L1 norm: 2.913.. Val L1 norm: 1.036.. Train Linf norm: 1951.617.. Val Linf norm: 31.072\n",
            "Epoch 81/142.. Train loss: 61.757.. Val loss: 54.358.. Train L1 norm: 1.322.. Val L1 norm: 1.036.. Train Linf norm: 323.441.. Val Linf norm: 31.078\n",
            "Epoch 82/142.. Train loss: 64.847.. Val loss: 54.358.. Train L1 norm: 3.640.. Val L1 norm: 1.036.. Train Linf norm: 2700.584.. Val Linf norm: 31.089\n",
            "Epoch 83/142.. Train loss: 183.518.. Val loss: 54.355.. Train L1 norm: 2.587.. Val L1 norm: 1.037.. Train Linf norm: 1619.197.. Val Linf norm: 31.175\n",
            "Epoch 84/142.. Train loss: 64.243.. Val loss: 54.355.. Train L1 norm: 1.627.. Val L1 norm: 1.037.. Train Linf norm: 639.106.. Val Linf norm: 31.185\n",
            "Epoch 85/142.. Train loss: 62.317.. Val loss: 54.355.. Train L1 norm: 1.379.. Val L1 norm: 1.037.. Train Linf norm: 375.991.. Val Linf norm: 31.193\n",
            "Epoch 86/142.. Train loss: 116.687.. Val loss: 54.356.. Train L1 norm: 2.341.. Val L1 norm: 1.036.. Train Linf norm: 1364.698.. Val Linf norm: 31.148\n",
            "Epoch 87/142.. Train loss: 143.233.. Val loss: 54.358.. Train L1 norm: 1.667.. Val L1 norm: 1.036.. Train Linf norm: 677.505.. Val Linf norm: 31.098\n",
            "Epoch 88/142.. Train loss: 70.394.. Val loss: 54.359.. Train L1 norm: 1.756.. Val L1 norm: 1.036.. Train Linf norm: 768.919.. Val Linf norm: 31.081\n",
            "Epoch 89/142.. Train loss: 64.503.. Val loss: 54.359.. Train L1 norm: 1.448.. Val L1 norm: 1.036.. Train Linf norm: 453.474.. Val Linf norm: 31.093\n",
            "Epoch 90/142.. Train loss: 80.144.. Val loss: 54.358.. Train L1 norm: 3.841.. Val L1 norm: 1.036.. Train Linf norm: 2903.437.. Val Linf norm: 31.119\n",
            "Epoch 91/142.. Train loss: 73.959.. Val loss: 54.357.. Train L1 norm: 1.555.. Val L1 norm: 1.036.. Train Linf norm: 561.887.. Val Linf norm: 31.142\n",
            "Epoch 92/142.. Train loss: 74.258.. Val loss: 54.356.. Train L1 norm: 2.825.. Val L1 norm: 1.037.. Train Linf norm: 1863.674.. Val Linf norm: 31.166\n",
            "Epoch 93/142.. Train loss: 129.894.. Val loss: 54.354.. Train L1 norm: 2.371.. Val L1 norm: 1.037.. Train Linf norm: 1397.833.. Val Linf norm: 31.218\n",
            "Epoch 94/142.. Train loss: 67.080.. Val loss: 54.355.. Train L1 norm: 1.223.. Val L1 norm: 1.037.. Train Linf norm: 224.197.. Val Linf norm: 31.210\n",
            "Epoch 95/142.. Train loss: 75.689.. Val loss: 54.354.. Train L1 norm: 2.170.. Val L1 norm: 1.037.. Train Linf norm: 1192.774.. Val Linf norm: 31.234\n",
            "Epoch 96/142.. Train loss: 126.676.. Val loss: 54.356.. Train L1 norm: 2.274.. Val L1 norm: 1.037.. Train Linf norm: 1298.840.. Val Linf norm: 31.188\n",
            "Epoch 97/142.. Train loss: 62.452.. Val loss: 54.355.. Train L1 norm: 1.789.. Val L1 norm: 1.037.. Train Linf norm: 802.329.. Val Linf norm: 31.196\n",
            "Epoch 98/142.. Train loss: 79.206.. Val loss: 54.354.. Train L1 norm: 2.933.. Val L1 norm: 1.037.. Train Linf norm: 1973.977.. Val Linf norm: 31.227\n",
            "Epoch 99/142.. Train loss: 70.427.. Val loss: 54.354.. Train L1 norm: 2.498.. Val L1 norm: 1.037.. Train Linf norm: 1527.772.. Val Linf norm: 31.245\n",
            "Epoch 100/142.. Train loss: 82.782.. Val loss: 54.352.. Train L1 norm: 1.218.. Val L1 norm: 1.037.. Train Linf norm: 216.934.. Val Linf norm: 31.287\n",
            "Epoch 101/142.. Train loss: 465.192.. Val loss: 54.350.. Train L1 norm: 3.117.. Val L1 norm: 1.037.. Train Linf norm: 2159.519.. Val Linf norm: 31.338\n",
            "Epoch 102/142.. Train loss: 65.577.. Val loss: 54.348.. Train L1 norm: 3.030.. Val L1 norm: 1.037.. Train Linf norm: 2072.943.. Val Linf norm: 31.392\n",
            "Epoch 103/142.. Train loss: 76.385.. Val loss: 54.349.. Train L1 norm: 2.836.. Val L1 norm: 1.037.. Train Linf norm: 1876.718.. Val Linf norm: 31.373\n",
            "Epoch 104/142.. Train loss: 63.867.. Val loss: 54.349.. Train L1 norm: 1.557.. Val L1 norm: 1.037.. Train Linf norm: 563.557.. Val Linf norm: 31.366\n",
            "Epoch 105/142.. Train loss: 123.856.. Val loss: 54.351.. Train L1 norm: 3.074.. Val L1 norm: 1.037.. Train Linf norm: 2119.111.. Val Linf norm: 31.318\n",
            "Epoch 106/142.. Train loss: 94.436.. Val loss: 54.350.. Train L1 norm: 1.338.. Val L1 norm: 1.037.. Train Linf norm: 339.583.. Val Linf norm: 31.362\n",
            "Epoch 107/142.. Train loss: 140.543.. Val loss: 54.352.. Train L1 norm: 2.032.. Val L1 norm: 1.037.. Train Linf norm: 1052.957.. Val Linf norm: 31.303\n",
            "Epoch 108/142.. Train loss: 66.343.. Val loss: 54.351.. Train L1 norm: 1.840.. Val L1 norm: 1.037.. Train Linf norm: 852.286.. Val Linf norm: 31.313\n",
            "Epoch 109/142.. Train loss: 105.150.. Val loss: 54.351.. Train L1 norm: 2.763.. Val L1 norm: 1.037.. Train Linf norm: 1797.595.. Val Linf norm: 31.323\n",
            "Epoch 110/142.. Train loss: 88.915.. Val loss: 54.351.. Train L1 norm: 1.807.. Val L1 norm: 1.037.. Train Linf norm: 820.462.. Val Linf norm: 31.330\n",
            "Epoch 111/142.. Train loss: 62.063.. Val loss: 54.351.. Train L1 norm: 2.951.. Val L1 norm: 1.037.. Train Linf norm: 1993.785.. Val Linf norm: 31.327\n",
            "Epoch 112/142.. Train loss: 110.852.. Val loss: 54.349.. Train L1 norm: 3.091.. Val L1 norm: 1.037.. Train Linf norm: 2136.170.. Val Linf norm: 31.381\n",
            "Epoch 113/142.. Train loss: 79.785.. Val loss: 54.350.. Train L1 norm: 1.969.. Val L1 norm: 1.037.. Train Linf norm: 985.461.. Val Linf norm: 31.355\n",
            "Epoch 114/142.. Train loss: 70.527.. Val loss: 54.350.. Train L1 norm: 2.147.. Val L1 norm: 1.037.. Train Linf norm: 1167.079.. Val Linf norm: 31.375\n",
            "Epoch 115/142.. Train loss: 267.637.. Val loss: 54.347.. Train L1 norm: 2.881.. Val L1 norm: 1.037.. Train Linf norm: 1922.002.. Val Linf norm: 31.450\n",
            "Epoch 116/142.. Train loss: 87.711.. Val loss: 54.347.. Train L1 norm: 3.443.. Val L1 norm: 1.037.. Train Linf norm: 2496.776.. Val Linf norm: 31.448\n",
            "Epoch 117/142.. Train loss: 74.581.. Val loss: 54.347.. Train L1 norm: 2.620.. Val L1 norm: 1.037.. Train Linf norm: 1652.061.. Val Linf norm: 31.450\n",
            "Epoch 118/142.. Train loss: 316.789.. Val loss: 54.344.. Train L1 norm: 2.711.. Val L1 norm: 1.037.. Train Linf norm: 1746.065.. Val Linf norm: 31.544\n",
            "Epoch 119/142.. Train loss: 121.889.. Val loss: 54.342.. Train L1 norm: 1.129.. Val L1 norm: 1.037.. Train Linf norm: 127.267.. Val Linf norm: 31.587\n",
            "Epoch 120/142.. Train loss: 111.676.. Val loss: 54.341.. Train L1 norm: 3.487.. Val L1 norm: 1.037.. Train Linf norm: 2542.066.. Val Linf norm: 31.632\n",
            "Epoch 121/142.. Train loss: 77.909.. Val loss: 54.341.. Train L1 norm: 2.350.. Val L1 norm: 1.037.. Train Linf norm: 1375.942.. Val Linf norm: 31.621\n",
            "Epoch 122/142.. Train loss: 62.470.. Val loss: 54.341.. Train L1 norm: 1.715.. Val L1 norm: 1.037.. Train Linf norm: 727.912.. Val Linf norm: 31.624\n",
            "Epoch 123/142.. Train loss: 115.716.. Val loss: 54.342.. Train L1 norm: 1.231.. Val L1 norm: 1.037.. Train Linf norm: 230.832.. Val Linf norm: 31.588\n",
            "Epoch 124/142.. Train loss: 71.199.. Val loss: 54.342.. Train L1 norm: 2.551.. Val L1 norm: 1.037.. Train Linf norm: 1584.350.. Val Linf norm: 31.590\n",
            "Epoch 125/142.. Train loss: 88.748.. Val loss: 54.341.. Train L1 norm: 2.796.. Val L1 norm: 1.037.. Train Linf norm: 1833.628.. Val Linf norm: 31.625\n",
            "Epoch 126/142.. Train loss: 173.781.. Val loss: 54.342.. Train L1 norm: 1.717.. Val L1 norm: 1.037.. Train Linf norm: 727.913.. Val Linf norm: 31.610\n",
            "Epoch 127/142.. Train loss: 89.193.. Val loss: 54.342.. Train L1 norm: 3.213.. Val L1 norm: 1.037.. Train Linf norm: 2261.185.. Val Linf norm: 31.585\n",
            "Epoch 128/142.. Train loss: 60.306.. Val loss: 54.342.. Train L1 norm: 2.558.. Val L1 norm: 1.037.. Train Linf norm: 1590.740.. Val Linf norm: 31.587\n",
            "Epoch 129/142.. Train loss: 233.513.. Val loss: 54.345.. Train L1 norm: 2.338.. Val L1 norm: 1.037.. Train Linf norm: 1365.619.. Val Linf norm: 31.513\n",
            "Epoch 130/142.. Train loss: 199.748.. Val loss: 54.348.. Train L1 norm: 1.736.. Val L1 norm: 1.037.. Train Linf norm: 748.803.. Val Linf norm: 31.439\n",
            "Epoch 131/142.. Train loss: 218.061.. Val loss: 54.346.. Train L1 norm: 3.429.. Val L1 norm: 1.037.. Train Linf norm: 2483.429.. Val Linf norm: 31.492\n",
            "Epoch 132/142.. Train loss: 74.275.. Val loss: 54.346.. Train L1 norm: 2.872.. Val L1 norm: 1.037.. Train Linf norm: 1911.230.. Val Linf norm: 31.491\n",
            "Epoch 133/142.. Train loss: 60.313.. Val loss: 54.346.. Train L1 norm: 2.129.. Val L1 norm: 1.037.. Train Linf norm: 1150.893.. Val Linf norm: 31.492\n",
            "Epoch 134/142.. Train loss: 81.259.. Val loss: 54.347.. Train L1 norm: 2.028.. Val L1 norm: 1.037.. Train Linf norm: 1047.315.. Val Linf norm: 31.476\n",
            "Epoch 135/142.. Train loss: 71.388.. Val loss: 54.347.. Train L1 norm: 2.779.. Val L1 norm: 1.037.. Train Linf norm: 1815.348.. Val Linf norm: 31.458\n",
            "Epoch 136/142.. Train loss: 249.366.. Val loss: 54.344.. Train L1 norm: 2.036.. Val L1 norm: 1.037.. Train Linf norm: 1051.723.. Val Linf norm: 31.534\n",
            "Epoch 137/142.. Train loss: 60.554.. Val loss: 54.344.. Train L1 norm: 2.828.. Val L1 norm: 1.037.. Train Linf norm: 1863.455.. Val Linf norm: 31.543\n",
            "Epoch 138/142.. Train loss: 125.041.. Val loss: 54.343.. Train L1 norm: 3.258.. Val L1 norm: 1.037.. Train Linf norm: 2308.275.. Val Linf norm: 31.582\n",
            "Epoch 139/142.. Train loss: 63.295.. Val loss: 54.343.. Train L1 norm: 2.783.. Val L1 norm: 1.037.. Train Linf norm: 1820.463.. Val Linf norm: 31.585\n",
            "Epoch 140/142.. Train loss: 68.043.. Val loss: 54.343.. Train L1 norm: 2.256.. Val L1 norm: 1.037.. Train Linf norm: 1281.239.. Val Linf norm: 31.572\n",
            "Epoch 141/142.. Train loss: 94.602.. Val loss: 54.344.. Train L1 norm: 2.769.. Val L1 norm: 1.037.. Train Linf norm: 1805.194.. Val Linf norm: 31.544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:07:13,282]\u001b[0m Trial 74 finished with value: 1.0370204893430075 and parameters: {'n_layers': 6, 'n_units_0': 3636, 'n_units_1': 915, 'n_units_2': 3498, 'n_units_3': 489, 'n_units_4': 449, 'n_units_5': 1417, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 1.0005178313658807e-06, 'batch_size': 1024, 'n_epochs': 142, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.20552642624692496, 'dropout_rate': 0.042668600523855016, 'weight_decay': 0.00035059748201699036, 'beta1': 0.9244144572319054, 'beta2': 0.9990922250248856, 'factor': 0.1328604056891655, 'patience': 7, 'threshold': 0.0017173462561345582}. Best is trial 22 with value: 1.0150012904485066.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 142/142.. Train loss: 64.723.. Val loss: 54.344.. Train L1 norm: 2.162.. Val L1 norm: 1.037.. Train Linf norm: 1185.039.. Val Linf norm: 31.549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "\u001b[32m[I 2023-05-29 09:07:15,737]\u001b[0m Trial 75 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/135.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:07:19,038]\u001b[0m Trial 76 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/142.. Train loss: 356.230.. Val loss: 52.025.. Train L1 norm: 4.267.. Val L1 norm: 1.238.. Train Linf norm: 1656.966.. Val Linf norm: 90.365\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:07:21,620]\u001b[0m Trial 77 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/132.. Train loss: 5.192.. Val loss: 3.922.. Train L1 norm: 23.241.. Val L1 norm: 2.083.. Train Linf norm: 22284.484.. Val Linf norm: 543.328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:07:24,185]\u001b[0m Trial 78 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/146.. Train loss: 6424.265.. Val loss: 169.785.. Train L1 norm: 2.390.. Val L1 norm: 3.877.. Train Linf norm: 297.576.. Val Linf norm: 369.337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:07:32,376]\u001b[0m Trial 79 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/137.. Train loss: 4.219.. Val loss: 3.882.. Train L1 norm: 57.979.. Val L1 norm: 4.521.. Train Linf norm: 3584.121.. Val Linf norm: 170.747\n",
            "Epoch 1/150.. Train loss: 347.891.. Val loss: 57.171.. Train L1 norm: 3.198.. Val L1 norm: 1.068.. Train Linf norm: 2203.800.. Val Linf norm: 42.006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:07:38,383]\u001b[0m Trial 80 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/150.. Train loss: 2625.216.. Val loss: 55.053.. Train L1 norm: 1.904.. Val L1 norm: 1.105.. Train Linf norm: 879.313.. Val Linf norm: 76.669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:07:41,128]\u001b[0m Trial 81 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/146.. Train loss: 129.042.. Val loss: 56.261.. Train L1 norm: 146.156.. Val L1 norm: 1.310.. Train Linf norm: 147331.447.. Val Linf norm: 208.026\n",
            "Epoch 1/143.. Train loss: 99.653.. Val loss: 54.750.. Train L1 norm: 2.033.. Val L1 norm: 1.036.. Train Linf norm: 1046.323.. Val Linf norm: 29.668\n",
            "Epoch 2/143.. Train loss: 61.518.. Val loss: 54.307.. Train L1 norm: 1.044.. Val L1 norm: 1.054.. Train Linf norm: 36.745.. Val Linf norm: 42.531\n",
            "Epoch 3/143.. Train loss: 211.397.. Val loss: 54.335.. Train L1 norm: 2.176.. Val L1 norm: 1.053.. Train Linf norm: 1193.791.. Val Linf norm: 41.982\n",
            "Epoch 4/143.. Train loss: 114.526.. Val loss: 55.115.. Train L1 norm: 3.726.. Val L1 norm: 1.027.. Train Linf norm: 2786.437.. Val Linf norm: 23.437\n",
            "Epoch 5/143.. Train loss: 217.846.. Val loss: 54.480.. Train L1 norm: 2.085.. Val L1 norm: 1.048.. Train Linf norm: 1102.850.. Val Linf norm: 38.678\n",
            "Epoch 6/143.. Train loss: 64.288.. Val loss: 54.147.. Train L1 norm: 2.552.. Val L1 norm: 1.061.. Train Linf norm: 1580.700.. Val Linf norm: 47.232\n",
            "Epoch 7/143.. Train loss: 386.279.. Val loss: 54.946.. Train L1 norm: 2.183.. Val L1 norm: 1.032.. Train Linf norm: 1205.000.. Val Linf norm: 26.991\n",
            "Epoch 8/143.. Train loss: 129.638.. Val loss: 54.464.. Train L1 norm: 1.048.. Val L1 norm: 1.048.. Train Linf norm: 39.495.. Val Linf norm: 38.342\n",
            "Epoch 9/143.. Train loss: 77.777.. Val loss: 54.693.. Train L1 norm: 1.303.. Val L1 norm: 1.041.. Train Linf norm: 303.857.. Val Linf norm: 33.896\n",
            "Epoch 10/143.. Train loss: 224.485.. Val loss: 55.358.. Train L1 norm: 1.378.. Val L1 norm: 1.024.. Train Linf norm: 381.810.. Val Linf norm: 20.957\n",
            "Epoch 11/143.. Train loss: 539.300.. Val loss: 54.603.. Train L1 norm: 2.851.. Val L1 norm: 1.046.. Train Linf norm: 1889.456.. Val Linf norm: 37.537\n",
            "Epoch 12/143.. Train loss: 89.274.. Val loss: 54.771.. Train L1 norm: 3.680.. Val L1 norm: 1.041.. Train Linf norm: 2737.466.. Val Linf norm: 34.630\n",
            "Epoch 13/143.. Train loss: 60.593.. Val loss: 54.788.. Train L1 norm: 1.056.. Val L1 norm: 1.042.. Train Linf norm: 39.500.. Val Linf norm: 34.907\n",
            "Epoch 14/143.. Train loss: 128.591.. Val loss: 54.746.. Train L1 norm: 3.049.. Val L1 norm: 1.043.. Train Linf norm: 2091.239.. Val Linf norm: 35.891\n",
            "Epoch 15/143.. Train loss: 81.706.. Val loss: 54.712.. Train L1 norm: 2.684.. Val L1 norm: 1.044.. Train Linf norm: 1717.113.. Val Linf norm: 36.774\n",
            "Epoch 16/143.. Train loss: 71.938.. Val loss: 54.720.. Train L1 norm: 1.051.. Val L1 norm: 1.044.. Train Linf norm: 45.318.. Val Linf norm: 36.656\n",
            "Epoch 17/143.. Train loss: 163.262.. Val loss: 54.792.. Train L1 norm: 1.415.. Val L1 norm: 1.042.. Train Linf norm: 420.103.. Val Linf norm: 35.089\n",
            "Epoch 18/143.. Train loss: 130.541.. Val loss: 54.747.. Train L1 norm: 3.344.. Val L1 norm: 1.044.. Train Linf norm: 2392.239.. Val Linf norm: 36.190\n",
            "Epoch 19/143.. Train loss: 117.326.. Val loss: 54.698.. Train L1 norm: 2.840.. Val L1 norm: 1.045.. Train Linf norm: 1879.223.. Val Linf norm: 37.411\n",
            "Epoch 20/143.. Train loss: 120.654.. Val loss: 54.637.. Train L1 norm: 2.279.. Val L1 norm: 1.048.. Train Linf norm: 1297.173.. Val Linf norm: 38.961\n",
            "Epoch 21/143.. Train loss: 107.368.. Val loss: 54.632.. Train L1 norm: 1.670.. Val L1 norm: 1.048.. Train Linf norm: 679.322.. Val Linf norm: 39.097\n",
            "Epoch 22/143.. Train loss: 265.904.. Val loss: 54.624.. Train L1 norm: 2.438.. Val L1 norm: 1.048.. Train Linf norm: 1463.559.. Val Linf norm: 39.307\n",
            "Epoch 23/143.. Train loss: 61.386.. Val loss: 54.621.. Train L1 norm: 1.982.. Val L1 norm: 1.048.. Train Linf norm: 999.645.. Val Linf norm: 39.385\n",
            "Epoch 24/143.. Train loss: 90.582.. Val loss: 54.616.. Train L1 norm: 1.949.. Val L1 norm: 1.048.. Train Linf norm: 966.519.. Val Linf norm: 39.513\n",
            "Epoch 25/143.. Train loss: 99.375.. Val loss: 54.621.. Train L1 norm: 3.399.. Val L1 norm: 1.048.. Train Linf norm: 2440.038.. Val Linf norm: 39.405\n",
            "Epoch 26/143.. Train loss: 254.683.. Val loss: 54.610.. Train L1 norm: 3.472.. Val L1 norm: 1.049.. Train Linf norm: 2520.442.. Val Linf norm: 39.672\n",
            "Epoch 27/143.. Train loss: 61.268.. Val loss: 54.610.. Train L1 norm: 3.828.. Val L1 norm: 1.049.. Train Linf norm: 2889.021.. Val Linf norm: 39.682\n",
            "Epoch 28/143.. Train loss: 130.651.. Val loss: 54.611.. Train L1 norm: 3.377.. Val L1 norm: 1.049.. Train Linf norm: 2426.761.. Val Linf norm: 39.669\n",
            "Epoch 29/143.. Train loss: 66.089.. Val loss: 54.611.. Train L1 norm: 3.006.. Val L1 norm: 1.049.. Train Linf norm: 2048.002.. Val Linf norm: 39.669\n",
            "Epoch 30/143.. Train loss: 165.450.. Val loss: 54.611.. Train L1 norm: 2.754.. Val L1 norm: 1.049.. Train Linf norm: 1787.554.. Val Linf norm: 39.676\n",
            "Epoch 31/143.. Train loss: 104.176.. Val loss: 54.611.. Train L1 norm: 2.305.. Val L1 norm: 1.049.. Train Linf norm: 1329.785.. Val Linf norm: 39.682\n",
            "Epoch 32/143.. Train loss: 135.207.. Val loss: 54.610.. Train L1 norm: 1.102.. Val L1 norm: 1.049.. Train Linf norm: 96.337.. Val Linf norm: 39.698\n",
            "Epoch 33/143.. Train loss: 119.637.. Val loss: 54.610.. Train L1 norm: 3.357.. Val L1 norm: 1.049.. Train Linf norm: 2406.980.. Val Linf norm: 39.693\n",
            "Epoch 34/143.. Train loss: 63.468.. Val loss: 54.611.. Train L1 norm: 2.658.. Val L1 norm: 1.049.. Train Linf norm: 1689.498.. Val Linf norm: 39.685\n",
            "Epoch 35/143.. Train loss: 495.843.. Val loss: 54.609.. Train L1 norm: 2.227.. Val L1 norm: 1.049.. Train Linf norm: 1250.021.. Val Linf norm: 39.721\n",
            "Epoch 36/143.. Train loss: 689.828.. Val loss: 54.607.. Train L1 norm: 3.276.. Val L1 norm: 1.049.. Train Linf norm: 2322.963.. Val Linf norm: 39.767\n",
            "Epoch 37/143.. Train loss: 62.069.. Val loss: 54.607.. Train L1 norm: 2.387.. Val L1 norm: 1.049.. Train Linf norm: 1414.499.. Val Linf norm: 39.770\n",
            "Epoch 38/143.. Train loss: 215.460.. Val loss: 54.608.. Train L1 norm: 2.498.. Val L1 norm: 1.049.. Train Linf norm: 1527.440.. Val Linf norm: 39.756\n",
            "Epoch 39/143.. Train loss: 210.159.. Val loss: 54.608.. Train L1 norm: 2.672.. Val L1 norm: 1.049.. Train Linf norm: 1703.715.. Val Linf norm: 39.766\n",
            "Epoch 40/143.. Train loss: 656.931.. Val loss: 54.605.. Train L1 norm: 2.299.. Val L1 norm: 1.049.. Train Linf norm: 1322.894.. Val Linf norm: 39.821\n",
            "Epoch 41/143.. Train loss: 61.516.. Val loss: 54.605.. Train L1 norm: 2.455.. Val L1 norm: 1.049.. Train Linf norm: 1482.211.. Val Linf norm: 39.823\n",
            "Epoch 42/143.. Train loss: 64.744.. Val loss: 54.605.. Train L1 norm: 1.757.. Val L1 norm: 1.049.. Train Linf norm: 764.305.. Val Linf norm: 39.823\n",
            "Epoch 43/143.. Train loss: 181.505.. Val loss: 54.605.. Train L1 norm: 2.634.. Val L1 norm: 1.049.. Train Linf norm: 1664.999.. Val Linf norm: 39.841\n",
            "Epoch 44/143.. Train loss: 107.286.. Val loss: 54.604.. Train L1 norm: 3.662.. Val L1 norm: 1.049.. Train Linf norm: 2719.445.. Val Linf norm: 39.850\n",
            "Epoch 45/143.. Train loss: 95.787.. Val loss: 54.604.. Train L1 norm: 2.809.. Val L1 norm: 1.049.. Train Linf norm: 1842.296.. Val Linf norm: 39.864\n",
            "Epoch 46/143.. Train loss: 142.333.. Val loss: 54.603.. Train L1 norm: 2.372.. Val L1 norm: 1.049.. Train Linf norm: 1398.798.. Val Linf norm: 39.879\n",
            "Epoch 47/143.. Train loss: 138.922.. Val loss: 54.603.. Train L1 norm: 1.326.. Val L1 norm: 1.049.. Train Linf norm: 326.936.. Val Linf norm: 39.892\n",
            "Epoch 48/143.. Train loss: 155.447.. Val loss: 54.603.. Train L1 norm: 2.978.. Val L1 norm: 1.049.. Train Linf norm: 2018.957.. Val Linf norm: 39.877\n",
            "Epoch 49/143.. Train loss: 61.448.. Val loss: 54.603.. Train L1 norm: 2.426.. Val L1 norm: 1.049.. Train Linf norm: 1453.847.. Val Linf norm: 39.879\n",
            "Epoch 50/143.. Train loss: 93.609.. Val loss: 54.603.. Train L1 norm: 1.640.. Val L1 norm: 1.049.. Train Linf norm: 649.735.. Val Linf norm: 39.887\n",
            "Epoch 51/143.. Train loss: 96.125.. Val loss: 54.603.. Train L1 norm: 3.196.. Val L1 norm: 1.049.. Train Linf norm: 2239.667.. Val Linf norm: 39.895\n",
            "Epoch 52/143.. Train loss: 62.721.. Val loss: 54.603.. Train L1 norm: 2.406.. Val L1 norm: 1.049.. Train Linf norm: 1433.234.. Val Linf norm: 39.896\n",
            "Epoch 53/143.. Train loss: 96.873.. Val loss: 54.602.. Train L1 norm: 3.295.. Val L1 norm: 1.049.. Train Linf norm: 2339.952.. Val Linf norm: 39.906\n",
            "Epoch 54/143.. Train loss: 146.453.. Val loss: 54.602.. Train L1 norm: 2.947.. Val L1 norm: 1.049.. Train Linf norm: 1986.037.. Val Linf norm: 39.922\n",
            "Epoch 55/143.. Train loss: 119.409.. Val loss: 54.602.. Train L1 norm: 3.160.. Val L1 norm: 1.049.. Train Linf norm: 2205.104.. Val Linf norm: 39.919\n",
            "Epoch 56/143.. Train loss: 73.451.. Val loss: 54.602.. Train L1 norm: 1.657.. Val L1 norm: 1.049.. Train Linf norm: 666.415.. Val Linf norm: 39.910\n",
            "Epoch 57/143.. Train loss: 154.851.. Val loss: 54.603.. Train L1 norm: 2.266.. Val L1 norm: 1.049.. Train Linf norm: 1288.851.. Val Linf norm: 39.892\n",
            "Epoch 58/143.. Train loss: 66.258.. Val loss: 54.603.. Train L1 norm: 4.560.. Val L1 norm: 1.049.. Train Linf norm: 3637.272.. Val Linf norm: 39.896\n",
            "Epoch 59/143.. Train loss: 60.825.. Val loss: 54.603.. Train L1 norm: 2.953.. Val L1 norm: 1.049.. Train Linf norm: 1980.245.. Val Linf norm: 39.898\n",
            "Epoch 60/143.. Train loss: 153.234.. Val loss: 54.604.. Train L1 norm: 2.080.. Val L1 norm: 1.049.. Train Linf norm: 1095.670.. Val Linf norm: 39.884\n",
            "Epoch 61/143.. Train loss: 251.965.. Val loss: 54.603.. Train L1 norm: 1.649.. Val L1 norm: 1.049.. Train Linf norm: 657.908.. Val Linf norm: 39.912\n",
            "Epoch 62/143.. Train loss: 352.201.. Val loss: 54.602.. Train L1 norm: 1.922.. Val L1 norm: 1.049.. Train Linf norm: 936.033.. Val Linf norm: 39.929\n",
            "Epoch 63/143.. Train loss: 206.754.. Val loss: 54.602.. Train L1 norm: 1.939.. Val L1 norm: 1.049.. Train Linf norm: 954.344.. Val Linf norm: 39.928\n",
            "Epoch 64/143.. Train loss: 306.134.. Val loss: 54.601.. Train L1 norm: 1.357.. Val L1 norm: 1.049.. Train Linf norm: 360.421.. Val Linf norm: 39.961\n",
            "Epoch 65/143.. Train loss: 63.675.. Val loss: 54.601.. Train L1 norm: 2.596.. Val L1 norm: 1.049.. Train Linf norm: 1626.430.. Val Linf norm: 39.965\n",
            "Epoch 66/143.. Train loss: 181.155.. Val loss: 54.600.. Train L1 norm: 3.654.. Val L1 norm: 1.049.. Train Linf norm: 2710.757.. Val Linf norm: 39.982\n",
            "Epoch 67/143.. Train loss: 414.915.. Val loss: 54.602.. Train L1 norm: 1.774.. Val L1 norm: 1.049.. Train Linf norm: 785.554.. Val Linf norm: 39.950\n",
            "Epoch 68/143.. Train loss: 59.901.. Val loss: 54.601.. Train L1 norm: 3.519.. Val L1 norm: 1.049.. Train Linf norm: 2571.833.. Val Linf norm: 39.951\n",
            "Epoch 69/143.. Train loss: 227.512.. Val loss: 54.601.. Train L1 norm: 2.170.. Val L1 norm: 1.049.. Train Linf norm: 1190.163.. Val Linf norm: 39.973\n",
            "Epoch 70/143.. Train loss: 316.082.. Val loss: 54.600.. Train L1 norm: 2.761.. Val L1 norm: 1.049.. Train Linf norm: 1796.149.. Val Linf norm: 40.004\n",
            "Epoch 71/143.. Train loss: 144.082.. Val loss: 54.599.. Train L1 norm: 2.589.. Val L1 norm: 1.049.. Train Linf norm: 1620.554.. Val Linf norm: 40.008\n",
            "Epoch 72/143.. Train loss: 98.393.. Val loss: 54.599.. Train L1 norm: 2.595.. Val L1 norm: 1.049.. Train Linf norm: 1620.831.. Val Linf norm: 40.017\n",
            "Epoch 73/143.. Train loss: 116.055.. Val loss: 54.599.. Train L1 norm: 3.123.. Val L1 norm: 1.049.. Train Linf norm: 2163.816.. Val Linf norm: 40.030\n",
            "Epoch 74/143.. Train loss: 67.451.. Val loss: 54.598.. Train L1 norm: 2.519.. Val L1 norm: 1.049.. Train Linf norm: 1546.099.. Val Linf norm: 40.034\n",
            "Epoch 75/143.. Train loss: 63.772.. Val loss: 54.598.. Train L1 norm: 3.411.. Val L1 norm: 1.049.. Train Linf norm: 2460.863.. Val Linf norm: 40.038\n",
            "Epoch 76/143.. Train loss: 195.903.. Val loss: 54.598.. Train L1 norm: 2.535.. Val L1 norm: 1.049.. Train Linf norm: 1566.399.. Val Linf norm: 40.042\n",
            "Epoch 77/143.. Train loss: 82.543.. Val loss: 54.597.. Train L1 norm: 4.751.. Val L1 norm: 1.049.. Train Linf norm: 3835.433.. Val Linf norm: 40.064\n",
            "Epoch 78/143.. Train loss: 210.427.. Val loss: 54.596.. Train L1 norm: 2.595.. Val L1 norm: 1.049.. Train Linf norm: 1625.867.. Val Linf norm: 40.089\n",
            "Epoch 79/143.. Train loss: 127.836.. Val loss: 54.597.. Train L1 norm: 2.388.. Val L1 norm: 1.049.. Train Linf norm: 1416.095.. Val Linf norm: 40.083\n",
            "Epoch 80/143.. Train loss: 69.840.. Val loss: 54.597.. Train L1 norm: 1.195.. Val L1 norm: 1.049.. Train Linf norm: 193.273.. Val Linf norm: 40.077\n",
            "Epoch 81/143.. Train loss: 70.148.. Val loss: 54.597.. Train L1 norm: 2.081.. Val L1 norm: 1.049.. Train Linf norm: 417.788.. Val Linf norm: 40.082\n",
            "Epoch 82/143.. Train loss: 63.701.. Val loss: 54.597.. Train L1 norm: 2.440.. Val L1 norm: 1.049.. Train Linf norm: 1466.053.. Val Linf norm: 40.086\n",
            "Epoch 83/143.. Train loss: 65.504.. Val loss: 54.597.. Train L1 norm: 1.906.. Val L1 norm: 1.049.. Train Linf norm: 921.069.. Val Linf norm: 40.088\n",
            "Epoch 84/143.. Train loss: 106.504.. Val loss: 54.596.. Train L1 norm: 3.319.. Val L1 norm: 1.049.. Train Linf norm: 2367.624.. Val Linf norm: 40.099\n",
            "Epoch 85/143.. Train loss: 257.317.. Val loss: 54.595.. Train L1 norm: 3.109.. Val L1 norm: 1.049.. Train Linf norm: 2151.716.. Val Linf norm: 40.127\n",
            "Epoch 86/143.. Train loss: 108.414.. Val loss: 54.596.. Train L1 norm: 3.375.. Val L1 norm: 1.049.. Train Linf norm: 2424.987.. Val Linf norm: 40.120\n",
            "Epoch 87/143.. Train loss: 73.776.. Val loss: 54.596.. Train L1 norm: 1.579.. Val L1 norm: 1.049.. Train Linf norm: 583.767.. Val Linf norm: 40.115\n",
            "Epoch 88/143.. Train loss: 60.387.. Val loss: 54.596.. Train L1 norm: 3.110.. Val L1 norm: 1.049.. Train Linf norm: 2153.518.. Val Linf norm: 40.117\n",
            "Epoch 89/143.. Train loss: 94.629.. Val loss: 54.595.. Train L1 norm: 2.062.. Val L1 norm: 1.049.. Train Linf norm: 1078.342.. Val Linf norm: 40.128\n",
            "Epoch 90/143.. Train loss: 164.871.. Val loss: 54.596.. Train L1 norm: 2.658.. Val L1 norm: 1.049.. Train Linf norm: 1687.751.. Val Linf norm: 40.120\n",
            "Epoch 91/143.. Train loss: 79.703.. Val loss: 54.596.. Train L1 norm: 3.553.. Val L1 norm: 1.049.. Train Linf norm: 2606.839.. Val Linf norm: 40.109\n",
            "Epoch 92/143.. Train loss: 119.895.. Val loss: 54.597.. Train L1 norm: 2.733.. Val L1 norm: 1.049.. Train Linf norm: 1765.119.. Val Linf norm: 40.096\n",
            "Epoch 93/143.. Train loss: 79.884.. Val loss: 54.597.. Train L1 norm: 3.368.. Val L1 norm: 1.049.. Train Linf norm: 2418.777.. Val Linf norm: 40.090\n",
            "Epoch 94/143.. Train loss: 86.899.. Val loss: 54.597.. Train L1 norm: 1.820.. Val L1 norm: 1.049.. Train Linf norm: 828.870.. Val Linf norm: 40.105\n",
            "Epoch 95/143.. Train loss: 121.359.. Val loss: 54.596.. Train L1 norm: 2.485.. Val L1 norm: 1.049.. Train Linf norm: 1513.832.. Val Linf norm: 40.125\n",
            "Epoch 96/143.. Train loss: 72.874.. Val loss: 54.596.. Train L1 norm: 1.904.. Val L1 norm: 1.049.. Train Linf norm: 918.058.. Val Linf norm: 40.132\n",
            "Epoch 97/143.. Train loss: 62.626.. Val loss: 54.596.. Train L1 norm: 2.685.. Val L1 norm: 1.049.. Train Linf norm: 1717.038.. Val Linf norm: 40.135\n",
            "Epoch 98/143.. Train loss: 80.356.. Val loss: 54.595.. Train L1 norm: 2.333.. Val L1 norm: 1.049.. Train Linf norm: 1358.413.. Val Linf norm: 40.143\n",
            "Epoch 99/143.. Train loss: 69.784.. Val loss: 54.595.. Train L1 norm: 2.493.. Val L1 norm: 1.049.. Train Linf norm: 1522.503.. Val Linf norm: 40.144\n",
            "Epoch 100/143.. Train loss: 140.189.. Val loss: 54.595.. Train L1 norm: 2.738.. Val L1 norm: 1.049.. Train Linf norm: 1773.621.. Val Linf norm: 40.164\n",
            "Epoch 101/143.. Train loss: 91.090.. Val loss: 54.594.. Train L1 norm: 1.932.. Val L1 norm: 1.049.. Train Linf norm: 946.704.. Val Linf norm: 40.174\n",
            "Epoch 102/143.. Train loss: 91.801.. Val loss: 54.594.. Train L1 norm: 2.006.. Val L1 norm: 1.049.. Train Linf norm: 1023.940.. Val Linf norm: 40.181\n",
            "Epoch 103/143.. Train loss: 60.013.. Val loss: 54.594.. Train L1 norm: 2.047.. Val L1 norm: 1.049.. Train Linf norm: 1063.582.. Val Linf norm: 40.191\n",
            "Epoch 104/143.. Train loss: 90.450.. Val loss: 54.593.. Train L1 norm: 2.367.. Val L1 norm: 1.049.. Train Linf norm: 1390.919.. Val Linf norm: 40.204\n",
            "Epoch 105/143.. Train loss: 61.209.. Val loss: 54.593.. Train L1 norm: 3.383.. Val L1 norm: 1.049.. Train Linf norm: 2432.508.. Val Linf norm: 40.207\n",
            "Epoch 106/143.. Train loss: 243.383.. Val loss: 54.592.. Train L1 norm: 3.211.. Val L1 norm: 1.049.. Train Linf norm: 2257.287.. Val Linf norm: 40.238\n",
            "Epoch 107/143.. Train loss: 103.216.. Val loss: 54.592.. Train L1 norm: 1.930.. Val L1 norm: 1.049.. Train Linf norm: 944.511.. Val Linf norm: 40.226\n",
            "Epoch 108/143.. Train loss: 65.305.. Val loss: 54.593.. Train L1 norm: 1.866.. Val L1 norm: 1.049.. Train Linf norm: 879.291.. Val Linf norm: 40.224\n",
            "Epoch 109/143.. Train loss: 135.129.. Val loss: 54.592.. Train L1 norm: 1.328.. Val L1 norm: 1.049.. Train Linf norm: 326.080.. Val Linf norm: 40.242\n",
            "Epoch 110/143.. Train loss: 86.802.. Val loss: 54.591.. Train L1 norm: 1.769.. Val L1 norm: 1.049.. Train Linf norm: 781.791.. Val Linf norm: 40.258\n",
            "Epoch 111/143.. Train loss: 60.330.. Val loss: 54.591.. Train L1 norm: 1.486.. Val L1 norm: 1.049.. Train Linf norm: 490.000.. Val Linf norm: 40.261\n",
            "Epoch 112/143.. Train loss: 184.836.. Val loss: 54.593.. Train L1 norm: 3.465.. Val L1 norm: 1.049.. Train Linf norm: 2516.776.. Val Linf norm: 40.227\n",
            "Epoch 113/143.. Train loss: 114.324.. Val loss: 54.592.. Train L1 norm: 2.476.. Val L1 norm: 1.049.. Train Linf norm: 1504.548.. Val Linf norm: 40.249\n",
            "Epoch 114/143.. Train loss: 141.150.. Val loss: 54.591.. Train L1 norm: 2.054.. Val L1 norm: 1.049.. Train Linf norm: 1073.097.. Val Linf norm: 40.272\n",
            "Epoch 115/143.. Train loss: 112.169.. Val loss: 54.591.. Train L1 norm: 2.967.. Val L1 norm: 1.049.. Train Linf norm: 2007.648.. Val Linf norm: 40.284\n",
            "Epoch 116/143.. Train loss: 142.077.. Val loss: 54.591.. Train L1 norm: 4.076.. Val L1 norm: 1.049.. Train Linf norm: 3143.860.. Val Linf norm: 40.273\n",
            "Epoch 117/143.. Train loss: 94.566.. Val loss: 54.591.. Train L1 norm: 1.956.. Val L1 norm: 1.049.. Train Linf norm: 972.442.. Val Linf norm: 40.274\n",
            "Epoch 118/143.. Train loss: 96.030.. Val loss: 54.591.. Train L1 norm: 2.483.. Val L1 norm: 1.049.. Train Linf norm: 1512.047.. Val Linf norm: 40.273\n",
            "Epoch 119/143.. Train loss: 98.377.. Val loss: 54.592.. Train L1 norm: 3.569.. Val L1 norm: 1.049.. Train Linf norm: 2624.450.. Val Linf norm: 40.269\n",
            "Epoch 120/143.. Train loss: 80.488.. Val loss: 54.592.. Train L1 norm: 2.261.. Val L1 norm: 1.049.. Train Linf norm: 1283.786.. Val Linf norm: 40.258\n",
            "Epoch 121/143.. Train loss: 394.787.. Val loss: 54.591.. Train L1 norm: 3.058.. Val L1 norm: 1.049.. Train Linf norm: 2100.250.. Val Linf norm: 40.290\n",
            "Epoch 122/143.. Train loss: 138.345.. Val loss: 54.592.. Train L1 norm: 4.157.. Val L1 norm: 1.049.. Train Linf norm: 3225.596.. Val Linf norm: 40.274\n",
            "Epoch 123/143.. Train loss: 136.465.. Val loss: 54.591.. Train L1 norm: 3.263.. Val L1 norm: 1.049.. Train Linf norm: 2309.917.. Val Linf norm: 40.294\n",
            "Epoch 124/143.. Train loss: 71.342.. Val loss: 54.591.. Train L1 norm: 3.474.. Val L1 norm: 1.049.. Train Linf norm: 2526.446.. Val Linf norm: 40.302\n",
            "Epoch 125/143.. Train loss: 94.140.. Val loss: 54.590.. Train L1 norm: 3.002.. Val L1 norm: 1.049.. Train Linf norm: 2044.298.. Val Linf norm: 40.318\n",
            "Epoch 126/143.. Train loss: 61.661.. Val loss: 54.590.. Train L1 norm: 1.591.. Val L1 norm: 1.049.. Train Linf norm: 597.665.. Val Linf norm: 40.321\n",
            "Epoch 127/143.. Train loss: 339.108.. Val loss: 54.588.. Train L1 norm: 3.346.. Val L1 norm: 1.050.. Train Linf norm: 2397.006.. Val Linf norm: 40.366\n",
            "Epoch 128/143.. Train loss: 110.107.. Val loss: 54.589.. Train L1 norm: 2.224.. Val L1 norm: 1.050.. Train Linf norm: 1245.875.. Val Linf norm: 40.354\n",
            "Epoch 129/143.. Train loss: 60.929.. Val loss: 54.589.. Train L1 norm: 3.323.. Val L1 norm: 1.050.. Train Linf norm: 2371.136.. Val Linf norm: 40.356\n",
            "Epoch 130/143.. Train loss: 71.933.. Val loss: 54.588.. Train L1 norm: 3.135.. Val L1 norm: 1.050.. Train Linf norm: 2178.748.. Val Linf norm: 40.363\n",
            "Epoch 131/143.. Train loss: 666.793.. Val loss: 54.591.. Train L1 norm: 2.027.. Val L1 norm: 1.049.. Train Linf norm: 1041.449.. Val Linf norm: 40.307\n",
            "Epoch 132/143.. Train loss: 210.014.. Val loss: 54.591.. Train L1 norm: 2.979.. Val L1 norm: 1.049.. Train Linf norm: 2018.361.. Val Linf norm: 40.289\n",
            "Epoch 133/143.. Train loss: 173.800.. Val loss: 54.591.. Train L1 norm: 2.246.. Val L1 norm: 1.049.. Train Linf norm: 1267.699.. Val Linf norm: 40.308\n",
            "Epoch 134/143.. Train loss: 340.445.. Val loss: 54.592.. Train L1 norm: 3.180.. Val L1 norm: 1.049.. Train Linf norm: 2226.474.. Val Linf norm: 40.288\n",
            "Epoch 135/143.. Train loss: 661.555.. Val loss: 54.590.. Train L1 norm: 3.089.. Val L1 norm: 1.049.. Train Linf norm: 2132.510.. Val Linf norm: 40.324\n",
            "Epoch 136/143.. Train loss: 173.811.. Val loss: 54.590.. Train L1 norm: 3.614.. Val L1 norm: 1.049.. Train Linf norm: 2669.551.. Val Linf norm: 40.337\n",
            "Epoch 137/143.. Train loss: 126.381.. Val loss: 54.589.. Train L1 norm: 2.044.. Val L1 norm: 1.050.. Train Linf norm: 1063.306.. Val Linf norm: 40.353\n",
            "Epoch 138/143.. Train loss: 105.387.. Val loss: 54.589.. Train L1 norm: 3.037.. Val L1 norm: 1.050.. Train Linf norm: 2078.367.. Val Linf norm: 40.363\n",
            "Epoch 139/143.. Train loss: 124.484.. Val loss: 54.589.. Train L1 norm: 2.420.. Val L1 norm: 1.050.. Train Linf norm: 1447.833.. Val Linf norm: 40.355\n",
            "Epoch 140/143.. Train loss: 60.220.. Val loss: 54.589.. Train L1 norm: 3.372.. Val L1 norm: 1.050.. Train Linf norm: 2417.366.. Val Linf norm: 40.355\n",
            "Epoch 141/143.. Train loss: 60.113.. Val loss: 54.589.. Train L1 norm: 3.294.. Val L1 norm: 1.050.. Train Linf norm: 2343.358.. Val Linf norm: 40.357\n",
            "Epoch 142/143.. Train loss: 64.960.. Val loss: 54.589.. Train L1 norm: 2.332.. Val L1 norm: 1.050.. Train Linf norm: 1355.372.. Val Linf norm: 40.360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:11:01,347]\u001b[0m Trial 82 finished with value: 1.049514224243164 and parameters: {'n_layers': 6, 'n_units_0': 3600, 'n_units_1': 205, 'n_units_2': 3750, 'n_units_3': 424, 'n_units_4': 220, 'n_units_5': 1300, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 1.0208555578716084e-06, 'batch_size': 1024, 'n_epochs': 143, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.20469599761941343, 'dropout_rate': 0.06091584896932732, 'weight_decay': 0.0009820708424865844, 'beta1': 0.9259420023665554, 'beta2': 0.9991732668429878, 'factor': 0.12820100599544104, 'patience': 6, 'threshold': 0.002130823259734895}. Best is trial 22 with value: 1.0150012904485066.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 143/143.. Train loss: 64.779.. Val loss: 54.589.. Train L1 norm: 3.002.. Val L1 norm: 1.050.. Train Linf norm: 2042.194.. Val Linf norm: 40.362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:11:04,576]\u001b[0m Trial 83 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/141.. Train loss: 1821.594.. Val loss: 52.635.. Train L1 norm: 6.294.. Val L1 norm: 1.129.. Train Linf norm: 5351.709.. Val Linf norm: 87.055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:11:07,195]\u001b[0m Trial 84 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/148.. Train loss: 3097.546.. Val loss: 52.314.. Train L1 norm: 5.934.. Val L1 norm: 1.162.. Train Linf norm: 4992.592.. Val Linf norm: 107.547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:11:09,590]\u001b[0m Trial 85 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/135.. Train loss: 2.596.. Val loss: 2.340.. Train L1 norm: 3.991.. Val L1 norm: 1.460.. Train Linf norm: 2988.438.. Val Linf norm: 258.774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:11:11,369]\u001b[0m Trial 86 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/145.. Train loss: 8800.775.. Val loss: 52.346.. Train L1 norm: 7.530.. Val L1 norm: 1.216.. Train Linf norm: 13132.583.. Val Linf norm: 243.806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:11:24,114]\u001b[0m Trial 87 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/140.. Train loss: 11369.956.. Val loss: 52.349.. Train L1 norm: 7.677.. Val L1 norm: 1.153.. Train Linf norm: 853.713.. Val Linf norm: 19.921\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:11:26,545]\u001b[0m Trial 88 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/143.. Train loss: 591.897.. Val loss: 51.834.. Train L1 norm: 6.299.. Val L1 norm: 1.213.. Train Linf norm: 5387.533.. Val Linf norm: 136.746\n",
            "Epoch 1/131.. Train loss: 2225.839.. Val loss: 59.082.. Train L1 norm: 3.566.. Val L1 norm: 1.048.. Train Linf norm: 2584.821.. Val Linf norm: 31.547\n",
            "Epoch 2/131.. Train loss: 95.730.. Val loss: 59.835.. Train L1 norm: 4.032.. Val L1 norm: 1.081.. Train Linf norm: 3074.295.. Val Linf norm: 51.717\n",
            "Epoch 3/131.. Train loss: 1045.058.. Val loss: 55.225.. Train L1 norm: 3.158.. Val L1 norm: 1.137.. Train Linf norm: 2163.810.. Val Linf norm: 104.459\n",
            "Epoch 4/131.. Train loss: 1145.374.. Val loss: 60.094.. Train L1 norm: 6.620.. Val L1 norm: 1.093.. Train Linf norm: 5710.744.. Val Linf norm: 56.251\n",
            "Epoch 5/131.. Train loss: 3809.171.. Val loss: 56.672.. Train L1 norm: 2.252.. Val L1 norm: 1.083.. Train Linf norm: 1234.313.. Val Linf norm: 66.523\n",
            "Epoch 6/131.. Train loss: 3081.549.. Val loss: 57.655.. Train L1 norm: 6.334.. Val L1 norm: 1.056.. Train Linf norm: 5407.353.. Val Linf norm: 40.899\n",
            "Epoch 7/131.. Train loss: 2124.544.. Val loss: 53.730.. Train L1 norm: 1.301.. Val L1 norm: 1.199.. Train Linf norm: 268.830.. Val Linf norm: 138.957\n",
            "Epoch 8/131.. Train loss: 2810.341.. Val loss: 56.536.. Train L1 norm: 6.167.. Val L1 norm: 1.090.. Train Linf norm: 5213.846.. Val Linf norm: 73.137\n",
            "Epoch 9/131.. Train loss: 608.447.. Val loss: 60.446.. Train L1 norm: 2.462.. Val L1 norm: 1.104.. Train Linf norm: 1466.365.. Val Linf norm: 60.948\n",
            "Epoch 10/131.. Train loss: 10216.441.. Val loss: 54.219.. Train L1 norm: 2.642.. Val L1 norm: 1.185.. Train Linf norm: 1624.835.. Val Linf norm: 132.880\n",
            "Epoch 11/131.. Train loss: 7473.798.. Val loss: 59.355.. Train L1 norm: 2.057.. Val L1 norm: 1.077.. Train Linf norm: 1036.673.. Val Linf norm: 49.018\n",
            "Epoch 12/131.. Train loss: 242.252.. Val loss: 60.544.. Train L1 norm: 2.383.. Val L1 norm: 1.111.. Train Linf norm: 1369.542.. Val Linf norm: 64.075\n",
            "Epoch 13/131.. Train loss: 2567.604.. Val loss: 56.264.. Train L1 norm: 2.192.. Val L1 norm: 1.093.. Train Linf norm: 1172.773.. Val Linf norm: 74.818\n",
            "Epoch 14/131.. Train loss: 308.539.. Val loss: 56.748.. Train L1 norm: 4.463.. Val L1 norm: 1.081.. Train Linf norm: 3508.470.. Val Linf norm: 65.191\n",
            "Epoch 15/131.. Train loss: 237.207.. Val loss: 56.701.. Train L1 norm: 2.121.. Val L1 norm: 1.083.. Train Linf norm: 1113.306.. Val Linf norm: 66.559\n",
            "Epoch 16/131.. Train loss: 781.823.. Val loss: 56.345.. Train L1 norm: 5.601.. Val L1 norm: 1.092.. Train Linf norm: 4676.614.. Val Linf norm: 74.185\n",
            "Epoch 17/131.. Train loss: 81.844.. Val loss: 56.389.. Train L1 norm: 2.897.. Val L1 norm: 1.091.. Train Linf norm: 1902.279.. Val Linf norm: 73.250\n",
            "Epoch 18/131.. Train loss: 755.215.. Val loss: 56.202.. Train L1 norm: 1.894.. Val L1 norm: 1.096.. Train Linf norm: 877.226.. Val Linf norm: 77.471\n",
            "Epoch 19/131.. Train loss: 226.905.. Val loss: 55.828.. Train L1 norm: 5.891.. Val L1 norm: 1.108.. Train Linf norm: 4970.125.. Val Linf norm: 85.787\n",
            "Epoch 20/131.. Train loss: 326.722.. Val loss: 56.016.. Train L1 norm: 5.639.. Val L1 norm: 1.102.. Train Linf norm: 4714.036.. Val Linf norm: 81.755\n",
            "Epoch 21/131.. Train loss: 241.959.. Val loss: 56.280.. Train L1 norm: 5.124.. Val L1 norm: 1.095.. Train Linf norm: 4187.328.. Val Linf norm: 76.168\n",
            "Epoch 22/131.. Train loss: 366.046.. Val loss: 56.315.. Train L1 norm: 5.615.. Val L1 norm: 1.094.. Train Linf norm: 4689.397.. Val Linf norm: 75.412\n",
            "Epoch 23/131.. Train loss: 63.278.. Val loss: 56.313.. Train L1 norm: 5.082.. Val L1 norm: 1.094.. Train Linf norm: 4140.343.. Val Linf norm: 75.459\n",
            "Epoch 24/131.. Train loss: 226.757.. Val loss: 56.341.. Train L1 norm: 2.841.. Val L1 norm: 1.093.. Train Linf norm: 1852.112.. Val Linf norm: 74.868\n",
            "Epoch 25/131.. Train loss: 684.570.. Val loss: 56.351.. Train L1 norm: 3.314.. Val L1 norm: 1.093.. Train Linf norm: 2325.618.. Val Linf norm: 74.661\n",
            "Epoch 26/131.. Train loss: 74.000.. Val loss: 56.465.. Train L1 norm: 1.754.. Val L1 norm: 1.090.. Train Linf norm: 737.596.. Val Linf norm: 72.199\n",
            "Epoch 27/131.. Train loss: 76.472.. Val loss: 56.457.. Train L1 norm: 2.024.. Val L1 norm: 1.090.. Train Linf norm: 1018.574.. Val Linf norm: 72.382\n",
            "Epoch 28/131.. Train loss: 105.144.. Val loss: 56.444.. Train L1 norm: 4.485.. Val L1 norm: 1.090.. Train Linf norm: 3530.899.. Val Linf norm: 72.669\n",
            "Epoch 29/131.. Train loss: 86.504.. Val loss: 56.445.. Train L1 norm: 3.241.. Val L1 norm: 1.090.. Train Linf norm: 2255.773.. Val Linf norm: 72.658\n",
            "Epoch 30/131.. Train loss: 70.540.. Val loss: 56.445.. Train L1 norm: 2.062.. Val L1 norm: 1.090.. Train Linf norm: 1029.030.. Val Linf norm: 72.664\n",
            "Epoch 31/131.. Train loss: 193.065.. Val loss: 56.442.. Train L1 norm: 2.884.. Val L1 norm: 1.090.. Train Linf norm: 1895.950.. Val Linf norm: 72.717\n",
            "Epoch 32/131.. Train loss: 132.008.. Val loss: 56.441.. Train L1 norm: 5.673.. Val L1 norm: 1.090.. Train Linf norm: 4749.936.. Val Linf norm: 72.752\n",
            "Epoch 33/131.. Train loss: 159.633.. Val loss: 56.439.. Train L1 norm: 1.979.. Val L1 norm: 1.090.. Train Linf norm: 971.011.. Val Linf norm: 72.794\n",
            "Epoch 34/131.. Train loss: 338.938.. Val loss: 56.443.. Train L1 norm: 4.786.. Val L1 norm: 1.090.. Train Linf norm: 3839.929.. Val Linf norm: 72.706\n",
            "Epoch 35/131.. Train loss: 486.297.. Val loss: 56.437.. Train L1 norm: 1.275.. Val L1 norm: 1.090.. Train Linf norm: 248.922.. Val Linf norm: 72.826\n",
            "Epoch 36/131.. Train loss: 175.301.. Val loss: 56.435.. Train L1 norm: 1.581.. Val L1 norm: 1.090.. Train Linf norm: 560.851.. Val Linf norm: 72.881\n",
            "Epoch 37/131.. Train loss: 93.859.. Val loss: 56.436.. Train L1 norm: 2.387.. Val L1 norm: 1.090.. Train Linf norm: 1378.689.. Val Linf norm: 72.866\n",
            "Epoch 38/131.. Train loss: 291.496.. Val loss: 56.436.. Train L1 norm: 2.335.. Val L1 norm: 1.090.. Train Linf norm: 1330.477.. Val Linf norm: 72.849\n",
            "Epoch 39/131.. Train loss: 2078.854.. Val loss: 56.453.. Train L1 norm: 4.240.. Val L1 norm: 1.090.. Train Linf norm: 3282.510.. Val Linf norm: 72.489\n",
            "Epoch 40/131.. Train loss: 831.992.. Val loss: 56.458.. Train L1 norm: 1.458.. Val L1 norm: 1.090.. Train Linf norm: 438.473.. Val Linf norm: 72.374\n",
            "Epoch 41/131.. Train loss: 438.404.. Val loss: 56.466.. Train L1 norm: 4.811.. Val L1 norm: 1.090.. Train Linf norm: 3868.299.. Val Linf norm: 72.199\n",
            "Epoch 42/131.. Train loss: 1219.554.. Val loss: 56.474.. Train L1 norm: 3.183.. Val L1 norm: 1.089.. Train Linf norm: 2200.921.. Val Linf norm: 72.025\n",
            "Epoch 43/131.. Train loss: 1325.427.. Val loss: 56.482.. Train L1 norm: 5.062.. Val L1 norm: 1.089.. Train Linf norm: 4124.420.. Val Linf norm: 71.871\n",
            "Epoch 44/131.. Train loss: 241.826.. Val loss: 56.484.. Train L1 norm: 6.054.. Val L1 norm: 1.089.. Train Linf norm: 5137.643.. Val Linf norm: 71.836\n",
            "Epoch 45/131.. Train loss: 1432.530.. Val loss: 56.491.. Train L1 norm: 1.172.. Val L1 norm: 1.089.. Train Linf norm: 141.508.. Val Linf norm: 71.660\n",
            "Epoch 46/131.. Train loss: 696.508.. Val loss: 56.483.. Train L1 norm: 3.457.. Val L1 norm: 1.089.. Train Linf norm: 2478.532.. Val Linf norm: 71.831\n",
            "Epoch 47/131.. Train loss: 1386.781.. Val loss: 56.476.. Train L1 norm: 3.738.. Val L1 norm: 1.089.. Train Linf norm: 2769.574.. Val Linf norm: 71.990\n",
            "Epoch 48/131.. Train loss: 1898.485.. Val loss: 56.478.. Train L1 norm: 5.125.. Val L1 norm: 1.089.. Train Linf norm: 4183.180.. Val Linf norm: 71.951\n",
            "Epoch 49/131.. Train loss: 839.277.. Val loss: 56.482.. Train L1 norm: 5.114.. Val L1 norm: 1.089.. Train Linf norm: 4176.025.. Val Linf norm: 71.859\n",
            "Epoch 50/131.. Train loss: 391.607.. Val loss: 56.477.. Train L1 norm: 1.576.. Val L1 norm: 1.089.. Train Linf norm: 558.863.. Val Linf norm: 71.984\n",
            "Epoch 51/131.. Train loss: 294.937.. Val loss: 56.478.. Train L1 norm: 3.119.. Val L1 norm: 1.089.. Train Linf norm: 2134.372.. Val Linf norm: 71.944\n",
            "Epoch 52/131.. Train loss: 391.078.. Val loss: 56.478.. Train L1 norm: 1.603.. Val L1 norm: 1.089.. Train Linf norm: 583.275.. Val Linf norm: 71.952\n",
            "Epoch 53/131.. Train loss: 1481.663.. Val loss: 56.464.. Train L1 norm: 4.049.. Val L1 norm: 1.090.. Train Linf norm: 3088.026.. Val Linf norm: 72.245\n",
            "Epoch 54/131.. Train loss: 1597.361.. Val loss: 56.455.. Train L1 norm: 1.631.. Val L1 norm: 1.090.. Train Linf norm: 613.037.. Val Linf norm: 72.449\n",
            "Epoch 55/131.. Train loss: 171.472.. Val loss: 56.452.. Train L1 norm: 1.893.. Val L1 norm: 1.090.. Train Linf norm: 879.097.. Val Linf norm: 72.515\n",
            "Epoch 56/131.. Train loss: 744.186.. Val loss: 56.458.. Train L1 norm: 4.437.. Val L1 norm: 1.090.. Train Linf norm: 3486.550.. Val Linf norm: 72.371\n",
            "Epoch 57/131.. Train loss: 83.005.. Val loss: 56.458.. Train L1 norm: 5.973.. Val L1 norm: 1.090.. Train Linf norm: 5060.645.. Val Linf norm: 72.385\n",
            "Epoch 58/131.. Train loss: 96.929.. Val loss: 56.458.. Train L1 norm: 4.947.. Val L1 norm: 1.090.. Train Linf norm: 4000.683.. Val Linf norm: 72.369\n",
            "Epoch 59/131.. Train loss: 61.125.. Val loss: 56.458.. Train L1 norm: 3.359.. Val L1 norm: 1.090.. Train Linf norm: 2377.511.. Val Linf norm: 72.370\n",
            "Epoch 60/131.. Train loss: 1277.191.. Val loss: 56.455.. Train L1 norm: 4.484.. Val L1 norm: 1.090.. Train Linf norm: 3523.180.. Val Linf norm: 72.444\n",
            "Epoch 61/131.. Train loss: 904.771.. Val loss: 56.440.. Train L1 norm: 1.367.. Val L1 norm: 1.090.. Train Linf norm: 342.350.. Val Linf norm: 72.771\n",
            "Epoch 62/131.. Train loss: 203.510.. Val loss: 56.442.. Train L1 norm: 1.347.. Val L1 norm: 1.090.. Train Linf norm: 317.696.. Val Linf norm: 72.725\n",
            "Epoch 63/131.. Train loss: 151.946.. Val loss: 56.444.. Train L1 norm: 2.801.. Val L1 norm: 1.090.. Train Linf norm: 1809.940.. Val Linf norm: 72.680\n",
            "Epoch 64/131.. Train loss: 96.453.. Val loss: 56.443.. Train L1 norm: 3.079.. Val L1 norm: 1.090.. Train Linf norm: 2096.629.. Val Linf norm: 72.698\n",
            "Epoch 65/131.. Train loss: 1250.663.. Val loss: 56.453.. Train L1 norm: 3.169.. Val L1 norm: 1.090.. Train Linf norm: 2179.170.. Val Linf norm: 72.489\n",
            "Epoch 66/131.. Train loss: 82.824.. Val loss: 56.452.. Train L1 norm: 3.745.. Val L1 norm: 1.090.. Train Linf norm: 2776.313.. Val Linf norm: 72.506\n",
            "Epoch 67/131.. Train loss: 667.362.. Val loss: 56.454.. Train L1 norm: 3.501.. Val L1 norm: 1.090.. Train Linf norm: 2524.380.. Val Linf norm: 72.452\n",
            "Epoch 68/131.. Train loss: 1336.792.. Val loss: 56.469.. Train L1 norm: 5.236.. Val L1 norm: 1.090.. Train Linf norm: 4304.124.. Val Linf norm: 72.125\n",
            "Epoch 69/131.. Train loss: 1078.023.. Val loss: 56.475.. Train L1 norm: 6.201.. Val L1 norm: 1.089.. Train Linf norm: 5293.707.. Val Linf norm: 72.007\n",
            "Epoch 70/131.. Train loss: 291.718.. Val loss: 56.484.. Train L1 norm: 1.778.. Val L1 norm: 1.089.. Train Linf norm: 765.167.. Val Linf norm: 71.806\n",
            "Epoch 71/131.. Train loss: 356.149.. Val loss: 56.480.. Train L1 norm: 1.571.. Val L1 norm: 1.089.. Train Linf norm: 550.713.. Val Linf norm: 71.894\n",
            "Epoch 72/131.. Train loss: 563.941.. Val loss: 56.485.. Train L1 norm: 2.423.. Val L1 norm: 1.089.. Train Linf norm: 1422.117.. Val Linf norm: 71.787\n",
            "Epoch 73/131.. Train loss: 178.319.. Val loss: 56.483.. Train L1 norm: 5.320.. Val L1 norm: 1.089.. Train Linf norm: 4385.788.. Val Linf norm: 71.849\n",
            "Epoch 74/131.. Train loss: 1011.551.. Val loss: 56.473.. Train L1 norm: 4.054.. Val L1 norm: 1.089.. Train Linf norm: 3092.249.. Val Linf norm: 72.051\n",
            "Epoch 75/131.. Train loss: 2144.503.. Val loss: 56.487.. Train L1 norm: 3.876.. Val L1 norm: 1.089.. Train Linf norm: 2912.112.. Val Linf norm: 71.771\n",
            "Epoch 76/131.. Train loss: 75.073.. Val loss: 56.486.. Train L1 norm: 2.122.. Val L1 norm: 1.089.. Train Linf norm: 1114.827.. Val Linf norm: 71.785\n",
            "Epoch 77/131.. Train loss: 995.530.. Val loss: 56.495.. Train L1 norm: 2.378.. Val L1 norm: 1.089.. Train Linf norm: 1375.502.. Val Linf norm: 71.593\n",
            "Epoch 78/131.. Train loss: 139.498.. Val loss: 56.493.. Train L1 norm: 4.093.. Val L1 norm: 1.089.. Train Linf norm: 3132.657.. Val Linf norm: 71.634\n",
            "Epoch 79/131.. Train loss: 61.619.. Val loss: 56.493.. Train L1 norm: 4.168.. Val L1 norm: 1.089.. Train Linf norm: 3214.669.. Val Linf norm: 71.637\n",
            "Epoch 80/131.. Train loss: 94.197.. Val loss: 56.492.. Train L1 norm: 2.955.. Val L1 norm: 1.089.. Train Linf norm: 1970.415.. Val Linf norm: 71.664\n",
            "Epoch 81/131.. Train loss: 1040.347.. Val loss: 56.501.. Train L1 norm: 1.296.. Val L1 norm: 1.089.. Train Linf norm: 267.365.. Val Linf norm: 71.458\n",
            "Epoch 82/131.. Train loss: 62.802.. Val loss: 56.501.. Train L1 norm: 6.367.. Val L1 norm: 1.089.. Train Linf norm: 5463.341.. Val Linf norm: 71.451\n",
            "Epoch 83/131.. Train loss: 75.124.. Val loss: 56.501.. Train L1 norm: 3.231.. Val L1 norm: 1.089.. Train Linf norm: 2254.969.. Val Linf norm: 71.466\n",
            "Epoch 84/131.. Train loss: 88.199.. Val loss: 56.501.. Train L1 norm: 2.281.. Val L1 norm: 1.089.. Train Linf norm: 1280.461.. Val Linf norm: 71.457\n",
            "Epoch 85/131.. Train loss: 859.459.. Val loss: 56.506.. Train L1 norm: 3.748.. Val L1 norm: 1.089.. Train Linf norm: 2780.910.. Val Linf norm: 71.349\n",
            "Epoch 86/131.. Train loss: 497.124.. Val loss: 56.516.. Train L1 norm: 2.328.. Val L1 norm: 1.088.. Train Linf norm: 1326.555.. Val Linf norm: 71.128\n",
            "Epoch 87/131.. Train loss: 322.348.. Val loss: 56.514.. Train L1 norm: 2.037.. Val L1 norm: 1.088.. Train Linf norm: 1027.700.. Val Linf norm: 71.186\n",
            "Epoch 88/131.. Train loss: 1000.219.. Val loss: 56.524.. Train L1 norm: 1.796.. Val L1 norm: 1.088.. Train Linf norm: 774.407.. Val Linf norm: 70.960\n",
            "Epoch 89/131.. Train loss: 165.238.. Val loss: 56.528.. Train L1 norm: 1.592.. Val L1 norm: 1.088.. Train Linf norm: 545.685.. Val Linf norm: 70.885\n",
            "Epoch 90/131.. Train loss: 1855.245.. Val loss: 56.516.. Train L1 norm: 2.955.. Val L1 norm: 1.088.. Train Linf norm: 1968.848.. Val Linf norm: 71.141\n",
            "Epoch 91/131.. Train loss: 392.766.. Val loss: 56.511.. Train L1 norm: 1.578.. Val L1 norm: 1.088.. Train Linf norm: 550.211.. Val Linf norm: 71.251\n",
            "Epoch 92/131.. Train loss: 714.096.. Val loss: 56.515.. Train L1 norm: 3.326.. Val L1 norm: 1.088.. Train Linf norm: 2343.689.. Val Linf norm: 71.175\n",
            "Epoch 93/131.. Train loss: 62.735.. Val loss: 56.515.. Train L1 norm: 1.187.. Val L1 norm: 1.088.. Train Linf norm: 154.978.. Val Linf norm: 71.178\n",
            "Epoch 94/131.. Train loss: 1308.750.. Val loss: 56.502.. Train L1 norm: 4.036.. Val L1 norm: 1.089.. Train Linf norm: 3076.279.. Val Linf norm: 71.448\n",
            "Epoch 95/131.. Train loss: 1495.485.. Val loss: 56.509.. Train L1 norm: 5.792.. Val L1 norm: 1.088.. Train Linf norm: 4874.489.. Val Linf norm: 71.296\n",
            "Epoch 96/131.. Train loss: 1732.426.. Val loss: 56.508.. Train L1 norm: 5.515.. Val L1 norm: 1.089.. Train Linf norm: 4590.936.. Val Linf norm: 71.330\n",
            "Epoch 97/131.. Train loss: 2390.355.. Val loss: 56.486.. Train L1 norm: 2.584.. Val L1 norm: 1.089.. Train Linf norm: 1586.003.. Val Linf norm: 71.791\n",
            "Epoch 98/131.. Train loss: 229.482.. Val loss: 56.489.. Train L1 norm: 1.308.. Val L1 norm: 1.089.. Train Linf norm: 280.652.. Val Linf norm: 71.730\n",
            "Epoch 99/131.. Train loss: 691.521.. Val loss: 56.496.. Train L1 norm: 1.315.. Val L1 norm: 1.089.. Train Linf norm: 272.049.. Val Linf norm: 71.558\n",
            "Epoch 100/131.. Train loss: 235.643.. Val loss: 56.499.. Train L1 norm: 4.027.. Val L1 norm: 1.089.. Train Linf norm: 3067.156.. Val Linf norm: 71.492\n",
            "Epoch 101/131.. Train loss: 646.629.. Val loss: 56.493.. Train L1 norm: 2.133.. Val L1 norm: 1.089.. Train Linf norm: 1119.882.. Val Linf norm: 71.634\n",
            "Epoch 102/131.. Train loss: 79.933.. Val loss: 56.492.. Train L1 norm: 6.955.. Val L1 norm: 1.089.. Train Linf norm: 6063.473.. Val Linf norm: 71.651\n",
            "Epoch 103/131.. Train loss: 1342.125.. Val loss: 56.503.. Train L1 norm: 4.406.. Val L1 norm: 1.089.. Train Linf norm: 3455.199.. Val Linf norm: 71.424\n",
            "Epoch 104/131.. Train loss: 3159.312.. Val loss: 56.495.. Train L1 norm: 1.538.. Val L1 norm: 1.089.. Train Linf norm: 516.732.. Val Linf norm: 71.587\n",
            "Epoch 105/131.. Train loss: 2003.396.. Val loss: 56.499.. Train L1 norm: 2.502.. Val L1 norm: 1.089.. Train Linf norm: 1507.183.. Val Linf norm: 71.507\n",
            "Epoch 106/131.. Train loss: 468.846.. Val loss: 56.503.. Train L1 norm: 3.969.. Val L1 norm: 1.089.. Train Linf norm: 3006.809.. Val Linf norm: 71.411\n",
            "Epoch 107/131.. Train loss: 68.601.. Val loss: 56.504.. Train L1 norm: 1.274.. Val L1 norm: 1.089.. Train Linf norm: 241.208.. Val Linf norm: 71.400\n",
            "Epoch 108/131.. Train loss: 302.433.. Val loss: 56.507.. Train L1 norm: 2.852.. Val L1 norm: 1.089.. Train Linf norm: 1861.306.. Val Linf norm: 71.324\n",
            "Epoch 109/131.. Train loss: 67.652.. Val loss: 56.507.. Train L1 norm: 6.643.. Val L1 norm: 1.089.. Train Linf norm: 5746.060.. Val Linf norm: 71.323\n",
            "Epoch 110/131.. Train loss: 60.723.. Val loss: 56.507.. Train L1 norm: 4.030.. Val L1 norm: 1.089.. Train Linf norm: 3071.565.. Val Linf norm: 71.325\n",
            "Epoch 111/131.. Train loss: 185.068.. Val loss: 56.504.. Train L1 norm: 5.368.. Val L1 norm: 1.089.. Train Linf norm: 4439.208.. Val Linf norm: 71.392\n",
            "Epoch 112/131.. Train loss: 247.715.. Val loss: 56.506.. Train L1 norm: 3.380.. Val L1 norm: 1.089.. Train Linf norm: 2401.038.. Val Linf norm: 71.368\n",
            "Epoch 113/131.. Train loss: 1380.775.. Val loss: 56.521.. Train L1 norm: 2.413.. Val L1 norm: 1.088.. Train Linf norm: 1411.071.. Val Linf norm: 71.029\n",
            "Epoch 114/131.. Train loss: 150.851.. Val loss: 56.523.. Train L1 norm: 1.445.. Val L1 norm: 1.088.. Train Linf norm: 419.171.. Val Linf norm: 70.984\n",
            "Epoch 115/131.. Train loss: 653.650.. Val loss: 56.516.. Train L1 norm: 2.866.. Val L1 norm: 1.088.. Train Linf norm: 1876.164.. Val Linf norm: 71.125\n",
            "Epoch 116/131.. Train loss: 358.109.. Val loss: 56.510.. Train L1 norm: 7.052.. Val L1 norm: 1.088.. Train Linf norm: 6161.595.. Val Linf norm: 71.250\n",
            "Epoch 117/131.. Train loss: 119.395.. Val loss: 56.511.. Train L1 norm: 4.932.. Val L1 norm: 1.088.. Train Linf norm: 3991.394.. Val Linf norm: 71.226\n",
            "Epoch 118/131.. Train loss: 61.559.. Val loss: 56.512.. Train L1 norm: 3.234.. Val L1 norm: 1.088.. Train Linf norm: 2244.787.. Val Linf norm: 71.216\n",
            "Epoch 119/131.. Train loss: 1432.234.. Val loss: 56.523.. Train L1 norm: 3.474.. Val L1 norm: 1.088.. Train Linf norm: 2500.573.. Val Linf norm: 70.966\n",
            "Epoch 120/131.. Train loss: 885.596.. Val loss: 56.515.. Train L1 norm: 5.502.. Val L1 norm: 1.088.. Train Linf norm: 4578.918.. Val Linf norm: 71.136\n",
            "Epoch 121/131.. Train loss: 181.338.. Val loss: 56.518.. Train L1 norm: 7.570.. Val L1 norm: 1.088.. Train Linf norm: 6691.116.. Val Linf norm: 71.078\n",
            "Epoch 122/131.. Train loss: 201.525.. Val loss: 56.521.. Train L1 norm: 3.862.. Val L1 norm: 1.088.. Train Linf norm: 2898.638.. Val Linf norm: 71.009\n",
            "Epoch 123/131.. Train loss: 714.151.. Val loss: 56.512.. Train L1 norm: 1.785.. Val L1 norm: 1.088.. Train Linf norm: 771.205.. Val Linf norm: 71.196\n",
            "Epoch 124/131.. Train loss: 2783.170.. Val loss: 56.496.. Train L1 norm: 4.301.. Val L1 norm: 1.089.. Train Linf norm: 3347.892.. Val Linf norm: 71.522\n",
            "Epoch 125/131.. Train loss: 60.882.. Val loss: 56.496.. Train L1 norm: 1.694.. Val L1 norm: 1.089.. Train Linf norm: 673.501.. Val Linf norm: 71.532\n",
            "Epoch 126/131.. Train loss: 436.027.. Val loss: 56.490.. Train L1 norm: 2.288.. Val L1 norm: 1.089.. Train Linf norm: 1285.194.. Val Linf norm: 71.650\n",
            "Epoch 127/131.. Train loss: 956.311.. Val loss: 56.488.. Train L1 norm: 2.122.. Val L1 norm: 1.089.. Train Linf norm: 1114.224.. Val Linf norm: 71.706\n",
            "Epoch 128/131.. Train loss: 169.673.. Val loss: 56.477.. Train L1 norm: 5.573.. Val L1 norm: 1.089.. Train Linf norm: 4649.243.. Val Linf norm: 71.951\n",
            "Epoch 129/131.. Train loss: 432.122.. Val loss: 56.482.. Train L1 norm: 1.518.. Val L1 norm: 1.089.. Train Linf norm: 490.807.. Val Linf norm: 71.844\n",
            "Epoch 130/131.. Train loss: 70.003.. Val loss: 56.481.. Train L1 norm: 3.113.. Val L1 norm: 1.089.. Train Linf norm: 2130.258.. Val Linf norm: 71.854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:16:22,281]\u001b[0m Trial 89 finished with value: 1.0892189060529074 and parameters: {'n_layers': 5, 'n_units_0': 3825, 'n_units_1': 844, 'n_units_2': 3474, 'n_units_3': 983, 'n_units_4': 679, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 1.514471871886847e-06, 'batch_size': 1024, 'n_epochs': 131, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.2340275592549041, 'dropout_rate': 0.0676401502582846, 'weight_decay': 0.0006439102249318283, 'beta1': 0.9211579380267618, 'beta2': 0.9990453224246237, 'factor': 0.1247135584782998, 'patience': 6, 'threshold': 0.0019123476162442637}. Best is trial 22 with value: 1.0150012904485066.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 131/131.. Train loss: 94.300.. Val loss: 56.480.. Train L1 norm: 6.359.. Val L1 norm: 1.089.. Train Linf norm: 5455.242.. Val Linf norm: 71.881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:16:26,707]\u001b[0m Trial 90 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/137.. Train loss: 7991.072.. Val loss: 53.578.. Train L1 norm: 5.491.. Val L1 norm: 1.089.. Train Linf norm: 4548.342.. Val Linf norm: 69.264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:16:31,899]\u001b[0m Trial 91 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/75.. Train loss: 2572.030.. Val loss: 39.713.. Train L1 norm: 70.708.. Val L1 norm: 5.020.. Train Linf norm: 17463.585.. Val Linf norm: 643.101\n",
            "Epoch 1/143.. Train loss: 187.695.. Val loss: 53.972.. Train L1 norm: 1.369.. Val L1 norm: 1.038.. Train Linf norm: 360.177.. Val Linf norm: 26.248\n",
            "Epoch 2/143.. Train loss: 185.809.. Val loss: 54.603.. Train L1 norm: 2.598.. Val L1 norm: 1.013.. Train Linf norm: 1629.837.. Val Linf norm: 9.328\n",
            "Epoch 3/143.. Train loss: 271.572.. Val loss: 55.569.. Train L1 norm: 1.387.. Val L1 norm: 1.025.. Train Linf norm: 387.048.. Val Linf norm: 22.557\n",
            "Epoch 4/143.. Train loss: 100.102.. Val loss: 56.034.. Train L1 norm: 3.135.. Val L1 norm: 1.039.. Train Linf norm: 2171.246.. Val Linf norm: 32.770\n",
            "Epoch 5/143.. Train loss: 417.999.. Val loss: 55.201.. Train L1 norm: 1.792.. Val L1 norm: 1.016.. Train Linf norm: 799.232.. Val Linf norm: 14.202\n",
            "Epoch 6/143.. Train loss: 508.418.. Val loss: 54.747.. Train L1 norm: 1.736.. Val L1 norm: 1.012.. Train Linf norm: 740.240.. Val Linf norm: 9.247\n",
            "Epoch 7/143.. Train loss: 754.224.. Val loss: 54.730.. Train L1 norm: 1.604.. Val L1 norm: 1.012.. Train Linf norm: 613.818.. Val Linf norm: 8.863\n",
            "Epoch 8/143.. Train loss: 128.590.. Val loss: 54.826.. Train L1 norm: 1.581.. Val L1 norm: 1.010.. Train Linf norm: 584.978.. Val Linf norm: 7.566\n",
            "Epoch 9/143.. Train loss: 73.267.. Val loss: 54.817.. Train L1 norm: 1.859.. Val L1 norm: 1.010.. Train Linf norm: 873.272.. Val Linf norm: 7.663\n",
            "Epoch 10/143.. Train loss: 216.624.. Val loss: 54.710.. Train L1 norm: 2.958.. Val L1 norm: 1.013.. Train Linf norm: 1998.322.. Val Linf norm: 10.073\n",
            "Epoch 11/143.. Train loss: 105.618.. Val loss: 54.682.. Train L1 norm: 1.507.. Val L1 norm: 1.014.. Train Linf norm: 511.143.. Val Linf norm: 10.852\n",
            "Epoch 12/143.. Train loss: 282.994.. Val loss: 54.685.. Train L1 norm: 2.267.. Val L1 norm: 1.014.. Train Linf norm: 1277.650.. Val Linf norm: 10.837\n",
            "Epoch 13/143.. Train loss: 95.672.. Val loss: 54.768.. Train L1 norm: 1.747.. Val L1 norm: 1.012.. Train Linf norm: 758.515.. Val Linf norm: 8.863\n",
            "Epoch 14/143.. Train loss: 176.939.. Val loss: 54.850.. Train L1 norm: 1.255.. Val L1 norm: 1.010.. Train Linf norm: 254.387.. Val Linf norm: 7.407\n",
            "Epoch 15/143.. Train loss: 78.123.. Val loss: 54.868.. Train L1 norm: 1.200.. Val L1 norm: 1.010.. Train Linf norm: 200.340.. Val Linf norm: 7.319\n",
            "Epoch 16/143.. Train loss: 235.931.. Val loss: 54.861.. Train L1 norm: 2.065.. Val L1 norm: 1.010.. Train Linf norm: 1082.253.. Val Linf norm: 7.314\n",
            "Epoch 17/143.. Train loss: 104.783.. Val loss: 54.866.. Train L1 norm: 1.457.. Val L1 norm: 1.010.. Train Linf norm: 463.110.. Val Linf norm: 7.290\n",
            "Epoch 18/143.. Train loss: 88.927.. Val loss: 54.863.. Train L1 norm: 2.531.. Val L1 norm: 1.010.. Train Linf norm: 1562.206.. Val Linf norm: 7.294\n",
            "Epoch 19/143.. Train loss: 64.993.. Val loss: 54.863.. Train L1 norm: 1.291.. Val L1 norm: 1.010.. Train Linf norm: 290.780.. Val Linf norm: 7.291\n",
            "Epoch 20/143.. Train loss: 81.830.. Val loss: 54.866.. Train L1 norm: 1.250.. Val L1 norm: 1.010.. Train Linf norm: 249.836.. Val Linf norm: 7.272\n",
            "Epoch 21/143.. Train loss: 292.169.. Val loss: 54.862.. Train L1 norm: 1.900.. Val L1 norm: 1.010.. Train Linf norm: 915.546.. Val Linf norm: 7.304\n",
            "Epoch 22/143.. Train loss: 64.947.. Val loss: 54.859.. Train L1 norm: 3.338.. Val L1 norm: 1.010.. Train Linf norm: 2387.093.. Val Linf norm: 7.342\n",
            "Epoch 23/143.. Train loss: 590.845.. Val loss: 54.858.. Train L1 norm: 1.160.. Val L1 norm: 1.010.. Train Linf norm: 156.607.. Val Linf norm: 7.352\n",
            "Epoch 24/143.. Train loss: 314.335.. Val loss: 54.857.. Train L1 norm: 1.428.. Val L1 norm: 1.010.. Train Linf norm: 429.622.. Val Linf norm: 7.353\n",
            "Epoch 25/143.. Train loss: 101.739.. Val loss: 54.856.. Train L1 norm: 1.615.. Val L1 norm: 1.010.. Train Linf norm: 622.042.. Val Linf norm: 7.368\n",
            "Epoch 26/143.. Train loss: 627.250.. Val loss: 54.855.. Train L1 norm: 2.239.. Val L1 norm: 1.010.. Train Linf norm: 1263.000.. Val Linf norm: 7.374\n",
            "Epoch 27/143.. Train loss: 166.701.. Val loss: 54.855.. Train L1 norm: 2.440.. Val L1 norm: 1.010.. Train Linf norm: 1469.588.. Val Linf norm: 7.372\n",
            "Epoch 28/143.. Train loss: 82.197.. Val loss: 54.855.. Train L1 norm: 1.281.. Val L1 norm: 1.010.. Train Linf norm: 276.378.. Val Linf norm: 7.373\n",
            "Epoch 29/143.. Train loss: 61.295.. Val loss: 54.855.. Train L1 norm: 3.094.. Val L1 norm: 1.010.. Train Linf norm: 2140.170.. Val Linf norm: 7.374\n",
            "Epoch 30/143.. Train loss: 904.435.. Val loss: 54.856.. Train L1 norm: 2.131.. Val L1 norm: 1.010.. Train Linf norm: 1152.382.. Val Linf norm: 7.367\n",
            "Epoch 31/143.. Train loss: 200.079.. Val loss: 54.856.. Train L1 norm: 1.260.. Val L1 norm: 1.010.. Train Linf norm: 260.742.. Val Linf norm: 7.362\n",
            "Epoch 32/143.. Train loss: 337.822.. Val loss: 54.856.. Train L1 norm: 1.319.. Val L1 norm: 1.010.. Train Linf norm: 319.470.. Val Linf norm: 7.367\n",
            "Epoch 33/143.. Train loss: 147.117.. Val loss: 54.856.. Train L1 norm: 1.684.. Val L1 norm: 1.010.. Train Linf norm: 694.986.. Val Linf norm: 7.364\n",
            "Epoch 34/143.. Train loss: 259.385.. Val loss: 54.857.. Train L1 norm: 1.221.. Val L1 norm: 1.010.. Train Linf norm: 219.632.. Val Linf norm: 7.360\n",
            "Epoch 35/143.. Train loss: 423.199.. Val loss: 54.858.. Train L1 norm: 1.244.. Val L1 norm: 1.010.. Train Linf norm: 243.645.. Val Linf norm: 7.352\n",
            "Epoch 36/143.. Train loss: 127.781.. Val loss: 54.858.. Train L1 norm: 1.272.. Val L1 norm: 1.010.. Train Linf norm: 273.664.. Val Linf norm: 7.350\n",
            "Epoch 37/143.. Train loss: 354.709.. Val loss: 54.859.. Train L1 norm: 1.760.. Val L1 norm: 1.010.. Train Linf norm: 772.690.. Val Linf norm: 7.342\n",
            "Epoch 38/143.. Train loss: 147.380.. Val loss: 54.859.. Train L1 norm: 2.030.. Val L1 norm: 1.010.. Train Linf norm: 1047.991.. Val Linf norm: 7.340\n",
            "Epoch 39/143.. Train loss: 248.007.. Val loss: 54.858.. Train L1 norm: 2.582.. Val L1 norm: 1.010.. Train Linf norm: 1614.333.. Val Linf norm: 7.344\n",
            "Epoch 40/143.. Train loss: 85.152.. Val loss: 54.858.. Train L1 norm: 2.266.. Val L1 norm: 1.010.. Train Linf norm: 1290.286.. Val Linf norm: 7.343\n",
            "Epoch 41/143.. Train loss: 665.408.. Val loss: 54.859.. Train L1 norm: 3.382.. Val L1 norm: 1.010.. Train Linf norm: 2431.923.. Val Linf norm: 7.336\n",
            "Epoch 42/143.. Train loss: 173.595.. Val loss: 54.859.. Train L1 norm: 1.361.. Val L1 norm: 1.010.. Train Linf norm: 362.637.. Val Linf norm: 7.337\n",
            "Epoch 43/143.. Train loss: 148.365.. Val loss: 54.859.. Train L1 norm: 1.332.. Val L1 norm: 1.010.. Train Linf norm: 334.257.. Val Linf norm: 7.341\n",
            "Epoch 44/143.. Train loss: 730.106.. Val loss: 54.859.. Train L1 norm: 1.238.. Val L1 norm: 1.010.. Train Linf norm: 236.579.. Val Linf norm: 7.336\n",
            "Epoch 45/143.. Train loss: 74.071.. Val loss: 54.860.. Train L1 norm: 1.147.. Val L1 norm: 1.010.. Train Linf norm: 143.574.. Val Linf norm: 7.331\n",
            "Epoch 46/143.. Train loss: 87.088.. Val loss: 54.860.. Train L1 norm: 3.440.. Val L1 norm: 1.010.. Train Linf norm: 2492.495.. Val Linf norm: 7.330\n",
            "Epoch 47/143.. Train loss: 164.313.. Val loss: 54.860.. Train L1 norm: 1.342.. Val L1 norm: 1.010.. Train Linf norm: 345.013.. Val Linf norm: 7.327\n",
            "Epoch 48/143.. Train loss: 63.453.. Val loss: 54.860.. Train L1 norm: 3.382.. Val L1 norm: 1.010.. Train Linf norm: 2433.153.. Val Linf norm: 7.327\n",
            "Epoch 49/143.. Train loss: 65.827.. Val loss: 54.860.. Train L1 norm: 1.977.. Val L1 norm: 1.010.. Train Linf norm: 993.831.. Val Linf norm: 7.328\n",
            "Epoch 50/143.. Train loss: 217.693.. Val loss: 54.860.. Train L1 norm: 1.560.. Val L1 norm: 1.010.. Train Linf norm: 568.316.. Val Linf norm: 7.332\n",
            "Epoch 51/143.. Train loss: 91.943.. Val loss: 54.860.. Train L1 norm: 1.111.. Val L1 norm: 1.010.. Train Linf norm: 108.241.. Val Linf norm: 7.331\n",
            "Epoch 52/143.. Train loss: 94.330.. Val loss: 54.860.. Train L1 norm: 2.028.. Val L1 norm: 1.010.. Train Linf norm: 1045.397.. Val Linf norm: 7.330\n",
            "Epoch 53/143.. Train loss: 61.947.. Val loss: 54.860.. Train L1 norm: 2.812.. Val L1 norm: 1.010.. Train Linf norm: 1849.701.. Val Linf norm: 7.330\n",
            "Epoch 54/143.. Train loss: 63.346.. Val loss: 54.860.. Train L1 norm: 1.249.. Val L1 norm: 1.010.. Train Linf norm: 248.013.. Val Linf norm: 7.330\n",
            "Epoch 55/143.. Train loss: 166.885.. Val loss: 54.860.. Train L1 norm: 2.076.. Val L1 norm: 1.010.. Train Linf norm: 1093.690.. Val Linf norm: 7.327\n",
            "Epoch 56/143.. Train loss: 174.888.. Val loss: 54.861.. Train L1 norm: 2.562.. Val L1 norm: 1.010.. Train Linf norm: 1593.865.. Val Linf norm: 7.322\n",
            "Epoch 57/143.. Train loss: 60.772.. Val loss: 54.861.. Train L1 norm: 2.356.. Val L1 norm: 1.010.. Train Linf norm: 1383.322.. Val Linf norm: 7.322\n",
            "Epoch 58/143.. Train loss: 118.597.. Val loss: 54.861.. Train L1 norm: 2.039.. Val L1 norm: 1.010.. Train Linf norm: 1058.072.. Val Linf norm: 7.325\n",
            "Epoch 59/143.. Train loss: 67.059.. Val loss: 54.861.. Train L1 norm: 3.046.. Val L1 norm: 1.010.. Train Linf norm: 2088.672.. Val Linf norm: 7.326\n",
            "Epoch 60/143.. Train loss: 189.526.. Val loss: 54.860.. Train L1 norm: 1.534.. Val L1 norm: 1.010.. Train Linf norm: 541.182.. Val Linf norm: 7.331\n",
            "Epoch 61/143.. Train loss: 211.793.. Val loss: 54.861.. Train L1 norm: 1.620.. Val L1 norm: 1.010.. Train Linf norm: 628.932.. Val Linf norm: 7.326\n",
            "Epoch 62/143.. Train loss: 73.737.. Val loss: 54.861.. Train L1 norm: 2.306.. Val L1 norm: 1.010.. Train Linf norm: 1329.892.. Val Linf norm: 7.324\n",
            "Epoch 63/143.. Train loss: 100.379.. Val loss: 54.861.. Train L1 norm: 1.231.. Val L1 norm: 1.010.. Train Linf norm: 231.170.. Val Linf norm: 7.322\n",
            "Epoch 64/143.. Train loss: 151.853.. Val loss: 54.861.. Train L1 norm: 2.076.. Val L1 norm: 1.010.. Train Linf norm: 1095.894.. Val Linf norm: 7.325\n",
            "Epoch 65/143.. Train loss: 226.703.. Val loss: 54.861.. Train L1 norm: 1.227.. Val L1 norm: 1.010.. Train Linf norm: 227.107.. Val Linf norm: 7.318\n",
            "Epoch 66/143.. Train loss: 79.508.. Val loss: 54.861.. Train L1 norm: 2.185.. Val L1 norm: 1.010.. Train Linf norm: 1207.113.. Val Linf norm: 7.319\n",
            "Epoch 67/143.. Train loss: 60.610.. Val loss: 54.861.. Train L1 norm: 2.726.. Val L1 norm: 1.010.. Train Linf norm: 1761.081.. Val Linf norm: 7.320\n",
            "Epoch 68/143.. Train loss: 136.576.. Val loss: 54.861.. Train L1 norm: 1.359.. Val L1 norm: 1.010.. Train Linf norm: 147.287.. Val Linf norm: 7.317\n",
            "Epoch 69/143.. Train loss: 68.480.. Val loss: 54.862.. Train L1 norm: 1.142.. Val L1 norm: 1.010.. Train Linf norm: 139.443.. Val Linf norm: 7.317\n",
            "Epoch 70/143.. Train loss: 390.867.. Val loss: 54.861.. Train L1 norm: 1.509.. Val L1 norm: 1.010.. Train Linf norm: 515.312.. Val Linf norm: 7.325\n",
            "Epoch 71/143.. Train loss: 92.887.. Val loss: 54.861.. Train L1 norm: 1.833.. Val L1 norm: 1.010.. Train Linf norm: 846.747.. Val Linf norm: 7.322\n",
            "Epoch 72/143.. Train loss: 150.016.. Val loss: 54.861.. Train L1 norm: 2.073.. Val L1 norm: 1.010.. Train Linf norm: 1092.715.. Val Linf norm: 7.324\n",
            "Epoch 73/143.. Train loss: 106.262.. Val loss: 54.861.. Train L1 norm: 2.748.. Val L1 norm: 1.010.. Train Linf norm: 1782.820.. Val Linf norm: 7.325\n",
            "Epoch 74/143.. Train loss: 68.630.. Val loss: 54.861.. Train L1 norm: 1.826.. Val L1 norm: 1.010.. Train Linf norm: 840.098.. Val Linf norm: 7.324\n",
            "Epoch 75/143.. Train loss: 596.440.. Val loss: 54.859.. Train L1 norm: 1.500.. Val L1 norm: 1.010.. Train Linf norm: 505.019.. Val Linf norm: 7.336\n",
            "Epoch 76/143.. Train loss: 220.555.. Val loss: 54.860.. Train L1 norm: 2.358.. Val L1 norm: 1.010.. Train Linf norm: 1385.432.. Val Linf norm: 7.331\n",
            "Epoch 77/143.. Train loss: 252.766.. Val loss: 54.861.. Train L1 norm: 1.269.. Val L1 norm: 1.010.. Train Linf norm: 268.740.. Val Linf norm: 7.324\n",
            "Epoch 78/143.. Train loss: 136.124.. Val loss: 54.861.. Train L1 norm: 1.922.. Val L1 norm: 1.010.. Train Linf norm: 935.465.. Val Linf norm: 7.319\n",
            "Epoch 79/143.. Train loss: 326.157.. Val loss: 54.862.. Train L1 norm: 1.783.. Val L1 norm: 1.010.. Train Linf norm: 795.921.. Val Linf norm: 7.309\n",
            "Epoch 80/143.. Train loss: 134.320.. Val loss: 54.863.. Train L1 norm: 2.810.. Val L1 norm: 1.010.. Train Linf norm: 1846.160.. Val Linf norm: 7.305\n",
            "Epoch 81/143.. Train loss: 61.105.. Val loss: 54.863.. Train L1 norm: 1.145.. Val L1 norm: 1.010.. Train Linf norm: 142.105.. Val Linf norm: 7.305\n",
            "Epoch 82/143.. Train loss: 103.301.. Val loss: 54.862.. Train L1 norm: 2.750.. Val L1 norm: 1.010.. Train Linf norm: 1786.532.. Val Linf norm: 7.309\n",
            "Epoch 83/143.. Train loss: 458.286.. Val loss: 54.861.. Train L1 norm: 1.688.. Val L1 norm: 1.010.. Train Linf norm: 695.779.. Val Linf norm: 7.319\n",
            "Epoch 84/143.. Train loss: 61.216.. Val loss: 54.861.. Train L1 norm: 2.200.. Val L1 norm: 1.010.. Train Linf norm: 1217.048.. Val Linf norm: 7.319\n",
            "Epoch 85/143.. Train loss: 61.457.. Val loss: 54.861.. Train L1 norm: 1.388.. Val L1 norm: 1.010.. Train Linf norm: 383.004.. Val Linf norm: 7.320\n",
            "Epoch 86/143.. Train loss: 80.565.. Val loss: 54.861.. Train L1 norm: 1.748.. Val L1 norm: 1.010.. Train Linf norm: 759.544.. Val Linf norm: 7.319\n",
            "Epoch 87/143.. Train loss: 76.849.. Val loss: 54.862.. Train L1 norm: 2.889.. Val L1 norm: 1.010.. Train Linf norm: 1926.153.. Val Linf norm: 7.313\n",
            "Epoch 88/143.. Train loss: 82.728.. Val loss: 54.862.. Train L1 norm: 2.853.. Val L1 norm: 1.010.. Train Linf norm: 1891.243.. Val Linf norm: 7.314\n",
            "Epoch 89/143.. Train loss: 331.531.. Val loss: 54.863.. Train L1 norm: 1.179.. Val L1 norm: 1.010.. Train Linf norm: 177.614.. Val Linf norm: 7.305\n",
            "Epoch 90/143.. Train loss: 130.251.. Val loss: 54.862.. Train L1 norm: 2.025.. Val L1 norm: 1.010.. Train Linf norm: 1042.718.. Val Linf norm: 7.308\n",
            "Epoch 91/143.. Train loss: 64.745.. Val loss: 54.862.. Train L1 norm: 1.273.. Val L1 norm: 1.010.. Train Linf norm: 273.991.. Val Linf norm: 7.308\n",
            "Epoch 92/143.. Train loss: 60.703.. Val loss: 54.862.. Train L1 norm: 1.497.. Val L1 norm: 1.010.. Train Linf norm: 504.038.. Val Linf norm: 7.308\n",
            "Epoch 93/143.. Train loss: 62.868.. Val loss: 54.862.. Train L1 norm: 1.669.. Val L1 norm: 1.010.. Train Linf norm: 676.938.. Val Linf norm: 7.308\n",
            "Epoch 94/143.. Train loss: 81.495.. Val loss: 54.862.. Train L1 norm: 1.896.. Val L1 norm: 1.010.. Train Linf norm: 910.192.. Val Linf norm: 7.310\n",
            "Epoch 95/143.. Train loss: 97.157.. Val loss: 54.862.. Train L1 norm: 1.765.. Val L1 norm: 1.010.. Train Linf norm: 776.180.. Val Linf norm: 7.309\n",
            "Epoch 96/143.. Train loss: 941.208.. Val loss: 54.861.. Train L1 norm: 1.652.. Val L1 norm: 1.010.. Train Linf norm: 661.367.. Val Linf norm: 7.320\n",
            "Epoch 97/143.. Train loss: 74.221.. Val loss: 54.861.. Train L1 norm: 2.733.. Val L1 norm: 1.010.. Train Linf norm: 1768.469.. Val Linf norm: 7.323\n",
            "Epoch 98/143.. Train loss: 61.677.. Val loss: 54.861.. Train L1 norm: 1.406.. Val L1 norm: 1.010.. Train Linf norm: 408.221.. Val Linf norm: 7.324\n",
            "Epoch 99/143.. Train loss: 206.729.. Val loss: 54.861.. Train L1 norm: 3.141.. Val L1 norm: 1.010.. Train Linf norm: 2186.905.. Val Linf norm: 7.317\n",
            "Epoch 100/143.. Train loss: 300.041.. Val loss: 54.860.. Train L1 norm: 1.442.. Val L1 norm: 1.010.. Train Linf norm: 447.103.. Val Linf norm: 7.325\n",
            "Epoch 101/143.. Train loss: 82.510.. Val loss: 54.860.. Train L1 norm: 1.439.. Val L1 norm: 1.010.. Train Linf norm: 443.431.. Val Linf norm: 7.330\n",
            "Epoch 102/143.. Train loss: 65.001.. Val loss: 54.860.. Train L1 norm: 1.936.. Val L1 norm: 1.010.. Train Linf norm: 952.396.. Val Linf norm: 7.330\n",
            "Epoch 103/143.. Train loss: 265.825.. Val loss: 54.861.. Train L1 norm: 2.224.. Val L1 norm: 1.010.. Train Linf norm: 1246.488.. Val Linf norm: 7.323\n",
            "Epoch 104/143.. Train loss: 92.341.. Val loss: 54.861.. Train L1 norm: 1.219.. Val L1 norm: 1.010.. Train Linf norm: 217.098.. Val Linf norm: 7.325\n",
            "Epoch 105/143.. Train loss: 62.673.. Val loss: 54.861.. Train L1 norm: 2.284.. Val L1 norm: 1.010.. Train Linf norm: 1308.175.. Val Linf norm: 7.325\n",
            "Epoch 106/143.. Train loss: 171.145.. Val loss: 54.861.. Train L1 norm: 3.181.. Val L1 norm: 1.010.. Train Linf norm: 2226.537.. Val Linf norm: 7.320\n",
            "Epoch 107/143.. Train loss: 80.534.. Val loss: 54.861.. Train L1 norm: 2.472.. Val L1 norm: 1.010.. Train Linf norm: 1500.779.. Val Linf norm: 7.322\n",
            "Epoch 108/143.. Train loss: 98.740.. Val loss: 54.861.. Train L1 norm: 1.465.. Val L1 norm: 1.010.. Train Linf norm: 466.677.. Val Linf norm: 7.324\n",
            "Epoch 109/143.. Train loss: 373.392.. Val loss: 54.861.. Train L1 norm: 1.445.. Val L1 norm: 1.010.. Train Linf norm: 447.762.. Val Linf norm: 7.322\n",
            "Epoch 110/143.. Train loss: 234.478.. Val loss: 54.861.. Train L1 norm: 2.243.. Val L1 norm: 1.010.. Train Linf norm: 1259.132.. Val Linf norm: 7.320\n",
            "Epoch 111/143.. Train loss: 66.097.. Val loss: 54.861.. Train L1 norm: 2.745.. Val L1 norm: 1.010.. Train Linf norm: 1781.592.. Val Linf norm: 7.320\n",
            "Epoch 112/143.. Train loss: 213.007.. Val loss: 54.862.. Train L1 norm: 2.237.. Val L1 norm: 1.010.. Train Linf norm: 1261.165.. Val Linf norm: 7.314\n",
            "Epoch 113/143.. Train loss: 284.202.. Val loss: 54.861.. Train L1 norm: 2.126.. Val L1 norm: 1.010.. Train Linf norm: 1144.552.. Val Linf norm: 7.321\n",
            "Epoch 114/143.. Train loss: 635.947.. Val loss: 54.860.. Train L1 norm: 2.069.. Val L1 norm: 1.010.. Train Linf norm: 1088.532.. Val Linf norm: 7.333\n",
            "Epoch 115/143.. Train loss: 140.874.. Val loss: 54.860.. Train L1 norm: 1.207.. Val L1 norm: 1.010.. Train Linf norm: 205.007.. Val Linf norm: 7.338\n",
            "Epoch 116/143.. Train loss: 71.033.. Val loss: 54.859.. Train L1 norm: 1.334.. Val L1 norm: 1.010.. Train Linf norm: 336.248.. Val Linf norm: 7.339\n",
            "Epoch 117/143.. Train loss: 61.194.. Val loss: 54.859.. Train L1 norm: 3.593.. Val L1 norm: 1.010.. Train Linf norm: 2649.624.. Val Linf norm: 7.339\n",
            "Epoch 118/143.. Train loss: 79.442.. Val loss: 54.860.. Train L1 norm: 1.691.. Val L1 norm: 1.010.. Train Linf norm: 701.568.. Val Linf norm: 7.339\n",
            "Epoch 119/143.. Train loss: 179.676.. Val loss: 54.859.. Train L1 norm: 1.138.. Val L1 norm: 1.010.. Train Linf norm: 127.647.. Val Linf norm: 7.341\n",
            "Epoch 120/143.. Train loss: 97.134.. Val loss: 54.859.. Train L1 norm: 1.813.. Val L1 norm: 1.010.. Train Linf norm: 825.502.. Val Linf norm: 7.347\n",
            "Epoch 121/143.. Train loss: 88.840.. Val loss: 54.859.. Train L1 norm: 2.786.. Val L1 norm: 1.010.. Train Linf norm: 1822.986.. Val Linf norm: 7.348\n",
            "Epoch 122/143.. Train loss: 119.529.. Val loss: 54.859.. Train L1 norm: 1.285.. Val L1 norm: 1.010.. Train Linf norm: 285.670.. Val Linf norm: 7.346\n",
            "Epoch 123/143.. Train loss: 107.206.. Val loss: 54.859.. Train L1 norm: 2.769.. Val L1 norm: 1.010.. Train Linf norm: 1805.900.. Val Linf norm: 7.347\n",
            "Epoch 124/143.. Train loss: 85.389.. Val loss: 54.859.. Train L1 norm: 1.584.. Val L1 norm: 1.010.. Train Linf norm: 591.314.. Val Linf norm: 7.345\n",
            "Epoch 125/143.. Train loss: 324.512.. Val loss: 54.858.. Train L1 norm: 3.340.. Val L1 norm: 1.010.. Train Linf norm: 2391.169.. Val Linf norm: 7.350\n",
            "Epoch 126/143.. Train loss: 81.931.. Val loss: 54.858.. Train L1 norm: 2.052.. Val L1 norm: 1.010.. Train Linf norm: 1070.112.. Val Linf norm: 7.351\n",
            "Epoch 127/143.. Train loss: 62.513.. Val loss: 54.858.. Train L1 norm: 1.551.. Val L1 norm: 1.010.. Train Linf norm: 557.454.. Val Linf norm: 7.351\n",
            "Epoch 128/143.. Train loss: 84.693.. Val loss: 54.858.. Train L1 norm: 1.108.. Val L1 norm: 1.010.. Train Linf norm: 104.134.. Val Linf norm: 7.350\n",
            "Epoch 129/143.. Train loss: 82.142.. Val loss: 54.859.. Train L1 norm: 2.356.. Val L1 norm: 1.010.. Train Linf norm: 1382.189.. Val Linf norm: 7.348\n",
            "Epoch 130/143.. Train loss: 61.104.. Val loss: 54.859.. Train L1 norm: 2.172.. Val L1 norm: 1.010.. Train Linf norm: 1194.190.. Val Linf norm: 7.347\n",
            "Epoch 131/143.. Train loss: 155.532.. Val loss: 54.858.. Train L1 norm: 3.129.. Val L1 norm: 1.010.. Train Linf norm: 2174.996.. Val Linf norm: 7.355\n",
            "Epoch 132/143.. Train loss: 62.690.. Val loss: 54.858.. Train L1 norm: 1.436.. Val L1 norm: 1.010.. Train Linf norm: 438.838.. Val Linf norm: 7.355\n",
            "Epoch 133/143.. Train loss: 200.649.. Val loss: 54.859.. Train L1 norm: 1.550.. Val L1 norm: 1.010.. Train Linf norm: 557.154.. Val Linf norm: 7.348\n",
            "Epoch 134/143.. Train loss: 62.341.. Val loss: 54.859.. Train L1 norm: 2.281.. Val L1 norm: 1.010.. Train Linf norm: 1304.157.. Val Linf norm: 7.347\n",
            "Epoch 135/143.. Train loss: 189.758.. Val loss: 54.858.. Train L1 norm: 2.444.. Val L1 norm: 1.010.. Train Linf norm: 1473.343.. Val Linf norm: 7.355\n",
            "Epoch 136/143.. Train loss: 97.431.. Val loss: 54.857.. Train L1 norm: 1.754.. Val L1 norm: 1.010.. Train Linf norm: 767.052.. Val Linf norm: 7.360\n",
            "Epoch 137/143.. Train loss: 372.398.. Val loss: 54.857.. Train L1 norm: 2.999.. Val L1 norm: 1.010.. Train Linf norm: 2040.148.. Val Linf norm: 7.364\n",
            "Epoch 138/143.. Train loss: 718.972.. Val loss: 54.857.. Train L1 norm: 3.384.. Val L1 norm: 1.010.. Train Linf norm: 2433.506.. Val Linf norm: 7.361\n",
            "Epoch 139/143.. Train loss: 92.210.. Val loss: 54.858.. Train L1 norm: 2.957.. Val L1 norm: 1.010.. Train Linf norm: 1997.526.. Val Linf norm: 7.360\n",
            "Epoch 140/143.. Train loss: 221.395.. Val loss: 54.857.. Train L1 norm: 3.055.. Val L1 norm: 1.010.. Train Linf norm: 2098.597.. Val Linf norm: 7.362\n",
            "Epoch 141/143.. Train loss: 545.191.. Val loss: 54.856.. Train L1 norm: 2.762.. Val L1 norm: 1.010.. Train Linf norm: 1797.835.. Val Linf norm: 7.373\n",
            "Epoch 142/143.. Train loss: 93.539.. Val loss: 54.856.. Train L1 norm: 1.055.. Val L1 norm: 1.010.. Train Linf norm: 50.460.. Val Linf norm: 7.375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:19:51,244]\u001b[0m Trial 92 finished with value: 1.0098264760335287 and parameters: {'n_layers': 6, 'n_units_0': 3558, 'n_units_1': 165, 'n_units_2': 3906, 'n_units_3': 434, 'n_units_4': 239, 'n_units_5': 1274, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 1.0435149104684462e-06, 'batch_size': 1024, 'n_epochs': 143, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.2022112945503146, 'dropout_rate': 0.06388872518840581, 'weight_decay': 0.0009246275300019842, 'beta1': 0.939104753876406, 'beta2': 0.9991856135686923, 'factor': 0.11351038537346028, 'patience': 6, 'threshold': 0.0024087472803278587}. Best is trial 92 with value: 1.0098264760335287.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 143/143.. Train loss: 107.076.. Val loss: 54.856.. Train L1 norm: 2.753.. Val L1 norm: 1.010.. Train Linf norm: 1789.928.. Val Linf norm: 7.377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:19:52,844]\u001b[0m Trial 93 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/148.. Train loss: 385.580.. Val loss: 53.025.. Train L1 norm: 3.715.. Val L1 norm: 1.099.. Train Linf norm: 2748.469.. Val Linf norm: 75.591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:19:54,162]\u001b[0m Trial 94 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/141.. Train loss: 72.089.. Val loss: 53.593.. Train L1 norm: 1.858.. Val L1 norm: 1.056.. Train Linf norm: 872.648.. Val Linf norm: 43.395\n",
            "Epoch 1/145.. Train loss: 410.196.. Val loss: 54.810.. Train L1 norm: 2.812.. Val L1 norm: 1.028.. Train Linf norm: 1830.443.. Val Linf norm: 24.500\n",
            "Epoch 2/145.. Train loss: 62.197.. Val loss: 54.454.. Train L1 norm: 1.502.. Val L1 norm: 1.038.. Train Linf norm: 507.903.. Val Linf norm: 32.439\n",
            "Epoch 3/145.. Train loss: 104.591.. Val loss: 52.951.. Train L1 norm: 4.361.. Val L1 norm: 1.106.. Train Linf norm: 3425.181.. Val Linf norm: 77.006\n",
            "Epoch 4/145.. Train loss: 1085.925.. Val loss: 54.451.. Train L1 norm: 4.939.. Val L1 norm: 1.037.. Train Linf norm: 4004.743.. Val Linf norm: 31.367\n",
            "Epoch 5/145.. Train loss: 105.968.. Val loss: 53.793.. Train L1 norm: 2.747.. Val L1 norm: 1.065.. Train Linf norm: 1780.040.. Val Linf norm: 50.015\n",
            "Epoch 6/145.. Train loss: 775.499.. Val loss: 53.466.. Train L1 norm: 2.832.. Val L1 norm: 1.082.. Train Linf norm: 1850.366.. Val Linf norm: 61.221\n",
            "Epoch 7/145.. Train loss: 222.749.. Val loss: 54.378.. Train L1 norm: 2.277.. Val L1 norm: 1.048.. Train Linf norm: 1298.051.. Val Linf norm: 42.760\n",
            "Epoch 8/145.. Train loss: 891.079.. Val loss: 56.180.. Train L1 norm: 1.909.. Val L1 norm: 1.031.. Train Linf norm: 921.718.. Val Linf norm: 23.908\n",
            "Epoch 9/145.. Train loss: 205.512.. Val loss: 55.723.. Train L1 norm: 2.175.. Val L1 norm: 1.025.. Train Linf norm: 1184.659.. Val Linf norm: 20.105\n",
            "Epoch 10/145.. Train loss: 66.905.. Val loss: 54.411.. Train L1 norm: 1.217.. Val L1 norm: 1.060.. Train Linf norm: 212.734.. Val Linf norm: 50.012\n",
            "Epoch 11/145.. Train loss: 455.488.. Val loss: 54.559.. Train L1 norm: 1.189.. Val L1 norm: 1.054.. Train Linf norm: 91.340.. Val Linf norm: 46.172\n",
            "Epoch 12/145.. Train loss: 224.644.. Val loss: 54.728.. Train L1 norm: 1.689.. Val L1 norm: 1.049.. Train Linf norm: 695.603.. Val Linf norm: 42.218\n",
            "Epoch 13/145.. Train loss: 63.470.. Val loss: 54.707.. Train L1 norm: 3.299.. Val L1 norm: 1.049.. Train Linf norm: 2345.382.. Val Linf norm: 42.864\n",
            "Epoch 14/145.. Train loss: 78.349.. Val loss: 54.637.. Train L1 norm: 2.489.. Val L1 norm: 1.052.. Train Linf norm: 1514.545.. Val Linf norm: 44.545\n",
            "Epoch 15/145.. Train loss: 60.949.. Val loss: 54.623.. Train L1 norm: 1.081.. Val L1 norm: 1.053.. Train Linf norm: 73.290.. Val Linf norm: 45.067\n",
            "Epoch 16/145.. Train loss: 240.168.. Val loss: 54.522.. Train L1 norm: 2.589.. Val L1 norm: 1.057.. Train Linf norm: 1614.923.. Val Linf norm: 47.952\n",
            "Epoch 17/145.. Train loss: 371.448.. Val loss: 54.570.. Train L1 norm: 1.214.. Val L1 norm: 1.054.. Train Linf norm: 205.532.. Val Linf norm: 46.098\n",
            "Epoch 18/145.. Train loss: 265.745.. Val loss: 54.586.. Train L1 norm: 2.377.. Val L1 norm: 1.054.. Train Linf norm: 1395.273.. Val Linf norm: 45.679\n",
            "Epoch 19/145.. Train loss: 89.129.. Val loss: 54.580.. Train L1 norm: 2.205.. Val L1 norm: 1.054.. Train Linf norm: 1222.499.. Val Linf norm: 45.887\n",
            "Epoch 20/145.. Train loss: 172.005.. Val loss: 54.565.. Train L1 norm: 3.128.. Val L1 norm: 1.055.. Train Linf norm: 2167.850.. Val Linf norm: 46.282\n",
            "Epoch 21/145.. Train loss: 97.788.. Val loss: 54.554.. Train L1 norm: 1.903.. Val L1 norm: 1.055.. Train Linf norm: 914.322.. Val Linf norm: 46.589\n",
            "Epoch 22/145.. Train loss: 316.253.. Val loss: 54.578.. Train L1 norm: 1.327.. Val L1 norm: 1.054.. Train Linf norm: 322.357.. Val Linf norm: 45.972\n",
            "Epoch 23/145.. Train loss: 61.214.. Val loss: 54.580.. Train L1 norm: 2.016.. Val L1 norm: 1.054.. Train Linf norm: 1028.336.. Val Linf norm: 45.921\n",
            "Epoch 24/145.. Train loss: 63.327.. Val loss: 54.578.. Train L1 norm: 2.172.. Val L1 norm: 1.054.. Train Linf norm: 1188.160.. Val Linf norm: 45.979\n",
            "Epoch 25/145.. Train loss: 592.386.. Val loss: 54.582.. Train L1 norm: 3.076.. Val L1 norm: 1.054.. Train Linf norm: 2114.542.. Val Linf norm: 45.903\n",
            "Epoch 26/145.. Train loss: 114.934.. Val loss: 54.581.. Train L1 norm: 3.165.. Val L1 norm: 1.054.. Train Linf norm: 2206.237.. Val Linf norm: 45.928\n",
            "Epoch 27/145.. Train loss: 391.051.. Val loss: 54.578.. Train L1 norm: 1.875.. Val L1 norm: 1.054.. Train Linf norm: 880.495.. Val Linf norm: 45.993\n",
            "Epoch 28/145.. Train loss: 161.650.. Val loss: 54.579.. Train L1 norm: 3.945.. Val L1 norm: 1.054.. Train Linf norm: 3004.889.. Val Linf norm: 45.974\n",
            "Epoch 29/145.. Train loss: 156.992.. Val loss: 54.579.. Train L1 norm: 1.978.. Val L1 norm: 1.054.. Train Linf norm: 990.157.. Val Linf norm: 45.988\n",
            "Epoch 30/145.. Train loss: 239.260.. Val loss: 54.580.. Train L1 norm: 1.519.. Val L1 norm: 1.054.. Train Linf norm: 519.625.. Val Linf norm: 45.973\n",
            "Epoch 31/145.. Train loss: 60.920.. Val loss: 54.580.. Train L1 norm: 2.644.. Val L1 norm: 1.054.. Train Linf norm: 1670.410.. Val Linf norm: 45.976\n",
            "Epoch 32/145.. Train loss: 61.016.. Val loss: 54.580.. Train L1 norm: 2.879.. Val L1 norm: 1.054.. Train Linf norm: 1912.890.. Val Linf norm: 45.980\n",
            "Epoch 33/145.. Train loss: 62.953.. Val loss: 54.580.. Train L1 norm: 3.263.. Val L1 norm: 1.054.. Train Linf norm: 2305.203.. Val Linf norm: 45.982\n",
            "Epoch 34/145.. Train loss: 148.197.. Val loss: 54.579.. Train L1 norm: 1.222.. Val L1 norm: 1.054.. Train Linf norm: 216.119.. Val Linf norm: 46.008\n",
            "Epoch 35/145.. Train loss: 109.008.. Val loss: 54.579.. Train L1 norm: 3.385.. Val L1 norm: 1.054.. Train Linf norm: 2428.645.. Val Linf norm: 45.991\n",
            "Epoch 36/145.. Train loss: 521.935.. Val loss: 54.576.. Train L1 norm: 1.401.. Val L1 norm: 1.054.. Train Linf norm: 398.733.. Val Linf norm: 46.083\n",
            "Epoch 37/145.. Train loss: 481.913.. Val loss: 54.573.. Train L1 norm: 3.234.. Val L1 norm: 1.054.. Train Linf norm: 2273.632.. Val Linf norm: 46.172\n",
            "Epoch 38/145.. Train loss: 60.445.. Val loss: 54.572.. Train L1 norm: 2.784.. Val L1 norm: 1.054.. Train Linf norm: 1814.175.. Val Linf norm: 46.176\n",
            "Epoch 39/145.. Train loss: 261.494.. Val loss: 54.571.. Train L1 norm: 1.976.. Val L1 norm: 1.054.. Train Linf norm: 988.471.. Val Linf norm: 46.228\n",
            "Epoch 40/145.. Train loss: 225.336.. Val loss: 54.571.. Train L1 norm: 2.350.. Val L1 norm: 1.054.. Train Linf norm: 1371.232.. Val Linf norm: 46.218\n",
            "Epoch 41/145.. Train loss: 69.094.. Val loss: 54.572.. Train L1 norm: 3.658.. Val L1 norm: 1.054.. Train Linf norm: 2711.840.. Val Linf norm: 46.190\n",
            "Epoch 42/145.. Train loss: 80.953.. Val loss: 54.572.. Train L1 norm: 3.607.. Val L1 norm: 1.054.. Train Linf norm: 2658.521.. Val Linf norm: 46.203\n",
            "Epoch 43/145.. Train loss: 92.143.. Val loss: 54.571.. Train L1 norm: 2.308.. Val L1 norm: 1.054.. Train Linf norm: 1328.233.. Val Linf norm: 46.225\n",
            "Epoch 44/145.. Train loss: 188.476.. Val loss: 54.572.. Train L1 norm: 2.518.. Val L1 norm: 1.054.. Train Linf norm: 1542.999.. Val Linf norm: 46.200\n",
            "Epoch 45/145.. Train loss: 207.406.. Val loss: 54.571.. Train L1 norm: 1.827.. Val L1 norm: 1.054.. Train Linf norm: 835.174.. Val Linf norm: 46.238\n",
            "Epoch 46/145.. Train loss: 203.257.. Val loss: 54.568.. Train L1 norm: 2.096.. Val L1 norm: 1.055.. Train Linf norm: 1109.089.. Val Linf norm: 46.323\n",
            "Epoch 47/145.. Train loss: 61.447.. Val loss: 54.568.. Train L1 norm: 1.928.. Val L1 norm: 1.055.. Train Linf norm: 939.991.. Val Linf norm: 46.327\n",
            "Epoch 48/145.. Train loss: 68.258.. Val loss: 54.567.. Train L1 norm: 2.654.. Val L1 norm: 1.055.. Train Linf norm: 1680.853.. Val Linf norm: 46.338\n",
            "Epoch 49/145.. Train loss: 78.007.. Val loss: 54.567.. Train L1 norm: 2.971.. Val L1 norm: 1.055.. Train Linf norm: 2006.204.. Val Linf norm: 46.352\n",
            "Epoch 50/145.. Train loss: 212.035.. Val loss: 54.569.. Train L1 norm: 1.686.. Val L1 norm: 1.055.. Train Linf norm: 689.729.. Val Linf norm: 46.299\n",
            "Epoch 51/145.. Train loss: 61.041.. Val loss: 54.569.. Train L1 norm: 2.257.. Val L1 norm: 1.055.. Train Linf norm: 1273.763.. Val Linf norm: 46.303\n",
            "Epoch 52/145.. Train loss: 82.035.. Val loss: 54.570.. Train L1 norm: 2.829.. Val L1 norm: 1.055.. Train Linf norm: 1861.255.. Val Linf norm: 46.288\n",
            "Epoch 53/145.. Train loss: 69.063.. Val loss: 54.570.. Train L1 norm: 1.427.. Val L1 norm: 1.055.. Train Linf norm: 425.394.. Val Linf norm: 46.279\n",
            "Epoch 54/145.. Train loss: 60.315.. Val loss: 54.570.. Train L1 norm: 2.423.. Val L1 norm: 1.055.. Train Linf norm: 1445.224.. Val Linf norm: 46.283\n",
            "Epoch 55/145.. Train loss: 331.403.. Val loss: 54.567.. Train L1 norm: 1.055.. Val L1 norm: 1.055.. Train Linf norm: 43.623.. Val Linf norm: 46.365\n",
            "Epoch 56/145.. Train loss: 69.293.. Val loss: 54.567.. Train L1 norm: 1.237.. Val L1 norm: 1.055.. Train Linf norm: 226.381.. Val Linf norm: 46.358\n",
            "Epoch 57/145.. Train loss: 454.236.. Val loss: 54.571.. Train L1 norm: 1.668.. Val L1 norm: 1.055.. Train Linf norm: 671.454.. Val Linf norm: 46.262\n",
            "Epoch 58/145.. Train loss: 71.333.. Val loss: 54.571.. Train L1 norm: 3.490.. Val L1 norm: 1.054.. Train Linf norm: 2539.413.. Val Linf norm: 46.252\n",
            "Epoch 59/145.. Train loss: 62.086.. Val loss: 54.571.. Train L1 norm: 1.578.. Val L1 norm: 1.054.. Train Linf norm: 580.122.. Val Linf norm: 46.253\n",
            "Epoch 60/145.. Train loss: 62.540.. Val loss: 54.571.. Train L1 norm: 3.497.. Val L1 norm: 1.055.. Train Linf norm: 2546.241.. Val Linf norm: 46.259\n",
            "Epoch 61/145.. Train loss: 61.165.. Val loss: 54.571.. Train L1 norm: 1.910.. Val L1 norm: 1.055.. Train Linf norm: 918.444.. Val Linf norm: 46.262\n",
            "Epoch 62/145.. Train loss: 152.968.. Val loss: 54.570.. Train L1 norm: 3.671.. Val L1 norm: 1.055.. Train Linf norm: 2722.661.. Val Linf norm: 46.309\n",
            "Epoch 63/145.. Train loss: 68.767.. Val loss: 54.569.. Train L1 norm: 1.730.. Val L1 norm: 1.055.. Train Linf norm: 735.800.. Val Linf norm: 46.335\n",
            "Epoch 64/145.. Train loss: 399.441.. Val loss: 54.566.. Train L1 norm: 3.088.. Val L1 norm: 1.055.. Train Linf norm: 2125.434.. Val Linf norm: 46.411\n",
            "Epoch 65/145.. Train loss: 70.552.. Val loss: 54.565.. Train L1 norm: 2.188.. Val L1 norm: 1.055.. Train Linf norm: 1203.811.. Val Linf norm: 46.442\n",
            "Epoch 66/145.. Train loss: 64.307.. Val loss: 54.565.. Train L1 norm: 1.795.. Val L1 norm: 1.055.. Train Linf norm: 801.805.. Val Linf norm: 46.442\n",
            "Epoch 67/145.. Train loss: 73.066.. Val loss: 54.565.. Train L1 norm: 4.634.. Val L1 norm: 1.055.. Train Linf norm: 3702.327.. Val Linf norm: 46.435\n",
            "Epoch 68/145.. Train loss: 115.908.. Val loss: 54.566.. Train L1 norm: 1.909.. Val L1 norm: 1.055.. Train Linf norm: 910.506.. Val Linf norm: 46.424\n",
            "Epoch 69/145.. Train loss: 63.249.. Val loss: 54.566.. Train L1 norm: 2.038.. Val L1 norm: 1.055.. Train Linf norm: 1051.625.. Val Linf norm: 46.410\n",
            "Epoch 70/145.. Train loss: 209.609.. Val loss: 54.569.. Train L1 norm: 2.370.. Val L1 norm: 1.055.. Train Linf norm: 1393.241.. Val Linf norm: 46.364\n",
            "Epoch 71/145.. Train loss: 82.350.. Val loss: 54.569.. Train L1 norm: 4.737.. Val L1 norm: 1.055.. Train Linf norm: 3817.495.. Val Linf norm: 46.349\n",
            "Epoch 72/145.. Train loss: 60.749.. Val loss: 54.569.. Train L1 norm: 1.216.. Val L1 norm: 1.055.. Train Linf norm: 209.758.. Val Linf norm: 46.352\n",
            "Epoch 73/145.. Train loss: 119.879.. Val loss: 54.571.. Train L1 norm: 2.107.. Val L1 norm: 1.055.. Train Linf norm: 1123.159.. Val Linf norm: 46.308\n",
            "Epoch 74/145.. Train loss: 60.657.. Val loss: 54.571.. Train L1 norm: 1.664.. Val L1 norm: 1.055.. Train Linf norm: 668.664.. Val Linf norm: 46.312\n",
            "Epoch 75/145.. Train loss: 121.496.. Val loss: 54.573.. Train L1 norm: 1.422.. Val L1 norm: 1.055.. Train Linf norm: 421.815.. Val Linf norm: 46.264\n",
            "Epoch 76/145.. Train loss: 60.765.. Val loss: 54.573.. Train L1 norm: 2.763.. Val L1 norm: 1.055.. Train Linf norm: 1795.016.. Val Linf norm: 46.267\n",
            "Epoch 77/145.. Train loss: 720.049.. Val loss: 54.568.. Train L1 norm: 1.330.. Val L1 norm: 1.055.. Train Linf norm: 324.940.. Val Linf norm: 46.381\n",
            "Epoch 78/145.. Train loss: 76.025.. Val loss: 54.567.. Train L1 norm: 2.055.. Val L1 norm: 1.055.. Train Linf norm: 1067.973.. Val Linf norm: 46.392\n",
            "Epoch 79/145.. Train loss: 74.955.. Val loss: 54.566.. Train L1 norm: 1.906.. Val L1 norm: 1.055.. Train Linf norm: 917.579.. Val Linf norm: 46.410\n",
            "Epoch 80/145.. Train loss: 313.689.. Val loss: 54.563.. Train L1 norm: 3.038.. Val L1 norm: 1.055.. Train Linf norm: 2074.942.. Val Linf norm: 46.496\n",
            "Epoch 81/145.. Train loss: 213.841.. Val loss: 54.560.. Train L1 norm: 2.759.. Val L1 norm: 1.055.. Train Linf norm: 1790.762.. Val Linf norm: 46.586\n",
            "Epoch 82/145.. Train loss: 283.212.. Val loss: 54.563.. Train L1 norm: 3.341.. Val L1 norm: 1.055.. Train Linf norm: 2384.293.. Val Linf norm: 46.526\n",
            "Epoch 83/145.. Train loss: 64.215.. Val loss: 54.563.. Train L1 norm: 4.787.. Val L1 norm: 1.055.. Train Linf norm: 3863.428.. Val Linf norm: 46.530\n",
            "Epoch 84/145.. Train loss: 67.281.. Val loss: 54.562.. Train L1 norm: 1.594.. Val L1 norm: 1.055.. Train Linf norm: 596.229.. Val Linf norm: 46.538\n",
            "Epoch 85/145.. Train loss: 136.368.. Val loss: 54.562.. Train L1 norm: 1.385.. Val L1 norm: 1.055.. Train Linf norm: 363.390.. Val Linf norm: 46.550\n",
            "Epoch 86/145.. Train loss: 130.468.. Val loss: 54.559.. Train L1 norm: 1.383.. Val L1 norm: 1.055.. Train Linf norm: 380.183.. Val Linf norm: 46.637\n",
            "Epoch 87/145.. Train loss: 250.638.. Val loss: 54.556.. Train L1 norm: 1.077.. Val L1 norm: 1.055.. Train Linf norm: 62.688.. Val Linf norm: 46.699\n",
            "Epoch 88/145.. Train loss: 62.792.. Val loss: 54.556.. Train L1 norm: 2.252.. Val L1 norm: 1.055.. Train Linf norm: 1272.466.. Val Linf norm: 46.708\n",
            "Epoch 89/145.. Train loss: 89.807.. Val loss: 54.554.. Train L1 norm: 2.022.. Val L1 norm: 1.055.. Train Linf norm: 1034.431.. Val Linf norm: 46.744\n",
            "Epoch 90/145.. Train loss: 415.536.. Val loss: 54.551.. Train L1 norm: 1.673.. Val L1 norm: 1.055.. Train Linf norm: 678.911.. Val Linf norm: 46.840\n",
            "Epoch 91/145.. Train loss: 115.957.. Val loss: 54.551.. Train L1 norm: 3.823.. Val L1 norm: 1.055.. Train Linf norm: 2877.943.. Val Linf norm: 46.824\n",
            "Epoch 92/145.. Train loss: 146.398.. Val loss: 54.553.. Train L1 norm: 3.242.. Val L1 norm: 1.055.. Train Linf norm: 2284.861.. Val Linf norm: 46.784\n",
            "Epoch 93/145.. Train loss: 388.884.. Val loss: 54.551.. Train L1 norm: 1.307.. Val L1 norm: 1.055.. Train Linf norm: 303.732.. Val Linf norm: 46.846\n",
            "Epoch 94/145.. Train loss: 60.887.. Val loss: 54.551.. Train L1 norm: 2.768.. Val L1 norm: 1.055.. Train Linf norm: 1798.208.. Val Linf norm: 46.851\n",
            "Epoch 95/145.. Train loss: 339.779.. Val loss: 54.549.. Train L1 norm: 3.465.. Val L1 norm: 1.055.. Train Linf norm: 2512.178.. Val Linf norm: 46.889\n",
            "Epoch 96/145.. Train loss: 71.458.. Val loss: 54.548.. Train L1 norm: 2.655.. Val L1 norm: 1.055.. Train Linf norm: 1681.590.. Val Linf norm: 46.944\n",
            "Epoch 97/145.. Train loss: 63.093.. Val loss: 54.548.. Train L1 norm: 3.884.. Val L1 norm: 1.055.. Train Linf norm: 2942.726.. Val Linf norm: 46.944\n",
            "Epoch 98/145.. Train loss: 62.474.. Val loss: 54.548.. Train L1 norm: 2.132.. Val L1 norm: 1.055.. Train Linf norm: 1146.580.. Val Linf norm: 46.942\n",
            "Epoch 99/145.. Train loss: 94.028.. Val loss: 54.549.. Train L1 norm: 3.384.. Val L1 norm: 1.055.. Train Linf norm: 2430.641.. Val Linf norm: 46.915\n",
            "Epoch 100/145.. Train loss: 123.546.. Val loss: 54.548.. Train L1 norm: 3.889.. Val L1 norm: 1.055.. Train Linf norm: 2946.151.. Val Linf norm: 46.947\n",
            "Epoch 101/145.. Train loss: 65.303.. Val loss: 54.548.. Train L1 norm: 3.958.. Val L1 norm: 1.055.. Train Linf norm: 3017.249.. Val Linf norm: 46.946\n",
            "Epoch 102/145.. Train loss: 92.965.. Val loss: 54.549.. Train L1 norm: 2.046.. Val L1 norm: 1.055.. Train Linf norm: 1059.049.. Val Linf norm: 46.914\n",
            "Epoch 103/145.. Train loss: 137.424.. Val loss: 54.548.. Train L1 norm: 4.852.. Val L1 norm: 1.055.. Train Linf norm: 3932.710.. Val Linf norm: 46.939\n",
            "Epoch 104/145.. Train loss: 127.327.. Val loss: 54.550.. Train L1 norm: 2.323.. Val L1 norm: 1.055.. Train Linf norm: 1340.688.. Val Linf norm: 46.880\n",
            "Epoch 105/145.. Train loss: 263.128.. Val loss: 54.552.. Train L1 norm: 3.074.. Val L1 norm: 1.055.. Train Linf norm: 2111.157.. Val Linf norm: 46.839\n",
            "Epoch 106/145.. Train loss: 159.610.. Val loss: 54.550.. Train L1 norm: 2.299.. Val L1 norm: 1.055.. Train Linf norm: 1320.012.. Val Linf norm: 46.900\n",
            "Epoch 107/145.. Train loss: 185.761.. Val loss: 54.552.. Train L1 norm: 1.291.. Val L1 norm: 1.055.. Train Linf norm: 285.529.. Val Linf norm: 46.852\n",
            "Epoch 108/145.. Train loss: 67.934.. Val loss: 54.552.. Train L1 norm: 2.285.. Val L1 norm: 1.055.. Train Linf norm: 1303.418.. Val Linf norm: 46.856\n",
            "Epoch 109/145.. Train loss: 95.379.. Val loss: 54.552.. Train L1 norm: 2.311.. Val L1 norm: 1.055.. Train Linf norm: 1330.843.. Val Linf norm: 46.842\n",
            "Epoch 110/145.. Train loss: 146.628.. Val loss: 54.554.. Train L1 norm: 3.101.. Val L1 norm: 1.055.. Train Linf norm: 2139.501.. Val Linf norm: 46.809\n",
            "Epoch 111/145.. Train loss: 410.100.. Val loss: 54.557.. Train L1 norm: 2.208.. Val L1 norm: 1.055.. Train Linf norm: 1223.160.. Val Linf norm: 46.723\n",
            "Epoch 112/145.. Train loss: 119.021.. Val loss: 54.558.. Train L1 norm: 2.024.. Val L1 norm: 1.055.. Train Linf norm: 1035.547.. Val Linf norm: 46.698\n",
            "Epoch 113/145.. Train loss: 234.115.. Val loss: 54.560.. Train L1 norm: 3.195.. Val L1 norm: 1.055.. Train Linf norm: 2236.465.. Val Linf norm: 46.642\n",
            "Epoch 114/145.. Train loss: 105.685.. Val loss: 54.559.. Train L1 norm: 2.174.. Val L1 norm: 1.055.. Train Linf norm: 1188.857.. Val Linf norm: 46.685\n",
            "Epoch 115/145.. Train loss: 195.939.. Val loss: 54.557.. Train L1 norm: 2.251.. Val L1 norm: 1.055.. Train Linf norm: 1271.214.. Val Linf norm: 46.740\n",
            "Epoch 116/145.. Train loss: 410.248.. Val loss: 54.551.. Train L1 norm: 2.333.. Val L1 norm: 1.055.. Train Linf norm: 1353.260.. Val Linf norm: 46.888\n",
            "Epoch 117/145.. Train loss: 76.685.. Val loss: 54.551.. Train L1 norm: 2.909.. Val L1 norm: 1.055.. Train Linf norm: 1943.401.. Val Linf norm: 46.911\n",
            "Epoch 118/145.. Train loss: 289.687.. Val loss: 54.548.. Train L1 norm: 1.926.. Val L1 norm: 1.056.. Train Linf norm: 929.544.. Val Linf norm: 46.986\n",
            "Epoch 119/145.. Train loss: 73.016.. Val loss: 54.548.. Train L1 norm: 3.555.. Val L1 norm: 1.056.. Train Linf norm: 2604.441.. Val Linf norm: 46.977\n",
            "Epoch 120/145.. Train loss: 164.862.. Val loss: 54.549.. Train L1 norm: 2.746.. Val L1 norm: 1.055.. Train Linf norm: 1777.669.. Val Linf norm: 46.951\n",
            "Epoch 121/145.. Train loss: 97.589.. Val loss: 54.549.. Train L1 norm: 2.354.. Val L1 norm: 1.055.. Train Linf norm: 1375.560.. Val Linf norm: 46.960\n",
            "Epoch 122/145.. Train loss: 61.140.. Val loss: 54.548.. Train L1 norm: 3.762.. Val L1 norm: 1.056.. Train Linf norm: 2816.065.. Val Linf norm: 46.976\n",
            "Epoch 123/145.. Train loss: 60.832.. Val loss: 54.548.. Train L1 norm: 1.460.. Val L1 norm: 1.056.. Train Linf norm: 461.225.. Val Linf norm: 46.981\n",
            "Epoch 124/145.. Train loss: 145.476.. Val loss: 54.546.. Train L1 norm: 2.978.. Val L1 norm: 1.056.. Train Linf norm: 2014.721.. Val Linf norm: 47.033\n",
            "Epoch 125/145.. Train loss: 61.567.. Val loss: 54.546.. Train L1 norm: 1.782.. Val L1 norm: 1.056.. Train Linf norm: 788.757.. Val Linf norm: 47.036\n",
            "Epoch 126/145.. Train loss: 485.605.. Val loss: 54.543.. Train L1 norm: 1.971.. Val L1 norm: 1.056.. Train Linf norm: 981.930.. Val Linf norm: 47.119\n",
            "Epoch 127/145.. Train loss: 298.828.. Val loss: 54.545.. Train L1 norm: 1.187.. Val L1 norm: 1.056.. Train Linf norm: 177.485.. Val Linf norm: 47.064\n",
            "Epoch 128/145.. Train loss: 71.545.. Val loss: 54.545.. Train L1 norm: 3.197.. Val L1 norm: 1.056.. Train Linf norm: 2237.736.. Val Linf norm: 47.057\n",
            "Epoch 129/145.. Train loss: 958.046.. Val loss: 54.542.. Train L1 norm: 2.611.. Val L1 norm: 1.056.. Train Linf norm: 1636.847.. Val Linf norm: 47.148\n",
            "Epoch 130/145.. Train loss: 81.539.. Val loss: 54.542.. Train L1 norm: 2.880.. Val L1 norm: 1.056.. Train Linf norm: 1910.424.. Val Linf norm: 47.130\n",
            "Epoch 131/145.. Train loss: 62.029.. Val loss: 54.542.. Train L1 norm: 2.519.. Val L1 norm: 1.056.. Train Linf norm: 1543.912.. Val Linf norm: 47.134\n",
            "Epoch 132/145.. Train loss: 331.280.. Val loss: 54.539.. Train L1 norm: 1.540.. Val L1 norm: 1.056.. Train Linf norm: 541.341.. Val Linf norm: 47.213\n",
            "Epoch 133/145.. Train loss: 66.975.. Val loss: 54.539.. Train L1 norm: 2.512.. Val L1 norm: 1.056.. Train Linf norm: 1536.786.. Val Linf norm: 47.225\n",
            "Epoch 134/145.. Train loss: 321.470.. Val loss: 54.541.. Train L1 norm: 4.270.. Val L1 norm: 1.056.. Train Linf norm: 3338.395.. Val Linf norm: 47.190\n",
            "Epoch 135/145.. Train loss: 83.410.. Val loss: 54.541.. Train L1 norm: 2.154.. Val L1 norm: 1.056.. Train Linf norm: 1159.541.. Val Linf norm: 47.188\n",
            "Epoch 136/145.. Train loss: 1079.732.. Val loss: 54.543.. Train L1 norm: 1.641.. Val L1 norm: 1.056.. Train Linf norm: 639.329.. Val Linf norm: 47.119\n",
            "Epoch 137/145.. Train loss: 472.099.. Val loss: 54.550.. Train L1 norm: 3.111.. Val L1 norm: 1.055.. Train Linf norm: 2150.727.. Val Linf norm: 46.961\n",
            "Epoch 138/145.. Train loss: 89.706.. Val loss: 54.550.. Train L1 norm: 2.627.. Val L1 norm: 1.055.. Train Linf norm: 1656.073.. Val Linf norm: 46.953\n",
            "Epoch 139/145.. Train loss: 563.848.. Val loss: 54.554.. Train L1 norm: 1.623.. Val L1 norm: 1.055.. Train Linf norm: 625.494.. Val Linf norm: 46.851\n",
            "Epoch 140/145.. Train loss: 380.276.. Val loss: 54.553.. Train L1 norm: 3.069.. Val L1 norm: 1.055.. Train Linf norm: 2102.786.. Val Linf norm: 46.893\n",
            "Epoch 141/145.. Train loss: 63.030.. Val loss: 54.552.. Train L1 norm: 2.870.. Val L1 norm: 1.055.. Train Linf norm: 1901.425.. Val Linf norm: 46.904\n",
            "Epoch 142/145.. Train loss: 84.665.. Val loss: 54.552.. Train L1 norm: 3.296.. Val L1 norm: 1.055.. Train Linf norm: 2339.145.. Val Linf norm: 46.917\n",
            "Epoch 143/145.. Train loss: 60.816.. Val loss: 54.552.. Train L1 norm: 1.186.. Val L1 norm: 1.055.. Train Linf norm: 94.046.. Val Linf norm: 46.921\n",
            "Epoch 144/145.. Train loss: 76.912.. Val loss: 54.552.. Train L1 norm: 2.739.. Val L1 norm: 1.055.. Train Linf norm: 1768.335.. Val Linf norm: 46.913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:27:13,183]\u001b[0m Trial 95 finished with value: 1.0553441636721292 and parameters: {'n_layers': 6, 'n_units_0': 3281, 'n_units_1': 2136, 'n_units_2': 3735, 'n_units_3': 99, 'n_units_4': 632, 'n_units_5': 1579, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 2.4108875903430557e-06, 'batch_size': 1024, 'n_epochs': 145, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.215701055879196, 'dropout_rate': 0.08173961096608943, 'weight_decay': 0.000991058020908288, 'beta1': 0.9269811622946714, 'beta2': 0.9991005686494, 'factor': 0.11622547920064892, 'patience': 6, 'threshold': 0.0026326737166969968}. Best is trial 92 with value: 1.0098264760335287.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 145/145.. Train loss: 328.209.. Val loss: 54.554.. Train L1 norm: 2.851.. Val L1 norm: 1.055.. Train Linf norm: 1884.296.. Val Linf norm: 46.860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:27:15,715]\u001b[0m Trial 96 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/144.. Train loss: 219.237.. Val loss: 53.855.. Train L1 norm: 1.107.. Val L1 norm: 1.060.. Train Linf norm: 188.532.. Val Linf norm: 78.791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "\u001b[32m[I 2023-05-29 09:27:17,964]\u001b[0m Trial 97 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/148.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:27:20,447]\u001b[0m Trial 98 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/139.. Train loss: 5.256.. Val loss: 4.229.. Train L1 norm: 41.413.. Val L1 norm: 3.611.. Train Linf norm: 20358.499.. Val Linf norm: 783.781\n",
            "Epoch 1/136.. Train loss: 1196.734.. Val loss: 53.733.. Train L1 norm: 4.677.. Val L1 norm: 1.041.. Train Linf norm: 3715.167.. Val Linf norm: 32.513\n",
            "Epoch 2/136.. Train loss: 1124.741.. Val loss: 54.384.. Train L1 norm: 2.090.. Val L1 norm: 1.034.. Train Linf norm: 1101.729.. Val Linf norm: 32.093\n",
            "Epoch 3/136.. Train loss: 59.597.. Val loss: 54.376.. Train L1 norm: 1.922.. Val L1 norm: 1.034.. Train Linf norm: 939.105.. Val Linf norm: 31.981\n",
            "Epoch 4/136.. Train loss: 286.411.. Val loss: 54.893.. Train L1 norm: 1.290.. Val L1 norm: 1.032.. Train Linf norm: 295.062.. Val Linf norm: 31.691\n",
            "Epoch 5/136.. Train loss: 61.654.. Val loss: 54.757.. Train L1 norm: 1.234.. Val L1 norm: 1.031.. Train Linf norm: 238.218.. Val Linf norm: 30.903\n",
            "Epoch 6/136.. Train loss: 71.379.. Val loss: 55.051.. Train L1 norm: 1.097.. Val L1 norm: 1.033.. Train Linf norm: 96.348.. Val Linf norm: 32.789\n",
            "Epoch 7/136.. Train loss: 86.978.. Val loss: 55.398.. Train L1 norm: 1.581.. Val L1 norm: 1.040.. Train Linf norm: 592.076.. Val Linf norm: 35.788\n",
            "Epoch 8/136.. Train loss: 85.776.. Val loss: 54.880.. Train L1 norm: 1.238.. Val L1 norm: 1.032.. Train Linf norm: 241.139.. Val Linf norm: 32.148\n",
            "Epoch 9/136.. Train loss: 60.492.. Val loss: 54.930.. Train L1 norm: 1.832.. Val L1 norm: 1.032.. Train Linf norm: 848.915.. Val Linf norm: 32.209\n",
            "Epoch 10/136.. Train loss: 193.769.. Val loss: 55.692.. Train L1 norm: 1.526.. Val L1 norm: 1.046.. Train Linf norm: 536.831.. Val Linf norm: 39.173\n",
            "Epoch 11/136.. Train loss: 255.112.. Val loss: 55.594.. Train L1 norm: 1.176.. Val L1 norm: 1.043.. Train Linf norm: 173.540.. Val Linf norm: 37.702\n",
            "Epoch 12/136.. Train loss: 101.910.. Val loss: 55.531.. Train L1 norm: 1.816.. Val L1 norm: 1.042.. Train Linf norm: 831.348.. Val Linf norm: 36.881\n",
            "Epoch 13/136.. Train loss: 95.380.. Val loss: 55.474.. Train L1 norm: 1.804.. Val L1 norm: 1.041.. Train Linf norm: 819.497.. Val Linf norm: 36.227\n",
            "Epoch 14/136.. Train loss: 87.845.. Val loss: 55.414.. Train L1 norm: 1.072.. Val L1 norm: 1.039.. Train Linf norm: 69.115.. Val Linf norm: 35.598\n",
            "Epoch 15/136.. Train loss: 121.752.. Val loss: 55.333.. Train L1 norm: 1.198.. Val L1 norm: 1.038.. Train Linf norm: 199.648.. Val Linf norm: 34.810\n",
            "Epoch 16/136.. Train loss: 64.471.. Val loss: 55.371.. Train L1 norm: 2.038.. Val L1 norm: 1.039.. Train Linf norm: 1060.339.. Val Linf norm: 35.083\n",
            "Epoch 17/136.. Train loss: 101.567.. Val loss: 55.423.. Train L1 norm: 1.098.. Val L1 norm: 1.039.. Train Linf norm: 93.974.. Val Linf norm: 35.522\n",
            "Epoch 18/136.. Train loss: 62.000.. Val loss: 55.376.. Train L1 norm: 1.030.. Val L1 norm: 1.038.. Train Linf norm: 24.439.. Val Linf norm: 34.958\n",
            "Epoch 19/136.. Train loss: 111.153.. Val loss: 55.275.. Train L1 norm: 1.133.. Val L1 norm: 1.037.. Train Linf norm: 131.936.. Val Linf norm: 34.234\n",
            "Epoch 20/136.. Train loss: 60.565.. Val loss: 55.275.. Train L1 norm: 1.420.. Val L1 norm: 1.037.. Train Linf norm: 427.269.. Val Linf norm: 34.243\n",
            "Epoch 21/136.. Train loss: 109.316.. Val loss: 55.264.. Train L1 norm: 1.369.. Val L1 norm: 1.037.. Train Linf norm: 374.696.. Val Linf norm: 34.171\n",
            "Epoch 22/136.. Train loss: 68.904.. Val loss: 55.272.. Train L1 norm: 1.152.. Val L1 norm: 1.037.. Train Linf norm: 154.046.. Val Linf norm: 34.218\n",
            "Epoch 23/136.. Train loss: 233.170.. Val loss: 55.259.. Train L1 norm: 1.687.. Val L1 norm: 1.036.. Train Linf norm: 701.023.. Val Linf norm: 34.129\n",
            "Epoch 24/136.. Train loss: 62.117.. Val loss: 55.262.. Train L1 norm: 1.248.. Val L1 norm: 1.036.. Train Linf norm: 251.638.. Val Linf norm: 34.152\n",
            "Epoch 25/136.. Train loss: 60.564.. Val loss: 55.263.. Train L1 norm: 1.330.. Val L1 norm: 1.037.. Train Linf norm: 333.371.. Val Linf norm: 34.155\n",
            "Epoch 26/136.. Train loss: 66.713.. Val loss: 55.254.. Train L1 norm: 1.472.. Val L1 norm: 1.036.. Train Linf norm: 480.468.. Val Linf norm: 34.091\n",
            "Epoch 27/136.. Train loss: 204.113.. Val loss: 55.240.. Train L1 norm: 1.417.. Val L1 norm: 1.036.. Train Linf norm: 423.481.. Val Linf norm: 33.983\n",
            "Epoch 28/136.. Train loss: 65.680.. Val loss: 55.234.. Train L1 norm: 2.596.. Val L1 norm: 1.036.. Train Linf norm: 1630.889.. Val Linf norm: 33.946\n",
            "Epoch 29/136.. Train loss: 209.897.. Val loss: 55.235.. Train L1 norm: 1.539.. Val L1 norm: 1.036.. Train Linf norm: 549.713.. Val Linf norm: 33.956\n",
            "Epoch 30/136.. Train loss: 61.298.. Val loss: 55.236.. Train L1 norm: 1.727.. Val L1 norm: 1.036.. Train Linf norm: 742.867.. Val Linf norm: 33.957\n",
            "Epoch 31/136.. Train loss: 72.039.. Val loss: 55.235.. Train L1 norm: 1.373.. Val L1 norm: 1.036.. Train Linf norm: 378.223.. Val Linf norm: 33.950\n",
            "Epoch 32/136.. Train loss: 76.940.. Val loss: 55.236.. Train L1 norm: 2.190.. Val L1 norm: 1.036.. Train Linf norm: 1216.576.. Val Linf norm: 33.959\n",
            "Epoch 33/136.. Train loss: 113.835.. Val loss: 55.237.. Train L1 norm: 1.088.. Val L1 norm: 1.036.. Train Linf norm: 87.488.. Val Linf norm: 33.969\n",
            "Epoch 34/136.. Train loss: 150.036.. Val loss: 55.239.. Train L1 norm: 1.860.. Val L1 norm: 1.036.. Train Linf norm: 878.929.. Val Linf norm: 33.977\n",
            "Epoch 35/136.. Train loss: 144.438.. Val loss: 55.237.. Train L1 norm: 1.144.. Val L1 norm: 1.036.. Train Linf norm: 144.116.. Val Linf norm: 33.966\n",
            "Epoch 36/136.. Train loss: 61.676.. Val loss: 55.238.. Train L1 norm: 2.394.. Val L1 norm: 1.036.. Train Linf norm: 1422.799.. Val Linf norm: 33.967\n",
            "Epoch 37/136.. Train loss: 63.297.. Val loss: 55.238.. Train L1 norm: 1.704.. Val L1 norm: 1.036.. Train Linf norm: 715.982.. Val Linf norm: 33.971\n",
            "Epoch 38/136.. Train loss: 60.509.. Val loss: 55.238.. Train L1 norm: 1.149.. Val L1 norm: 1.036.. Train Linf norm: 147.420.. Val Linf norm: 33.972\n",
            "Epoch 39/136.. Train loss: 66.604.. Val loss: 55.237.. Train L1 norm: 1.118.. Val L1 norm: 1.036.. Train Linf norm: 118.592.. Val Linf norm: 33.962\n",
            "Epoch 40/136.. Train loss: 68.303.. Val loss: 55.236.. Train L1 norm: 1.114.. Val L1 norm: 1.036.. Train Linf norm: 113.752.. Val Linf norm: 33.951\n",
            "Epoch 41/136.. Train loss: 77.966.. Val loss: 55.237.. Train L1 norm: 1.260.. Val L1 norm: 1.036.. Train Linf norm: 262.741.. Val Linf norm: 33.958\n",
            "Epoch 42/136.. Train loss: 83.645.. Val loss: 55.238.. Train L1 norm: 1.850.. Val L1 norm: 1.036.. Train Linf norm: 868.009.. Val Linf norm: 33.964\n",
            "Epoch 43/136.. Train loss: 82.741.. Val loss: 55.240.. Train L1 norm: 1.292.. Val L1 norm: 1.036.. Train Linf norm: 297.494.. Val Linf norm: 33.973\n",
            "Epoch 44/136.. Train loss: 69.917.. Val loss: 55.241.. Train L1 norm: 1.393.. Val L1 norm: 1.036.. Train Linf norm: 399.585.. Val Linf norm: 33.981\n",
            "Epoch 45/136.. Train loss: 72.107.. Val loss: 55.240.. Train L1 norm: 1.458.. Val L1 norm: 1.036.. Train Linf norm: 466.559.. Val Linf norm: 33.970\n",
            "Epoch 46/136.. Train loss: 171.033.. Val loss: 55.241.. Train L1 norm: 1.506.. Val L1 norm: 1.036.. Train Linf norm: 514.669.. Val Linf norm: 33.982\n",
            "Epoch 47/136.. Train loss: 61.049.. Val loss: 55.241.. Train L1 norm: 1.538.. Val L1 norm: 1.036.. Train Linf norm: 549.068.. Val Linf norm: 33.983\n",
            "Epoch 48/136.. Train loss: 71.598.. Val loss: 55.242.. Train L1 norm: 1.538.. Val L1 norm: 1.036.. Train Linf norm: 547.822.. Val Linf norm: 33.988\n",
            "Epoch 49/136.. Train loss: 155.248.. Val loss: 55.244.. Train L1 norm: 1.689.. Val L1 norm: 1.036.. Train Linf norm: 701.867.. Val Linf norm: 33.999\n",
            "Epoch 50/136.. Train loss: 170.151.. Val loss: 55.245.. Train L1 norm: 1.149.. Val L1 norm: 1.036.. Train Linf norm: 150.090.. Val Linf norm: 34.009\n",
            "Epoch 51/136.. Train loss: 62.584.. Val loss: 55.246.. Train L1 norm: 2.537.. Val L1 norm: 1.036.. Train Linf norm: 1569.985.. Val Linf norm: 34.011\n",
            "Epoch 52/136.. Train loss: 111.629.. Val loss: 55.244.. Train L1 norm: 1.829.. Val L1 norm: 1.036.. Train Linf norm: 846.331.. Val Linf norm: 33.999\n",
            "Epoch 53/136.. Train loss: 74.763.. Val loss: 55.243.. Train L1 norm: 1.755.. Val L1 norm: 1.036.. Train Linf norm: 769.259.. Val Linf norm: 33.992\n",
            "Epoch 54/136.. Train loss: 62.262.. Val loss: 55.243.. Train L1 norm: 1.183.. Val L1 norm: 1.036.. Train Linf norm: 185.393.. Val Linf norm: 33.987\n",
            "Epoch 55/136.. Train loss: 65.250.. Val loss: 55.242.. Train L1 norm: 1.718.. Val L1 norm: 1.036.. Train Linf norm: 732.883.. Val Linf norm: 33.982\n",
            "Epoch 56/136.. Train loss: 76.198.. Val loss: 55.240.. Train L1 norm: 1.287.. Val L1 norm: 1.036.. Train Linf norm: 290.997.. Val Linf norm: 33.974\n",
            "Epoch 57/136.. Train loss: 246.354.. Val loss: 55.239.. Train L1 norm: 1.045.. Val L1 norm: 1.036.. Train Linf norm: 43.291.. Val Linf norm: 33.963\n",
            "Epoch 58/136.. Train loss: 95.052.. Val loss: 55.238.. Train L1 norm: 2.242.. Val L1 norm: 1.036.. Train Linf norm: 1269.719.. Val Linf norm: 33.955\n",
            "Epoch 59/136.. Train loss: 83.008.. Val loss: 55.237.. Train L1 norm: 2.559.. Val L1 norm: 1.036.. Train Linf norm: 1591.930.. Val Linf norm: 33.947\n",
            "Epoch 60/136.. Train loss: 146.984.. Val loss: 55.238.. Train L1 norm: 1.231.. Val L1 norm: 1.036.. Train Linf norm: 234.817.. Val Linf norm: 33.954\n",
            "Epoch 61/136.. Train loss: 111.822.. Val loss: 55.239.. Train L1 norm: 1.756.. Val L1 norm: 1.036.. Train Linf norm: 772.325.. Val Linf norm: 33.961\n",
            "Epoch 62/136.. Train loss: 60.935.. Val loss: 55.239.. Train L1 norm: 1.558.. Val L1 norm: 1.036.. Train Linf norm: 569.340.. Val Linf norm: 33.962\n",
            "Epoch 63/136.. Train loss: 96.465.. Val loss: 55.241.. Train L1 norm: 2.441.. Val L1 norm: 1.036.. Train Linf norm: 1474.576.. Val Linf norm: 33.970\n",
            "Epoch 64/136.. Train loss: 63.365.. Val loss: 55.241.. Train L1 norm: 2.128.. Val L1 norm: 1.036.. Train Linf norm: 1152.041.. Val Linf norm: 33.974\n",
            "Epoch 65/136.. Train loss: 124.899.. Val loss: 55.240.. Train L1 norm: 1.515.. Val L1 norm: 1.036.. Train Linf norm: 522.679.. Val Linf norm: 33.962\n",
            "Epoch 66/136.. Train loss: 88.267.. Val loss: 55.238.. Train L1 norm: 1.038.. Val L1 norm: 1.036.. Train Linf norm: 37.226.. Val Linf norm: 33.953\n",
            "Epoch 67/136.. Train loss: 135.261.. Val loss: 55.237.. Train L1 norm: 1.150.. Val L1 norm: 1.036.. Train Linf norm: 150.099.. Val Linf norm: 33.939\n",
            "Epoch 68/136.. Train loss: 60.507.. Val loss: 55.237.. Train L1 norm: 1.128.. Val L1 norm: 1.036.. Train Linf norm: 127.675.. Val Linf norm: 33.939\n",
            "Epoch 69/136.. Train loss: 106.339.. Val loss: 55.238.. Train L1 norm: 1.633.. Val L1 norm: 1.036.. Train Linf norm: 644.750.. Val Linf norm: 33.948\n",
            "Epoch 70/136.. Train loss: 79.861.. Val loss: 55.237.. Train L1 norm: 1.160.. Val L1 norm: 1.036.. Train Linf norm: 160.080.. Val Linf norm: 33.939\n",
            "Epoch 71/136.. Train loss: 60.443.. Val loss: 55.237.. Train L1 norm: 3.090.. Val L1 norm: 1.036.. Train Linf norm: 2138.157.. Val Linf norm: 33.938\n",
            "Epoch 72/136.. Train loss: 60.463.. Val loss: 55.236.. Train L1 norm: 1.462.. Val L1 norm: 1.036.. Train Linf norm: 469.988.. Val Linf norm: 33.936\n",
            "Epoch 73/136.. Train loss: 100.008.. Val loss: 55.235.. Train L1 norm: 1.780.. Val L1 norm: 1.036.. Train Linf norm: 797.183.. Val Linf norm: 33.922\n",
            "Epoch 74/136.. Train loss: 96.263.. Val loss: 55.236.. Train L1 norm: 1.335.. Val L1 norm: 1.036.. Train Linf norm: 341.349.. Val Linf norm: 33.931\n",
            "Epoch 75/136.. Train loss: 73.601.. Val loss: 55.237.. Train L1 norm: 1.506.. Val L1 norm: 1.036.. Train Linf norm: 515.643.. Val Linf norm: 33.942\n",
            "Epoch 76/136.. Train loss: 60.425.. Val loss: 55.237.. Train L1 norm: 1.028.. Val L1 norm: 1.036.. Train Linf norm: 25.441.. Val Linf norm: 33.941\n",
            "Epoch 77/136.. Train loss: 60.472.. Val loss: 55.237.. Train L1 norm: 1.110.. Val L1 norm: 1.036.. Train Linf norm: 109.425.. Val Linf norm: 33.940\n",
            "Epoch 78/136.. Train loss: 205.956.. Val loss: 55.236.. Train L1 norm: 1.528.. Val L1 norm: 1.036.. Train Linf norm: 538.539.. Val Linf norm: 33.925\n",
            "Epoch 79/136.. Train loss: 60.768.. Val loss: 55.236.. Train L1 norm: 1.051.. Val L1 norm: 1.036.. Train Linf norm: 50.332.. Val Linf norm: 33.927\n",
            "Epoch 80/136.. Train loss: 72.332.. Val loss: 55.235.. Train L1 norm: 1.082.. Val L1 norm: 1.036.. Train Linf norm: 81.836.. Val Linf norm: 33.918\n",
            "Epoch 81/136.. Train loss: 78.797.. Val loss: 55.233.. Train L1 norm: 1.638.. Val L1 norm: 1.036.. Train Linf norm: 650.580.. Val Linf norm: 33.909\n",
            "Epoch 82/136.. Train loss: 142.380.. Val loss: 55.232.. Train L1 norm: 1.439.. Val L1 norm: 1.036.. Train Linf norm: 446.620.. Val Linf norm: 33.895\n",
            "Epoch 83/136.. Train loss: 152.005.. Val loss: 55.230.. Train L1 norm: 1.306.. Val L1 norm: 1.036.. Train Linf norm: 311.058.. Val Linf norm: 33.883\n",
            "Epoch 84/136.. Train loss: 101.419.. Val loss: 55.232.. Train L1 norm: 1.253.. Val L1 norm: 1.036.. Train Linf norm: 256.009.. Val Linf norm: 33.892\n",
            "Epoch 85/136.. Train loss: 83.478.. Val loss: 55.232.. Train L1 norm: 1.027.. Val L1 norm: 1.036.. Train Linf norm: 25.437.. Val Linf norm: 33.896\n",
            "Epoch 86/136.. Train loss: 88.407.. Val loss: 55.231.. Train L1 norm: 1.194.. Val L1 norm: 1.036.. Train Linf norm: 196.418.. Val Linf norm: 33.887\n",
            "Epoch 87/136.. Train loss: 69.502.. Val loss: 55.232.. Train L1 norm: 2.606.. Val L1 norm: 1.036.. Train Linf norm: 1642.216.. Val Linf norm: 33.891\n",
            "Epoch 88/136.. Train loss: 84.624.. Val loss: 55.233.. Train L1 norm: 1.037.. Val L1 norm: 1.036.. Train Linf norm: 34.516.. Val Linf norm: 33.899\n",
            "Epoch 89/136.. Train loss: 97.712.. Val loss: 55.234.. Train L1 norm: 1.429.. Val L1 norm: 1.036.. Train Linf norm: 435.185.. Val Linf norm: 33.907\n",
            "Epoch 90/136.. Train loss: 66.730.. Val loss: 55.235.. Train L1 norm: 1.883.. Val L1 norm: 1.036.. Train Linf norm: 901.780.. Val Linf norm: 33.911\n",
            "Epoch 91/136.. Train loss: 73.393.. Val loss: 55.234.. Train L1 norm: 1.750.. Val L1 norm: 1.036.. Train Linf norm: 765.067.. Val Linf norm: 33.902\n",
            "Epoch 92/136.. Train loss: 129.226.. Val loss: 55.235.. Train L1 norm: 1.928.. Val L1 norm: 1.036.. Train Linf norm: 946.429.. Val Linf norm: 33.911\n",
            "Epoch 93/136.. Train loss: 72.288.. Val loss: 55.234.. Train L1 norm: 1.142.. Val L1 norm: 1.036.. Train Linf norm: 142.603.. Val Linf norm: 33.902\n",
            "Epoch 94/136.. Train loss: 63.169.. Val loss: 55.234.. Train L1 norm: 1.982.. Val L1 norm: 1.036.. Train Linf norm: 1002.302.. Val Linf norm: 33.896\n",
            "Epoch 95/136.. Train loss: 62.024.. Val loss: 55.234.. Train L1 norm: 1.274.. Val L1 norm: 1.036.. Train Linf norm: 276.798.. Val Linf norm: 33.899\n",
            "Epoch 96/136.. Train loss: 61.085.. Val loss: 55.235.. Train L1 norm: 1.337.. Val L1 norm: 1.036.. Train Linf norm: 342.577.. Val Linf norm: 33.903\n",
            "Epoch 97/136.. Train loss: 60.879.. Val loss: 55.235.. Train L1 norm: 1.566.. Val L1 norm: 1.036.. Train Linf norm: 578.137.. Val Linf norm: 33.908\n",
            "Epoch 98/136.. Train loss: 84.720.. Val loss: 55.234.. Train L1 norm: 1.038.. Val L1 norm: 1.036.. Train Linf norm: 36.709.. Val Linf norm: 33.897\n",
            "Epoch 99/136.. Train loss: 96.167.. Val loss: 55.232.. Train L1 norm: 1.643.. Val L1 norm: 1.036.. Train Linf norm: 653.069.. Val Linf norm: 33.886\n",
            "Epoch 100/136.. Train loss: 72.501.. Val loss: 55.233.. Train L1 norm: 1.352.. Val L1 norm: 1.036.. Train Linf norm: 357.363.. Val Linf norm: 33.892\n",
            "Epoch 101/136.. Train loss: 77.807.. Val loss: 55.235.. Train L1 norm: 1.841.. Val L1 norm: 1.036.. Train Linf norm: 858.896.. Val Linf norm: 33.901\n",
            "Epoch 102/136.. Train loss: 77.332.. Val loss: 55.233.. Train L1 norm: 1.797.. Val L1 norm: 1.036.. Train Linf norm: 814.223.. Val Linf norm: 33.889\n",
            "Epoch 103/136.. Train loss: 70.524.. Val loss: 55.232.. Train L1 norm: 1.088.. Val L1 norm: 1.036.. Train Linf norm: 86.315.. Val Linf norm: 33.881\n",
            "Epoch 104/136.. Train loss: 62.855.. Val loss: 55.233.. Train L1 norm: 1.027.. Val L1 norm: 1.036.. Train Linf norm: 25.254.. Val Linf norm: 33.889\n",
            "Epoch 105/136.. Train loss: 60.587.. Val loss: 55.234.. Train L1 norm: 1.098.. Val L1 norm: 1.036.. Train Linf norm: 97.094.. Val Linf norm: 33.891\n",
            "Epoch 106/136.. Train loss: 61.479.. Val loss: 55.234.. Train L1 norm: 1.092.. Val L1 norm: 1.036.. Train Linf norm: 91.092.. Val Linf norm: 33.897\n",
            "Epoch 107/136.. Train loss: 69.135.. Val loss: 55.233.. Train L1 norm: 1.021.. Val L1 norm: 1.036.. Train Linf norm: 18.260.. Val Linf norm: 33.885\n",
            "Epoch 108/136.. Train loss: 62.526.. Val loss: 55.232.. Train L1 norm: 1.349.. Val L1 norm: 1.036.. Train Linf norm: 355.580.. Val Linf norm: 33.878\n",
            "Epoch 109/136.. Train loss: 94.427.. Val loss: 55.233.. Train L1 norm: 1.088.. Val L1 norm: 1.036.. Train Linf norm: 88.299.. Val Linf norm: 33.892\n",
            "Epoch 110/136.. Train loss: 62.421.. Val loss: 55.232.. Train L1 norm: 1.487.. Val L1 norm: 1.036.. Train Linf norm: 495.778.. Val Linf norm: 33.883\n",
            "Epoch 111/136.. Train loss: 75.571.. Val loss: 55.233.. Train L1 norm: 1.577.. Val L1 norm: 1.036.. Train Linf norm: 589.683.. Val Linf norm: 33.891\n",
            "Epoch 112/136.. Train loss: 87.545.. Val loss: 55.234.. Train L1 norm: 2.059.. Val L1 norm: 1.036.. Train Linf norm: 1081.844.. Val Linf norm: 33.899\n",
            "Epoch 113/136.. Train loss: 115.497.. Val loss: 55.232.. Train L1 norm: 1.543.. Val L1 norm: 1.036.. Train Linf norm: 552.670.. Val Linf norm: 33.887\n",
            "Epoch 114/136.. Train loss: 67.623.. Val loss: 55.231.. Train L1 norm: 1.170.. Val L1 norm: 1.036.. Train Linf norm: 171.131.. Val Linf norm: 33.879\n",
            "Epoch 115/136.. Train loss: 61.475.. Val loss: 55.232.. Train L1 norm: 1.772.. Val L1 norm: 1.036.. Train Linf norm: 788.541.. Val Linf norm: 33.881\n",
            "Epoch 116/136.. Train loss: 76.611.. Val loss: 55.233.. Train L1 norm: 1.165.. Val L1 norm: 1.036.. Train Linf norm: 167.297.. Val Linf norm: 33.889\n",
            "Epoch 117/136.. Train loss: 60.502.. Val loss: 55.233.. Train L1 norm: 1.068.. Val L1 norm: 1.036.. Train Linf norm: 67.679.. Val Linf norm: 33.889\n",
            "Epoch 118/136.. Train loss: 70.721.. Val loss: 55.234.. Train L1 norm: 2.986.. Val L1 norm: 1.036.. Train Linf norm: 2030.458.. Val Linf norm: 33.897\n",
            "Epoch 119/136.. Train loss: 90.607.. Val loss: 55.233.. Train L1 norm: 1.523.. Val L1 norm: 1.036.. Train Linf norm: 532.119.. Val Linf norm: 33.884\n",
            "Epoch 120/136.. Train loss: 95.890.. Val loss: 55.234.. Train L1 norm: 1.502.. Val L1 norm: 1.036.. Train Linf norm: 511.962.. Val Linf norm: 33.894\n",
            "Epoch 121/136.. Train loss: 60.583.. Val loss: 55.234.. Train L1 norm: 1.951.. Val L1 norm: 1.036.. Train Linf norm: 971.339.. Val Linf norm: 33.893\n",
            "Epoch 122/136.. Train loss: 60.887.. Val loss: 55.234.. Train L1 norm: 1.871.. Val L1 norm: 1.036.. Train Linf norm: 888.653.. Val Linf norm: 33.895\n",
            "Epoch 123/136.. Train loss: 118.335.. Val loss: 55.236.. Train L1 norm: 2.094.. Val L1 norm: 1.036.. Train Linf norm: 1117.527.. Val Linf norm: 33.905\n",
            "Epoch 124/136.. Train loss: 61.153.. Val loss: 55.236.. Train L1 norm: 1.341.. Val L1 norm: 1.036.. Train Linf norm: 346.714.. Val Linf norm: 33.908\n",
            "Epoch 125/136.. Train loss: 108.449.. Val loss: 55.234.. Train L1 norm: 1.362.. Val L1 norm: 1.036.. Train Linf norm: 366.826.. Val Linf norm: 33.895\n",
            "Epoch 126/136.. Train loss: 66.548.. Val loss: 55.235.. Train L1 norm: 1.456.. Val L1 norm: 1.036.. Train Linf norm: 464.171.. Val Linf norm: 33.900\n",
            "Epoch 127/136.. Train loss: 179.351.. Val loss: 55.234.. Train L1 norm: 1.266.. Val L1 norm: 1.036.. Train Linf norm: 109.965.. Val Linf norm: 33.885\n",
            "Epoch 128/136.. Train loss: 63.879.. Val loss: 55.234.. Train L1 norm: 1.312.. Val L1 norm: 1.036.. Train Linf norm: 315.421.. Val Linf norm: 33.889\n",
            "Epoch 129/136.. Train loss: 154.675.. Val loss: 55.236.. Train L1 norm: 1.169.. Val L1 norm: 1.036.. Train Linf norm: 168.921.. Val Linf norm: 33.899\n",
            "Epoch 130/136.. Train loss: 61.948.. Val loss: 55.235.. Train L1 norm: 1.736.. Val L1 norm: 1.036.. Train Linf norm: 751.823.. Val Linf norm: 33.894\n",
            "Epoch 131/136.. Train loss: 60.488.. Val loss: 55.235.. Train L1 norm: 1.540.. Val L1 norm: 1.036.. Train Linf norm: 550.153.. Val Linf norm: 33.892\n",
            "Epoch 132/136.. Train loss: 138.156.. Val loss: 55.233.. Train L1 norm: 1.713.. Val L1 norm: 1.036.. Train Linf norm: 727.330.. Val Linf norm: 33.882\n",
            "Epoch 133/136.. Train loss: 126.919.. Val loss: 55.232.. Train L1 norm: 1.803.. Val L1 norm: 1.036.. Train Linf norm: 819.297.. Val Linf norm: 33.871\n",
            "Epoch 134/136.. Train loss: 63.586.. Val loss: 55.231.. Train L1 norm: 2.193.. Val L1 norm: 1.036.. Train Linf norm: 1217.182.. Val Linf norm: 33.867\n",
            "Epoch 135/136.. Train loss: 61.959.. Val loss: 55.231.. Train L1 norm: 1.069.. Val L1 norm: 1.036.. Train Linf norm: 67.445.. Val Linf norm: 33.861\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:32:08,984]\u001b[0m Trial 99 finished with value: 1.0359791986147562 and parameters: {'n_layers': 7, 'n_units_0': 3841, 'n_units_1': 591, 'n_units_2': 4020, 'n_units_3': 671, 'n_units_4': 1384, 'n_units_5': 784, 'n_units_6': 500, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'RMSprop', 'lr': 1.8992419007517338e-06, 'batch_size': 1024, 'n_epochs': 136, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.1978005574826646, 'dropout_rate': 0.09423334657181064, 'factor': 0.128477941201901, 'patience': 8, 'threshold': 0.0010292381232802734}. Best is trial 92 with value: 1.0098264760335287.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 136/136.. Train loss: 70.731.. Val loss: 55.232.. Train L1 norm: 1.882.. Val L1 norm: 1.036.. Train Linf norm: 899.988.. Val Linf norm: 33.868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:32:11,289]\u001b[0m Trial 100 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/136.. Train loss: 4.589.. Val loss: 4.181.. Train L1 norm: 11.772.. Val L1 norm: 1.494.. Train Linf norm: 10925.302.. Val Linf norm: 283.310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:32:15,062]\u001b[0m Trial 101 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/127.. Train loss: 3.018.. Val loss: 2.134.. Train L1 norm: 11.433.. Val L1 norm: 2.522.. Train Linf norm: 9501.180.. Val Linf norm: 742.749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:32:16,619]\u001b[0m Trial 102 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/141.. Train loss: 1046.716.. Val loss: 52.956.. Train L1 norm: 5.900.. Val L1 norm: 1.140.. Train Linf norm: 4960.424.. Val Linf norm: 99.565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:32:18,092]\u001b[0m Trial 103 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150.. Train loss: 7415.611.. Val loss: 49.999.. Train L1 norm: 8.676.. Val L1 norm: 1.279.. Train Linf norm: 7751.899.. Val Linf norm: 171.548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:32:20,803]\u001b[0m Trial 104 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/144.. Train loss: 3595.263.. Val loss: 53.143.. Train L1 norm: 4.900.. Val L1 norm: 1.119.. Train Linf norm: 3950.225.. Val Linf norm: 89.142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:32:27,536]\u001b[0m Trial 105 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/147.. Train loss: 6521.680.. Val loss: 50.757.. Train L1 norm: 8.922.. Val L1 norm: 1.287.. Train Linf norm: 506.366.. Val Linf norm: 18.787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:32:30,098]\u001b[0m Trial 106 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/133.. Train loss: 469.633.. Val loss: 53.670.. Train L1 norm: 2.267.. Val L1 norm: 1.054.. Train Linf norm: 1291.884.. Val Linf norm: 41.180\n",
            "Epoch 1/139.. Train loss: 71.490.. Val loss: 53.851.. Train L1 norm: 2.432.. Val L1 norm: 1.044.. Train Linf norm: 2909.455.. Val Linf norm: 54.607\n",
            "Epoch 2/139.. Train loss: 672.699.. Val loss: 54.336.. Train L1 norm: 2.546.. Val L1 norm: 1.032.. Train Linf norm: 3123.087.. Val Linf norm: 46.669\n",
            "Epoch 3/139.. Train loss: 69.450.. Val loss: 54.699.. Train L1 norm: 1.563.. Val L1 norm: 1.033.. Train Linf norm: 1132.623.. Val Linf norm: 53.979\n",
            "Epoch 4/139.. Train loss: 74.252.. Val loss: 54.514.. Train L1 norm: 2.044.. Val L1 norm: 1.032.. Train Linf norm: 2120.726.. Val Linf norm: 51.342\n",
            "Epoch 5/139.. Train loss: 253.981.. Val loss: 55.129.. Train L1 norm: 1.853.. Val L1 norm: 1.039.. Train Linf norm: 1730.481.. Val Linf norm: 63.403\n",
            "Epoch 6/139.. Train loss: 130.008.. Val loss: 54.979.. Train L1 norm: 2.376.. Val L1 norm: 1.037.. Train Linf norm: 2804.163.. Val Linf norm: 60.234\n",
            "Epoch 7/139.. Train loss: 67.298.. Val loss: 54.588.. Train L1 norm: 1.401.. Val L1 norm: 1.033.. Train Linf norm: 171.603.. Val Linf norm: 52.457\n",
            "Epoch 8/139.. Train loss: 174.621.. Val loss: 55.085.. Train L1 norm: 1.123.. Val L1 norm: 1.038.. Train Linf norm: 237.752.. Val Linf norm: 61.484\n",
            "Epoch 9/139.. Train loss: 172.447.. Val loss: 55.671.. Train L1 norm: 1.516.. Val L1 norm: 1.051.. Train Linf norm: 1037.384.. Val Linf norm: 76.327\n",
            "Epoch 10/139.. Train loss: 72.645.. Val loss: 55.627.. Train L1 norm: 1.255.. Val L1 norm: 1.049.. Train Linf norm: 495.047.. Val Linf norm: 74.162\n",
            "Epoch 11/139.. Train loss: 229.736.. Val loss: 54.917.. Train L1 norm: 1.681.. Val L1 norm: 1.035.. Train Linf norm: 1377.425.. Val Linf norm: 56.665\n",
            "Epoch 12/139.. Train loss: 66.455.. Val loss: 55.011.. Train L1 norm: 1.269.. Val L1 norm: 1.036.. Train Linf norm: 533.089.. Val Linf norm: 58.637\n",
            "Epoch 13/139.. Train loss: 175.756.. Val loss: 54.696.. Train L1 norm: 1.959.. Val L1 norm: 1.031.. Train Linf norm: 1950.465.. Val Linf norm: 50.618\n",
            "Epoch 14/139.. Train loss: 59.949.. Val loss: 54.607.. Train L1 norm: 2.941.. Val L1 norm: 1.030.. Train Linf norm: 3958.750.. Val Linf norm: 48.589\n",
            "Epoch 15/139.. Train loss: 98.274.. Val loss: 54.808.. Train L1 norm: 2.389.. Val L1 norm: 1.031.. Train Linf norm: 2826.963.. Val Linf norm: 49.956\n",
            "Epoch 16/139.. Train loss: 65.863.. Val loss: 55.045.. Train L1 norm: 1.903.. Val L1 norm: 1.033.. Train Linf norm: 1838.479.. Val Linf norm: 54.156\n",
            "Epoch 17/139.. Train loss: 60.835.. Val loss: 55.033.. Train L1 norm: 1.430.. Val L1 norm: 1.033.. Train Linf norm: 865.942.. Val Linf norm: 54.073\n",
            "Epoch 18/139.. Train loss: 60.694.. Val loss: 54.959.. Train L1 norm: 2.481.. Val L1 norm: 1.033.. Train Linf norm: 3019.499.. Val Linf norm: 52.740\n",
            "Epoch 19/139.. Train loss: 64.771.. Val loss: 54.897.. Train L1 norm: 1.913.. Val L1 norm: 1.032.. Train Linf norm: 1852.404.. Val Linf norm: 51.556\n",
            "Epoch 20/139.. Train loss: 139.396.. Val loss: 55.145.. Train L1 norm: 3.470.. Val L1 norm: 1.035.. Train Linf norm: 5046.847.. Val Linf norm: 55.845\n",
            "Epoch 21/139.. Train loss: 97.298.. Val loss: 55.046.. Train L1 norm: 1.734.. Val L1 norm: 1.033.. Train Linf norm: 1490.622.. Val Linf norm: 53.495\n",
            "Epoch 22/139.. Train loss: 60.356.. Val loss: 54.938.. Train L1 norm: 3.004.. Val L1 norm: 1.032.. Train Linf norm: 4086.510.. Val Linf norm: 51.469\n",
            "Epoch 23/139.. Train loss: 100.065.. Val loss: 55.287.. Train L1 norm: 2.428.. Val L1 norm: 1.037.. Train Linf norm: 2910.120.. Val Linf norm: 58.266\n",
            "Epoch 24/139.. Train loss: 104.743.. Val loss: 54.924.. Train L1 norm: 2.753.. Val L1 norm: 1.033.. Train Linf norm: 3570.279.. Val Linf norm: 52.172\n",
            "Epoch 25/139.. Train loss: 84.438.. Val loss: 54.505.. Train L1 norm: 1.449.. Val L1 norm: 1.030.. Train Linf norm: 900.052.. Val Linf norm: 45.911\n",
            "Epoch 26/139.. Train loss: 245.187.. Val loss: 55.119.. Train L1 norm: 1.752.. Val L1 norm: 1.037.. Train Linf norm: 314.873.. Val Linf norm: 58.317\n",
            "Epoch 27/139.. Train loss: 71.967.. Val loss: 55.044.. Train L1 norm: 1.631.. Val L1 norm: 1.036.. Train Linf norm: 1274.243.. Val Linf norm: 57.350\n",
            "Epoch 28/139.. Train loss: 156.587.. Val loss: 54.415.. Train L1 norm: 2.736.. Val L1 norm: 1.036.. Train Linf norm: 3536.814.. Val Linf norm: 51.898\n",
            "Epoch 29/139.. Train loss: 176.064.. Val loss: 55.038.. Train L1 norm: 1.718.. Val L1 norm: 1.038.. Train Linf norm: 1446.043.. Val Linf norm: 59.444\n",
            "Epoch 30/139.. Train loss: 60.854.. Val loss: 55.033.. Train L1 norm: 2.128.. Val L1 norm: 1.038.. Train Linf norm: 2290.223.. Val Linf norm: 59.500\n",
            "Epoch 31/139.. Train loss: 122.380.. Val loss: 54.568.. Train L1 norm: 2.807.. Val L1 norm: 1.032.. Train Linf norm: 3681.794.. Val Linf norm: 49.448\n",
            "Epoch 32/139.. Train loss: 72.058.. Val loss: 54.615.. Train L1 norm: 3.142.. Val L1 norm: 1.032.. Train Linf norm: 4358.203.. Val Linf norm: 49.426\n",
            "Epoch 33/139.. Train loss: 88.996.. Val loss: 54.878.. Train L1 norm: 2.789.. Val L1 norm: 1.035.. Train Linf norm: 706.453.. Val Linf norm: 55.312\n",
            "Epoch 34/139.. Train loss: 60.412.. Val loss: 55.101.. Train L1 norm: 1.686.. Val L1 norm: 1.039.. Train Linf norm: 1387.554.. Val Linf norm: 60.596\n",
            "Epoch 35/139.. Train loss: 166.739.. Val loss: 54.500.. Train L1 norm: 1.339.. Val L1 norm: 1.037.. Train Linf norm: 675.840.. Val Linf norm: 56.045\n",
            "Epoch 36/139.. Train loss: 164.884.. Val loss: 54.908.. Train L1 norm: 1.534.. Val L1 norm: 1.035.. Train Linf norm: 1072.016.. Val Linf norm: 55.299\n",
            "Epoch 37/139.. Train loss: 60.701.. Val loss: 54.888.. Train L1 norm: 1.662.. Val L1 norm: 1.035.. Train Linf norm: 1337.567.. Val Linf norm: 55.289\n",
            "Epoch 38/139.. Train loss: 68.928.. Val loss: 54.752.. Train L1 norm: 1.835.. Val L1 norm: 1.034.. Train Linf norm: 1688.441.. Val Linf norm: 53.146\n",
            "Epoch 39/139.. Train loss: 60.634.. Val loss: 54.646.. Train L1 norm: 4.242.. Val L1 norm: 1.036.. Train Linf norm: 6616.581.. Val Linf norm: 54.670\n",
            "Epoch 40/139.. Train loss: 179.407.. Val loss: 55.340.. Train L1 norm: 2.287.. Val L1 norm: 1.040.. Train Linf norm: 2607.943.. Val Linf norm: 61.582\n",
            "Epoch 41/139.. Train loss: 327.585.. Val loss: 54.599.. Train L1 norm: 1.633.. Val L1 norm: 1.037.. Train Linf norm: 1274.105.. Val Linf norm: 56.274\n",
            "Epoch 42/139.. Train loss: 108.041.. Val loss: 54.070.. Train L1 norm: 2.489.. Val L1 norm: 1.059.. Train Linf norm: 3018.761.. Val Linf norm: 81.996\n",
            "Epoch 43/139.. Train loss: 242.412.. Val loss: 54.229.. Train L1 norm: 2.082.. Val L1 norm: 1.052.. Train Linf norm: 2171.935.. Val Linf norm: 73.989\n",
            "Epoch 44/139.. Train loss: 60.589.. Val loss: 54.580.. Train L1 norm: 3.617.. Val L1 norm: 1.038.. Train Linf norm: 5333.888.. Val Linf norm: 56.559\n",
            "Epoch 45/139.. Train loss: 59.792.. Val loss: 54.537.. Train L1 norm: 4.914.. Val L1 norm: 1.040.. Train Linf norm: 7987.840.. Val Linf norm: 59.069\n",
            "Epoch 46/139.. Train loss: 60.480.. Val loss: 54.577.. Train L1 norm: 4.247.. Val L1 norm: 1.040.. Train Linf norm: 6616.601.. Val Linf norm: 58.282\n",
            "Epoch 47/139.. Train loss: 60.293.. Val loss: 54.523.. Train L1 norm: 1.642.. Val L1 norm: 1.042.. Train Linf norm: 1289.627.. Val Linf norm: 61.316\n",
            "Epoch 48/139.. Train loss: 70.523.. Val loss: 54.333.. Train L1 norm: 3.438.. Val L1 norm: 1.049.. Train Linf norm: 4966.518.. Val Linf norm: 71.041\n",
            "Epoch 49/139.. Train loss: 59.738.. Val loss: 54.310.. Train L1 norm: 1.318.. Val L1 norm: 1.051.. Train Linf norm: 619.383.. Val Linf norm: 72.845\n",
            "Epoch 50/139.. Train loss: 59.546.. Val loss: 54.290.. Train L1 norm: 3.045.. Val L1 norm: 1.052.. Train Linf norm: 4154.563.. Val Linf norm: 74.452\n",
            "Epoch 51/139.. Train loss: 62.021.. Val loss: 54.166.. Train L1 norm: 3.657.. Val L1 norm: 1.057.. Train Linf norm: 5395.410.. Val Linf norm: 80.520\n",
            "Epoch 52/139.. Train loss: 101.115.. Val loss: 53.768.. Train L1 norm: 3.582.. Val L1 norm: 1.075.. Train Linf norm: 5242.079.. Val Linf norm: 99.109\n",
            "Epoch 53/139.. Train loss: 220.790.. Val loss: 54.080.. Train L1 norm: 3.150.. Val L1 norm: 1.061.. Train Linf norm: 4343.238.. Val Linf norm: 85.017\n",
            "Epoch 54/139.. Train loss: 62.964.. Val loss: 54.017.. Train L1 norm: 2.922.. Val L1 norm: 1.065.. Train Linf norm: 3891.258.. Val Linf norm: 88.803\n",
            "Epoch 55/139.. Train loss: 59.391.. Val loss: 53.992.. Train L1 norm: 3.335.. Val L1 norm: 1.066.. Train Linf norm: 917.515.. Val Linf norm: 90.616\n",
            "Epoch 56/139.. Train loss: 71.083.. Val loss: 54.127.. Train L1 norm: 3.227.. Val L1 norm: 1.061.. Train Linf norm: 4520.767.. Val Linf norm: 85.772\n",
            "Epoch 57/139.. Train loss: 200.361.. Val loss: 54.732.. Train L1 norm: 1.760.. Val L1 norm: 1.041.. Train Linf norm: 1523.867.. Val Linf norm: 62.254\n",
            "Epoch 58/139.. Train loss: 189.154.. Val loss: 54.302.. Train L1 norm: 2.474.. Val L1 norm: 1.057.. Train Linf norm: 2975.115.. Val Linf norm: 82.796\n",
            "Epoch 59/139.. Train loss: 60.326.. Val loss: 54.253.. Train L1 norm: 4.057.. Val L1 norm: 1.060.. Train Linf norm: 6220.460.. Val Linf norm: 85.907\n",
            "Epoch 60/139.. Train loss: 68.126.. Val loss: 54.096.. Train L1 norm: 2.432.. Val L1 norm: 1.068.. Train Linf norm: 2896.220.. Val Linf norm: 94.759\n",
            "Epoch 61/139.. Train loss: 171.451.. Val loss: 54.345.. Train L1 norm: 4.412.. Val L1 norm: 1.058.. Train Linf norm: 6938.282.. Val Linf norm: 85.050\n",
            "Epoch 62/139.. Train loss: 77.204.. Val loss: 54.295.. Train L1 norm: 2.278.. Val L1 norm: 1.060.. Train Linf norm: 2568.634.. Val Linf norm: 87.458\n",
            "Epoch 63/139.. Train loss: 182.987.. Val loss: 54.748.. Train L1 norm: 1.827.. Val L1 norm: 1.043.. Train Linf norm: 1660.853.. Val Linf norm: 65.863\n",
            "Epoch 64/139.. Train loss: 78.566.. Val loss: 54.633.. Train L1 norm: 2.060.. Val L1 norm: 1.047.. Train Linf norm: 2145.013.. Val Linf norm: 72.114\n",
            "Epoch 65/139.. Train loss: 65.958.. Val loss: 54.663.. Train L1 norm: 3.067.. Val L1 norm: 1.047.. Train Linf norm: 4206.112.. Val Linf norm: 70.971\n",
            "Epoch 66/139.. Train loss: 203.979.. Val loss: 54.166.. Train L1 norm: 3.644.. Val L1 norm: 1.065.. Train Linf norm: 5371.824.. Val Linf norm: 92.525\n",
            "Epoch 67/139.. Train loss: 62.157.. Val loss: 54.195.. Train L1 norm: 3.377.. Val L1 norm: 1.064.. Train Linf norm: 4827.618.. Val Linf norm: 91.137\n",
            "Epoch 68/139.. Train loss: 62.819.. Val loss: 54.173.. Train L1 norm: 2.276.. Val L1 norm: 1.065.. Train Linf norm: 2568.300.. Val Linf norm: 92.728\n",
            "Epoch 69/139.. Train loss: 93.361.. Val loss: 54.209.. Train L1 norm: 3.598.. Val L1 norm: 1.064.. Train Linf norm: 5278.163.. Val Linf norm: 91.501\n",
            "Epoch 70/139.. Train loss: 150.680.. Val loss: 54.070.. Train L1 norm: 3.595.. Val L1 norm: 1.070.. Train Linf norm: 5277.714.. Val Linf norm: 98.392\n",
            "Epoch 71/139.. Train loss: 78.407.. Val loss: 53.984.. Train L1 norm: 2.955.. Val L1 norm: 1.075.. Train Linf norm: 3938.603.. Val Linf norm: 102.784\n",
            "Epoch 72/139.. Train loss: 61.180.. Val loss: 54.064.. Train L1 norm: 2.524.. Val L1 norm: 1.072.. Train Linf norm: 3074.003.. Val Linf norm: 100.195\n",
            "Epoch 73/139.. Train loss: 61.564.. Val loss: 54.108.. Train L1 norm: 3.371.. Val L1 norm: 1.071.. Train Linf norm: 4807.151.. Val Linf norm: 98.986\n",
            "Epoch 74/139.. Train loss: 60.402.. Val loss: 54.115.. Train L1 norm: 3.856.. Val L1 norm: 1.071.. Train Linf norm: 5807.050.. Val Linf norm: 99.590\n",
            "Epoch 75/139.. Train loss: 76.798.. Val loss: 53.920.. Train L1 norm: 4.474.. Val L1 norm: 1.080.. Train Linf norm: 7064.479.. Val Linf norm: 109.540\n",
            "Epoch 76/139.. Train loss: 61.053.. Val loss: 53.917.. Train L1 norm: 3.649.. Val L1 norm: 1.081.. Train Linf norm: 5368.476.. Val Linf norm: 110.322\n",
            "Epoch 77/139.. Train loss: 121.281.. Val loss: 53.744.. Train L1 norm: 4.013.. Val L1 norm: 1.090.. Train Linf norm: 6114.781.. Val Linf norm: 120.564\n",
            "Epoch 78/139.. Train loss: 291.284.. Val loss: 54.168.. Train L1 norm: 5.440.. Val L1 norm: 1.071.. Train Linf norm: 9028.451.. Val Linf norm: 100.781\n",
            "Epoch 79/139.. Train loss: 68.541.. Val loss: 54.143.. Train L1 norm: 4.339.. Val L1 norm: 1.072.. Train Linf norm: 6792.228.. Val Linf norm: 102.985\n",
            "Epoch 80/139.. Train loss: 128.736.. Val loss: 54.227.. Train L1 norm: 4.330.. Val L1 norm: 1.069.. Train Linf norm: 6741.278.. Val Linf norm: 99.552\n",
            "Epoch 81/139.. Train loss: 77.053.. Val loss: 54.514.. Train L1 norm: 3.232.. Val L1 norm: 1.058.. Train Linf norm: 4513.776.. Val Linf norm: 86.459\n",
            "Epoch 82/139.. Train loss: 134.385.. Val loss: 54.050.. Train L1 norm: 3.030.. Val L1 norm: 1.079.. Train Linf norm: 4113.450.. Val Linf norm: 109.868\n",
            "Epoch 83/139.. Train loss: 61.142.. Val loss: 53.995.. Train L1 norm: 4.351.. Val L1 norm: 1.082.. Train Linf norm: 6806.619.. Val Linf norm: 113.260\n",
            "Epoch 84/139.. Train loss: 68.672.. Val loss: 53.991.. Train L1 norm: 3.087.. Val L1 norm: 1.082.. Train Linf norm: 4219.283.. Val Linf norm: 113.894\n",
            "Epoch 85/139.. Train loss: 76.844.. Val loss: 53.892.. Train L1 norm: 3.190.. Val L1 norm: 1.087.. Train Linf norm: 4432.382.. Val Linf norm: 119.565\n",
            "Epoch 86/139.. Train loss: 86.284.. Val loss: 54.056.. Train L1 norm: 4.244.. Val L1 norm: 1.078.. Train Linf norm: 6587.367.. Val Linf norm: 109.041\n",
            "Epoch 87/139.. Train loss: 162.698.. Val loss: 54.412.. Train L1 norm: 3.430.. Val L1 norm: 1.062.. Train Linf norm: 4903.627.. Val Linf norm: 90.480\n",
            "Epoch 88/139.. Train loss: 125.400.. Val loss: 54.161.. Train L1 norm: 3.551.. Val L1 norm: 1.073.. Train Linf norm: 5183.220.. Val Linf norm: 103.968\n",
            "Epoch 89/139.. Train loss: 68.197.. Val loss: 53.932.. Train L1 norm: 2.100.. Val L1 norm: 1.084.. Train Linf norm: 2202.559.. Val Linf norm: 116.070\n",
            "Epoch 90/139.. Train loss: 60.552.. Val loss: 53.854.. Train L1 norm: 4.382.. Val L1 norm: 1.088.. Train Linf norm: 6871.379.. Val Linf norm: 121.125\n",
            "Epoch 91/139.. Train loss: 90.061.. Val loss: 54.123.. Train L1 norm: 3.101.. Val L1 norm: 1.077.. Train Linf norm: 842.152.. Val Linf norm: 108.912\n",
            "Epoch 92/139.. Train loss: 59.772.. Val loss: 54.162.. Train L1 norm: 4.317.. Val L1 norm: 1.076.. Train Linf norm: 6748.132.. Val Linf norm: 107.837\n",
            "Epoch 93/139.. Train loss: 141.359.. Val loss: 53.843.. Train L1 norm: 4.006.. Val L1 norm: 1.091.. Train Linf norm: 6079.236.. Val Linf norm: 124.771\n",
            "Epoch 94/139.. Train loss: 184.177.. Val loss: 53.959.. Train L1 norm: 3.518.. Val L1 norm: 1.086.. Train Linf norm: 5087.291.. Val Linf norm: 120.065\n",
            "Epoch 95/139.. Train loss: 103.824.. Val loss: 53.933.. Train L1 norm: 4.903.. Val L1 norm: 1.088.. Train Linf norm: 7940.534.. Val Linf norm: 121.732\n",
            "Epoch 96/139.. Train loss: 114.254.. Val loss: 53.744.. Train L1 norm: 4.625.. Val L1 norm: 1.098.. Train Linf norm: 7355.005.. Val Linf norm: 132.869\n",
            "Epoch 97/139.. Train loss: 79.819.. Val loss: 53.738.. Train L1 norm: 2.795.. Val L1 norm: 1.099.. Train Linf norm: 3602.167.. Val Linf norm: 134.559\n",
            "Epoch 98/139.. Train loss: 73.005.. Val loss: 53.955.. Train L1 norm: 3.844.. Val L1 norm: 1.089.. Train Linf norm: 5755.608.. Val Linf norm: 123.941\n",
            "Epoch 99/139.. Train loss: 78.831.. Val loss: 53.674.. Train L1 norm: 3.983.. Val L1 norm: 1.104.. Train Linf norm: 6034.699.. Val Linf norm: 140.405\n",
            "Epoch 100/139.. Train loss: 182.379.. Val loss: 54.130.. Train L1 norm: 4.322.. Val L1 norm: 1.084.. Train Linf norm: 6736.076.. Val Linf norm: 119.578\n",
            "Epoch 101/139.. Train loss: 59.819.. Val loss: 54.232.. Train L1 norm: 3.036.. Val L1 norm: 1.081.. Train Linf norm: 4082.673.. Val Linf norm: 116.054\n",
            "Epoch 102/139.. Train loss: 94.550.. Val loss: 54.191.. Train L1 norm: 3.347.. Val L1 norm: 1.083.. Train Linf norm: 4758.059.. Val Linf norm: 118.671\n",
            "Epoch 103/139.. Train loss: 59.334.. Val loss: 53.975.. Train L1 norm: 4.226.. Val L1 norm: 1.093.. Train Linf norm: 6548.646.. Val Linf norm: 130.014\n",
            "Epoch 104/139.. Train loss: 60.867.. Val loss: 53.988.. Train L1 norm: 3.723.. Val L1 norm: 1.094.. Train Linf norm: 5499.159.. Val Linf norm: 130.490\n",
            "Epoch 105/139.. Train loss: 59.546.. Val loss: 54.003.. Train L1 norm: 3.998.. Val L1 norm: 1.094.. Train Linf norm: 6078.886.. Val Linf norm: 130.680\n",
            "Epoch 106/139.. Train loss: 71.134.. Val loss: 53.890.. Train L1 norm: 5.158.. Val L1 norm: 1.099.. Train Linf norm: 8443.981.. Val Linf norm: 136.341\n",
            "Epoch 107/139.. Train loss: 85.354.. Val loss: 53.657.. Train L1 norm: 4.784.. Val L1 norm: 1.109.. Train Linf norm: 7675.185.. Val Linf norm: 147.167\n",
            "Epoch 108/139.. Train loss: 96.647.. Val loss: 53.749.. Train L1 norm: 4.409.. Val L1 norm: 1.105.. Train Linf norm: 6895.566.. Val Linf norm: 142.790\n",
            "Epoch 109/139.. Train loss: 59.927.. Val loss: 53.731.. Train L1 norm: 3.168.. Val L1 norm: 1.107.. Train Linf norm: 4366.698.. Val Linf norm: 144.521\n",
            "Epoch 110/139.. Train loss: 60.966.. Val loss: 53.767.. Train L1 norm: 4.715.. Val L1 norm: 1.106.. Train Linf norm: 7529.811.. Val Linf norm: 143.674\n",
            "Epoch 111/139.. Train loss: 85.263.. Val loss: 54.012.. Train L1 norm: 3.851.. Val L1 norm: 1.095.. Train Linf norm: 5759.405.. Val Linf norm: 132.542\n",
            "Epoch 112/139.. Train loss: 143.560.. Val loss: 53.594.. Train L1 norm: 4.542.. Val L1 norm: 1.113.. Train Linf norm: 7183.311.. Val Linf norm: 151.512\n",
            "Epoch 113/139.. Train loss: 106.941.. Val loss: 53.741.. Train L1 norm: 4.813.. Val L1 norm: 1.107.. Train Linf norm: 7693.630.. Val Linf norm: 144.368\n",
            "Epoch 114/139.. Train loss: 59.537.. Val loss: 53.884.. Train L1 norm: 3.204.. Val L1 norm: 1.100.. Train Linf norm: 4439.282.. Val Linf norm: 137.718\n",
            "Epoch 115/139.. Train loss: 73.919.. Val loss: 54.082.. Train L1 norm: 4.048.. Val L1 norm: 1.093.. Train Linf norm: 6164.210.. Val Linf norm: 130.275\n",
            "Epoch 116/139.. Train loss: 61.215.. Val loss: 54.049.. Train L1 norm: 4.739.. Val L1 norm: 1.094.. Train Linf norm: 7591.978.. Val Linf norm: 132.261\n",
            "Epoch 117/139.. Train loss: 386.711.. Val loss: 53.171.. Train L1 norm: 5.017.. Val L1 norm: 1.136.. Train Linf norm: 8144.042.. Val Linf norm: 174.894\n",
            "Epoch 118/139.. Train loss: 214.791.. Val loss: 53.701.. Train L1 norm: 4.244.. Val L1 norm: 1.109.. Train Linf norm: 6554.821.. Val Linf norm: 147.330\n",
            "Epoch 119/139.. Train loss: 126.457.. Val loss: 54.136.. Train L1 norm: 3.753.. Val L1 norm: 1.092.. Train Linf norm: 5573.259.. Val Linf norm: 130.130\n",
            "Epoch 120/139.. Train loss: 89.161.. Val loss: 54.026.. Train L1 norm: 4.391.. Val L1 norm: 1.098.. Train Linf norm: 6890.325.. Val Linf norm: 136.702\n",
            "Epoch 121/139.. Train loss: 60.128.. Val loss: 53.830.. Train L1 norm: 3.463.. Val L1 norm: 1.107.. Train Linf norm: 4930.974.. Val Linf norm: 147.339\n",
            "Epoch 122/139.. Train loss: 77.814.. Val loss: 53.642.. Train L1 norm: 3.897.. Val L1 norm: 1.117.. Train Linf norm: 5854.762.. Val Linf norm: 157.327\n",
            "Epoch 123/139.. Train loss: 64.821.. Val loss: 53.601.. Train L1 norm: 4.553.. Val L1 norm: 1.119.. Train Linf norm: 1390.403.. Val Linf norm: 160.230\n",
            "Epoch 124/139.. Train loss: 194.514.. Val loss: 54.050.. Train L1 norm: 4.827.. Val L1 norm: 1.102.. Train Linf norm: 7757.380.. Val Linf norm: 141.989\n",
            "Epoch 125/139.. Train loss: 173.232.. Val loss: 53.642.. Train L1 norm: 4.001.. Val L1 norm: 1.120.. Train Linf norm: 6049.431.. Val Linf norm: 160.031\n",
            "Epoch 126/139.. Train loss: 88.736.. Val loss: 53.710.. Train L1 norm: 4.706.. Val L1 norm: 1.117.. Train Linf norm: 7503.625.. Val Linf norm: 156.873\n",
            "Epoch 127/139.. Train loss: 66.321.. Val loss: 53.749.. Train L1 norm: 3.980.. Val L1 norm: 1.116.. Train Linf norm: 6021.076.. Val Linf norm: 155.570\n",
            "Epoch 128/139.. Train loss: 61.592.. Val loss: 53.803.. Train L1 norm: 3.706.. Val L1 norm: 1.113.. Train Linf norm: 5464.821.. Val Linf norm: 153.019\n",
            "Epoch 129/139.. Train loss: 59.299.. Val loss: 53.783.. Train L1 norm: 3.905.. Val L1 norm: 1.115.. Train Linf norm: 5869.452.. Val Linf norm: 154.676\n",
            "Epoch 130/139.. Train loss: 99.116.. Val loss: 53.605.. Train L1 norm: 2.832.. Val L1 norm: 1.124.. Train Linf norm: 3671.181.. Val Linf norm: 164.987\n",
            "Epoch 131/139.. Train loss: 102.159.. Val loss: 53.666.. Train L1 norm: 3.941.. Val L1 norm: 1.122.. Train Linf norm: 5927.405.. Val Linf norm: 162.176\n",
            "Epoch 132/139.. Train loss: 61.525.. Val loss: 53.713.. Train L1 norm: 4.610.. Val L1 norm: 1.120.. Train Linf norm: 7296.568.. Val Linf norm: 160.390\n",
            "Epoch 133/139.. Train loss: 77.971.. Val loss: 53.773.. Train L1 norm: 4.513.. Val L1 norm: 1.118.. Train Linf norm: 7099.288.. Val Linf norm: 158.495\n",
            "Epoch 134/139.. Train loss: 157.291.. Val loss: 53.613.. Train L1 norm: 4.567.. Val L1 norm: 1.125.. Train Linf norm: 7222.051.. Val Linf norm: 165.225\n",
            "Epoch 135/139.. Train loss: 131.870.. Val loss: 53.791.. Train L1 norm: 5.899.. Val L1 norm: 1.117.. Train Linf norm: 9944.691.. Val Linf norm: 157.280\n",
            "Epoch 136/139.. Train loss: 59.317.. Val loss: 53.852.. Train L1 norm: 4.712.. Val L1 norm: 1.114.. Train Linf norm: 7516.142.. Val Linf norm: 155.400\n",
            "Epoch 137/139.. Train loss: 64.183.. Val loss: 53.763.. Train L1 norm: 3.761.. Val L1 norm: 1.119.. Train Linf norm: 5554.304.. Val Linf norm: 160.425\n",
            "Epoch 138/139.. Train loss: 60.365.. Val loss: 53.684.. Train L1 norm: 4.954.. Val L1 norm: 1.123.. Train Linf norm: 8011.114.. Val Linf norm: 164.817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:36:55,379]\u001b[0m Trial 107 finished with value: 1.1248441493988037 and parameters: {'n_layers': 6, 'n_units_0': 3690, 'n_units_1': 391, 'n_units_2': 3632, 'n_units_3': 1073, 'n_units_4': 139, 'n_units_5': 1446, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 1.2845930148019806e-06, 'batch_size': 2048, 'n_epochs': 139, 'scheduler': 'CosineAnnealingLR', 'prelu_init': 0.20672591738955545, 'dropout_rate': 0.05778031297857033, 'weight_decay': 0.0007908496514976405, 'beta1': 0.9192107264391448, 'beta2': 0.9991412690935637, 't_max_fraction': 0.2399277774663584, 'eta_min': 1.6118430770716082e-06}. Best is trial 92 with value: 1.0098264760335287.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 139/139.. Train loss: 59.171.. Val loss: 53.662.. Train L1 norm: 3.884.. Val L1 norm: 1.125.. Train Linf norm: 5817.241.. Val Linf norm: 166.528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:36:57,770]\u001b[0m Trial 108 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/142.. Train loss: 634.687.. Val loss: 58.772.. Train L1 norm: 5.798.. Val L1 norm: 1.129.. Train Linf norm: 4833.158.. Val Linf norm: 78.602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:36:59,935]\u001b[0m Trial 109 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/147.. Train loss: 14728.782.. Val loss: 53.035.. Train L1 norm: 17.669.. Val L1 norm: 1.257.. Train Linf norm: 16894.423.. Val Linf norm: 161.653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:37:07,992]\u001b[0m Trial 110 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/138.. Train loss: 2673.762.. Val loss: 46.498.. Train L1 norm: 24.492.. Val L1 norm: 2.785.. Train Linf norm: 2983.025.. Val Linf norm: 162.502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:37:30,107]\u001b[0m Trial 111 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/144.. Train loss: 4081.533.. Val loss: 51.168.. Train L1 norm: 5.923.. Val L1 norm: 1.191.. Train Linf norm: 159.268.. Val Linf norm: 7.804\n",
            "Epoch 1/141.. Train loss: 61.221.. Val loss: 54.467.. Train L1 norm: 2.404.. Val L1 norm: 1.015.. Train Linf norm: 1430.777.. Val Linf norm: 12.876\n",
            "Epoch 2/141.. Train loss: 77.686.. Val loss: 53.789.. Train L1 norm: 2.677.. Val L1 norm: 1.043.. Train Linf norm: 1710.611.. Val Linf norm: 33.116\n",
            "Epoch 3/141.. Train loss: 153.061.. Val loss: 54.288.. Train L1 norm: 1.988.. Val L1 norm: 1.025.. Train Linf norm: 1005.504.. Val Linf norm: 20.305\n",
            "Epoch 4/141.. Train loss: 130.781.. Val loss: 53.722.. Train L1 norm: 5.347.. Val L1 norm: 1.048.. Train Linf norm: 4443.627.. Val Linf norm: 36.400\n",
            "Epoch 5/141.. Train loss: 128.429.. Val loss: 53.935.. Train L1 norm: 2.467.. Val L1 norm: 1.039.. Train Linf norm: 1491.775.. Val Linf norm: 30.289\n",
            "Epoch 6/141.. Train loss: 587.191.. Val loss: 54.902.. Train L1 norm: 1.994.. Val L1 norm: 1.012.. Train Linf norm: 1006.299.. Val Linf norm: 11.804\n",
            "Epoch 7/141.. Train loss: 74.598.. Val loss: 54.719.. Train L1 norm: 2.206.. Val L1 norm: 1.013.. Train Linf norm: 1231.962.. Val Linf norm: 11.439\n",
            "Epoch 8/141.. Train loss: 75.002.. Val loss: 54.474.. Train L1 norm: 3.234.. Val L1 norm: 1.018.. Train Linf norm: 2281.112.. Val Linf norm: 15.037\n",
            "Epoch 9/141.. Train loss: 187.560.. Val loss: 53.939.. Train L1 norm: 1.534.. Val L1 norm: 1.039.. Train Linf norm: 542.577.. Val Linf norm: 29.874\n",
            "Epoch 10/141.. Train loss: 175.129.. Val loss: 53.966.. Train L1 norm: 1.972.. Val L1 norm: 1.038.. Train Linf norm: 989.114.. Val Linf norm: 29.164\n",
            "Epoch 11/141.. Train loss: 201.412.. Val loss: 54.051.. Train L1 norm: 2.993.. Val L1 norm: 1.034.. Train Linf norm: 2035.643.. Val Linf norm: 26.734\n",
            "Epoch 12/141.. Train loss: 948.276.. Val loss: 54.164.. Train L1 norm: 1.604.. Val L1 norm: 1.029.. Train Linf norm: 613.655.. Val Linf norm: 23.544\n",
            "Epoch 13/141.. Train loss: 71.126.. Val loss: 54.181.. Train L1 norm: 1.214.. Val L1 norm: 1.029.. Train Linf norm: 215.474.. Val Linf norm: 23.134\n",
            "Epoch 14/141.. Train loss: 101.404.. Val loss: 54.212.. Train L1 norm: 2.782.. Val L1 norm: 1.028.. Train Linf norm: 1820.689.. Val Linf norm: 22.311\n",
            "Epoch 15/141.. Train loss: 277.423.. Val loss: 54.274.. Train L1 norm: 3.708.. Val L1 norm: 1.025.. Train Linf norm: 2768.349.. Val Linf norm: 20.638\n",
            "Epoch 16/141.. Train loss: 255.273.. Val loss: 54.333.. Train L1 norm: 1.052.. Val L1 norm: 1.023.. Train Linf norm: 47.764.. Val Linf norm: 19.211\n",
            "Epoch 17/141.. Train loss: 87.949.. Val loss: 54.335.. Train L1 norm: 1.359.. Val L1 norm: 1.023.. Train Linf norm: 363.822.. Val Linf norm: 19.167\n",
            "Epoch 18/141.. Train loss: 248.997.. Val loss: 54.342.. Train L1 norm: 2.591.. Val L1 norm: 1.023.. Train Linf norm: 1624.432.. Val Linf norm: 19.013\n",
            "Epoch 19/141.. Train loss: 580.493.. Val loss: 54.350.. Train L1 norm: 3.344.. Val L1 norm: 1.023.. Train Linf norm: 2395.065.. Val Linf norm: 18.777\n",
            "Epoch 20/141.. Train loss: 127.818.. Val loss: 54.351.. Train L1 norm: 1.733.. Val L1 norm: 1.023.. Train Linf norm: 746.020.. Val Linf norm: 18.762\n",
            "Epoch 21/141.. Train loss: 507.429.. Val loss: 54.362.. Train L1 norm: 1.175.. Val L1 norm: 1.022.. Train Linf norm: 175.792.. Val Linf norm: 18.443\n",
            "Epoch 22/141.. Train loss: 167.285.. Val loss: 54.368.. Train L1 norm: 2.212.. Val L1 norm: 1.022.. Train Linf norm: 1237.418.. Val Linf norm: 18.297\n",
            "Epoch 23/141.. Train loss: 71.706.. Val loss: 54.369.. Train L1 norm: 1.171.. Val L1 norm: 1.022.. Train Linf norm: 171.380.. Val Linf norm: 18.261\n",
            "Epoch 24/141.. Train loss: 117.355.. Val loss: 54.370.. Train L1 norm: 1.126.. Val L1 norm: 1.022.. Train Linf norm: 117.255.. Val Linf norm: 18.255\n",
            "Epoch 25/141.. Train loss: 213.187.. Val loss: 54.369.. Train L1 norm: 2.992.. Val L1 norm: 1.022.. Train Linf norm: 2036.499.. Val Linf norm: 18.265\n",
            "Epoch 26/141.. Train loss: 84.507.. Val loss: 54.369.. Train L1 norm: 1.552.. Val L1 norm: 1.022.. Train Linf norm: 561.453.. Val Linf norm: 18.275\n",
            "Epoch 27/141.. Train loss: 95.233.. Val loss: 54.369.. Train L1 norm: 2.705.. Val L1 norm: 1.022.. Train Linf norm: 1741.888.. Val Linf norm: 18.273\n",
            "Epoch 28/141.. Train loss: 113.751.. Val loss: 54.369.. Train L1 norm: 2.451.. Val L1 norm: 1.022.. Train Linf norm: 1482.241.. Val Linf norm: 18.268\n",
            "Epoch 29/141.. Train loss: 73.256.. Val loss: 54.369.. Train L1 norm: 1.822.. Val L1 norm: 1.022.. Train Linf norm: 835.858.. Val Linf norm: 18.270\n",
            "Epoch 30/141.. Train loss: 154.340.. Val loss: 54.369.. Train L1 norm: 1.285.. Val L1 norm: 1.022.. Train Linf norm: 286.424.. Val Linf norm: 18.268\n",
            "Epoch 31/141.. Train loss: 125.822.. Val loss: 54.370.. Train L1 norm: 3.046.. Val L1 norm: 1.022.. Train Linf norm: 2091.061.. Val Linf norm: 18.258\n",
            "Epoch 32/141.. Train loss: 107.970.. Val loss: 54.370.. Train L1 norm: 1.068.. Val L1 norm: 1.022.. Train Linf norm: 66.742.. Val Linf norm: 18.245\n",
            "Epoch 33/141.. Train loss: 487.649.. Val loss: 54.371.. Train L1 norm: 2.025.. Val L1 norm: 1.022.. Train Linf norm: 1046.354.. Val Linf norm: 18.223\n",
            "Epoch 34/141.. Train loss: 78.165.. Val loss: 54.371.. Train L1 norm: 2.996.. Val L1 norm: 1.022.. Train Linf norm: 2039.647.. Val Linf norm: 18.224\n",
            "Epoch 35/141.. Train loss: 337.391.. Val loss: 54.371.. Train L1 norm: 1.426.. Val L1 norm: 1.022.. Train Linf norm: 433.076.. Val Linf norm: 18.213\n",
            "Epoch 36/141.. Train loss: 80.027.. Val loss: 54.372.. Train L1 norm: 1.459.. Val L1 norm: 1.022.. Train Linf norm: 466.515.. Val Linf norm: 18.196\n",
            "Epoch 37/141.. Train loss: 129.392.. Val loss: 54.372.. Train L1 norm: 1.585.. Val L1 norm: 1.022.. Train Linf norm: 593.903.. Val Linf norm: 18.198\n",
            "Epoch 38/141.. Train loss: 80.147.. Val loss: 54.371.. Train L1 norm: 3.284.. Val L1 norm: 1.022.. Train Linf norm: 2335.729.. Val Linf norm: 18.225\n",
            "Epoch 39/141.. Train loss: 73.762.. Val loss: 54.371.. Train L1 norm: 1.869.. Val L1 norm: 1.022.. Train Linf norm: 885.111.. Val Linf norm: 18.221\n",
            "Epoch 40/141.. Train loss: 70.664.. Val loss: 54.371.. Train L1 norm: 3.673.. Val L1 norm: 1.022.. Train Linf norm: 2732.920.. Val Linf norm: 18.223\n",
            "Epoch 41/141.. Train loss: 59.952.. Val loss: 54.371.. Train L1 norm: 1.327.. Val L1 norm: 1.022.. Train Linf norm: 332.449.. Val Linf norm: 18.225\n",
            "Epoch 42/141.. Train loss: 178.105.. Val loss: 54.371.. Train L1 norm: 1.151.. Val L1 norm: 1.022.. Train Linf norm: 147.230.. Val Linf norm: 18.216\n",
            "Epoch 43/141.. Train loss: 60.514.. Val loss: 54.371.. Train L1 norm: 2.009.. Val L1 norm: 1.022.. Train Linf norm: 1029.441.. Val Linf norm: 18.216\n",
            "Epoch 44/141.. Train loss: 282.626.. Val loss: 54.372.. Train L1 norm: 2.807.. Val L1 norm: 1.022.. Train Linf norm: 1846.406.. Val Linf norm: 18.199\n",
            "Epoch 45/141.. Train loss: 387.229.. Val loss: 54.372.. Train L1 norm: 3.009.. Val L1 norm: 1.022.. Train Linf norm: 2053.633.. Val Linf norm: 18.192\n",
            "Epoch 46/141.. Train loss: 135.227.. Val loss: 54.373.. Train L1 norm: 3.129.. Val L1 norm: 1.022.. Train Linf norm: 2177.160.. Val Linf norm: 18.170\n",
            "Epoch 47/141.. Train loss: 113.941.. Val loss: 54.373.. Train L1 norm: 1.757.. Val L1 norm: 1.022.. Train Linf norm: 770.454.. Val Linf norm: 18.163\n",
            "Epoch 48/141.. Train loss: 60.189.. Val loss: 54.373.. Train L1 norm: 2.403.. Val L1 norm: 1.022.. Train Linf norm: 1434.035.. Val Linf norm: 18.164\n",
            "Epoch 49/141.. Train loss: 118.029.. Val loss: 54.373.. Train L1 norm: 1.537.. Val L1 norm: 1.022.. Train Linf norm: 543.951.. Val Linf norm: 18.158\n",
            "Epoch 50/141.. Train loss: 109.145.. Val loss: 54.374.. Train L1 norm: 1.309.. Val L1 norm: 1.022.. Train Linf norm: 310.799.. Val Linf norm: 18.151\n",
            "Epoch 51/141.. Train loss: 116.905.. Val loss: 54.374.. Train L1 norm: 1.341.. Val L1 norm: 1.022.. Train Linf norm: 346.140.. Val Linf norm: 18.146\n",
            "Epoch 52/141.. Train loss: 405.434.. Val loss: 54.375.. Train L1 norm: 1.609.. Val L1 norm: 1.022.. Train Linf norm: 619.371.. Val Linf norm: 18.129\n",
            "Epoch 53/141.. Train loss: 106.424.. Val loss: 54.375.. Train L1 norm: 1.199.. Val L1 norm: 1.022.. Train Linf norm: 201.685.. Val Linf norm: 18.122\n",
            "Epoch 54/141.. Train loss: 181.996.. Val loss: 54.375.. Train L1 norm: 2.756.. Val L1 norm: 1.022.. Train Linf norm: 1794.446.. Val Linf norm: 18.127\n",
            "Epoch 55/141.. Train loss: 75.281.. Val loss: 54.375.. Train L1 norm: 2.880.. Val L1 norm: 1.022.. Train Linf norm: 1921.347.. Val Linf norm: 18.128\n",
            "Epoch 56/141.. Train loss: 60.794.. Val loss: 54.375.. Train L1 norm: 1.451.. Val L1 norm: 1.022.. Train Linf norm: 457.216.. Val Linf norm: 18.128\n",
            "Epoch 57/141.. Train loss: 88.720.. Val loss: 54.375.. Train L1 norm: 2.378.. Val L1 norm: 1.022.. Train Linf norm: 1406.652.. Val Linf norm: 18.127\n",
            "Epoch 58/141.. Train loss: 78.384.. Val loss: 54.375.. Train L1 norm: 1.956.. Val L1 norm: 1.022.. Train Linf norm: 974.629.. Val Linf norm: 18.128\n",
            "Epoch 59/141.. Train loss: 181.579.. Val loss: 54.375.. Train L1 norm: 2.755.. Val L1 norm: 1.022.. Train Linf norm: 1791.569.. Val Linf norm: 18.117\n",
            "Epoch 60/141.. Train loss: 60.273.. Val loss: 54.375.. Train L1 norm: 2.646.. Val L1 norm: 1.022.. Train Linf norm: 1681.952.. Val Linf norm: 18.117\n",
            "Epoch 61/141.. Train loss: 326.501.. Val loss: 54.374.. Train L1 norm: 1.035.. Val L1 norm: 1.022.. Train Linf norm: 31.590.. Val Linf norm: 18.140\n",
            "Epoch 62/141.. Train loss: 121.615.. Val loss: 54.375.. Train L1 norm: 1.593.. Val L1 norm: 1.022.. Train Linf norm: 603.165.. Val Linf norm: 18.132\n",
            "Epoch 63/141.. Train loss: 69.654.. Val loss: 54.375.. Train L1 norm: 2.078.. Val L1 norm: 1.022.. Train Linf norm: 1101.179.. Val Linf norm: 18.130\n",
            "Epoch 64/141.. Train loss: 61.739.. Val loss: 54.375.. Train L1 norm: 2.089.. Val L1 norm: 1.022.. Train Linf norm: 1110.601.. Val Linf norm: 18.131\n",
            "Epoch 65/141.. Train loss: 412.080.. Val loss: 54.376.. Train L1 norm: 2.222.. Val L1 norm: 1.022.. Train Linf norm: 1249.571.. Val Linf norm: 18.105\n",
            "Epoch 66/141.. Train loss: 60.179.. Val loss: 54.376.. Train L1 norm: 1.402.. Val L1 norm: 1.022.. Train Linf norm: 407.668.. Val Linf norm: 18.105\n",
            "Epoch 67/141.. Train loss: 766.641.. Val loss: 54.377.. Train L1 norm: 1.211.. Val L1 norm: 1.022.. Train Linf norm: 212.098.. Val Linf norm: 18.074\n",
            "Epoch 68/141.. Train loss: 69.315.. Val loss: 54.377.. Train L1 norm: 1.042.. Val L1 norm: 1.022.. Train Linf norm: 40.542.. Val Linf norm: 18.076\n",
            "Epoch 69/141.. Train loss: 67.271.. Val loss: 54.377.. Train L1 norm: 1.940.. Val L1 norm: 1.022.. Train Linf norm: 959.290.. Val Linf norm: 18.076\n",
            "Epoch 70/141.. Train loss: 86.553.. Val loss: 54.377.. Train L1 norm: 1.435.. Val L1 norm: 1.022.. Train Linf norm: 442.034.. Val Linf norm: 18.080\n",
            "Epoch 71/141.. Train loss: 345.490.. Val loss: 54.377.. Train L1 norm: 2.515.. Val L1 norm: 1.022.. Train Linf norm: 1546.135.. Val Linf norm: 18.062\n",
            "Epoch 72/141.. Train loss: 176.966.. Val loss: 54.377.. Train L1 norm: 1.850.. Val L1 norm: 1.022.. Train Linf norm: 866.579.. Val Linf norm: 18.072\n",
            "Epoch 73/141.. Train loss: 127.348.. Val loss: 54.377.. Train L1 norm: 1.037.. Val L1 norm: 1.022.. Train Linf norm: 35.949.. Val Linf norm: 18.085\n",
            "Epoch 74/141.. Train loss: 85.079.. Val loss: 54.376.. Train L1 norm: 1.468.. Val L1 norm: 1.022.. Train Linf norm: 475.709.. Val Linf norm: 18.089\n",
            "Epoch 75/141.. Train loss: 233.433.. Val loss: 54.377.. Train L1 norm: 2.161.. Val L1 norm: 1.022.. Train Linf norm: 1182.103.. Val Linf norm: 18.071\n",
            "Epoch 76/141.. Train loss: 81.767.. Val loss: 54.377.. Train L1 norm: 2.332.. Val L1 norm: 1.022.. Train Linf norm: 1361.251.. Val Linf norm: 18.072\n",
            "Epoch 77/141.. Train loss: 127.396.. Val loss: 54.377.. Train L1 norm: 2.010.. Val L1 norm: 1.022.. Train Linf norm: 1030.685.. Val Linf norm: 18.069\n",
            "Epoch 78/141.. Train loss: 63.023.. Val loss: 54.377.. Train L1 norm: 1.149.. Val L1 norm: 1.022.. Train Linf norm: 149.363.. Val Linf norm: 18.069\n",
            "Epoch 79/141.. Train loss: 64.600.. Val loss: 54.377.. Train L1 norm: 1.108.. Val L1 norm: 1.022.. Train Linf norm: 107.418.. Val Linf norm: 18.071\n",
            "Epoch 80/141.. Train loss: 173.784.. Val loss: 54.378.. Train L1 norm: 1.822.. Val L1 norm: 1.022.. Train Linf norm: 838.641.. Val Linf norm: 18.057\n",
            "Epoch 81/141.. Train loss: 72.895.. Val loss: 54.378.. Train L1 norm: 2.618.. Val L1 norm: 1.022.. Train Linf norm: 1652.149.. Val Linf norm: 18.055\n",
            "Epoch 82/141.. Train loss: 117.577.. Val loss: 54.378.. Train L1 norm: 1.796.. Val L1 norm: 1.022.. Train Linf norm: 811.998.. Val Linf norm: 18.046\n",
            "Epoch 83/141.. Train loss: 196.911.. Val loss: 54.378.. Train L1 norm: 1.635.. Val L1 norm: 1.022.. Train Linf norm: 647.064.. Val Linf norm: 18.037\n",
            "Epoch 84/141.. Train loss: 135.334.. Val loss: 54.379.. Train L1 norm: 1.946.. Val L1 norm: 1.022.. Train Linf norm: 965.366.. Val Linf norm: 18.026\n",
            "Epoch 85/141.. Train loss: 70.831.. Val loss: 54.379.. Train L1 norm: 1.971.. Val L1 norm: 1.022.. Train Linf norm: 991.092.. Val Linf norm: 18.023\n",
            "Epoch 86/141.. Train loss: 78.021.. Val loss: 54.379.. Train L1 norm: 2.332.. Val L1 norm: 1.022.. Train Linf norm: 1360.070.. Val Linf norm: 18.021\n",
            "Epoch 87/141.. Train loss: 71.404.. Val loss: 54.379.. Train L1 norm: 2.830.. Val L1 norm: 1.022.. Train Linf norm: 1869.999.. Val Linf norm: 18.024\n",
            "Epoch 88/141.. Train loss: 155.033.. Val loss: 54.379.. Train L1 norm: 2.808.. Val L1 norm: 1.022.. Train Linf norm: 1848.331.. Val Linf norm: 18.013\n",
            "Epoch 89/141.. Train loss: 60.074.. Val loss: 54.379.. Train L1 norm: 1.176.. Val L1 norm: 1.022.. Train Linf norm: 177.412.. Val Linf norm: 18.012\n",
            "Epoch 90/141.. Train loss: 62.256.. Val loss: 54.379.. Train L1 norm: 1.549.. Val L1 norm: 1.022.. Train Linf norm: 558.568.. Val Linf norm: 18.013\n",
            "Epoch 91/141.. Train loss: 117.961.. Val loss: 54.380.. Train L1 norm: 2.808.. Val L1 norm: 1.022.. Train Linf norm: 1847.037.. Val Linf norm: 18.003\n",
            "Epoch 92/141.. Train loss: 94.646.. Val loss: 54.380.. Train L1 norm: 2.687.. Val L1 norm: 1.022.. Train Linf norm: 1723.802.. Val Linf norm: 17.998\n",
            "Epoch 93/141.. Train loss: 118.990.. Val loss: 54.381.. Train L1 norm: 1.665.. Val L1 norm: 1.022.. Train Linf norm: 674.842.. Val Linf norm: 17.983\n",
            "Epoch 94/141.. Train loss: 60.003.. Val loss: 54.380.. Train L1 norm: 1.177.. Val L1 norm: 1.022.. Train Linf norm: 178.031.. Val Linf norm: 17.984\n",
            "Epoch 95/141.. Train loss: 65.029.. Val loss: 54.380.. Train L1 norm: 1.664.. Val L1 norm: 1.022.. Train Linf norm: 676.586.. Val Linf norm: 17.986\n",
            "Epoch 96/141.. Train loss: 134.723.. Val loss: 54.381.. Train L1 norm: 1.216.. Val L1 norm: 1.022.. Train Linf norm: 217.199.. Val Linf norm: 17.975\n",
            "Epoch 97/141.. Train loss: 151.823.. Val loss: 54.382.. Train L1 norm: 1.910.. Val L1 norm: 1.022.. Train Linf norm: 929.480.. Val Linf norm: 17.953\n",
            "Epoch 98/141.. Train loss: 60.082.. Val loss: 54.382.. Train L1 norm: 1.162.. Val L1 norm: 1.022.. Train Linf norm: 161.828.. Val Linf norm: 17.954\n",
            "Epoch 99/141.. Train loss: 71.270.. Val loss: 54.382.. Train L1 norm: 1.323.. Val L1 norm: 1.022.. Train Linf norm: 328.639.. Val Linf norm: 17.958\n",
            "Epoch 100/141.. Train loss: 60.571.. Val loss: 54.382.. Train L1 norm: 1.082.. Val L1 norm: 1.022.. Train Linf norm: 80.247.. Val Linf norm: 17.959\n",
            "Epoch 101/141.. Train loss: 131.123.. Val loss: 54.382.. Train L1 norm: 1.671.. Val L1 norm: 1.022.. Train Linf norm: 683.787.. Val Linf norm: 17.946\n",
            "Epoch 102/141.. Train loss: 118.319.. Val loss: 54.382.. Train L1 norm: 4.104.. Val L1 norm: 1.022.. Train Linf norm: 3174.071.. Val Linf norm: 17.938\n",
            "Epoch 103/141.. Train loss: 73.915.. Val loss: 54.382.. Train L1 norm: 3.659.. Val L1 norm: 1.022.. Train Linf norm: 2719.184.. Val Linf norm: 17.941\n",
            "Epoch 104/141.. Train loss: 60.643.. Val loss: 54.382.. Train L1 norm: 2.241.. Val L1 norm: 1.022.. Train Linf norm: 1267.268.. Val Linf norm: 17.942\n",
            "Epoch 105/141.. Train loss: 348.047.. Val loss: 54.383.. Train L1 norm: 1.204.. Val L1 norm: 1.022.. Train Linf norm: 205.537.. Val Linf norm: 17.912\n",
            "Epoch 106/141.. Train loss: 229.958.. Val loss: 54.384.. Train L1 norm: 1.466.. Val L1 norm: 1.022.. Train Linf norm: 472.889.. Val Linf norm: 17.894\n",
            "Epoch 107/141.. Train loss: 296.901.. Val loss: 54.385.. Train L1 norm: 3.665.. Val L1 norm: 1.022.. Train Linf norm: 2725.315.. Val Linf norm: 17.869\n",
            "Epoch 108/141.. Train loss: 66.763.. Val loss: 54.385.. Train L1 norm: 2.604.. Val L1 norm: 1.022.. Train Linf norm: 1637.911.. Val Linf norm: 17.871\n",
            "Epoch 109/141.. Train loss: 65.730.. Val loss: 54.385.. Train L1 norm: 2.367.. Val L1 norm: 1.022.. Train Linf norm: 1395.019.. Val Linf norm: 17.870\n",
            "Epoch 110/141.. Train loss: 143.402.. Val loss: 54.385.. Train L1 norm: 1.766.. Val L1 norm: 1.022.. Train Linf norm: 781.035.. Val Linf norm: 17.856\n",
            "Epoch 111/141.. Train loss: 173.596.. Val loss: 54.386.. Train L1 norm: 2.083.. Val L1 norm: 1.022.. Train Linf norm: 1104.623.. Val Linf norm: 17.840\n",
            "Epoch 112/141.. Train loss: 75.478.. Val loss: 54.386.. Train L1 norm: 1.146.. Val L1 norm: 1.022.. Train Linf norm: 145.554.. Val Linf norm: 17.832\n",
            "Epoch 113/141.. Train loss: 60.346.. Val loss: 54.386.. Train L1 norm: 1.790.. Val L1 norm: 1.022.. Train Linf norm: 804.865.. Val Linf norm: 17.832\n",
            "Epoch 114/141.. Train loss: 545.216.. Val loss: 54.388.. Train L1 norm: 1.293.. Val L1 norm: 1.021.. Train Linf norm: 297.048.. Val Linf norm: 17.795\n",
            "Epoch 115/141.. Train loss: 74.796.. Val loss: 54.387.. Train L1 norm: 1.591.. Val L1 norm: 1.021.. Train Linf norm: 602.331.. Val Linf norm: 17.799\n",
            "Epoch 116/141.. Train loss: 369.833.. Val loss: 54.388.. Train L1 norm: 1.873.. Val L1 norm: 1.021.. Train Linf norm: 887.576.. Val Linf norm: 17.782\n",
            "Epoch 117/141.. Train loss: 410.514.. Val loss: 54.389.. Train L1 norm: 2.576.. Val L1 norm: 1.021.. Train Linf norm: 1610.857.. Val Linf norm: 17.764\n",
            "Epoch 118/141.. Train loss: 110.540.. Val loss: 54.389.. Train L1 norm: 2.163.. Val L1 norm: 1.021.. Train Linf norm: 1188.207.. Val Linf norm: 17.748\n",
            "Epoch 119/141.. Train loss: 80.135.. Val loss: 54.390.. Train L1 norm: 2.024.. Val L1 norm: 1.021.. Train Linf norm: 1042.137.. Val Linf norm: 17.746\n",
            "Epoch 120/141.. Train loss: 246.740.. Val loss: 54.390.. Train L1 norm: 3.149.. Val L1 norm: 1.021.. Train Linf norm: 2196.479.. Val Linf norm: 17.731\n",
            "Epoch 121/141.. Train loss: 120.926.. Val loss: 54.390.. Train L1 norm: 1.720.. Val L1 norm: 1.021.. Train Linf norm: 732.563.. Val Linf norm: 17.738\n",
            "Epoch 122/141.. Train loss: 383.581.. Val loss: 54.390.. Train L1 norm: 3.180.. Val L1 norm: 1.021.. Train Linf norm: 2229.702.. Val Linf norm: 17.725\n",
            "Epoch 123/141.. Train loss: 308.875.. Val loss: 54.391.. Train L1 norm: 1.951.. Val L1 norm: 1.021.. Train Linf norm: 971.727.. Val Linf norm: 17.706\n",
            "Epoch 124/141.. Train loss: 61.234.. Val loss: 54.391.. Train L1 norm: 1.963.. Val L1 norm: 1.021.. Train Linf norm: 982.550.. Val Linf norm: 17.705\n",
            "Epoch 125/141.. Train loss: 574.036.. Val loss: 54.392.. Train L1 norm: 4.317.. Val L1 norm: 1.021.. Train Linf norm: 3393.899.. Val Linf norm: 17.678\n",
            "Epoch 126/141.. Train loss: 484.320.. Val loss: 54.393.. Train L1 norm: 2.490.. Val L1 norm: 1.021.. Train Linf norm: 1521.666.. Val Linf norm: 17.662\n",
            "Epoch 127/141.. Train loss: 72.027.. Val loss: 54.393.. Train L1 norm: 1.480.. Val L1 norm: 1.021.. Train Linf norm: 489.619.. Val Linf norm: 17.652\n",
            "Epoch 128/141.. Train loss: 60.152.. Val loss: 54.393.. Train L1 norm: 1.093.. Val L1 norm: 1.021.. Train Linf norm: 92.208.. Val Linf norm: 17.653\n",
            "Epoch 129/141.. Train loss: 64.292.. Val loss: 54.393.. Train L1 norm: 1.651.. Val L1 norm: 1.021.. Train Linf norm: 664.213.. Val Linf norm: 17.653\n",
            "Epoch 130/141.. Train loss: 80.517.. Val loss: 54.393.. Train L1 norm: 2.598.. Val L1 norm: 1.021.. Train Linf norm: 1631.510.. Val Linf norm: 17.651\n",
            "Epoch 131/141.. Train loss: 61.141.. Val loss: 54.393.. Train L1 norm: 2.893.. Val L1 norm: 1.021.. Train Linf norm: 1935.363.. Val Linf norm: 17.652\n",
            "Epoch 132/141.. Train loss: 118.754.. Val loss: 54.393.. Train L1 norm: 2.161.. Val L1 norm: 1.021.. Train Linf norm: 1185.933.. Val Linf norm: 17.644\n",
            "Epoch 133/141.. Train loss: 119.822.. Val loss: 54.394.. Train L1 norm: 2.683.. Val L1 norm: 1.021.. Train Linf norm: 1718.591.. Val Linf norm: 17.640\n",
            "Epoch 134/141.. Train loss: 60.545.. Val loss: 54.394.. Train L1 norm: 1.239.. Val L1 norm: 1.021.. Train Linf norm: 241.333.. Val Linf norm: 17.639\n",
            "Epoch 135/141.. Train loss: 89.778.. Val loss: 54.394.. Train L1 norm: 2.401.. Val L1 norm: 1.021.. Train Linf norm: 1431.883.. Val Linf norm: 17.636\n",
            "Epoch 136/141.. Train loss: 61.736.. Val loss: 54.394.. Train L1 norm: 1.578.. Val L1 norm: 1.021.. Train Linf norm: 586.098.. Val Linf norm: 17.633\n",
            "Epoch 137/141.. Train loss: 94.189.. Val loss: 54.394.. Train L1 norm: 2.661.. Val L1 norm: 1.021.. Train Linf norm: 1697.936.. Val Linf norm: 17.640\n",
            "Epoch 138/141.. Train loss: 192.297.. Val loss: 54.394.. Train L1 norm: 1.045.. Val L1 norm: 1.021.. Train Linf norm: 42.582.. Val Linf norm: 17.621\n",
            "Epoch 139/141.. Train loss: 185.692.. Val loss: 54.395.. Train L1 norm: 2.265.. Val L1 norm: 1.021.. Train Linf norm: 1291.835.. Val Linf norm: 17.611\n",
            "Epoch 140/141.. Train loss: 147.831.. Val loss: 54.395.. Train L1 norm: 1.169.. Val L1 norm: 1.021.. Train Linf norm: 168.752.. Val Linf norm: 17.599\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:40:58,784]\u001b[0m Trial 112 finished with value: 1.021207171535492 and parameters: {'n_layers': 6, 'n_units_0': 3589, 'n_units_1': 194, 'n_units_2': 3818, 'n_units_3': 457, 'n_units_4': 269, 'n_units_5': 1323, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 1.0822402808375606e-06, 'batch_size': 1024, 'n_epochs': 141, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.1999193307817708, 'dropout_rate': 0.0697799090319256, 'weight_decay': 0.0009269744825248493, 'beta1': 0.9382520737245487, 'beta2': 0.999186850307055, 'factor': 0.11605110164427501, 'patience': 6, 'threshold': 0.002207245048938629}. Best is trial 92 with value: 1.0098264760335287.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 141/141.. Train loss: 60.340.. Val loss: 54.395.. Train L1 norm: 1.092.. Val L1 norm: 1.021.. Train Linf norm: 89.015.. Val Linf norm: 17.600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:41:00,213]\u001b[0m Trial 113 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/141.. Train loss: 594.745.. Val loss: 53.164.. Train L1 norm: 3.174.. Val L1 norm: 1.112.. Train Linf norm: 2178.394.. Val Linf norm: 83.115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:41:01,899]\u001b[0m Trial 114 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150.. Train loss: 935.569.. Val loss: 52.929.. Train L1 norm: 4.767.. Val L1 norm: 1.104.. Train Linf norm: 3809.761.. Val Linf norm: 74.426\n",
            "Epoch 1/136.. Train loss: 1430.064.. Val loss: 55.769.. Train L1 norm: 3.882.. Val L1 norm: 1.030.. Train Linf norm: 2933.345.. Val Linf norm: 24.129\n",
            "Epoch 2/136.. Train loss: 82.773.. Val loss: 55.684.. Train L1 norm: 1.394.. Val L1 norm: 1.026.. Train Linf norm: 392.572.. Val Linf norm: 20.940\n",
            "Epoch 3/136.. Train loss: 63.713.. Val loss: 55.617.. Train L1 norm: 3.294.. Val L1 norm: 1.023.. Train Linf norm: 2339.209.. Val Linf norm: 18.820\n",
            "Epoch 4/136.. Train loss: 77.819.. Val loss: 55.230.. Train L1 norm: 2.020.. Val L1 norm: 1.013.. Train Linf norm: 1037.024.. Val Linf norm: 11.832\n",
            "Epoch 5/136.. Train loss: 92.748.. Val loss: 55.530.. Train L1 norm: 2.309.. Val L1 norm: 1.019.. Train Linf norm: 1333.061.. Val Linf norm: 16.231\n",
            "Epoch 6/136.. Train loss: 140.510.. Val loss: 55.892.. Train L1 norm: 3.859.. Val L1 norm: 1.032.. Train Linf norm: 2916.256.. Val Linf norm: 24.945\n",
            "Epoch 7/136.. Train loss: 578.055.. Val loss: 55.214.. Train L1 norm: 1.642.. Val L1 norm: 1.013.. Train Linf norm: 645.611.. Val Linf norm: 11.928\n",
            "Epoch 8/136.. Train loss: 149.242.. Val loss: 55.398.. Train L1 norm: 1.444.. Val L1 norm: 1.015.. Train Linf norm: 447.318.. Val Linf norm: 13.211\n",
            "Epoch 9/136.. Train loss: 62.605.. Val loss: 55.365.. Train L1 norm: 2.916.. Val L1 norm: 1.013.. Train Linf norm: 1952.996.. Val Linf norm: 12.134\n",
            "Epoch 10/136.. Train loss: 147.662.. Val loss: 55.740.. Train L1 norm: 1.592.. Val L1 norm: 1.024.. Train Linf norm: 596.234.. Val Linf norm: 19.917\n",
            "Epoch 11/136.. Train loss: 213.398.. Val loss: 55.031.. Train L1 norm: 2.157.. Val L1 norm: 1.015.. Train Linf norm: 1175.878.. Val Linf norm: 13.743\n",
            "Epoch 12/136.. Train loss: 359.539.. Val loss: 55.386.. Train L1 norm: 1.262.. Val L1 norm: 1.013.. Train Linf norm: 261.888.. Val Linf norm: 11.707\n",
            "Epoch 13/136.. Train loss: 62.442.. Val loss: 55.633.. Train L1 norm: 2.128.. Val L1 norm: 1.020.. Train Linf norm: 1145.304.. Val Linf norm: 16.695\n",
            "Epoch 14/136.. Train loss: 127.847.. Val loss: 55.925.. Train L1 norm: 1.751.. Val L1 norm: 1.030.. Train Linf norm: 758.165.. Val Linf norm: 23.368\n",
            "Epoch 15/136.. Train loss: 198.719.. Val loss: 55.780.. Train L1 norm: 1.687.. Val L1 norm: 1.024.. Train Linf norm: 689.276.. Val Linf norm: 19.090\n",
            "Epoch 16/136.. Train loss: 222.075.. Val loss: 55.778.. Train L1 norm: 1.535.. Val L1 norm: 1.024.. Train Linf norm: 540.381.. Val Linf norm: 19.022\n",
            "Epoch 17/136.. Train loss: 430.781.. Val loss: 55.250.. Train L1 norm: 1.627.. Val L1 norm: 1.011.. Train Linf norm: 635.455.. Val Linf norm: 9.766\n",
            "Epoch 18/136.. Train loss: 60.803.. Val loss: 55.222.. Train L1 norm: 2.511.. Val L1 norm: 1.011.. Train Linf norm: 1541.150.. Val Linf norm: 10.466\n",
            "Epoch 19/136.. Train loss: 61.292.. Val loss: 55.219.. Train L1 norm: 1.550.. Val L1 norm: 1.011.. Train Linf norm: 557.576.. Val Linf norm: 10.561\n",
            "Epoch 20/136.. Train loss: 61.699.. Val loss: 55.207.. Train L1 norm: 1.081.. Val L1 norm: 1.011.. Train Linf norm: 76.521.. Val Linf norm: 10.752\n",
            "Epoch 21/136.. Train loss: 163.317.. Val loss: 55.258.. Train L1 norm: 1.567.. Val L1 norm: 1.011.. Train Linf norm: 575.434.. Val Linf norm: 10.068\n",
            "Epoch 22/136.. Train loss: 307.049.. Val loss: 55.338.. Train L1 norm: 1.109.. Val L1 norm: 1.011.. Train Linf norm: 105.795.. Val Linf norm: 9.317\n",
            "Epoch 23/136.. Train loss: 102.977.. Val loss: 55.308.. Train L1 norm: 2.391.. Val L1 norm: 1.011.. Train Linf norm: 1417.501.. Val Linf norm: 9.608\n",
            "Epoch 24/136.. Train loss: 62.563.. Val loss: 55.311.. Train L1 norm: 1.389.. Val L1 norm: 1.011.. Train Linf norm: 391.244.. Val Linf norm: 9.628\n",
            "Epoch 25/136.. Train loss: 64.296.. Val loss: 55.301.. Train L1 norm: 1.266.. Val L1 norm: 1.011.. Train Linf norm: 266.516.. Val Linf norm: 9.782\n",
            "Epoch 26/136.. Train loss: 132.371.. Val loss: 55.292.. Train L1 norm: 2.289.. Val L1 norm: 1.011.. Train Linf norm: 1313.583.. Val Linf norm: 9.874\n",
            "Epoch 27/136.. Train loss: 78.383.. Val loss: 55.296.. Train L1 norm: 1.035.. Val L1 norm: 1.011.. Train Linf norm: 29.640.. Val Linf norm: 9.836\n",
            "Epoch 28/136.. Train loss: 394.003.. Val loss: 55.286.. Train L1 norm: 1.215.. Val L1 norm: 1.011.. Train Linf norm: 214.278.. Val Linf norm: 9.961\n",
            "Epoch 29/136.. Train loss: 192.468.. Val loss: 55.291.. Train L1 norm: 1.477.. Val L1 norm: 1.011.. Train Linf norm: 482.215.. Val Linf norm: 9.935\n",
            "Epoch 30/136.. Train loss: 94.837.. Val loss: 55.295.. Train L1 norm: 1.499.. Val L1 norm: 1.011.. Train Linf norm: 504.500.. Val Linf norm: 9.892\n",
            "Epoch 31/136.. Train loss: 174.678.. Val loss: 55.287.. Train L1 norm: 2.479.. Val L1 norm: 1.011.. Train Linf norm: 1507.753.. Val Linf norm: 10.003\n",
            "Epoch 32/136.. Train loss: 62.045.. Val loss: 55.282.. Train L1 norm: 1.053.. Val L1 norm: 1.011.. Train Linf norm: 49.414.. Val Linf norm: 10.062\n",
            "Epoch 33/136.. Train loss: 61.841.. Val loss: 55.282.. Train L1 norm: 2.458.. Val L1 norm: 1.011.. Train Linf norm: 1485.769.. Val Linf norm: 10.063\n",
            "Epoch 34/136.. Train loss: 93.482.. Val loss: 55.282.. Train L1 norm: 1.155.. Val L1 norm: 1.011.. Train Linf norm: 152.598.. Val Linf norm: 10.070\n",
            "Epoch 35/136.. Train loss: 102.566.. Val loss: 55.282.. Train L1 norm: 1.249.. Val L1 norm: 1.011.. Train Linf norm: 248.255.. Val Linf norm: 10.067\n",
            "Epoch 36/136.. Train loss: 168.244.. Val loss: 55.283.. Train L1 norm: 2.795.. Val L1 norm: 1.011.. Train Linf norm: 1831.052.. Val Linf norm: 10.055\n",
            "Epoch 37/136.. Train loss: 286.348.. Val loss: 55.285.. Train L1 norm: 1.627.. Val L1 norm: 1.011.. Train Linf norm: 634.846.. Val Linf norm: 10.033\n",
            "Epoch 38/136.. Train loss: 110.087.. Val loss: 55.286.. Train L1 norm: 2.678.. Val L1 norm: 1.011.. Train Linf norm: 1711.795.. Val Linf norm: 10.025\n",
            "Epoch 39/136.. Train loss: 132.993.. Val loss: 55.286.. Train L1 norm: 2.059.. Val L1 norm: 1.011.. Train Linf norm: 1078.530.. Val Linf norm: 10.019\n",
            "Epoch 40/136.. Train loss: 155.773.. Val loss: 55.285.. Train L1 norm: 1.402.. Val L1 norm: 1.011.. Train Linf norm: 406.262.. Val Linf norm: 10.033\n",
            "Epoch 41/136.. Train loss: 204.860.. Val loss: 55.284.. Train L1 norm: 1.945.. Val L1 norm: 1.011.. Train Linf norm: 961.698.. Val Linf norm: 10.052\n",
            "Epoch 42/136.. Train loss: 79.278.. Val loss: 55.283.. Train L1 norm: 1.736.. Val L1 norm: 1.011.. Train Linf norm: 747.576.. Val Linf norm: 10.056\n",
            "Epoch 43/136.. Train loss: 113.865.. Val loss: 55.283.. Train L1 norm: 1.093.. Val L1 norm: 1.011.. Train Linf norm: 84.361.. Val Linf norm: 10.064\n",
            "Epoch 44/136.. Train loss: 124.846.. Val loss: 55.284.. Train L1 norm: 1.333.. Val L1 norm: 1.011.. Train Linf norm: 334.743.. Val Linf norm: 10.059\n",
            "Epoch 45/136.. Train loss: 131.663.. Val loss: 55.283.. Train L1 norm: 1.692.. Val L1 norm: 1.011.. Train Linf norm: 702.512.. Val Linf norm: 10.067\n",
            "Epoch 46/136.. Train loss: 205.158.. Val loss: 55.281.. Train L1 norm: 1.523.. Val L1 norm: 1.011.. Train Linf norm: 529.881.. Val Linf norm: 10.090\n",
            "Epoch 47/136.. Train loss: 90.730.. Val loss: 55.281.. Train L1 norm: 1.267.. Val L1 norm: 1.011.. Train Linf norm: 266.790.. Val Linf norm: 10.097\n",
            "Epoch 48/136.. Train loss: 432.744.. Val loss: 55.279.. Train L1 norm: 2.437.. Val L1 norm: 1.011.. Train Linf norm: 1462.597.. Val Linf norm: 10.122\n",
            "Epoch 49/136.. Train loss: 80.467.. Val loss: 55.278.. Train L1 norm: 1.349.. Val L1 norm: 1.011.. Train Linf norm: 351.055.. Val Linf norm: 10.126\n",
            "Epoch 50/136.. Train loss: 154.166.. Val loss: 55.279.. Train L1 norm: 1.259.. Val L1 norm: 1.011.. Train Linf norm: 258.073.. Val Linf norm: 10.117\n",
            "Epoch 51/136.. Train loss: 188.682.. Val loss: 55.280.. Train L1 norm: 1.588.. Val L1 norm: 1.011.. Train Linf norm: 591.957.. Val Linf norm: 10.109\n",
            "Epoch 52/136.. Train loss: 74.828.. Val loss: 55.280.. Train L1 norm: 1.806.. Val L1 norm: 1.011.. Train Linf norm: 818.055.. Val Linf norm: 10.109\n",
            "Epoch 53/136.. Train loss: 214.094.. Val loss: 55.278.. Train L1 norm: 1.896.. Val L1 norm: 1.011.. Train Linf norm: 910.854.. Val Linf norm: 10.124\n",
            "Epoch 54/136.. Train loss: 353.489.. Val loss: 55.280.. Train L1 norm: 1.639.. Val L1 norm: 1.011.. Train Linf norm: 647.489.. Val Linf norm: 10.103\n",
            "Epoch 55/136.. Train loss: 82.436.. Val loss: 55.279.. Train L1 norm: 1.441.. Val L1 norm: 1.011.. Train Linf norm: 446.319.. Val Linf norm: 10.108\n",
            "Epoch 56/136.. Train loss: 64.411.. Val loss: 55.279.. Train L1 norm: 1.962.. Val L1 norm: 1.011.. Train Linf norm: 978.815.. Val Linf norm: 10.111\n",
            "Epoch 57/136.. Train loss: 91.667.. Val loss: 55.280.. Train L1 norm: 1.460.. Val L1 norm: 1.011.. Train Linf norm: 465.096.. Val Linf norm: 10.108\n",
            "Epoch 58/136.. Train loss: 64.225.. Val loss: 55.280.. Train L1 norm: 1.268.. Val L1 norm: 1.011.. Train Linf norm: 269.012.. Val Linf norm: 10.109\n",
            "Epoch 59/136.. Train loss: 186.306.. Val loss: 55.278.. Train L1 norm: 1.863.. Val L1 norm: 1.011.. Train Linf norm: 877.652.. Val Linf norm: 10.126\n",
            "Epoch 60/136.. Train loss: 128.926.. Val loss: 55.278.. Train L1 norm: 1.059.. Val L1 norm: 1.011.. Train Linf norm: 55.025.. Val Linf norm: 10.139\n",
            "Epoch 61/136.. Train loss: 184.098.. Val loss: 55.277.. Train L1 norm: 1.761.. Val L1 norm: 1.011.. Train Linf norm: 772.059.. Val Linf norm: 10.145\n",
            "Epoch 62/136.. Train loss: 137.230.. Val loss: 55.275.. Train L1 norm: 2.162.. Val L1 norm: 1.011.. Train Linf norm: 1183.098.. Val Linf norm: 10.168\n",
            "Epoch 63/136.. Train loss: 61.271.. Val loss: 55.275.. Train L1 norm: 1.430.. Val L1 norm: 1.011.. Train Linf norm: 434.827.. Val Linf norm: 10.169\n",
            "Epoch 64/136.. Train loss: 119.115.. Val loss: 55.276.. Train L1 norm: 1.586.. Val L1 norm: 1.011.. Train Linf norm: 592.534.. Val Linf norm: 10.162\n",
            "Epoch 65/136.. Train loss: 115.866.. Val loss: 55.277.. Train L1 norm: 1.229.. Val L1 norm: 1.011.. Train Linf norm: 227.995.. Val Linf norm: 10.153\n",
            "Epoch 66/136.. Train loss: 194.734.. Val loss: 55.275.. Train L1 norm: 1.411.. Val L1 norm: 1.011.. Train Linf norm: 414.696.. Val Linf norm: 10.173\n",
            "Epoch 67/136.. Train loss: 65.044.. Val loss: 55.275.. Train L1 norm: 2.209.. Val L1 norm: 1.011.. Train Linf norm: 1231.756.. Val Linf norm: 10.174\n",
            "Epoch 68/136.. Train loss: 123.067.. Val loss: 55.275.. Train L1 norm: 1.091.. Val L1 norm: 1.011.. Train Linf norm: 86.858.. Val Linf norm: 10.181\n",
            "Epoch 69/136.. Train loss: 121.536.. Val loss: 55.274.. Train L1 norm: 2.004.. Val L1 norm: 1.011.. Train Linf norm: 1022.925.. Val Linf norm: 10.194\n",
            "Epoch 70/136.. Train loss: 374.217.. Val loss: 55.276.. Train L1 norm: 1.746.. Val L1 norm: 1.011.. Train Linf norm: 758.549.. Val Linf norm: 10.178\n",
            "Epoch 71/136.. Train loss: 72.582.. Val loss: 55.275.. Train L1 norm: 2.050.. Val L1 norm: 1.011.. Train Linf norm: 1069.870.. Val Linf norm: 10.181\n",
            "Epoch 72/136.. Train loss: 400.844.. Val loss: 55.274.. Train L1 norm: 1.378.. Val L1 norm: 1.011.. Train Linf norm: 380.344.. Val Linf norm: 10.205\n",
            "Epoch 73/136.. Train loss: 128.769.. Val loss: 55.274.. Train L1 norm: 3.755.. Val L1 norm: 1.011.. Train Linf norm: 2814.686.. Val Linf norm: 10.197\n",
            "Epoch 74/136.. Train loss: 64.935.. Val loss: 55.274.. Train L1 norm: 1.094.. Val L1 norm: 1.011.. Train Linf norm: 82.758.. Val Linf norm: 10.199\n",
            "Epoch 75/136.. Train loss: 105.713.. Val loss: 55.274.. Train L1 norm: 1.691.. Val L1 norm: 1.011.. Train Linf norm: 699.995.. Val Linf norm: 10.208\n",
            "Epoch 76/136.. Train loss: 84.478.. Val loss: 55.274.. Train L1 norm: 2.769.. Val L1 norm: 1.011.. Train Linf norm: 1805.185.. Val Linf norm: 10.207\n",
            "Epoch 77/136.. Train loss: 147.051.. Val loss: 55.273.. Train L1 norm: 1.339.. Val L1 norm: 1.011.. Train Linf norm: 341.368.. Val Linf norm: 10.222\n",
            "Epoch 78/136.. Train loss: 136.861.. Val loss: 55.273.. Train L1 norm: 1.566.. Val L1 norm: 1.011.. Train Linf norm: 573.070.. Val Linf norm: 10.225\n",
            "Epoch 79/136.. Train loss: 66.966.. Val loss: 55.272.. Train L1 norm: 1.740.. Val L1 norm: 1.011.. Train Linf norm: 751.026.. Val Linf norm: 10.230\n",
            "Epoch 80/136.. Train loss: 61.985.. Val loss: 55.272.. Train L1 norm: 1.507.. Val L1 norm: 1.011.. Train Linf norm: 512.693.. Val Linf norm: 10.231\n",
            "Epoch 81/136.. Train loss: 67.065.. Val loss: 55.272.. Train L1 norm: 1.279.. Val L1 norm: 1.011.. Train Linf norm: 276.176.. Val Linf norm: 10.230\n",
            "Epoch 82/136.. Train loss: 94.801.. Val loss: 55.273.. Train L1 norm: 1.468.. Val L1 norm: 1.011.. Train Linf norm: 473.279.. Val Linf norm: 10.224\n",
            "Epoch 83/136.. Train loss: 62.754.. Val loss: 55.273.. Train L1 norm: 2.501.. Val L1 norm: 1.011.. Train Linf norm: 1529.174.. Val Linf norm: 10.224\n",
            "Epoch 84/136.. Train loss: 90.574.. Val loss: 55.273.. Train L1 norm: 1.819.. Val L1 norm: 1.011.. Train Linf norm: 833.742.. Val Linf norm: 10.223\n",
            "Epoch 85/136.. Train loss: 65.813.. Val loss: 55.273.. Train L1 norm: 2.370.. Val L1 norm: 1.011.. Train Linf norm: 1396.325.. Val Linf norm: 10.222\n",
            "Epoch 86/136.. Train loss: 85.049.. Val loss: 55.273.. Train L1 norm: 1.611.. Val L1 norm: 1.011.. Train Linf norm: 620.418.. Val Linf norm: 10.226\n",
            "Epoch 87/136.. Train loss: 217.794.. Val loss: 55.274.. Train L1 norm: 2.020.. Val L1 norm: 1.011.. Train Linf norm: 1038.126.. Val Linf norm: 10.222\n",
            "Epoch 88/136.. Train loss: 86.439.. Val loss: 55.274.. Train L1 norm: 1.669.. Val L1 norm: 1.011.. Train Linf norm: 678.990.. Val Linf norm: 10.221\n",
            "Epoch 89/136.. Train loss: 86.282.. Val loss: 55.274.. Train L1 norm: 1.420.. Val L1 norm: 1.011.. Train Linf norm: 424.095.. Val Linf norm: 10.225\n",
            "Epoch 90/136.. Train loss: 63.134.. Val loss: 55.273.. Train L1 norm: 2.256.. Val L1 norm: 1.011.. Train Linf norm: 1279.561.. Val Linf norm: 10.227\n",
            "Epoch 91/136.. Train loss: 65.073.. Val loss: 55.273.. Train L1 norm: 1.228.. Val L1 norm: 1.011.. Train Linf norm: 227.076.. Val Linf norm: 10.230\n",
            "Epoch 92/136.. Train loss: 289.779.. Val loss: 55.275.. Train L1 norm: 1.321.. Val L1 norm: 1.011.. Train Linf norm: 321.498.. Val Linf norm: 10.204\n",
            "Epoch 93/136.. Train loss: 71.714.. Val loss: 55.275.. Train L1 norm: 1.149.. Val L1 norm: 1.011.. Train Linf norm: 144.055.. Val Linf norm: 10.203\n",
            "Epoch 94/136.. Train loss: 138.674.. Val loss: 55.275.. Train L1 norm: 1.509.. Val L1 norm: 1.011.. Train Linf norm: 509.538.. Val Linf norm: 10.209\n",
            "Epoch 95/136.. Train loss: 69.717.. Val loss: 55.274.. Train L1 norm: 1.564.. Val L1 norm: 1.011.. Train Linf norm: 571.914.. Val Linf norm: 10.221\n",
            "Epoch 96/136.. Train loss: 64.090.. Val loss: 55.274.. Train L1 norm: 2.144.. Val L1 norm: 1.011.. Train Linf norm: 1162.894.. Val Linf norm: 10.221\n",
            "Epoch 97/136.. Train loss: 149.946.. Val loss: 55.273.. Train L1 norm: 3.018.. Val L1 norm: 1.011.. Train Linf norm: 2061.081.. Val Linf norm: 10.235\n",
            "Epoch 98/136.. Train loss: 98.990.. Val loss: 55.274.. Train L1 norm: 2.004.. Val L1 norm: 1.011.. Train Linf norm: 1022.468.. Val Linf norm: 10.225\n",
            "Epoch 99/136.. Train loss: 60.977.. Val loss: 55.274.. Train L1 norm: 1.081.. Val L1 norm: 1.011.. Train Linf norm: 76.979.. Val Linf norm: 10.227\n",
            "Epoch 100/136.. Train loss: 202.019.. Val loss: 55.273.. Train L1 norm: 1.452.. Val L1 norm: 1.011.. Train Linf norm: 448.283.. Val Linf norm: 10.240\n",
            "Epoch 101/136.. Train loss: 72.370.. Val loss: 55.272.. Train L1 norm: 2.509.. Val L1 norm: 1.011.. Train Linf norm: 1538.941.. Val Linf norm: 10.252\n",
            "Epoch 102/136.. Train loss: 65.150.. Val loss: 55.272.. Train L1 norm: 1.120.. Val L1 norm: 1.011.. Train Linf norm: 116.547.. Val Linf norm: 10.252\n",
            "Epoch 103/136.. Train loss: 63.539.. Val loss: 55.272.. Train L1 norm: 1.618.. Val L1 norm: 1.011.. Train Linf norm: 629.019.. Val Linf norm: 10.254\n",
            "Epoch 104/136.. Train loss: 444.544.. Val loss: 55.274.. Train L1 norm: 1.460.. Val L1 norm: 1.011.. Train Linf norm: 465.494.. Val Linf norm: 10.227\n",
            "Epoch 105/136.. Train loss: 124.746.. Val loss: 55.273.. Train L1 norm: 3.291.. Val L1 norm: 1.011.. Train Linf norm: 2339.454.. Val Linf norm: 10.240\n",
            "Epoch 106/136.. Train loss: 143.658.. Val loss: 55.274.. Train L1 norm: 1.149.. Val L1 norm: 1.011.. Train Linf norm: 146.805.. Val Linf norm: 10.228\n",
            "Epoch 107/136.. Train loss: 149.024.. Val loss: 55.275.. Train L1 norm: 1.341.. Val L1 norm: 1.011.. Train Linf norm: 343.170.. Val Linf norm: 10.217\n",
            "Epoch 108/136.. Train loss: 81.557.. Val loss: 55.275.. Train L1 norm: 1.407.. Val L1 norm: 1.011.. Train Linf norm: 410.678.. Val Linf norm: 10.222\n",
            "Epoch 109/136.. Train loss: 374.818.. Val loss: 55.272.. Train L1 norm: 1.600.. Val L1 norm: 1.011.. Train Linf norm: 607.617.. Val Linf norm: 10.255\n",
            "Epoch 110/136.. Train loss: 81.317.. Val loss: 55.272.. Train L1 norm: 1.718.. Val L1 norm: 1.011.. Train Linf norm: 727.544.. Val Linf norm: 10.261\n",
            "Epoch 111/136.. Train loss: 144.338.. Val loss: 55.271.. Train L1 norm: 2.362.. Val L1 norm: 1.011.. Train Linf norm: 1388.637.. Val Linf norm: 10.277\n",
            "Epoch 112/136.. Train loss: 70.938.. Val loss: 55.270.. Train L1 norm: 2.711.. Val L1 norm: 1.011.. Train Linf norm: 1746.172.. Val Linf norm: 10.281\n",
            "Epoch 113/136.. Train loss: 60.998.. Val loss: 55.270.. Train L1 norm: 1.808.. Val L1 norm: 1.011.. Train Linf norm: 820.515.. Val Linf norm: 10.282\n",
            "Epoch 114/136.. Train loss: 74.674.. Val loss: 55.271.. Train L1 norm: 1.489.. Val L1 norm: 1.011.. Train Linf norm: 494.285.. Val Linf norm: 10.281\n",
            "Epoch 115/136.. Train loss: 114.564.. Val loss: 55.271.. Train L1 norm: 1.605.. Val L1 norm: 1.011.. Train Linf norm: 614.943.. Val Linf norm: 10.269\n",
            "Epoch 116/136.. Train loss: 282.128.. Val loss: 55.269.. Train L1 norm: 2.151.. Val L1 norm: 1.011.. Train Linf norm: 1172.536.. Val Linf norm: 10.290\n",
            "Epoch 117/136.. Train loss: 128.917.. Val loss: 55.270.. Train L1 norm: 2.232.. Val L1 norm: 1.011.. Train Linf norm: 1255.990.. Val Linf norm: 10.281\n",
            "Epoch 118/136.. Train loss: 61.441.. Val loss: 55.270.. Train L1 norm: 1.963.. Val L1 norm: 1.011.. Train Linf norm: 979.661.. Val Linf norm: 10.281\n",
            "Epoch 119/136.. Train loss: 68.890.. Val loss: 55.270.. Train L1 norm: 1.493.. Val L1 norm: 1.011.. Train Linf norm: 497.462.. Val Linf norm: 10.285\n",
            "Epoch 120/136.. Train loss: 584.601.. Val loss: 55.266.. Train L1 norm: 2.836.. Val L1 norm: 1.011.. Train Linf norm: 1874.019.. Val Linf norm: 10.321\n",
            "Epoch 121/136.. Train loss: 78.348.. Val loss: 55.266.. Train L1 norm: 2.359.. Val L1 norm: 1.011.. Train Linf norm: 1385.904.. Val Linf norm: 10.325\n",
            "Epoch 122/136.. Train loss: 61.243.. Val loss: 55.266.. Train L1 norm: 1.947.. Val L1 norm: 1.011.. Train Linf norm: 963.521.. Val Linf norm: 10.327\n",
            "Epoch 123/136.. Train loss: 64.837.. Val loss: 55.266.. Train L1 norm: 1.825.. Val L1 norm: 1.011.. Train Linf norm: 838.349.. Val Linf norm: 10.327\n",
            "Epoch 124/136.. Train loss: 61.165.. Val loss: 55.266.. Train L1 norm: 2.671.. Val L1 norm: 1.011.. Train Linf norm: 1706.574.. Val Linf norm: 10.328\n",
            "Epoch 125/136.. Train loss: 74.802.. Val loss: 55.266.. Train L1 norm: 1.885.. Val L1 norm: 1.011.. Train Linf norm: 900.344.. Val Linf norm: 10.332\n",
            "Epoch 126/136.. Train loss: 72.383.. Val loss: 55.266.. Train L1 norm: 1.338.. Val L1 norm: 1.011.. Train Linf norm: 338.773.. Val Linf norm: 10.334\n",
            "Epoch 127/136.. Train loss: 112.118.. Val loss: 55.266.. Train L1 norm: 1.518.. Val L1 norm: 1.011.. Train Linf norm: 521.910.. Val Linf norm: 10.329\n",
            "Epoch 128/136.. Train loss: 278.854.. Val loss: 55.268.. Train L1 norm: 1.226.. Val L1 norm: 1.011.. Train Linf norm: 224.894.. Val Linf norm: 10.306\n",
            "Epoch 129/136.. Train loss: 102.332.. Val loss: 55.267.. Train L1 norm: 1.383.. Val L1 norm: 1.011.. Train Linf norm: 386.717.. Val Linf norm: 10.313\n",
            "Epoch 130/136.. Train loss: 461.831.. Val loss: 55.269.. Train L1 norm: 1.306.. Val L1 norm: 1.011.. Train Linf norm: 308.034.. Val Linf norm: 10.295\n",
            "Epoch 131/136.. Train loss: 97.874.. Val loss: 55.269.. Train L1 norm: 1.762.. Val L1 norm: 1.011.. Train Linf norm: 775.094.. Val Linf norm: 10.295\n",
            "Epoch 132/136.. Train loss: 70.867.. Val loss: 55.269.. Train L1 norm: 1.244.. Val L1 norm: 1.011.. Train Linf norm: 243.740.. Val Linf norm: 10.298\n",
            "Epoch 133/136.. Train loss: 98.185.. Val loss: 55.268.. Train L1 norm: 2.203.. Val L1 norm: 1.011.. Train Linf norm: 1224.317.. Val Linf norm: 10.306\n",
            "Epoch 134/136.. Train loss: 133.571.. Val loss: 55.269.. Train L1 norm: 1.084.. Val L1 norm: 1.011.. Train Linf norm: 79.164.. Val Linf norm: 10.296\n",
            "Epoch 135/136.. Train loss: 228.127.. Val loss: 55.270.. Train L1 norm: 3.990.. Val L1 norm: 1.011.. Train Linf norm: 3056.051.. Val Linf norm: 10.284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:44:01,941]\u001b[0m Trial 115 finished with value: 1.0113479706446329 and parameters: {'n_layers': 6, 'n_units_0': 1260, 'n_units_1': 133, 'n_units_2': 3617, 'n_units_3': 513, 'n_units_4': 115, 'n_units_5': 1368, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 1.0035762172077518e-06, 'batch_size': 1024, 'n_epochs': 136, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.21203882043757916, 'dropout_rate': 0.05228246875070297, 'weight_decay': 0.0008872694155290681, 'beta1': 0.9359444808446425, 'beta2': 0.9992879398990946, 'factor': 0.13761113466948488, 'patience': 6, 'threshold': 0.0020157189205415875}. Best is trial 92 with value: 1.0098264760335287.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 136/136.. Train loss: 89.251.. Val loss: 55.270.. Train L1 norm: 2.045.. Val L1 norm: 1.011.. Train Linf norm: 1061.108.. Val Linf norm: 10.279\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:44:04,063]\u001b[0m Trial 116 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/129.. Train loss: 4712.733.. Val loss: 52.198.. Train L1 norm: 6.839.. Val L1 norm: 1.133.. Train Linf norm: 5916.869.. Val Linf norm: 90.701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:44:05,983]\u001b[0m Trial 117 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/134.. Train loss: 19999.822.. Val loss: 59.635.. Train L1 norm: 13.687.. Val L1 norm: 1.222.. Train Linf norm: 12856.630.. Val Linf norm: 140.851\n",
            "Epoch 1/136.. Train loss: 663.174.. Val loss: 55.743.. Train L1 norm: 7.058.. Val L1 norm: 1.016.. Train Linf norm: 6145.651.. Val Linf norm: 13.346\n",
            "Epoch 2/136.. Train loss: 61.037.. Val loss: 54.695.. Train L1 norm: 1.924.. Val L1 norm: 1.026.. Train Linf norm: 942.021.. Val Linf norm: 23.991\n",
            "Epoch 3/136.. Train loss: 357.565.. Val loss: 56.106.. Train L1 norm: 2.353.. Val L1 norm: 1.025.. Train Linf norm: 1377.930.. Val Linf norm: 18.559\n",
            "Epoch 4/136.. Train loss: 402.051.. Val loss: 55.163.. Train L1 norm: 1.520.. Val L1 norm: 1.020.. Train Linf norm: 527.538.. Val Linf norm: 18.664\n",
            "Epoch 5/136.. Train loss: 128.080.. Val loss: 55.880.. Train L1 norm: 2.787.. Val L1 norm: 1.016.. Train Linf norm: 1823.313.. Val Linf norm: 12.024\n",
            "Epoch 6/136.. Train loss: 387.043.. Val loss: 54.841.. Train L1 norm: 1.053.. Val L1 norm: 1.036.. Train Linf norm: 48.237.. Val Linf norm: 31.632\n",
            "Epoch 7/136.. Train loss: 83.714.. Val loss: 55.227.. Train L1 norm: 1.426.. Val L1 norm: 1.027.. Train Linf norm: 428.969.. Val Linf norm: 24.552\n",
            "Epoch 8/136.. Train loss: 61.479.. Val loss: 55.035.. Train L1 norm: 2.163.. Val L1 norm: 1.035.. Train Linf norm: 1185.449.. Val Linf norm: 31.407\n",
            "Epoch 9/136.. Train loss: 226.316.. Val loss: 53.968.. Train L1 norm: 2.704.. Val L1 norm: 1.080.. Train Linf norm: 1733.124.. Val Linf norm: 62.201\n",
            "Epoch 10/136.. Train loss: 208.416.. Val loss: 54.723.. Train L1 norm: 3.503.. Val L1 norm: 1.051.. Train Linf norm: 2548.924.. Val Linf norm: 42.760\n",
            "Epoch 11/136.. Train loss: 60.524.. Val loss: 54.784.. Train L1 norm: 3.149.. Val L1 norm: 1.052.. Train Linf norm: 2188.861.. Val Linf norm: 43.652\n",
            "Epoch 12/136.. Train loss: 220.475.. Val loss: 55.781.. Train L1 norm: 1.055.. Val L1 norm: 1.029.. Train Linf norm: 47.470.. Val Linf norm: 24.899\n",
            "Epoch 13/136.. Train loss: 110.815.. Val loss: 55.454.. Train L1 norm: 1.821.. Val L1 norm: 1.036.. Train Linf norm: 833.921.. Val Linf norm: 32.250\n",
            "Epoch 14/136.. Train loss: 157.825.. Val loss: 55.054.. Train L1 norm: 1.827.. Val L1 norm: 1.048.. Train Linf norm: 838.766.. Val Linf norm: 41.844\n",
            "Epoch 15/136.. Train loss: 100.991.. Val loss: 54.732.. Train L1 norm: 2.092.. Val L1 norm: 1.059.. Train Linf norm: 1107.195.. Val Linf norm: 49.350\n",
            "Epoch 16/136.. Train loss: 61.569.. Val loss: 54.588.. Train L1 norm: 2.249.. Val L1 norm: 1.066.. Train Linf norm: 1265.244.. Val Linf norm: 53.708\n",
            "Epoch 17/136.. Train loss: 160.897.. Val loss: 54.949.. Train L1 norm: 2.270.. Val L1 norm: 1.054.. Train Linf norm: 1290.492.. Val Linf norm: 46.400\n",
            "Epoch 18/136.. Train loss: 67.033.. Val loss: 54.547.. Train L1 norm: 2.162.. Val L1 norm: 1.070.. Train Linf norm: 1177.340.. Val Linf norm: 56.584\n",
            "Epoch 19/136.. Train loss: 114.843.. Val loss: 54.119.. Train L1 norm: 2.308.. Val L1 norm: 1.088.. Train Linf norm: 1318.276.. Val Linf norm: 68.523\n",
            "Epoch 20/136.. Train loss: 127.444.. Val loss: 53.730.. Train L1 norm: 2.251.. Val L1 norm: 1.107.. Train Linf norm: 1253.047.. Val Linf norm: 80.342\n",
            "Epoch 21/136.. Train loss: 120.423.. Val loss: 54.079.. Train L1 norm: 3.038.. Val L1 norm: 1.093.. Train Linf norm: 2051.461.. Val Linf norm: 72.000\n",
            "Epoch 22/136.. Train loss: 76.869.. Val loss: 54.311.. Train L1 norm: 3.588.. Val L1 norm: 1.085.. Train Linf norm: 2624.317.. Val Linf norm: 66.724\n",
            "Epoch 23/136.. Train loss: 80.898.. Val loss: 54.617.. Train L1 norm: 2.082.. Val L1 norm: 1.075.. Train Linf norm: 1085.001.. Val Linf norm: 60.842\n",
            "Epoch 24/136.. Train loss: 144.632.. Val loss: 54.233.. Train L1 norm: 3.324.. Val L1 norm: 1.092.. Train Linf norm: 2355.649.. Val Linf norm: 71.392\n",
            "Epoch 25/136.. Train loss: 165.132.. Val loss: 54.037.. Train L1 norm: 2.393.. Val L1 norm: 1.101.. Train Linf norm: 1399.034.. Val Linf norm: 77.016\n",
            "Epoch 26/136.. Train loss: 66.211.. Val loss: 54.104.. Train L1 norm: 3.748.. Val L1 norm: 1.098.. Train Linf norm: 2788.632.. Val Linf norm: 75.598\n",
            "Epoch 27/136.. Train loss: 133.135.. Val loss: 54.270.. Train L1 norm: 2.623.. Val L1 norm: 1.092.. Train Linf norm: 1633.582.. Val Linf norm: 71.330\n",
            "Epoch 28/136.. Train loss: 112.160.. Val loss: 54.121.. Train L1 norm: 2.343.. Val L1 norm: 1.099.. Train Linf norm: 1346.792.. Val Linf norm: 75.913\n",
            "Epoch 29/136.. Train loss: 60.691.. Val loss: 54.156.. Train L1 norm: 3.436.. Val L1 norm: 1.098.. Train Linf norm: 2466.526.. Val Linf norm: 75.344\n",
            "Epoch 30/136.. Train loss: 70.196.. Val loss: 54.023.. Train L1 norm: 2.338.. Val L1 norm: 1.104.. Train Linf norm: 1332.327.. Val Linf norm: 79.587\n",
            "Epoch 31/136.. Train loss: 69.449.. Val loss: 54.116.. Train L1 norm: 3.824.. Val L1 norm: 1.101.. Train Linf norm: 2860.740.. Val Linf norm: 77.217\n",
            "Epoch 32/136.. Train loss: 107.045.. Val loss: 54.272.. Train L1 norm: 4.109.. Val L1 norm: 1.094.. Train Linf norm: 3157.463.. Val Linf norm: 73.348\n",
            "Epoch 33/136.. Train loss: 214.098.. Val loss: 54.456.. Train L1 norm: 2.401.. Val L1 norm: 1.088.. Train Linf norm: 1409.433.. Val Linf norm: 69.000\n",
            "Epoch 34/136.. Train loss: 61.690.. Val loss: 54.387.. Train L1 norm: 2.344.. Val L1 norm: 1.091.. Train Linf norm: 1355.208.. Val Linf norm: 71.160\n",
            "Epoch 35/136.. Train loss: 59.805.. Val loss: 54.360.. Train L1 norm: 2.331.. Val L1 norm: 1.093.. Train Linf norm: 1339.501.. Val Linf norm: 72.280\n",
            "Epoch 36/136.. Train loss: 138.482.. Val loss: 54.524.. Train L1 norm: 1.851.. Val L1 norm: 1.087.. Train Linf norm: 839.143.. Val Linf norm: 68.893\n",
            "Epoch 37/136.. Train loss: 229.720.. Val loss: 54.432.. Train L1 norm: 3.042.. Val L1 norm: 1.091.. Train Linf norm: 2068.851.. Val Linf norm: 71.394\n",
            "Epoch 38/136.. Train loss: 75.497.. Val loss: 54.471.. Train L1 norm: 2.904.. Val L1 norm: 1.090.. Train Linf norm: 1924.660.. Val Linf norm: 70.625\n",
            "Epoch 39/136.. Train loss: 62.023.. Val loss: 54.446.. Train L1 norm: 2.315.. Val L1 norm: 1.091.. Train Linf norm: 1323.414.. Val Linf norm: 71.442\n",
            "Epoch 40/136.. Train loss: 59.787.. Val loss: 54.442.. Train L1 norm: 2.168.. Val L1 norm: 1.091.. Train Linf norm: 1173.157.. Val Linf norm: 71.733\n",
            "Epoch 41/136.. Train loss: 60.066.. Val loss: 54.447.. Train L1 norm: 1.988.. Val L1 norm: 1.092.. Train Linf norm: 984.302.. Val Linf norm: 71.771\n",
            "Epoch 42/136.. Train loss: 89.539.. Val loss: 54.365.. Train L1 norm: 3.776.. Val L1 norm: 1.095.. Train Linf norm: 2817.776.. Val Linf norm: 73.861\n",
            "Epoch 43/136.. Train loss: 68.091.. Val loss: 54.294.. Train L1 norm: 3.098.. Val L1 norm: 1.098.. Train Linf norm: 2122.584.. Val Linf norm: 75.837\n",
            "Epoch 44/136.. Train loss: 65.077.. Val loss: 54.342.. Train L1 norm: 4.873.. Val L1 norm: 1.096.. Train Linf norm: 3940.894.. Val Linf norm: 74.764\n",
            "Epoch 45/136.. Train loss: 141.000.. Val loss: 54.243.. Train L1 norm: 3.068.. Val L1 norm: 1.101.. Train Linf norm: 2091.611.. Val Linf norm: 77.452\n",
            "Epoch 46/136.. Train loss: 74.851.. Val loss: 54.282.. Train L1 norm: 3.040.. Val L1 norm: 1.099.. Train Linf norm: 2060.714.. Val Linf norm: 76.614\n",
            "Epoch 47/136.. Train loss: 180.385.. Val loss: 54.210.. Train L1 norm: 3.454.. Val L1 norm: 1.102.. Train Linf norm: 2486.562.. Val Linf norm: 78.663\n",
            "Epoch 48/136.. Train loss: 84.809.. Val loss: 54.149.. Train L1 norm: 2.887.. Val L1 norm: 1.105.. Train Linf norm: 1906.122.. Val Linf norm: 80.465\n",
            "Epoch 49/136.. Train loss: 62.358.. Val loss: 54.134.. Train L1 norm: 1.875.. Val L1 norm: 1.106.. Train Linf norm: 866.424.. Val Linf norm: 80.953\n",
            "Epoch 50/136.. Train loss: 74.578.. Val loss: 54.154.. Train L1 norm: 3.879.. Val L1 norm: 1.105.. Train Linf norm: 2918.434.. Val Linf norm: 80.464\n",
            "Epoch 51/136.. Train loss: 61.121.. Val loss: 54.164.. Train L1 norm: 3.586.. Val L1 norm: 1.105.. Train Linf norm: 2619.092.. Val Linf norm: 80.279\n",
            "Epoch 52/136.. Train loss: 112.588.. Val loss: 54.191.. Train L1 norm: 4.695.. Val L1 norm: 1.104.. Train Linf norm: 3754.488.. Val Linf norm: 79.608\n",
            "Epoch 53/136.. Train loss: 59.627.. Val loss: 54.185.. Train L1 norm: 3.305.. Val L1 norm: 1.104.. Train Linf norm: 2323.801.. Val Linf norm: 79.853\n",
            "Epoch 54/136.. Train loss: 64.534.. Val loss: 54.163.. Train L1 norm: 4.145.. Val L1 norm: 1.105.. Train Linf norm: 3194.755.. Val Linf norm: 80.514\n",
            "Epoch 55/136.. Train loss: 60.631.. Val loss: 54.147.. Train L1 norm: 3.319.. Val L1 norm: 1.106.. Train Linf norm: 2346.040.. Val Linf norm: 81.015\n",
            "Epoch 56/136.. Train loss: 59.743.. Val loss: 54.138.. Train L1 norm: 3.130.. Val L1 norm: 1.107.. Train Linf norm: 2153.224.. Val Linf norm: 81.313\n",
            "Epoch 57/136.. Train loss: 127.755.. Val loss: 54.172.. Train L1 norm: 3.818.. Val L1 norm: 1.105.. Train Linf norm: 2855.929.. Val Linf norm: 80.471\n",
            "Epoch 58/136.. Train loss: 59.547.. Val loss: 54.164.. Train L1 norm: 3.393.. Val L1 norm: 1.106.. Train Linf norm: 2419.313.. Val Linf norm: 80.765\n",
            "Epoch 59/136.. Train loss: 75.275.. Val loss: 54.132.. Train L1 norm: 3.038.. Val L1 norm: 1.107.. Train Linf norm: 2055.675.. Val Linf norm: 81.668\n",
            "Epoch 60/136.. Train loss: 89.577.. Val loss: 54.158.. Train L1 norm: 3.084.. Val L1 norm: 1.106.. Train Linf norm: 2104.140.. Val Linf norm: 81.103\n",
            "Epoch 61/136.. Train loss: 59.727.. Val loss: 54.160.. Train L1 norm: 2.723.. Val L1 norm: 1.106.. Train Linf norm: 1735.977.. Val Linf norm: 81.073\n",
            "Epoch 62/136.. Train loss: 81.432.. Val loss: 54.147.. Train L1 norm: 3.607.. Val L1 norm: 1.107.. Train Linf norm: 2638.475.. Val Linf norm: 81.463\n",
            "Epoch 63/136.. Train loss: 81.122.. Val loss: 54.135.. Train L1 norm: 2.956.. Val L1 norm: 1.107.. Train Linf norm: 1972.490.. Val Linf norm: 81.804\n",
            "Epoch 64/136.. Train loss: 168.813.. Val loss: 54.146.. Train L1 norm: 3.801.. Val L1 norm: 1.107.. Train Linf norm: 2833.911.. Val Linf norm: 81.523\n",
            "Epoch 65/136.. Train loss: 332.771.. Val loss: 54.157.. Train L1 norm: 3.585.. Val L1 norm: 1.106.. Train Linf norm: 2612.986.. Val Linf norm: 81.250\n",
            "Epoch 66/136.. Train loss: 163.151.. Val loss: 54.167.. Train L1 norm: 4.205.. Val L1 norm: 1.106.. Train Linf norm: 3252.446.. Val Linf norm: 81.056\n",
            "Epoch 67/136.. Train loss: 104.235.. Val loss: 54.175.. Train L1 norm: 3.261.. Val L1 norm: 1.106.. Train Linf norm: 2281.790.. Val Linf norm: 80.872\n",
            "Epoch 68/136.. Train loss: 63.675.. Val loss: 54.170.. Train L1 norm: 2.363.. Val L1 norm: 1.106.. Train Linf norm: 1364.877.. Val Linf norm: 81.037\n",
            "Epoch 69/136.. Train loss: 129.963.. Val loss: 54.156.. Train L1 norm: 3.504.. Val L1 norm: 1.107.. Train Linf norm: 2534.437.. Val Linf norm: 81.435\n",
            "Epoch 70/136.. Train loss: 61.370.. Val loss: 54.152.. Train L1 norm: 2.743.. Val L1 norm: 1.107.. Train Linf norm: 1756.861.. Val Linf norm: 81.578\n",
            "Epoch 71/136.. Train loss: 60.139.. Val loss: 54.155.. Train L1 norm: 3.590.. Val L1 norm: 1.107.. Train Linf norm: 2621.615.. Val Linf norm: 81.522\n",
            "Epoch 72/136.. Train loss: 59.777.. Val loss: 54.156.. Train L1 norm: 2.503.. Val L1 norm: 1.107.. Train Linf norm: 1509.781.. Val Linf norm: 81.529\n",
            "Epoch 73/136.. Train loss: 107.698.. Val loss: 54.161.. Train L1 norm: 3.630.. Val L1 norm: 1.107.. Train Linf norm: 2664.975.. Val Linf norm: 81.392\n",
            "Epoch 74/136.. Train loss: 137.115.. Val loss: 54.166.. Train L1 norm: 3.266.. Val L1 norm: 1.106.. Train Linf norm: 2289.021.. Val Linf norm: 81.268\n",
            "Epoch 75/136.. Train loss: 84.699.. Val loss: 54.170.. Train L1 norm: 2.932.. Val L1 norm: 1.106.. Train Linf norm: 1950.449.. Val Linf norm: 81.185\n",
            "Epoch 76/136.. Train loss: 82.642.. Val loss: 54.165.. Train L1 norm: 2.962.. Val L1 norm: 1.106.. Train Linf norm: 1979.965.. Val Linf norm: 81.316\n",
            "Epoch 77/136.. Train loss: 67.307.. Val loss: 54.169.. Train L1 norm: 3.482.. Val L1 norm: 1.106.. Train Linf norm: 2511.195.. Val Linf norm: 81.241\n",
            "Epoch 78/136.. Train loss: 66.365.. Val loss: 54.171.. Train L1 norm: 2.983.. Val L1 norm: 1.106.. Train Linf norm: 2000.655.. Val Linf norm: 81.180\n",
            "Epoch 79/136.. Train loss: 91.989.. Val loss: 54.175.. Train L1 norm: 2.379.. Val L1 norm: 1.106.. Train Linf norm: 1383.215.. Val Linf norm: 81.075\n",
            "Epoch 80/136.. Train loss: 65.124.. Val loss: 54.172.. Train L1 norm: 3.451.. Val L1 norm: 1.106.. Train Linf norm: 2480.830.. Val Linf norm: 81.166\n",
            "Epoch 81/136.. Train loss: 204.031.. Val loss: 54.178.. Train L1 norm: 2.933.. Val L1 norm: 1.106.. Train Linf norm: 1947.581.. Val Linf norm: 81.039\n",
            "Epoch 82/136.. Train loss: 167.627.. Val loss: 54.182.. Train L1 norm: 3.542.. Val L1 norm: 1.106.. Train Linf norm: 2571.457.. Val Linf norm: 80.930\n",
            "Epoch 83/136.. Train loss: 184.771.. Val loss: 54.177.. Train L1 norm: 2.986.. Val L1 norm: 1.106.. Train Linf norm: 2003.305.. Val Linf norm: 81.066\n",
            "Epoch 84/136.. Train loss: 93.019.. Val loss: 54.181.. Train L1 norm: 3.960.. Val L1 norm: 1.106.. Train Linf norm: 3000.678.. Val Linf norm: 80.970\n",
            "Epoch 85/136.. Train loss: 137.534.. Val loss: 54.183.. Train L1 norm: 2.688.. Val L1 norm: 1.106.. Train Linf norm: 1673.998.. Val Linf norm: 80.927\n",
            "Epoch 86/136.. Train loss: 87.067.. Val loss: 54.181.. Train L1 norm: 3.095.. Val L1 norm: 1.106.. Train Linf norm: 2118.138.. Val Linf norm: 80.970\n",
            "Epoch 87/136.. Train loss: 68.540.. Val loss: 54.182.. Train L1 norm: 3.615.. Val L1 norm: 1.106.. Train Linf norm: 2647.709.. Val Linf norm: 80.947\n",
            "Epoch 88/136.. Train loss: 121.225.. Val loss: 54.185.. Train L1 norm: 1.493.. Val L1 norm: 1.106.. Train Linf norm: 474.566.. Val Linf norm: 80.885\n",
            "Epoch 89/136.. Train loss: 98.768.. Val loss: 54.186.. Train L1 norm: 4.045.. Val L1 norm: 1.106.. Train Linf norm: 3076.465.. Val Linf norm: 80.862\n",
            "Epoch 90/136.. Train loss: 59.757.. Val loss: 54.186.. Train L1 norm: 3.156.. Val L1 norm: 1.106.. Train Linf norm: 2179.421.. Val Linf norm: 80.863\n",
            "Epoch 91/136.. Train loss: 223.419.. Val loss: 54.189.. Train L1 norm: 3.508.. Val L1 norm: 1.106.. Train Linf norm: 2537.475.. Val Linf norm: 80.801\n",
            "Epoch 92/136.. Train loss: 78.684.. Val loss: 54.187.. Train L1 norm: 4.891.. Val L1 norm: 1.106.. Train Linf norm: 3955.577.. Val Linf norm: 80.842\n",
            "Epoch 93/136.. Train loss: 60.216.. Val loss: 54.186.. Train L1 norm: 2.400.. Val L1 norm: 1.106.. Train Linf norm: 1403.962.. Val Linf norm: 80.862\n",
            "Epoch 94/136.. Train loss: 60.016.. Val loss: 54.187.. Train L1 norm: 2.824.. Val L1 norm: 1.106.. Train Linf norm: 1840.964.. Val Linf norm: 80.860\n",
            "Epoch 95/136.. Train loss: 81.185.. Val loss: 54.185.. Train L1 norm: 2.863.. Val L1 norm: 1.106.. Train Linf norm: 1876.781.. Val Linf norm: 80.918\n",
            "Epoch 96/136.. Train loss: 64.608.. Val loss: 54.184.. Train L1 norm: 4.053.. Val L1 norm: 1.106.. Train Linf norm: 3091.965.. Val Linf norm: 80.953\n",
            "Epoch 97/136.. Train loss: 115.946.. Val loss: 54.183.. Train L1 norm: 3.878.. Val L1 norm: 1.106.. Train Linf norm: 2920.081.. Val Linf norm: 80.981\n",
            "Epoch 98/136.. Train loss: 68.067.. Val loss: 54.182.. Train L1 norm: 3.327.. Val L1 norm: 1.106.. Train Linf norm: 2349.029.. Val Linf norm: 81.003\n",
            "Epoch 99/136.. Train loss: 80.402.. Val loss: 54.181.. Train L1 norm: 3.850.. Val L1 norm: 1.106.. Train Linf norm: 2891.213.. Val Linf norm: 81.025\n",
            "Epoch 100/136.. Train loss: 59.614.. Val loss: 54.181.. Train L1 norm: 2.705.. Val L1 norm: 1.106.. Train Linf norm: 655.129.. Val Linf norm: 81.031\n",
            "Epoch 101/136.. Train loss: 132.253.. Val loss: 54.182.. Train L1 norm: 3.115.. Val L1 norm: 1.106.. Train Linf norm: 2136.328.. Val Linf norm: 81.010\n",
            "Epoch 102/136.. Train loss: 61.917.. Val loss: 54.182.. Train L1 norm: 3.349.. Val L1 norm: 1.106.. Train Linf norm: 2377.702.. Val Linf norm: 81.004\n",
            "Epoch 103/136.. Train loss: 109.746.. Val loss: 54.181.. Train L1 norm: 2.734.. Val L1 norm: 1.106.. Train Linf norm: 1747.564.. Val Linf norm: 81.030\n",
            "Epoch 104/136.. Train loss: 63.488.. Val loss: 54.182.. Train L1 norm: 3.559.. Val L1 norm: 1.106.. Train Linf norm: 2590.292.. Val Linf norm: 81.022\n",
            "Epoch 105/136.. Train loss: 63.178.. Val loss: 54.182.. Train L1 norm: 3.399.. Val L1 norm: 1.106.. Train Linf norm: 2427.010.. Val Linf norm: 81.010\n",
            "Epoch 106/136.. Train loss: 65.247.. Val loss: 54.183.. Train L1 norm: 3.170.. Val L1 norm: 1.106.. Train Linf norm: 2191.192.. Val Linf norm: 80.998\n",
            "Epoch 107/136.. Train loss: 66.125.. Val loss: 54.183.. Train L1 norm: 4.469.. Val L1 norm: 1.106.. Train Linf norm: 3521.730.. Val Linf norm: 80.981\n",
            "Epoch 108/136.. Train loss: 176.770.. Val loss: 54.182.. Train L1 norm: 3.704.. Val L1 norm: 1.106.. Train Linf norm: 2741.395.. Val Linf norm: 81.012\n",
            "Epoch 109/136.. Train loss: 80.525.. Val loss: 54.182.. Train L1 norm: 3.097.. Val L1 norm: 1.106.. Train Linf norm: 2116.656.. Val Linf norm: 81.006\n",
            "Epoch 110/136.. Train loss: 110.001.. Val loss: 54.182.. Train L1 norm: 3.104.. Val L1 norm: 1.106.. Train Linf norm: 2126.572.. Val Linf norm: 81.017\n",
            "Epoch 111/136.. Train loss: 60.940.. Val loss: 54.182.. Train L1 norm: 3.382.. Val L1 norm: 1.106.. Train Linf norm: 2410.784.. Val Linf norm: 81.016\n",
            "Epoch 112/136.. Train loss: 60.078.. Val loss: 54.182.. Train L1 norm: 3.247.. Val L1 norm: 1.106.. Train Linf norm: 2274.814.. Val Linf norm: 81.020\n",
            "Epoch 113/136.. Train loss: 67.154.. Val loss: 54.182.. Train L1 norm: 4.342.. Val L1 norm: 1.106.. Train Linf norm: 3394.477.. Val Linf norm: 81.028\n",
            "Epoch 114/136.. Train loss: 218.416.. Val loss: 54.181.. Train L1 norm: 3.449.. Val L1 norm: 1.106.. Train Linf norm: 2477.186.. Val Linf norm: 81.039\n",
            "Epoch 115/136.. Train loss: 79.228.. Val loss: 54.182.. Train L1 norm: 3.818.. Val L1 norm: 1.106.. Train Linf norm: 2856.389.. Val Linf norm: 81.034\n",
            "Epoch 116/136.. Train loss: 80.386.. Val loss: 54.181.. Train L1 norm: 3.361.. Val L1 norm: 1.106.. Train Linf norm: 2388.865.. Val Linf norm: 81.045\n",
            "Epoch 117/136.. Train loss: 59.820.. Val loss: 54.181.. Train L1 norm: 3.381.. Val L1 norm: 1.106.. Train Linf norm: 2408.150.. Val Linf norm: 81.049\n",
            "Epoch 118/136.. Train loss: 121.756.. Val loss: 54.181.. Train L1 norm: 3.142.. Val L1 norm: 1.106.. Train Linf norm: 2162.954.. Val Linf norm: 81.059\n",
            "Epoch 119/136.. Train loss: 63.614.. Val loss: 54.180.. Train L1 norm: 2.813.. Val L1 norm: 1.106.. Train Linf norm: 1827.777.. Val Linf norm: 81.065\n",
            "Epoch 120/136.. Train loss: 69.301.. Val loss: 54.181.. Train L1 norm: 3.330.. Val L1 norm: 1.106.. Train Linf norm: 2357.053.. Val Linf norm: 81.057\n",
            "Epoch 121/136.. Train loss: 220.824.. Val loss: 54.181.. Train L1 norm: 2.597.. Val L1 norm: 1.106.. Train Linf norm: 1592.894.. Val Linf norm: 81.053\n",
            "Epoch 122/136.. Train loss: 118.114.. Val loss: 54.181.. Train L1 norm: 2.032.. Val L1 norm: 1.106.. Train Linf norm: 1024.029.. Val Linf norm: 81.050\n",
            "Epoch 123/136.. Train loss: 407.334.. Val loss: 54.181.. Train L1 norm: 3.889.. Val L1 norm: 1.106.. Train Linf norm: 2930.579.. Val Linf norm: 81.045\n",
            "Epoch 124/136.. Train loss: 85.080.. Val loss: 54.181.. Train L1 norm: 2.472.. Val L1 norm: 1.106.. Train Linf norm: 1477.117.. Val Linf norm: 81.043\n",
            "Epoch 125/136.. Train loss: 67.389.. Val loss: 54.181.. Train L1 norm: 1.704.. Val L1 norm: 1.106.. Train Linf norm: 692.212.. Val Linf norm: 81.042\n",
            "Epoch 126/136.. Train loss: 66.430.. Val loss: 54.181.. Train L1 norm: 3.486.. Val L1 norm: 1.106.. Train Linf norm: 2517.742.. Val Linf norm: 81.040\n",
            "Epoch 127/136.. Train loss: 77.265.. Val loss: 54.181.. Train L1 norm: 3.155.. Val L1 norm: 1.106.. Train Linf norm: 2174.924.. Val Linf norm: 81.044\n",
            "Epoch 128/136.. Train loss: 66.520.. Val loss: 54.181.. Train L1 norm: 3.900.. Val L1 norm: 1.106.. Train Linf norm: 2939.673.. Val Linf norm: 81.041\n",
            "Epoch 129/136.. Train loss: 145.817.. Val loss: 54.182.. Train L1 norm: 3.704.. Val L1 norm: 1.106.. Train Linf norm: 2739.904.. Val Linf norm: 81.038\n",
            "Epoch 130/136.. Train loss: 122.701.. Val loss: 54.181.. Train L1 norm: 2.401.. Val L1 norm: 1.106.. Train Linf norm: 1405.310.. Val Linf norm: 81.042\n",
            "Epoch 131/136.. Train loss: 193.635.. Val loss: 54.182.. Train L1 norm: 3.720.. Val L1 norm: 1.106.. Train Linf norm: 2757.349.. Val Linf norm: 81.039\n",
            "Epoch 132/136.. Train loss: 63.061.. Val loss: 54.182.. Train L1 norm: 3.489.. Val L1 norm: 1.106.. Train Linf norm: 2519.235.. Val Linf norm: 81.038\n",
            "Epoch 133/136.. Train loss: 128.909.. Val loss: 54.182.. Train L1 norm: 2.468.. Val L1 norm: 1.106.. Train Linf norm: 1473.542.. Val Linf norm: 81.036\n",
            "Epoch 134/136.. Train loss: 59.853.. Val loss: 54.182.. Train L1 norm: 2.498.. Val L1 norm: 1.106.. Train Linf norm: 1504.883.. Val Linf norm: 81.036\n",
            "Epoch 135/136.. Train loss: 98.823.. Val loss: 54.182.. Train L1 norm: 2.896.. Val L1 norm: 1.106.. Train Linf norm: 1914.153.. Val Linf norm: 81.035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:47:45,017]\u001b[0m Trial 118 finished with value: 1.105981767209371 and parameters: {'n_layers': 7, 'n_units_0': 1944, 'n_units_1': 174, 'n_units_2': 3733, 'n_units_3': 473, 'n_units_4': 141, 'n_units_5': 1447, 'n_units_6': 1594, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'RMSprop', 'lr': 3.988911789934381e-06, 'batch_size': 1024, 'n_epochs': 136, 'scheduler': 'StepLR', 'prelu_init': 0.20851536327963663, 'dropout_rate': 0.10230066492020566, 'step_size': 12, 'gamma': 0.4178902691922051}. Best is trial 92 with value: 1.0098264760335287.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 136/136.. Train loss: 136.314.. Val loss: 54.182.. Train L1 norm: 1.625.. Val L1 norm: 1.106.. Train Linf norm: 609.280.. Val Linf norm: 81.034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:47:46,889]\u001b[0m Trial 119 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/140.. Train loss: 718.265.. Val loss: 48.846.. Train L1 norm: 9.770.. Val L1 norm: 1.571.. Train Linf norm: 17640.233.. Val Linf norm: 574.265\n",
            "Epoch 1/92.. Train loss: 4.730.. Val loss: 4.556.. Train L1 norm: 1.217.. Val L1 norm: 1.024.. Train Linf norm: 50.730.. Val Linf norm: 7.390\n",
            "Epoch 2/92.. Train loss: 4.557.. Val loss: 4.587.. Train L1 norm: 6.060.. Val L1 norm: 1.062.. Train Linf norm: 1295.309.. Val Linf norm: 14.828\n",
            "Epoch 3/92.. Train loss: 4.515.. Val loss: 4.254.. Train L1 norm: 8.043.. Val L1 norm: 1.345.. Train Linf norm: 1798.017.. Val Linf norm: 68.940\n",
            "Epoch 4/92.. Train loss: 4.724.. Val loss: 4.334.. Train L1 norm: 9.422.. Val L1 norm: 1.229.. Train Linf norm: 2139.140.. Val Linf norm: 49.065\n",
            "Epoch 5/92.. Train loss: 4.566.. Val loss: 4.359.. Train L1 norm: 7.395.. Val L1 norm: 1.198.. Train Linf norm: 1632.146.. Val Linf norm: 43.659\n",
            "Epoch 6/92.. Train loss: 4.485.. Val loss: 4.419.. Train L1 norm: 10.643.. Val L1 norm: 1.143.. Train Linf norm: 2458.882.. Val Linf norm: 31.848\n",
            "Epoch 7/92.. Train loss: 4.390.. Val loss: 4.154.. Train L1 norm: 9.921.. Val L1 norm: 1.524.. Train Linf norm: 2269.478.. Val Linf norm: 99.502\n",
            "Epoch 8/92.. Train loss: 4.679.. Val loss: 4.257.. Train L1 norm: 15.346.. Val L1 norm: 1.355.. Train Linf norm: 3633.506.. Val Linf norm: 71.768\n",
            "Epoch 9/92.. Train loss: 4.502.. Val loss: 4.342.. Train L1 norm: 15.921.. Val L1 norm: 1.248.. Train Linf norm: 3793.978.. Val Linf norm: 52.161\n",
            "Epoch 10/92.. Train loss: 4.316.. Val loss: 4.107.. Train L1 norm: 15.422.. Val L1 norm: 1.622.. Train Linf norm: 3663.548.. Val Linf norm: 115.968\n",
            "Epoch 11/92.. Train loss: 4.620.. Val loss: 4.188.. Train L1 norm: 18.292.. Val L1 norm: 1.480.. Train Linf norm: 4402.418.. Val Linf norm: 93.252\n",
            "Epoch 12/92.. Train loss: 4.502.. Val loss: 4.274.. Train L1 norm: 14.695.. Val L1 norm: 1.361.. Train Linf norm: 3479.607.. Val Linf norm: 72.169\n",
            "Epoch 13/92.. Train loss: 4.399.. Val loss: 4.329.. Train L1 norm: 13.818.. Val L1 norm: 1.314.. Train Linf norm: 3260.851.. Val Linf norm: 61.766\n",
            "Epoch 14/92.. Train loss: 4.428.. Val loss: 4.054.. Train L1 norm: 16.644.. Val L1 norm: 1.742.. Train Linf norm: 3971.671.. Val Linf norm: 135.920\n",
            "Epoch 15/92.. Train loss: 4.527.. Val loss: 4.161.. Train L1 norm: 25.738.. Val L1 norm: 1.552.. Train Linf norm: 6277.350.. Val Linf norm: 105.012\n",
            "Epoch 16/92.. Train loss: 4.376.. Val loss: 4.218.. Train L1 norm: 16.810.. Val L1 norm: 1.483.. Train Linf norm: 4022.393.. Val Linf norm: 91.731\n",
            "Epoch 17/92.. Train loss: 4.345.. Val loss: 3.983.. Train L1 norm: 27.817.. Val L1 norm: 1.910.. Train Linf norm: 6811.646.. Val Linf norm: 162.239\n",
            "Epoch 18/92.. Train loss: 4.394.. Val loss: 4.088.. Train L1 norm: 24.917.. Val L1 norm: 1.705.. Train Linf norm: 6056.910.. Val Linf norm: 129.866\n",
            "Epoch 19/92.. Train loss: 4.169.. Val loss: 3.896.. Train L1 norm: 30.332.. Val L1 norm: 2.135.. Train Linf norm: 7425.650.. Val Linf norm: 196.469\n",
            "Epoch 20/92.. Train loss: 4.627.. Val loss: 4.024.. Train L1 norm: 31.476.. Val L1 norm: 1.843.. Train Linf norm: 7710.613.. Val Linf norm: 152.126\n",
            "Epoch 21/92.. Train loss: 4.277.. Val loss: 4.066.. Train L1 norm: 27.618.. Val L1 norm: 1.771.. Train Linf norm: 6756.339.. Val Linf norm: 140.276\n",
            "Epoch 22/92.. Train loss: 4.255.. Val loss: 3.877.. Train L1 norm: 29.309.. Val L1 norm: 2.204.. Train Linf norm: 7158.042.. Val Linf norm: 207.398\n",
            "Epoch 23/92.. Train loss: 4.507.. Val loss: 4.014.. Train L1 norm: 33.901.. Val L1 norm: 1.881.. Train Linf norm: 8320.125.. Val Linf norm: 157.888\n",
            "Epoch 24/92.. Train loss: 4.305.. Val loss: 4.087.. Train L1 norm: 32.311.. Val L1 norm: 1.750.. Train Linf norm: 7947.448.. Val Linf norm: 136.108\n",
            "Epoch 25/92.. Train loss: 4.172.. Val loss: 3.876.. Train L1 norm: 33.603.. Val L1 norm: 2.224.. Train Linf norm: 8261.292.. Val Linf norm: 210.291\n",
            "Epoch 26/92.. Train loss: 4.430.. Val loss: 3.989.. Train L1 norm: 37.972.. Val L1 norm: 1.957.. Train Linf norm: 9369.110.. Val Linf norm: 169.762\n",
            "Epoch 27/92.. Train loss: 4.297.. Val loss: 4.089.. Train L1 norm: 34.529.. Val L1 norm: 1.778.. Train Linf norm: 8495.922.. Val Linf norm: 140.036\n",
            "Epoch 28/92.. Train loss: 4.305.. Val loss: 3.894.. Train L1 norm: 23.275.. Val L1 norm: 2.203.. Train Linf norm: 5634.545.. Val Linf norm: 207.516\n",
            "Epoch 29/92.. Train loss: 4.302.. Val loss: 3.906.. Train L1 norm: 35.143.. Val L1 norm: 2.174.. Train Linf norm: 8635.983.. Val Linf norm: 203.045\n",
            "Epoch 30/92.. Train loss: 4.249.. Val loss: 3.920.. Train L1 norm: 36.409.. Val L1 norm: 2.142.. Train Linf norm: 8958.828.. Val Linf norm: 198.267\n",
            "Epoch 31/92.. Train loss: 4.168.. Val loss: 3.932.. Train L1 norm: 33.696.. Val L1 norm: 2.114.. Train Linf norm: 8277.134.. Val Linf norm: 193.878\n",
            "Epoch 32/92.. Train loss: 4.216.. Val loss: 3.946.. Train L1 norm: 36.101.. Val L1 norm: 2.083.. Train Linf norm: 8896.016.. Val Linf norm: 189.219\n",
            "Epoch 33/92.. Train loss: 4.213.. Val loss: 3.962.. Train L1 norm: 29.942.. Val L1 norm: 2.051.. Train Linf norm: 7325.143.. Val Linf norm: 184.112\n",
            "Epoch 34/92.. Train loss: 4.212.. Val loss: 3.974.. Train L1 norm: 32.988.. Val L1 norm: 2.025.. Train Linf norm: 8100.736.. Val Linf norm: 180.065\n",
            "Epoch 35/92.. Train loss: 4.203.. Val loss: 3.969.. Train L1 norm: 33.776.. Val L1 norm: 2.036.. Train Linf norm: 8305.827.. Val Linf norm: 181.677\n",
            "Epoch 36/92.. Train loss: 4.205.. Val loss: 3.965.. Train L1 norm: 35.442.. Val L1 norm: 2.045.. Train Linf norm: 8734.719.. Val Linf norm: 183.179\n",
            "Epoch 37/92.. Train loss: 4.177.. Val loss: 3.961.. Train L1 norm: 35.837.. Val L1 norm: 2.055.. Train Linf norm: 8830.889.. Val Linf norm: 184.678\n",
            "Epoch 38/92.. Train loss: 4.236.. Val loss: 3.957.. Train L1 norm: 32.153.. Val L1 norm: 2.063.. Train Linf norm: 7889.036.. Val Linf norm: 185.963\n",
            "Epoch 39/92.. Train loss: 4.247.. Val loss: 3.953.. Train L1 norm: 37.727.. Val L1 norm: 2.073.. Train Linf norm: 9301.255.. Val Linf norm: 187.458\n",
            "Epoch 40/92.. Train loss: 4.177.. Val loss: 3.955.. Train L1 norm: 32.127.. Val L1 norm: 2.069.. Train Linf norm: 7880.959.. Val Linf norm: 186.873\n",
            "Epoch 41/92.. Train loss: 4.133.. Val loss: 3.955.. Train L1 norm: 36.455.. Val L1 norm: 2.068.. Train Linf norm: 8990.448.. Val Linf norm: 186.749\n",
            "Epoch 42/92.. Train loss: 4.143.. Val loss: 3.955.. Train L1 norm: 23.977.. Val L1 norm: 2.069.. Train Linf norm: 5798.326.. Val Linf norm: 186.919\n",
            "Epoch 43/92.. Train loss: 4.147.. Val loss: 3.955.. Train L1 norm: 30.043.. Val L1 norm: 2.070.. Train Linf norm: 7349.721.. Val Linf norm: 187.083\n",
            "Epoch 44/92.. Train loss: 4.175.. Val loss: 3.955.. Train L1 norm: 35.445.. Val L1 norm: 2.069.. Train Linf norm: 8728.001.. Val Linf norm: 186.959\n",
            "Epoch 45/92.. Train loss: 4.210.. Val loss: 3.954.. Train L1 norm: 34.463.. Val L1 norm: 2.070.. Train Linf norm: 8476.402.. Val Linf norm: 187.119\n",
            "Epoch 46/92.. Train loss: 4.179.. Val loss: 3.955.. Train L1 norm: 32.331.. Val L1 norm: 2.070.. Train Linf norm: 7927.215.. Val Linf norm: 186.991\n",
            "Epoch 47/92.. Train loss: 4.274.. Val loss: 3.955.. Train L1 norm: 35.868.. Val L1 norm: 2.070.. Train Linf norm: 8837.982.. Val Linf norm: 187.011\n",
            "Epoch 48/92.. Train loss: 4.159.. Val loss: 3.955.. Train L1 norm: 31.845.. Val L1 norm: 2.070.. Train Linf norm: 7812.513.. Val Linf norm: 187.031\n",
            "Epoch 49/92.. Train loss: 4.227.. Val loss: 3.955.. Train L1 norm: 36.185.. Val L1 norm: 2.070.. Train Linf norm: 8919.606.. Val Linf norm: 187.048\n",
            "Epoch 50/92.. Train loss: 4.198.. Val loss: 3.955.. Train L1 norm: 33.767.. Val L1 norm: 2.070.. Train Linf norm: 8303.354.. Val Linf norm: 187.026\n",
            "Epoch 51/92.. Train loss: 4.155.. Val loss: 3.955.. Train L1 norm: 33.959.. Val L1 norm: 2.070.. Train Linf norm: 8353.813.. Val Linf norm: 187.006\n",
            "Epoch 52/92.. Train loss: 4.154.. Val loss: 3.955.. Train L1 norm: 37.499.. Val L1 norm: 2.070.. Train Linf norm: 9254.390.. Val Linf norm: 186.984\n",
            "Epoch 53/92.. Train loss: 4.178.. Val loss: 3.955.. Train L1 norm: 34.618.. Val L1 norm: 2.070.. Train Linf norm: 8515.889.. Val Linf norm: 187.006\n",
            "Epoch 54/92.. Train loss: 4.163.. Val loss: 3.955.. Train L1 norm: 33.937.. Val L1 norm: 2.070.. Train Linf norm: 8345.457.. Val Linf norm: 187.027\n",
            "Epoch 55/92.. Train loss: 4.257.. Val loss: 3.955.. Train L1 norm: 35.065.. Val L1 norm: 2.070.. Train Linf norm: 8628.248.. Val Linf norm: 187.045\n",
            "Epoch 56/92.. Train loss: 4.165.. Val loss: 3.955.. Train L1 norm: 32.790.. Val L1 norm: 2.070.. Train Linf norm: 8053.339.. Val Linf norm: 187.066\n",
            "Epoch 57/92.. Train loss: 4.171.. Val loss: 3.955.. Train L1 norm: 36.415.. Val L1 norm: 2.070.. Train Linf norm: 8980.376.. Val Linf norm: 187.084\n",
            "Epoch 58/92.. Train loss: 4.135.. Val loss: 3.955.. Train L1 norm: 34.931.. Val L1 norm: 2.070.. Train Linf norm: 8600.599.. Val Linf norm: 187.066\n",
            "Epoch 59/92.. Train loss: 4.131.. Val loss: 3.955.. Train L1 norm: 34.490.. Val L1 norm: 2.070.. Train Linf norm: 8486.801.. Val Linf norm: 187.089\n",
            "Epoch 60/92.. Train loss: 4.220.. Val loss: 3.955.. Train L1 norm: 36.694.. Val L1 norm: 2.070.. Train Linf norm: 9048.898.. Val Linf norm: 187.073\n",
            "Epoch 61/92.. Train loss: 4.296.. Val loss: 3.955.. Train L1 norm: 33.777.. Val L1 norm: 2.070.. Train Linf norm: 8304.424.. Val Linf norm: 187.091\n",
            "Epoch 62/92.. Train loss: 4.177.. Val loss: 3.954.. Train L1 norm: 29.264.. Val L1 norm: 2.070.. Train Linf norm: 7148.884.. Val Linf norm: 187.112\n",
            "Epoch 63/92.. Train loss: 4.145.. Val loss: 3.955.. Train L1 norm: 32.007.. Val L1 norm: 2.070.. Train Linf norm: 7850.605.. Val Linf norm: 187.091\n",
            "Epoch 64/92.. Train loss: 4.211.. Val loss: 3.955.. Train L1 norm: 34.466.. Val L1 norm: 2.070.. Train Linf norm: 8477.039.. Val Linf norm: 187.071\n",
            "Epoch 65/92.. Train loss: 4.192.. Val loss: 3.955.. Train L1 norm: 34.795.. Val L1 norm: 2.070.. Train Linf norm: 8564.654.. Val Linf norm: 187.050\n",
            "Epoch 66/92.. Train loss: 4.190.. Val loss: 3.955.. Train L1 norm: 29.705.. Val L1 norm: 2.070.. Train Linf norm: 7261.592.. Val Linf norm: 187.031\n",
            "Epoch 67/92.. Train loss: 4.264.. Val loss: 3.955.. Train L1 norm: 37.741.. Val L1 norm: 2.070.. Train Linf norm: 9319.442.. Val Linf norm: 187.048\n",
            "Epoch 68/92.. Train loss: 4.157.. Val loss: 3.955.. Train L1 norm: 29.197.. Val L1 norm: 2.070.. Train Linf norm: 7130.945.. Val Linf norm: 187.067\n",
            "Epoch 69/92.. Train loss: 4.218.. Val loss: 3.955.. Train L1 norm: 34.515.. Val L1 norm: 2.070.. Train Linf norm: 8490.889.. Val Linf norm: 187.089\n",
            "Epoch 70/92.. Train loss: 4.207.. Val loss: 3.955.. Train L1 norm: 32.852.. Val L1 norm: 2.070.. Train Linf norm: 8062.645.. Val Linf norm: 187.108\n",
            "Epoch 71/92.. Train loss: 4.191.. Val loss: 3.954.. Train L1 norm: 36.898.. Val L1 norm: 2.070.. Train Linf norm: 9101.900.. Val Linf norm: 187.127\n",
            "Epoch 72/92.. Train loss: 4.235.. Val loss: 3.955.. Train L1 norm: 31.993.. Val L1 norm: 2.070.. Train Linf norm: 7847.981.. Val Linf norm: 187.104\n",
            "Epoch 73/92.. Train loss: 4.212.. Val loss: 3.954.. Train L1 norm: 32.372.. Val L1 norm: 2.070.. Train Linf norm: 7946.269.. Val Linf norm: 187.122\n",
            "Epoch 74/92.. Train loss: 4.248.. Val loss: 3.954.. Train L1 norm: 35.208.. Val L1 norm: 2.071.. Train Linf norm: 8659.526.. Val Linf norm: 187.140\n",
            "Epoch 75/92.. Train loss: 4.176.. Val loss: 3.954.. Train L1 norm: 38.539.. Val L1 norm: 2.070.. Train Linf norm: 9520.605.. Val Linf norm: 187.117\n",
            "Epoch 76/92.. Train loss: 4.198.. Val loss: 3.954.. Train L1 norm: 40.269.. Val L1 norm: 2.071.. Train Linf norm: 9962.433.. Val Linf norm: 187.136\n",
            "Epoch 77/92.. Train loss: 4.230.. Val loss: 3.954.. Train L1 norm: 36.958.. Val L1 norm: 2.071.. Train Linf norm: 9115.825.. Val Linf norm: 187.155\n",
            "Epoch 78/92.. Train loss: 4.199.. Val loss: 3.954.. Train L1 norm: 30.160.. Val L1 norm: 2.071.. Train Linf norm: 7375.820.. Val Linf norm: 187.175\n",
            "Epoch 79/92.. Train loss: 4.138.. Val loss: 3.954.. Train L1 norm: 36.912.. Val L1 norm: 2.071.. Train Linf norm: 9108.719.. Val Linf norm: 187.154\n",
            "Epoch 80/92.. Train loss: 4.198.. Val loss: 3.954.. Train L1 norm: 30.688.. Val L1 norm: 2.070.. Train Linf norm: 7514.011.. Val Linf norm: 187.131\n",
            "Epoch 81/92.. Train loss: 4.164.. Val loss: 3.954.. Train L1 norm: 30.954.. Val L1 norm: 2.071.. Train Linf norm: 7582.362.. Val Linf norm: 187.151\n",
            "Epoch 82/92.. Train loss: 4.194.. Val loss: 3.954.. Train L1 norm: 29.663.. Val L1 norm: 2.071.. Train Linf norm: 7249.557.. Val Linf norm: 187.170\n",
            "Epoch 83/92.. Train loss: 4.149.. Val loss: 3.954.. Train L1 norm: 34.227.. Val L1 norm: 2.071.. Train Linf norm: 8420.358.. Val Linf norm: 187.151\n",
            "Epoch 84/92.. Train loss: 4.163.. Val loss: 3.954.. Train L1 norm: 31.762.. Val L1 norm: 2.071.. Train Linf norm: 7791.532.. Val Linf norm: 187.173\n",
            "Epoch 85/92.. Train loss: 4.257.. Val loss: 3.954.. Train L1 norm: 35.682.. Val L1 norm: 2.071.. Train Linf norm: 8788.183.. Val Linf norm: 187.192\n",
            "Epoch 86/92.. Train loss: 4.227.. Val loss: 3.954.. Train L1 norm: 37.167.. Val L1 norm: 2.071.. Train Linf norm: 9168.450.. Val Linf norm: 187.211\n",
            "Epoch 87/92.. Train loss: 4.152.. Val loss: 3.954.. Train L1 norm: 35.572.. Val L1 norm: 2.071.. Train Linf norm: 8760.547.. Val Linf norm: 187.231\n",
            "Epoch 88/92.. Train loss: 4.208.. Val loss: 3.954.. Train L1 norm: 32.071.. Val L1 norm: 2.071.. Train Linf norm: 7864.096.. Val Linf norm: 187.251\n",
            "Epoch 89/92.. Train loss: 4.165.. Val loss: 3.954.. Train L1 norm: 36.758.. Val L1 norm: 2.071.. Train Linf norm: 9067.660.. Val Linf norm: 187.271\n",
            "Epoch 90/92.. Train loss: 4.140.. Val loss: 3.954.. Train L1 norm: 31.906.. Val L1 norm: 2.072.. Train Linf norm: 7823.592.. Val Linf norm: 187.290\n",
            "Epoch 91/92.. Train loss: 4.196.. Val loss: 3.954.. Train L1 norm: 33.386.. Val L1 norm: 2.072.. Train Linf norm: 8203.498.. Val Linf norm: 187.310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:50:52,200]\u001b[0m Trial 120 finished with value: 2.071517966461182 and parameters: {'n_layers': 6, 'n_units_0': 1196, 'n_units_1': 532, 'n_units_2': 3640, 'n_units_3': 679, 'n_units_4': 220, 'n_units_5': 1149, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'Huber', 'optimizer': 'SGD', 'lr': 6.634204169983568e-06, 'batch_size': 256, 'n_epochs': 92, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.22490409108368487, 'dropout_rate': 0.046856825877012426, 'weight_decay': 0.0008633768425131213, 'momentum': 0.3427518159069912, 'factor': 0.13921968353114025, 'patience': 5, 'threshold': 0.0025835802354522647}. Best is trial 92 with value: 1.0098264760335287.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 92/92.. Train loss: 4.176.. Val loss: 3.954.. Train L1 norm: 36.634.. Val L1 norm: 2.072.. Train Linf norm: 9036.961.. Val Linf norm: 187.290\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:50:53,485]\u001b[0m Trial 121 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/137.. Train loss: 5.087.. Val loss: 4.940.. Train L1 norm: 2.674.. Val L1 norm: 1.080.. Train Linf norm: 1702.252.. Val Linf norm: 56.625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:50:54,929]\u001b[0m Trial 122 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/146.. Train loss: 931.149.. Val loss: 54.469.. Train L1 norm: 4.666.. Val L1 norm: 1.102.. Train Linf norm: 3711.964.. Val Linf norm: 71.561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:50:56,505]\u001b[0m Trial 123 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/142.. Train loss: 305.779.. Val loss: 53.767.. Train L1 norm: 5.014.. Val L1 norm: 1.055.. Train Linf norm: 4071.387.. Val Linf norm: 44.770\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:50:58,400]\u001b[0m Trial 124 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/148.. Train loss: 1424.516.. Val loss: 54.172.. Train L1 norm: 3.150.. Val L1 norm: 1.054.. Train Linf norm: 2175.562.. Val Linf norm: 38.944\n",
            "Epoch 1/144.. Train loss: 642.142.. Val loss: 55.121.. Train L1 norm: 2.461.. Val L1 norm: 1.040.. Train Linf norm: 571.774.. Val Linf norm: 36.932\n",
            "Epoch 2/144.. Train loss: 240.275.. Val loss: 53.390.. Train L1 norm: 3.370.. Val L1 norm: 1.063.. Train Linf norm: 2416.994.. Val Linf norm: 48.450\n",
            "Epoch 3/144.. Train loss: 476.767.. Val loss: 55.901.. Train L1 norm: 1.877.. Val L1 norm: 1.058.. Train Linf norm: 879.558.. Val Linf norm: 44.424\n",
            "Epoch 4/144.. Train loss: 971.837.. Val loss: 53.013.. Train L1 norm: 3.165.. Val L1 norm: 1.083.. Train Linf norm: 2200.463.. Val Linf norm: 61.157\n",
            "Epoch 5/144.. Train loss: 470.001.. Val loss: 55.105.. Train L1 norm: 3.249.. Val L1 norm: 1.039.. Train Linf norm: 2293.056.. Val Linf norm: 36.595\n",
            "Epoch 6/144.. Train loss: 85.925.. Val loss: 54.423.. Train L1 norm: 2.486.. Val L1 norm: 1.037.. Train Linf norm: 1516.655.. Val Linf norm: 34.018\n",
            "Epoch 7/144.. Train loss: 59.974.. Val loss: 54.268.. Train L1 norm: 3.222.. Val L1 norm: 1.039.. Train Linf norm: 2270.621.. Val Linf norm: 33.907\n",
            "Epoch 8/144.. Train loss: 118.689.. Val loss: 55.432.. Train L1 norm: 3.277.. Val L1 norm: 1.045.. Train Linf norm: 2328.310.. Val Linf norm: 38.289\n",
            "Epoch 9/144.. Train loss: 465.051.. Val loss: 52.717.. Train L1 norm: 4.936.. Val L1 norm: 1.103.. Train Linf norm: 4005.006.. Val Linf norm: 73.699\n",
            "Epoch 10/144.. Train loss: 576.321.. Val loss: 55.217.. Train L1 norm: 5.575.. Val L1 norm: 1.038.. Train Linf norm: 4656.091.. Val Linf norm: 34.782\n",
            "Epoch 11/144.. Train loss: 840.730.. Val loss: 55.262.. Train L1 norm: 2.350.. Val L1 norm: 1.039.. Train Linf norm: 1375.175.. Val Linf norm: 34.707\n",
            "Epoch 12/144.. Train loss: 477.921.. Val loss: 54.661.. Train L1 norm: 4.679.. Val L1 norm: 1.034.. Train Linf norm: 3756.487.. Val Linf norm: 30.445\n",
            "Epoch 13/144.. Train loss: 91.636.. Val loss: 54.041.. Train L1 norm: 3.732.. Val L1 norm: 1.051.. Train Linf norm: 2787.658.. Val Linf norm: 41.057\n",
            "Epoch 14/144.. Train loss: 67.379.. Val loss: 54.375.. Train L1 norm: 3.449.. Val L1 norm: 1.040.. Train Linf norm: 2498.794.. Val Linf norm: 33.458\n",
            "Epoch 15/144.. Train loss: 59.619.. Val loss: 54.334.. Train L1 norm: 2.887.. Val L1 norm: 1.043.. Train Linf norm: 1925.019.. Val Linf norm: 35.582\n",
            "Epoch 16/144.. Train loss: 63.237.. Val loss: 54.564.. Train L1 norm: 3.082.. Val L1 norm: 1.037.. Train Linf norm: 2124.234.. Val Linf norm: 31.007\n",
            "Epoch 17/144.. Train loss: 70.714.. Val loss: 54.508.. Train L1 norm: 2.953.. Val L1 norm: 1.039.. Train Linf norm: 1992.367.. Val Linf norm: 32.435\n",
            "Epoch 18/144.. Train loss: 60.265.. Val loss: 54.493.. Train L1 norm: 2.724.. Val L1 norm: 1.040.. Train Linf norm: 1759.725.. Val Linf norm: 32.899\n",
            "Epoch 19/144.. Train loss: 121.085.. Val loss: 54.395.. Train L1 norm: 3.032.. Val L1 norm: 1.043.. Train Linf norm: 2074.423.. Val Linf norm: 35.480\n",
            "Epoch 20/144.. Train loss: 89.140.. Val loss: 54.239.. Train L1 norm: 3.308.. Val L1 norm: 1.049.. Train Linf norm: 2355.447.. Val Linf norm: 39.694\n",
            "Epoch 21/144.. Train loss: 69.317.. Val loss: 54.277.. Train L1 norm: 2.731.. Val L1 norm: 1.048.. Train Linf norm: 1762.064.. Val Linf norm: 38.890\n",
            "Epoch 22/144.. Train loss: 65.804.. Val loss: 54.239.. Train L1 norm: 2.793.. Val L1 norm: 1.049.. Train Linf norm: 1826.193.. Val Linf norm: 40.003\n",
            "Epoch 23/144.. Train loss: 59.559.. Val loss: 54.235.. Train L1 norm: 2.977.. Val L1 norm: 1.050.. Train Linf norm: 2016.979.. Val Linf norm: 40.218\n",
            "Epoch 24/144.. Train loss: 61.165.. Val loss: 54.233.. Train L1 norm: 3.790.. Val L1 norm: 1.050.. Train Linf norm: 2848.372.. Val Linf norm: 40.291\n",
            "Epoch 25/144.. Train loss: 62.773.. Val loss: 54.235.. Train L1 norm: 3.419.. Val L1 norm: 1.050.. Train Linf norm: 2467.121.. Val Linf norm: 40.248\n",
            "Epoch 26/144.. Train loss: 59.424.. Val loss: 54.236.. Train L1 norm: 4.091.. Val L1 norm: 1.050.. Train Linf norm: 3155.910.. Val Linf norm: 40.250\n",
            "Epoch 27/144.. Train loss: 83.408.. Val loss: 54.239.. Train L1 norm: 3.552.. Val L1 norm: 1.050.. Train Linf norm: 2603.711.. Val Linf norm: 40.175\n",
            "Epoch 28/144.. Train loss: 145.708.. Val loss: 54.225.. Train L1 norm: 3.220.. Val L1 norm: 1.050.. Train Linf norm: 2263.221.. Val Linf norm: 40.553\n",
            "Epoch 29/144.. Train loss: 63.505.. Val loss: 54.229.. Train L1 norm: 3.550.. Val L1 norm: 1.050.. Train Linf norm: 2601.286.. Val Linf norm: 40.466\n",
            "Epoch 30/144.. Train loss: 59.407.. Val loss: 54.229.. Train L1 norm: 3.761.. Val L1 norm: 1.050.. Train Linf norm: 2816.843.. Val Linf norm: 40.478\n",
            "Epoch 31/144.. Train loss: 59.595.. Val loss: 54.229.. Train L1 norm: 3.415.. Val L1 norm: 1.050.. Train Linf norm: 2464.471.. Val Linf norm: 40.480\n",
            "Epoch 32/144.. Train loss: 75.225.. Val loss: 54.228.. Train L1 norm: 2.694.. Val L1 norm: 1.050.. Train Linf norm: 1723.350.. Val Linf norm: 40.495\n",
            "Epoch 33/144.. Train loss: 59.664.. Val loss: 54.228.. Train L1 norm: 3.402.. Val L1 norm: 1.050.. Train Linf norm: 2447.212.. Val Linf norm: 40.497\n",
            "Epoch 34/144.. Train loss: 64.603.. Val loss: 54.228.. Train L1 norm: 3.800.. Val L1 norm: 1.050.. Train Linf norm: 2858.587.. Val Linf norm: 40.494\n",
            "Epoch 35/144.. Train loss: 61.307.. Val loss: 54.228.. Train L1 norm: 3.346.. Val L1 norm: 1.050.. Train Linf norm: 2392.046.. Val Linf norm: 40.494\n",
            "Epoch 36/144.. Train loss: 76.764.. Val loss: 54.229.. Train L1 norm: 3.900.. Val L1 norm: 1.050.. Train Linf norm: 2960.645.. Val Linf norm: 40.478\n",
            "Epoch 37/144.. Train loss: 59.577.. Val loss: 54.229.. Train L1 norm: 3.447.. Val L1 norm: 1.050.. Train Linf norm: 2496.303.. Val Linf norm: 40.480\n",
            "Epoch 38/144.. Train loss: 84.136.. Val loss: 54.230.. Train L1 norm: 2.825.. Val L1 norm: 1.050.. Train Linf norm: 1859.349.. Val Linf norm: 40.457\n",
            "Epoch 39/144.. Train loss: 81.753.. Val loss: 54.231.. Train L1 norm: 3.710.. Val L1 norm: 1.050.. Train Linf norm: 2764.480.. Val Linf norm: 40.437\n",
            "Epoch 40/144.. Train loss: 63.303.. Val loss: 54.230.. Train L1 norm: 3.525.. Val L1 norm: 1.050.. Train Linf norm: 2576.056.. Val Linf norm: 40.445\n",
            "Epoch 41/144.. Train loss: 95.244.. Val loss: 54.229.. Train L1 norm: 3.834.. Val L1 norm: 1.050.. Train Linf norm: 2892.868.. Val Linf norm: 40.485\n",
            "Epoch 42/144.. Train loss: 75.315.. Val loss: 54.228.. Train L1 norm: 3.564.. Val L1 norm: 1.050.. Train Linf norm: 2616.821.. Val Linf norm: 40.510\n",
            "Epoch 43/144.. Train loss: 60.203.. Val loss: 54.228.. Train L1 norm: 3.836.. Val L1 norm: 1.050.. Train Linf norm: 2894.486.. Val Linf norm: 40.514\n",
            "Epoch 44/144.. Train loss: 80.424.. Val loss: 54.229.. Train L1 norm: 3.352.. Val L1 norm: 1.050.. Train Linf norm: 2394.881.. Val Linf norm: 40.489\n",
            "Epoch 45/144.. Train loss: 73.114.. Val loss: 54.228.. Train L1 norm: 2.775.. Val L1 norm: 1.050.. Train Linf norm: 1808.507.. Val Linf norm: 40.513\n",
            "Epoch 46/144.. Train loss: 59.562.. Val loss: 54.228.. Train L1 norm: 3.413.. Val L1 norm: 1.050.. Train Linf norm: 2461.271.. Val Linf norm: 40.515\n",
            "Epoch 47/144.. Train loss: 62.711.. Val loss: 54.228.. Train L1 norm: 3.892.. Val L1 norm: 1.050.. Train Linf norm: 2952.285.. Val Linf norm: 40.515\n",
            "Epoch 48/144.. Train loss: 80.370.. Val loss: 54.227.. Train L1 norm: 3.303.. Val L1 norm: 1.050.. Train Linf norm: 2350.054.. Val Linf norm: 40.542\n",
            "Epoch 49/144.. Train loss: 84.807.. Val loss: 54.226.. Train L1 norm: 3.242.. Val L1 norm: 1.050.. Train Linf norm: 2285.090.. Val Linf norm: 40.565\n",
            "Epoch 50/144.. Train loss: 62.572.. Val loss: 54.226.. Train L1 norm: 3.010.. Val L1 norm: 1.050.. Train Linf norm: 2047.657.. Val Linf norm: 40.584\n",
            "Epoch 51/144.. Train loss: 61.013.. Val loss: 54.226.. Train L1 norm: 2.656.. Val L1 norm: 1.050.. Train Linf norm: 1687.154.. Val Linf norm: 40.583\n",
            "Epoch 52/144.. Train loss: 79.782.. Val loss: 54.225.. Train L1 norm: 3.662.. Val L1 norm: 1.050.. Train Linf norm: 2717.195.. Val Linf norm: 40.619\n",
            "Epoch 53/144.. Train loss: 67.852.. Val loss: 54.224.. Train L1 norm: 3.509.. Val L1 norm: 1.050.. Train Linf norm: 2559.635.. Val Linf norm: 40.640\n",
            "Epoch 54/144.. Train loss: 61.482.. Val loss: 54.224.. Train L1 norm: 3.206.. Val L1 norm: 1.050.. Train Linf norm: 2248.752.. Val Linf norm: 40.638\n",
            "Epoch 55/144.. Train loss: 61.074.. Val loss: 54.224.. Train L1 norm: 3.299.. Val L1 norm: 1.050.. Train Linf norm: 2342.520.. Val Linf norm: 40.636\n",
            "Epoch 56/144.. Train loss: 94.615.. Val loss: 54.222.. Train L1 norm: 3.506.. Val L1 norm: 1.050.. Train Linf norm: 2555.714.. Val Linf norm: 40.697\n",
            "Epoch 57/144.. Train loss: 66.167.. Val loss: 54.221.. Train L1 norm: 3.974.. Val L1 norm: 1.050.. Train Linf norm: 3035.018.. Val Linf norm: 40.715\n",
            "Epoch 58/144.. Train loss: 71.749.. Val loss: 54.221.. Train L1 norm: 2.414.. Val L1 norm: 1.050.. Train Linf norm: 1438.268.. Val Linf norm: 40.726\n",
            "Epoch 59/144.. Train loss: 63.636.. Val loss: 54.220.. Train L1 norm: 3.990.. Val L1 norm: 1.050.. Train Linf norm: 3053.222.. Val Linf norm: 40.743\n",
            "Epoch 60/144.. Train loss: 76.192.. Val loss: 54.222.. Train L1 norm: 3.726.. Val L1 norm: 1.050.. Train Linf norm: 2777.738.. Val Linf norm: 40.712\n",
            "Epoch 61/144.. Train loss: 63.086.. Val loss: 54.222.. Train L1 norm: 2.856.. Val L1 norm: 1.050.. Train Linf norm: 1890.386.. Val Linf norm: 40.705\n",
            "Epoch 62/144.. Train loss: 59.688.. Val loss: 54.222.. Train L1 norm: 3.149.. Val L1 norm: 1.050.. Train Linf norm: 2190.003.. Val Linf norm: 40.704\n",
            "Epoch 63/144.. Train loss: 78.585.. Val loss: 54.224.. Train L1 norm: 3.101.. Val L1 norm: 1.050.. Train Linf norm: 2140.484.. Val Linf norm: 40.659\n",
            "Epoch 64/144.. Train loss: 63.276.. Val loss: 54.224.. Train L1 norm: 3.296.. Val L1 norm: 1.050.. Train Linf norm: 2339.375.. Val Linf norm: 40.650\n",
            "Epoch 65/144.. Train loss: 67.037.. Val loss: 54.223.. Train L1 norm: 4.302.. Val L1 norm: 1.050.. Train Linf norm: 3371.178.. Val Linf norm: 40.671\n",
            "Epoch 66/144.. Train loss: 59.558.. Val loss: 54.223.. Train L1 norm: 4.317.. Val L1 norm: 1.050.. Train Linf norm: 3388.107.. Val Linf norm: 40.677\n",
            "Epoch 67/144.. Train loss: 59.656.. Val loss: 54.223.. Train L1 norm: 2.753.. Val L1 norm: 1.050.. Train Linf norm: 1784.102.. Val Linf norm: 40.679\n",
            "Epoch 68/144.. Train loss: 62.434.. Val loss: 54.224.. Train L1 norm: 3.366.. Val L1 norm: 1.050.. Train Linf norm: 2413.426.. Val Linf norm: 40.672\n",
            "Epoch 69/144.. Train loss: 59.492.. Val loss: 54.224.. Train L1 norm: 3.716.. Val L1 norm: 1.050.. Train Linf norm: 2770.986.. Val Linf norm: 40.671\n",
            "Epoch 70/144.. Train loss: 61.326.. Val loss: 54.223.. Train L1 norm: 3.587.. Val L1 norm: 1.050.. Train Linf norm: 2638.049.. Val Linf norm: 40.680\n",
            "Epoch 71/144.. Train loss: 87.497.. Val loss: 54.224.. Train L1 norm: 3.403.. Val L1 norm: 1.050.. Train Linf norm: 2450.838.. Val Linf norm: 40.657\n",
            "Epoch 72/144.. Train loss: 60.054.. Val loss: 54.226.. Train L1 norm: 3.632.. Val L1 norm: 1.050.. Train Linf norm: 2686.070.. Val Linf norm: 40.616\n",
            "Epoch 73/144.. Train loss: 92.880.. Val loss: 54.229.. Train L1 norm: 3.388.. Val L1 norm: 1.050.. Train Linf norm: 2434.688.. Val Linf norm: 40.551\n",
            "Epoch 74/144.. Train loss: 66.264.. Val loss: 54.230.. Train L1 norm: 3.536.. Val L1 norm: 1.050.. Train Linf norm: 2589.043.. Val Linf norm: 40.506\n",
            "Epoch 75/144.. Train loss: 68.535.. Val loss: 54.230.. Train L1 norm: 2.678.. Val L1 norm: 1.050.. Train Linf norm: 1707.096.. Val Linf norm: 40.516\n",
            "Epoch 76/144.. Train loss: 62.703.. Val loss: 54.229.. Train L1 norm: 3.362.. Val L1 norm: 1.050.. Train Linf norm: 2410.027.. Val Linf norm: 40.541\n",
            "Epoch 77/144.. Train loss: 72.608.. Val loss: 54.231.. Train L1 norm: 3.439.. Val L1 norm: 1.050.. Train Linf norm: 2487.551.. Val Linf norm: 40.480\n",
            "Epoch 78/144.. Train loss: 60.128.. Val loss: 54.232.. Train L1 norm: 3.074.. Val L1 norm: 1.050.. Train Linf norm: 2114.251.. Val Linf norm: 40.478\n",
            "Epoch 79/144.. Train loss: 93.613.. Val loss: 54.228.. Train L1 norm: 3.427.. Val L1 norm: 1.050.. Train Linf norm: 2475.201.. Val Linf norm: 40.567\n",
            "Epoch 80/144.. Train loss: 79.362.. Val loss: 54.226.. Train L1 norm: 3.166.. Val L1 norm: 1.050.. Train Linf norm: 2207.125.. Val Linf norm: 40.640\n",
            "Epoch 81/144.. Train loss: 70.794.. Val loss: 54.227.. Train L1 norm: 3.671.. Val L1 norm: 1.050.. Train Linf norm: 2725.417.. Val Linf norm: 40.602\n",
            "Epoch 82/144.. Train loss: 152.561.. Val loss: 54.228.. Train L1 norm: 3.356.. Val L1 norm: 1.050.. Train Linf norm: 2401.807.. Val Linf norm: 40.586\n",
            "Epoch 83/144.. Train loss: 61.939.. Val loss: 54.236.. Train L1 norm: 3.669.. Val L1 norm: 1.050.. Train Linf norm: 2723.714.. Val Linf norm: 40.384\n",
            "Epoch 84/144.. Train loss: 63.945.. Val loss: 54.236.. Train L1 norm: 3.272.. Val L1 norm: 1.050.. Train Linf norm: 2317.213.. Val Linf norm: 40.376\n",
            "Epoch 85/144.. Train loss: 88.476.. Val loss: 54.234.. Train L1 norm: 3.612.. Val L1 norm: 1.050.. Train Linf norm: 2664.293.. Val Linf norm: 40.419\n",
            "Epoch 86/144.. Train loss: 61.202.. Val loss: 54.234.. Train L1 norm: 4.124.. Val L1 norm: 1.050.. Train Linf norm: 3187.988.. Val Linf norm: 40.427\n",
            "Epoch 87/144.. Train loss: 59.519.. Val loss: 54.234.. Train L1 norm: 3.494.. Val L1 norm: 1.050.. Train Linf norm: 2542.948.. Val Linf norm: 40.429\n",
            "Epoch 88/144.. Train loss: 66.502.. Val loss: 54.235.. Train L1 norm: 3.508.. Val L1 norm: 1.050.. Train Linf norm: 2560.085.. Val Linf norm: 40.414\n",
            "Epoch 89/144.. Train loss: 61.844.. Val loss: 54.235.. Train L1 norm: 3.268.. Val L1 norm: 1.050.. Train Linf norm: 2311.954.. Val Linf norm: 40.410\n",
            "Epoch 90/144.. Train loss: 61.044.. Val loss: 54.235.. Train L1 norm: 2.712.. Val L1 norm: 1.050.. Train Linf norm: 1742.469.. Val Linf norm: 40.417\n",
            "Epoch 91/144.. Train loss: 172.388.. Val loss: 54.234.. Train L1 norm: 3.566.. Val L1 norm: 1.050.. Train Linf norm: 2617.400.. Val Linf norm: 40.451\n",
            "Epoch 92/144.. Train loss: 79.415.. Val loss: 54.232.. Train L1 norm: 3.730.. Val L1 norm: 1.050.. Train Linf norm: 2785.791.. Val Linf norm: 40.493\n",
            "Epoch 93/144.. Train loss: 72.301.. Val loss: 54.231.. Train L1 norm: 3.267.. Val L1 norm: 1.050.. Train Linf norm: 2310.664.. Val Linf norm: 40.515\n",
            "Epoch 94/144.. Train loss: 71.087.. Val loss: 54.232.. Train L1 norm: 2.965.. Val L1 norm: 1.050.. Train Linf norm: 2003.131.. Val Linf norm: 40.492\n",
            "Epoch 95/144.. Train loss: 62.322.. Val loss: 54.232.. Train L1 norm: 3.834.. Val L1 norm: 1.050.. Train Linf norm: 2892.807.. Val Linf norm: 40.486\n",
            "Epoch 96/144.. Train loss: 83.712.. Val loss: 54.231.. Train L1 norm: 3.425.. Val L1 norm: 1.050.. Train Linf norm: 2473.894.. Val Linf norm: 40.536\n",
            "Epoch 97/144.. Train loss: 59.498.. Val loss: 54.231.. Train L1 norm: 3.799.. Val L1 norm: 1.050.. Train Linf norm: 2855.411.. Val Linf norm: 40.539\n",
            "Epoch 98/144.. Train loss: 59.940.. Val loss: 54.231.. Train L1 norm: 2.818.. Val L1 norm: 1.050.. Train Linf norm: 1851.781.. Val Linf norm: 40.539\n",
            "Epoch 99/144.. Train loss: 74.475.. Val loss: 54.232.. Train L1 norm: 3.446.. Val L1 norm: 1.050.. Train Linf norm: 2496.287.. Val Linf norm: 40.507\n",
            "Epoch 100/144.. Train loss: 61.421.. Val loss: 54.232.. Train L1 norm: 2.579.. Val L1 norm: 1.050.. Train Linf norm: 1607.378.. Val Linf norm: 40.508\n",
            "Epoch 101/144.. Train loss: 77.660.. Val loss: 54.230.. Train L1 norm: 3.428.. Val L1 norm: 1.050.. Train Linf norm: 2475.952.. Val Linf norm: 40.569\n",
            "Epoch 102/144.. Train loss: 60.370.. Val loss: 54.230.. Train L1 norm: 3.755.. Val L1 norm: 1.050.. Train Linf norm: 2810.707.. Val Linf norm: 40.569\n",
            "Epoch 103/144.. Train loss: 65.957.. Val loss: 54.230.. Train L1 norm: 3.935.. Val L1 norm: 1.050.. Train Linf norm: 2994.836.. Val Linf norm: 40.554\n",
            "Epoch 104/144.. Train loss: 62.766.. Val loss: 54.230.. Train L1 norm: 3.268.. Val L1 norm: 1.050.. Train Linf norm: 2309.441.. Val Linf norm: 40.560\n",
            "Epoch 105/144.. Train loss: 59.677.. Val loss: 54.230.. Train L1 norm: 3.420.. Val L1 norm: 1.050.. Train Linf norm: 2469.663.. Val Linf norm: 40.569\n",
            "Epoch 106/144.. Train loss: 67.593.. Val loss: 54.229.. Train L1 norm: 4.054.. Val L1 norm: 1.050.. Train Linf norm: 3116.201.. Val Linf norm: 40.592\n",
            "Epoch 107/144.. Train loss: 59.726.. Val loss: 54.229.. Train L1 norm: 3.698.. Val L1 norm: 1.050.. Train Linf norm: 2754.325.. Val Linf norm: 40.603\n",
            "Epoch 108/144.. Train loss: 81.464.. Val loss: 54.226.. Train L1 norm: 3.838.. Val L1 norm: 1.050.. Train Linf norm: 2895.407.. Val Linf norm: 40.663\n",
            "Epoch 109/144.. Train loss: 76.863.. Val loss: 54.225.. Train L1 norm: 3.738.. Val L1 norm: 1.050.. Train Linf norm: 2779.671.. Val Linf norm: 40.714\n",
            "Epoch 110/144.. Train loss: 59.937.. Val loss: 54.224.. Train L1 norm: 2.859.. Val L1 norm: 1.050.. Train Linf norm: 1891.840.. Val Linf norm: 40.719\n",
            "Epoch 111/144.. Train loss: 59.590.. Val loss: 54.224.. Train L1 norm: 3.539.. Val L1 norm: 1.050.. Train Linf norm: 2588.743.. Val Linf norm: 40.722\n",
            "Epoch 112/144.. Train loss: 103.718.. Val loss: 54.221.. Train L1 norm: 3.434.. Val L1 norm: 1.050.. Train Linf norm: 2483.743.. Val Linf norm: 40.817\n",
            "Epoch 113/144.. Train loss: 118.836.. Val loss: 54.220.. Train L1 norm: 3.493.. Val L1 norm: 1.050.. Train Linf norm: 2542.463.. Val Linf norm: 40.837\n",
            "Epoch 114/144.. Train loss: 59.469.. Val loss: 54.217.. Train L1 norm: 3.456.. Val L1 norm: 1.051.. Train Linf norm: 2499.422.. Val Linf norm: 40.926\n",
            "Epoch 115/144.. Train loss: 66.291.. Val loss: 54.218.. Train L1 norm: 2.931.. Val L1 norm: 1.051.. Train Linf norm: 1964.275.. Val Linf norm: 40.905\n",
            "Epoch 116/144.. Train loss: 82.673.. Val loss: 54.220.. Train L1 norm: 3.735.. Val L1 norm: 1.050.. Train Linf norm: 2790.179.. Val Linf norm: 40.853\n",
            "Epoch 117/144.. Train loss: 170.047.. Val loss: 54.215.. Train L1 norm: 3.918.. Val L1 norm: 1.051.. Train Linf norm: 2976.508.. Val Linf norm: 40.970\n",
            "Epoch 118/144.. Train loss: 70.457.. Val loss: 54.215.. Train L1 norm: 3.404.. Val L1 norm: 1.051.. Train Linf norm: 2451.159.. Val Linf norm: 40.970\n",
            "Epoch 119/144.. Train loss: 59.495.. Val loss: 54.215.. Train L1 norm: 3.830.. Val L1 norm: 1.051.. Train Linf norm: 2887.249.. Val Linf norm: 40.972\n",
            "Epoch 120/144.. Train loss: 80.260.. Val loss: 54.213.. Train L1 norm: 3.036.. Val L1 norm: 1.051.. Train Linf norm: 2073.972.. Val Linf norm: 41.027\n",
            "Epoch 121/144.. Train loss: 77.501.. Val loss: 54.213.. Train L1 norm: 3.204.. Val L1 norm: 1.051.. Train Linf norm: 2244.783.. Val Linf norm: 41.044\n",
            "Epoch 122/144.. Train loss: 101.922.. Val loss: 54.208.. Train L1 norm: 4.049.. Val L1 norm: 1.051.. Train Linf norm: 3113.090.. Val Linf norm: 41.153\n",
            "Epoch 123/144.. Train loss: 67.260.. Val loss: 54.209.. Train L1 norm: 3.427.. Val L1 norm: 1.051.. Train Linf norm: 2476.460.. Val Linf norm: 41.149\n",
            "Epoch 124/144.. Train loss: 59.926.. Val loss: 54.209.. Train L1 norm: 3.663.. Val L1 norm: 1.051.. Train Linf norm: 2712.869.. Val Linf norm: 41.150\n",
            "Epoch 125/144.. Train loss: 61.410.. Val loss: 54.209.. Train L1 norm: 3.645.. Val L1 norm: 1.051.. Train Linf norm: 2697.840.. Val Linf norm: 41.148\n",
            "Epoch 126/144.. Train loss: 61.879.. Val loss: 54.209.. Train L1 norm: 2.769.. Val L1 norm: 1.051.. Train Linf norm: 1802.342.. Val Linf norm: 41.141\n",
            "Epoch 127/144.. Train loss: 62.705.. Val loss: 54.210.. Train L1 norm: 3.388.. Val L1 norm: 1.051.. Train Linf norm: 2435.619.. Val Linf norm: 41.124\n",
            "Epoch 128/144.. Train loss: 59.630.. Val loss: 54.210.. Train L1 norm: 3.277.. Val L1 norm: 1.051.. Train Linf norm: 2323.152.. Val Linf norm: 41.126\n",
            "Epoch 129/144.. Train loss: 59.681.. Val loss: 54.210.. Train L1 norm: 2.893.. Val L1 norm: 1.051.. Train Linf norm: 1929.166.. Val Linf norm: 41.128\n",
            "Epoch 130/144.. Train loss: 61.630.. Val loss: 54.209.. Train L1 norm: 3.218.. Val L1 norm: 1.051.. Train Linf norm: 2261.751.. Val Linf norm: 41.140\n",
            "Epoch 131/144.. Train loss: 78.593.. Val loss: 54.212.. Train L1 norm: 2.820.. Val L1 norm: 1.051.. Train Linf norm: 1853.643.. Val Linf norm: 41.086\n",
            "Epoch 132/144.. Train loss: 61.616.. Val loss: 54.211.. Train L1 norm: 3.898.. Val L1 norm: 1.051.. Train Linf norm: 2957.093.. Val Linf norm: 41.095\n",
            "Epoch 133/144.. Train loss: 59.830.. Val loss: 54.211.. Train L1 norm: 3.733.. Val L1 norm: 1.051.. Train Linf norm: 2789.228.. Val Linf norm: 41.099\n",
            "Epoch 134/144.. Train loss: 85.569.. Val loss: 54.214.. Train L1 norm: 3.885.. Val L1 norm: 1.051.. Train Linf norm: 2944.888.. Val Linf norm: 41.029\n",
            "Epoch 135/144.. Train loss: 73.617.. Val loss: 54.216.. Train L1 norm: 3.954.. Val L1 norm: 1.051.. Train Linf norm: 3014.026.. Val Linf norm: 40.982\n",
            "Epoch 136/144.. Train loss: 83.394.. Val loss: 54.214.. Train L1 norm: 3.448.. Val L1 norm: 1.051.. Train Linf norm: 2498.519.. Val Linf norm: 41.031\n",
            "Epoch 137/144.. Train loss: 63.111.. Val loss: 54.214.. Train L1 norm: 3.456.. Val L1 norm: 1.051.. Train Linf norm: 2504.161.. Val Linf norm: 41.041\n",
            "Epoch 138/144.. Train loss: 61.525.. Val loss: 54.214.. Train L1 norm: 3.282.. Val L1 norm: 1.051.. Train Linf norm: 2319.707.. Val Linf norm: 41.032\n",
            "Epoch 139/144.. Train loss: 129.052.. Val loss: 54.219.. Train L1 norm: 3.188.. Val L1 norm: 1.051.. Train Linf norm: 2230.913.. Val Linf norm: 40.903\n",
            "Epoch 140/144.. Train loss: 59.985.. Val loss: 54.219.. Train L1 norm: 4.021.. Val L1 norm: 1.051.. Train Linf norm: 3083.534.. Val Linf norm: 40.905\n",
            "Epoch 141/144.. Train loss: 82.203.. Val loss: 54.222.. Train L1 norm: 3.529.. Val L1 norm: 1.050.. Train Linf norm: 2581.473.. Val Linf norm: 40.843\n",
            "Epoch 142/144.. Train loss: 60.891.. Val loss: 54.221.. Train L1 norm: 2.804.. Val L1 norm: 1.050.. Train Linf norm: 1837.244.. Val Linf norm: 40.853\n",
            "Epoch 143/144.. Train loss: 61.989.. Val loss: 54.222.. Train L1 norm: 3.322.. Val L1 norm: 1.050.. Train Linf norm: 2368.387.. Val Linf norm: 40.844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:56:58,585]\u001b[0m Trial 125 finished with value: 1.0505260329882304 and parameters: {'n_layers': 6, 'n_units_0': 3676, 'n_units_1': 686, 'n_units_2': 3932, 'n_units_3': 393, 'n_units_4': 2588, 'n_units_5': 1373, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 1.4797943211424077e-06, 'batch_size': 1024, 'n_epochs': 144, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.21553370989497334, 'dropout_rate': 0.013303073741505442, 'weight_decay': 0.0005432261453622943, 'beta1': 0.9296366517429326, 'beta2': 0.9991679080384512, 'factor': 0.10889378766639038, 'patience': 6, 'threshold': 0.0025251920595964375}. Best is trial 92 with value: 1.0098264760335287.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 144/144.. Train loss: 70.410.. Val loss: 54.220.. Train L1 norm: 3.128.. Val L1 norm: 1.051.. Train Linf norm: 2170.880.. Val Linf norm: 40.890\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:57:02,410]\u001b[0m Trial 126 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/144.. Train loss: 31354.250.. Val loss: 51.475.. Train L1 norm: 21.949.. Val L1 norm: 1.247.. Train Linf norm: 21181.745.. Val Linf norm: 159.510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:57:05,447]\u001b[0m Trial 127 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/132.. Train loss: 2857.321.. Val loss: 45.521.. Train L1 norm: 22.439.. Val L1 norm: 2.798.. Train Linf norm: 10666.732.. Val Linf norm: 576.970\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:57:07,496]\u001b[0m Trial 128 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/140.. Train loss: 2.541.. Val loss: 2.405.. Train L1 norm: 2.974.. Val L1 norm: 1.268.. Train Linf norm: 1948.931.. Val Linf norm: 167.586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:57:10,506]\u001b[0m Trial 129 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/143.. Train loss: 2343.574.. Val loss: 52.554.. Train L1 norm: 7.123.. Val L1 norm: 1.118.. Train Linf norm: 6203.646.. Val Linf norm: 83.110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:57:20,168]\u001b[0m Trial 130 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/125.. Train loss: 4017.906.. Val loss: 54.284.. Train L1 norm: 6.582.. Val L1 norm: 1.078.. Train Linf norm: 358.006.. Val Linf norm: 6.485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:57:22,004]\u001b[0m Trial 131 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/146.. Train loss: 146.914.. Val loss: 48.244.. Train L1 norm: 13.447.. Val L1 norm: 1.520.. Train Linf norm: 12670.141.. Val Linf norm: 284.999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 09:57:24,109]\u001b[0m Trial 132 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/148.. Train loss: 3937.597.. Val loss: 51.680.. Train L1 norm: 7.400.. Val L1 norm: 1.173.. Train Linf norm: 6494.985.. Val Linf norm: 111.836\n",
            "Epoch 1/145.. Train loss: 1122.145.. Val loss: 54.378.. Train L1 norm: 5.155.. Val L1 norm: 1.042.. Train Linf norm: 4221.256.. Val Linf norm: 35.307\n",
            "Epoch 2/145.. Train loss: 271.986.. Val loss: 53.456.. Train L1 norm: 3.571.. Val L1 norm: 1.082.. Train Linf norm: 2620.356.. Val Linf norm: 61.363\n",
            "Epoch 3/145.. Train loss: 335.032.. Val loss: 54.763.. Train L1 norm: 2.963.. Val L1 norm: 1.032.. Train Linf norm: 2000.993.. Val Linf norm: 27.434\n",
            "Epoch 4/145.. Train loss: 163.801.. Val loss: 53.536.. Train L1 norm: 1.203.. Val L1 norm: 1.081.. Train Linf norm: 196.059.. Val Linf norm: 61.098\n",
            "Epoch 5/145.. Train loss: 622.964.. Val loss: 55.490.. Train L1 norm: 3.550.. Val L1 norm: 1.023.. Train Linf norm: 2598.000.. Val Linf norm: 19.617\n",
            "Epoch 6/145.. Train loss: 266.819.. Val loss: 55.158.. Train L1 norm: 1.705.. Val L1 norm: 1.027.. Train Linf norm: 712.742.. Val Linf norm: 22.923\n",
            "Epoch 7/145.. Train loss: 101.222.. Val loss: 54.752.. Train L1 norm: 2.153.. Val L1 norm: 1.039.. Train Linf norm: 1166.401.. Val Linf norm: 33.544\n",
            "Epoch 8/145.. Train loss: 214.571.. Val loss: 53.322.. Train L1 norm: 3.652.. Val L1 norm: 1.101.. Train Linf norm: 2698.981.. Val Linf norm: 74.063\n",
            "Epoch 9/145.. Train loss: 179.177.. Val loss: 54.456.. Train L1 norm: 3.150.. Val L1 norm: 1.051.. Train Linf norm: 2183.055.. Val Linf norm: 42.965\n",
            "Epoch 10/145.. Train loss: 67.351.. Val loss: 54.802.. Train L1 norm: 2.130.. Val L1 norm: 1.041.. Train Linf norm: 1148.263.. Val Linf norm: 35.289\n",
            "Epoch 11/145.. Train loss: 285.313.. Val loss: 53.107.. Train L1 norm: 4.149.. Val L1 norm: 1.115.. Train Linf norm: 3198.434.. Val Linf norm: 83.015\n",
            "Epoch 12/145.. Train loss: 341.621.. Val loss: 54.815.. Train L1 norm: 4.467.. Val L1 norm: 1.045.. Train Linf norm: 3517.934.. Val Linf norm: 37.900\n",
            "Epoch 13/145.. Train loss: 199.885.. Val loss: 53.797.. Train L1 norm: 3.725.. Val L1 norm: 1.084.. Train Linf norm: 2778.915.. Val Linf norm: 64.453\n",
            "Epoch 14/145.. Train loss: 124.690.. Val loss: 54.549.. Train L1 norm: 3.613.. Val L1 norm: 1.057.. Train Linf norm: 2654.173.. Val Linf norm: 47.333\n",
            "Epoch 15/145.. Train loss: 112.386.. Val loss: 53.950.. Train L1 norm: 3.583.. Val L1 norm: 1.082.. Train Linf norm: 2634.886.. Val Linf norm: 63.709\n",
            "Epoch 16/145.. Train loss: 98.115.. Val loss: 54.509.. Train L1 norm: 1.934.. Val L1 norm: 1.062.. Train Linf norm: 941.197.. Val Linf norm: 51.039\n",
            "Epoch 17/145.. Train loss: 241.801.. Val loss: 52.870.. Train L1 norm: 3.257.. Val L1 norm: 1.136.. Train Linf norm: 2285.221.. Val Linf norm: 97.060\n",
            "Epoch 18/145.. Train loss: 689.890.. Val loss: 54.578.. Train L1 norm: 3.824.. Val L1 norm: 1.061.. Train Linf norm: 2849.055.. Val Linf norm: 51.070\n",
            "Epoch 19/145.. Train loss: 1109.993.. Val loss: 54.017.. Train L1 norm: 1.883.. Val L1 norm: 1.084.. Train Linf norm: 887.001.. Val Linf norm: 65.612\n",
            "Epoch 20/145.. Train loss: 514.857.. Val loss: 54.445.. Train L1 norm: 4.840.. Val L1 norm: 1.067.. Train Linf norm: 3901.935.. Val Linf norm: 55.129\n",
            "Epoch 21/145.. Train loss: 186.372.. Val loss: 54.110.. Train L1 norm: 3.065.. Val L1 norm: 1.082.. Train Linf norm: 2092.574.. Val Linf norm: 65.075\n",
            "Epoch 22/145.. Train loss: 378.982.. Val loss: 54.826.. Train L1 norm: 4.356.. Val L1 norm: 1.056.. Train Linf norm: 3411.020.. Val Linf norm: 46.950\n",
            "Epoch 23/145.. Train loss: 204.734.. Val loss: 53.859.. Train L1 norm: 3.396.. Val L1 norm: 1.094.. Train Linf norm: 2438.098.. Val Linf norm: 72.391\n",
            "Epoch 24/145.. Train loss: 138.456.. Val loss: 54.618.. Train L1 norm: 3.996.. Val L1 norm: 1.065.. Train Linf norm: 3047.776.. Val Linf norm: 53.835\n",
            "Epoch 25/145.. Train loss: 101.460.. Val loss: 54.530.. Train L1 norm: 2.490.. Val L1 norm: 1.069.. Train Linf norm: 1512.381.. Val Linf norm: 56.146\n",
            "Epoch 26/145.. Train loss: 184.138.. Val loss: 54.327.. Train L1 norm: 2.915.. Val L1 norm: 1.076.. Train Linf norm: 1945.576.. Val Linf norm: 61.327\n",
            "Epoch 27/145.. Train loss: 205.968.. Val loss: 54.268.. Train L1 norm: 3.160.. Val L1 norm: 1.079.. Train Linf norm: 2187.423.. Val Linf norm: 62.910\n",
            "Epoch 28/145.. Train loss: 59.798.. Val loss: 54.115.. Train L1 norm: 2.613.. Val L1 norm: 1.085.. Train Linf norm: 1633.017.. Val Linf norm: 66.874\n",
            "Epoch 29/145.. Train loss: 64.374.. Val loss: 54.084.. Train L1 norm: 2.071.. Val L1 norm: 1.086.. Train Linf norm: 1075.453.. Val Linf norm: 67.800\n",
            "Epoch 30/145.. Train loss: 305.257.. Val loss: 54.298.. Train L1 norm: 3.080.. Val L1 norm: 1.078.. Train Linf norm: 2107.896.. Val Linf norm: 62.608\n",
            "Epoch 31/145.. Train loss: 123.392.. Val loss: 54.153.. Train L1 norm: 3.475.. Val L1 norm: 1.084.. Train Linf norm: 934.476.. Val Linf norm: 66.510\n",
            "Epoch 32/145.. Train loss: 75.769.. Val loss: 54.161.. Train L1 norm: 2.920.. Val L1 norm: 1.084.. Train Linf norm: 1944.878.. Val Linf norm: 66.345\n",
            "Epoch 33/145.. Train loss: 59.752.. Val loss: 54.165.. Train L1 norm: 2.620.. Val L1 norm: 1.084.. Train Linf norm: 1630.878.. Val Linf norm: 66.279\n",
            "Epoch 34/145.. Train loss: 70.299.. Val loss: 54.170.. Train L1 norm: 3.649.. Val L1 norm: 1.084.. Train Linf norm: 2692.093.. Val Linf norm: 66.175\n",
            "Epoch 35/145.. Train loss: 91.583.. Val loss: 54.192.. Train L1 norm: 3.698.. Val L1 norm: 1.083.. Train Linf norm: 2742.354.. Val Linf norm: 65.642\n",
            "Epoch 36/145.. Train loss: 86.769.. Val loss: 54.210.. Train L1 norm: 3.138.. Val L1 norm: 1.082.. Train Linf norm: 2170.575.. Val Linf norm: 65.192\n",
            "Epoch 37/145.. Train loss: 64.312.. Val loss: 54.216.. Train L1 norm: 2.995.. Val L1 norm: 1.082.. Train Linf norm: 2023.623.. Val Linf norm: 65.050\n",
            "Epoch 38/145.. Train loss: 109.168.. Val loss: 54.192.. Train L1 norm: 3.401.. Val L1 norm: 1.083.. Train Linf norm: 2440.310.. Val Linf norm: 65.697\n",
            "Epoch 39/145.. Train loss: 74.425.. Val loss: 54.190.. Train L1 norm: 4.153.. Val L1 norm: 1.083.. Train Linf norm: 3206.978.. Val Linf norm: 65.754\n",
            "Epoch 40/145.. Train loss: 101.876.. Val loss: 54.186.. Train L1 norm: 3.352.. Val L1 norm: 1.083.. Train Linf norm: 2383.424.. Val Linf norm: 65.853\n",
            "Epoch 41/145.. Train loss: 64.259.. Val loss: 54.185.. Train L1 norm: 3.113.. Val L1 norm: 1.083.. Train Linf norm: 2143.038.. Val Linf norm: 65.884\n",
            "Epoch 42/145.. Train loss: 60.204.. Val loss: 54.185.. Train L1 norm: 3.350.. Val L1 norm: 1.083.. Train Linf norm: 2386.291.. Val Linf norm: 65.891\n",
            "Epoch 43/145.. Train loss: 59.745.. Val loss: 54.184.. Train L1 norm: 3.190.. Val L1 norm: 1.083.. Train Linf norm: 2222.232.. Val Linf norm: 65.898\n",
            "Epoch 44/145.. Train loss: 195.934.. Val loss: 54.189.. Train L1 norm: 4.461.. Val L1 norm: 1.083.. Train Linf norm: 3525.064.. Val Linf norm: 65.792\n",
            "Epoch 45/145.. Train loss: 60.594.. Val loss: 54.192.. Train L1 norm: 4.081.. Val L1 norm: 1.083.. Train Linf norm: 3135.233.. Val Linf norm: 65.728\n",
            "Epoch 46/145.. Train loss: 73.957.. Val loss: 54.189.. Train L1 norm: 3.600.. Val L1 norm: 1.083.. Train Linf norm: 2644.390.. Val Linf norm: 65.792\n",
            "Epoch 47/145.. Train loss: 94.957.. Val loss: 54.187.. Train L1 norm: 2.758.. Val L1 norm: 1.083.. Train Linf norm: 1778.800.. Val Linf norm: 65.848\n",
            "Epoch 48/145.. Train loss: 90.594.. Val loss: 54.182.. Train L1 norm: 2.773.. Val L1 norm: 1.083.. Train Linf norm: 1795.326.. Val Linf norm: 65.993\n",
            "Epoch 49/145.. Train loss: 64.173.. Val loss: 54.182.. Train L1 norm: 3.699.. Val L1 norm: 1.083.. Train Linf norm: 2744.457.. Val Linf norm: 65.986\n",
            "Epoch 50/145.. Train loss: 116.268.. Val loss: 54.182.. Train L1 norm: 4.091.. Val L1 norm: 1.083.. Train Linf norm: 3146.074.. Val Linf norm: 66.007\n",
            "Epoch 51/145.. Train loss: 73.904.. Val loss: 54.176.. Train L1 norm: 2.657.. Val L1 norm: 1.084.. Train Linf norm: 1677.971.. Val Linf norm: 66.146\n",
            "Epoch 52/145.. Train loss: 108.891.. Val loss: 54.170.. Train L1 norm: 3.242.. Val L1 norm: 1.084.. Train Linf norm: 2271.972.. Val Linf norm: 66.302\n",
            "Epoch 53/145.. Train loss: 114.785.. Val loss: 54.173.. Train L1 norm: 3.011.. Val L1 norm: 1.084.. Train Linf norm: 2032.065.. Val Linf norm: 66.227\n",
            "Epoch 54/145.. Train loss: 62.238.. Val loss: 54.172.. Train L1 norm: 3.480.. Val L1 norm: 1.084.. Train Linf norm: 2518.251.. Val Linf norm: 66.255\n",
            "Epoch 55/145.. Train loss: 161.105.. Val loss: 54.177.. Train L1 norm: 3.226.. Val L1 norm: 1.084.. Train Linf norm: 2260.180.. Val Linf norm: 66.133\n",
            "Epoch 56/145.. Train loss: 64.939.. Val loss: 54.181.. Train L1 norm: 2.840.. Val L1 norm: 1.083.. Train Linf norm: 1865.215.. Val Linf norm: 66.045\n",
            "Epoch 57/145.. Train loss: 101.985.. Val loss: 54.185.. Train L1 norm: 4.237.. Val L1 norm: 1.083.. Train Linf norm: 3296.038.. Val Linf norm: 65.941\n",
            "Epoch 58/145.. Train loss: 66.498.. Val loss: 54.184.. Train L1 norm: 3.378.. Val L1 norm: 1.083.. Train Linf norm: 2415.520.. Val Linf norm: 65.990\n",
            "Epoch 59/145.. Train loss: 60.209.. Val loss: 54.183.. Train L1 norm: 3.267.. Val L1 norm: 1.083.. Train Linf norm: 2300.763.. Val Linf norm: 66.003\n",
            "Epoch 60/145.. Train loss: 66.129.. Val loss: 54.185.. Train L1 norm: 3.790.. Val L1 norm: 1.083.. Train Linf norm: 2835.674.. Val Linf norm: 65.964\n",
            "Epoch 61/145.. Train loss: 63.607.. Val loss: 54.186.. Train L1 norm: 4.188.. Val L1 norm: 1.083.. Train Linf norm: 3245.225.. Val Linf norm: 65.946\n",
            "Epoch 62/145.. Train loss: 59.715.. Val loss: 54.186.. Train L1 norm: 3.543.. Val L1 norm: 1.083.. Train Linf norm: 2584.899.. Val Linf norm: 65.942\n",
            "Epoch 63/145.. Train loss: 106.568.. Val loss: 54.181.. Train L1 norm: 3.725.. Val L1 norm: 1.083.. Train Linf norm: 2770.083.. Val Linf norm: 66.077\n",
            "Epoch 64/145.. Train loss: 229.067.. Val loss: 54.186.. Train L1 norm: 4.589.. Val L1 norm: 1.083.. Train Linf norm: 3654.202.. Val Linf norm: 65.945\n",
            "Epoch 65/145.. Train loss: 87.419.. Val loss: 54.187.. Train L1 norm: 3.029.. Val L1 norm: 1.083.. Train Linf norm: 2059.636.. Val Linf norm: 65.942\n",
            "Epoch 66/145.. Train loss: 77.970.. Val loss: 54.183.. Train L1 norm: 3.787.. Val L1 norm: 1.083.. Train Linf norm: 2832.606.. Val Linf norm: 66.044\n",
            "Epoch 67/145.. Train loss: 64.639.. Val loss: 54.183.. Train L1 norm: 2.822.. Val L1 norm: 1.083.. Train Linf norm: 1846.341.. Val Linf norm: 66.029\n",
            "Epoch 68/145.. Train loss: 149.852.. Val loss: 54.189.. Train L1 norm: 3.477.. Val L1 norm: 1.083.. Train Linf norm: 2515.265.. Val Linf norm: 65.889\n",
            "Epoch 69/145.. Train loss: 76.898.. Val loss: 54.194.. Train L1 norm: 2.069.. Val L1 norm: 1.083.. Train Linf norm: 1060.183.. Val Linf norm: 65.792\n",
            "Epoch 70/145.. Train loss: 73.688.. Val loss: 54.192.. Train L1 norm: 2.508.. Val L1 norm: 1.083.. Train Linf norm: 1524.972.. Val Linf norm: 65.849\n",
            "Epoch 71/145.. Train loss: 60.223.. Val loss: 54.192.. Train L1 norm: 3.458.. Val L1 norm: 1.083.. Train Linf norm: 2498.671.. Val Linf norm: 65.854\n",
            "Epoch 72/145.. Train loss: 132.159.. Val loss: 54.189.. Train L1 norm: 4.084.. Val L1 norm: 1.083.. Train Linf norm: 3139.574.. Val Linf norm: 65.917\n",
            "Epoch 73/145.. Train loss: 151.734.. Val loss: 54.174.. Train L1 norm: 3.605.. Val L1 norm: 1.084.. Train Linf norm: 2648.303.. Val Linf norm: 66.309\n",
            "Epoch 74/145.. Train loss: 59.642.. Val loss: 54.174.. Train L1 norm: 4.348.. Val L1 norm: 1.084.. Train Linf norm: 3409.048.. Val Linf norm: 66.317\n",
            "Epoch 75/145.. Train loss: 70.280.. Val loss: 54.171.. Train L1 norm: 3.568.. Val L1 norm: 1.084.. Train Linf norm: 2608.859.. Val Linf norm: 66.384\n",
            "Epoch 76/145.. Train loss: 96.693.. Val loss: 54.165.. Train L1 norm: 3.179.. Val L1 norm: 1.084.. Train Linf norm: 2209.167.. Val Linf norm: 66.552\n",
            "Epoch 77/145.. Train loss: 69.090.. Val loss: 54.167.. Train L1 norm: 3.947.. Val L1 norm: 1.084.. Train Linf norm: 2997.012.. Val Linf norm: 66.497\n",
            "Epoch 78/145.. Train loss: 61.373.. Val loss: 54.166.. Train L1 norm: 3.586.. Val L1 norm: 1.084.. Train Linf norm: 2627.044.. Val Linf norm: 66.526\n",
            "Epoch 79/145.. Train loss: 59.771.. Val loss: 54.166.. Train L1 norm: 3.424.. Val L1 norm: 1.084.. Train Linf norm: 2463.472.. Val Linf norm: 66.532\n",
            "Epoch 80/145.. Train loss: 145.829.. Val loss: 54.175.. Train L1 norm: 3.988.. Val L1 norm: 1.084.. Train Linf norm: 3038.329.. Val Linf norm: 66.301\n",
            "Epoch 81/145.. Train loss: 66.126.. Val loss: 54.177.. Train L1 norm: 3.764.. Val L1 norm: 1.084.. Train Linf norm: 2812.331.. Val Linf norm: 66.257\n",
            "Epoch 82/145.. Train loss: 92.676.. Val loss: 54.181.. Train L1 norm: 3.077.. Val L1 norm: 1.084.. Train Linf norm: 2105.326.. Val Linf norm: 66.169\n",
            "Epoch 83/145.. Train loss: 124.796.. Val loss: 54.190.. Train L1 norm: 3.372.. Val L1 norm: 1.083.. Train Linf norm: 2410.265.. Val Linf norm: 65.944\n",
            "Epoch 84/145.. Train loss: 85.532.. Val loss: 54.186.. Train L1 norm: 2.432.. Val L1 norm: 1.083.. Train Linf norm: 1447.160.. Val Linf norm: 66.035\n",
            "Epoch 85/145.. Train loss: 62.277.. Val loss: 54.184.. Train L1 norm: 4.014.. Val L1 norm: 1.083.. Train Linf norm: 3064.496.. Val Linf norm: 66.105\n",
            "Epoch 86/145.. Train loss: 75.421.. Val loss: 54.187.. Train L1 norm: 2.521.. Val L1 norm: 1.083.. Train Linf norm: 1537.314.. Val Linf norm: 66.013\n",
            "Epoch 87/145.. Train loss: 66.154.. Val loss: 54.185.. Train L1 norm: 3.544.. Val L1 norm: 1.083.. Train Linf norm: 2585.185.. Val Linf norm: 66.086\n",
            "Epoch 88/145.. Train loss: 77.481.. Val loss: 54.180.. Train L1 norm: 3.341.. Val L1 norm: 1.084.. Train Linf norm: 2379.662.. Val Linf norm: 66.205\n",
            "Epoch 89/145.. Train loss: 132.899.. Val loss: 54.189.. Train L1 norm: 2.947.. Val L1 norm: 1.083.. Train Linf norm: 1974.084.. Val Linf norm: 65.986\n",
            "Epoch 90/145.. Train loss: 59.644.. Val loss: 54.189.. Train L1 norm: 3.824.. Val L1 norm: 1.083.. Train Linf norm: 2872.169.. Val Linf norm: 65.990\n",
            "Epoch 91/145.. Train loss: 91.103.. Val loss: 54.183.. Train L1 norm: 3.939.. Val L1 norm: 1.083.. Train Linf norm: 2987.562.. Val Linf norm: 66.146\n",
            "Epoch 92/145.. Train loss: 74.891.. Val loss: 54.186.. Train L1 norm: 3.778.. Val L1 norm: 1.083.. Train Linf norm: 2818.223.. Val Linf norm: 66.070\n",
            "Epoch 93/145.. Train loss: 62.375.. Val loss: 54.187.. Train L1 norm: 3.365.. Val L1 norm: 1.083.. Train Linf norm: 2402.988.. Val Linf norm: 66.059\n",
            "Epoch 94/145.. Train loss: 67.001.. Val loss: 54.189.. Train L1 norm: 3.137.. Val L1 norm: 1.083.. Train Linf norm: 2169.685.. Val Linf norm: 66.002\n",
            "Epoch 95/145.. Train loss: 69.302.. Val loss: 54.192.. Train L1 norm: 3.388.. Val L1 norm: 1.083.. Train Linf norm: 2425.626.. Val Linf norm: 65.936\n",
            "Epoch 96/145.. Train loss: 71.085.. Val loss: 54.197.. Train L1 norm: 3.386.. Val L1 norm: 1.083.. Train Linf norm: 2423.376.. Val Linf norm: 65.817\n",
            "Epoch 97/145.. Train loss: 66.511.. Val loss: 54.194.. Train L1 norm: 2.952.. Val L1 norm: 1.083.. Train Linf norm: 1978.948.. Val Linf norm: 65.886\n",
            "Epoch 98/145.. Train loss: 135.239.. Val loss: 54.204.. Train L1 norm: 4.484.. Val L1 norm: 1.083.. Train Linf norm: 3547.383.. Val Linf norm: 65.656\n",
            "Epoch 99/145.. Train loss: 97.005.. Val loss: 54.209.. Train L1 norm: 2.905.. Val L1 norm: 1.083.. Train Linf norm: 1932.647.. Val Linf norm: 65.529\n",
            "Epoch 100/145.. Train loss: 61.340.. Val loss: 54.210.. Train L1 norm: 3.619.. Val L1 norm: 1.082.. Train Linf norm: 2663.564.. Val Linf norm: 65.503\n",
            "Epoch 101/145.. Train loss: 63.527.. Val loss: 54.208.. Train L1 norm: 2.899.. Val L1 norm: 1.083.. Train Linf norm: 1926.072.. Val Linf norm: 65.553\n",
            "Epoch 102/145.. Train loss: 65.007.. Val loss: 54.207.. Train L1 norm: 3.126.. Val L1 norm: 1.083.. Train Linf norm: 2157.699.. Val Linf norm: 65.606\n",
            "Epoch 103/145.. Train loss: 130.799.. Val loss: 54.194.. Train L1 norm: 4.924.. Val L1 norm: 1.083.. Train Linf norm: 3993.723.. Val Linf norm: 65.922\n",
            "Epoch 104/145.. Train loss: 66.527.. Val loss: 54.191.. Train L1 norm: 3.939.. Val L1 norm: 1.083.. Train Linf norm: 2991.576.. Val Linf norm: 66.005\n",
            "Epoch 105/145.. Train loss: 113.044.. Val loss: 54.198.. Train L1 norm: 2.383.. Val L1 norm: 1.083.. Train Linf norm: 1397.253.. Val Linf norm: 65.828\n",
            "Epoch 106/145.. Train loss: 59.682.. Val loss: 54.199.. Train L1 norm: 3.877.. Val L1 norm: 1.083.. Train Linf norm: 2927.489.. Val Linf norm: 65.820\n",
            "Epoch 107/145.. Train loss: 76.319.. Val loss: 54.194.. Train L1 norm: 4.489.. Val L1 norm: 1.083.. Train Linf norm: 3554.947.. Val Linf norm: 65.932\n",
            "Epoch 108/145.. Train loss: 60.187.. Val loss: 54.194.. Train L1 norm: 4.531.. Val L1 norm: 1.083.. Train Linf norm: 3596.799.. Val Linf norm: 65.956\n",
            "Epoch 109/145.. Train loss: 85.006.. Val loss: 54.200.. Train L1 norm: 4.100.. Val L1 norm: 1.083.. Train Linf norm: 3154.484.. Val Linf norm: 65.815\n",
            "Epoch 110/145.. Train loss: 61.192.. Val loss: 54.201.. Train L1 norm: 4.130.. Val L1 norm: 1.083.. Train Linf norm: 3185.312.. Val Linf norm: 65.790\n",
            "Epoch 111/145.. Train loss: 203.604.. Val loss: 54.212.. Train L1 norm: 4.329.. Val L1 norm: 1.082.. Train Linf norm: 3389.225.. Val Linf norm: 65.497\n",
            "Epoch 112/145.. Train loss: 64.038.. Val loss: 54.212.. Train L1 norm: 3.704.. Val L1 norm: 1.082.. Train Linf norm: 2743.048.. Val Linf norm: 65.504\n",
            "Epoch 113/145.. Train loss: 94.648.. Val loss: 54.217.. Train L1 norm: 3.738.. Val L1 norm: 1.082.. Train Linf norm: 2784.508.. Val Linf norm: 65.383\n",
            "Epoch 114/145.. Train loss: 152.721.. Val loss: 54.228.. Train L1 norm: 3.300.. Val L1 norm: 1.082.. Train Linf norm: 2336.089.. Val Linf norm: 65.128\n",
            "Epoch 115/145.. Train loss: 124.736.. Val loss: 54.220.. Train L1 norm: 3.550.. Val L1 norm: 1.082.. Train Linf norm: 2592.401.. Val Linf norm: 65.324\n",
            "Epoch 116/145.. Train loss: 62.343.. Val loss: 54.222.. Train L1 norm: 3.006.. Val L1 norm: 1.082.. Train Linf norm: 2034.091.. Val Linf norm: 65.291\n",
            "Epoch 117/145.. Train loss: 170.142.. Val loss: 54.210.. Train L1 norm: 3.888.. Val L1 norm: 1.083.. Train Linf norm: 2931.881.. Val Linf norm: 65.572\n",
            "Epoch 118/145.. Train loss: 94.829.. Val loss: 54.204.. Train L1 norm: 2.661.. Val L1 norm: 1.083.. Train Linf norm: 1680.799.. Val Linf norm: 65.750\n",
            "Epoch 119/145.. Train loss: 120.992.. Val loss: 54.196.. Train L1 norm: 4.617.. Val L1 norm: 1.083.. Train Linf norm: 3684.140.. Val Linf norm: 65.952\n",
            "Epoch 120/145.. Train loss: 108.295.. Val loss: 54.189.. Train L1 norm: 3.732.. Val L1 norm: 1.083.. Train Linf norm: 2779.373.. Val Linf norm: 66.142\n",
            "Epoch 121/145.. Train loss: 65.585.. Val loss: 54.186.. Train L1 norm: 2.680.. Val L1 norm: 1.084.. Train Linf norm: 1701.515.. Val Linf norm: 66.218\n",
            "Epoch 122/145.. Train loss: 75.284.. Val loss: 54.190.. Train L1 norm: 3.758.. Val L1 norm: 1.083.. Train Linf norm: 2803.179.. Val Linf norm: 66.116\n",
            "Epoch 123/145.. Train loss: 207.324.. Val loss: 54.179.. Train L1 norm: 3.465.. Val L1 norm: 1.084.. Train Linf norm: 2502.052.. Val Linf norm: 66.406\n",
            "Epoch 124/145.. Train loss: 60.854.. Val loss: 54.179.. Train L1 norm: 3.742.. Val L1 norm: 1.084.. Train Linf norm: 2786.486.. Val Linf norm: 66.401\n",
            "Epoch 125/145.. Train loss: 115.823.. Val loss: 54.183.. Train L1 norm: 3.315.. Val L1 norm: 1.084.. Train Linf norm: 2352.625.. Val Linf norm: 66.291\n",
            "Epoch 126/145.. Train loss: 356.300.. Val loss: 54.173.. Train L1 norm: 4.070.. Val L1 norm: 1.084.. Train Linf norm: 3125.035.. Val Linf norm: 66.552\n",
            "Epoch 127/145.. Train loss: 69.983.. Val loss: 54.169.. Train L1 norm: 3.115.. Val L1 norm: 1.084.. Train Linf norm: 2145.645.. Val Linf norm: 66.650\n",
            "Epoch 128/145.. Train loss: 59.673.. Val loss: 54.169.. Train L1 norm: 3.665.. Val L1 norm: 1.084.. Train Linf norm: 2709.506.. Val Linf norm: 66.657\n",
            "Epoch 129/145.. Train loss: 60.074.. Val loss: 54.169.. Train L1 norm: 3.099.. Val L1 norm: 1.084.. Train Linf norm: 2123.565.. Val Linf norm: 66.660\n",
            "Epoch 130/145.. Train loss: 60.107.. Val loss: 54.169.. Train L1 norm: 3.120.. Val L1 norm: 1.084.. Train Linf norm: 2137.371.. Val Linf norm: 66.660\n",
            "Epoch 131/145.. Train loss: 164.170.. Val loss: 54.177.. Train L1 norm: 3.864.. Val L1 norm: 1.084.. Train Linf norm: 2912.097.. Val Linf norm: 66.463\n",
            "Epoch 132/145.. Train loss: 74.019.. Val loss: 54.181.. Train L1 norm: 3.798.. Val L1 norm: 1.084.. Train Linf norm: 2845.632.. Val Linf norm: 66.363\n",
            "Epoch 133/145.. Train loss: 105.564.. Val loss: 54.183.. Train L1 norm: 3.816.. Val L1 norm: 1.084.. Train Linf norm: 2840.460.. Val Linf norm: 66.324\n",
            "Epoch 134/145.. Train loss: 70.485.. Val loss: 54.185.. Train L1 norm: 2.881.. Val L1 norm: 1.084.. Train Linf norm: 1903.016.. Val Linf norm: 66.273\n",
            "Epoch 135/145.. Train loss: 203.757.. Val loss: 54.194.. Train L1 norm: 4.675.. Val L1 norm: 1.083.. Train Linf norm: 3735.866.. Val Linf norm: 66.052\n",
            "Epoch 136/145.. Train loss: 84.546.. Val loss: 54.193.. Train L1 norm: 3.065.. Val L1 norm: 1.083.. Train Linf norm: 2096.254.. Val Linf norm: 66.066\n",
            "Epoch 137/145.. Train loss: 59.913.. Val loss: 54.189.. Train L1 norm: 3.572.. Val L1 norm: 1.084.. Train Linf norm: 2614.363.. Val Linf norm: 66.182\n",
            "Epoch 138/145.. Train loss: 154.951.. Val loss: 54.180.. Train L1 norm: 2.697.. Val L1 norm: 1.084.. Train Linf norm: 1718.305.. Val Linf norm: 66.413\n",
            "Epoch 139/145.. Train loss: 86.312.. Val loss: 54.176.. Train L1 norm: 3.258.. Val L1 norm: 1.084.. Train Linf norm: 2292.826.. Val Linf norm: 66.508\n",
            "Epoch 140/145.. Train loss: 75.461.. Val loss: 54.176.. Train L1 norm: 3.313.. Val L1 norm: 1.084.. Train Linf norm: 2349.838.. Val Linf norm: 66.518\n",
            "Epoch 141/145.. Train loss: 108.765.. Val loss: 54.172.. Train L1 norm: 3.526.. Val L1 norm: 1.084.. Train Linf norm: 2560.156.. Val Linf norm: 66.618\n",
            "Epoch 142/145.. Train loss: 85.552.. Val loss: 54.168.. Train L1 norm: 3.359.. Val L1 norm: 1.084.. Train Linf norm: 2395.925.. Val Linf norm: 66.740\n",
            "Epoch 143/145.. Train loss: 73.749.. Val loss: 54.168.. Train L1 norm: 3.322.. Val L1 norm: 1.084.. Train Linf norm: 2357.826.. Val Linf norm: 66.723\n",
            "Epoch 144/145.. Train loss: 68.697.. Val loss: 54.173.. Train L1 norm: 3.561.. Val L1 norm: 1.084.. Train Linf norm: 2602.461.. Val Linf norm: 66.608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:03:38,170]\u001b[0m Trial 133 finished with value: 1.0839447633743287 and parameters: {'n_layers': 6, 'n_units_0': 3771, 'n_units_1': 1188, 'n_units_2': 2162, 'n_units_3': 483, 'n_units_4': 2806, 'n_units_5': 1094, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 1.6067780388084334e-06, 'batch_size': 1024, 'n_epochs': 145, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.1912214411333009, 'dropout_rate': 0.03792136992904878, 'weight_decay': 0.0005942370237612037, 'beta1': 0.9286497433876328, 'beta2': 0.9991532380632784, 'factor': 0.16152780789676252, 'patience': 6, 'threshold': 0.002140954558179225}. Best is trial 92 with value: 1.0098264760335287.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 145/145.. Train loss: 103.110.. Val loss: 54.180.. Train L1 norm: 3.309.. Val L1 norm: 1.084.. Train Linf norm: 2342.141.. Val Linf norm: 66.454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:03:40,424]\u001b[0m Trial 134 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/138.. Train loss: 1123.021.. Val loss: 56.927.. Train L1 norm: 2.989.. Val L1 norm: 1.077.. Train Linf norm: 1989.288.. Val Linf norm: 46.045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:03:43,799]\u001b[0m Trial 135 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150.. Train loss: 10123.581.. Val loss: 51.496.. Train L1 norm: 8.093.. Val L1 norm: 1.206.. Train Linf norm: 7164.251.. Val Linf norm: 136.548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:03:46,847]\u001b[0m Trial 136 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/143.. Train loss: 570.961.. Val loss: 53.438.. Train L1 norm: 4.898.. Val L1 norm: 1.163.. Train Linf norm: 3960.329.. Val Linf norm: 112.080\n",
            "Epoch 1/146.. Train loss: 5227.117.. Val loss: 56.701.. Train L1 norm: 1.984.. Val L1 norm: 1.028.. Train Linf norm: 1941.247.. Val Linf norm: 35.765\n",
            "Epoch 2/146.. Train loss: 107.634.. Val loss: 55.872.. Train L1 norm: 1.396.. Val L1 norm: 1.016.. Train Linf norm: 761.320.. Val Linf norm: 19.569\n",
            "Epoch 3/146.. Train loss: 2179.674.. Val loss: 57.934.. Train L1 norm: 1.718.. Val L1 norm: 1.094.. Train Linf norm: 1408.659.. Val Linf norm: 95.456\n",
            "Epoch 4/146.. Train loss: 1296.595.. Val loss: 56.694.. Train L1 norm: 2.965.. Val L1 norm: 1.040.. Train Linf norm: 3914.626.. Val Linf norm: 45.300\n",
            "Epoch 5/146.. Train loss: 638.543.. Val loss: 56.070.. Train L1 norm: 1.601.. Val L1 norm: 1.019.. Train Linf norm: 1181.107.. Val Linf norm: 21.970\n",
            "Epoch 6/146.. Train loss: 2038.891.. Val loss: 55.451.. Train L1 norm: 2.223.. Val L1 norm: 1.034.. Train Linf norm: 2455.200.. Val Linf norm: 47.823\n",
            "Epoch 7/146.. Train loss: 2566.886.. Val loss: 56.331.. Train L1 norm: 2.020.. Val L1 norm: 1.021.. Train Linf norm: 2034.946.. Val Linf norm: 21.858\n",
            "Epoch 8/146.. Train loss: 197.785.. Val loss: 57.363.. Train L1 norm: 3.137.. Val L1 norm: 1.068.. Train Linf norm: 4312.776.. Val Linf norm: 68.091\n",
            "Epoch 9/146.. Train loss: 560.078.. Val loss: 57.061.. Train L1 norm: 3.631.. Val L1 norm: 1.052.. Train Linf norm: 5302.708.. Val Linf norm: 53.574\n",
            "Epoch 10/146.. Train loss: 482.654.. Val loss: 53.884.. Train L1 norm: 2.170.. Val L1 norm: 1.134.. Train Linf norm: 2306.230.. Val Linf norm: 153.853\n",
            "Epoch 11/146.. Train loss: 3964.274.. Val loss: 55.033.. Train L1 norm: 6.319.. Val L1 norm: 1.062.. Train Linf norm: 10744.713.. Val Linf norm: 78.912\n",
            "Epoch 12/146.. Train loss: 241.812.. Val loss: 55.560.. Train L1 norm: 4.616.. Val L1 norm: 1.035.. Train Linf norm: 7356.065.. Val Linf norm: 46.111\n",
            "Epoch 13/146.. Train loss: 892.588.. Val loss: 57.250.. Train L1 norm: 2.592.. Val L1 norm: 1.044.. Train Linf norm: 3214.798.. Val Linf norm: 43.630\n",
            "Epoch 14/146.. Train loss: 333.539.. Val loss: 57.219.. Train L1 norm: 1.452.. Val L1 norm: 1.040.. Train Linf norm: 824.977.. Val Linf norm: 38.419\n",
            "Epoch 15/146.. Train loss: 63.554.. Val loss: 56.503.. Train L1 norm: 4.347.. Val L1 norm: 1.025.. Train Linf norm: 6807.339.. Val Linf norm: 28.161\n",
            "Epoch 16/146.. Train loss: 154.011.. Val loss: 55.648.. Train L1 norm: 4.227.. Val L1 norm: 1.043.. Train Linf norm: 6555.350.. Val Linf norm: 62.251\n",
            "Epoch 17/146.. Train loss: 62.165.. Val loss: 55.618.. Train L1 norm: 1.515.. Val L1 norm: 1.045.. Train Linf norm: 1004.747.. Val Linf norm: 63.805\n",
            "Epoch 18/146.. Train loss: 1485.526.. Val loss: 55.748.. Train L1 norm: 3.279.. Val L1 norm: 1.040.. Train Linf norm: 4623.406.. Val Linf norm: 58.532\n",
            "Epoch 19/146.. Train loss: 61.048.. Val loss: 56.003.. Train L1 norm: 1.691.. Val L1 norm: 1.034.. Train Linf norm: 1368.942.. Val Linf norm: 48.144\n",
            "Epoch 20/146.. Train loss: 134.132.. Val loss: 55.955.. Train L1 norm: 2.714.. Val L1 norm: 1.035.. Train Linf norm: 3460.461.. Val Linf norm: 50.372\n",
            "Epoch 21/146.. Train loss: 539.519.. Val loss: 55.734.. Train L1 norm: 1.994.. Val L1 norm: 1.043.. Train Linf norm: 1990.715.. Val Linf norm: 61.325\n",
            "Epoch 22/146.. Train loss: 3390.953.. Val loss: 55.863.. Train L1 norm: 5.006.. Val L1 norm: 1.039.. Train Linf norm: 8150.944.. Val Linf norm: 55.828\n",
            "Epoch 23/146.. Train loss: 204.415.. Val loss: 55.923.. Train L1 norm: 2.107.. Val L1 norm: 1.037.. Train Linf norm: 2219.766.. Val Linf norm: 53.157\n",
            "Epoch 24/146.. Train loss: 744.456.. Val loss: 55.936.. Train L1 norm: 4.580.. Val L1 norm: 1.037.. Train Linf norm: 7287.560.. Val Linf norm: 52.643\n",
            "Epoch 25/146.. Train loss: 158.970.. Val loss: 55.908.. Train L1 norm: 1.115.. Val L1 norm: 1.037.. Train Linf norm: 189.861.. Val Linf norm: 53.882\n",
            "Epoch 26/146.. Train loss: 480.046.. Val loss: 55.939.. Train L1 norm: 4.899.. Val L1 norm: 1.036.. Train Linf norm: 7936.095.. Val Linf norm: 52.531\n",
            "Epoch 27/146.. Train loss: 696.580.. Val loss: 55.961.. Train L1 norm: 1.399.. Val L1 norm: 1.036.. Train Linf norm: 765.030.. Val Linf norm: 51.595\n",
            "Epoch 28/146.. Train loss: 95.205.. Val loss: 55.995.. Train L1 norm: 2.592.. Val L1 norm: 1.035.. Train Linf norm: 3215.821.. Val Linf norm: 50.278\n",
            "Epoch 29/146.. Train loss: 122.163.. Val loss: 55.995.. Train L1 norm: 2.006.. Val L1 norm: 1.035.. Train Linf norm: 2011.078.. Val Linf norm: 50.263\n",
            "Epoch 30/146.. Train loss: 353.739.. Val loss: 55.999.. Train L1 norm: 1.109.. Val L1 norm: 1.035.. Train Linf norm: 173.999.. Val Linf norm: 50.075\n",
            "Epoch 31/146.. Train loss: 71.368.. Val loss: 55.999.. Train L1 norm: 5.187.. Val L1 norm: 1.035.. Train Linf norm: 8523.279.. Val Linf norm: 50.058\n",
            "Epoch 32/146.. Train loss: 148.145.. Val loss: 56.001.. Train L1 norm: 1.456.. Val L1 norm: 1.035.. Train Linf norm: 880.042.. Val Linf norm: 49.984\n",
            "Epoch 33/146.. Train loss: 983.244.. Val loss: 55.994.. Train L1 norm: 3.449.. Val L1 norm: 1.035.. Train Linf norm: 4902.851.. Val Linf norm: 50.320\n",
            "Epoch 34/146.. Train loss: 434.144.. Val loss: 55.997.. Train L1 norm: 1.486.. Val L1 norm: 1.035.. Train Linf norm: 945.349.. Val Linf norm: 50.173\n",
            "Epoch 35/146.. Train loss: 62.316.. Val loss: 55.997.. Train L1 norm: 3.163.. Val L1 norm: 1.035.. Train Linf norm: 4384.955.. Val Linf norm: 50.172\n",
            "Epoch 36/146.. Train loss: 924.328.. Val loss: 55.997.. Train L1 norm: 1.662.. Val L1 norm: 1.035.. Train Linf norm: 1296.290.. Val Linf norm: 50.197\n",
            "Epoch 37/146.. Train loss: 597.037.. Val loss: 55.996.. Train L1 norm: 1.205.. Val L1 norm: 1.035.. Train Linf norm: 368.198.. Val Linf norm: 50.234\n",
            "Epoch 38/146.. Train loss: 86.526.. Val loss: 55.996.. Train L1 norm: 3.447.. Val L1 norm: 1.035.. Train Linf norm: 4958.007.. Val Linf norm: 50.235\n",
            "Epoch 39/146.. Train loss: 447.455.. Val loss: 55.996.. Train L1 norm: 2.145.. Val L1 norm: 1.035.. Train Linf norm: 2276.184.. Val Linf norm: 50.251\n",
            "Epoch 40/146.. Train loss: 72.101.. Val loss: 55.996.. Train L1 norm: 1.162.. Val L1 norm: 1.035.. Train Linf norm: 271.276.. Val Linf norm: 50.252\n",
            "Epoch 41/146.. Train loss: 285.014.. Val loss: 55.995.. Train L1 norm: 3.598.. Val L1 norm: 1.035.. Train Linf norm: 5275.951.. Val Linf norm: 50.262\n",
            "Epoch 42/146.. Train loss: 86.093.. Val loss: 55.995.. Train L1 norm: 2.467.. Val L1 norm: 1.035.. Train Linf norm: 2957.781.. Val Linf norm: 50.265\n",
            "Epoch 43/146.. Train loss: 93.582.. Val loss: 55.995.. Train L1 norm: 2.588.. Val L1 norm: 1.035.. Train Linf norm: 3207.870.. Val Linf norm: 50.268\n",
            "Epoch 44/146.. Train loss: 65.292.. Val loss: 55.995.. Train L1 norm: 5.005.. Val L1 norm: 1.035.. Train Linf norm: 8152.163.. Val Linf norm: 50.269\n",
            "Epoch 45/146.. Train loss: 67.403.. Val loss: 55.995.. Train L1 norm: 1.760.. Val L1 norm: 1.035.. Train Linf norm: 1501.846.. Val Linf norm: 50.270\n",
            "Epoch 46/146.. Train loss: 65.113.. Val loss: 55.995.. Train L1 norm: 1.180.. Val L1 norm: 1.035.. Train Linf norm: 309.215.. Val Linf norm: 50.272\n",
            "Epoch 47/146.. Train loss: 113.806.. Val loss: 55.995.. Train L1 norm: 3.773.. Val L1 norm: 1.035.. Train Linf norm: 5630.873.. Val Linf norm: 50.269\n",
            "Epoch 48/146.. Train loss: 116.627.. Val loss: 55.995.. Train L1 norm: 2.835.. Val L1 norm: 1.035.. Train Linf norm: 3702.441.. Val Linf norm: 50.274\n",
            "Epoch 49/146.. Train loss: 95.832.. Val loss: 55.995.. Train L1 norm: 2.026.. Val L1 norm: 1.035.. Train Linf norm: 2044.928.. Val Linf norm: 50.279\n",
            "Epoch 50/146.. Train loss: 130.870.. Val loss: 55.995.. Train L1 norm: 2.918.. Val L1 norm: 1.035.. Train Linf norm: 3882.003.. Val Linf norm: 50.288\n",
            "Epoch 51/146.. Train loss: 62.075.. Val loss: 55.995.. Train L1 norm: 1.921.. Val L1 norm: 1.035.. Train Linf norm: 1841.627.. Val Linf norm: 50.289\n",
            "Epoch 52/146.. Train loss: 949.487.. Val loss: 55.994.. Train L1 norm: 4.005.. Val L1 norm: 1.035.. Train Linf norm: 6101.670.. Val Linf norm: 50.332\n",
            "Epoch 53/146.. Train loss: 68.499.. Val loss: 55.994.. Train L1 norm: 4.376.. Val L1 norm: 1.035.. Train Linf norm: 6865.985.. Val Linf norm: 50.337\n",
            "Epoch 54/146.. Train loss: 509.285.. Val loss: 55.994.. Train L1 norm: 4.227.. Val L1 norm: 1.035.. Train Linf norm: 6560.574.. Val Linf norm: 50.313\n",
            "Epoch 55/146.. Train loss: 122.827.. Val loss: 55.994.. Train L1 norm: 3.648.. Val L1 norm: 1.035.. Train Linf norm: 5322.638.. Val Linf norm: 50.311\n",
            "Epoch 56/146.. Train loss: 819.416.. Val loss: 55.994.. Train L1 norm: 4.396.. Val L1 norm: 1.035.. Train Linf norm: 6907.279.. Val Linf norm: 50.321\n",
            "Epoch 57/146.. Train loss: 75.139.. Val loss: 55.993.. Train L1 norm: 2.096.. Val L1 norm: 1.035.. Train Linf norm: 2198.044.. Val Linf norm: 50.364\n",
            "Epoch 58/146.. Train loss: 773.479.. Val loss: 55.994.. Train L1 norm: 4.273.. Val L1 norm: 1.035.. Train Linf norm: 6649.452.. Val Linf norm: 50.342\n",
            "Epoch 59/146.. Train loss: 471.877.. Val loss: 55.995.. Train L1 norm: 1.570.. Val L1 norm: 1.035.. Train Linf norm: 1106.458.. Val Linf norm: 50.303\n",
            "Epoch 60/146.. Train loss: 61.016.. Val loss: 55.995.. Train L1 norm: 4.080.. Val L1 norm: 1.035.. Train Linf norm: 6262.501.. Val Linf norm: 50.292\n",
            "Epoch 61/146.. Train loss: 95.476.. Val loss: 55.995.. Train L1 norm: 1.463.. Val L1 norm: 1.035.. Train Linf norm: 898.036.. Val Linf norm: 50.290\n",
            "Epoch 62/146.. Train loss: 693.395.. Val loss: 55.995.. Train L1 norm: 3.023.. Val L1 norm: 1.035.. Train Linf norm: 4090.685.. Val Linf norm: 50.311\n",
            "Epoch 63/146.. Train loss: 65.422.. Val loss: 55.994.. Train L1 norm: 2.642.. Val L1 norm: 1.035.. Train Linf norm: 3312.074.. Val Linf norm: 50.334\n",
            "Epoch 64/146.. Train loss: 97.870.. Val loss: 55.994.. Train L1 norm: 2.881.. Val L1 norm: 1.035.. Train Linf norm: 3800.797.. Val Linf norm: 50.332\n",
            "Epoch 65/146.. Train loss: 285.030.. Val loss: 55.995.. Train L1 norm: 3.863.. Val L1 norm: 1.035.. Train Linf norm: 5806.736.. Val Linf norm: 50.326\n",
            "Epoch 66/146.. Train loss: 440.597.. Val loss: 55.994.. Train L1 norm: 1.779.. Val L1 norm: 1.035.. Train Linf norm: 1550.108.. Val Linf norm: 50.333\n",
            "Epoch 67/146.. Train loss: 342.342.. Val loss: 55.995.. Train L1 norm: 2.654.. Val L1 norm: 1.035.. Train Linf norm: 3332.210.. Val Linf norm: 50.331\n",
            "Epoch 68/146.. Train loss: 95.621.. Val loss: 55.995.. Train L1 norm: 5.068.. Val L1 norm: 1.035.. Train Linf norm: 8287.958.. Val Linf norm: 50.328\n",
            "Epoch 69/146.. Train loss: 61.779.. Val loss: 55.995.. Train L1 norm: 1.326.. Val L1 norm: 1.035.. Train Linf norm: 626.723.. Val Linf norm: 50.328\n",
            "Epoch 70/146.. Train loss: 68.625.. Val loss: 55.995.. Train L1 norm: 2.367.. Val L1 norm: 1.035.. Train Linf norm: 2750.409.. Val Linf norm: 50.330\n",
            "Epoch 71/146.. Train loss: 157.426.. Val loss: 55.995.. Train L1 norm: 1.184.. Val L1 norm: 1.035.. Train Linf norm: 325.836.. Val Linf norm: 50.324\n",
            "Epoch 72/146.. Train loss: 1456.418.. Val loss: 55.993.. Train L1 norm: 3.127.. Val L1 norm: 1.035.. Train Linf norm: 4297.999.. Val Linf norm: 50.398\n",
            "Epoch 73/146.. Train loss: 66.235.. Val loss: 55.993.. Train L1 norm: 2.015.. Val L1 norm: 1.035.. Train Linf norm: 2022.269.. Val Linf norm: 50.401\n",
            "Epoch 74/146.. Train loss: 1104.557.. Val loss: 55.992.. Train L1 norm: 2.570.. Val L1 norm: 1.035.. Train Linf norm: 3159.930.. Val Linf norm: 50.430\n",
            "Epoch 75/146.. Train loss: 135.600.. Val loss: 55.992.. Train L1 norm: 4.196.. Val L1 norm: 1.036.. Train Linf norm: 6489.890.. Val Linf norm: 50.460\n",
            "Epoch 76/146.. Train loss: 488.300.. Val loss: 55.992.. Train L1 norm: 1.138.. Val L1 norm: 1.036.. Train Linf norm: 241.584.. Val Linf norm: 50.438\n",
            "Epoch 77/146.. Train loss: 76.247.. Val loss: 55.993.. Train L1 norm: 1.672.. Val L1 norm: 1.035.. Train Linf norm: 1325.881.. Val Linf norm: 50.429\n",
            "Epoch 78/146.. Train loss: 445.191.. Val loss: 55.992.. Train L1 norm: 2.435.. Val L1 norm: 1.036.. Train Linf norm: 2894.414.. Val Linf norm: 50.450\n",
            "Epoch 79/146.. Train loss: 183.915.. Val loss: 55.992.. Train L1 norm: 3.405.. Val L1 norm: 1.036.. Train Linf norm: 4878.290.. Val Linf norm: 50.466\n",
            "Epoch 80/146.. Train loss: 347.254.. Val loss: 55.992.. Train L1 norm: 1.935.. Val L1 norm: 1.036.. Train Linf norm: 1869.942.. Val Linf norm: 50.460\n",
            "Epoch 81/146.. Train loss: 1587.207.. Val loss: 55.991.. Train L1 norm: 1.193.. Val L1 norm: 1.036.. Train Linf norm: 343.030.. Val Linf norm: 50.521\n",
            "Epoch 82/146.. Train loss: 83.449.. Val loss: 55.991.. Train L1 norm: 5.546.. Val L1 norm: 1.036.. Train Linf norm: 9262.923.. Val Linf norm: 50.529\n",
            "Epoch 83/146.. Train loss: 166.763.. Val loss: 55.990.. Train L1 norm: 2.130.. Val L1 norm: 1.036.. Train Linf norm: 2270.129.. Val Linf norm: 50.536\n",
            "Epoch 84/146.. Train loss: 200.345.. Val loss: 55.990.. Train L1 norm: 5.504.. Val L1 norm: 1.036.. Train Linf norm: 9164.334.. Val Linf norm: 50.528\n",
            "Epoch 85/146.. Train loss: 1568.986.. Val loss: 55.992.. Train L1 norm: 2.825.. Val L1 norm: 1.036.. Train Linf norm: 3687.222.. Val Linf norm: 50.454\n",
            "Epoch 86/146.. Train loss: 461.692.. Val loss: 55.993.. Train L1 norm: 2.652.. Val L1 norm: 1.035.. Train Linf norm: 3325.226.. Val Linf norm: 50.416\n",
            "Epoch 87/146.. Train loss: 205.269.. Val loss: 55.993.. Train L1 norm: 2.168.. Val L1 norm: 1.035.. Train Linf norm: 2348.073.. Val Linf norm: 50.420\n",
            "Epoch 88/146.. Train loss: 112.255.. Val loss: 55.992.. Train L1 norm: 3.422.. Val L1 norm: 1.035.. Train Linf norm: 4917.154.. Val Linf norm: 50.435\n",
            "Epoch 89/146.. Train loss: 109.830.. Val loss: 55.992.. Train L1 norm: 2.502.. Val L1 norm: 1.035.. Train Linf norm: 3028.477.. Val Linf norm: 50.442\n",
            "Epoch 90/146.. Train loss: 373.882.. Val loss: 55.993.. Train L1 norm: 3.593.. Val L1 norm: 1.035.. Train Linf norm: 5257.959.. Val Linf norm: 50.422\n",
            "Epoch 91/146.. Train loss: 1134.731.. Val loss: 55.994.. Train L1 norm: 1.149.. Val L1 norm: 1.035.. Train Linf norm: 258.720.. Val Linf norm: 50.367\n",
            "Epoch 92/146.. Train loss: 133.296.. Val loss: 55.994.. Train L1 norm: 3.335.. Val L1 norm: 1.035.. Train Linf norm: 4733.792.. Val Linf norm: 50.351\n",
            "Epoch 93/146.. Train loss: 132.672.. Val loss: 55.995.. Train L1 norm: 3.227.. Val L1 norm: 1.035.. Train Linf norm: 4505.803.. Val Linf norm: 50.344\n",
            "Epoch 94/146.. Train loss: 66.381.. Val loss: 55.995.. Train L1 norm: 6.651.. Val L1 norm: 1.035.. Train Linf norm: 11527.067.. Val Linf norm: 50.344\n",
            "Epoch 95/146.. Train loss: 329.340.. Val loss: 55.995.. Train L1 norm: 3.646.. Val L1 norm: 1.035.. Train Linf norm: 5376.271.. Val Linf norm: 50.350\n",
            "Epoch 96/146.. Train loss: 815.555.. Val loss: 55.993.. Train L1 norm: 2.880.. Val L1 norm: 1.035.. Train Linf norm: 3806.011.. Val Linf norm: 50.420\n",
            "Epoch 97/146.. Train loss: 68.377.. Val loss: 55.993.. Train L1 norm: 1.855.. Val L1 norm: 1.035.. Train Linf norm: 1700.237.. Val Linf norm: 50.435\n",
            "Epoch 98/146.. Train loss: 1009.447.. Val loss: 55.991.. Train L1 norm: 4.863.. Val L1 norm: 1.036.. Train Linf norm: 7862.449.. Val Linf norm: 50.509\n",
            "Epoch 99/146.. Train loss: 483.915.. Val loss: 55.992.. Train L1 norm: 2.770.. Val L1 norm: 1.036.. Train Linf norm: 3566.767.. Val Linf norm: 50.475\n",
            "Epoch 100/146.. Train loss: 109.101.. Val loss: 55.992.. Train L1 norm: 4.542.. Val L1 norm: 1.036.. Train Linf norm: 7204.517.. Val Linf norm: 50.465\n",
            "Epoch 101/146.. Train loss: 457.795.. Val loss: 55.993.. Train L1 norm: 3.582.. Val L1 norm: 1.035.. Train Linf norm: 5243.476.. Val Linf norm: 50.428\n",
            "Epoch 102/146.. Train loss: 163.176.. Val loss: 55.994.. Train L1 norm: 4.282.. Val L1 norm: 1.035.. Train Linf norm: 6675.739.. Val Linf norm: 50.413\n",
            "Epoch 103/146.. Train loss: 476.138.. Val loss: 55.994.. Train L1 norm: 1.499.. Val L1 norm: 1.035.. Train Linf norm: 974.624.. Val Linf norm: 50.381\n",
            "Epoch 104/146.. Train loss: 503.521.. Val loss: 55.994.. Train L1 norm: 1.306.. Val L1 norm: 1.035.. Train Linf norm: 567.002.. Val Linf norm: 50.421\n",
            "Epoch 105/146.. Train loss: 454.076.. Val loss: 55.994.. Train L1 norm: 1.402.. Val L1 norm: 1.035.. Train Linf norm: 765.596.. Val Linf norm: 50.395\n",
            "Epoch 106/146.. Train loss: 577.944.. Val loss: 55.995.. Train L1 norm: 1.688.. Val L1 norm: 1.035.. Train Linf norm: 1354.623.. Val Linf norm: 50.349\n",
            "Epoch 107/146.. Train loss: 63.452.. Val loss: 55.996.. Train L1 norm: 2.491.. Val L1 norm: 1.035.. Train Linf norm: 3010.456.. Val Linf norm: 50.340\n",
            "Epoch 108/146.. Train loss: 247.760.. Val loss: 55.995.. Train L1 norm: 5.303.. Val L1 norm: 1.035.. Train Linf norm: 8765.662.. Val Linf norm: 50.351\n",
            "Epoch 109/146.. Train loss: 80.024.. Val loss: 55.995.. Train L1 norm: 3.290.. Val L1 norm: 1.035.. Train Linf norm: 4640.257.. Val Linf norm: 50.358\n",
            "Epoch 110/146.. Train loss: 66.455.. Val loss: 55.995.. Train L1 norm: 2.646.. Val L1 norm: 1.035.. Train Linf norm: 3317.850.. Val Linf norm: 50.359\n",
            "Epoch 111/146.. Train loss: 467.370.. Val loss: 55.995.. Train L1 norm: 4.369.. Val L1 norm: 1.035.. Train Linf norm: 6848.720.. Val Linf norm: 50.401\n",
            "Epoch 112/146.. Train loss: 126.952.. Val loss: 55.995.. Train L1 norm: 2.177.. Val L1 norm: 1.035.. Train Linf norm: 2364.289.. Val Linf norm: 50.400\n",
            "Epoch 113/146.. Train loss: 598.324.. Val loss: 55.994.. Train L1 norm: 2.214.. Val L1 norm: 1.035.. Train Linf norm: 2435.815.. Val Linf norm: 50.413\n",
            "Epoch 114/146.. Train loss: 175.378.. Val loss: 55.994.. Train L1 norm: 4.460.. Val L1 norm: 1.035.. Train Linf norm: 7035.526.. Val Linf norm: 50.435\n",
            "Epoch 115/146.. Train loss: 427.501.. Val loss: 55.995.. Train L1 norm: 4.400.. Val L1 norm: 1.035.. Train Linf norm: 6912.952.. Val Linf norm: 50.400\n",
            "Epoch 116/146.. Train loss: 608.560.. Val loss: 55.994.. Train L1 norm: 2.234.. Val L1 norm: 1.035.. Train Linf norm: 2474.066.. Val Linf norm: 50.430\n",
            "Epoch 117/146.. Train loss: 420.808.. Val loss: 55.993.. Train L1 norm: 1.984.. Val L1 norm: 1.036.. Train Linf norm: 1961.805.. Val Linf norm: 50.474\n",
            "Epoch 118/146.. Train loss: 414.612.. Val loss: 55.993.. Train L1 norm: 3.502.. Val L1 norm: 1.036.. Train Linf norm: 5075.363.. Val Linf norm: 50.496\n",
            "Epoch 119/146.. Train loss: 244.294.. Val loss: 55.992.. Train L1 norm: 3.579.. Val L1 norm: 1.036.. Train Linf norm: 5238.561.. Val Linf norm: 50.549\n",
            "Epoch 120/146.. Train loss: 478.988.. Val loss: 55.993.. Train L1 norm: 1.973.. Val L1 norm: 1.036.. Train Linf norm: 1943.964.. Val Linf norm: 50.519\n",
            "Epoch 121/146.. Train loss: 472.340.. Val loss: 55.994.. Train L1 norm: 3.951.. Val L1 norm: 1.036.. Train Linf norm: 5987.303.. Val Linf norm: 50.482\n",
            "Epoch 122/146.. Train loss: 478.812.. Val loss: 55.994.. Train L1 norm: 1.850.. Val L1 norm: 1.036.. Train Linf norm: 1695.346.. Val Linf norm: 50.449\n",
            "Epoch 123/146.. Train loss: 322.740.. Val loss: 55.994.. Train L1 norm: 2.861.. Val L1 norm: 1.036.. Train Linf norm: 3758.594.. Val Linf norm: 50.456\n",
            "Epoch 124/146.. Train loss: 887.193.. Val loss: 55.993.. Train L1 norm: 4.422.. Val L1 norm: 1.036.. Train Linf norm: 6956.579.. Val Linf norm: 50.515\n",
            "Epoch 125/146.. Train loss: 530.531.. Val loss: 55.993.. Train L1 norm: 4.559.. Val L1 norm: 1.036.. Train Linf norm: 7234.550.. Val Linf norm: 50.516\n",
            "Epoch 126/146.. Train loss: 1146.880.. Val loss: 55.993.. Train L1 norm: 1.470.. Val L1 norm: 1.036.. Train Linf norm: 911.293.. Val Linf norm: 50.534\n",
            "Epoch 127/146.. Train loss: 94.072.. Val loss: 55.992.. Train L1 norm: 1.866.. Val L1 norm: 1.036.. Train Linf norm: 1730.934.. Val Linf norm: 50.605\n",
            "Epoch 128/146.. Train loss: 214.740.. Val loss: 55.992.. Train L1 norm: 1.889.. Val L1 norm: 1.036.. Train Linf norm: 1773.459.. Val Linf norm: 50.596\n",
            "Epoch 129/146.. Train loss: 449.650.. Val loss: 55.991.. Train L1 norm: 3.177.. Val L1 norm: 1.036.. Train Linf norm: 4404.241.. Val Linf norm: 50.621\n",
            "Epoch 130/146.. Train loss: 65.079.. Val loss: 55.991.. Train L1 norm: 1.466.. Val L1 norm: 1.036.. Train Linf norm: 908.990.. Val Linf norm: 50.622\n",
            "Epoch 131/146.. Train loss: 450.243.. Val loss: 55.992.. Train L1 norm: 2.635.. Val L1 norm: 1.036.. Train Linf norm: 3289.430.. Val Linf norm: 50.593\n",
            "Epoch 132/146.. Train loss: 83.362.. Val loss: 55.992.. Train L1 norm: 3.962.. Val L1 norm: 1.036.. Train Linf norm: 5997.880.. Val Linf norm: 50.591\n",
            "Epoch 133/146.. Train loss: 62.899.. Val loss: 55.992.. Train L1 norm: 3.680.. Val L1 norm: 1.036.. Train Linf norm: 5437.532.. Val Linf norm: 50.592\n",
            "Epoch 134/146.. Train loss: 63.223.. Val loss: 55.992.. Train L1 norm: 1.450.. Val L1 norm: 1.036.. Train Linf norm: 875.083.. Val Linf norm: 50.593\n",
            "Epoch 135/146.. Train loss: 167.294.. Val loss: 55.992.. Train L1 norm: 1.790.. Val L1 norm: 1.036.. Train Linf norm: 1573.715.. Val Linf norm: 50.603\n",
            "Epoch 136/146.. Train loss: 87.682.. Val loss: 55.992.. Train L1 norm: 1.494.. Val L1 norm: 1.036.. Train Linf norm: 965.372.. Val Linf norm: 50.608\n",
            "Epoch 137/146.. Train loss: 1970.949.. Val loss: 55.993.. Train L1 norm: 3.224.. Val L1 norm: 1.036.. Train Linf norm: 4508.503.. Val Linf norm: 50.570\n",
            "Epoch 138/146.. Train loss: 298.585.. Val loss: 55.995.. Train L1 norm: 3.283.. Val L1 norm: 1.036.. Train Linf norm: 4626.631.. Val Linf norm: 50.461\n",
            "Epoch 139/146.. Train loss: 430.983.. Val loss: 55.994.. Train L1 norm: 3.573.. Val L1 norm: 1.036.. Train Linf norm: 5220.027.. Val Linf norm: 50.495\n",
            "Epoch 140/146.. Train loss: 151.324.. Val loss: 55.994.. Train L1 norm: 1.673.. Val L1 norm: 1.036.. Train Linf norm: 1332.341.. Val Linf norm: 50.488\n",
            "Epoch 141/146.. Train loss: 210.852.. Val loss: 55.995.. Train L1 norm: 2.093.. Val L1 norm: 1.036.. Train Linf norm: 2191.821.. Val Linf norm: 50.477\n",
            "Epoch 142/146.. Train loss: 237.141.. Val loss: 55.995.. Train L1 norm: 3.861.. Val L1 norm: 1.036.. Train Linf norm: 5809.920.. Val Linf norm: 50.477\n",
            "Epoch 143/146.. Train loss: 861.109.. Val loss: 55.996.. Train L1 norm: 2.942.. Val L1 norm: 1.035.. Train Linf norm: 3896.549.. Val Linf norm: 50.434\n",
            "Epoch 144/146.. Train loss: 143.065.. Val loss: 55.996.. Train L1 norm: 3.641.. Val L1 norm: 1.035.. Train Linf norm: 5353.938.. Val Linf norm: 50.417\n",
            "Epoch 145/146.. Train loss: 78.958.. Val loss: 55.996.. Train L1 norm: 5.516.. Val L1 norm: 1.035.. Train Linf norm: 9181.336.. Val Linf norm: 50.420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:07:26,564]\u001b[0m Trial 137 finished with value: 1.0354928470611573 and parameters: {'n_layers': 5, 'n_units_0': 3682, 'n_units_1': 135, 'n_units_2': 3241, 'n_units_3': 402, 'n_units_4': 2605, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 3.555369438711491e-06, 'batch_size': 2048, 'n_epochs': 146, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.2166453267579995, 'dropout_rate': 0.060526088608757224, 'weight_decay': 0.0008688906471349253, 'beta1': 0.9355355744888899, 'beta2': 0.9992027976678598, 'factor': 0.15020131424451627, 'patience': 5, 'threshold': 0.0019758137633171073}. Best is trial 92 with value: 1.0098264760335287.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 146/146.. Train loss: 76.692.. Val loss: 55.996.. Train L1 norm: 1.937.. Val L1 norm: 1.035.. Train Linf norm: 1877.818.. Val Linf norm: 50.419\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:07:27,893]\u001b[0m Trial 138 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/141.. Train loss: 584.323.. Val loss: 56.927.. Train L1 norm: 16.012.. Val L1 norm: 2.121.. Train Linf norm: 30086.477.. Val Linf norm: 968.438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:07:30,858]\u001b[0m Trial 139 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/121.. Train loss: 163.339.. Val loss: 50.756.. Train L1 norm: 7.138.. Val L1 norm: 1.319.. Train Linf norm: 12376.938.. Val Linf norm: 325.943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:07:32,894]\u001b[0m Trial 140 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/139.. Train loss: 6038.134.. Val loss: 52.464.. Train L1 norm: 4.364.. Val L1 norm: 1.152.. Train Linf norm: 6717.352.. Val Linf norm: 186.623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:07:34,785]\u001b[0m Trial 141 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/135.. Train loss: 19509.522.. Val loss: 59.494.. Train L1 norm: 4.448.. Val L1 norm: 1.068.. Train Linf norm: 6776.751.. Val Linf norm: 67.409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:07:36,768]\u001b[0m Trial 142 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/146.. Train loss: 1101.258.. Val loss: 54.461.. Train L1 norm: 2.199.. Val L1 norm: 1.047.. Train Linf norm: 1213.955.. Val Linf norm: 38.302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:07:41,068]\u001b[0m Trial 143 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/148.. Train loss: 476.238.. Val loss: 53.682.. Train L1 norm: 2.699.. Val L1 norm: 1.067.. Train Linf norm: 1719.349.. Val Linf norm: 50.704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:08:08,187]\u001b[0m Trial 144 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/143.. Train loss: 19232.886.. Val loss: 51.889.. Train L1 norm: 17.988.. Val L1 norm: 1.196.. Train Linf norm: 544.788.. Val Linf norm: 7.917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:08:10,774]\u001b[0m Trial 145 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/146.. Train loss: 3097.423.. Val loss: 51.405.. Train L1 norm: 7.821.. Val L1 norm: 1.244.. Train Linf norm: 6923.020.. Val Linf norm: 149.379\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:08:16,243]\u001b[0m Trial 146 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/142.. Train loss: 405.722.. Val loss: 54.472.. Train L1 norm: 2.190.. Val L1 norm: 1.050.. Train Linf norm: 153.689.. Val Linf norm: 7.890\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:08:19,644]\u001b[0m Trial 147 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/145.. Train loss: 4.996.. Val loss: 4.833.. Train L1 norm: 6.044.. Val L1 norm: 1.254.. Train Linf norm: 10201.741.. Val Linf norm: 280.415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:08:21,873]\u001b[0m Trial 148 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/148.. Train loss: 4.559.. Val loss: 4.534.. Train L1 norm: 3.087.. Val L1 norm: 1.068.. Train Linf norm: 2089.055.. Val Linf norm: 53.472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:08:23,052]\u001b[0m Trial 149 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/139.. Train loss: 1344.046.. Val loss: 53.175.. Train L1 norm: 1.845.. Val L1 norm: 1.078.. Train Linf norm: 833.750.. Val Linf norm: 55.437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:08:24,625]\u001b[0m Trial 150 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/149.. Train loss: 318487611.006.. Val loss: 4898001.395.. Train L1 norm: 1376.725.. Val L1 norm: 227.491.. Train Linf norm: 335804.684.. Val Linf norm: 32481.989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:08:29,037]\u001b[0m Trial 151 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/61.. Train loss: 12674.795.. Val loss: 49.998.. Train L1 norm: 11.143.. Val L1 norm: 1.306.. Train Linf norm: 10229.031.. Val Linf norm: 185.019\n",
            "Epoch 1/143.. Train loss: 785.831.. Val loss: 54.331.. Train L1 norm: 1.906.. Val L1 norm: 1.027.. Train Linf norm: 910.192.. Val Linf norm: 22.975\n",
            "Epoch 2/143.. Train loss: 79.848.. Val loss: 54.141.. Train L1 norm: 2.908.. Val L1 norm: 1.035.. Train Linf norm: 1948.443.. Val Linf norm: 28.778\n",
            "Epoch 3/143.. Train loss: 889.719.. Val loss: 55.102.. Train L1 norm: 2.692.. Val L1 norm: 1.011.. Train Linf norm: 1726.611.. Val Linf norm: 11.816\n",
            "Epoch 4/143.. Train loss: 61.700.. Val loss: 55.184.. Train L1 norm: 1.172.. Val L1 norm: 1.011.. Train Linf norm: 172.651.. Val Linf norm: 11.848\n",
            "Epoch 5/143.. Train loss: 67.535.. Val loss: 55.335.. Train L1 norm: 1.920.. Val L1 norm: 1.013.. Train Linf norm: 937.965.. Val Linf norm: 12.851\n",
            "Epoch 6/143.. Train loss: 427.851.. Val loss: 56.346.. Train L1 norm: 2.808.. Val L1 norm: 1.043.. Train Linf norm: 1839.468.. Val Linf norm: 30.464\n",
            "Epoch 7/143.. Train loss: 1305.237.. Val loss: 55.637.. Train L1 norm: 1.251.. Val L1 norm: 1.020.. Train Linf norm: 243.225.. Val Linf norm: 18.036\n",
            "Epoch 8/143.. Train loss: 65.752.. Val loss: 54.454.. Train L1 norm: 1.322.. Val L1 norm: 1.030.. Train Linf norm: 324.650.. Val Linf norm: 24.701\n",
            "Epoch 9/143.. Train loss: 71.311.. Val loss: 54.262.. Train L1 norm: 1.923.. Val L1 norm: 1.038.. Train Linf norm: 940.272.. Val Linf norm: 30.683\n",
            "Epoch 10/143.. Train loss: 88.482.. Val loss: 54.223.. Train L1 norm: 2.838.. Val L1 norm: 1.040.. Train Linf norm: 1874.874.. Val Linf norm: 31.871\n",
            "Epoch 11/143.. Train loss: 231.256.. Val loss: 54.284.. Train L1 norm: 2.661.. Val L1 norm: 1.038.. Train Linf norm: 1692.175.. Val Linf norm: 30.373\n",
            "Epoch 12/143.. Train loss: 168.674.. Val loss: 54.349.. Train L1 norm: 1.767.. Val L1 norm: 1.035.. Train Linf norm: 768.297.. Val Linf norm: 28.743\n",
            "Epoch 13/143.. Train loss: 187.111.. Val loss: 54.305.. Train L1 norm: 2.081.. Val L1 norm: 1.037.. Train Linf norm: 1098.916.. Val Linf norm: 30.027\n",
            "Epoch 14/143.. Train loss: 298.891.. Val loss: 54.348.. Train L1 norm: 2.308.. Val L1 norm: 1.036.. Train Linf norm: 1331.299.. Val Linf norm: 29.028\n",
            "Epoch 15/143.. Train loss: 122.794.. Val loss: 54.439.. Train L1 norm: 2.376.. Val L1 norm: 1.033.. Train Linf norm: 1402.516.. Val Linf norm: 26.854\n",
            "Epoch 16/143.. Train loss: 244.948.. Val loss: 54.523.. Train L1 norm: 1.543.. Val L1 norm: 1.030.. Train Linf norm: 550.722.. Val Linf norm: 24.887\n",
            "Epoch 17/143.. Train loss: 75.990.. Val loss: 54.528.. Train L1 norm: 3.471.. Val L1 norm: 1.030.. Train Linf norm: 2524.232.. Val Linf norm: 24.756\n",
            "Epoch 18/143.. Train loss: 386.342.. Val loss: 54.515.. Train L1 norm: 1.919.. Val L1 norm: 1.030.. Train Linf norm: 935.970.. Val Linf norm: 25.117\n",
            "Epoch 19/143.. Train loss: 126.745.. Val loss: 54.508.. Train L1 norm: 1.985.. Val L1 norm: 1.030.. Train Linf norm: 1002.148.. Val Linf norm: 25.293\n",
            "Epoch 20/143.. Train loss: 107.544.. Val loss: 54.502.. Train L1 norm: 2.478.. Val L1 norm: 1.031.. Train Linf norm: 1506.622.. Val Linf norm: 25.462\n",
            "Epoch 21/143.. Train loss: 264.034.. Val loss: 54.514.. Train L1 norm: 2.305.. Val L1 norm: 1.030.. Train Linf norm: 1328.818.. Val Linf norm: 25.195\n",
            "Epoch 22/143.. Train loss: 118.462.. Val loss: 54.518.. Train L1 norm: 1.988.. Val L1 norm: 1.030.. Train Linf norm: 1004.966.. Val Linf norm: 25.101\n",
            "Epoch 23/143.. Train loss: 60.820.. Val loss: 54.518.. Train L1 norm: 1.612.. Val L1 norm: 1.030.. Train Linf norm: 620.126.. Val Linf norm: 25.121\n",
            "Epoch 24/143.. Train loss: 76.324.. Val loss: 54.518.. Train L1 norm: 2.728.. Val L1 norm: 1.030.. Train Linf norm: 1763.139.. Val Linf norm: 25.117\n",
            "Epoch 25/143.. Train loss: 71.623.. Val loss: 54.518.. Train L1 norm: 1.384.. Val L1 norm: 1.030.. Train Linf norm: 387.193.. Val Linf norm: 25.112\n",
            "Epoch 26/143.. Train loss: 113.673.. Val loss: 54.519.. Train L1 norm: 2.705.. Val L1 norm: 1.030.. Train Linf norm: 1737.495.. Val Linf norm: 25.095\n",
            "Epoch 27/143.. Train loss: 150.122.. Val loss: 54.519.. Train L1 norm: 2.221.. Val L1 norm: 1.030.. Train Linf norm: 1242.370.. Val Linf norm: 25.083\n",
            "Epoch 28/143.. Train loss: 82.441.. Val loss: 54.520.. Train L1 norm: 1.814.. Val L1 norm: 1.030.. Train Linf norm: 827.358.. Val Linf norm: 25.083\n",
            "Epoch 29/143.. Train loss: 85.886.. Val loss: 54.520.. Train L1 norm: 1.697.. Val L1 norm: 1.030.. Train Linf norm: 705.164.. Val Linf norm: 25.078\n",
            "Epoch 30/143.. Train loss: 138.721.. Val loss: 54.519.. Train L1 norm: 3.044.. Val L1 norm: 1.030.. Train Linf norm: 2086.899.. Val Linf norm: 25.095\n",
            "Epoch 31/143.. Train loss: 63.061.. Val loss: 54.519.. Train L1 norm: 1.759.. Val L1 norm: 1.030.. Train Linf norm: 769.955.. Val Linf norm: 25.095\n",
            "Epoch 32/143.. Train loss: 60.152.. Val loss: 54.519.. Train L1 norm: 2.865.. Val L1 norm: 1.030.. Train Linf norm: 1901.564.. Val Linf norm: 25.097\n",
            "Epoch 33/143.. Train loss: 121.801.. Val loss: 54.520.. Train L1 norm: 1.932.. Val L1 norm: 1.030.. Train Linf norm: 949.424.. Val Linf norm: 25.079\n",
            "Epoch 34/143.. Train loss: 127.991.. Val loss: 54.521.. Train L1 norm: 2.575.. Val L1 norm: 1.030.. Train Linf norm: 1608.119.. Val Linf norm: 25.062\n",
            "Epoch 35/143.. Train loss: 77.489.. Val loss: 54.521.. Train L1 norm: 3.056.. Val L1 norm: 1.030.. Train Linf norm: 2098.535.. Val Linf norm: 25.072\n",
            "Epoch 36/143.. Train loss: 294.683.. Val loss: 54.521.. Train L1 norm: 2.495.. Val L1 norm: 1.030.. Train Linf norm: 1524.886.. Val Linf norm: 25.051\n",
            "Epoch 37/143.. Train loss: 119.033.. Val loss: 54.521.. Train L1 norm: 2.137.. Val L1 norm: 1.030.. Train Linf norm: 1158.320.. Val Linf norm: 25.067\n",
            "Epoch 38/143.. Train loss: 60.026.. Val loss: 54.521.. Train L1 norm: 1.776.. Val L1 norm: 1.030.. Train Linf norm: 789.031.. Val Linf norm: 25.069\n",
            "Epoch 39/143.. Train loss: 116.481.. Val loss: 54.520.. Train L1 norm: 1.542.. Val L1 norm: 1.030.. Train Linf norm: 544.572.. Val Linf norm: 25.085\n",
            "Epoch 40/143.. Train loss: 59.922.. Val loss: 54.520.. Train L1 norm: 2.542.. Val L1 norm: 1.030.. Train Linf norm: 1573.309.. Val Linf norm: 25.092\n",
            "Epoch 41/143.. Train loss: 168.780.. Val loss: 54.519.. Train L1 norm: 3.051.. Val L1 norm: 1.030.. Train Linf norm: 2094.926.. Val Linf norm: 25.126\n",
            "Epoch 42/143.. Train loss: 163.162.. Val loss: 54.519.. Train L1 norm: 2.250.. Val L1 norm: 1.030.. Train Linf norm: 1272.950.. Val Linf norm: 25.107\n",
            "Epoch 43/143.. Train loss: 75.424.. Val loss: 54.520.. Train L1 norm: 3.056.. Val L1 norm: 1.030.. Train Linf norm: 2093.944.. Val Linf norm: 25.098\n",
            "Epoch 44/143.. Train loss: 192.963.. Val loss: 54.519.. Train L1 norm: 2.216.. Val L1 norm: 1.030.. Train Linf norm: 1238.182.. Val Linf norm: 25.123\n",
            "Epoch 45/143.. Train loss: 866.352.. Val loss: 54.521.. Train L1 norm: 1.808.. Val L1 norm: 1.030.. Train Linf norm: 809.359.. Val Linf norm: 25.060\n",
            "Epoch 46/143.. Train loss: 69.319.. Val loss: 54.522.. Train L1 norm: 1.574.. Val L1 norm: 1.030.. Train Linf norm: 580.264.. Val Linf norm: 25.056\n",
            "Epoch 47/143.. Train loss: 96.231.. Val loss: 54.522.. Train L1 norm: 1.758.. Val L1 norm: 1.030.. Train Linf norm: 770.675.. Val Linf norm: 25.048\n",
            "Epoch 48/143.. Train loss: 174.056.. Val loss: 54.523.. Train L1 norm: 1.894.. Val L1 norm: 1.030.. Train Linf norm: 910.674.. Val Linf norm: 25.024\n",
            "Epoch 49/143.. Train loss: 80.202.. Val loss: 54.523.. Train L1 norm: 3.038.. Val L1 norm: 1.030.. Train Linf norm: 2083.195.. Val Linf norm: 25.019\n",
            "Epoch 50/143.. Train loss: 78.749.. Val loss: 54.524.. Train L1 norm: 3.144.. Val L1 norm: 1.030.. Train Linf norm: 2189.377.. Val Linf norm: 25.009\n",
            "Epoch 51/143.. Train loss: 86.137.. Val loss: 54.524.. Train L1 norm: 1.514.. Val L1 norm: 1.030.. Train Linf norm: 519.677.. Val Linf norm: 25.002\n",
            "Epoch 52/143.. Train loss: 238.217.. Val loss: 54.526.. Train L1 norm: 3.378.. Val L1 norm: 1.030.. Train Linf norm: 2429.241.. Val Linf norm: 24.966\n",
            "Epoch 53/143.. Train loss: 256.374.. Val loss: 54.527.. Train L1 norm: 2.774.. Val L1 norm: 1.030.. Train Linf norm: 1809.997.. Val Linf norm: 24.927\n",
            "Epoch 54/143.. Train loss: 60.209.. Val loss: 54.528.. Train L1 norm: 2.403.. Val L1 norm: 1.030.. Train Linf norm: 1431.389.. Val Linf norm: 24.922\n",
            "Epoch 55/143.. Train loss: 560.874.. Val loss: 54.529.. Train L1 norm: 1.612.. Val L1 norm: 1.030.. Train Linf norm: 621.264.. Val Linf norm: 24.883\n",
            "Epoch 56/143.. Train loss: 85.739.. Val loss: 54.530.. Train L1 norm: 1.758.. Val L1 norm: 1.030.. Train Linf norm: 770.524.. Val Linf norm: 24.851\n",
            "Epoch 57/143.. Train loss: 84.104.. Val loss: 54.530.. Train L1 norm: 2.907.. Val L1 norm: 1.030.. Train Linf norm: 1945.866.. Val Linf norm: 24.851\n",
            "Epoch 58/143.. Train loss: 297.064.. Val loss: 54.532.. Train L1 norm: 1.651.. Val L1 norm: 1.030.. Train Linf norm: 660.774.. Val Linf norm: 24.804\n",
            "Epoch 59/143.. Train loss: 61.524.. Val loss: 54.532.. Train L1 norm: 2.360.. Val L1 norm: 1.030.. Train Linf norm: 1385.681.. Val Linf norm: 24.803\n",
            "Epoch 60/143.. Train loss: 80.717.. Val loss: 54.533.. Train L1 norm: 2.248.. Val L1 norm: 1.030.. Train Linf norm: 1273.373.. Val Linf norm: 24.796\n",
            "Epoch 61/143.. Train loss: 61.396.. Val loss: 54.533.. Train L1 norm: 1.474.. Val L1 norm: 1.030.. Train Linf norm: 477.421.. Val Linf norm: 24.799\n",
            "Epoch 62/143.. Train loss: 60.074.. Val loss: 54.533.. Train L1 norm: 2.069.. Val L1 norm: 1.030.. Train Linf norm: 1089.527.. Val Linf norm: 24.801\n",
            "Epoch 63/143.. Train loss: 205.305.. Val loss: 54.532.. Train L1 norm: 2.632.. Val L1 norm: 1.030.. Train Linf norm: 1664.313.. Val Linf norm: 24.823\n",
            "Epoch 64/143.. Train loss: 90.261.. Val loss: 54.531.. Train L1 norm: 1.181.. Val L1 norm: 1.030.. Train Linf norm: 178.118.. Val Linf norm: 24.846\n",
            "Epoch 65/143.. Train loss: 102.109.. Val loss: 54.531.. Train L1 norm: 2.391.. Val L1 norm: 1.030.. Train Linf norm: 1418.445.. Val Linf norm: 24.838\n",
            "Epoch 66/143.. Train loss: 59.889.. Val loss: 54.532.. Train L1 norm: 1.513.. Val L1 norm: 1.030.. Train Linf norm: 520.212.. Val Linf norm: 24.836\n",
            "Epoch 67/143.. Train loss: 63.399.. Val loss: 54.532.. Train L1 norm: 2.638.. Val L1 norm: 1.030.. Train Linf norm: 1670.170.. Val Linf norm: 24.835\n",
            "Epoch 68/143.. Train loss: 79.584.. Val loss: 54.532.. Train L1 norm: 1.063.. Val L1 norm: 1.030.. Train Linf norm: 57.596.. Val Linf norm: 24.825\n",
            "Epoch 69/143.. Train loss: 68.846.. Val loss: 54.532.. Train L1 norm: 2.102.. Val L1 norm: 1.030.. Train Linf norm: 1122.838.. Val Linf norm: 24.821\n",
            "Epoch 70/143.. Train loss: 153.616.. Val loss: 54.534.. Train L1 norm: 1.268.. Val L1 norm: 1.030.. Train Linf norm: 267.433.. Val Linf norm: 24.786\n",
            "Epoch 71/143.. Train loss: 62.511.. Val loss: 54.533.. Train L1 norm: 2.868.. Val L1 norm: 1.030.. Train Linf norm: 1901.582.. Val Linf norm: 24.791\n",
            "Epoch 72/143.. Train loss: 258.717.. Val loss: 54.535.. Train L1 norm: 1.162.. Val L1 norm: 1.030.. Train Linf norm: 157.190.. Val Linf norm: 24.758\n",
            "Epoch 73/143.. Train loss: 59.782.. Val loss: 54.535.. Train L1 norm: 2.363.. Val L1 norm: 1.030.. Train Linf norm: 1389.608.. Val Linf norm: 24.759\n",
            "Epoch 74/143.. Train loss: 179.517.. Val loss: 54.536.. Train L1 norm: 2.494.. Val L1 norm: 1.030.. Train Linf norm: 1524.649.. Val Linf norm: 24.723\n",
            "Epoch 75/143.. Train loss: 78.232.. Val loss: 54.537.. Train L1 norm: 2.192.. Val L1 norm: 1.030.. Train Linf norm: 1214.398.. Val Linf norm: 24.715\n",
            "Epoch 76/143.. Train loss: 59.912.. Val loss: 54.537.. Train L1 norm: 2.374.. Val L1 norm: 1.030.. Train Linf norm: 1400.751.. Val Linf norm: 24.717\n",
            "Epoch 77/143.. Train loss: 86.227.. Val loss: 54.537.. Train L1 norm: 2.065.. Val L1 norm: 1.030.. Train Linf norm: 1084.951.. Val Linf norm: 24.704\n",
            "Epoch 78/143.. Train loss: 60.452.. Val loss: 54.537.. Train L1 norm: 3.300.. Val L1 norm: 1.030.. Train Linf norm: 2349.247.. Val Linf norm: 24.707\n",
            "Epoch 79/143.. Train loss: 155.743.. Val loss: 54.536.. Train L1 norm: 1.850.. Val L1 norm: 1.030.. Train Linf norm: 865.097.. Val Linf norm: 24.737\n",
            "Epoch 80/143.. Train loss: 150.787.. Val loss: 54.535.. Train L1 norm: 1.945.. Val L1 norm: 1.030.. Train Linf norm: 961.609.. Val Linf norm: 24.767\n",
            "Epoch 81/143.. Train loss: 81.928.. Val loss: 54.534.. Train L1 norm: 1.251.. Val L1 norm: 1.030.. Train Linf norm: 250.386.. Val Linf norm: 24.791\n",
            "Epoch 82/143.. Train loss: 322.730.. Val loss: 54.536.. Train L1 norm: 2.387.. Val L1 norm: 1.030.. Train Linf norm: 1414.048.. Val Linf norm: 24.751\n",
            "Epoch 83/143.. Train loss: 70.165.. Val loss: 54.536.. Train L1 norm: 1.854.. Val L1 norm: 1.030.. Train Linf norm: 870.062.. Val Linf norm: 24.760\n",
            "Epoch 84/143.. Train loss: 181.576.. Val loss: 54.537.. Train L1 norm: 1.736.. Val L1 norm: 1.030.. Train Linf norm: 746.401.. Val Linf norm: 24.733\n",
            "Epoch 85/143.. Train loss: 118.791.. Val loss: 54.536.. Train L1 norm: 1.545.. Val L1 norm: 1.030.. Train Linf norm: 550.634.. Val Linf norm: 24.764\n",
            "Epoch 86/143.. Train loss: 210.315.. Val loss: 54.537.. Train L1 norm: 2.080.. Val L1 norm: 1.030.. Train Linf norm: 1095.352.. Val Linf norm: 24.724\n",
            "Epoch 87/143.. Train loss: 84.532.. Val loss: 54.537.. Train L1 norm: 1.313.. Val L1 norm: 1.030.. Train Linf norm: 313.226.. Val Linf norm: 24.739\n",
            "Epoch 88/143.. Train loss: 87.532.. Val loss: 54.537.. Train L1 norm: 2.370.. Val L1 norm: 1.030.. Train Linf norm: 1395.926.. Val Linf norm: 24.731\n",
            "Epoch 89/143.. Train loss: 61.608.. Val loss: 54.537.. Train L1 norm: 1.126.. Val L1 norm: 1.030.. Train Linf norm: 119.358.. Val Linf norm: 24.733\n",
            "Epoch 90/143.. Train loss: 78.898.. Val loss: 54.537.. Train L1 norm: 4.179.. Val L1 norm: 1.030.. Train Linf norm: 3248.351.. Val Linf norm: 24.724\n",
            "Epoch 91/143.. Train loss: 92.121.. Val loss: 54.538.. Train L1 norm: 1.509.. Val L1 norm: 1.030.. Train Linf norm: 516.019.. Val Linf norm: 24.708\n",
            "Epoch 92/143.. Train loss: 113.460.. Val loss: 54.539.. Train L1 norm: 2.943.. Val L1 norm: 1.030.. Train Linf norm: 1980.494.. Val Linf norm: 24.695\n",
            "Epoch 93/143.. Train loss: 109.141.. Val loss: 54.538.. Train L1 norm: 1.756.. Val L1 norm: 1.030.. Train Linf norm: 764.860.. Val Linf norm: 24.713\n",
            "Epoch 94/143.. Train loss: 63.030.. Val loss: 54.538.. Train L1 norm: 2.153.. Val L1 norm: 1.030.. Train Linf norm: 1174.256.. Val Linf norm: 24.720\n",
            "Epoch 95/143.. Train loss: 62.094.. Val loss: 54.538.. Train L1 norm: 4.426.. Val L1 norm: 1.030.. Train Linf norm: 3502.329.. Val Linf norm: 24.724\n",
            "Epoch 96/143.. Train loss: 153.510.. Val loss: 54.539.. Train L1 norm: 1.405.. Val L1 norm: 1.030.. Train Linf norm: 404.266.. Val Linf norm: 24.693\n",
            "Epoch 97/143.. Train loss: 151.644.. Val loss: 54.538.. Train L1 norm: 1.922.. Val L1 norm: 1.030.. Train Linf norm: 938.845.. Val Linf norm: 24.723\n",
            "Epoch 98/143.. Train loss: 83.253.. Val loss: 54.537.. Train L1 norm: 2.608.. Val L1 norm: 1.030.. Train Linf norm: 1640.526.. Val Linf norm: 24.737\n",
            "Epoch 99/143.. Train loss: 474.926.. Val loss: 54.538.. Train L1 norm: 1.288.. Val L1 norm: 1.030.. Train Linf norm: 287.281.. Val Linf norm: 24.720\n",
            "Epoch 100/143.. Train loss: 69.835.. Val loss: 54.539.. Train L1 norm: 1.439.. Val L1 norm: 1.030.. Train Linf norm: 444.079.. Val Linf norm: 24.685\n",
            "Epoch 101/143.. Train loss: 313.334.. Val loss: 54.541.. Train L1 norm: 1.204.. Val L1 norm: 1.030.. Train Linf norm: 202.813.. Val Linf norm: 24.632\n",
            "Epoch 102/143.. Train loss: 80.202.. Val loss: 54.542.. Train L1 norm: 2.426.. Val L1 norm: 1.030.. Train Linf norm: 1448.029.. Val Linf norm: 24.627\n",
            "Epoch 103/143.. Train loss: 130.859.. Val loss: 54.541.. Train L1 norm: 1.103.. Val L1 norm: 1.030.. Train Linf norm: 98.547.. Val Linf norm: 24.649\n",
            "Epoch 104/143.. Train loss: 311.518.. Val loss: 54.543.. Train L1 norm: 1.461.. Val L1 norm: 1.029.. Train Linf norm: 462.858.. Val Linf norm: 24.602\n",
            "Epoch 105/143.. Train loss: 65.145.. Val loss: 54.543.. Train L1 norm: 2.599.. Val L1 norm: 1.029.. Train Linf norm: 1631.179.. Val Linf norm: 24.607\n",
            "Epoch 106/143.. Train loss: 68.269.. Val loss: 54.543.. Train L1 norm: 1.066.. Val L1 norm: 1.029.. Train Linf norm: 61.010.. Val Linf norm: 24.604\n",
            "Epoch 107/143.. Train loss: 67.544.. Val loss: 54.543.. Train L1 norm: 2.288.. Val L1 norm: 1.029.. Train Linf norm: 1311.392.. Val Linf norm: 24.601\n",
            "Epoch 108/143.. Train loss: 61.766.. Val loss: 54.543.. Train L1 norm: 2.475.. Val L1 norm: 1.029.. Train Linf norm: 1501.157.. Val Linf norm: 24.601\n",
            "Epoch 109/143.. Train loss: 65.509.. Val loss: 54.543.. Train L1 norm: 2.216.. Val L1 norm: 1.029.. Train Linf norm: 1240.245.. Val Linf norm: 24.600\n",
            "Epoch 110/143.. Train loss: 65.379.. Val loss: 54.543.. Train L1 norm: 1.701.. Val L1 norm: 1.029.. Train Linf norm: 712.052.. Val Linf norm: 24.607\n",
            "Epoch 111/143.. Train loss: 116.326.. Val loss: 54.542.. Train L1 norm: 2.375.. Val L1 norm: 1.030.. Train Linf norm: 1401.832.. Val Linf norm: 24.630\n",
            "Epoch 112/143.. Train loss: 104.571.. Val loss: 54.543.. Train L1 norm: 3.106.. Val L1 norm: 1.029.. Train Linf norm: 2149.622.. Val Linf norm: 24.615\n",
            "Epoch 113/143.. Train loss: 149.507.. Val loss: 54.542.. Train L1 norm: 2.034.. Val L1 norm: 1.030.. Train Linf norm: 1048.165.. Val Linf norm: 24.647\n",
            "Epoch 114/143.. Train loss: 66.038.. Val loss: 54.541.. Train L1 norm: 2.146.. Val L1 norm: 1.030.. Train Linf norm: 1166.155.. Val Linf norm: 24.660\n",
            "Epoch 115/143.. Train loss: 338.294.. Val loss: 54.539.. Train L1 norm: 4.276.. Val L1 norm: 1.030.. Train Linf norm: 3349.721.. Val Linf norm: 24.710\n",
            "Epoch 116/143.. Train loss: 87.914.. Val loss: 54.539.. Train L1 norm: 2.948.. Val L1 norm: 1.030.. Train Linf norm: 1989.864.. Val Linf norm: 24.710\n",
            "Epoch 117/143.. Train loss: 67.768.. Val loss: 54.539.. Train L1 norm: 2.325.. Val L1 norm: 1.030.. Train Linf norm: 1350.305.. Val Linf norm: 24.718\n",
            "Epoch 118/143.. Train loss: 60.068.. Val loss: 54.539.. Train L1 norm: 1.110.. Val L1 norm: 1.030.. Train Linf norm: 106.807.. Val Linf norm: 24.721\n",
            "Epoch 119/143.. Train loss: 149.619.. Val loss: 54.540.. Train L1 norm: 1.247.. Val L1 norm: 1.030.. Train Linf norm: 247.654.. Val Linf norm: 24.690\n",
            "Epoch 120/143.. Train loss: 62.590.. Val loss: 54.540.. Train L1 norm: 1.933.. Val L1 norm: 1.030.. Train Linf norm: 949.384.. Val Linf norm: 24.690\n",
            "Epoch 121/143.. Train loss: 163.220.. Val loss: 54.541.. Train L1 norm: 2.913.. Val L1 norm: 1.030.. Train Linf norm: 1952.430.. Val Linf norm: 24.680\n",
            "Epoch 122/143.. Train loss: 71.672.. Val loss: 54.542.. Train L1 norm: 3.285.. Val L1 norm: 1.030.. Train Linf norm: 2333.438.. Val Linf norm: 24.646\n",
            "Epoch 123/143.. Train loss: 247.145.. Val loss: 54.543.. Train L1 norm: 2.984.. Val L1 norm: 1.029.. Train Linf norm: 2026.553.. Val Linf norm: 24.616\n",
            "Epoch 124/143.. Train loss: 137.776.. Val loss: 54.545.. Train L1 norm: 1.712.. Val L1 norm: 1.029.. Train Linf norm: 725.517.. Val Linf norm: 24.575\n",
            "Epoch 125/143.. Train loss: 93.260.. Val loss: 54.544.. Train L1 norm: 1.762.. Val L1 norm: 1.029.. Train Linf norm: 773.173.. Val Linf norm: 24.595\n",
            "Epoch 126/143.. Train loss: 62.998.. Val loss: 54.544.. Train L1 norm: 3.838.. Val L1 norm: 1.029.. Train Linf norm: 2901.433.. Val Linf norm: 24.604\n",
            "Epoch 127/143.. Train loss: 76.077.. Val loss: 54.543.. Train L1 norm: 2.199.. Val L1 norm: 1.030.. Train Linf norm: 1221.418.. Val Linf norm: 24.622\n",
            "Epoch 128/143.. Train loss: 153.044.. Val loss: 54.544.. Train L1 norm: 2.519.. Val L1 norm: 1.029.. Train Linf norm: 1550.207.. Val Linf norm: 24.598\n",
            "Epoch 129/143.. Train loss: 96.088.. Val loss: 54.544.. Train L1 norm: 2.512.. Val L1 norm: 1.029.. Train Linf norm: 1542.412.. Val Linf norm: 24.615\n",
            "Epoch 130/143.. Train loss: 64.091.. Val loss: 54.544.. Train L1 norm: 1.061.. Val L1 norm: 1.029.. Train Linf norm: 56.231.. Val Linf norm: 24.613\n",
            "Epoch 131/143.. Train loss: 121.062.. Val loss: 54.543.. Train L1 norm: 2.499.. Val L1 norm: 1.030.. Train Linf norm: 1529.708.. Val Linf norm: 24.638\n",
            "Epoch 132/143.. Train loss: 225.513.. Val loss: 54.541.. Train L1 norm: 1.871.. Val L1 norm: 1.030.. Train Linf norm: 887.343.. Val Linf norm: 24.687\n",
            "Epoch 133/143.. Train loss: 64.750.. Val loss: 54.541.. Train L1 norm: 1.651.. Val L1 norm: 1.030.. Train Linf norm: 660.450.. Val Linf norm: 24.694\n",
            "Epoch 134/143.. Train loss: 68.754.. Val loss: 54.540.. Train L1 norm: 1.594.. Val L1 norm: 1.030.. Train Linf norm: 600.259.. Val Linf norm: 24.704\n",
            "Epoch 135/143.. Train loss: 155.676.. Val loss: 54.542.. Train L1 norm: 3.337.. Val L1 norm: 1.030.. Train Linf norm: 2388.016.. Val Linf norm: 24.669\n",
            "Epoch 136/143.. Train loss: 63.745.. Val loss: 54.542.. Train L1 norm: 1.094.. Val L1 norm: 1.030.. Train Linf norm: 83.620.. Val Linf norm: 24.667\n",
            "Epoch 137/143.. Train loss: 313.642.. Val loss: 54.540.. Train L1 norm: 2.305.. Val L1 norm: 1.030.. Train Linf norm: 1330.308.. Val Linf norm: 24.712\n",
            "Epoch 138/143.. Train loss: 67.270.. Val loss: 54.539.. Train L1 norm: 2.513.. Val L1 norm: 1.030.. Train Linf norm: 1543.464.. Val Linf norm: 24.735\n",
            "Epoch 139/143.. Train loss: 76.528.. Val loss: 54.539.. Train L1 norm: 3.046.. Val L1 norm: 1.030.. Train Linf norm: 2088.749.. Val Linf norm: 24.735\n",
            "Epoch 140/143.. Train loss: 125.771.. Val loss: 54.539.. Train L1 norm: 1.343.. Val L1 norm: 1.030.. Train Linf norm: 345.270.. Val Linf norm: 24.755\n",
            "Epoch 141/143.. Train loss: 137.975.. Val loss: 54.540.. Train L1 norm: 1.423.. Val L1 norm: 1.030.. Train Linf norm: 426.827.. Val Linf norm: 24.726\n",
            "Epoch 142/143.. Train loss: 119.661.. Val loss: 54.541.. Train L1 norm: 2.710.. Val L1 norm: 1.030.. Train Linf norm: 1744.651.. Val Linf norm: 24.703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:11:55,020]\u001b[0m Trial 152 finished with value: 1.0296222688039145 and parameters: {'n_layers': 6, 'n_units_0': 3570, 'n_units_1': 209, 'n_units_2': 3957, 'n_units_3': 444, 'n_units_4': 198, 'n_units_5': 1212, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 1.8988544178602165e-06, 'batch_size': 1024, 'n_epochs': 143, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.21109530924378447, 'dropout_rate': 0.06278130422948922, 'weight_decay': 0.0009722920910374711, 'beta1': 0.9379201744409034, 'beta2': 0.9992230653577401, 'factor': 0.11472837500816442, 'patience': 6, 'threshold': 0.0022509265944248816}. Best is trial 92 with value: 1.0098264760335287.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 143/143.. Train loss: 62.403.. Val loss: 54.541.. Train L1 norm: 3.858.. Val L1 norm: 1.030.. Train Linf norm: 2920.968.. Val Linf norm: 24.708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:11:57,030]\u001b[0m Trial 153 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/109.. Train loss: 692.285.. Val loss: 54.111.. Train L1 norm: 4.360.. Val L1 norm: 1.050.. Train Linf norm: 3417.068.. Val Linf norm: 42.998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:11:58,838]\u001b[0m Trial 154 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/144.. Train loss: 1582.517.. Val loss: 49.205.. Train L1 norm: 11.486.. Val L1 norm: 1.582.. Train Linf norm: 10588.974.. Val Linf norm: 340.750\n",
            "Epoch 1/141.. Train loss: 369.671.. Val loss: 54.308.. Train L1 norm: 1.881.. Val L1 norm: 1.027.. Train Linf norm: 895.678.. Val Linf norm: 25.911\n",
            "Epoch 2/141.. Train loss: 810.325.. Val loss: 56.065.. Train L1 norm: 1.246.. Val L1 norm: 1.028.. Train Linf norm: 246.494.. Val Linf norm: 18.276\n",
            "Epoch 3/141.. Train loss: 592.636.. Val loss: 54.617.. Train L1 norm: 2.837.. Val L1 norm: 1.026.. Train Linf norm: 1859.102.. Val Linf norm: 24.885\n",
            "Epoch 4/141.. Train loss: 986.940.. Val loss: 57.205.. Train L1 norm: 2.104.. Val L1 norm: 1.075.. Train Linf norm: 1109.730.. Val Linf norm: 41.222\n",
            "Epoch 5/141.. Train loss: 83.747.. Val loss: 56.602.. Train L1 norm: 2.036.. Val L1 norm: 1.045.. Train Linf norm: 1031.870.. Val Linf norm: 26.574\n",
            "Epoch 6/141.. Train loss: 72.504.. Val loss: 56.086.. Train L1 norm: 2.209.. Val L1 norm: 1.022.. Train Linf norm: 1221.168.. Val Linf norm: 13.863\n",
            "Epoch 7/141.. Train loss: 61.986.. Val loss: 55.983.. Train L1 norm: 1.805.. Val L1 norm: 1.017.. Train Linf norm: 814.392.. Val Linf norm: 11.880\n",
            "Epoch 8/141.. Train loss: 67.105.. Val loss: 55.574.. Train L1 norm: 1.129.. Val L1 norm: 1.013.. Train Linf norm: 125.216.. Val Linf norm: 12.724\n",
            "Epoch 9/141.. Train loss: 166.033.. Val loss: 55.615.. Train L1 norm: 1.703.. Val L1 norm: 1.013.. Train Linf norm: 717.265.. Val Linf norm: 12.751\n",
            "Epoch 10/141.. Train loss: 102.251.. Val loss: 55.893.. Train L1 norm: 1.522.. Val L1 norm: 1.016.. Train Linf norm: 529.675.. Val Linf norm: 12.768\n",
            "Epoch 11/141.. Train loss: 138.719.. Val loss: 55.817.. Train L1 norm: 1.388.. Val L1 norm: 1.015.. Train Linf norm: 390.729.. Val Linf norm: 12.816\n",
            "Epoch 12/141.. Train loss: 79.537.. Val loss: 55.904.. Train L1 norm: 1.187.. Val L1 norm: 1.016.. Train Linf norm: 183.624.. Val Linf norm: 12.864\n",
            "Epoch 13/141.. Train loss: 153.547.. Val loss: 56.134.. Train L1 norm: 1.751.. Val L1 norm: 1.020.. Train Linf norm: 761.926.. Val Linf norm: 12.991\n",
            "Epoch 14/141.. Train loss: 62.845.. Val loss: 56.143.. Train L1 norm: 1.603.. Val L1 norm: 1.020.. Train Linf norm: 607.936.. Val Linf norm: 13.025\n",
            "Epoch 15/141.. Train loss: 65.819.. Val loss: 56.087.. Train L1 norm: 1.602.. Val L1 norm: 1.019.. Train Linf norm: 605.945.. Val Linf norm: 13.016\n",
            "Epoch 16/141.. Train loss: 96.637.. Val loss: 56.069.. Train L1 norm: 1.596.. Val L1 norm: 1.019.. Train Linf norm: 601.742.. Val Linf norm: 13.005\n",
            "Epoch 17/141.. Train loss: 382.520.. Val loss: 56.025.. Train L1 norm: 1.298.. Val L1 norm: 1.018.. Train Linf norm: 295.415.. Val Linf norm: 12.853\n",
            "Epoch 18/141.. Train loss: 78.309.. Val loss: 56.030.. Train L1 norm: 1.291.. Val L1 norm: 1.018.. Train Linf norm: 289.566.. Val Linf norm: 12.857\n",
            "Epoch 19/141.. Train loss: 69.242.. Val loss: 56.026.. Train L1 norm: 1.199.. Val L1 norm: 1.018.. Train Linf norm: 195.620.. Val Linf norm: 12.862\n",
            "Epoch 20/141.. Train loss: 71.377.. Val loss: 56.029.. Train L1 norm: 1.959.. Val L1 norm: 1.018.. Train Linf norm: 972.730.. Val Linf norm: 12.867\n",
            "Epoch 21/141.. Train loss: 132.890.. Val loss: 56.002.. Train L1 norm: 1.336.. Val L1 norm: 1.018.. Train Linf norm: 334.614.. Val Linf norm: 12.890\n",
            "Epoch 22/141.. Train loss: 89.435.. Val loss: 55.981.. Train L1 norm: 1.909.. Val L1 norm: 1.017.. Train Linf norm: 922.567.. Val Linf norm: 12.903\n",
            "Epoch 23/141.. Train loss: 96.338.. Val loss: 55.979.. Train L1 norm: 1.572.. Val L1 norm: 1.017.. Train Linf norm: 579.050.. Val Linf norm: 12.905\n",
            "Epoch 24/141.. Train loss: 80.365.. Val loss: 55.978.. Train L1 norm: 2.010.. Val L1 norm: 1.017.. Train Linf norm: 1026.118.. Val Linf norm: 12.907\n",
            "Epoch 25/141.. Train loss: 67.913.. Val loss: 55.977.. Train L1 norm: 1.534.. Val L1 norm: 1.017.. Train Linf norm: 538.864.. Val Linf norm: 12.909\n",
            "Epoch 26/141.. Train loss: 63.594.. Val loss: 55.977.. Train L1 norm: 1.802.. Val L1 norm: 1.017.. Train Linf norm: 814.134.. Val Linf norm: 12.909\n",
            "Epoch 27/141.. Train loss: 63.320.. Val loss: 55.977.. Train L1 norm: 1.717.. Val L1 norm: 1.017.. Train Linf norm: 727.243.. Val Linf norm: 12.910\n",
            "Epoch 28/141.. Train loss: 63.308.. Val loss: 55.977.. Train L1 norm: 1.343.. Val L1 norm: 1.017.. Train Linf norm: 343.001.. Val Linf norm: 12.911\n",
            "Epoch 29/141.. Train loss: 69.265.. Val loss: 55.977.. Train L1 norm: 1.675.. Val L1 norm: 1.017.. Train Linf norm: 683.568.. Val Linf norm: 12.911\n",
            "Epoch 30/141.. Train loss: 68.748.. Val loss: 55.977.. Train L1 norm: 1.605.. Val L1 norm: 1.017.. Train Linf norm: 611.896.. Val Linf norm: 12.912\n",
            "Epoch 31/141.. Train loss: 95.670.. Val loss: 55.974.. Train L1 norm: 1.959.. Val L1 norm: 1.017.. Train Linf norm: 974.228.. Val Linf norm: 12.913\n",
            "Epoch 32/141.. Train loss: 70.953.. Val loss: 55.975.. Train L1 norm: 1.636.. Val L1 norm: 1.017.. Train Linf norm: 642.887.. Val Linf norm: 12.914\n",
            "Epoch 33/141.. Train loss: 77.212.. Val loss: 55.973.. Train L1 norm: 1.704.. Val L1 norm: 1.017.. Train Linf norm: 713.200.. Val Linf norm: 12.915\n",
            "Epoch 34/141.. Train loss: 95.388.. Val loss: 55.971.. Train L1 norm: 2.403.. Val L1 norm: 1.017.. Train Linf norm: 1428.992.. Val Linf norm: 12.916\n",
            "Epoch 35/141.. Train loss: 105.173.. Val loss: 55.967.. Train L1 norm: 2.317.. Val L1 norm: 1.017.. Train Linf norm: 1339.885.. Val Linf norm: 12.914\n",
            "Epoch 36/141.. Train loss: 73.681.. Val loss: 55.966.. Train L1 norm: 1.370.. Val L1 norm: 1.017.. Train Linf norm: 370.905.. Val Linf norm: 12.916\n",
            "Epoch 37/141.. Train loss: 65.439.. Val loss: 55.965.. Train L1 norm: 1.188.. Val L1 norm: 1.017.. Train Linf norm: 184.839.. Val Linf norm: 12.917\n",
            "Epoch 38/141.. Train loss: 65.678.. Val loss: 55.965.. Train L1 norm: 1.932.. Val L1 norm: 1.017.. Train Linf norm: 947.360.. Val Linf norm: 12.918\n",
            "Epoch 39/141.. Train loss: 85.628.. Val loss: 55.963.. Train L1 norm: 1.758.. Val L1 norm: 1.017.. Train Linf norm: 769.284.. Val Linf norm: 12.919\n",
            "Epoch 40/141.. Train loss: 432.002.. Val loss: 55.954.. Train L1 norm: 1.555.. Val L1 norm: 1.017.. Train Linf norm: 560.559.. Val Linf norm: 12.904\n",
            "Epoch 41/141.. Train loss: 92.558.. Val loss: 55.951.. Train L1 norm: 1.469.. Val L1 norm: 1.017.. Train Linf norm: 473.008.. Val Linf norm: 12.906\n",
            "Epoch 42/141.. Train loss: 171.132.. Val loss: 55.946.. Train L1 norm: 2.361.. Val L1 norm: 1.017.. Train Linf norm: 1385.882.. Val Linf norm: 12.907\n",
            "Epoch 43/141.. Train loss: 69.450.. Val loss: 55.944.. Train L1 norm: 1.301.. Val L1 norm: 1.017.. Train Linf norm: 301.029.. Val Linf norm: 12.909\n",
            "Epoch 44/141.. Train loss: 62.711.. Val loss: 55.944.. Train L1 norm: 1.834.. Val L1 norm: 1.017.. Train Linf norm: 847.312.. Val Linf norm: 12.910\n",
            "Epoch 45/141.. Train loss: 84.663.. Val loss: 55.946.. Train L1 norm: 1.697.. Val L1 norm: 1.017.. Train Linf norm: 705.850.. Val Linf norm: 12.911\n",
            "Epoch 46/141.. Train loss: 63.462.. Val loss: 55.947.. Train L1 norm: 1.102.. Val L1 norm: 1.017.. Train Linf norm: 96.673.. Val Linf norm: 12.911\n",
            "Epoch 47/141.. Train loss: 99.307.. Val loss: 55.943.. Train L1 norm: 1.162.. Val L1 norm: 1.017.. Train Linf norm: 158.547.. Val Linf norm: 12.913\n",
            "Epoch 48/141.. Train loss: 64.295.. Val loss: 55.944.. Train L1 norm: 1.421.. Val L1 norm: 1.017.. Train Linf norm: 423.651.. Val Linf norm: 12.913\n",
            "Epoch 49/141.. Train loss: 176.172.. Val loss: 55.949.. Train L1 norm: 1.893.. Val L1 norm: 1.017.. Train Linf norm: 906.843.. Val Linf norm: 12.907\n",
            "Epoch 50/141.. Train loss: 62.068.. Val loss: 55.949.. Train L1 norm: 1.384.. Val L1 norm: 1.017.. Train Linf norm: 386.991.. Val Linf norm: 12.908\n",
            "Epoch 51/141.. Train loss: 253.717.. Val loss: 55.955.. Train L1 norm: 1.418.. Val L1 norm: 1.017.. Train Linf norm: 421.081.. Val Linf norm: 12.897\n",
            "Epoch 52/141.. Train loss: 76.693.. Val loss: 55.953.. Train L1 norm: 1.141.. Val L1 norm: 1.017.. Train Linf norm: 136.115.. Val Linf norm: 12.899\n",
            "Epoch 53/141.. Train loss: 64.479.. Val loss: 55.954.. Train L1 norm: 1.298.. Val L1 norm: 1.017.. Train Linf norm: 296.566.. Val Linf norm: 12.900\n",
            "Epoch 54/141.. Train loss: 77.246.. Val loss: 55.952.. Train L1 norm: 1.617.. Val L1 norm: 1.017.. Train Linf norm: 624.375.. Val Linf norm: 12.901\n",
            "Epoch 55/141.. Train loss: 65.123.. Val loss: 55.952.. Train L1 norm: 1.953.. Val L1 norm: 1.017.. Train Linf norm: 968.554.. Val Linf norm: 12.903\n",
            "Epoch 56/141.. Train loss: 99.399.. Val loss: 55.948.. Train L1 norm: 2.290.. Val L1 norm: 1.017.. Train Linf norm: 1313.395.. Val Linf norm: 12.903\n",
            "Epoch 57/141.. Train loss: 130.710.. Val loss: 55.945.. Train L1 norm: 1.024.. Val L1 norm: 1.017.. Train Linf norm: 17.703.. Val Linf norm: 12.905\n",
            "Epoch 58/141.. Train loss: 98.588.. Val loss: 55.947.. Train L1 norm: 1.223.. Val L1 norm: 1.017.. Train Linf norm: 221.821.. Val Linf norm: 12.903\n",
            "Epoch 59/141.. Train loss: 89.221.. Val loss: 55.946.. Train L1 norm: 1.069.. Val L1 norm: 1.017.. Train Linf norm: 63.366.. Val Linf norm: 12.904\n",
            "Epoch 60/141.. Train loss: 73.349.. Val loss: 55.943.. Train L1 norm: 1.349.. Val L1 norm: 1.017.. Train Linf norm: 350.582.. Val Linf norm: 12.904\n",
            "Epoch 61/141.. Train loss: 101.917.. Val loss: 55.941.. Train L1 norm: 1.047.. Val L1 norm: 1.017.. Train Linf norm: 40.916.. Val Linf norm: 12.910\n",
            "Epoch 62/141.. Train loss: 62.716.. Val loss: 55.940.. Train L1 norm: 2.079.. Val L1 norm: 1.017.. Train Linf norm: 1097.635.. Val Linf norm: 12.912\n",
            "Epoch 63/141.. Train loss: 63.828.. Val loss: 55.940.. Train L1 norm: 1.555.. Val L1 norm: 1.017.. Train Linf norm: 561.194.. Val Linf norm: 12.912\n",
            "Epoch 64/141.. Train loss: 62.545.. Val loss: 55.941.. Train L1 norm: 1.581.. Val L1 norm: 1.017.. Train Linf norm: 587.839.. Val Linf norm: 12.913\n",
            "Epoch 65/141.. Train loss: 145.632.. Val loss: 55.946.. Train L1 norm: 1.679.. Val L1 norm: 1.017.. Train Linf norm: 687.216.. Val Linf norm: 12.912\n",
            "Epoch 66/141.. Train loss: 66.924.. Val loss: 55.946.. Train L1 norm: 1.982.. Val L1 norm: 1.017.. Train Linf norm: 998.090.. Val Linf norm: 12.912\n",
            "Epoch 67/141.. Train loss: 64.172.. Val loss: 55.945.. Train L1 norm: 1.154.. Val L1 norm: 1.017.. Train Linf norm: 150.006.. Val Linf norm: 12.914\n",
            "Epoch 68/141.. Train loss: 80.805.. Val loss: 55.947.. Train L1 norm: 1.212.. Val L1 norm: 1.017.. Train Linf norm: 209.598.. Val Linf norm: 12.913\n",
            "Epoch 69/141.. Train loss: 62.500.. Val loss: 55.947.. Train L1 norm: 1.641.. Val L1 norm: 1.017.. Train Linf norm: 648.618.. Val Linf norm: 12.913\n",
            "Epoch 70/141.. Train loss: 62.016.. Val loss: 55.947.. Train L1 norm: 1.255.. Val L1 norm: 1.017.. Train Linf norm: 254.144.. Val Linf norm: 12.914\n",
            "Epoch 71/141.. Train loss: 103.189.. Val loss: 55.944.. Train L1 norm: 1.877.. Val L1 norm: 1.017.. Train Linf norm: 890.477.. Val Linf norm: 12.914\n",
            "Epoch 72/141.. Train loss: 86.328.. Val loss: 55.941.. Train L1 norm: 1.494.. Val L1 norm: 1.017.. Train Linf norm: 497.984.. Val Linf norm: 12.915\n",
            "Epoch 73/141.. Train loss: 78.784.. Val loss: 55.940.. Train L1 norm: 1.341.. Val L1 norm: 1.017.. Train Linf norm: 341.492.. Val Linf norm: 12.915\n",
            "Epoch 74/141.. Train loss: 64.198.. Val loss: 55.936.. Train L1 norm: 1.800.. Val L1 norm: 1.017.. Train Linf norm: 811.459.. Val Linf norm: 12.911\n",
            "Epoch 75/141.. Train loss: 128.699.. Val loss: 55.942.. Train L1 norm: 1.572.. Val L1 norm: 1.017.. Train Linf norm: 577.133.. Val Linf norm: 12.913\n",
            "Epoch 76/141.. Train loss: 67.226.. Val loss: 55.942.. Train L1 norm: 1.586.. Val L1 norm: 1.017.. Train Linf norm: 591.252.. Val Linf norm: 12.914\n",
            "Epoch 77/141.. Train loss: 101.128.. Val loss: 55.946.. Train L1 norm: 1.243.. Val L1 norm: 1.017.. Train Linf norm: 240.652.. Val Linf norm: 12.919\n",
            "Epoch 78/141.. Train loss: 88.869.. Val loss: 55.948.. Train L1 norm: 1.910.. Val L1 norm: 1.017.. Train Linf norm: 924.631.. Val Linf norm: 12.918\n",
            "Epoch 79/141.. Train loss: 62.120.. Val loss: 55.949.. Train L1 norm: 1.365.. Val L1 norm: 1.017.. Train Linf norm: 365.663.. Val Linf norm: 12.916\n",
            "Epoch 80/141.. Train loss: 71.416.. Val loss: 55.951.. Train L1 norm: 2.043.. Val L1 norm: 1.017.. Train Linf norm: 1061.389.. Val Linf norm: 12.918\n",
            "Epoch 81/141.. Train loss: 112.701.. Val loss: 55.945.. Train L1 norm: 1.750.. Val L1 norm: 1.017.. Train Linf norm: 759.450.. Val Linf norm: 12.917\n",
            "Epoch 82/141.. Train loss: 66.114.. Val loss: 55.944.. Train L1 norm: 1.315.. Val L1 norm: 1.017.. Train Linf norm: 314.098.. Val Linf norm: 12.917\n",
            "Epoch 83/141.. Train loss: 69.459.. Val loss: 55.945.. Train L1 norm: 1.989.. Val L1 norm: 1.017.. Train Linf norm: 1006.144.. Val Linf norm: 12.917\n",
            "Epoch 84/141.. Train loss: 68.074.. Val loss: 55.944.. Train L1 norm: 1.024.. Val L1 norm: 1.017.. Train Linf norm: 18.005.. Val Linf norm: 12.917\n",
            "Epoch 85/141.. Train loss: 67.231.. Val loss: 55.943.. Train L1 norm: 2.077.. Val L1 norm: 1.017.. Train Linf norm: 1094.809.. Val Linf norm: 12.918\n",
            "Epoch 86/141.. Train loss: 67.727.. Val loss: 55.941.. Train L1 norm: 1.371.. Val L1 norm: 1.017.. Train Linf norm: 373.192.. Val Linf norm: 12.919\n",
            "Epoch 87/141.. Train loss: 99.634.. Val loss: 55.945.. Train L1 norm: 2.101.. Val L1 norm: 1.017.. Train Linf norm: 1120.052.. Val Linf norm: 12.917\n",
            "Epoch 88/141.. Train loss: 65.131.. Val loss: 55.946.. Train L1 norm: 1.697.. Val L1 norm: 1.017.. Train Linf norm: 706.741.. Val Linf norm: 12.919\n",
            "Epoch 89/141.. Train loss: 74.052.. Val loss: 55.948.. Train L1 norm: 1.040.. Val L1 norm: 1.017.. Train Linf norm: 33.406.. Val Linf norm: 12.918\n",
            "Epoch 90/141.. Train loss: 80.944.. Val loss: 55.948.. Train L1 norm: 1.148.. Val L1 norm: 1.017.. Train Linf norm: 143.642.. Val Linf norm: 12.918\n",
            "Epoch 91/141.. Train loss: 97.132.. Val loss: 55.935.. Train L1 norm: 1.432.. Val L1 norm: 1.017.. Train Linf norm: 435.846.. Val Linf norm: 12.922\n",
            "Epoch 92/141.. Train loss: 62.548.. Val loss: 55.935.. Train L1 norm: 1.519.. Val L1 norm: 1.017.. Train Linf norm: 524.158.. Val Linf norm: 12.922\n",
            "Epoch 93/141.. Train loss: 112.298.. Val loss: 55.932.. Train L1 norm: 1.075.. Val L1 norm: 1.017.. Train Linf norm: 69.259.. Val Linf norm: 12.923\n",
            "Epoch 94/141.. Train loss: 69.537.. Val loss: 55.929.. Train L1 norm: 1.671.. Val L1 norm: 1.016.. Train Linf norm: 678.352.. Val Linf norm: 12.923\n",
            "Epoch 95/141.. Train loss: 63.727.. Val loss: 55.927.. Train L1 norm: 1.832.. Val L1 norm: 1.016.. Train Linf norm: 844.431.. Val Linf norm: 12.924\n",
            "Epoch 96/141.. Train loss: 73.433.. Val loss: 55.925.. Train L1 norm: 2.033.. Val L1 norm: 1.016.. Train Linf norm: 1050.734.. Val Linf norm: 12.926\n",
            "Epoch 97/141.. Train loss: 68.572.. Val loss: 55.926.. Train L1 norm: 1.170.. Val L1 norm: 1.016.. Train Linf norm: 167.307.. Val Linf norm: 12.926\n",
            "Epoch 98/141.. Train loss: 107.898.. Val loss: 55.922.. Train L1 norm: 1.619.. Val L1 norm: 1.016.. Train Linf norm: 627.190.. Val Linf norm: 12.927\n",
            "Epoch 99/141.. Train loss: 62.078.. Val loss: 55.921.. Train L1 norm: 2.097.. Val L1 norm: 1.016.. Train Linf norm: 1115.410.. Val Linf norm: 12.928\n",
            "Epoch 100/141.. Train loss: 64.318.. Val loss: 55.921.. Train L1 norm: 1.756.. Val L1 norm: 1.016.. Train Linf norm: 767.412.. Val Linf norm: 12.928\n",
            "Epoch 101/141.. Train loss: 98.236.. Val loss: 55.926.. Train L1 norm: 1.134.. Val L1 norm: 1.016.. Train Linf norm: 128.426.. Val Linf norm: 12.924\n",
            "Epoch 102/141.. Train loss: 70.110.. Val loss: 55.928.. Train L1 norm: 1.688.. Val L1 norm: 1.016.. Train Linf norm: 696.610.. Val Linf norm: 12.924\n",
            "Epoch 103/141.. Train loss: 66.232.. Val loss: 55.929.. Train L1 norm: 1.785.. Val L1 norm: 1.016.. Train Linf norm: 796.103.. Val Linf norm: 12.927\n",
            "Epoch 104/141.. Train loss: 143.246.. Val loss: 55.922.. Train L1 norm: 2.090.. Val L1 norm: 1.016.. Train Linf norm: 1108.995.. Val Linf norm: 12.924\n",
            "Epoch 105/141.. Train loss: 66.631.. Val loss: 55.920.. Train L1 norm: 1.494.. Val L1 norm: 1.016.. Train Linf norm: 498.911.. Val Linf norm: 12.925\n",
            "Epoch 106/141.. Train loss: 61.863.. Val loss: 55.920.. Train L1 norm: 1.558.. Val L1 norm: 1.016.. Train Linf norm: 565.749.. Val Linf norm: 12.926\n",
            "Epoch 107/141.. Train loss: 70.476.. Val loss: 55.922.. Train L1 norm: 1.546.. Val L1 norm: 1.016.. Train Linf norm: 552.789.. Val Linf norm: 12.927\n",
            "Epoch 108/141.. Train loss: 62.458.. Val loss: 55.922.. Train L1 norm: 1.844.. Val L1 norm: 1.016.. Train Linf norm: 856.158.. Val Linf norm: 12.928\n",
            "Epoch 109/141.. Train loss: 77.185.. Val loss: 55.924.. Train L1 norm: 1.670.. Val L1 norm: 1.016.. Train Linf norm: 679.544.. Val Linf norm: 12.927\n",
            "Epoch 110/141.. Train loss: 63.330.. Val loss: 55.925.. Train L1 norm: 1.485.. Val L1 norm: 1.016.. Train Linf norm: 489.439.. Val Linf norm: 12.927\n",
            "Epoch 111/141.. Train loss: 61.743.. Val loss: 55.924.. Train L1 norm: 1.430.. Val L1 norm: 1.016.. Train Linf norm: 434.357.. Val Linf norm: 12.928\n",
            "Epoch 112/141.. Train loss: 100.009.. Val loss: 55.919.. Train L1 norm: 1.789.. Val L1 norm: 1.016.. Train Linf norm: 801.364.. Val Linf norm: 12.934\n",
            "Epoch 113/141.. Train loss: 189.509.. Val loss: 55.911.. Train L1 norm: 1.369.. Val L1 norm: 1.016.. Train Linf norm: 367.514.. Val Linf norm: 12.931\n",
            "Epoch 114/141.. Train loss: 78.121.. Val loss: 55.908.. Train L1 norm: 1.878.. Val L1 norm: 1.016.. Train Linf norm: 892.350.. Val Linf norm: 12.935\n",
            "Epoch 115/141.. Train loss: 102.323.. Val loss: 55.904.. Train L1 norm: 1.077.. Val L1 norm: 1.016.. Train Linf norm: 72.231.. Val Linf norm: 12.938\n",
            "Epoch 116/141.. Train loss: 75.609.. Val loss: 55.901.. Train L1 norm: 1.146.. Val L1 norm: 1.016.. Train Linf norm: 143.394.. Val Linf norm: 12.940\n",
            "Epoch 117/141.. Train loss: 87.940.. Val loss: 55.902.. Train L1 norm: 1.182.. Val L1 norm: 1.016.. Train Linf norm: 181.803.. Val Linf norm: 12.939\n",
            "Epoch 118/141.. Train loss: 89.361.. Val loss: 55.907.. Train L1 norm: 1.738.. Val L1 norm: 1.016.. Train Linf norm: 748.893.. Val Linf norm: 12.937\n",
            "Epoch 119/141.. Train loss: 72.947.. Val loss: 55.910.. Train L1 norm: 1.868.. Val L1 norm: 1.016.. Train Linf norm: 880.587.. Val Linf norm: 12.934\n",
            "Epoch 120/141.. Train loss: 118.374.. Val loss: 55.907.. Train L1 norm: 1.185.. Val L1 norm: 1.016.. Train Linf norm: 182.070.. Val Linf norm: 12.934\n",
            "Epoch 121/141.. Train loss: 62.821.. Val loss: 55.903.. Train L1 norm: 1.173.. Val L1 norm: 1.016.. Train Linf norm: 170.355.. Val Linf norm: 12.934\n",
            "Epoch 122/141.. Train loss: 62.412.. Val loss: 55.903.. Train L1 norm: 1.788.. Val L1 norm: 1.016.. Train Linf norm: 800.627.. Val Linf norm: 12.934\n",
            "Epoch 123/141.. Train loss: 63.506.. Val loss: 55.902.. Train L1 norm: 1.340.. Val L1 norm: 1.016.. Train Linf norm: 342.181.. Val Linf norm: 12.934\n",
            "Epoch 124/141.. Train loss: 68.612.. Val loss: 55.901.. Train L1 norm: 1.896.. Val L1 norm: 1.016.. Train Linf norm: 909.513.. Val Linf norm: 12.935\n",
            "Epoch 125/141.. Train loss: 64.124.. Val loss: 55.901.. Train L1 norm: 1.616.. Val L1 norm: 1.016.. Train Linf norm: 624.110.. Val Linf norm: 12.937\n",
            "Epoch 126/141.. Train loss: 79.153.. Val loss: 55.904.. Train L1 norm: 1.660.. Val L1 norm: 1.016.. Train Linf norm: 669.161.. Val Linf norm: 12.936\n",
            "Epoch 127/141.. Train loss: 83.639.. Val loss: 55.901.. Train L1 norm: 1.843.. Val L1 norm: 1.016.. Train Linf norm: 856.837.. Val Linf norm: 12.936\n",
            "Epoch 128/141.. Train loss: 62.095.. Val loss: 55.900.. Train L1 norm: 2.963.. Val L1 norm: 1.016.. Train Linf norm: 2003.555.. Val Linf norm: 12.936\n",
            "Epoch 129/141.. Train loss: 64.684.. Val loss: 55.899.. Train L1 norm: 1.413.. Val L1 norm: 1.016.. Train Linf norm: 416.193.. Val Linf norm: 12.938\n",
            "Epoch 130/141.. Train loss: 71.612.. Val loss: 55.901.. Train L1 norm: 1.854.. Val L1 norm: 1.016.. Train Linf norm: 868.725.. Val Linf norm: 12.934\n",
            "Epoch 131/141.. Train loss: 93.475.. Val loss: 55.906.. Train L1 norm: 1.423.. Val L1 norm: 1.016.. Train Linf norm: 425.688.. Val Linf norm: 12.932\n",
            "Epoch 132/141.. Train loss: 62.347.. Val loss: 55.907.. Train L1 norm: 1.431.. Val L1 norm: 1.016.. Train Linf norm: 434.714.. Val Linf norm: 12.932\n",
            "Epoch 133/141.. Train loss: 62.134.. Val loss: 55.907.. Train L1 norm: 1.475.. Val L1 norm: 1.016.. Train Linf norm: 479.010.. Val Linf norm: 12.932\n",
            "Epoch 134/141.. Train loss: 112.099.. Val loss: 55.913.. Train L1 norm: 1.823.. Val L1 norm: 1.016.. Train Linf norm: 835.789.. Val Linf norm: 12.926\n",
            "Epoch 135/141.. Train loss: 75.200.. Val loss: 55.910.. Train L1 norm: 1.670.. Val L1 norm: 1.016.. Train Linf norm: 678.815.. Val Linf norm: 12.930\n",
            "Epoch 136/141.. Train loss: 73.862.. Val loss: 55.906.. Train L1 norm: 1.422.. Val L1 norm: 1.016.. Train Linf norm: 424.013.. Val Linf norm: 12.930\n",
            "Epoch 137/141.. Train loss: 67.820.. Val loss: 55.905.. Train L1 norm: 1.823.. Val L1 norm: 1.016.. Train Linf norm: 835.459.. Val Linf norm: 12.930\n",
            "Epoch 138/141.. Train loss: 101.250.. Val loss: 55.910.. Train L1 norm: 1.498.. Val L1 norm: 1.016.. Train Linf norm: 503.032.. Val Linf norm: 12.923\n",
            "Epoch 139/141.. Train loss: 101.101.. Val loss: 55.904.. Train L1 norm: 1.617.. Val L1 norm: 1.016.. Train Linf norm: 624.931.. Val Linf norm: 12.923\n",
            "Epoch 140/141.. Train loss: 113.792.. Val loss: 55.898.. Train L1 norm: 1.398.. Val L1 norm: 1.016.. Train Linf norm: 400.793.. Val Linf norm: 12.921\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:16:20,897]\u001b[0m Trial 155 finished with value: 1.0160089398066203 and parameters: {'n_layers': 6, 'n_units_0': 3551, 'n_units_1': 18, 'n_units_2': 3761, 'n_units_3': 695, 'n_units_4': 1828, 'n_units_5': 1587, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 2.3966011840096783e-06, 'batch_size': 1024, 'n_epochs': 141, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.21517165530201324, 'dropout_rate': 0.02229634724812045, 'weight_decay': 0.0006544116740654679, 'beta1': 0.9280322154557743, 'beta2': 0.999135992617076, 'factor': 0.11692085498151437, 'patience': 6, 'threshold': 0.0016025307169564742}. Best is trial 92 with value: 1.0098264760335287.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 141/141.. Train loss: 62.238.. Val loss: 55.898.. Train L1 norm: 1.599.. Val L1 norm: 1.016.. Train Linf norm: 606.926.. Val Linf norm: 12.922\n",
            "Epoch 1/137.. Train loss: 1258.341.. Val loss: 54.136.. Train L1 norm: 2.408.. Val L1 norm: 1.040.. Train Linf norm: 1430.011.. Val Linf norm: 34.149\n",
            "Epoch 2/137.. Train loss: 204.718.. Val loss: 54.656.. Train L1 norm: 2.287.. Val L1 norm: 1.023.. Train Linf norm: 1312.637.. Val Linf norm: 20.933\n",
            "Epoch 3/137.. Train loss: 67.537.. Val loss: 54.635.. Train L1 norm: 1.896.. Val L1 norm: 1.026.. Train Linf norm: 914.667.. Val Linf norm: 23.265\n",
            "Epoch 4/137.. Train loss: 93.027.. Val loss: 54.580.. Train L1 norm: 1.949.. Val L1 norm: 1.030.. Train Linf norm: 968.433.. Val Linf norm: 26.569\n",
            "Epoch 5/137.. Train loss: 68.417.. Val loss: 54.708.. Train L1 norm: 1.578.. Val L1 norm: 1.027.. Train Linf norm: 590.366.. Val Linf norm: 24.376\n",
            "Epoch 6/137.. Train loss: 97.503.. Val loss: 54.855.. Train L1 norm: 1.984.. Val L1 norm: 1.025.. Train Linf norm: 1005.274.. Val Linf norm: 22.597\n",
            "Epoch 7/137.. Train loss: 228.293.. Val loss: 54.542.. Train L1 norm: 1.263.. Val L1 norm: 1.039.. Train Linf norm: 269.057.. Val Linf norm: 33.076\n",
            "Epoch 8/137.. Train loss: 109.056.. Val loss: 54.514.. Train L1 norm: 2.428.. Val L1 norm: 1.040.. Train Linf norm: 1455.004.. Val Linf norm: 34.527\n",
            "Epoch 9/137.. Train loss: 79.529.. Val loss: 54.714.. Train L1 norm: 2.435.. Val L1 norm: 1.036.. Train Linf norm: 1462.535.. Val Linf norm: 31.373\n",
            "Epoch 10/137.. Train loss: 60.293.. Val loss: 54.851.. Train L1 norm: 1.782.. Val L1 norm: 1.034.. Train Linf norm: 798.024.. Val Linf norm: 29.916\n",
            "Epoch 11/137.. Train loss: 89.920.. Val loss: 54.774.. Train L1 norm: 2.262.. Val L1 norm: 1.037.. Train Linf norm: 1288.126.. Val Linf norm: 32.279\n",
            "Epoch 12/137.. Train loss: 68.826.. Val loss: 54.742.. Train L1 norm: 2.254.. Val L1 norm: 1.038.. Train Linf norm: 1280.140.. Val Linf norm: 33.343\n",
            "Epoch 13/137.. Train loss: 76.664.. Val loss: 54.702.. Train L1 norm: 1.842.. Val L1 norm: 1.040.. Train Linf norm: 856.726.. Val Linf norm: 34.571\n",
            "Epoch 14/137.. Train loss: 60.891.. Val loss: 54.658.. Train L1 norm: 2.186.. Val L1 norm: 1.042.. Train Linf norm: 1208.573.. Val Linf norm: 35.913\n",
            "Epoch 15/137.. Train loss: 65.088.. Val loss: 54.610.. Train L1 norm: 1.987.. Val L1 norm: 1.044.. Train Linf norm: 1004.018.. Val Linf norm: 37.386\n",
            "Epoch 16/137.. Train loss: 59.969.. Val loss: 54.603.. Train L1 norm: 2.101.. Val L1 norm: 1.044.. Train Linf norm: 1121.510.. Val Linf norm: 37.793\n",
            "Epoch 17/137.. Train loss: 62.754.. Val loss: 54.616.. Train L1 norm: 2.626.. Val L1 norm: 1.044.. Train Linf norm: 1657.727.. Val Linf norm: 37.652\n",
            "Epoch 18/137.. Train loss: 61.406.. Val loss: 54.610.. Train L1 norm: 2.261.. Val L1 norm: 1.045.. Train Linf norm: 1285.679.. Val Linf norm: 38.015\n",
            "Epoch 19/137.. Train loss: 61.704.. Val loss: 54.569.. Train L1 norm: 2.154.. Val L1 norm: 1.047.. Train Linf norm: 1174.143.. Val Linf norm: 39.331\n",
            "Epoch 20/137.. Train loss: 60.379.. Val loss: 54.567.. Train L1 norm: 2.377.. Val L1 norm: 1.047.. Train Linf norm: 1402.305.. Val Linf norm: 39.407\n",
            "Epoch 21/137.. Train loss: 63.581.. Val loss: 54.564.. Train L1 norm: 2.347.. Val L1 norm: 1.047.. Train Linf norm: 1371.188.. Val Linf norm: 39.528\n",
            "Epoch 22/137.. Train loss: 61.105.. Val loss: 54.561.. Train L1 norm: 2.437.. Val L1 norm: 1.047.. Train Linf norm: 1465.067.. Val Linf norm: 39.624\n",
            "Epoch 23/137.. Train loss: 59.938.. Val loss: 54.560.. Train L1 norm: 2.361.. Val L1 norm: 1.047.. Train Linf norm: 1387.432.. Val Linf norm: 39.673\n",
            "Epoch 24/137.. Train loss: 60.164.. Val loss: 54.561.. Train L1 norm: 1.769.. Val L1 norm: 1.047.. Train Linf norm: 780.615.. Val Linf norm: 39.683\n",
            "Epoch 25/137.. Train loss: 59.913.. Val loss: 54.561.. Train L1 norm: 2.460.. Val L1 norm: 1.047.. Train Linf norm: 1486.644.. Val Linf norm: 39.714\n",
            "Epoch 26/137.. Train loss: 60.024.. Val loss: 54.559.. Train L1 norm: 2.353.. Val L1 norm: 1.047.. Train Linf norm: 1378.254.. Val Linf norm: 39.774\n",
            "Epoch 27/137.. Train loss: 61.981.. Val loss: 54.562.. Train L1 norm: 2.424.. Val L1 norm: 1.047.. Train Linf norm: 1450.577.. Val Linf norm: 39.738\n",
            "Epoch 28/137.. Train loss: 60.101.. Val loss: 54.560.. Train L1 norm: 2.109.. Val L1 norm: 1.047.. Train Linf norm: 1127.122.. Val Linf norm: 39.812\n",
            "Epoch 29/137.. Train loss: 60.046.. Val loss: 54.560.. Train L1 norm: 2.096.. Val L1 norm: 1.047.. Train Linf norm: 1115.176.. Val Linf norm: 39.816\n",
            "Epoch 30/137.. Train loss: 59.922.. Val loss: 54.560.. Train L1 norm: 2.373.. Val L1 norm: 1.047.. Train Linf norm: 1398.870.. Val Linf norm: 39.820\n",
            "Epoch 31/137.. Train loss: 60.959.. Val loss: 54.560.. Train L1 norm: 2.185.. Val L1 norm: 1.047.. Train Linf norm: 1205.417.. Val Linf norm: 39.829\n",
            "Epoch 32/137.. Train loss: 60.007.. Val loss: 54.560.. Train L1 norm: 2.026.. Val L1 norm: 1.047.. Train Linf norm: 1042.798.. Val Linf norm: 39.833\n",
            "Epoch 33/137.. Train loss: 65.777.. Val loss: 54.559.. Train L1 norm: 2.205.. Val L1 norm: 1.047.. Train Linf norm: 1227.004.. Val Linf norm: 39.857\n",
            "Epoch 34/137.. Train loss: 60.186.. Val loss: 54.559.. Train L1 norm: 2.200.. Val L1 norm: 1.047.. Train Linf norm: 1220.340.. Val Linf norm: 39.866\n",
            "Epoch 35/137.. Train loss: 61.488.. Val loss: 54.559.. Train L1 norm: 2.081.. Val L1 norm: 1.047.. Train Linf norm: 1099.908.. Val Linf norm: 39.863\n",
            "Epoch 36/137.. Train loss: 59.973.. Val loss: 54.559.. Train L1 norm: 1.885.. Val L1 norm: 1.047.. Train Linf norm: 899.387.. Val Linf norm: 39.866\n",
            "Epoch 37/137.. Train loss: 59.886.. Val loss: 54.559.. Train L1 norm: 1.914.. Val L1 norm: 1.047.. Train Linf norm: 928.106.. Val Linf norm: 39.871\n",
            "Epoch 38/137.. Train loss: 61.289.. Val loss: 54.559.. Train L1 norm: 2.297.. Val L1 norm: 1.047.. Train Linf norm: 1320.486.. Val Linf norm: 39.869\n",
            "Epoch 39/137.. Train loss: 60.756.. Val loss: 54.559.. Train L1 norm: 2.300.. Val L1 norm: 1.047.. Train Linf norm: 1323.643.. Val Linf norm: 39.868\n",
            "Epoch 40/137.. Train loss: 62.623.. Val loss: 54.560.. Train L1 norm: 2.192.. Val L1 norm: 1.047.. Train Linf norm: 1212.770.. Val Linf norm: 39.858\n",
            "Epoch 41/137.. Train loss: 61.398.. Val loss: 54.560.. Train L1 norm: 2.331.. Val L1 norm: 1.047.. Train Linf norm: 1355.448.. Val Linf norm: 39.854\n",
            "Epoch 42/137.. Train loss: 65.873.. Val loss: 54.559.. Train L1 norm: 2.267.. Val L1 norm: 1.047.. Train Linf norm: 1290.624.. Val Linf norm: 39.880\n",
            "Epoch 43/137.. Train loss: 60.215.. Val loss: 54.559.. Train L1 norm: 2.255.. Val L1 norm: 1.047.. Train Linf norm: 1277.965.. Val Linf norm: 39.883\n",
            "Epoch 44/137.. Train loss: 59.940.. Val loss: 54.559.. Train L1 norm: 2.021.. Val L1 norm: 1.047.. Train Linf norm: 1038.213.. Val Linf norm: 39.889\n",
            "Epoch 45/137.. Train loss: 62.885.. Val loss: 54.559.. Train L1 norm: 2.409.. Val L1 norm: 1.047.. Train Linf norm: 1435.229.. Val Linf norm: 39.905\n",
            "Epoch 46/137.. Train loss: 60.110.. Val loss: 54.558.. Train L1 norm: 2.406.. Val L1 norm: 1.047.. Train Linf norm: 1432.321.. Val Linf norm: 39.917\n",
            "Epoch 47/137.. Train loss: 59.912.. Val loss: 54.558.. Train L1 norm: 1.877.. Val L1 norm: 1.047.. Train Linf norm: 890.586.. Val Linf norm: 39.922\n",
            "Epoch 48/137.. Train loss: 62.767.. Val loss: 54.558.. Train L1 norm: 2.336.. Val L1 norm: 1.047.. Train Linf norm: 1357.978.. Val Linf norm: 39.941\n",
            "Epoch 49/137.. Train loss: 62.491.. Val loss: 54.558.. Train L1 norm: 1.933.. Val L1 norm: 1.047.. Train Linf norm: 947.760.. Val Linf norm: 39.930\n",
            "Epoch 50/137.. Train loss: 60.048.. Val loss: 54.558.. Train L1 norm: 1.981.. Val L1 norm: 1.047.. Train Linf norm: 997.601.. Val Linf norm: 39.937\n",
            "Epoch 51/137.. Train loss: 60.561.. Val loss: 54.557.. Train L1 norm: 2.428.. Val L1 norm: 1.047.. Train Linf norm: 1454.504.. Val Linf norm: 39.951\n",
            "Epoch 52/137.. Train loss: 63.558.. Val loss: 54.557.. Train L1 norm: 2.364.. Val L1 norm: 1.047.. Train Linf norm: 1388.279.. Val Linf norm: 39.977\n",
            "Epoch 53/137.. Train loss: 60.496.. Val loss: 54.557.. Train L1 norm: 2.314.. Val L1 norm: 1.047.. Train Linf norm: 1338.226.. Val Linf norm: 39.977\n",
            "Epoch 54/137.. Train loss: 60.220.. Val loss: 54.557.. Train L1 norm: 2.304.. Val L1 norm: 1.047.. Train Linf norm: 1326.494.. Val Linf norm: 39.980\n",
            "Epoch 55/137.. Train loss: 61.557.. Val loss: 54.556.. Train L1 norm: 2.203.. Val L1 norm: 1.048.. Train Linf norm: 1225.630.. Val Linf norm: 40.002\n",
            "Epoch 56/137.. Train loss: 60.140.. Val loss: 54.556.. Train L1 norm: 2.193.. Val L1 norm: 1.048.. Train Linf norm: 1215.873.. Val Linf norm: 40.012\n",
            "Epoch 57/137.. Train loss: 60.324.. Val loss: 54.555.. Train L1 norm: 2.330.. Val L1 norm: 1.048.. Train Linf norm: 1350.079.. Val Linf norm: 40.023\n",
            "Epoch 58/137.. Train loss: 59.907.. Val loss: 54.555.. Train L1 norm: 2.268.. Val L1 norm: 1.048.. Train Linf norm: 1291.205.. Val Linf norm: 40.029\n",
            "Epoch 59/137.. Train loss: 60.166.. Val loss: 54.555.. Train L1 norm: 2.065.. Val L1 norm: 1.048.. Train Linf norm: 1082.654.. Val Linf norm: 40.029\n",
            "Epoch 60/137.. Train loss: 60.495.. Val loss: 54.556.. Train L1 norm: 2.149.. Val L1 norm: 1.048.. Train Linf norm: 1168.776.. Val Linf norm: 40.024\n",
            "Epoch 61/137.. Train loss: 61.729.. Val loss: 54.555.. Train L1 norm: 2.706.. Val L1 norm: 1.048.. Train Linf norm: 1739.628.. Val Linf norm: 40.048\n",
            "Epoch 62/137.. Train loss: 61.274.. Val loss: 54.555.. Train L1 norm: 2.436.. Val L1 norm: 1.048.. Train Linf norm: 1459.629.. Val Linf norm: 40.043\n",
            "Epoch 63/137.. Train loss: 64.858.. Val loss: 54.557.. Train L1 norm: 2.324.. Val L1 norm: 1.048.. Train Linf norm: 1347.945.. Val Linf norm: 40.001\n",
            "Epoch 64/137.. Train loss: 62.097.. Val loss: 54.556.. Train L1 norm: 1.926.. Val L1 norm: 1.048.. Train Linf norm: 940.581.. Val Linf norm: 40.026\n",
            "Epoch 65/137.. Train loss: 60.345.. Val loss: 54.555.. Train L1 norm: 2.532.. Val L1 norm: 1.048.. Train Linf norm: 1560.158.. Val Linf norm: 40.042\n",
            "Epoch 66/137.. Train loss: 66.903.. Val loss: 54.554.. Train L1 norm: 2.209.. Val L1 norm: 1.048.. Train Linf norm: 1230.632.. Val Linf norm: 40.087\n",
            "Epoch 67/137.. Train loss: 59.969.. Val loss: 54.554.. Train L1 norm: 2.172.. Val L1 norm: 1.048.. Train Linf norm: 1192.314.. Val Linf norm: 40.093\n",
            "Epoch 68/137.. Train loss: 63.738.. Val loss: 54.555.. Train L1 norm: 2.242.. Val L1 norm: 1.048.. Train Linf norm: 1263.358.. Val Linf norm: 40.063\n",
            "Epoch 69/137.. Train loss: 65.304.. Val loss: 54.554.. Train L1 norm: 2.266.. Val L1 norm: 1.048.. Train Linf norm: 1288.508.. Val Linf norm: 40.103\n",
            "Epoch 70/137.. Train loss: 60.367.. Val loss: 54.554.. Train L1 norm: 2.150.. Val L1 norm: 1.048.. Train Linf norm: 1169.701.. Val Linf norm: 40.104\n",
            "Epoch 71/137.. Train loss: 60.326.. Val loss: 54.554.. Train L1 norm: 2.245.. Val L1 norm: 1.048.. Train Linf norm: 1266.565.. Val Linf norm: 40.096\n",
            "Epoch 72/137.. Train loss: 60.261.. Val loss: 54.554.. Train L1 norm: 2.307.. Val L1 norm: 1.048.. Train Linf norm: 1330.884.. Val Linf norm: 40.110\n",
            "Epoch 73/137.. Train loss: 65.788.. Val loss: 54.553.. Train L1 norm: 2.227.. Val L1 norm: 1.048.. Train Linf norm: 1247.993.. Val Linf norm: 40.141\n",
            "Epoch 74/137.. Train loss: 59.958.. Val loss: 54.552.. Train L1 norm: 1.975.. Val L1 norm: 1.048.. Train Linf norm: 991.378.. Val Linf norm: 40.151\n",
            "Epoch 75/137.. Train loss: 75.316.. Val loss: 54.550.. Train L1 norm: 2.265.. Val L1 norm: 1.048.. Train Linf norm: 1288.064.. Val Linf norm: 40.217\n",
            "Epoch 76/137.. Train loss: 61.256.. Val loss: 54.549.. Train L1 norm: 1.775.. Val L1 norm: 1.048.. Train Linf norm: 784.280.. Val Linf norm: 40.255\n",
            "Epoch 77/137.. Train loss: 64.109.. Val loss: 54.550.. Train L1 norm: 2.012.. Val L1 norm: 1.048.. Train Linf norm: 1028.535.. Val Linf norm: 40.218\n",
            "Epoch 78/137.. Train loss: 59.924.. Val loss: 54.550.. Train L1 norm: 2.417.. Val L1 norm: 1.048.. Train Linf norm: 1440.971.. Val Linf norm: 40.222\n",
            "Epoch 79/137.. Train loss: 59.939.. Val loss: 54.550.. Train L1 norm: 2.328.. Val L1 norm: 1.048.. Train Linf norm: 1352.473.. Val Linf norm: 40.228\n",
            "Epoch 80/137.. Train loss: 63.364.. Val loss: 54.549.. Train L1 norm: 2.177.. Val L1 norm: 1.048.. Train Linf norm: 1197.871.. Val Linf norm: 40.267\n",
            "Epoch 81/137.. Train loss: 66.157.. Val loss: 54.550.. Train L1 norm: 2.179.. Val L1 norm: 1.048.. Train Linf norm: 1198.211.. Val Linf norm: 40.223\n",
            "Epoch 82/137.. Train loss: 61.336.. Val loss: 54.550.. Train L1 norm: 2.270.. Val L1 norm: 1.048.. Train Linf norm: 1291.507.. Val Linf norm: 40.244\n",
            "Epoch 83/137.. Train loss: 60.394.. Val loss: 54.549.. Train L1 norm: 2.144.. Val L1 norm: 1.048.. Train Linf norm: 1164.387.. Val Linf norm: 40.259\n",
            "Epoch 84/137.. Train loss: 61.367.. Val loss: 54.548.. Train L1 norm: 2.114.. Val L1 norm: 1.048.. Train Linf norm: 1131.971.. Val Linf norm: 40.290\n",
            "Epoch 85/137.. Train loss: 59.889.. Val loss: 54.548.. Train L1 norm: 2.159.. Val L1 norm: 1.048.. Train Linf norm: 1178.322.. Val Linf norm: 40.295\n",
            "Epoch 86/137.. Train loss: 60.181.. Val loss: 54.548.. Train L1 norm: 2.357.. Val L1 norm: 1.048.. Train Linf norm: 1381.366.. Val Linf norm: 40.307\n",
            "Epoch 87/137.. Train loss: 61.952.. Val loss: 54.549.. Train L1 norm: 2.271.. Val L1 norm: 1.048.. Train Linf norm: 1290.794.. Val Linf norm: 40.288\n",
            "Epoch 88/137.. Train loss: 60.062.. Val loss: 54.549.. Train L1 norm: 2.345.. Val L1 norm: 1.048.. Train Linf norm: 1368.476.. Val Linf norm: 40.287\n",
            "Epoch 89/137.. Train loss: 59.943.. Val loss: 54.549.. Train L1 norm: 2.220.. Val L1 norm: 1.048.. Train Linf norm: 1241.263.. Val Linf norm: 40.293\n",
            "Epoch 90/137.. Train loss: 59.872.. Val loss: 54.549.. Train L1 norm: 2.187.. Val L1 norm: 1.048.. Train Linf norm: 1208.105.. Val Linf norm: 40.299\n",
            "Epoch 91/137.. Train loss: 79.612.. Val loss: 54.552.. Train L1 norm: 1.925.. Val L1 norm: 1.048.. Train Linf norm: 939.410.. Val Linf norm: 40.214\n",
            "Epoch 92/137.. Train loss: 62.968.. Val loss: 54.551.. Train L1 norm: 2.171.. Val L1 norm: 1.048.. Train Linf norm: 1192.045.. Val Linf norm: 40.255\n",
            "Epoch 93/137.. Train loss: 61.187.. Val loss: 54.551.. Train L1 norm: 2.596.. Val L1 norm: 1.048.. Train Linf norm: 1628.509.. Val Linf norm: 40.237\n",
            "Epoch 94/137.. Train loss: 62.154.. Val loss: 54.552.. Train L1 norm: 2.181.. Val L1 norm: 1.048.. Train Linf norm: 1199.922.. Val Linf norm: 40.219\n",
            "Epoch 95/137.. Train loss: 59.911.. Val loss: 54.552.. Train L1 norm: 2.061.. Val L1 norm: 1.048.. Train Linf norm: 1079.245.. Val Linf norm: 40.214\n",
            "Epoch 96/137.. Train loss: 59.949.. Val loss: 54.552.. Train L1 norm: 2.540.. Val L1 norm: 1.048.. Train Linf norm: 1568.395.. Val Linf norm: 40.222\n",
            "Epoch 97/137.. Train loss: 62.883.. Val loss: 54.554.. Train L1 norm: 2.295.. Val L1 norm: 1.048.. Train Linf norm: 1318.341.. Val Linf norm: 40.187\n",
            "Epoch 98/137.. Train loss: 59.925.. Val loss: 54.554.. Train L1 norm: 2.250.. Val L1 norm: 1.048.. Train Linf norm: 1272.019.. Val Linf norm: 40.192\n",
            "Epoch 99/137.. Train loss: 60.066.. Val loss: 54.554.. Train L1 norm: 2.186.. Val L1 norm: 1.048.. Train Linf norm: 1207.303.. Val Linf norm: 40.191\n",
            "Epoch 100/137.. Train loss: 62.107.. Val loss: 54.555.. Train L1 norm: 2.664.. Val L1 norm: 1.048.. Train Linf norm: 1695.542.. Val Linf norm: 40.158\n",
            "Epoch 101/137.. Train loss: 60.076.. Val loss: 54.555.. Train L1 norm: 2.178.. Val L1 norm: 1.048.. Train Linf norm: 1197.999.. Val Linf norm: 40.153\n",
            "Epoch 102/137.. Train loss: 74.899.. Val loss: 54.558.. Train L1 norm: 2.418.. Val L1 norm: 1.048.. Train Linf norm: 1444.349.. Val Linf norm: 40.081\n",
            "Epoch 103/137.. Train loss: 63.100.. Val loss: 54.560.. Train L1 norm: 2.349.. Val L1 norm: 1.048.. Train Linf norm: 1374.290.. Val Linf norm: 40.041\n",
            "Epoch 104/137.. Train loss: 63.198.. Val loss: 54.558.. Train L1 norm: 2.268.. Val L1 norm: 1.048.. Train Linf norm: 1290.273.. Val Linf norm: 40.086\n",
            "Epoch 105/137.. Train loss: 60.016.. Val loss: 54.558.. Train L1 norm: 2.548.. Val L1 norm: 1.048.. Train Linf norm: 1577.646.. Val Linf norm: 40.097\n",
            "Epoch 106/137.. Train loss: 67.343.. Val loss: 54.555.. Train L1 norm: 2.390.. Val L1 norm: 1.048.. Train Linf norm: 1415.834.. Val Linf norm: 40.171\n",
            "Epoch 107/137.. Train loss: 60.532.. Val loss: 54.554.. Train L1 norm: 2.372.. Val L1 norm: 1.048.. Train Linf norm: 1396.093.. Val Linf norm: 40.191\n",
            "Epoch 108/137.. Train loss: 61.555.. Val loss: 54.554.. Train L1 norm: 2.260.. Val L1 norm: 1.048.. Train Linf norm: 1281.317.. Val Linf norm: 40.212\n",
            "Epoch 109/137.. Train loss: 60.606.. Val loss: 54.552.. Train L1 norm: 2.057.. Val L1 norm: 1.048.. Train Linf norm: 1073.901.. Val Linf norm: 40.254\n",
            "Epoch 110/137.. Train loss: 72.296.. Val loss: 54.549.. Train L1 norm: 2.665.. Val L1 norm: 1.048.. Train Linf norm: 1697.480.. Val Linf norm: 40.335\n",
            "Epoch 111/137.. Train loss: 60.299.. Val loss: 54.549.. Train L1 norm: 2.321.. Val L1 norm: 1.048.. Train Linf norm: 1345.258.. Val Linf norm: 40.357\n",
            "Epoch 112/137.. Train loss: 59.878.. Val loss: 54.549.. Train L1 norm: 2.286.. Val L1 norm: 1.048.. Train Linf norm: 1308.863.. Val Linf norm: 40.362\n",
            "Epoch 113/137.. Train loss: 69.733.. Val loss: 54.551.. Train L1 norm: 2.355.. Val L1 norm: 1.048.. Train Linf norm: 1380.392.. Val Linf norm: 40.311\n",
            "Epoch 114/137.. Train loss: 59.884.. Val loss: 54.551.. Train L1 norm: 2.130.. Val L1 norm: 1.048.. Train Linf norm: 1145.609.. Val Linf norm: 40.306\n",
            "Epoch 115/137.. Train loss: 63.439.. Val loss: 54.549.. Train L1 norm: 2.246.. Val L1 norm: 1.048.. Train Linf norm: 1267.958.. Val Linf norm: 40.353\n",
            "Epoch 116/137.. Train loss: 75.083.. Val loss: 54.546.. Train L1 norm: 2.326.. Val L1 norm: 1.048.. Train Linf norm: 1348.093.. Val Linf norm: 40.439\n",
            "Epoch 117/137.. Train loss: 59.889.. Val loss: 54.546.. Train L1 norm: 2.603.. Val L1 norm: 1.048.. Train Linf norm: 1632.667.. Val Linf norm: 40.446\n",
            "Epoch 118/137.. Train loss: 60.404.. Val loss: 54.546.. Train L1 norm: 2.127.. Val L1 norm: 1.048.. Train Linf norm: 1145.758.. Val Linf norm: 40.441\n",
            "Epoch 119/137.. Train loss: 72.397.. Val loss: 54.549.. Train L1 norm: 2.236.. Val L1 norm: 1.048.. Train Linf norm: 1258.334.. Val Linf norm: 40.361\n",
            "Epoch 120/137.. Train loss: 61.817.. Val loss: 54.548.. Train L1 norm: 2.220.. Val L1 norm: 1.048.. Train Linf norm: 1241.709.. Val Linf norm: 40.394\n",
            "Epoch 121/137.. Train loss: 61.913.. Val loss: 54.548.. Train L1 norm: 2.375.. Val L1 norm: 1.048.. Train Linf norm: 1399.613.. Val Linf norm: 40.379\n",
            "Epoch 122/137.. Train loss: 60.368.. Val loss: 54.549.. Train L1 norm: 2.136.. Val L1 norm: 1.048.. Train Linf norm: 1156.418.. Val Linf norm: 40.367\n",
            "Epoch 123/137.. Train loss: 60.881.. Val loss: 54.550.. Train L1 norm: 2.293.. Val L1 norm: 1.048.. Train Linf norm: 1315.412.. Val Linf norm: 40.354\n",
            "Epoch 124/137.. Train loss: 59.996.. Val loss: 54.549.. Train L1 norm: 2.260.. Val L1 norm: 1.048.. Train Linf norm: 1282.088.. Val Linf norm: 40.363\n",
            "Epoch 125/137.. Train loss: 60.559.. Val loss: 54.550.. Train L1 norm: 2.168.. Val L1 norm: 1.048.. Train Linf norm: 1188.039.. Val Linf norm: 40.351\n",
            "Epoch 126/137.. Train loss: 65.196.. Val loss: 54.549.. Train L1 norm: 2.356.. Val L1 norm: 1.048.. Train Linf norm: 1381.319.. Val Linf norm: 40.387\n",
            "Epoch 127/137.. Train loss: 61.122.. Val loss: 54.548.. Train L1 norm: 2.216.. Val L1 norm: 1.048.. Train Linf norm: 1237.104.. Val Linf norm: 40.422\n",
            "Epoch 128/137.. Train loss: 63.184.. Val loss: 54.549.. Train L1 norm: 2.316.. Val L1 norm: 1.048.. Train Linf norm: 1338.925.. Val Linf norm: 40.388\n",
            "Epoch 129/137.. Train loss: 70.700.. Val loss: 54.546.. Train L1 norm: 2.451.. Val L1 norm: 1.048.. Train Linf norm: 1478.008.. Val Linf norm: 40.459\n",
            "Epoch 130/137.. Train loss: 60.053.. Val loss: 54.546.. Train L1 norm: 2.361.. Val L1 norm: 1.048.. Train Linf norm: 1384.693.. Val Linf norm: 40.472\n",
            "Epoch 131/137.. Train loss: 62.436.. Val loss: 54.547.. Train L1 norm: 2.022.. Val L1 norm: 1.048.. Train Linf norm: 1038.370.. Val Linf norm: 40.446\n",
            "Epoch 132/137.. Train loss: 60.337.. Val loss: 54.547.. Train L1 norm: 2.352.. Val L1 norm: 1.048.. Train Linf norm: 1373.592.. Val Linf norm: 40.439\n",
            "Epoch 133/137.. Train loss: 60.741.. Val loss: 54.547.. Train L1 norm: 2.328.. Val L1 norm: 1.048.. Train Linf norm: 1352.008.. Val Linf norm: 40.453\n",
            "Epoch 134/137.. Train loss: 59.892.. Val loss: 54.547.. Train L1 norm: 2.103.. Val L1 norm: 1.048.. Train Linf norm: 1122.078.. Val Linf norm: 40.466\n",
            "Epoch 135/137.. Train loss: 61.679.. Val loss: 54.545.. Train L1 norm: 2.359.. Val L1 norm: 1.048.. Train Linf norm: 1384.121.. Val Linf norm: 40.513\n",
            "Epoch 136/137.. Train loss: 61.665.. Val loss: 54.544.. Train L1 norm: 2.223.. Val L1 norm: 1.048.. Train Linf norm: 1244.645.. Val Linf norm: 40.549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:20:14,330]\u001b[0m Trial 156 finished with value: 1.0483307592391968 and parameters: {'n_layers': 7, 'n_units_0': 3409, 'n_units_1': 21, 'n_units_2': 3881, 'n_units_3': 715, 'n_units_4': 225, 'n_units_5': 1853, 'n_units_6': 989, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 3.6211601564582004e-06, 'batch_size': 1024, 'n_epochs': 137, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.21565510807233465, 'dropout_rate': 0.020319570411139465, 'weight_decay': 0.0012713531415172628, 'beta1': 0.9284078652670256, 'beta2': 0.9991995095619018, 'factor': 0.11590692923348939, 'patience': 8, 'threshold': 0.0010184440239047366}. Best is trial 92 with value: 1.0098264760335287.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 137/137.. Train loss: 60.806.. Val loss: 54.543.. Train L1 norm: 2.273.. Val L1 norm: 1.048.. Train Linf norm: 1294.522.. Val Linf norm: 40.580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:20:16,502]\u001b[0m Trial 157 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/136.. Train loss: 2.719.. Val loss: 2.322.. Train L1 norm: 14.570.. Val L1 norm: 1.518.. Train Linf norm: 13720.257.. Val Linf norm: 293.098\n",
            "Epoch 1/141.. Train loss: 447.047.. Val loss: 54.512.. Train L1 norm: 7.627.. Val L1 norm: 1.036.. Train Linf norm: 3381.793.. Val Linf norm: 18.456\n",
            "Epoch 2/141.. Train loss: 63.481.. Val loss: 55.117.. Train L1 norm: 1.725.. Val L1 norm: 1.028.. Train Linf norm: 372.372.. Val Linf norm: 14.742\n",
            "Epoch 3/141.. Train loss: 111.716.. Val loss: 53.449.. Train L1 norm: 3.302.. Val L1 norm: 1.099.. Train Linf norm: 1174.130.. Val Linf norm: 43.454\n",
            "Epoch 4/141.. Train loss: 316.848.. Val loss: 55.396.. Train L1 norm: 1.547.. Val L1 norm: 1.041.. Train Linf norm: 278.145.. Val Linf norm: 20.631\n",
            "Epoch 5/141.. Train loss: 258.209.. Val loss: 53.790.. Train L1 norm: 1.518.. Val L1 norm: 1.112.. Train Linf norm: 261.561.. Val Linf norm: 48.658\n",
            "Epoch 6/141.. Train loss: 176.032.. Val loss: 54.864.. Train L1 norm: 3.635.. Val L1 norm: 1.084.. Train Linf norm: 1337.569.. Val Linf norm: 38.734\n",
            "Epoch 7/141.. Train loss: 148.028.. Val loss: 53.335.. Train L1 norm: 3.280.. Val L1 norm: 1.170.. Train Linf norm: 1152.491.. Val Linf norm: 70.560\n",
            "Epoch 8/141.. Train loss: 198.387.. Val loss: 54.320.. Train L1 norm: 3.378.. Val L1 norm: 1.146.. Train Linf norm: 1202.573.. Val Linf norm: 63.038\n",
            "Epoch 9/141.. Train loss: 94.341.. Val loss: 53.447.. Train L1 norm: 3.563.. Val L1 norm: 1.212.. Train Linf norm: 1293.946.. Val Linf norm: 86.573\n",
            "Epoch 10/141.. Train loss: 174.441.. Val loss: 53.579.. Train L1 norm: 6.140.. Val L1 norm: 1.236.. Train Linf norm: 2597.145.. Val Linf norm: 95.709\n",
            "Epoch 11/141.. Train loss: 108.779.. Val loss: 51.979.. Train L1 norm: 6.549.. Val L1 norm: 1.349.. Train Linf norm: 2800.703.. Val Linf norm: 133.020\n",
            "Epoch 12/141.. Train loss: 335.866.. Val loss: 54.285.. Train L1 norm: 7.030.. Val L1 norm: 1.294.. Train Linf norm: 3042.299.. Val Linf norm: 116.415\n",
            "Epoch 13/141.. Train loss: 226.494.. Val loss: 51.216.. Train L1 norm: 6.242.. Val L1 norm: 1.473.. Train Linf norm: 2635.609.. Val Linf norm: 173.785\n",
            "Epoch 14/141.. Train loss: 2422.386.. Val loss: 67.031.. Train L1 norm: 3.640.. Val L1 norm: 1.288.. Train Linf norm: 1281.957.. Val Linf norm: 114.955\n",
            "Epoch 15/141.. Train loss: 1987.784.. Val loss: 83.175.. Train L1 norm: 27.820.. Val L1 norm: 1.551.. Train Linf norm: 13547.602.. Val Linf norm: 197.726\n",
            "Epoch 16/141.. Train loss: 233.331.. Val loss: 64.380.. Train L1 norm: 25.610.. Val L1 norm: 2.027.. Train Linf norm: 12327.080.. Val Linf norm: 331.813\n",
            "Epoch 17/141.. Train loss: 13528.721.. Val loss: 252.210.. Train L1 norm: 5.889.. Val L1 norm: 1.703.. Train Linf norm: 2359.831.. Val Linf norm: 245.572\n",
            "Epoch 18/141.. Train loss: 3501.089.. Val loss: 73.019.. Train L1 norm: 12.440.. Val L1 norm: 1.582.. Train Linf norm: 5310.731.. Val Linf norm: 215.992\n",
            "Epoch 19/141.. Train loss: 6294.466.. Val loss: 1368.616.. Train L1 norm: 5.378.. Val L1 norm: 1.970.. Train Linf norm: 2067.753.. Val Linf norm: 320.253\n",
            "Epoch 20/141.. Train loss: 1185.445.. Val loss: 2048.819.. Train L1 norm: 6.790.. Val L1 norm: 7.709.. Train Linf norm: 2731.625.. Val Linf norm: 1359.109\n",
            "Epoch 21/141.. Train loss: 1666262.458.. Val loss: 4152.642.. Train L1 norm: 26.006.. Val L1 norm: 1.998.. Train Linf norm: 11479.536.. Val Linf norm: 322.934\n",
            "Epoch 22/141.. Train loss: 26986.167.. Val loss: 543.073.. Train L1 norm: 31.509.. Val L1 norm: 2.203.. Train Linf norm: 13675.259.. Val Linf norm: 367.362\n",
            "Epoch 23/141.. Train loss: 34807.951.. Val loss: 4216.303.. Train L1 norm: 28.270.. Val L1 norm: 5.045.. Train Linf norm: 11163.290.. Val Linf norm: 1083.822\n",
            "Epoch 24/141.. Train loss: 235704.960.. Val loss: 158158.828.. Train L1 norm: 1227.101.. Val L1 norm: 99.313.. Train Linf norm: 621724.578.. Val Linf norm: 19653.779\n",
            "Epoch 25/141.. Train loss: 533205750.066.. Val loss: 447.753.. Train L1 norm: 63.771.. Val L1 norm: 3.624.. Train Linf norm: 30928.122.. Val Linf norm: 725.092\n",
            "Epoch 26/141.. Train loss: 211319.221.. Val loss: 712.024.. Train L1 norm: 42.842.. Val L1 norm: 10.056.. Train Linf norm: 19122.499.. Val Linf norm: 1882.443\n",
            "Epoch 27/141.. Train loss: 3494154.780.. Val loss: 100.829.. Train L1 norm: 16.313.. Val L1 norm: 1.410.. Train Linf norm: 6071.673.. Val Linf norm: 105.556\n",
            "Epoch 28/141.. Train loss: 107954.852.. Val loss: 71.285.. Train L1 norm: 4.306.. Val L1 norm: 2.327.. Train Linf norm: 1557.066.. Val Linf norm: 416.062\n",
            "Epoch 29/141.. Train loss: 1724418.686.. Val loss: 143.395.. Train L1 norm: 76.708.. Val L1 norm: 1.608.. Train Linf norm: 37970.506.. Val Linf norm: 224.751\n",
            "Epoch 30/141.. Train loss: 12958.509.. Val loss: 136.511.. Train L1 norm: 13.821.. Val L1 norm: 2.083.. Train Linf norm: 6455.826.. Val Linf norm: 358.597\n",
            "Epoch 31/141.. Train loss: 7099.352.. Val loss: 136.868.. Train L1 norm: 18.219.. Val L1 norm: 1.793.. Train Linf norm: 8598.983.. Val Linf norm: 280.034\n",
            "Epoch 32/141.. Train loss: 1412.474.. Val loss: 134.485.. Train L1 norm: 21.757.. Val L1 norm: 2.014.. Train Linf norm: 10430.541.. Val Linf norm: 339.854\n",
            "Epoch 33/141.. Train loss: 1608.331.. Val loss: 136.583.. Train L1 norm: 20.232.. Val L1 norm: 1.756.. Train Linf norm: 9693.634.. Val Linf norm: 269.930\n",
            "Epoch 34/141.. Train loss: 1498.646.. Val loss: 139.405.. Train L1 norm: 14.749.. Val L1 norm: 1.566.. Train Linf norm: 6935.200.. Val Linf norm: 211.146\n",
            "Epoch 35/141.. Train loss: 2166.020.. Val loss: 136.508.. Train L1 norm: 13.405.. Val L1 norm: 1.768.. Train Linf norm: 6245.715.. Val Linf norm: 273.551\n",
            "Epoch 36/141.. Train loss: 100.168.. Val loss: 135.742.. Train L1 norm: 17.495.. Val L1 norm: 1.862.. Train Linf norm: 8289.624.. Val Linf norm: 299.863\n",
            "Epoch 37/141.. Train loss: 225.631.. Val loss: 136.136.. Train L1 norm: 15.496.. Val L1 norm: 1.802.. Train Linf norm: 7264.394.. Val Linf norm: 283.583\n",
            "Epoch 38/141.. Train loss: 2763.496.. Val loss: 134.207.. Train L1 norm: 18.698.. Val L1 norm: 2.094.. Train Linf norm: 8895.358.. Val Linf norm: 361.875\n",
            "Epoch 39/141.. Train loss: 5192.814.. Val loss: 137.190.. Train L1 norm: 22.518.. Val L1 norm: 1.771.. Train Linf norm: 10811.587.. Val Linf norm: 274.464\n",
            "Epoch 40/141.. Train loss: 3597.204.. Val loss: 136.399.. Train L1 norm: 14.815.. Val L1 norm: 1.817.. Train Linf norm: 6944.460.. Val Linf norm: 287.461\n",
            "Epoch 41/141.. Train loss: 992.770.. Val loss: 135.824.. Train L1 norm: 20.839.. Val L1 norm: 1.857.. Train Linf norm: 10000.945.. Val Linf norm: 298.333\n",
            "Epoch 42/141.. Train loss: 1950.375.. Val loss: 137.310.. Train L1 norm: 18.315.. Val L1 norm: 1.698.. Train Linf norm: 8725.610.. Val Linf norm: 253.841\n",
            "Epoch 43/141.. Train loss: 1964.828.. Val loss: 135.412.. Train L1 norm: 19.682.. Val L1 norm: 1.855.. Train Linf norm: 9428.496.. Val Linf norm: 298.135\n",
            "Epoch 44/141.. Train loss: 329.510.. Val loss: 134.740.. Train L1 norm: 11.120.. Val L1 norm: 1.915.. Train Linf norm: 5021.694.. Val Linf norm: 314.493\n",
            "Epoch 45/141.. Train loss: 4056.968.. Val loss: 136.012.. Train L1 norm: 14.501.. Val L1 norm: 1.741.. Train Linf norm: 6753.466.. Val Linf norm: 266.393\n",
            "Epoch 46/141.. Train loss: 186.928.. Val loss: 135.820.. Train L1 norm: 12.699.. Val L1 norm: 1.766.. Train Linf norm: 5858.761.. Val Linf norm: 273.368\n",
            "Epoch 47/141.. Train loss: 3863.327.. Val loss: 136.953.. Train L1 norm: 9.899.. Val L1 norm: 1.645.. Train Linf norm: 4442.400.. Val Linf norm: 237.168\n",
            "Epoch 48/141.. Train loss: 16791.210.. Val loss: 133.669.. Train L1 norm: 15.363.. Val L1 norm: 1.846.. Train Linf norm: 7241.591.. Val Linf norm: 295.155\n",
            "Epoch 49/141.. Train loss: 665.568.. Val loss: 132.805.. Train L1 norm: 19.093.. Val L1 norm: 1.902.. Train Linf norm: 9101.898.. Val Linf norm: 310.545\n",
            "Epoch 50/141.. Train loss: 2208.847.. Val loss: 133.395.. Train L1 norm: 19.460.. Val L1 norm: 1.821.. Train Linf norm: 9263.706.. Val Linf norm: 288.263\n",
            "Epoch 51/141.. Train loss: 616.230.. Val loss: 133.747.. Train L1 norm: 15.596.. Val L1 norm: 1.785.. Train Linf norm: 7329.304.. Val Linf norm: 278.434\n",
            "Epoch 52/141.. Train loss: 11848.509.. Val loss: 133.102.. Train L1 norm: 16.225.. Val L1 norm: 1.910.. Train Linf norm: 7653.644.. Val Linf norm: 312.640\n",
            "Epoch 53/141.. Train loss: 269.769.. Val loss: 133.141.. Train L1 norm: 20.884.. Val L1 norm: 1.914.. Train Linf norm: 10009.115.. Val Linf norm: 313.783\n",
            "Epoch 54/141.. Train loss: 662.542.. Val loss: 132.880.. Train L1 norm: 16.402.. Val L1 norm: 1.944.. Train Linf norm: 7714.523.. Val Linf norm: 322.177\n",
            "Epoch 55/141.. Train loss: 937.013.. Val loss: 133.093.. Train L1 norm: 14.771.. Val L1 norm: 1.917.. Train Linf norm: 6871.145.. Val Linf norm: 314.968\n",
            "Epoch 56/141.. Train loss: 2609.727.. Val loss: 134.264.. Train L1 norm: 13.492.. Val L1 norm: 1.797.. Train Linf norm: 6251.253.. Val Linf norm: 281.794\n",
            "Epoch 57/141.. Train loss: 568.493.. Val loss: 133.727.. Train L1 norm: 16.481.. Val L1 norm: 1.853.. Train Linf norm: 7776.548.. Val Linf norm: 297.480\n",
            "Epoch 58/141.. Train loss: 63.801.. Val loss: 133.660.. Train L1 norm: 12.745.. Val L1 norm: 1.856.. Train Linf norm: 5859.943.. Val Linf norm: 298.491\n",
            "Epoch 59/141.. Train loss: 525.717.. Val loss: 132.985.. Train L1 norm: 14.963.. Val L1 norm: 1.931.. Train Linf norm: 6985.699.. Val Linf norm: 319.233\n",
            "Epoch 60/141.. Train loss: 1198.808.. Val loss: 132.278.. Train L1 norm: 22.231.. Val L1 norm: 2.093.. Train Linf norm: 10682.185.. Val Linf norm: 362.653\n",
            "Epoch 61/141.. Train loss: 75.938.. Val loss: 132.015.. Train L1 norm: 17.650.. Val L1 norm: 2.100.. Train Linf norm: 8314.227.. Val Linf norm: 364.690\n",
            "Epoch 62/141.. Train loss: 656.911.. Val loss: 132.930.. Train L1 norm: 18.350.. Val L1 norm: 1.962.. Train Linf norm: 8687.779.. Val Linf norm: 328.280\n",
            "Epoch 63/141.. Train loss: 1387.648.. Val loss: 131.043.. Train L1 norm: 17.727.. Val L1 norm: 2.242.. Train Linf norm: 8350.907.. Val Linf norm: 402.314\n",
            "Epoch 64/141.. Train loss: 12698.116.. Val loss: 132.124.. Train L1 norm: 22.748.. Val L1 norm: 1.654.. Train Linf norm: 10891.577.. Val Linf norm: 240.635\n",
            "Epoch 65/141.. Train loss: 32876.973.. Val loss: 124.589.. Train L1 norm: 7.243.. Val L1 norm: 2.468.. Train Linf norm: 2944.758.. Val Linf norm: 459.760\n",
            "Epoch 66/141.. Train loss: 31144.154.. Val loss: 120.357.. Train L1 norm: 24.154.. Val L1 norm: 2.081.. Train Linf norm: 11507.074.. Val Linf norm: 358.314\n",
            "Epoch 67/141.. Train loss: 25873.732.. Val loss: 104.437.. Train L1 norm: 6.680.. Val L1 norm: 1.956.. Train Linf norm: 2806.427.. Val Linf norm: 322.776\n",
            "Epoch 68/141.. Train loss: 594.887.. Val loss: 105.806.. Train L1 norm: 19.196.. Val L1 norm: 1.918.. Train Linf norm: 9132.405.. Val Linf norm: 313.176\n",
            "Epoch 69/141.. Train loss: 465.923.. Val loss: 106.552.. Train L1 norm: 14.148.. Val L1 norm: 1.845.. Train Linf norm: 6576.537.. Val Linf norm: 293.155\n",
            "Epoch 70/141.. Train loss: 321.371.. Val loss: 105.701.. Train L1 norm: 16.220.. Val L1 norm: 1.931.. Train Linf norm: 7631.391.. Val Linf norm: 318.058\n",
            "Epoch 71/141.. Train loss: 497.219.. Val loss: 105.602.. Train L1 norm: 13.609.. Val L1 norm: 1.831.. Train Linf norm: 6297.349.. Val Linf norm: 290.303\n",
            "Epoch 72/141.. Train loss: 221.942.. Val loss: 104.156.. Train L1 norm: 15.025.. Val L1 norm: 1.922.. Train Linf norm: 7013.099.. Val Linf norm: 316.813\n",
            "Epoch 73/141.. Train loss: 57.819.. Val loss: 103.549.. Train L1 norm: 13.796.. Val L1 norm: 1.962.. Train Linf norm: 6378.143.. Val Linf norm: 328.441\n",
            "Epoch 74/141.. Train loss: 63.745.. Val loss: 103.035.. Train L1 norm: 16.714.. Val L1 norm: 1.973.. Train Linf norm: 7846.060.. Val Linf norm: 332.456\n",
            "Epoch 75/141.. Train loss: 299.065.. Val loss: 101.365.. Train L1 norm: 18.281.. Val L1 norm: 2.122.. Train Linf norm: 8650.531.. Val Linf norm: 373.833\n",
            "Epoch 76/141.. Train loss: 996.726.. Val loss: 105.256.. Train L1 norm: 14.625.. Val L1 norm: 1.885.. Train Linf norm: 6796.492.. Val Linf norm: 309.550\n",
            "Epoch 77/141.. Train loss: 619.157.. Val loss: 102.868.. Train L1 norm: 15.783.. Val L1 norm: 2.116.. Train Linf norm: 7368.895.. Val Linf norm: 374.413\n",
            "Epoch 78/141.. Train loss: 76.809.. Val loss: 102.459.. Train L1 norm: 17.142.. Val L1 norm: 2.110.. Train Linf norm: 8047.747.. Val Linf norm: 373.659\n",
            "Epoch 79/141.. Train loss: 147.822.. Val loss: 101.155.. Train L1 norm: 17.452.. Val L1 norm: 2.249.. Train Linf norm: 8190.823.. Val Linf norm: 412.556\n",
            "Epoch 80/141.. Train loss: 476.279.. Val loss: 100.217.. Train L1 norm: 16.502.. Val L1 norm: 2.077.. Train Linf norm: 7723.249.. Val Linf norm: 366.816\n",
            "Epoch 81/141.. Train loss: 2445.325.. Val loss: 96.318.. Train L1 norm: 16.810.. Val L1 norm: 2.623.. Train Linf norm: 7855.432.. Val Linf norm: 513.027\n",
            "Epoch 82/141.. Train loss: 4050.700.. Val loss: 103.583.. Train L1 norm: 15.087.. Val L1 norm: 2.030.. Train Linf norm: 6978.740.. Val Linf norm: 355.431\n",
            "Epoch 83/141.. Train loss: 3284.253.. Val loss: 95.123.. Train L1 norm: 25.725.. Val L1 norm: 2.582.. Train Linf norm: 12390.631.. Val Linf norm: 504.226\n",
            "Epoch 84/141.. Train loss: 4382.793.. Val loss: 95.364.. Train L1 norm: 27.294.. Val L1 norm: 2.055.. Train Linf norm: 13149.874.. Val Linf norm: 363.865\n",
            "Epoch 85/141.. Train loss: 5832.556.. Val loss: 91.533.. Train L1 norm: 14.600.. Val L1 norm: 2.562.. Train Linf norm: 6744.565.. Val Linf norm: 501.854\n",
            "Epoch 86/141.. Train loss: 5145.162.. Val loss: 93.102.. Train L1 norm: 16.114.. Val L1 norm: 2.101.. Train Linf norm: 7437.220.. Val Linf norm: 376.456\n",
            "Epoch 87/141.. Train loss: 3092.089.. Val loss: 83.374.. Train L1 norm: 20.197.. Val L1 norm: 2.415.. Train Linf norm: 9559.574.. Val Linf norm: 462.742\n",
            "Epoch 88/141.. Train loss: 365.529.. Val loss: 86.602.. Train L1 norm: 23.258.. Val L1 norm: 2.354.. Train Linf norm: 11129.580.. Val Linf norm: 447.809\n",
            "Epoch 89/141.. Train loss: 281.631.. Val loss: 82.887.. Train L1 norm: 20.817.. Val L1 norm: 2.472.. Train Linf norm: 9859.624.. Val Linf norm: 480.400\n",
            "Epoch 90/141.. Train loss: 63.409.. Val loss: 82.364.. Train L1 norm: 22.547.. Val L1 norm: 2.510.. Train Linf norm: 10741.665.. Val Linf norm: 491.363\n",
            "Epoch 91/141.. Train loss: 73.579.. Val loss: 82.066.. Train L1 norm: 21.113.. Val L1 norm: 2.531.. Train Linf norm: 9994.951.. Val Linf norm: 497.681\n",
            "Epoch 92/141.. Train loss: 163.799.. Val loss: 79.253.. Train L1 norm: 22.350.. Val L1 norm: 2.620.. Train Linf norm: 10611.000.. Val Linf norm: 522.100\n",
            "Epoch 93/141.. Train loss: 392.138.. Val loss: 82.825.. Train L1 norm: 22.507.. Val L1 norm: 2.572.. Train Linf norm: 10687.423.. Val Linf norm: 510.471\n",
            "Epoch 94/141.. Train loss: 101.516.. Val loss: 81.184.. Train L1 norm: 23.394.. Val L1 norm: 2.632.. Train Linf norm: 11134.700.. Val Linf norm: 527.131\n",
            "Epoch 95/141.. Train loss: 359.100.. Val loss: 83.073.. Train L1 norm: 22.594.. Val L1 norm: 2.557.. Train Linf norm: 10728.560.. Val Linf norm: 507.599\n",
            "Epoch 96/141.. Train loss: 172.164.. Val loss: 81.655.. Train L1 norm: 22.670.. Val L1 norm: 2.620.. Train Linf norm: 10770.343.. Val Linf norm: 524.964\n",
            "Epoch 97/141.. Train loss: 106.041.. Val loss: 80.953.. Train L1 norm: 23.619.. Val L1 norm: 2.668.. Train Linf norm: 11249.982.. Val Linf norm: 538.031\n",
            "Epoch 98/141.. Train loss: 55.600.. Val loss: 80.766.. Train L1 norm: 25.529.. Val L1 norm: 2.683.. Train Linf norm: 12214.129.. Val Linf norm: 542.584\n",
            "Epoch 99/141.. Train loss: 67.926.. Val loss: 80.133.. Train L1 norm: 24.837.. Val L1 norm: 2.711.. Train Linf norm: 11868.782.. Val Linf norm: 550.439\n",
            "Epoch 100/141.. Train loss: 54.459.. Val loss: 79.844.. Train L1 norm: 24.475.. Val L1 norm: 2.728.. Train Linf norm: 11646.537.. Val Linf norm: 555.359\n",
            "Epoch 101/141.. Train loss: 169.873.. Val loss: 80.679.. Train L1 norm: 25.479.. Val L1 norm: 2.712.. Train Linf norm: 12179.154.. Val Linf norm: 551.605\n",
            "Epoch 102/141.. Train loss: 112.665.. Val loss: 79.798.. Train L1 norm: 24.219.. Val L1 norm: 2.743.. Train Linf norm: 11526.541.. Val Linf norm: 559.987\n",
            "Epoch 103/141.. Train loss: 53.865.. Val loss: 79.522.. Train L1 norm: 24.868.. Val L1 norm: 2.756.. Train Linf norm: 11864.288.. Val Linf norm: 563.795\n",
            "Epoch 104/141.. Train loss: 74.055.. Val loss: 79.601.. Train L1 norm: 25.252.. Val L1 norm: 2.758.. Train Linf norm: 12041.486.. Val Linf norm: 564.701\n",
            "Epoch 105/141.. Train loss: 69.630.. Val loss: 79.132.. Train L1 norm: 25.361.. Val L1 norm: 2.777.. Train Linf norm: 12112.085.. Val Linf norm: 570.053\n",
            "Epoch 106/141.. Train loss: 158.387.. Val loss: 78.549.. Train L1 norm: 25.601.. Val L1 norm: 2.798.. Train Linf norm: 12243.268.. Val Linf norm: 575.845\n",
            "Epoch 107/141.. Train loss: 320.786.. Val loss: 78.912.. Train L1 norm: 25.278.. Val L1 norm: 2.789.. Train Linf norm: 12053.539.. Val Linf norm: 573.592\n",
            "Epoch 108/141.. Train loss: 85.317.. Val loss: 78.535.. Train L1 norm: 25.560.. Val L1 norm: 2.810.. Train Linf norm: 12202.593.. Val Linf norm: 579.496\n",
            "Epoch 109/141.. Train loss: 56.916.. Val loss: 78.491.. Train L1 norm: 25.422.. Val L1 norm: 2.820.. Train Linf norm: 12135.535.. Val Linf norm: 582.553\n",
            "Epoch 110/141.. Train loss: 144.260.. Val loss: 76.822.. Train L1 norm: 26.678.. Val L1 norm: 2.881.. Train Linf norm: 12765.074.. Val Linf norm: 598.686\n",
            "Epoch 111/141.. Train loss: 177.684.. Val loss: 79.012.. Train L1 norm: 26.031.. Val L1 norm: 2.832.. Train Linf norm: 12416.649.. Val Linf norm: 586.638\n",
            "Epoch 112/141.. Train loss: 195.873.. Val loss: 75.951.. Train L1 norm: 26.735.. Val L1 norm: 2.926.. Train Linf norm: 12791.739.. Val Linf norm: 611.668\n",
            "Epoch 113/141.. Train loss: 65.933.. Val loss: 75.942.. Train L1 norm: 26.422.. Val L1 norm: 2.937.. Train Linf norm: 12603.646.. Val Linf norm: 615.608\n",
            "Epoch 114/141.. Train loss: 118.448.. Val loss: 79.381.. Train L1 norm: 26.173.. Val L1 norm: 2.890.. Train Linf norm: 12491.565.. Val Linf norm: 604.858\n",
            "Epoch 115/141.. Train loss: 459.712.. Val loss: 71.491.. Train L1 norm: 28.753.. Val L1 norm: 3.129.. Train Linf norm: 13776.971.. Val Linf norm: 667.087\n",
            "Epoch 116/141.. Train loss: 1704.072.. Val loss: 73.367.. Train L1 norm: 31.811.. Val L1 norm: 3.006.. Train Linf norm: 15312.309.. Val Linf norm: 637.366\n",
            "Epoch 117/141.. Train loss: 6539.393.. Val loss: 77.875.. Train L1 norm: 41.899.. Val L1 norm: 3.855.. Train Linf norm: 20394.351.. Val Linf norm: 855.581\n",
            "Epoch 118/141.. Train loss: 14247.055.. Val loss: 107.587.. Train L1 norm: 22.032.. Val L1 norm: 2.754.. Train Linf norm: 10336.540.. Val Linf norm: 555.483\n",
            "Epoch 119/141.. Train loss: 103.606.. Val loss: 108.860.. Train L1 norm: 12.610.. Val L1 norm: 2.428.. Train Linf norm: 5634.937.. Val Linf norm: 477.378\n",
            "Epoch 120/141.. Train loss: 35162.537.. Val loss: 385.555.. Train L1 norm: 41.395.. Val L1 norm: 3.509.. Train Linf norm: 14525.010.. Val Linf norm: 753.888\n",
            "Epoch 121/141.. Train loss: 65309.065.. Val loss: 175.033.. Train L1 norm: 21.774.. Val L1 norm: 2.923.. Train Linf norm: 8233.746.. Val Linf norm: 386.178\n",
            "Epoch 122/141.. Train loss: 1071398.123.. Val loss: 4361.440.. Train L1 norm: 34.266.. Val L1 norm: 2.915.. Train Linf norm: 16362.243.. Val Linf norm: 577.413\n",
            "Epoch 123/141.. Train loss: 90504.710.. Val loss: 27690.732.. Train L1 norm: 25.839.. Val L1 norm: 77.710.. Train Linf norm: 11663.637.. Val Linf norm: 15427.983\n",
            "Epoch 124/141.. Train loss: 166020021.361.. Val loss: 257.895.. Train L1 norm: 59.972.. Val L1 norm: 3.910.. Train Linf norm: 18622.655.. Val Linf norm: 808.003\n",
            "Epoch 125/141.. Train loss: 215576.661.. Val loss: 227.132.. Train L1 norm: 32.794.. Val L1 norm: 2.881.. Train Linf norm: 15653.540.. Val Linf norm: 360.199\n",
            "Epoch 126/141.. Train loss: 784933.404.. Val loss: 72.992.. Train L1 norm: 25.969.. Val L1 norm: 2.563.. Train Linf norm: 12312.190.. Val Linf norm: 495.422\n",
            "Epoch 127/141.. Train loss: 9727.356.. Val loss: 91.733.. Train L1 norm: 5.462.. Val L1 norm: 1.915.. Train Linf norm: 2055.504.. Val Linf norm: 320.674\n",
            "Epoch 128/141.. Train loss: 1147.026.. Val loss: 98.095.. Train L1 norm: 3.046.. Val L1 norm: 1.808.. Train Linf norm: 880.842.. Val Linf norm: 286.164\n",
            "Epoch 129/141.. Train loss: 3209.084.. Val loss: 83.395.. Train L1 norm: 6.101.. Val L1 norm: 2.123.. Train Linf norm: 2421.401.. Val Linf norm: 378.355\n",
            "Epoch 130/141.. Train loss: 4676.468.. Val loss: 98.991.. Train L1 norm: 8.992.. Val L1 norm: 1.814.. Train Linf norm: 3896.030.. Val Linf norm: 288.408\n",
            "Epoch 131/141.. Train loss: 9813.392.. Val loss: 76.410.. Train L1 norm: 5.641.. Val L1 norm: 2.298.. Train Linf norm: 2211.904.. Val Linf norm: 426.909\n",
            "Epoch 132/141.. Train loss: 7657.434.. Val loss: 92.353.. Train L1 norm: 9.134.. Val L1 norm: 1.942.. Train Linf norm: 3910.391.. Val Linf norm: 329.157\n",
            "Epoch 133/141.. Train loss: 1691.874.. Val loss: 83.717.. Train L1 norm: 13.752.. Val L1 norm: 2.156.. Train Linf norm: 6324.156.. Val Linf norm: 388.564\n",
            "Epoch 134/141.. Train loss: 627.817.. Val loss: 87.475.. Train L1 norm: 7.293.. Val L1 norm: 2.028.. Train Linf norm: 2999.122.. Val Linf norm: 354.342\n",
            "Epoch 135/141.. Train loss: 1229.638.. Val loss: 78.398.. Train L1 norm: 5.655.. Val L1 norm: 2.268.. Train Linf norm: 2174.984.. Val Linf norm: 419.440\n",
            "Epoch 136/141.. Train loss: 1684.318.. Val loss: 85.192.. Train L1 norm: 15.962.. Val L1 norm: 2.085.. Train Linf norm: 7405.237.. Val Linf norm: 370.000\n",
            "Epoch 137/141.. Train loss: 1122.082.. Val loss: 80.481.. Train L1 norm: 8.736.. Val L1 norm: 2.232.. Train Linf norm: 3734.779.. Val Linf norm: 409.457\n",
            "Epoch 138/141.. Train loss: 1616.251.. Val loss: 87.400.. Train L1 norm: 14.692.. Val L1 norm: 2.040.. Train Linf norm: 6801.693.. Val Linf norm: 357.977\n",
            "Epoch 139/141.. Train loss: 911.802.. Val loss: 80.478.. Train L1 norm: 10.641.. Val L1 norm: 2.243.. Train Linf norm: 4710.236.. Val Linf norm: 413.324\n",
            "Epoch 140/141.. Train loss: 196.950.. Val loss: 82.855.. Train L1 norm: 8.419.. Val L1 norm: 2.182.. Train Linf norm: 3564.500.. Val Linf norm: 396.340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:27:56,375]\u001b[0m Trial 158 finished with value: 2.091923948160807 and parameters: {'n_layers': 8, 'n_units_0': 3415, 'n_units_1': 211, 'n_units_2': 3856, 'n_units_3': 1017, 'n_units_4': 1591, 'n_units_5': 1778, 'n_units_6': 922, 'n_units_7': 3092, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 5.37951601750477e-06, 'batch_size': 512, 'n_epochs': 141, 'scheduler': 'CosineAnnealingLR', 'prelu_init': 0.21151191251412807, 'dropout_rate': 0.0588332288646134, 'weight_decay': 0.0015602865417327224, 'beta1': 0.9273050771553845, 'beta2': 0.9991453784023004, 't_max_fraction': 0.18869812360782112, 'eta_min': 4.098589600523511e-05}. Best is trial 92 with value: 1.0098264760335287.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 141/141.. Train loss: 424.417.. Val loss: 86.321.. Train L1 norm: 10.160.. Val L1 norm: 2.092.. Train Linf norm: 4473.085.. Val Linf norm: 372.314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:27:58,772]\u001b[0m Trial 159 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/137.. Train loss: 401.046.. Val loss: 47.953.. Train L1 norm: 17.561.. Val L1 norm: 2.318.. Train Linf norm: 16439.694.. Val Linf norm: 763.752\n",
            "Epoch 1/129.. Train loss: 234.228.. Val loss: 55.702.. Train L1 norm: 5.590.. Val L1 norm: 1.020.. Train Linf norm: 9334.277.. Val Linf norm: 29.344\n",
            "Epoch 2/129.. Train loss: 318.372.. Val loss: 52.850.. Train L1 norm: 2.081.. Val L1 norm: 1.123.. Train Linf norm: 2168.429.. Val Linf norm: 149.198\n",
            "Epoch 3/129.. Train loss: 1214.594.. Val loss: 55.178.. Train L1 norm: 3.972.. Val L1 norm: 1.031.. Train Linf norm: 5970.151.. Val Linf norm: 51.377\n",
            "Epoch 4/129.. Train loss: 367.076.. Val loss: 53.789.. Train L1 norm: 1.715.. Val L1 norm: 1.087.. Train Linf norm: 1424.153.. Val Linf norm: 114.744\n",
            "Epoch 5/129.. Train loss: 224.969.. Val loss: 55.655.. Train L1 norm: 1.199.. Val L1 norm: 1.029.. Train Linf norm: 371.926.. Val Linf norm: 47.322\n",
            "Epoch 6/129.. Train loss: 370.105.. Val loss: 53.592.. Train L1 norm: 2.495.. Val L1 norm: 1.107.. Train Linf norm: 3028.795.. Val Linf norm: 138.972\n",
            "Epoch 7/129.. Train loss: 153.820.. Val loss: 54.558.. Train L1 norm: 2.450.. Val L1 norm: 1.071.. Train Linf norm: 2914.985.. Val Linf norm: 103.913\n",
            "Epoch 8/129.. Train loss: 62.561.. Val loss: 54.960.. Train L1 norm: 1.637.. Val L1 norm: 1.061.. Train Linf norm: 1267.325.. Val Linf norm: 95.472\n",
            "Epoch 9/129.. Train loss: 124.699.. Val loss: 54.379.. Train L1 norm: 2.850.. Val L1 norm: 1.087.. Train Linf norm: 3749.162.. Val Linf norm: 123.138\n",
            "Epoch 10/129.. Train loss: 66.249.. Val loss: 53.762.. Train L1 norm: 2.672.. Val L1 norm: 1.116.. Train Linf norm: 3343.605.. Val Linf norm: 152.675\n",
            "Epoch 11/129.. Train loss: 84.669.. Val loss: 54.767.. Train L1 norm: 2.722.. Val L1 norm: 1.080.. Train Linf norm: 3459.313.. Val Linf norm: 119.078\n",
            "Epoch 12/129.. Train loss: 329.871.. Val loss: 54.391.. Train L1 norm: 3.458.. Val L1 norm: 1.095.. Train Linf norm: 4971.715.. Val Linf norm: 133.991\n",
            "Epoch 13/129.. Train loss: 59.986.. Val loss: 54.342.. Train L1 norm: 3.783.. Val L1 norm: 1.097.. Train Linf norm: 5638.737.. Val Linf norm: 136.663\n",
            "Epoch 14/129.. Train loss: 69.164.. Val loss: 54.248.. Train L1 norm: 3.062.. Val L1 norm: 1.102.. Train Linf norm: 4157.043.. Val Linf norm: 141.208\n",
            "Epoch 15/129.. Train loss: 93.525.. Val loss: 54.086.. Train L1 norm: 3.770.. Val L1 norm: 1.109.. Train Linf norm: 5598.235.. Val Linf norm: 148.946\n",
            "Epoch 16/129.. Train loss: 61.904.. Val loss: 54.026.. Train L1 norm: 2.714.. Val L1 norm: 1.112.. Train Linf norm: 3427.600.. Val Linf norm: 152.344\n",
            "Epoch 17/129.. Train loss: 62.844.. Val loss: 53.997.. Train L1 norm: 4.279.. Val L1 norm: 1.114.. Train Linf norm: 6627.331.. Val Linf norm: 154.376\n",
            "Epoch 18/129.. Train loss: 61.796.. Val loss: 53.971.. Train L1 norm: 2.739.. Val L1 norm: 1.116.. Train Linf norm: 3457.981.. Val Linf norm: 156.373\n",
            "Epoch 19/129.. Train loss: 59.641.. Val loss: 53.972.. Train L1 norm: 3.603.. Val L1 norm: 1.117.. Train Linf norm: 5242.720.. Val Linf norm: 157.121\n",
            "Epoch 20/129.. Train loss: 62.433.. Val loss: 53.933.. Train L1 norm: 3.030.. Val L1 norm: 1.119.. Train Linf norm: 4069.655.. Val Linf norm: 159.618\n",
            "Epoch 21/129.. Train loss: 63.501.. Val loss: 53.935.. Train L1 norm: 4.147.. Val L1 norm: 1.119.. Train Linf norm: 6363.881.. Val Linf norm: 159.658\n",
            "Epoch 22/129.. Train loss: 60.628.. Val loss: 53.940.. Train L1 norm: 3.456.. Val L1 norm: 1.119.. Train Linf norm: 4944.476.. Val Linf norm: 159.546\n",
            "Epoch 23/129.. Train loss: 106.060.. Val loss: 53.946.. Train L1 norm: 4.218.. Val L1 norm: 1.119.. Train Linf norm: 6500.413.. Val Linf norm: 159.412\n",
            "Epoch 24/129.. Train loss: 67.100.. Val loss: 53.972.. Train L1 norm: 3.533.. Val L1 norm: 1.118.. Train Linf norm: 5102.567.. Val Linf norm: 158.381\n",
            "Epoch 25/129.. Train loss: 84.312.. Val loss: 53.961.. Train L1 norm: 3.358.. Val L1 norm: 1.118.. Train Linf norm: 4748.123.. Val Linf norm: 158.884\n",
            "Epoch 26/129.. Train loss: 61.292.. Val loss: 53.959.. Train L1 norm: 3.194.. Val L1 norm: 1.118.. Train Linf norm: 4405.888.. Val Linf norm: 159.026\n",
            "Epoch 27/129.. Train loss: 64.319.. Val loss: 53.951.. Train L1 norm: 3.673.. Val L1 norm: 1.119.. Train Linf norm: 5385.929.. Val Linf norm: 159.442\n",
            "Epoch 28/129.. Train loss: 96.549.. Val loss: 53.976.. Train L1 norm: 3.807.. Val L1 norm: 1.118.. Train Linf norm: 5666.057.. Val Linf norm: 158.584\n",
            "Epoch 29/129.. Train loss: 74.942.. Val loss: 53.967.. Train L1 norm: 3.066.. Val L1 norm: 1.118.. Train Linf norm: 4147.904.. Val Linf norm: 159.018\n",
            "Epoch 30/129.. Train loss: 59.428.. Val loss: 53.966.. Train L1 norm: 4.041.. Val L1 norm: 1.118.. Train Linf norm: 6145.118.. Val Linf norm: 159.078\n",
            "Epoch 31/129.. Train loss: 60.077.. Val loss: 53.966.. Train L1 norm: 3.053.. Val L1 norm: 1.118.. Train Linf norm: 4113.589.. Val Linf norm: 159.091\n",
            "Epoch 32/129.. Train loss: 61.004.. Val loss: 53.966.. Train L1 norm: 3.200.. Val L1 norm: 1.118.. Train Linf norm: 4405.139.. Val Linf norm: 159.112\n",
            "Epoch 33/129.. Train loss: 125.686.. Val loss: 53.963.. Train L1 norm: 3.973.. Val L1 norm: 1.118.. Train Linf norm: 6003.520.. Val Linf norm: 159.243\n",
            "Epoch 34/129.. Train loss: 66.383.. Val loss: 53.960.. Train L1 norm: 3.856.. Val L1 norm: 1.118.. Train Linf norm: 5747.534.. Val Linf norm: 159.365\n",
            "Epoch 35/129.. Train loss: 59.526.. Val loss: 53.959.. Train L1 norm: 3.881.. Val L1 norm: 1.119.. Train Linf norm: 5814.927.. Val Linf norm: 159.439\n",
            "Epoch 36/129.. Train loss: 60.718.. Val loss: 53.959.. Train L1 norm: 3.070.. Val L1 norm: 1.119.. Train Linf norm: 851.790.. Val Linf norm: 159.440\n",
            "Epoch 37/129.. Train loss: 66.523.. Val loss: 53.958.. Train L1 norm: 3.827.. Val L1 norm: 1.119.. Train Linf norm: 5705.694.. Val Linf norm: 159.498\n",
            "Epoch 38/129.. Train loss: 60.616.. Val loss: 53.957.. Train L1 norm: 4.072.. Val L1 norm: 1.119.. Train Linf norm: 6200.557.. Val Linf norm: 159.544\n",
            "Epoch 39/129.. Train loss: 76.239.. Val loss: 53.957.. Train L1 norm: 3.780.. Val L1 norm: 1.119.. Train Linf norm: 5612.694.. Val Linf norm: 159.539\n",
            "Epoch 40/129.. Train loss: 87.451.. Val loss: 53.957.. Train L1 norm: 3.081.. Val L1 norm: 1.119.. Train Linf norm: 4176.809.. Val Linf norm: 159.542\n",
            "Epoch 41/129.. Train loss: 61.160.. Val loss: 53.957.. Train L1 norm: 3.620.. Val L1 norm: 1.119.. Train Linf norm: 5285.756.. Val Linf norm: 159.551\n",
            "Epoch 42/129.. Train loss: 60.476.. Val loss: 53.957.. Train L1 norm: 3.866.. Val L1 norm: 1.119.. Train Linf norm: 5786.813.. Val Linf norm: 159.554\n",
            "Epoch 43/129.. Train loss: 66.493.. Val loss: 53.957.. Train L1 norm: 3.267.. Val L1 norm: 1.119.. Train Linf norm: 4559.505.. Val Linf norm: 159.552\n",
            "Epoch 44/129.. Train loss: 67.631.. Val loss: 53.957.. Train L1 norm: 3.742.. Val L1 norm: 1.119.. Train Linf norm: 5529.322.. Val Linf norm: 159.559\n",
            "Epoch 45/129.. Train loss: 60.148.. Val loss: 53.957.. Train L1 norm: 3.575.. Val L1 norm: 1.119.. Train Linf norm: 5189.535.. Val Linf norm: 159.562\n",
            "Epoch 46/129.. Train loss: 59.608.. Val loss: 53.957.. Train L1 norm: 3.355.. Val L1 norm: 1.119.. Train Linf norm: 4740.113.. Val Linf norm: 159.563\n",
            "Epoch 47/129.. Train loss: 63.022.. Val loss: 53.957.. Train L1 norm: 3.147.. Val L1 norm: 1.119.. Train Linf norm: 4309.153.. Val Linf norm: 159.562\n",
            "Epoch 48/129.. Train loss: 62.240.. Val loss: 53.957.. Train L1 norm: 3.024.. Val L1 norm: 1.119.. Train Linf norm: 4057.228.. Val Linf norm: 159.566\n",
            "Epoch 49/129.. Train loss: 62.134.. Val loss: 53.957.. Train L1 norm: 3.727.. Val L1 norm: 1.119.. Train Linf norm: 5420.204.. Val Linf norm: 159.571\n",
            "Epoch 50/129.. Train loss: 64.025.. Val loss: 53.957.. Train L1 norm: 3.449.. Val L1 norm: 1.119.. Train Linf norm: 4927.717.. Val Linf norm: 159.569\n",
            "Epoch 51/129.. Train loss: 72.806.. Val loss: 53.957.. Train L1 norm: 3.166.. Val L1 norm: 1.119.. Train Linf norm: 4354.435.. Val Linf norm: 159.579\n",
            "Epoch 52/129.. Train loss: 60.522.. Val loss: 53.957.. Train L1 norm: 3.950.. Val L1 norm: 1.119.. Train Linf norm: 5948.751.. Val Linf norm: 159.584\n",
            "Epoch 53/129.. Train loss: 61.803.. Val loss: 53.957.. Train L1 norm: 3.911.. Val L1 norm: 1.119.. Train Linf norm: 5880.334.. Val Linf norm: 159.584\n",
            "Epoch 54/129.. Train loss: 178.885.. Val loss: 53.956.. Train L1 norm: 4.074.. Val L1 norm: 1.119.. Train Linf norm: 6208.357.. Val Linf norm: 159.619\n",
            "Epoch 55/129.. Train loss: 59.447.. Val loss: 53.956.. Train L1 norm: 3.129.. Val L1 norm: 1.119.. Train Linf norm: 4275.171.. Val Linf norm: 159.627\n",
            "Epoch 56/129.. Train loss: 65.164.. Val loss: 53.956.. Train L1 norm: 3.750.. Val L1 norm: 1.119.. Train Linf norm: 5543.328.. Val Linf norm: 159.625\n",
            "Epoch 57/129.. Train loss: 64.343.. Val loss: 53.956.. Train L1 norm: 3.233.. Val L1 norm: 1.119.. Train Linf norm: 4484.436.. Val Linf norm: 159.630\n",
            "Epoch 58/129.. Train loss: 59.664.. Val loss: 53.956.. Train L1 norm: 3.035.. Val L1 norm: 1.119.. Train Linf norm: 4078.699.. Val Linf norm: 159.634\n",
            "Epoch 59/129.. Train loss: 94.971.. Val loss: 53.955.. Train L1 norm: 3.257.. Val L1 norm: 1.119.. Train Linf norm: 4535.161.. Val Linf norm: 159.658\n",
            "Epoch 60/129.. Train loss: 85.140.. Val loss: 53.955.. Train L1 norm: 3.469.. Val L1 norm: 1.119.. Train Linf norm: 4954.097.. Val Linf norm: 159.674\n",
            "Epoch 61/129.. Train loss: 80.154.. Val loss: 53.955.. Train L1 norm: 4.246.. Val L1 norm: 1.119.. Train Linf norm: 6555.826.. Val Linf norm: 159.669\n",
            "Epoch 62/129.. Train loss: 69.437.. Val loss: 53.955.. Train L1 norm: 2.661.. Val L1 norm: 1.119.. Train Linf norm: 3316.952.. Val Linf norm: 159.662\n",
            "Epoch 63/129.. Train loss: 59.800.. Val loss: 53.955.. Train L1 norm: 2.778.. Val L1 norm: 1.119.. Train Linf norm: 3555.795.. Val Linf norm: 159.665\n",
            "Epoch 64/129.. Train loss: 59.742.. Val loss: 53.955.. Train L1 norm: 3.272.. Val L1 norm: 1.119.. Train Linf norm: 4571.258.. Val Linf norm: 159.667\n",
            "Epoch 65/129.. Train loss: 76.270.. Val loss: 53.955.. Train L1 norm: 4.099.. Val L1 norm: 1.119.. Train Linf norm: 6257.732.. Val Linf norm: 159.663\n",
            "Epoch 66/129.. Train loss: 60.038.. Val loss: 53.956.. Train L1 norm: 3.411.. Val L1 norm: 1.119.. Train Linf norm: 4850.346.. Val Linf norm: 159.661\n",
            "Epoch 67/129.. Train loss: 129.395.. Val loss: 53.955.. Train L1 norm: 4.412.. Val L1 norm: 1.119.. Train Linf norm: 6902.159.. Val Linf norm: 159.684\n",
            "Epoch 68/129.. Train loss: 62.642.. Val loss: 53.955.. Train L1 norm: 4.188.. Val L1 norm: 1.119.. Train Linf norm: 6446.647.. Val Linf norm: 159.705\n",
            "Epoch 69/129.. Train loss: 197.132.. Val loss: 53.955.. Train L1 norm: 4.422.. Val L1 norm: 1.119.. Train Linf norm: 6921.174.. Val Linf norm: 159.695\n",
            "Epoch 70/129.. Train loss: 111.352.. Val loss: 53.956.. Train L1 norm: 3.344.. Val L1 norm: 1.119.. Train Linf norm: 4714.568.. Val Linf norm: 159.646\n",
            "Epoch 71/129.. Train loss: 69.335.. Val loss: 53.956.. Train L1 norm: 3.647.. Val L1 norm: 1.119.. Train Linf norm: 5336.110.. Val Linf norm: 159.652\n",
            "Epoch 72/129.. Train loss: 63.063.. Val loss: 53.956.. Train L1 norm: 3.798.. Val L1 norm: 1.119.. Train Linf norm: 5641.329.. Val Linf norm: 159.657\n",
            "Epoch 73/129.. Train loss: 62.861.. Val loss: 53.956.. Train L1 norm: 3.459.. Val L1 norm: 1.119.. Train Linf norm: 4899.518.. Val Linf norm: 159.665\n",
            "Epoch 74/129.. Train loss: 75.551.. Val loss: 53.956.. Train L1 norm: 3.678.. Val L1 norm: 1.119.. Train Linf norm: 5403.814.. Val Linf norm: 159.661\n",
            "Epoch 75/129.. Train loss: 105.352.. Val loss: 53.956.. Train L1 norm: 3.852.. Val L1 norm: 1.119.. Train Linf norm: 5757.363.. Val Linf norm: 159.678\n",
            "Epoch 76/129.. Train loss: 63.416.. Val loss: 53.955.. Train L1 norm: 3.830.. Val L1 norm: 1.119.. Train Linf norm: 5710.071.. Val Linf norm: 159.682\n",
            "Epoch 77/129.. Train loss: 156.995.. Val loss: 53.956.. Train L1 norm: 2.934.. Val L1 norm: 1.119.. Train Linf norm: 3874.275.. Val Linf norm: 159.648\n",
            "Epoch 78/129.. Train loss: 66.194.. Val loss: 53.956.. Train L1 norm: 3.914.. Val L1 norm: 1.119.. Train Linf norm: 5878.187.. Val Linf norm: 159.642\n",
            "Epoch 79/129.. Train loss: 145.482.. Val loss: 53.957.. Train L1 norm: 3.188.. Val L1 norm: 1.119.. Train Linf norm: 4397.521.. Val Linf norm: 159.618\n",
            "Epoch 80/129.. Train loss: 69.529.. Val loss: 53.957.. Train L1 norm: 3.377.. Val L1 norm: 1.119.. Train Linf norm: 4782.094.. Val Linf norm: 159.624\n",
            "Epoch 81/129.. Train loss: 61.996.. Val loss: 53.957.. Train L1 norm: 3.593.. Val L1 norm: 1.119.. Train Linf norm: 5226.743.. Val Linf norm: 159.630\n",
            "Epoch 82/129.. Train loss: 60.489.. Val loss: 53.957.. Train L1 norm: 3.098.. Val L1 norm: 1.119.. Train Linf norm: 4207.332.. Val Linf norm: 159.634\n",
            "Epoch 83/129.. Train loss: 68.445.. Val loss: 53.957.. Train L1 norm: 3.491.. Val L1 norm: 1.119.. Train Linf norm: 5013.922.. Val Linf norm: 159.630\n",
            "Epoch 84/129.. Train loss: 92.307.. Val loss: 53.957.. Train L1 norm: 3.001.. Val L1 norm: 1.119.. Train Linf norm: 3993.417.. Val Linf norm: 159.639\n",
            "Epoch 85/129.. Train loss: 59.488.. Val loss: 53.957.. Train L1 norm: 3.718.. Val L1 norm: 1.119.. Train Linf norm: 5481.245.. Val Linf norm: 159.650\n",
            "Epoch 86/129.. Train loss: 59.810.. Val loss: 53.957.. Train L1 norm: 3.732.. Val L1 norm: 1.119.. Train Linf norm: 5514.263.. Val Linf norm: 159.652\n",
            "Epoch 87/129.. Train loss: 68.104.. Val loss: 53.957.. Train L1 norm: 3.360.. Val L1 norm: 1.119.. Train Linf norm: 4744.986.. Val Linf norm: 159.652\n",
            "Epoch 88/129.. Train loss: 65.430.. Val loss: 53.957.. Train L1 norm: 3.409.. Val L1 norm: 1.119.. Train Linf norm: 4845.712.. Val Linf norm: 159.641\n",
            "Epoch 89/129.. Train loss: 60.476.. Val loss: 53.957.. Train L1 norm: 3.875.. Val L1 norm: 1.119.. Train Linf norm: 5809.292.. Val Linf norm: 159.642\n",
            "Epoch 90/129.. Train loss: 66.859.. Val loss: 53.957.. Train L1 norm: 3.816.. Val L1 norm: 1.119.. Train Linf norm: 5678.529.. Val Linf norm: 159.652\n",
            "Epoch 91/129.. Train loss: 83.325.. Val loss: 53.957.. Train L1 norm: 2.513.. Val L1 norm: 1.119.. Train Linf norm: 3006.781.. Val Linf norm: 159.638\n",
            "Epoch 92/129.. Train loss: 64.072.. Val loss: 53.957.. Train L1 norm: 3.990.. Val L1 norm: 1.119.. Train Linf norm: 6038.387.. Val Linf norm: 159.632\n",
            "Epoch 93/129.. Train loss: 101.873.. Val loss: 53.958.. Train L1 norm: 3.398.. Val L1 norm: 1.119.. Train Linf norm: 4819.102.. Val Linf norm: 159.615\n",
            "Epoch 94/129.. Train loss: 88.375.. Val loss: 53.957.. Train L1 norm: 3.298.. Val L1 norm: 1.119.. Train Linf norm: 4623.118.. Val Linf norm: 159.626\n",
            "Epoch 95/129.. Train loss: 60.364.. Val loss: 53.957.. Train L1 norm: 3.950.. Val L1 norm: 1.119.. Train Linf norm: 5956.936.. Val Linf norm: 159.630\n",
            "Epoch 96/129.. Train loss: 62.266.. Val loss: 53.957.. Train L1 norm: 3.251.. Val L1 norm: 1.119.. Train Linf norm: 4505.277.. Val Linf norm: 159.635\n",
            "Epoch 97/129.. Train loss: 90.654.. Val loss: 53.958.. Train L1 norm: 3.318.. Val L1 norm: 1.119.. Train Linf norm: 4659.110.. Val Linf norm: 159.622\n",
            "Epoch 98/129.. Train loss: 70.264.. Val loss: 53.958.. Train L1 norm: 3.810.. Val L1 norm: 1.119.. Train Linf norm: 5671.423.. Val Linf norm: 159.628\n",
            "Epoch 99/129.. Train loss: 86.707.. Val loss: 53.958.. Train L1 norm: 3.594.. Val L1 norm: 1.119.. Train Linf norm: 5229.858.. Val Linf norm: 159.611\n",
            "Epoch 100/129.. Train loss: 78.271.. Val loss: 53.958.. Train L1 norm: 3.630.. Val L1 norm: 1.119.. Train Linf norm: 5300.293.. Val Linf norm: 159.605\n",
            "Epoch 101/129.. Train loss: 63.018.. Val loss: 53.958.. Train L1 norm: 3.628.. Val L1 norm: 1.119.. Train Linf norm: 5299.444.. Val Linf norm: 159.595\n",
            "Epoch 102/129.. Train loss: 66.380.. Val loss: 53.958.. Train L1 norm: 4.052.. Val L1 norm: 1.119.. Train Linf norm: 6156.107.. Val Linf norm: 159.605\n",
            "Epoch 103/129.. Train loss: 63.018.. Val loss: 53.958.. Train L1 norm: 3.791.. Val L1 norm: 1.119.. Train Linf norm: 5625.889.. Val Linf norm: 159.604\n",
            "Epoch 104/129.. Train loss: 86.451.. Val loss: 53.958.. Train L1 norm: 3.127.. Val L1 norm: 1.119.. Train Linf norm: 4257.687.. Val Linf norm: 159.621\n",
            "Epoch 105/129.. Train loss: 70.997.. Val loss: 53.958.. Train L1 norm: 3.544.. Val L1 norm: 1.119.. Train Linf norm: 5114.736.. Val Linf norm: 159.636\n",
            "Epoch 106/129.. Train loss: 238.698.. Val loss: 53.958.. Train L1 norm: 3.187.. Val L1 norm: 1.119.. Train Linf norm: 4378.265.. Val Linf norm: 159.605\n",
            "Epoch 107/129.. Train loss: 121.863.. Val loss: 53.958.. Train L1 norm: 3.988.. Val L1 norm: 1.119.. Train Linf norm: 6013.083.. Val Linf norm: 159.623\n",
            "Epoch 108/129.. Train loss: 101.539.. Val loss: 53.957.. Train L1 norm: 3.355.. Val L1 norm: 1.119.. Train Linf norm: 4737.346.. Val Linf norm: 159.651\n",
            "Epoch 109/129.. Train loss: 93.993.. Val loss: 53.957.. Train L1 norm: 3.730.. Val L1 norm: 1.119.. Train Linf norm: 5460.899.. Val Linf norm: 159.688\n",
            "Epoch 110/129.. Train loss: 59.851.. Val loss: 53.957.. Train L1 norm: 3.220.. Val L1 norm: 1.119.. Train Linf norm: 4461.731.. Val Linf norm: 159.690\n",
            "Epoch 111/129.. Train loss: 59.749.. Val loss: 53.957.. Train L1 norm: 3.717.. Val L1 norm: 1.119.. Train Linf norm: 5436.785.. Val Linf norm: 159.692\n",
            "Epoch 112/129.. Train loss: 67.171.. Val loss: 53.957.. Train L1 norm: 3.643.. Val L1 norm: 1.119.. Train Linf norm: 5320.380.. Val Linf norm: 159.686\n",
            "Epoch 113/129.. Train loss: 65.601.. Val loss: 53.957.. Train L1 norm: 3.736.. Val L1 norm: 1.119.. Train Linf norm: 5521.253.. Val Linf norm: 159.682\n",
            "Epoch 114/129.. Train loss: 70.933.. Val loss: 53.957.. Train L1 norm: 3.404.. Val L1 norm: 1.119.. Train Linf norm: 4839.408.. Val Linf norm: 159.671\n",
            "Epoch 115/129.. Train loss: 64.880.. Val loss: 53.957.. Train L1 norm: 3.512.. Val L1 norm: 1.119.. Train Linf norm: 5014.566.. Val Linf norm: 159.668\n",
            "Epoch 116/129.. Train loss: 67.573.. Val loss: 53.957.. Train L1 norm: 3.323.. Val L1 norm: 1.119.. Train Linf norm: 4671.420.. Val Linf norm: 159.678\n",
            "Epoch 117/129.. Train loss: 101.618.. Val loss: 53.957.. Train L1 norm: 4.666.. Val L1 norm: 1.119.. Train Linf norm: 7422.108.. Val Linf norm: 159.703\n",
            "Epoch 118/129.. Train loss: 59.500.. Val loss: 53.956.. Train L1 norm: 3.885.. Val L1 norm: 1.119.. Train Linf norm: 5822.282.. Val Linf norm: 159.707\n",
            "Epoch 119/129.. Train loss: 73.217.. Val loss: 53.956.. Train L1 norm: 3.709.. Val L1 norm: 1.119.. Train Linf norm: 5453.514.. Val Linf norm: 159.719\n",
            "Epoch 120/129.. Train loss: 66.566.. Val loss: 53.956.. Train L1 norm: 4.075.. Val L1 norm: 1.119.. Train Linf norm: 6212.869.. Val Linf norm: 159.721\n",
            "Epoch 121/129.. Train loss: 62.861.. Val loss: 53.956.. Train L1 norm: 3.147.. Val L1 norm: 1.119.. Train Linf norm: 4312.271.. Val Linf norm: 159.720\n",
            "Epoch 122/129.. Train loss: 70.865.. Val loss: 53.956.. Train L1 norm: 2.764.. Val L1 norm: 1.119.. Train Linf norm: 3526.564.. Val Linf norm: 159.733\n",
            "Epoch 123/129.. Train loss: 72.262.. Val loss: 53.956.. Train L1 norm: 4.047.. Val L1 norm: 1.119.. Train Linf norm: 6149.722.. Val Linf norm: 159.751\n",
            "Epoch 124/129.. Train loss: 60.645.. Val loss: 53.956.. Train L1 norm: 3.311.. Val L1 norm: 1.119.. Train Linf norm: 4647.983.. Val Linf norm: 159.756\n",
            "Epoch 125/129.. Train loss: 93.513.. Val loss: 53.955.. Train L1 norm: 2.677.. Val L1 norm: 1.119.. Train Linf norm: 3343.172.. Val Linf norm: 159.778\n",
            "Epoch 126/129.. Train loss: 72.583.. Val loss: 53.955.. Train L1 norm: 4.114.. Val L1 norm: 1.119.. Train Linf norm: 6289.180.. Val Linf norm: 159.771\n",
            "Epoch 127/129.. Train loss: 59.485.. Val loss: 53.955.. Train L1 norm: 2.533.. Val L1 norm: 1.119.. Train Linf norm: 3050.161.. Val Linf norm: 159.773\n",
            "Epoch 128/129.. Train loss: 80.606.. Val loss: 53.955.. Train L1 norm: 3.395.. Val L1 norm: 1.119.. Train Linf norm: 4819.891.. Val Linf norm: 159.794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:31:40,906]\u001b[0m Trial 160 finished with value: 1.118842179107666 and parameters: {'n_layers': 7, 'n_units_0': 3502, 'n_units_1': 148, 'n_units_2': 3635, 'n_units_3': 852, 'n_units_4': 195, 'n_units_5': 1037, 'n_units_6': 799, 'hidden_activation': 'PReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 1.4392876633590541e-05, 'batch_size': 2048, 'n_epochs': 129, 'scheduler': 'ReduceLROnPlateau', 'prelu_init': 0.2229501626280442, 'dropout_rate': 0.0771249599462569, 'weight_decay': 0.0010599140814456956, 'beta1': 0.932150640881949, 'beta2': 0.9991997366474599, 'factor': 0.12422359624113397, 'patience': 8, 'threshold': 0.0010779379724713677}. Best is trial 92 with value: 1.0098264760335287.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 129/129.. Train loss: 85.297.. Val loss: 53.954.. Train L1 norm: 3.366.. Val L1 norm: 1.119.. Train Linf norm: 4758.095.. Val Linf norm: 159.808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:31:43,808]\u001b[0m Trial 161 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/134.. Train loss: 256.934.. Val loss: 52.975.. Train L1 norm: 5.588.. Val L1 norm: 1.138.. Train Linf norm: 4624.680.. Val Linf norm: 97.987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:31:45,592]\u001b[0m Trial 162 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/140.. Train loss: 336.003.. Val loss: 54.403.. Train L1 norm: 1.174.. Val L1 norm: 1.047.. Train Linf norm: 170.816.. Val Linf norm: 35.360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:31:48,170]\u001b[0m Trial 163 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/142.. Train loss: 399.017.. Val loss: 56.850.. Train L1 norm: 1.155.. Val L1 norm: 1.057.. Train Linf norm: 135.734.. Val Linf norm: 37.865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:31:50,638]\u001b[0m Trial 164 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/144.. Train loss: 774.987.. Val loss: 54.147.. Train L1 norm: 3.547.. Val L1 norm: 1.056.. Train Linf norm: 2582.730.. Val Linf norm: 45.410\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:31:54,068]\u001b[0m Trial 165 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/138.. Train loss: 25378.108.. Val loss: 370.786.. Train L1 norm: 40.065.. Val L1 norm: 3.213.. Train Linf norm: 39172.471.. Val Linf norm: 881.620\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:31:55,979]\u001b[0m Trial 166 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/141.. Train loss: 72731071782.916.. Val loss: 7642302143.693.. Train L1 norm: 21153.269.. Val L1 norm: 36248.572.. Train Linf norm: 14410928.857.. Val Linf norm: 12723462.316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:32:02,827]\u001b[0m Trial 167 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/146.. Train loss: 4019483030.918.. Val loss: 157487557321.370.. Train L1 norm: 42144.434.. Val L1 norm: 13340.567.. Train Linf norm: 2546596.084.. Val Linf norm: 501634.486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:32:06,273]\u001b[0m Trial 168 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/143.. Train loss: 2245.227.. Val loss: 53.165.. Train L1 norm: 8.481.. Val L1 norm: 1.132.. Train Linf norm: 7571.621.. Val Linf norm: 91.309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:32:08,026]\u001b[0m Trial 169 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/139.. Train loss: 1720.553.. Val loss: 52.916.. Train L1 norm: 4.747.. Val L1 norm: 1.126.. Train Linf norm: 3790.861.. Val Linf norm: 88.968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:32:11,542]\u001b[0m Trial 170 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/147.. Train loss: 557.796.. Val loss: 58.423.. Train L1 norm: 2.279.. Val L1 norm: 1.110.. Train Linf norm: 1293.267.. Val Linf norm: 62.311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:32:14,226]\u001b[0m Trial 171 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/144.. Train loss: 4088.151.. Val loss: 51.953.. Train L1 norm: 10.559.. Val L1 norm: 1.507.. Train Linf norm: 9649.995.. Val Linf norm: 288.525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:32:16,387]\u001b[0m Trial 172 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150.. Train loss: 122.508.. Val loss: 54.067.. Train L1 norm: 1.650.. Val L1 norm: 1.061.. Train Linf norm: 649.086.. Val Linf norm: 48.010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:32:18,332]\u001b[0m Trial 173 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/147.. Train loss: 154282.863.. Val loss: 510.281.. Train L1 norm: 23.460.. Val L1 norm: 1.281.. Train Linf norm: 22590.645.. Val Linf norm: 125.104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:32:19,952]\u001b[0m Trial 174 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/145.. Train loss: 385.325.. Val loss: 53.297.. Train L1 norm: 4.309.. Val L1 norm: 1.106.. Train Linf norm: 3355.506.. Val Linf norm: 76.505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:32:21,876]\u001b[0m Trial 175 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/103.. Train loss: 1548.053.. Val loss: 52.407.. Train L1 norm: 3.829.. Val L1 norm: 1.114.. Train Linf norm: 2831.521.. Val Linf norm: 77.727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:32:26,943]\u001b[0m Trial 176 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/115.. Train loss: 1989.436.. Val loss: 52.591.. Train L1 norm: 5.356.. Val L1 norm: 1.108.. Train Linf norm: 4396.190.. Val Linf norm: 74.583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:32:29,228]\u001b[0m Trial 177 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/142.. Train loss: 720.337.. Val loss: 55.997.. Train L1 norm: 3.446.. Val L1 norm: 1.078.. Train Linf norm: 4940.134.. Val Linf norm: 97.306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:32:33,635]\u001b[0m Trial 178 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/140.. Train loss: 5.770.. Val loss: 4.300.. Train L1 norm: 29.067.. Val L1 norm: 2.358.. Train Linf norm: 3562.756.. Val Linf norm: 128.953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:32:38,278]\u001b[0m Trial 179 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/82.. Train loss: 5.745.. Val loss: 3.660.. Train L1 norm: 25.278.. Val L1 norm: 3.107.. Train Linf norm: 24130.323.. Val Linf norm: 977.013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "\u001b[32m[I 2023-05-29 10:32:46,811]\u001b[0m Trial 180 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/137.. Train loss: nan.. Val loss: nan.. Train L1 norm: nan.. Val L1 norm: nan.. Train Linf norm: nan.. Val Linf norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:32:50,259]\u001b[0m Trial 181 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/132.. Train loss: 2342.257.. Val loss: 54.035.. Train L1 norm: 5.347.. Val L1 norm: 1.062.. Train Linf norm: 4425.883.. Val Linf norm: 47.954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:32:52,249]\u001b[0m Trial 182 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/143.. Train loss: 2951.847.. Val loss: 54.710.. Train L1 norm: 4.215.. Val L1 norm: 1.057.. Train Linf norm: 3269.945.. Val Linf norm: 42.885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:32:57,308]\u001b[0m Trial 183 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/145.. Train loss: 3076.688.. Val loss: 53.766.. Train L1 norm: 6.393.. Val L1 norm: 1.074.. Train Linf norm: 5471.623.. Val Linf norm: 56.977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:32:58,807]\u001b[0m Trial 184 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/148.. Train loss: 117458023183.211.. Val loss: 56226437396.378.. Train L1 norm: 1115.164.. Val L1 norm: 7687.587.. Train Linf norm: 312266.123.. Val Linf norm: 2541183.944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:33:00,307]\u001b[0m Trial 185 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/142.. Train loss: 1902.820.. Val loss: 54.416.. Train L1 norm: 3.407.. Val L1 norm: 1.072.. Train Linf norm: 2437.715.. Val Linf norm: 49.052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:33:02,171]\u001b[0m Trial 186 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/146.. Train loss: 952.997.. Val loss: 61.567.. Train L1 norm: 6.315.. Val L1 norm: 1.090.. Train Linf norm: 4924.098.. Val Linf norm: 52.341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:33:05,428]\u001b[0m Trial 187 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/140.. Train loss: 1757.245.. Val loss: 54.349.. Train L1 norm: 4.479.. Val L1 norm: 1.060.. Train Linf norm: 3522.185.. Val Linf norm: 48.273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:33:07,243]\u001b[0m Trial 188 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150.. Train loss: 2.843.. Val loss: 2.180.. Train L1 norm: 3.208.. Val L1 norm: 2.164.. Train Linf norm: 1792.129.. Val Linf norm: 590.283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:33:09,477]\u001b[0m Trial 189 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/143.. Train loss: 368.798.. Val loss: 54.030.. Train L1 norm: 1.736.. Val L1 norm: 1.044.. Train Linf norm: 1475.699.. Val Linf norm: 52.898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:33:12,332]\u001b[0m Trial 190 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/147.. Train loss: 3582.233.. Val loss: 56.222.. Train L1 norm: 6.468.. Val L1 norm: 1.089.. Train Linf norm: 1375.796.. Val Linf norm: 20.718\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:33:14,138]\u001b[0m Trial 191 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/135.. Train loss: 882.022.. Val loss: 45.447.. Train L1 norm: 16.941.. Val L1 norm: 2.418.. Train Linf norm: 16003.854.. Val Linf norm: 755.091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:33:17,681]\u001b[0m Trial 192 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/144.. Train loss: 340.825.. Val loss: 53.612.. Train L1 norm: 2.308.. Val L1 norm: 1.104.. Train Linf norm: 1315.723.. Val Linf norm: 76.955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:33:21,264]\u001b[0m Trial 193 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/145.. Train loss: 69.716.. Val loss: 53.783.. Train L1 norm: 2.033.. Val L1 norm: 1.059.. Train Linf norm: 1051.925.. Val Linf norm: 46.340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:33:23,012]\u001b[0m Trial 194 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/141.. Train loss: 2236838.467.. Val loss: 37326.849.. Train L1 norm: 93.469.. Val L1 norm: 37.811.. Train Linf norm: 68936.574.. Val Linf norm: 14310.142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:33:26,220]\u001b[0m Trial 195 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/148.. Train loss: 633.456.. Val loss: 52.761.. Train L1 norm: 2.093.. Val L1 norm: 1.116.. Train Linf norm: 1097.880.. Val Linf norm: 79.932\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:33:30,828]\u001b[0m Trial 196 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/145.. Train loss: 1346.765.. Val loss: 52.715.. Train L1 norm: 4.715.. Val L1 norm: 1.107.. Train Linf norm: 1894.798.. Val Linf norm: 44.685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:33:33,012]\u001b[0m Trial 197 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/139.. Train loss: 3835.394.. Val loss: 43.674.. Train L1 norm: 30.625.. Val L1 norm: 4.154.. Train Linf norm: 29114.967.. Val Linf norm: 1689.775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:33:37,204]\u001b[0m Trial 198 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/142.. Train loss: 1686.286.. Val loss: 57.398.. Train L1 norm: 2.750.. Val L1 norm: 1.077.. Train Linf norm: 1768.374.. Val Linf norm: 49.846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-29 10:33:39,202]\u001b[0m Trial 199 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/138.. Train loss: 264.282.. Val loss: 54.023.. Train L1 norm: 3.229.. Val L1 norm: 1.070.. Train Linf norm: 2260.372.. Val Linf norm: 54.646\n",
            "Best trial:\n",
            "  Value:  1.0098264760335287\n",
            "  Params: \n",
            "    n_layers: 6\n",
            "    n_units_0: 3558\n",
            "    n_units_1: 165\n",
            "    n_units_2: 3906\n",
            "    n_units_3: 434\n",
            "    n_units_4: 239\n",
            "    n_units_5: 1274\n",
            "    hidden_activation: PReLU\n",
            "    output_activation: Linear\n",
            "    loss: MSE\n",
            "    optimizer: Adam\n",
            "    lr: 1.0435149104684462e-06\n",
            "    batch_size: 1024\n",
            "    n_epochs: 143\n",
            "    scheduler: ReduceLROnPlateau\n",
            "    prelu_init: 0.2022112945503146\n",
            "    dropout_rate: 0.06388872518840581\n",
            "    weight_decay: 0.0009246275300019842\n",
            "    beta1: 0.939104753876406\n",
            "    beta2: 0.9991856135686923\n",
            "    factor: 0.11351038537346028\n",
            "    patience: 6\n",
            "    threshold: 0.0024087472803278587\n"
          ]
        }
      ],
      "source": [
        "if OPTIMIZE:\n",
        "    # Creating a study object with Optuna with TPE sampler and median pruner \n",
        "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner())\n",
        "\n",
        "    # Running Optuna with 100 trials when we are optimizing.\n",
        "    study.optimize(objective, n_trials=N_TRIALS)\n",
        "\n",
        "    # Printing the best trial information\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "    print(\"  Value: \", trial.value)\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(f\"    {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RVl1j4fMsq0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Y_KmYluJtxGv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmMfE9_dUZiS"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phyiHlWEUZiT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "303c59b1-c5ba-4896-c41d-d5328fdcc8b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-77-ddc3c9d46736>:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.FrozenTrial.suggest_float` instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-6, 1)\n",
            "<ipython-input-77-ddc3c9d46736>:63: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.FrozenTrial.suggest_float` instead.\n",
            "  init = trial.suggest_uniform(\"prelu_init\", 0.1, 0.3)\n",
            "<ipython-input-77-ddc3c9d46736>:76: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.FrozenTrial.suggest_float` instead.\n",
            "  dropout_rate = trial.suggest_uniform(\"dropout_rate\", 0.0, 0.5)\n",
            "<ipython-input-77-ddc3c9d46736>:149: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.FrozenTrial.suggest_float` instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
            "<ipython-input-77-ddc3c9d46736>:150: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.FrozenTrial.suggest_float` instead.\n",
            "  beta1 = trial.suggest_uniform(\"beta1\", 0.9, 0.999)\n",
            "<ipython-input-77-ddc3c9d46736>:151: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.FrozenTrial.suggest_float` instead.\n",
            "  beta2 = trial.suggest_uniform(\"beta2\", 0.999, 0.9999)\n",
            "<ipython-input-77-ddc3c9d46736>:178: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.FrozenTrial.suggest_float` instead.\n",
            "  factor = trial.suggest_uniform(\"factor\", 0.1, 0.5)\n",
            "<ipython-input-77-ddc3c9d46736>:180: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.FrozenTrial.suggest_float` instead.\n",
            "  threshold = trial.suggest_loguniform(\"threshold\", 1e-4, 1e-2)\n"
          ]
        }
      ],
      "source": [
        "# Creating the best network and optimizer using the best hyperparameters\n",
        "if OPTIMIZE:\n",
        "    net, \\\n",
        "    loss_fn, \\\n",
        "    optimizer, \\\n",
        "    batch_size, \\\n",
        "    n_epochs, \\\n",
        "    scheduler, \\\n",
        "    loss_name, \\\n",
        "    optimizer_name, \\\n",
        "    scheduler_name, \\\n",
        "    n_units, \\\n",
        "    n_layers, \\\n",
        "    hidden_activation, \\\n",
        "    output_activation, \\\n",
        "    lr, \\\n",
        "    dropout_rate = create_model(trial, optimize=True)\n",
        "\n",
        "# Creating the network with predefined hyperparameters\n",
        "else:\n",
        "    net, \\\n",
        "    loss_fn, \\\n",
        "    optimizer, \\\n",
        "    batch_size, \\\n",
        "    n_epochs, \\\n",
        "    scheduler, \\\n",
        "    loss_name, \\\n",
        "    optimizer_name, \\\n",
        "    scheduler_name, \\\n",
        "    n_units, \\\n",
        "    n_layers, \\\n",
        "    hidden_activation, \\\n",
        "    output_activation, \\\n",
        "    lr, \\\n",
        "    dropout_rate = create_model(trial=None, optimize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yq-oY81UZiU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "194e1a09-3b9f-4f39-9e24-e978cbeaf51d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss_fn: Net(\n",
            "  (hidden_activation): PReLU(num_parameters=1)\n",
            "  (output_activation): Identity()\n",
            "  (layers): ModuleList(\n",
            "    (0): Linear(in_features=14, out_features=3558, bias=True)\n",
            "    (1): Linear(in_features=3558, out_features=165, bias=True)\n",
            "    (2): Linear(in_features=165, out_features=3906, bias=True)\n",
            "    (3): Linear(in_features=3906, out_features=434, bias=True)\n",
            "    (4): Linear(in_features=434, out_features=239, bias=True)\n",
            "    (5): Linear(in_features=239, out_features=1274, bias=True)\n",
            "    (6): Linear(in_features=1274, out_features=1, bias=True)\n",
            "  )\n",
            "  (dropouts): ModuleList(\n",
            "    (0-5): 6 x Dropout(p=0.06388872518840581, inplace=False)\n",
            "  )\n",
            ")\n",
            "loss_fn: MSELoss()\n",
            "batch_size: 1024\n",
            "n_epochs: 143\n",
            "scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff7f6a56950>\n",
            "loss_name: MSE\n",
            "optimizer_name: Adam\n",
            "scheduler_name: ReduceLROnPlateau\n",
            "n_units: [3558, 165, 3906, 434, 239, 1274]\n",
            "n_layers: 6\n",
            "hidden_activation: PReLU(num_parameters=1)\n",
            "output_activation: Identity()\n",
            "lr 1.0435149104684462e-06\n",
            "dropout_rate 0.06388872518840581\n"
          ]
        }
      ],
      "source": [
        "print(\"loss_fn:\", net)\n",
        "print(\"loss_fn:\", loss_fn)\n",
        "print(\"batch_size:\", batch_size)\n",
        "print(\"n_epochs:\", n_epochs)\n",
        "print(\"scheduler:\", scheduler)\n",
        "print(\"loss_name:\", loss_name)\n",
        "print(\"optimizer_name:\", optimizer_name)\n",
        "print(\"scheduler_name:\", scheduler_name)\n",
        "print(\"n_units:\", n_units)\n",
        "print(\"n_layers:\", n_layers)\n",
        "print(\"hidden_activation:\", hidden_activation)\n",
        "print(\"output_activation:\", output_activation)\n",
        "print(\"lr\", lr)\n",
        "print(\"dropout_rate\", dropout_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7aLWdZyUZiW",
        "outputId": "8fb1e8c6-5aa3-43a1-a255-712cf9396d0c"
      },
      "source": [
        "After optimizing with Optuna and deciding on the best hyperparameters, we combine the training and validatin set, using that as the new training set for training the model, and then evaluate the model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTPX6lHCyUfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e34e992b-8464-4558-a1d5-a72018153bd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/143.. Train loss: 329.768.. Test loss: 48.529.. Train L1 norm: 2.462.. Test L1 norm: 1.096.. Train Linf norm: 1471.091.. Test Linf norm: 60.961\n",
            "Epoch 2/143.. Train loss: 60.670.. Test loss: 48.983.. Train L1 norm: 4.506.. Test L1 norm: 1.070.. Train Linf norm: 3560.750.. Test Linf norm: 48.352\n",
            "Epoch 3/143.. Train loss: 214.825.. Test loss: 49.636.. Train L1 norm: 3.375.. Test L1 norm: 1.048.. Train Linf norm: 2411.026.. Test Linf norm: 37.282\n",
            "Epoch 4/143.. Train loss: 254.824.. Test loss: 50.308.. Train L1 norm: 2.069.. Test L1 norm: 1.034.. Train Linf norm: 1078.050.. Test Linf norm: 29.582\n",
            "Epoch 5/143.. Train loss: 260.798.. Test loss: 49.881.. Train L1 norm: 2.081.. Test L1 norm: 1.044.. Train Linf norm: 1092.550.. Test Linf norm: 35.539\n",
            "Epoch 6/143.. Train loss: 175.556.. Test loss: 50.356.. Train L1 norm: 3.099.. Test L1 norm: 1.036.. Train Linf norm: 2132.998.. Test Linf norm: 30.844\n",
            "Epoch 7/143.. Train loss: 86.489.. Test loss: 51.020.. Train L1 norm: 2.918.. Test L1 norm: 1.031.. Train Linf norm: 1951.534.. Test Linf norm: 25.750\n",
            "Epoch 8/143.. Train loss: 224.847.. Test loss: 50.363.. Train L1 norm: 1.491.. Test L1 norm: 1.038.. Train Linf norm: 488.584.. Test Linf norm: 32.251\n",
            "Epoch 9/143.. Train loss: 267.431.. Test loss: 49.904.. Train L1 norm: 1.555.. Test L1 norm: 1.048.. Train Linf norm: 554.042.. Test Linf norm: 38.104\n",
            "Epoch 10/143.. Train loss: 60.805.. Test loss: 49.161.. Train L1 norm: 4.303.. Test L1 norm: 1.071.. Train Linf norm: 3358.625.. Test Linf norm: 50.252\n",
            "Epoch 11/143.. Train loss: 109.200.. Test loss: 49.208.. Train L1 norm: 3.393.. Test L1 norm: 1.070.. Train Linf norm: 2423.142.. Test Linf norm: 49.935\n",
            "Epoch 12/143.. Train loss: 127.787.. Test loss: 49.741.. Train L1 norm: 2.095.. Test L1 norm: 1.054.. Train Linf norm: 1097.889.. Test Linf norm: 41.547\n",
            "Epoch 13/143.. Train loss: 89.886.. Test loss: 49.668.. Train L1 norm: 4.625.. Test L1 norm: 1.056.. Train Linf norm: 3696.562.. Test Linf norm: 42.857\n",
            "Epoch 14/143.. Train loss: 271.385.. Test loss: 50.296.. Train L1 norm: 3.621.. Test L1 norm: 1.042.. Train Linf norm: 2666.721.. Test Linf norm: 34.975\n",
            "Epoch 15/143.. Train loss: 84.374.. Test loss: 50.883.. Train L1 norm: 2.680.. Test L1 norm: 1.037.. Train Linf norm: 1707.245.. Test Linf norm: 29.997\n",
            "Epoch 16/143.. Train loss: 169.275.. Test loss: 50.071.. Train L1 norm: 1.096.. Test L1 norm: 1.048.. Train Linf norm: 86.205.. Test Linf norm: 38.638\n",
            "Epoch 17/143.. Train loss: 105.817.. Test loss: 49.607.. Train L1 norm: 1.171.. Test L1 norm: 1.060.. Train Linf norm: 158.663.. Test Linf norm: 45.007\n",
            "Epoch 18/143.. Train loss: 98.019.. Test loss: 49.904.. Train L1 norm: 2.740.. Test L1 norm: 1.053.. Train Linf norm: 1761.527.. Test Linf norm: 41.371\n",
            "Epoch 19/143.. Train loss: 99.988.. Test loss: 49.440.. Train L1 norm: 2.784.. Test L1 norm: 1.067.. Train Linf norm: 1807.592.. Test Linf norm: 48.435\n",
            "Epoch 20/143.. Train loss: 89.017.. Test loss: 49.908.. Train L1 norm: 2.019.. Test L1 norm: 1.054.. Train Linf norm: 1023.130.. Test Linf norm: 42.750\n",
            "Epoch 21/143.. Train loss: 69.459.. Test loss: 49.684.. Train L1 norm: 2.279.. Test L1 norm: 1.061.. Train Linf norm: 1293.802.. Test Linf norm: 46.323\n",
            "Epoch 22/143.. Train loss: 85.753.. Test loss: 50.085.. Train L1 norm: 2.539.. Test L1 norm: 1.053.. Train Linf norm: 1557.194.. Test Linf norm: 42.235\n",
            "Epoch 23/143.. Train loss: 76.171.. Test loss: 49.723.. Train L1 norm: 1.386.. Test L1 norm: 1.062.. Train Linf norm: 376.502.. Test Linf norm: 47.287\n",
            "Epoch 24/143.. Train loss: 62.064.. Test loss: 49.540.. Train L1 norm: 4.529.. Test L1 norm: 1.069.. Train Linf norm: 3594.720.. Test Linf norm: 50.424\n",
            "Epoch 25/143.. Train loss: 270.601.. Test loss: 48.708.. Train L1 norm: 5.207.. Test L1 norm: 1.103.. Train Linf norm: 4272.248.. Test Linf norm: 67.032\n",
            "Epoch 26/143.. Train loss: 127.133.. Test loss: 49.035.. Train L1 norm: 4.169.. Test L1 norm: 1.087.. Train Linf norm: 3207.492.. Test Linf norm: 59.485\n",
            "Epoch 27/143.. Train loss: 115.465.. Test loss: 49.490.. Train L1 norm: 3.483.. Test L1 norm: 1.071.. Train Linf norm: 2514.062.. Test Linf norm: 52.066\n",
            "Epoch 28/143.. Train loss: 551.442.. Test loss: 50.790.. Train L1 norm: 4.276.. Test L1 norm: 1.046.. Train Linf norm: 3332.660.. Test Linf norm: 36.500\n",
            "Epoch 29/143.. Train loss: 241.138.. Test loss: 49.988.. Train L1 norm: 3.841.. Test L1 norm: 1.059.. Train Linf norm: 2893.415.. Test Linf norm: 46.236\n",
            "Epoch 30/143.. Train loss: 70.301.. Test loss: 50.175.. Train L1 norm: 2.816.. Test L1 norm: 1.056.. Train Linf norm: 1841.779.. Test Linf norm: 44.659\n",
            "Epoch 31/143.. Train loss: 128.491.. Test loss: 49.731.. Train L1 norm: 4.974.. Test L1 norm: 1.068.. Train Linf norm: 4047.421.. Test Linf norm: 51.078\n",
            "Epoch 32/143.. Train loss: 120.616.. Test loss: 50.178.. Train L1 norm: 2.372.. Test L1 norm: 1.057.. Train Linf norm: 1384.424.. Test Linf norm: 45.681\n",
            "Epoch 33/143.. Train loss: 118.838.. Test loss: 49.733.. Train L1 norm: 1.927.. Test L1 norm: 1.070.. Train Linf norm: 928.221.. Test Linf norm: 51.967\n",
            "Epoch 34/143.. Train loss: 60.554.. Test loss: 49.674.. Train L1 norm: 4.609.. Test L1 norm: 1.072.. Train Linf norm: 3667.965.. Test Linf norm: 53.528\n",
            "Epoch 35/143.. Train loss: 294.437.. Test loss: 50.314.. Train L1 norm: 2.894.. Test L1 norm: 1.058.. Train Linf norm: 1917.090.. Test Linf norm: 46.108\n",
            "Epoch 36/143.. Train loss: 59.912.. Test loss: 50.721.. Train L1 norm: 2.338.. Test L1 norm: 1.053.. Train Linf norm: 1342.176.. Test Linf norm: 42.170\n",
            "Epoch 37/143.. Train loss: 342.258.. Test loss: 49.656.. Train L1 norm: 2.063.. Test L1 norm: 1.076.. Train Linf norm: 1069.613.. Test Linf norm: 55.418\n",
            "Epoch 38/143.. Train loss: 61.074.. Test loss: 49.660.. Train L1 norm: 4.032.. Test L1 norm: 1.076.. Train Linf norm: 3073.304.. Test Linf norm: 55.989\n",
            "Epoch 39/143.. Train loss: 563.453.. Test loss: 50.978.. Train L1 norm: 1.419.. Test L1 norm: 1.052.. Train Linf norm: 412.477.. Test Linf norm: 41.365\n",
            "Epoch 40/143.. Train loss: 406.181.. Test loss: 49.824.. Train L1 norm: 3.575.. Test L1 norm: 1.073.. Train Linf norm: 2611.139.. Test Linf norm: 53.946\n",
            "Epoch 41/143.. Train loss: 109.097.. Test loss: 49.630.. Train L1 norm: 2.343.. Test L1 norm: 1.079.. Train Linf norm: 1352.267.. Test Linf norm: 57.428\n",
            "Epoch 42/143.. Train loss: 60.095.. Test loss: 49.357.. Train L1 norm: 5.247.. Test L1 norm: 1.088.. Train Linf norm: 4319.000.. Test Linf norm: 62.433\n",
            "Epoch 43/143.. Train loss: 80.603.. Test loss: 49.482.. Train L1 norm: 1.825.. Test L1 norm: 1.085.. Train Linf norm: 810.334.. Test Linf norm: 60.977\n",
            "Epoch 44/143.. Train loss: 119.161.. Test loss: 49.257.. Train L1 norm: 3.282.. Test L1 norm: 1.094.. Train Linf norm: 2305.688.. Test Linf norm: 65.526\n",
            "Epoch 45/143.. Train loss: 102.109.. Test loss: 49.558.. Train L1 norm: 3.718.. Test L1 norm: 1.084.. Train Linf norm: 2751.627.. Test Linf norm: 60.932\n",
            "Epoch 46/143.. Train loss: 83.348.. Test loss: 49.832.. Train L1 norm: 4.129.. Test L1 norm: 1.078.. Train Linf norm: 3179.762.. Test Linf norm: 57.774\n",
            "Epoch 47/143.. Train loss: 63.486.. Test loss: 49.703.. Train L1 norm: 2.929.. Test L1 norm: 1.082.. Train Linf norm: 1949.239.. Test Linf norm: 60.086\n",
            "Epoch 48/143.. Train loss: 59.289.. Test loss: 49.627.. Train L1 norm: 4.034.. Test L1 norm: 1.085.. Train Linf norm: 3079.428.. Test Linf norm: 61.797\n",
            "Epoch 49/143.. Train loss: 119.143.. Test loss: 49.223.. Train L1 norm: 2.938.. Test L1 norm: 1.099.. Train Linf norm: 1921.747.. Test Linf norm: 68.929\n",
            "Epoch 50/143.. Train loss: 390.524.. Test loss: 49.715.. Train L1 norm: 3.758.. Test L1 norm: 1.085.. Train Linf norm: 2788.245.. Test Linf norm: 61.938\n",
            "Epoch 51/143.. Train loss: 60.726.. Test loss: 50.205.. Train L1 norm: 2.642.. Test L1 norm: 1.074.. Train Linf norm: 1650.390.. Test Linf norm: 56.594\n",
            "Epoch 52/143.. Train loss: 138.557.. Test loss: 49.723.. Train L1 norm: 2.503.. Test L1 norm: 1.086.. Train Linf norm: 1512.790.. Test Linf norm: 62.891\n",
            "Epoch 53/143.. Train loss: 226.435.. Test loss: 50.037.. Train L1 norm: 2.927.. Test L1 norm: 1.079.. Train Linf norm: 1939.828.. Test Linf norm: 59.070\n",
            "Epoch 54/143.. Train loss: 153.620.. Test loss: 50.270.. Train L1 norm: 3.535.. Test L1 norm: 1.074.. Train Linf norm: 2575.018.. Test Linf norm: 56.958\n",
            "Epoch 55/143.. Train loss: 65.700.. Test loss: 49.524.. Train L1 norm: 3.221.. Test L1 norm: 1.093.. Train Linf norm: 2241.239.. Test Linf norm: 66.856\n",
            "Epoch 56/143.. Train loss: 64.714.. Test loss: 49.616.. Train L1 norm: 3.341.. Test L1 norm: 1.092.. Train Linf norm: 2363.822.. Test Linf norm: 66.149\n",
            "Epoch 57/143.. Train loss: 60.233.. Test loss: 49.567.. Train L1 norm: 3.043.. Test L1 norm: 1.094.. Train Linf norm: 2060.935.. Test Linf norm: 67.476\n",
            "Epoch 58/143.. Train loss: 108.734.. Test loss: 49.947.. Train L1 norm: 2.840.. Test L1 norm: 1.085.. Train Linf norm: 1850.985.. Test Linf norm: 63.017\n",
            "Epoch 59/143.. Train loss: 126.086.. Test loss: 49.548.. Train L1 norm: 4.248.. Test L1 norm: 1.097.. Train Linf norm: 3295.571.. Test Linf norm: 69.252\n",
            "Epoch 60/143.. Train loss: 140.588.. Test loss: 49.979.. Train L1 norm: 3.770.. Test L1 norm: 1.086.. Train Linf norm: 2801.752.. Test Linf norm: 63.671\n",
            "Epoch 61/143.. Train loss: 67.609.. Test loss: 50.244.. Train L1 norm: 3.243.. Test L1 norm: 1.081.. Train Linf norm: 2265.870.. Test Linf norm: 61.326\n",
            "Epoch 62/143.. Train loss: 241.082.. Test loss: 49.444.. Train L1 norm: 3.467.. Test L1 norm: 1.103.. Train Linf norm: 2495.378.. Test Linf norm: 72.685\n",
            "Epoch 63/143.. Train loss: 271.288.. Test loss: 49.691.. Train L1 norm: 2.492.. Test L1 norm: 1.096.. Train Linf norm: 1488.626.. Test Linf norm: 69.659\n",
            "Epoch 64/143.. Train loss: 287.199.. Test loss: 49.309.. Train L1 norm: 3.579.. Test L1 norm: 1.108.. Train Linf norm: 2610.892.. Test Linf norm: 75.673\n",
            "Epoch 65/143.. Train loss: 152.508.. Test loss: 49.713.. Train L1 norm: 3.625.. Test L1 norm: 1.096.. Train Linf norm: 2653.368.. Test Linf norm: 69.606\n",
            "Epoch 66/143.. Train loss: 95.194.. Test loss: 49.375.. Train L1 norm: 4.421.. Test L1 norm: 1.107.. Train Linf norm: 3466.792.. Test Linf norm: 75.358\n",
            "Epoch 67/143.. Train loss: 251.877.. Test loss: 48.915.. Train L1 norm: 3.063.. Test L1 norm: 1.126.. Train Linf norm: 2067.614.. Test Linf norm: 84.384\n",
            "Epoch 68/143.. Train loss: 60.384.. Test loss: 48.913.. Train L1 norm: 4.655.. Test L1 norm: 1.126.. Train Linf norm: 3693.783.. Test Linf norm: 84.992\n",
            "Epoch 69/143.. Train loss: 174.342.. Test loss: 49.500.. Train L1 norm: 3.209.. Test L1 norm: 1.107.. Train Linf norm: 2218.982.. Test Linf norm: 75.701\n",
            "Epoch 70/143.. Train loss: 171.082.. Test loss: 48.991.. Train L1 norm: 3.925.. Test L1 norm: 1.126.. Train Linf norm: 2945.356.. Test Linf norm: 84.656\n",
            "Epoch 71/143.. Train loss: 148.820.. Test loss: 49.269.. Train L1 norm: 4.398.. Test L1 norm: 1.115.. Train Linf norm: 3431.224.. Test Linf norm: 79.579\n",
            "Epoch 72/143.. Train loss: 78.980.. Test loss: 49.132.. Train L1 norm: 4.685.. Test L1 norm: 1.121.. Train Linf norm: 3733.304.. Test Linf norm: 82.565\n",
            "Epoch 73/143.. Train loss: 84.204.. Test loss: 49.248.. Train L1 norm: 3.324.. Test L1 norm: 1.117.. Train Linf norm: 2336.452.. Test Linf norm: 81.145\n",
            "Epoch 74/143.. Train loss: 96.901.. Test loss: 49.592.. Train L1 norm: 3.480.. Test L1 norm: 1.108.. Train Linf norm: 2498.518.. Test Linf norm: 76.619\n",
            "Epoch 75/143.. Train loss: 414.613.. Test loss: 48.824.. Train L1 norm: 3.434.. Test L1 norm: 1.135.. Train Linf norm: 2441.240.. Test Linf norm: 89.773\n",
            "Epoch 76/143.. Train loss: 187.205.. Test loss: 49.239.. Train L1 norm: 3.280.. Test L1 norm: 1.121.. Train Linf norm: 2281.975.. Test Linf norm: 83.222\n",
            "Epoch 77/143.. Train loss: 58.463.. Test loss: 49.363.. Train L1 norm: 4.023.. Test L1 norm: 1.118.. Train Linf norm: 3055.139.. Test Linf norm: 81.953\n",
            "Epoch 78/143.. Train loss: 58.300.. Test loss: 49.340.. Train L1 norm: 4.219.. Test L1 norm: 1.120.. Train Linf norm: 3253.704.. Test Linf norm: 82.941\n",
            "Epoch 79/143.. Train loss: 109.282.. Test loss: 49.784.. Train L1 norm: 3.208.. Test L1 norm: 1.109.. Train Linf norm: 2220.344.. Test Linf norm: 77.597\n",
            "Epoch 80/143.. Train loss: 188.445.. Test loss: 49.237.. Train L1 norm: 3.346.. Test L1 norm: 1.125.. Train Linf norm: 2359.296.. Test Linf norm: 85.782\n",
            "Epoch 81/143.. Train loss: 69.553.. Test loss: 49.220.. Train L1 norm: 4.768.. Test L1 norm: 1.126.. Train Linf norm: 3813.427.. Test Linf norm: 86.569\n",
            "Epoch 82/143.. Train loss: 171.251.. Test loss: 49.107.. Train L1 norm: 5.230.. Test L1 norm: 1.130.. Train Linf norm: 4279.159.. Test Linf norm: 88.551\n",
            "Epoch 83/143.. Train loss: 117.143.. Test loss: 49.124.. Train L1 norm: 4.155.. Test L1 norm: 1.129.. Train Linf norm: 3182.992.. Test Linf norm: 88.162\n",
            "Epoch 84/143.. Train loss: 69.671.. Test loss: 48.977.. Train L1 norm: 3.760.. Test L1 norm: 1.136.. Train Linf norm: 2771.216.. Test Linf norm: 91.453\n",
            "Epoch 85/143.. Train loss: 354.398.. Test loss: 49.685.. Train L1 norm: 3.901.. Test L1 norm: 1.115.. Train Linf norm: 2929.857.. Test Linf norm: 81.604\n",
            "Epoch 86/143.. Train loss: 76.502.. Test loss: 49.840.. Train L1 norm: 4.342.. Test L1 norm: 1.112.. Train Linf norm: 3383.489.. Test Linf norm: 80.341\n",
            "Epoch 87/143.. Train loss: 102.423.. Test loss: 49.513.. Train L1 norm: 4.666.. Test L1 norm: 1.122.. Train Linf norm: 3711.561.. Test Linf norm: 85.364\n",
            "Epoch 88/143.. Train loss: 184.255.. Test loss: 49.167.. Train L1 norm: 3.727.. Test L1 norm: 1.134.. Train Linf norm: 2735.633.. Test Linf norm: 91.349\n",
            "Epoch 89/143.. Train loss: 60.112.. Test loss: 49.053.. Train L1 norm: 3.133.. Test L1 norm: 1.139.. Train Linf norm: 2134.423.. Test Linf norm: 93.871\n",
            "Epoch 90/143.. Train loss: 60.460.. Test loss: 49.008.. Train L1 norm: 3.808.. Test L1 norm: 1.141.. Train Linf norm: 2817.401.. Test Linf norm: 95.234\n",
            "Epoch 91/143.. Train loss: 61.395.. Test loss: 49.071.. Train L1 norm: 5.428.. Test L1 norm: 1.140.. Train Linf norm: 4473.734.. Test Linf norm: 94.821\n",
            "Epoch 92/143.. Train loss: 84.864.. Test loss: 48.868.. Train L1 norm: 4.311.. Test L1 norm: 1.149.. Train Linf norm: 3335.214.. Test Linf norm: 98.860\n",
            "Epoch 93/143.. Train loss: 57.996.. Test loss: 48.786.. Train L1 norm: 5.278.. Test L1 norm: 1.153.. Train Linf norm: 4323.972.. Test Linf norm: 100.946\n",
            "Epoch 94/143.. Train loss: 79.529.. Test loss: 48.995.. Train L1 norm: 5.017.. Test L1 norm: 1.146.. Train Linf norm: 4061.015.. Test Linf norm: 98.087\n",
            "Epoch 95/143.. Train loss: 79.386.. Test loss: 49.288.. Train L1 norm: 4.430.. Test L1 norm: 1.138.. Train Linf norm: 3462.720.. Test Linf norm: 94.356\n",
            "Epoch 96/143.. Train loss: 114.450.. Test loss: 49.646.. Train L1 norm: 4.523.. Test L1 norm: 1.129.. Train Linf norm: 3562.556.. Test Linf norm: 90.217\n",
            "Epoch 97/143.. Train loss: 63.644.. Test loss: 49.464.. Train L1 norm: 4.012.. Test L1 norm: 1.135.. Train Linf norm: 3038.926.. Test Linf norm: 93.266\n",
            "Epoch 98/143.. Train loss: 62.698.. Test loss: 49.273.. Train L1 norm: 4.611.. Test L1 norm: 1.142.. Train Linf norm: 3649.962.. Test Linf norm: 96.572\n",
            "Epoch 99/143.. Train loss: 682.991.. Test loss: 48.440.. Train L1 norm: 4.207.. Test L1 norm: 1.174.. Train Linf norm: 3214.031.. Test Linf norm: 111.918\n",
            "Epoch 100/143.. Train loss: 130.506.. Test loss: 48.758.. Train L1 norm: 4.237.. Test L1 norm: 1.161.. Train Linf norm: 3252.522.. Test Linf norm: 106.190\n",
            "Epoch 101/143.. Train loss: 75.135.. Test loss: 48.805.. Train L1 norm: 5.655.. Test L1 norm: 1.161.. Train Linf norm: 4701.850.. Test Linf norm: 105.939\n",
            "Epoch 102/143.. Train loss: 85.558.. Test loss: 48.674.. Train L1 norm: 5.025.. Test L1 norm: 1.166.. Train Linf norm: 4061.658.. Test Linf norm: 108.542\n",
            "Epoch 103/143.. Train loss: 63.313.. Test loss: 48.751.. Train L1 norm: 5.180.. Test L1 norm: 1.164.. Train Linf norm: 4219.101.. Test Linf norm: 107.621\n",
            "Epoch 104/143.. Train loss: 90.285.. Test loss: 48.509.. Train L1 norm: 5.323.. Test L1 norm: 1.175.. Train Linf norm: 4362.770.. Test Linf norm: 112.865\n",
            "Epoch 105/143.. Train loss: 115.946.. Test loss: 48.145.. Train L1 norm: 4.990.. Test L1 norm: 1.194.. Train Linf norm: 4009.018.. Test Linf norm: 121.673\n",
            "Epoch 106/143.. Train loss: 59.601.. Test loss: 48.140.. Train L1 norm: 5.252.. Test L1 norm: 1.195.. Train Linf norm: 4269.652.. Test Linf norm: 122.329\n",
            "Epoch 107/143.. Train loss: 155.104.. Test loss: 48.569.. Train L1 norm: 4.829.. Test L1 norm: 1.175.. Train Linf norm: 3851.016.. Test Linf norm: 113.307\n",
            "Epoch 108/143.. Train loss: 130.261.. Test loss: 48.208.. Train L1 norm: 3.883.. Test L1 norm: 1.193.. Train Linf norm: 2867.141.. Test Linf norm: 121.441\n",
            "Epoch 109/143.. Train loss: 282.180.. Test loss: 48.652.. Train L1 norm: 4.958.. Test L1 norm: 1.173.. Train Linf norm: 3981.329.. Test Linf norm: 112.764\n",
            "Epoch 110/143.. Train loss: 76.554.. Test loss: 48.491.. Train L1 norm: 3.305.. Test L1 norm: 1.180.. Train Linf norm: 2294.416.. Test Linf norm: 116.384\n",
            "Epoch 111/143.. Train loss: 90.082.. Test loss: 48.249.. Train L1 norm: 4.490.. Test L1 norm: 1.193.. Train Linf norm: 3500.057.. Test Linf norm: 122.047\n",
            "Epoch 112/143.. Train loss: 116.040.. Test loss: 48.505.. Train L1 norm: 5.897.. Test L1 norm: 1.182.. Train Linf norm: 4932.655.. Test Linf norm: 117.185\n",
            "Epoch 113/143.. Train loss: 79.386.. Test loss: 48.356.. Train L1 norm: 5.287.. Test L1 norm: 1.190.. Train Linf norm: 4310.497.. Test Linf norm: 120.869\n",
            "Epoch 114/143.. Train loss: 99.305.. Test loss: 48.655.. Train L1 norm: 5.646.. Test L1 norm: 1.177.. Train Linf norm: 4684.867.. Test Linf norm: 115.205\n",
            "Epoch 115/143.. Train loss: 86.411.. Test loss: 48.491.. Train L1 norm: 5.206.. Test L1 norm: 1.185.. Train Linf norm: 4237.934.. Test Linf norm: 118.802\n",
            "Epoch 116/143.. Train loss: 92.451.. Test loss: 48.587.. Train L1 norm: 5.241.. Test L1 norm: 1.181.. Train Linf norm: 4272.254.. Test Linf norm: 117.320\n",
            "Epoch 117/143.. Train loss: 68.554.. Test loss: 48.494.. Train L1 norm: 6.110.. Test L1 norm: 1.186.. Train Linf norm: 5153.245.. Test Linf norm: 119.693\n",
            "Epoch 118/143.. Train loss: 143.374.. Test loss: 48.879.. Train L1 norm: 4.558.. Test L1 norm: 1.171.. Train Linf norm: 3578.717.. Test Linf norm: 113.229\n",
            "Epoch 119/143.. Train loss: 136.679.. Test loss: 48.590.. Train L1 norm: 5.320.. Test L1 norm: 1.183.. Train Linf norm: 4360.271.. Test Linf norm: 118.911\n",
            "Epoch 120/143.. Train loss: 106.210.. Test loss: 48.635.. Train L1 norm: 5.508.. Test L1 norm: 1.182.. Train Linf norm: 4543.728.. Test Linf norm: 118.598\n",
            "Epoch 121/143.. Train loss: 65.651.. Test loss: 48.668.. Train L1 norm: 4.112.. Test L1 norm: 1.181.. Train Linf norm: 3121.862.. Test Linf norm: 118.526\n",
            "Epoch 122/143.. Train loss: 87.797.. Test loss: 48.408.. Train L1 norm: 3.436.. Test L1 norm: 1.193.. Train Linf norm: 2419.777.. Test Linf norm: 124.074\n",
            "Epoch 123/143.. Train loss: 73.899.. Test loss: 48.229.. Train L1 norm: 6.297.. Test L1 norm: 1.203.. Train Linf norm: 5342.161.. Test Linf norm: 128.414\n",
            "Epoch 124/143.. Train loss: 61.790.. Test loss: 48.304.. Train L1 norm: 5.558.. Test L1 norm: 1.200.. Train Linf norm: 4587.883.. Test Linf norm: 127.395\n",
            "Epoch 125/143.. Train loss: 58.685.. Test loss: 48.321.. Train L1 norm: 5.320.. Test L1 norm: 1.200.. Train Linf norm: 4346.960.. Test Linf norm: 127.571\n",
            "Epoch 126/143.. Train loss: 71.087.. Test loss: 48.188.. Train L1 norm: 5.902.. Test L1 norm: 1.208.. Train Linf norm: 4938.458.. Test Linf norm: 131.107\n",
            "Epoch 127/143.. Train loss: 67.355.. Test loss: 48.323.. Train L1 norm: 5.582.. Test L1 norm: 1.202.. Train Linf norm: 4606.606.. Test Linf norm: 128.737\n",
            "Epoch 128/143.. Train loss: 66.472.. Test loss: 48.230.. Train L1 norm: 4.935.. Test L1 norm: 1.207.. Train Linf norm: 3949.759.. Test Linf norm: 131.429\n",
            "Epoch 129/143.. Train loss: 109.304.. Test loss: 48.532.. Train L1 norm: 5.381.. Test L1 norm: 1.194.. Train Linf norm: 4408.652.. Test Linf norm: 125.696\n",
            "Epoch 130/143.. Train loss: 94.185.. Test loss: 48.806.. Train L1 norm: 4.984.. Test L1 norm: 1.184.. Train Linf norm: 4006.584.. Test Linf norm: 121.296\n",
            "Epoch 131/143.. Train loss: 58.452.. Test loss: 48.762.. Train L1 norm: 4.203.. Test L1 norm: 1.187.. Train Linf norm: 3209.802.. Test Linf norm: 122.617\n",
            "Epoch 132/143.. Train loss: 133.953.. Test loss: 49.214.. Train L1 norm: 4.493.. Test L1 norm: 1.173.. Train Linf norm: 3504.051.. Test Linf norm: 116.471\n",
            "Epoch 133/143.. Train loss: 130.641.. Test loss: 48.795.. Train L1 norm: 4.661.. Test L1 norm: 1.187.. Train Linf norm: 3669.353.. Test Linf norm: 123.003\n",
            "Epoch 134/143.. Train loss: 65.164.. Test loss: 48.615.. Train L1 norm: 5.616.. Test L1 norm: 1.195.. Train Linf norm: 4654.279.. Test Linf norm: 126.749\n",
            "Epoch 135/143.. Train loss: 58.090.. Test loss: 48.543.. Train L1 norm: 4.106.. Test L1 norm: 1.199.. Train Linf norm: 3102.266.. Test Linf norm: 128.702\n",
            "Epoch 136/143.. Train loss: 61.713.. Test loss: 48.427.. Train L1 norm: 4.784.. Test L1 norm: 1.204.. Train Linf norm: 3794.194.. Test Linf norm: 131.310\n",
            "Epoch 137/143.. Train loss: 85.311.. Test loss: 48.692.. Train L1 norm: 3.879.. Test L1 norm: 1.195.. Train Linf norm: 2869.281.. Test Linf norm: 127.218\n",
            "Epoch 138/143.. Train loss: 111.720.. Test loss: 48.380.. Train L1 norm: 5.741.. Test L1 norm: 1.209.. Train Linf norm: 4777.175.. Test Linf norm: 133.346\n",
            "Epoch 139/143.. Train loss: 59.065.. Test loss: 48.376.. Train L1 norm: 3.730.. Test L1 norm: 1.210.. Train Linf norm: 2708.757.. Test Linf norm: 134.017\n",
            "Epoch 140/143.. Train loss: 97.292.. Test loss: 48.788.. Train L1 norm: 5.438.. Test L1 norm: 1.195.. Train Linf norm: 4463.908.. Test Linf norm: 127.315\n",
            "Epoch 141/143.. Train loss: 135.135.. Test loss: 48.628.. Train L1 norm: 3.667.. Test L1 norm: 1.202.. Train Linf norm: 2650.557.. Test Linf norm: 130.510\n",
            "Epoch 142/143.. Train loss: 59.491.. Test loss: 48.394.. Train L1 norm: 4.656.. Test L1 norm: 1.212.. Train Linf norm: 3658.978.. Test Linf norm: 135.031\n",
            "Epoch 143/143.. Train loss: 64.694.. Test loss: 48.557.. Train L1 norm: 6.385.. Test L1 norm: 1.206.. Train Linf norm: 5435.109.. Test Linf norm: 132.623\n"
          ]
        }
      ],
      "source": [
        "x_train_val = torch.cat((x_train, x_val), 0)\n",
        "y_train_val = torch.cat((y_train, y_val), 0)\n",
        "\n",
        "train_val_loader = torch.utils.data.DataLoader(\n",
        "    torch.utils.data.TensorDataset(x_train_val, y_train_val), batch_size=batch_size, shuffle=True\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    torch.utils.data.TensorDataset(x_test, y_test), batch_size=batch_size\n",
        ")\n",
        "\n",
        "train_losses, _, test_losses, train_metrics, _, test_metrics, = train_and_eval(\n",
        "    net, loss_fn, optimizer, batch_size, n_epochs, scheduler, train_val_loader, None, test_loader, None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akNucrgMUZiW"
      },
      "source": [
        "## Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHsrs2Y-UZic"
      },
      "outputs": [],
      "source": [
        "# create a dictionary to store the rest of the variables\n",
        "import json\n",
        "\n",
        "# save the network to a .pth file\n",
        "torch.save(net.state_dict(), \"net.pth\")\n",
        "save_file(\"net.pth\")\n",
        "\n",
        "# save the optimizer to a .pth file\n",
        "torch.save(optimizer.state_dict(), \"optimizer.pth\")\n",
        "save_file(\"optimizer.pth\")\n",
        "\n",
        "# save the scheduler to a .pth file if it is not None\n",
        "if scheduler is not None:\n",
        "  torch.save(scheduler.state_dict(), \"scheduler.pth\")\n",
        "  save_file(\"scheduler.pth\")\n",
        "\n",
        "# create a dictionary to store the rest of the variables\n",
        "if isinstance(hidden_activation.__class__.__name__, list):\n",
        "    hidden_activation_name = hidden_activation.__class__.__name__[0]\n",
        "else:\n",
        "    hidden_activation_name = hidden_activation.__class__.__name__\n",
        "\n",
        "if isinstance(output_activation.__class__.__name__, list):\n",
        "    output_activation_name = output_activation.__class__.__name__[0]\n",
        "else:\n",
        "    output_activation_name = output_activation.__class__.__name__\n",
        "\n",
        "var_dict = {\n",
        "  \"batch_size\": batch_size,\n",
        "  \"n_epochs\": n_epochs,\n",
        "  \"loss_name\": loss_name,\n",
        "  \"optimizer_name\": optimizer_name,\n",
        "  \"scheduler_name\": scheduler_name,\n",
        "  \"n_units\": n_units,\n",
        "  \"n_layers\": n_layers,\n",
        "  \"hidden_activation_name\": hidden_activation_name,\n",
        "  \"output_activation_name\": output_activation_name,\n",
        "  \"lr\": lr,\n",
        "  \"dropout_rate\": dropout_rate\n",
        "}\n",
        "\n",
        "# Adding the subparameters to the var_dict for saving\n",
        "if hidden_activation_name == \"LeakyReLU\":\n",
        "  var_dict[\"leakyrelu_slope\"] = hidden_activation.negative_slope\n",
        "elif hidden_activation_name == \"PReLU\":\n",
        "  var_dict[\"prelu_init\"] = hidden_activation.weight.item()  # .item() is needed because weight is a tensor\n",
        "elif hidden_activation_name == \"SoftPlus\":\n",
        "  var_dict[\"softplus_beta\"] = hidden_activation.beta\n",
        "\n",
        "# save the dictionary to a .json file\n",
        "with open(\"var_dict.json\", \"w\") as f:\n",
        "  json.dump(var_dict, f)\n",
        "save_file(\"var_dict.json\")\n",
        "\n",
        "# Saving the output of the training using pandas\n",
        "train_df = pd.DataFrame(\n",
        "    {\n",
        "        \"train_loss\": train_losses,\n",
        "        \"test_loss\": test_losses,\n",
        "        \"train_l1_norm\": [m[\"l1_norm\"] for m in train_metrics],\n",
        "        \"test_l1_norm\": [m[\"l1_norm\"] for m in test_metrics],\n",
        "        \"train_linf_norm\": [m[\"linf_norm\"] for m in train_metrics],\n",
        "        \"test_linf_norm\": [m[\"linf_norm\"] for m in test_metrics],\n",
        "    }\n",
        ")\n",
        "train_df.to_csv(\"train_output.csv\", index=False)\n",
        "save_file(\"train_output.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYdsif-7yUfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4902d626-5708-4f52-fbcc-646aa32acd0a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['batch_size',\n",
              " 'n_epochs',\n",
              " 'loss_name',\n",
              " 'optimizer_name',\n",
              " 'scheduler_name',\n",
              " 'n_units',\n",
              " 'n_layers',\n",
              " 'hidden_activation_name',\n",
              " 'output_activation_name',\n",
              " 'lr',\n",
              " 'dropout_rate',\n",
              " 'prelu_init']"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "[key for key in var_dict.keys()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a2djZAjyUfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "03fafc77-3ef4-4908-d7f4-7bd4fdb6434c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PReLU'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Identity'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "#%%script echo skipping\n",
        "\n",
        "hidden_activation_name\n",
        "output_activation_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU23l7dIUZie"
      },
      "source": [
        "## Visualizing the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cippWZS6UZie",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4fcfb8f0-158a-4751-a6fa-1f0880268e43"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 0 Axes>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff7f6c3a440>]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff7f57816f0>]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Epoch')"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'L1 Norm')"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.001, 100.0)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff7f6a56410>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff7f676a950>]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff7f676b6d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Epoch')"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Linf Norm')"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.001, 100.0)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff7f5781cc0>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff7f3fc2c50>]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff7f3fc2e00>]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Epoch')"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'MSE Loss')"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1e-07, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff7b9564bb0>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plotting the losses and metrics for the best network \n",
        "plt.figure(figsize=(12, 8))\n",
        "#plt.subplot(2, 2, 1)\n",
        "#plt.plot(train_losses, label=\"Train Loss\")\n",
        "#plt.plot(test_losses, label=\"Test Loss\")\n",
        "#plt.xlabel(\"Epoch\")\n",
        "#plt.ylabel(\"Loss\")\n",
        "#plt.legend()\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot([m[\"l1_norm\"] for m in train_metrics], label=\"Train L1 Norm\")\n",
        "plt.plot([m[\"l1_norm\"] for m in test_metrics], label=\"Test L1 Norm\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"L1 Norm\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-3, 1e2)\n",
        "plt.legend()\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot([m[\"linf_norm\"] for m in train_metrics], label=\"Train Linf Norm\")\n",
        "plt.plot([m[\"linf_norm\"] for m in test_metrics], label=\"Test Linf Norm\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Linf Norm\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-3, 1e2)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Added plotting MSE of training data and MSE of test data in one plot \n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(train_losses,label=\"training data\")\n",
        "plt.plot(test_losses,label=\"test data\")\n",
        "#if scheduler is not None:\n",
        "#    plt.plot([scheduler.get_last_lr()[0] for _ in range(n_epochs)], label=\"Learning rate\") \n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(f\"{loss_name} Loss\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-7, 1e0)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiEDutxIUZig"
      },
      "source": [
        "## Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7Mj990wUZih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5813379-be85-489d-fa1a-5e8e3af92406"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# load the dictionary from the .json file\n",
        "with open(\"var_dict.json\", \"r\") as f:\n",
        "  var_dict_loaded = json.load(f)\n",
        "\n",
        "# extract the variables from the dictionary\n",
        "batch_size_loaded = var_dict_loaded[\"batch_size\"]\n",
        "n_epochs_loaded = var_dict_loaded[\"n_epochs\"]\n",
        "loss_name_loaded = var_dict_loaded[\"loss_name\"]\n",
        "optimizer_name_loaded = var_dict_loaded[\"optimizer_name\"]\n",
        "scheduler_name_loaded = var_dict_loaded[\"scheduler_name\"]\n",
        "n_units_loaded = var_dict_loaded[\"n_units\"]\n",
        "n_layers_loaded = var_dict_loaded[\"n_layers\"]\n",
        "hidden_activation_name_loaded = var_dict_loaded[\"hidden_activation_name\"]\n",
        "output_activation_name_loaded = var_dict_loaded[\"output_activation_name\"]\n",
        "lr_loaded = var_dict_loaded[\"lr\"]\n",
        "dropout_rate_loaded = var_dict_loaded[\"dropout_rate\"]\n",
        "\n",
        "# Loading the activation functions with subparameters\n",
        "if hidden_activation_name_loaded == \"ReLU\":\n",
        "  hidden_activation_loaded = nn.ReLU()\n",
        "elif hidden_activation_name_loaded == \"LeakyReLU\":\n",
        "  negative_slope_loaded = var_dict_loaded.get(\"leakyrelu_slope\", 0.01)  # For backwards compatibiltiy with earlier models; default to 0.01 if not present\n",
        "  hidden_activation_loaded = nn.LeakyReLU(negative_slope=negative_slope_loaded)\n",
        "elif hidden_activation_name_loaded == \"ELU\":\n",
        "  hidden_activation_loaded = nn.ELU()\n",
        "elif hidden_activation_name_loaded == \"PReLU\":\n",
        "  init_loaded = var_dict_loaded.get(\"prelu_init\", 0.25)  # Default to 0.25 if not present\n",
        "  hidden_activation_loaded = nn.PReLU(init=init_loaded)\n",
        "elif hidden_activation_name_loaded == \"Swish\":\n",
        "    class Swish(nn.Module):\n",
        "        def forward(self, x):\n",
        "            return x * torch.sigmoid(x)\n",
        "    hidden_activation_loaded = Swish()\n",
        "elif hidden_activation_name_loaded == \"GELU\":\n",
        "    hidden_activation_loaded = nn.GELU()\n",
        "elif hidden_activation_name_loaded == \"SoftPlus\":\n",
        "  beta_loaded = var_dict_loaded.get(\"softplus_beta\", 1)  # Default to 1 if not present\n",
        "  hidden_activation_loaded = nn.Softplus(beta=beta_loaded)\n",
        "\n",
        "\n",
        "\n",
        "# We used to have options here, but since we have a regression problem with continuous output, we only use Linear.\n",
        "output_activation_loaded = nn.Identity()\n",
        "\n",
        "# load the network from the .pth file\n",
        "net_loaded = Net(\n",
        "    n_layers_loaded, \n",
        "    n_units_loaded, \n",
        "    hidden_activation_loaded, \n",
        "    output_activation_loaded, \n",
        "    dropout_rate_loaded\n",
        ").to(device)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        " net_loaded.load_state_dict(torch.load(\"net.pth\"))\n",
        "else: \n",
        "  net_loaded.load_state_dict(torch.load(\"net.pth\", map_location=torch.device('cpu')))\n",
        "\n",
        "\n",
        "# Creating the loss function from its name\n",
        "if loss_name_loaded == \"MSE\":\n",
        "    loss_fn_loaded = nn.MSELoss()\n",
        "elif loss_name_loaded == \"MAE\":\n",
        "    loss_fn_loaded = nn.L1Loss()\n",
        "elif loss_name_loaded == \"Huber\":\n",
        "    loss_fn_loaded = nn.SmoothL1Loss() \n",
        "elif loss_name_loaded == \"Quantile\":\n",
        "    def quantile_loss(y_pred, y_true, q=0.5):\n",
        "        e = y_pred - y_true\n",
        "        return torch.mean(torch.max(q*e, (q-1)*e))\n",
        "    loss_fn_loaded = quantile_loss\n",
        "\n",
        "# load the optimizer from the .pth file\n",
        "if torch.cuda.is_available():\n",
        "  optimizer_loaded_state_dict = torch.load(\"optimizer.pth\")\n",
        "else:\n",
        "  optimizer_loaded_state_dict = torch.load(\"optimizer.pth\", map_location=torch.device('cpu'))\n",
        "\n",
        "if optimizer_name_loaded == \"SGD\":\n",
        "  # Added getting the weight decay and momentum parameters from the state dict\n",
        "  weight_decay_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"weight_decay\"]\n",
        "  momentum_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"momentum\"]\n",
        "  optimizer_loaded = optim.SGD(net_loaded.parameters(), lr=lr_loaded, weight_decay=weight_decay_loaded, momentum=momentum_loaded)\n",
        "elif optimizer_name_loaded == \"Adam\":\n",
        "  # Added getting the weight decay and beta parameters from the state dict\n",
        "  weight_decay_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"weight_decay\"]\n",
        "  beta1_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"betas\"][0]\n",
        "  beta2_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"betas\"][1]\n",
        "  optimizer_loaded = optim.Adam(net_loaded.parameters(), lr=lr_loaded, weight_decay=weight_decay_loaded, betas=(beta1_loaded, beta2_loaded))\n",
        "elif optimizer_name_loaded == \"RMSprop\":\n",
        "  optimizer_loaded = optim.RMSprop(net_loaded.parameters(), lr=lr_loaded)\n",
        "elif optimizer_name_loaded == \"Adagrad\":\n",
        "  # Added loading the Adagrad optimizer\n",
        "  optimizer_loaded = optim.Adagrad(net_loaded.parameters(), lr=lr_loaded)\n",
        "optimizer_loaded.load_state_dict(optimizer_loaded_state_dict)\n",
        "\n",
        "# load the scheduler from the .pth file\n",
        "if torch.cuda.is_available():\n",
        "  scheduler_loaded_state_dict = torch.load(\"scheduler.pth\")\n",
        "else: \n",
        "  scheduler_loaded_state_dict = torch.load(\"scheduler.pth\", map_location=torch.device('cpu'))\n",
        "\n",
        "if scheduler_name_loaded == \"StepLR\":\n",
        "  # Added getting the step_size and gamma parameters from the state dict\n",
        "  step_size_loaded = scheduler_loaded_state_dict[\"step_size\"]\n",
        "  gamma_loaded = scheduler_loaded_state_dict[\"gamma\"]\n",
        "  scheduler_loaded = optim.lr_scheduler.StepLR(optimizer_loaded, step_size=step_size_loaded, gamma=gamma_loaded)\n",
        "elif scheduler_name_loaded == \"ExponentialLR\":\n",
        "  # Added getting the gamma parameter from the state dict\n",
        "  gamma_loaded = scheduler_loaded_state_dict[\"gamma\"]\n",
        "  scheduler_loaded = optim.lr_scheduler.ExponentialLR(optimizer_loaded, gamma=gamma_loaded)\n",
        "elif scheduler_name_loaded == \"CosineAnnealingLR\":\n",
        "  # Added getting the T_max parameter from the state dict\n",
        "  T_max_loaded = scheduler_loaded_state_dict[\"T_max\"]\n",
        "  scheduler_loaded = optim.lr_scheduler.CosineAnnealingLR(optimizer_loaded, T_max=T_max_loaded)\n",
        "elif scheduler_name_loaded == \"ReduceLROnPlateau\":\n",
        "  # Added getting the mode, factor, patience, threshold and min_lr parameters from the state dict\n",
        "  mode_loaded = scheduler_loaded_state_dict[\"mode\"]\n",
        "  factor_loaded = scheduler_loaded_state_dict[\"factor\"]\n",
        "  patience_loaded = scheduler_loaded_state_dict[\"patience\"]\n",
        "  threshold_loaded = scheduler_loaded_state_dict[\"threshold\"]\n",
        "  min_lr_loaded = scheduler_loaded_state_dict[\"min_lrs\"][0]\n",
        "  scheduler_loaded = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                    optimizer_loaded, mode=mode_loaded, factor=factor_loaded, patience=patience_loaded, threshold=threshold_loaded, min_lr=min_lr_loaded\n",
        "                )\n",
        "elif scheduler_name_loaded == \"CyclicLR\":  # Added CyclicLR\n",
        "    base_lr_loaded = scheduler_loaded_state_dict[\"base_lr\"]\n",
        "    max_lr_loaded = scheduler_loaded_state_dict[\"max_lr\"]\n",
        "    step_size_up_loaded = scheduler_loaded_state_dict[\"step_size_up\"]\n",
        "    scheduler_loaded = optim.lr_scheduler.CyclicLR(\n",
        "                    optimizer, base_lr=base_lr_loaded, max_lr=max_lr_loaded, step_size_up=step_size_up_loaded\n",
        "                )\n",
        "else:\n",
        "  scheduler_loaded = None\n",
        "\n",
        "if scheduler_loaded is not None:\n",
        "  # Added loading the state dict to the scheduler_loaded\n",
        "  scheduler_loaded.load_state_dict(scheduler_loaded_state_dict)\n",
        "\n",
        "# Loading the output of the training using pandas\n",
        "train_df_loaded = pd.read_csv(\"train_output.csv\")\n",
        "train_losses_loaded = train_df_loaded[\"train_loss\"].tolist()\n",
        "test_losses_loaded = train_df_loaded[\"test_loss\"].tolist()\n",
        "train_metrics_loaded = [\n",
        "    {\n",
        "        \"l1_norm\": train_df_loaded[\"train_l1_norm\"][i],\n",
        "        \"linf_norm\": train_df_loaded[\"train_linf_norm\"][i],\n",
        "    }\n",
        "    for i in range(len(train_df_loaded))\n",
        "]\n",
        "test_metrics_loaded = [\n",
        "    {\n",
        "        \"l1_norm\": train_df_loaded[\"test_l1_norm\"][i],\n",
        "        \"linf_norm\": train_df_loaded[\"test_linf_norm\"][i],\n",
        "    }\n",
        "    for i in range(len(train_df_loaded))\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlZ11zTlyUfg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aeb05dc2-b604-4efd-e1d6-cb510cf96b30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {},
          "execution_count": 90
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "143"
            ]
          },
          "metadata": {},
          "execution_count": 90
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'MSE'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Adam'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ReduceLROnPlateau'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3558, 165, 3906, 434, 239, 1274]"
            ]
          },
          "metadata": {},
          "execution_count": 90
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 90
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PReLU'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Identity'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0435149104684462e-06"
            ]
          },
          "metadata": {},
          "execution_count": 90
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06388872518840581"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "# %%script echo skipping\n",
        "\n",
        "var_dict_loaded[\"batch_size\"]\n",
        "var_dict_loaded[\"n_epochs\"]\n",
        "var_dict_loaded[\"loss_name\"]\n",
        "var_dict_loaded[\"optimizer_name\"]\n",
        "var_dict_loaded[\"scheduler_name\"]\n",
        "var_dict_loaded[\"n_units\"]\n",
        "var_dict_loaded[\"n_layers\"]\n",
        "var_dict_loaded[\"hidden_activation_name\"]\n",
        "var_dict_loaded[\"output_activation_name\"]\n",
        "var_dict_loaded[\"lr\"]\n",
        "var_dict_loaded[\"dropout_rate\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jissHfjkyUfg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72ea49e3-4c13-4a31-8762-61f5c044e392"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "scheduler_loaded_state_dict[\"mode\"]\n",
        "scheduler_loaded_state_dict[\"factor\"]\n",
        "scheduler_loaded_state_dict[\"patience\"]\n",
        "scheduler_loaded_state_dict[\"threshold\"]\n",
        "scheduler_loaded_state_dict[\"min_lrs\"]\n",
        "\n",
        "\n",
        "scheduler_loaded_state_dict[\"base_lr\"]\n",
        "scheduler_loaded_state_dict[\"max_lr\"]\n",
        "scheduler_loaded_state_dict[\"step_size_up\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QTzUSU6yUfh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "d93459bc-350a-47f2-ca53-052be94501f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 1024,\n",
              " 'n_epochs': 143,\n",
              " 'loss_name': 'MSE',\n",
              " 'optimizer_name': 'Adam',\n",
              " 'scheduler_name': 'ReduceLROnPlateau',\n",
              " 'n_units': [3558, 165, 3906, 434, 239, 1274],\n",
              " 'n_layers': 6,\n",
              " 'hidden_activation_name': 'PReLU',\n",
              " 'output_activation_name': 'Identity',\n",
              " 'lr': 1.0435149104684462e-06,\n",
              " 'dropout_rate': 0.06388872518840581,\n",
              " 'prelu_init': 0.20173096656799316}"
            ]
          },
          "metadata": {},
          "execution_count": 92
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PReLU'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "# %%script echo skipping\n",
        "\n",
        "var_dict\n",
        "hidden_activation_name_loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ_fcj7zUZii",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aa4e26ca-778b-41e8-e38b-9577a5b40e62"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.20173096656799316"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "143"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'MSE'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Adam'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ReduceLROnPlateau'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3558, 165, 3906, 434, 239, 1274]"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PReLU'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Identity'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0435149104684462e-06"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06388872518840581"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PReLU(num_parameters=1)"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Identity()"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (hidden_activation): PReLU(num_parameters=1)\n",
              "  (output_activation): Identity()\n",
              "  (layers): ModuleList(\n",
              "    (0): Linear(in_features=14, out_features=3558, bias=True)\n",
              "    (1): Linear(in_features=3558, out_features=165, bias=True)\n",
              "    (2): Linear(in_features=165, out_features=3906, bias=True)\n",
              "    (3): Linear(in_features=3906, out_features=434, bias=True)\n",
              "    (4): Linear(in_features=434, out_features=239, bias=True)\n",
              "    (5): Linear(in_features=239, out_features=1274, bias=True)\n",
              "    (6): Linear(in_features=1274, out_features=1, bias=True)\n",
              "  )\n",
              "  (dropouts): ModuleList(\n",
              "    (0-5): 6 x Dropout(p=0.06388872518840581, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'training': True,\n",
              " '_parameters': OrderedDict(),\n",
              " '_buffers': OrderedDict(),\n",
              " '_non_persistent_buffers_set': set(),\n",
              " '_backward_pre_hooks': OrderedDict(),\n",
              " '_backward_hooks': OrderedDict(),\n",
              " '_is_full_backward_hook': None,\n",
              " '_forward_hooks': OrderedDict(),\n",
              " '_forward_hooks_with_kwargs': OrderedDict(),\n",
              " '_forward_pre_hooks': OrderedDict(),\n",
              " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
              " '_state_dict_hooks': OrderedDict(),\n",
              " '_state_dict_pre_hooks': OrderedDict(),\n",
              " '_load_state_dict_pre_hooks': OrderedDict(),\n",
              " '_load_state_dict_post_hooks': OrderedDict(),\n",
              " '_modules': OrderedDict([('hidden_activation', PReLU(num_parameters=1)),\n",
              "              ('output_activation', Identity()),\n",
              "              ('layers',\n",
              "               ModuleList(\n",
              "                 (0): Linear(in_features=14, out_features=3558, bias=True)\n",
              "                 (1): Linear(in_features=3558, out_features=165, bias=True)\n",
              "                 (2): Linear(in_features=165, out_features=3906, bias=True)\n",
              "                 (3): Linear(in_features=3906, out_features=434, bias=True)\n",
              "                 (4): Linear(in_features=434, out_features=239, bias=True)\n",
              "                 (5): Linear(in_features=239, out_features=1274, bias=True)\n",
              "                 (6): Linear(in_features=1274, out_features=1, bias=True)\n",
              "               )),\n",
              "              ('dropouts',\n",
              "               ModuleList(\n",
              "                 (0-5): 6 x Dropout(p=0.06388872518840581, inplace=False)\n",
              "               ))]),\n",
              " 'n_layers': 6,\n",
              " 'n_units': [3558, 165, 3906, 434, 239, 1274],\n",
              " 'dropout_rate': 0.06388872518840581}"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MSELoss()"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.939104753876406, 0.9991856135686923)\n",
              "    capturable: False\n",
              "    differentiable: False\n",
              "    eps: 1e-08\n",
              "    foreach: None\n",
              "    fused: None\n",
              "    lr: 1.0435149104684462e-06\n",
              "    maximize: False\n",
              "    weight_decay: 0.0009246275300019842\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'defaults': {'lr': 1.0435149104684462e-06,\n",
              "  'betas': (0.939104753876406, 0.9991856135686923),\n",
              "  'eps': 1e-08,\n",
              "  'weight_decay': 0.0009246275300019842,\n",
              "  'amsgrad': False,\n",
              "  'maximize': False,\n",
              "  'foreach': None,\n",
              "  'capturable': False,\n",
              "  'differentiable': False,\n",
              "  'fused': None},\n",
              " '_optimizer_step_pre_hooks': OrderedDict(),\n",
              " '_optimizer_step_post_hooks': OrderedDict(),\n",
              " '_zero_grad_profile_name': 'Optimizer.zero_grad#Adam.zero_grad',\n",
              " 'state': defaultdict(dict,\n",
              "             {Parameter containing:\n",
              "              tensor([0.2017], device='cuda:0', requires_grad=True): {'step': tensor(12012.),\n",
              "               'exp_avg': tensor([-4.1726], device='cuda:0'),\n",
              "               'exp_avg_sq': tensor([9821984.], device='cuda:0')},\n",
              "              Parameter containing:\n",
              "              tensor([[ 0.1714, -0.2086, -0.0275,  ..., -0.0289,  0.2055,  0.0674],\n",
              "                      [-0.1348,  0.1487,  0.1684,  ..., -0.0689, -0.1292,  0.2476],\n",
              "                      [ 0.0141, -0.2602, -0.0598,  ..., -0.0233, -0.1510, -0.1562],\n",
              "                      ...,\n",
              "                      [ 0.0950,  0.2630,  0.2416,  ...,  0.0394,  0.1964,  0.0878],\n",
              "                      [ 0.0915, -0.0232,  0.2064,  ..., -0.1801,  0.0730,  0.1871],\n",
              "                      [ 0.0377, -0.2164, -0.0513,  ..., -0.1552,  0.0815,  0.1792]],\n",
              "                     device='cuda:0', requires_grad=True): {'step': tensor(12012.),\n",
              "               'exp_avg': tensor([[-3.2700e-04,  3.0904e-02,  4.6735e-01,  ..., -4.0351e-04,\n",
              "                         1.7145e-04, -3.1579e-04],\n",
              "                       [ 4.6759e-04,  1.7621e-02, -1.9557e-02,  ...,  1.3970e-04,\n",
              "                        -1.0952e-04,  4.3259e-04],\n",
              "                       [ 2.1510e-03,  1.4096e-01,  5.8833e-01,  ...,  5.2644e-04,\n",
              "                        -1.1187e-04,  4.0292e-04],\n",
              "                       ...,\n",
              "                       [ 8.2701e-04,  7.2764e-03, -1.4708e-01,  ...,  2.7596e-04,\n",
              "                         1.9267e-04,  3.2004e-04],\n",
              "                       [ 6.9562e-04,  1.6870e-02,  7.3013e-02,  ..., -1.6642e-05,\n",
              "                         7.4941e-05,  3.2282e-04],\n",
              "                       [ 9.3203e-05,  2.1040e-03,  1.0996e-03,  ..., -1.0192e-04,\n",
              "                         7.7488e-05,  2.0753e-04]], device='cuda:0'),\n",
              "               'exp_avg_sq': tensor([[2.0157e-02, 9.4602e+02, 7.7194e+04,  ..., 1.7675e-07, 2.9782e-08,\n",
              "                        1.1292e-07],\n",
              "                       [2.4694e-04, 1.1553e+01, 9.4384e+02,  ..., 2.1279e-08, 1.2245e-08,\n",
              "                        1.9011e-07],\n",
              "                       [4.1510e-03, 1.9412e+02, 1.5844e+04,  ..., 3.1258e-07, 1.2854e-08,\n",
              "                        1.9428e-07],\n",
              "                       ...,\n",
              "                       [3.1914e-03, 1.4972e+02, 1.2214e+04,  ..., 6.8630e-08, 3.7405e-08,\n",
              "                        9.3449e-08],\n",
              "                       [5.2365e-03, 2.4577e+02, 2.0049e+04,  ..., 1.1388e-08, 5.2955e-09,\n",
              "                        6.9852e-08],\n",
              "                       [2.3600e-04, 1.1078e+01, 9.0349e+02,  ..., 1.2996e-08, 6.1059e-09,\n",
              "                        3.9951e-08]], device='cuda:0')},\n",
              "              Parameter containing:\n",
              "              tensor([-0.0751, -0.2434,  0.0402,  ...,  0.1357, -0.0321, -0.1180],\n",
              "                     device='cuda:0', requires_grad=True): {'step': tensor(12012.),\n",
              "               'exp_avg': tensor([-4.4822e-04, -2.1086e-05,  5.8509e-04,  ...,  3.6463e-04,\n",
              "                        1.2027e-04, -6.7040e-05], device='cuda:0'),\n",
              "               'exp_avg_sq': tensor([2.1517e-07, 1.7043e-09, 3.8191e-07,  ..., 1.2203e-07, 6.8965e-09,\n",
              "                       6.4326e-09], device='cuda:0')},\n",
              "              Parameter containing:\n",
              "              tensor([[-0.0106,  0.0039, -0.0041,  ..., -0.0065,  0.0139, -0.0117],\n",
              "                      [-0.0014,  0.0015,  0.0052,  ...,  0.0156,  0.0045,  0.0103],\n",
              "                      [ 0.0160, -0.0106,  0.0124,  ...,  0.0112,  0.0148,  0.0165],\n",
              "                      ...,\n",
              "                      [-0.0047,  0.0093, -0.0130,  ..., -0.0110, -0.0068, -0.0029],\n",
              "                      [ 0.0148,  0.0095, -0.0126,  ..., -0.0108, -0.0073,  0.0041],\n",
              "                      [ 0.0092, -0.0091, -0.0096,  ...,  0.0011, -0.0060,  0.0016]],\n",
              "                     device='cuda:0', requires_grad=True): {'step': tensor(12012.),\n",
              "               'exp_avg': tensor([[-0.1383,  0.0042, -0.5897,  ..., -0.1097, -0.3997, -0.0837],\n",
              "                       [-0.0110, -0.0035, -0.0238,  ..., -0.0014, -0.0223, -0.0178],\n",
              "                       [-0.0815, -0.0214, -0.0544,  ..., -0.0093, -0.1544, -0.1215],\n",
              "                       ...,\n",
              "                       [-0.0409, -0.0186,  0.0281,  ...,  0.0232, -0.0467, -0.0814],\n",
              "                       [ 0.0467, -0.0229,  0.4195,  ...,  0.1150,  0.2106, -0.0487],\n",
              "                       [ 0.0011,  0.0032, -0.0029,  ...,  0.0057,  0.0077,  0.0081]],\n",
              "                      device='cuda:0'),\n",
              "               'exp_avg_sq': tensor([[8.9146e+02, 2.6107e+01, 3.1322e+04,  ..., 2.0108e+03, 1.1420e+04,\n",
              "                        4.6684e+03],\n",
              "                       [3.4062e+01, 1.1832e+00, 1.2173e+03,  ..., 9.9297e+01, 5.1098e+02,\n",
              "                        1.5733e+02],\n",
              "                       [1.3627e+03, 4.0595e+01, 4.4801e+04,  ..., 2.7791e+03, 1.0576e+04,\n",
              "                        6.5666e+03],\n",
              "                       ...,\n",
              "                       [1.2638e+03, 3.9917e+01, 4.2230e+04,  ..., 3.0295e+03, 1.7633e+04,\n",
              "                        6.5363e+03],\n",
              "                       [1.1046e+03, 3.0723e+01, 3.7511e+04,  ..., 2.5072e+03, 1.1794e+04,\n",
              "                        5.6623e+03],\n",
              "                       [2.6757e+01, 9.9321e-01, 1.1344e+03,  ..., 8.0307e+01, 4.4280e+02,\n",
              "                        1.7000e+02]], device='cuda:0')},\n",
              "              Parameter containing:\n",
              "              tensor([-0.0088, -0.0120, -0.0203,  0.0120,  0.0028,  0.0135,  0.0199, -0.0234,\n",
              "                       0.0095,  0.0150,  0.0055,  0.0213, -0.0063,  0.0171, -0.0009,  0.0216,\n",
              "                      -0.0017, -0.0179,  0.0171, -0.0160,  0.0194, -0.0197,  0.0034, -0.0215,\n",
              "                      -0.0241,  0.0031,  0.0063, -0.0204,  0.0042,  0.0061,  0.0248,  0.0085,\n",
              "                       0.0207,  0.0006, -0.0094, -0.0177, -0.0045, -0.0162, -0.0062, -0.0017,\n",
              "                       0.0017,  0.0127, -0.0273,  0.0057,  0.0036, -0.0268, -0.0224,  0.0032,\n",
              "                      -0.0073,  0.0132,  0.0221,  0.0085,  0.0077,  0.0144,  0.0211, -0.0036,\n",
              "                       0.0152, -0.0072,  0.0016, -0.0064, -0.0176,  0.0154,  0.0016,  0.0166,\n",
              "                       0.0051, -0.0153,  0.0106,  0.0081, -0.0186, -0.0081,  0.0119, -0.0081,\n",
              "                       0.0034, -0.0049,  0.0007,  0.0181,  0.0035, -0.0021, -0.0017,  0.0201,\n",
              "                      -0.0048,  0.0198, -0.0153,  0.0036,  0.0087,  0.0131, -0.0165, -0.0017,\n",
              "                       0.0188,  0.0120, -0.0249,  0.0127, -0.0035,  0.0219,  0.0053,  0.0078,\n",
              "                       0.0205, -0.0078, -0.0035,  0.0124,  0.0008,  0.0255,  0.0073,  0.0007,\n",
              "                       0.0018, -0.0190,  0.0145, -0.0150,  0.0136, -0.0152,  0.0039, -0.0001,\n",
              "                      -0.0053,  0.0060,  0.0133, -0.0175,  0.0040, -0.0051, -0.0111,  0.0063,\n",
              "                       0.0042,  0.0117, -0.0055, -0.0130, -0.0064,  0.0092,  0.0009, -0.0125,\n",
              "                      -0.0197, -0.0169,  0.0042, -0.0083,  0.0057,  0.0211,  0.0142, -0.0072,\n",
              "                      -0.0219, -0.0241,  0.0171,  0.0041,  0.0232,  0.0049,  0.0160, -0.0143,\n",
              "                      -0.0226,  0.0076, -0.0076, -0.0014,  0.0044, -0.0098, -0.0075, -0.0019,\n",
              "                       0.0061, -0.0059,  0.0211, -0.0086, -0.0138, -0.0035,  0.0104, -0.0025,\n",
              "                      -0.0007, -0.0189, -0.0014,  0.0025,  0.0040], device='cuda:0',\n",
              "                     requires_grad=True): {'step': tensor(12012.),\n",
              "               'exp_avg': tensor([ 5.2673e-03,  1.0537e-03,  2.2077e-03,  2.6548e-04,  6.0177e-05,\n",
              "                       -5.0699e-04, -3.8383e-03,  1.9411e-03, -1.2558e-03, -6.0684e-03,\n",
              "                        3.6886e-03, -1.4441e-03, -1.0449e-03, -3.1249e-03, -8.8932e-03,\n",
              "                        6.8912e-04, -2.9127e-03,  2.7975e-04, -6.0958e-03, -5.1626e-05,\n",
              "                       -7.8973e-03,  2.5422e-03, -9.0481e-04,  1.6840e-03,  7.9189e-04,\n",
              "                       -2.3684e-03, -4.2953e-04,  3.1962e-03, -9.4354e-03, -2.3177e-03,\n",
              "                       -5.8576e-04, -2.8588e-03,  9.0554e-05,  6.9053e-05,  2.6942e-04,\n",
              "                       -1.3299e-03,  5.9795e-05,  2.9663e-05,  2.6905e-03, -3.8958e-04,\n",
              "                       -2.1402e-03,  2.0368e-04,  5.3443e-03, -1.2723e-03, -6.7055e-03,\n",
              "                        1.0109e-02,  3.4701e-03,  5.1324e-03,  8.0815e-04, -5.0150e-03,\n",
              "                       -1.8082e-03, -1.1332e-03, -1.2098e-02, -7.6491e-03, -3.0948e-03,\n",
              "                        1.9735e-05, -7.0166e-04,  1.2061e-02, -1.1278e-03, -3.9234e-03,\n",
              "                        1.5463e-03, -5.6891e-04, -1.6370e-03, -7.5089e-03, -1.3540e-03,\n",
              "                        2.0414e-03, -9.6253e-04, -5.9845e-03,  9.2484e-04,  3.1503e-03,\n",
              "                       -1.0595e-03, -1.3769e-04,  3.4076e-04,  1.0262e-03,  2.0533e-03,\n",
              "                        4.1265e-04,  2.1110e-03,  1.0519e-03, -6.1570e-03, -1.5835e-03,\n",
              "                        1.2792e-03, -2.6262e-03,  7.0891e-03, -3.3932e-04, -1.1745e-03,\n",
              "                       -9.2400e-04,  7.2685e-04, -2.6225e-03,  5.1175e-05, -2.0268e-03,\n",
              "                       -6.8348e-05, -1.5794e-03,  1.3567e-03, -6.3298e-03, -1.5178e-03,\n",
              "                        3.4647e-04, -4.3408e-03, -1.7524e-05,  7.1141e-06, -2.6397e-04,\n",
              "                        1.5447e-03, -8.5723e-04,  1.4352e-03,  9.2289e-04, -2.4443e-04,\n",
              "                        5.5160e-03,  9.1892e-05,  7.6604e-05, -3.9823e-03,  2.4677e-03,\n",
              "                        2.2324e-03, -3.3792e-03,  3.2372e-03,  6.3505e-04, -5.6782e-04,\n",
              "                       -1.7537e-04,  4.3905e-03,  4.3769e-03,  4.4496e-04,  1.9942e-03,\n",
              "                        1.7979e-04, -2.8772e-03, -4.9537e-04,  1.5096e-03, -4.0114e-04,\n",
              "                       -2.1136e-03,  3.9368e-03,  5.0455e-04,  9.3083e-04,  2.6020e-03,\n",
              "                        7.4970e-06,  1.2701e-03, -3.5165e-04, -6.9877e-03, -6.1182e-04,\n",
              "                        3.5831e-03,  4.0681e-03,  1.3443e-03, -2.6182e-03,  2.3198e-03,\n",
              "                        5.4723e-05,  2.0118e-03, -1.0305e-03,  1.3188e-03,  1.7035e-03,\n",
              "                       -8.1129e-04, -1.9698e-05,  1.8376e-04, -1.3490e-03, -4.0103e-04,\n",
              "                        3.6837e-03, -5.9128e-04,  7.6614e-05,  3.6026e-03, -1.4309e-03,\n",
              "                       -2.2759e-05,  4.4760e-03, -6.3327e-04,  1.1251e-03,  3.1988e-04,\n",
              "                        3.0610e-03,  1.1033e-03,  4.8836e-03,  3.3316e-03, -2.7492e-03],\n",
              "                      device='cuda:0'),\n",
              "               'exp_avg_sq': tensor([1.4984e-05, 1.3273e-06, 4.0625e-06, 7.3785e-08, 8.4869e-08, 1.0891e-07,\n",
              "                       1.8009e-05, 5.7542e-06, 1.0833e-06, 2.9454e-05, 1.2136e-05, 3.9789e-06,\n",
              "                       1.2437e-06, 1.8305e-05, 7.7336e-05, 4.1952e-07, 6.3556e-06, 3.8535e-07,\n",
              "                       4.5157e-05, 1.6676e-07, 3.5706e-05, 7.6947e-06, 1.0363e-06, 2.9065e-06,\n",
              "                       1.0770e-06, 2.5287e-06, 2.0612e-06, 1.7494e-05, 6.8038e-05, 2.1724e-06,\n",
              "                       8.3416e-07, 2.4793e-05, 4.3989e-07, 2.3336e-06, 8.5082e-07, 3.0111e-06,\n",
              "                       9.2700e-08, 2.0757e-07, 6.7182e-06, 7.8883e-07, 5.9931e-06, 4.7785e-07,\n",
              "                       4.0164e-05, 1.6095e-06, 4.3446e-05, 1.2376e-04, 9.5674e-06, 1.6985e-05,\n",
              "                       5.1002e-07, 2.4409e-05, 3.4122e-06, 1.4319e-06, 1.3888e-04, 8.9035e-05,\n",
              "                       1.4410e-05, 5.9824e-08, 8.2393e-07, 1.0872e-04, 9.2532e-07, 7.7538e-06,\n",
              "                       5.9511e-06, 1.8221e-06, 4.0704e-06, 7.3146e-05, 5.1743e-06, 6.5074e-06,\n",
              "                       2.2137e-06, 3.6722e-05, 1.2561e-06, 1.0637e-05, 9.9512e-06, 3.0650e-07,\n",
              "                       9.5272e-08, 1.2121e-06, 6.4625e-06, 1.7559e-06, 3.6337e-06, 1.4850e-06,\n",
              "                       6.1427e-05, 2.6478e-06, 2.0218e-06, 5.9877e-06, 4.5972e-05, 2.9387e-07,\n",
              "                       2.3966e-06, 4.9202e-07, 9.6662e-07, 7.8190e-06, 1.2247e-07, 2.5134e-06,\n",
              "                       3.1912e-06, 3.8066e-06, 2.0113e-06, 5.4120e-05, 1.4375e-06, 1.2101e-07,\n",
              "                       2.1608e-05, 3.1906e-06, 3.6963e-07, 4.4955e-07, 8.1109e-07, 4.5951e-07,\n",
              "                       2.7054e-06, 3.5023e-06, 1.9374e-07, 2.3108e-05, 2.9131e-06, 5.8649e-08,\n",
              "                       6.3160e-06, 5.8689e-06, 5.5749e-06, 7.7940e-06, 1.3732e-05, 3.5689e-06,\n",
              "                       9.5003e-07, 1.0806e-06, 1.4282e-05, 1.0497e-05, 8.2328e-07, 3.4810e-06,\n",
              "                       9.7517e-07, 4.6722e-06, 4.3808e-07, 7.9286e-06, 5.5877e-07, 7.7092e-06,\n",
              "                       1.0487e-05, 2.0609e-07, 1.1670e-06, 7.4649e-06, 3.3457e-07, 8.3277e-06,\n",
              "                       1.4011e-07, 4.7806e-05, 2.9142e-07, 1.6151e-05, 1.0743e-05, 1.7663e-06,\n",
              "                       9.1509e-06, 1.0545e-05, 9.3681e-07, 5.6417e-06, 1.6500e-06, 1.8919e-06,\n",
              "                       1.9211e-06, 1.0694e-05, 1.6986e-07, 4.5943e-08, 1.4280e-06, 1.2779e-06,\n",
              "                       1.7583e-05, 5.4485e-07, 6.0521e-08, 1.5831e-05, 2.3737e-06, 1.4845e-06,\n",
              "                       2.6437e-05, 3.5950e-07, 1.4835e-06, 8.7742e-08, 1.7106e-05, 1.7402e-06,\n",
              "                       3.0860e-05, 1.0543e-05, 9.4572e-06], device='cuda:0')},\n",
              "              Parameter containing:\n",
              "              tensor([[ 0.0728,  0.0024,  0.0557,  ..., -0.0147, -0.0591, -0.0098],\n",
              "                      [ 0.0333, -0.0365, -0.0048,  ..., -0.0680,  0.0217,  0.0010],\n",
              "                      [ 0.0679,  0.0396,  0.0450,  ..., -0.0432,  0.0165,  0.0549],\n",
              "                      ...,\n",
              "                      [ 0.0465,  0.0120,  0.0610,  ...,  0.0627,  0.0060,  0.0742],\n",
              "                      [ 0.0102, -0.0165, -0.0463,  ...,  0.0601, -0.0433,  0.0691],\n",
              "                      [ 0.0358,  0.0011,  0.0296,  ...,  0.0675,  0.0149, -0.0330]],\n",
              "                     device='cuda:0', requires_grad=True): {'step': tensor(12012.),\n",
              "               'exp_avg': tensor([[ 0.0111, -0.0041,  0.0171,  ..., -0.0078,  0.0182, -0.0025],\n",
              "                       [ 0.0086, -0.0030,  0.0122,  ..., -0.0059,  0.0157, -0.0010],\n",
              "                       [ 0.0288, -0.0106,  0.0433,  ...,  0.0111,  0.0434, -0.0069],\n",
              "                       ...,\n",
              "                       [-0.0053,  0.0020,  0.0016,  ..., -0.0094, -0.0002, -0.0011],\n",
              "                       [ 0.0064, -0.0025,  0.0050,  ...,  0.0046,  0.0086,  0.0007],\n",
              "                       [-0.0349,  0.0146, -0.0599,  ..., -0.0070, -0.0638,  0.0087]],\n",
              "                      device='cuda:0'),\n",
              "               'exp_avg_sq': tensor([[6.5860e+01, 1.2229e+01, 4.8325e+02,  ..., 4.0260e+02, 4.8791e+02,\n",
              "                        2.9582e+00],\n",
              "                       [1.4484e+02, 1.0487e+01, 3.0521e+02,  ..., 1.9250e+02, 3.4044e+02,\n",
              "                        3.6329e+00],\n",
              "                       [6.3831e+01, 9.0880e+00, 2.4683e+02,  ..., 1.6756e+02, 2.3596e+02,\n",
              "                        2.9944e+00],\n",
              "                       ...,\n",
              "                       [6.2867e+01, 8.8075e+00, 2.9448e+02,  ..., 2.1953e+02, 2.5629e+02,\n",
              "                        2.9408e+00],\n",
              "                       [6.4898e+01, 7.8982e+00, 1.9507e+02,  ..., 1.2360e+02, 2.0469e+02,\n",
              "                        1.6225e+00],\n",
              "                       [2.7213e+02, 4.7751e+01, 8.1140e+02,  ..., 4.7455e+02, 1.6724e+03,\n",
              "                        9.8859e+00]], device='cuda:0')},\n",
              "              Parameter containing:\n",
              "              tensor([ 0.0293,  0.0403,  0.0065,  ...,  0.0763, -0.0358,  0.0399],\n",
              "                     device='cuda:0', requires_grad=True): {'step': tensor(12012.),\n",
              "               'exp_avg': tensor([-3.8353e-04,  1.8933e-04, -1.2270e-03,  ..., -5.3998e-05,\n",
              "                        5.1358e-04,  1.0126e-03], device='cuda:0'),\n",
              "               'exp_avg_sq': tensor([1.7595e-07, 4.3964e-08, 2.2951e-06,  ..., 2.9763e-08, 3.0453e-07,\n",
              "                       1.4413e-06], device='cuda:0')},\n",
              "              Parameter containing:\n",
              "              tensor([[ 0.0048,  0.0068, -0.0156,  ..., -0.0128, -0.0045,  0.0130],\n",
              "                      [-0.0050,  0.0063,  0.0004,  ...,  0.0096, -0.0056, -0.0016],\n",
              "                      [ 0.0127, -0.0076, -0.0152,  ...,  0.0049, -0.0079,  0.0032],\n",
              "                      ...,\n",
              "                      [ 0.0131, -0.0054,  0.0022,  ..., -0.0082,  0.0106, -0.0127],\n",
              "                      [-0.0097, -0.0015,  0.0150,  ..., -0.0139,  0.0082,  0.0113],\n",
              "                      [ 0.0072, -0.0033, -0.0096,  ..., -0.0082, -0.0090, -0.0142]],\n",
              "                     device='cuda:0', requires_grad=True): {'step': tensor(12012.),\n",
              "               'exp_avg': tensor([[-0.0489,  0.0044,  0.0114,  ..., -0.0057, -0.0868, -0.0564],\n",
              "                       [ 0.0477,  0.0348,  0.0074,  ...,  0.0623,  0.0524,  0.0452],\n",
              "                       [-0.0226, -0.0184, -0.0173,  ..., -0.0240, -0.0236, -0.0141],\n",
              "                       ...,\n",
              "                       [ 0.1293,  0.1282,  0.1777,  ...,  0.1396,  0.1356,  0.0617],\n",
              "                       [-0.0121, -0.0575, -0.0899,  ..., -0.0513,  0.0073,  0.0218],\n",
              "                       [-0.0041, -0.0028, -0.0329,  ...,  0.0118, -0.0075,  0.0050]],\n",
              "                      device='cuda:0'),\n",
              "               'exp_avg_sq': tensor([[1027.1737,  428.5407,  325.9799,  ...,  824.8548,  760.3669,\n",
              "                         316.0468],\n",
              "                       [ 138.2625,   73.8483,   36.0595,  ...,  110.2665,   88.7780,\n",
              "                          47.7122],\n",
              "                       [  87.3118,   38.1104,   18.5114,  ...,   70.0086,   56.4639,\n",
              "                          30.4369],\n",
              "                       ...,\n",
              "                       [3217.4595, 2758.6841,  955.9703,  ..., 4321.2969, 3284.0024,\n",
              "                        1978.1278],\n",
              "                       [1826.9500, 1275.1285,  732.8843,  ..., 1797.2727, 1962.2670,\n",
              "                         457.8731],\n",
              "                       [3057.9358, 2374.5442, 1242.3093,  ..., 3208.4893, 2735.4871,\n",
              "                         704.1403]], device='cuda:0')},\n",
              "              Parameter containing:\n",
              "              tensor([-1.4644e-02,  1.3843e-02, -1.2527e-02, -1.8397e-02,  2.4651e-02,\n",
              "                      -2.2901e-03,  1.0966e-02, -7.8560e-03,  6.8127e-04,  2.2343e-03,\n",
              "                       8.6920e-03,  3.6325e-04,  4.7929e-04,  1.9350e-02,  2.5472e-02,\n",
              "                       1.3570e-02, -2.2760e-02, -3.0134e-03, -1.3062e-02, -2.5668e-02,\n",
              "                      -1.2565e-02,  2.2170e-02, -2.1142e-02,  1.6898e-02,  9.7861e-03,\n",
              "                      -2.1066e-03, -1.6608e-02,  1.4650e-02,  2.2596e-03, -1.8276e-02,\n",
              "                       2.4887e-03,  1.5826e-03,  1.6918e-02,  2.1985e-02, -5.6207e-03,\n",
              "                       2.0825e-03,  1.7231e-02,  1.8756e-02,  5.6741e-03,  1.3048e-02,\n",
              "                      -1.3364e-02,  2.4034e-02, -1.7756e-02, -9.4156e-04,  1.5844e-02,\n",
              "                       3.6329e-03, -1.7657e-02,  1.0409e-02,  6.7563e-03, -6.1903e-03,\n",
              "                      -6.3573e-04, -2.4493e-02, -3.6173e-03,  2.3673e-02,  1.4898e-02,\n",
              "                      -2.3911e-03, -1.5265e-02,  4.2672e-03, -7.7612e-04,  2.1112e-02,\n",
              "                      -7.0015e-03, -1.1139e-02,  1.2268e-03, -1.6741e-02,  1.7541e-02,\n",
              "                      -1.4529e-02,  2.5273e-02,  1.2298e-02,  2.2361e-02, -1.5622e-02,\n",
              "                      -1.1946e-02, -1.1218e-02, -5.4795e-03, -1.6717e-02,  1.5478e-02,\n",
              "                       1.6456e-02,  5.5651e-03,  1.0748e-02,  7.2967e-03,  2.4066e-02,\n",
              "                       1.7235e-02, -1.9173e-02, -2.2804e-02,  6.4362e-03, -1.4290e-02,\n",
              "                       2.4794e-02,  1.6925e-02,  2.6937e-03,  1.7403e-02, -7.6603e-03,\n",
              "                       2.2308e-02,  1.6655e-02,  1.3902e-02, -7.3759e-03, -2.5543e-02,\n",
              "                      -2.4332e-03, -4.4513e-03,  1.6072e-02, -6.7764e-04, -2.5473e-02,\n",
              "                       3.1916e-04,  4.4499e-03, -1.5601e-02, -2.0498e-02,  1.1919e-03,\n",
              "                      -1.9543e-04,  2.7326e-02,  4.6801e-04, -2.6475e-02,  4.2807e-03,\n",
              "                       1.7919e-02, -9.9845e-03, -2.7001e-02, -2.3675e-02,  1.9171e-03,\n",
              "                      -5.6959e-03, -8.3362e-03, -1.7565e-03,  2.5014e-02, -1.5443e-02,\n",
              "                      -1.5901e-02, -3.5966e-03,  1.9060e-02, -8.9553e-03,  1.6295e-03,\n",
              "                      -1.6863e-03, -2.2393e-02, -2.3305e-02, -8.2628e-03,  1.4140e-03,\n",
              "                      -1.0867e-03, -3.2346e-03,  2.1182e-02, -1.9241e-03,  7.4982e-03,\n",
              "                      -3.3342e-03, -1.7012e-02,  1.1298e-02, -9.5925e-03,  1.5533e-02,\n",
              "                      -1.3206e-03, -1.9074e-02, -2.6056e-02, -1.4066e-02, -1.9910e-03,\n",
              "                       2.3553e-04,  1.7014e-02,  5.2460e-03,  8.8764e-03, -1.5260e-02,\n",
              "                      -2.6359e-02,  5.9548e-03,  1.2944e-03,  2.5762e-02,  1.6239e-02,\n",
              "                       1.1412e-02,  3.7344e-03,  6.9325e-03,  1.3500e-02, -8.3098e-03,\n",
              "                       1.7460e-02,  2.2192e-02, -9.8030e-03,  1.1987e-02, -8.8472e-03,\n",
              "                       5.9974e-04,  3.2075e-03, -2.7225e-03, -1.3326e-02,  2.2168e-02,\n",
              "                      -2.0909e-02, -9.5618e-03, -1.4464e-02, -2.5885e-02, -1.3820e-02,\n",
              "                       8.3289e-03,  5.7389e-03, -2.1209e-02,  9.1357e-03,  1.4905e-03,\n",
              "                      -7.4595e-03,  1.0407e-02,  1.2859e-03,  1.3825e-02,  3.2070e-03,\n",
              "                       8.8182e-03,  7.8281e-03,  1.2127e-03,  1.1035e-02,  8.5492e-03,\n",
              "                      -4.9211e-03, -9.8871e-03,  2.6144e-02, -1.3497e-02,  2.4007e-02,\n",
              "                      -1.9702e-03, -7.2920e-03,  2.2792e-02,  2.0227e-02,  7.7653e-03,\n",
              "                       2.7562e-03, -1.3269e-02,  8.4357e-03,  1.7608e-02, -5.5362e-03,\n",
              "                       2.7428e-02, -4.9257e-03, -9.1144e-03, -6.1610e-03,  3.6534e-03,\n",
              "                      -7.9441e-03, -8.0316e-03,  1.9137e-02,  2.3530e-02, -9.7611e-03,\n",
              "                       9.5943e-04, -4.3924e-03, -5.9950e-03, -2.0498e-02,  1.0096e-02,\n",
              "                       1.0628e-02,  5.8918e-03,  7.0928e-03, -2.3847e-03,  2.6356e-02,\n",
              "                       2.0929e-02, -2.6767e-03,  1.6108e-02, -8.9011e-03, -9.6668e-03,\n",
              "                       2.7426e-03, -8.4323e-03, -5.5784e-03,  1.5348e-02,  9.6576e-03,\n",
              "                       1.9061e-02,  6.4060e-03, -1.5861e-03,  6.4374e-04,  2.1010e-02,\n",
              "                      -1.0807e-02,  9.3299e-03,  9.3520e-03,  3.0722e-03,  1.0389e-02,\n",
              "                       1.7838e-03, -5.8128e-03,  1.1658e-02, -2.7595e-03,  2.1957e-03,\n",
              "                      -2.3064e-03, -4.3018e-03, -1.5722e-02,  5.6121e-03,  8.5441e-04,\n",
              "                      -7.5549e-04,  2.3234e-02, -5.3248e-03,  1.7665e-03, -1.0592e-02,\n",
              "                      -1.2120e-02, -1.3246e-02,  1.0791e-02, -3.1659e-03, -6.5286e-03,\n",
              "                       1.4590e-02, -1.1002e-02, -2.7125e-02, -7.3621e-03,  1.4300e-02,\n",
              "                       1.3117e-02,  1.1664e-02, -2.1498e-02,  9.1874e-03,  2.1436e-03,\n",
              "                      -1.6381e-03, -6.4482e-03, -1.7519e-02,  1.5859e-03,  3.8960e-03,\n",
              "                      -2.2561e-02,  8.3653e-03,  1.1313e-02,  2.6038e-02, -1.1439e-02,\n",
              "                       2.0768e-02, -7.4530e-03,  1.5476e-02, -1.9201e-02,  6.0592e-03,\n",
              "                       3.9559e-03, -1.0961e-02, -2.6849e-02, -1.4753e-03, -9.2095e-03,\n",
              "                       7.9068e-04, -3.0674e-03,  2.4397e-02, -7.3466e-03,  1.4752e-03,\n",
              "                       1.4022e-02,  1.2814e-03, -1.8170e-02,  1.6697e-03,  1.3434e-03,\n",
              "                       2.8009e-02, -6.2157e-03, -2.3876e-03,  2.0979e-03, -2.5089e-02,\n",
              "                       2.5124e-02,  1.7657e-02,  6.8489e-03, -1.7483e-02,  1.3389e-02,\n",
              "                       1.8967e-02,  2.5338e-02, -1.2681e-02, -2.4966e-02, -1.6271e-02,\n",
              "                       1.1686e-02,  2.1885e-02,  9.6003e-03,  2.3377e-02,  5.4437e-03,\n",
              "                       2.3863e-02, -1.2017e-02, -1.9796e-02,  1.1716e-03, -1.0416e-02,\n",
              "                      -2.0184e-02,  2.5519e-04,  7.9147e-03,  3.9813e-03,  1.4158e-02,\n",
              "                       2.5038e-02, -3.9819e-03, -1.7372e-02,  1.1258e-02, -9.5821e-03,\n",
              "                       2.0847e-02, -2.4019e-02, -3.4895e-05, -1.1505e-02, -3.5578e-03,\n",
              "                      -1.3804e-02, -1.3102e-02, -1.6777e-02, -2.3434e-02, -6.4262e-04,\n",
              "                      -1.1724e-02,  2.2873e-02,  7.0825e-03, -4.6588e-03,  1.6240e-02,\n",
              "                      -3.4072e-03, -3.4415e-03, -3.5931e-03, -2.6664e-03, -1.4400e-02,\n",
              "                       1.1790e-02,  7.0499e-03,  8.7596e-03,  2.2459e-02,  3.8535e-03,\n",
              "                       1.1919e-02, -4.3202e-03,  1.5887e-02,  3.1076e-03, -2.0931e-02,\n",
              "                      -3.7907e-03,  8.5344e-03, -2.9826e-03, -4.1587e-03,  6.8159e-03,\n",
              "                       6.7377e-03, -1.5817e-02, -2.2048e-03,  3.2126e-03, -3.0469e-03,\n",
              "                       1.7571e-02, -1.4804e-02,  1.5351e-02,  6.9015e-03, -9.0027e-03,\n",
              "                      -2.0431e-02,  1.9026e-03,  1.7629e-03, -2.3028e-02,  2.2354e-02,\n",
              "                      -5.1609e-03, -5.2296e-04,  8.2431e-03,  1.3441e-02, -1.9158e-02,\n",
              "                      -2.3097e-03, -3.0997e-03, -1.8904e-02,  3.0504e-03, -1.0565e-02,\n",
              "                      -6.8210e-03,  1.3265e-02,  4.5539e-03, -2.6578e-02, -5.3818e-03,\n",
              "                      -1.0341e-02,  1.7870e-02, -1.2073e-02, -2.0424e-02, -1.8491e-03,\n",
              "                      -1.4946e-02,  1.7958e-02,  1.0010e-02, -1.6775e-02,  4.7408e-03,\n",
              "                      -1.9533e-02, -2.2132e-02,  9.5863e-03, -6.1018e-03,  2.7769e-03,\n",
              "                       2.2510e-02, -4.1825e-03,  1.9992e-02,  9.9069e-03, -1.5425e-02,\n",
              "                       7.4287e-03,  2.1685e-02, -1.4970e-02, -1.8587e-02, -8.3793e-03,\n",
              "                      -2.0370e-03, -1.9584e-02,  4.5874e-05,  2.4449e-02], device='cuda:0',\n",
              "                     requires_grad=True): {'step': tensor(12012.),\n",
              "               'exp_avg': tensor([-7.7478e-04, -1.8986e-02,  2.3770e-03,  2.8147e-03, -1.5630e-02,\n",
              "                       -3.0797e-02, -2.9911e-03, -1.3517e-04,  5.7974e-03, -1.0261e-02,\n",
              "                       -6.9809e-03, -1.1753e-02, -1.6185e-02, -2.0815e-02, -4.0613e-03,\n",
              "                       -2.2549e-03,  2.2665e-02, -4.8922e-03,  9.4975e-03,  9.9172e-03,\n",
              "                        1.1370e-03, -1.2955e-02,  3.1718e-03,  8.6157e-04, -2.8580e-02,\n",
              "                        7.7273e-03,  3.0709e-02, -6.8587e-03,  6.7208e-03,  9.6965e-03,\n",
              "                        6.3277e-03, -2.1234e-03, -1.9398e-03, -8.0470e-04,  1.0251e-02,\n",
              "                        5.0773e-03, -2.0179e-02, -8.1631e-03, -3.1126e-03, -1.8521e-03,\n",
              "                        1.9118e-03, -6.5097e-03,  1.8818e-02,  2.6318e-02, -5.6468e-04,\n",
              "                        2.0204e-02,  2.2001e-02, -1.1771e-02, -8.3045e-03,  3.3738e-03,\n",
              "                        5.3453e-03,  2.3651e-03, -1.9090e-02, -5.7559e-03, -2.5734e-02,\n",
              "                        2.5167e-02, -3.5322e-04, -3.3775e-04, -6.8074e-03, -1.4657e-03,\n",
              "                        1.4805e-02, -7.3833e-03,  1.6375e-03,  1.3081e-02, -1.4724e-02,\n",
              "                        2.0549e-03, -2.5815e-03,  3.2032e-03, -3.1378e-02,  1.8894e-02,\n",
              "                        4.3719e-03,  1.5690e-03, -9.7618e-04,  3.3377e-03, -5.3117e-03,\n",
              "                       -1.9468e-03, -1.5987e-02, -1.5651e-02, -1.1758e-02, -8.7032e-03,\n",
              "                       -8.6909e-03,  6.7135e-03,  1.3333e-02,  3.0915e-04,  9.9258e-03,\n",
              "                       -1.4378e-02, -5.7077e-03,  8.1847e-03, -1.8461e-02,  5.5336e-03,\n",
              "                       -1.6284e-02, -4.5679e-03, -9.6894e-04,  1.1692e-02,  2.3589e-02,\n",
              "                       -4.1099e-02,  4.6257e-03, -3.7880e-03, -1.1229e-03,  8.6684e-03,\n",
              "                       -5.2355e-03, -3.6356e-03,  8.8329e-03,  2.3145e-03,  3.0789e-03,\n",
              "                       -1.5694e-03, -1.2467e-02, -8.5429e-03,  1.8510e-02,  1.9943e-02,\n",
              "                       -4.5339e-03,  9.5763e-04,  7.3628e-03,  8.2080e-03,  5.3629e-03,\n",
              "                        7.9323e-03,  4.9388e-03, -8.6035e-03, -3.1745e-02,  3.2122e-03,\n",
              "                        3.3696e-03,  2.4419e-03, -1.3819e-02, -1.9945e-03, -1.6339e-02,\n",
              "                        1.6002e-02,  1.1286e-02,  2.3654e-02,  1.9389e-02, -6.1995e-04,\n",
              "                       -1.0376e-02,  4.3255e-02, -6.8751e-03, -1.1878e-03,  2.0446e-02,\n",
              "                        3.7120e-04,  2.5696e-02, -7.7402e-03,  3.5454e-04,  3.2515e-03,\n",
              "                        3.2020e-04,  1.6200e-03,  3.5497e-03,  7.8872e-04,  4.5980e-03,\n",
              "                       -6.2235e-03, -3.3236e-02, -1.7917e-02, -1.2444e-02, -7.8158e-03,\n",
              "                        2.4677e-02, -6.5257e-04, -7.1029e-03, -1.8681e-02, -6.8126e-03,\n",
              "                       -1.4441e-02,  2.2429e-02, -2.0947e-02, -1.2833e-02,  1.1358e-02,\n",
              "                       -1.3070e-02, -7.8088e-03,  1.0314e-02,  2.6334e-03,  7.5533e-03,\n",
              "                        4.1795e-02,  1.7717e-02, -1.7349e-02,  2.9058e-02, -7.3743e-03,\n",
              "                        5.5532e-03,  1.6705e-03,  3.8263e-03,  3.7094e-02,  1.4779e-02,\n",
              "                        2.3524e-03, -1.4212e-02,  2.0459e-02, -4.9577e-03, -2.6270e-02,\n",
              "                        1.5447e-02, -2.8972e-03, -1.2109e-02, -1.9186e-02,  1.8149e-02,\n",
              "                       -8.6518e-03,  7.6227e-04,  4.6511e-04, -2.4213e-03, -5.2306e-04,\n",
              "                       -2.6976e-03,  4.3119e-03, -9.5847e-03,  7.3069e-03, -1.5210e-02,\n",
              "                        1.6752e-02, -5.1847e-03, -1.7656e-02, -5.1161e-03, -1.9445e-02,\n",
              "                       -3.6162e-02,  1.0540e-02, -7.3594e-03,  5.8636e-03,  9.9742e-03,\n",
              "                       -6.8278e-03,  4.0450e-03, -2.6932e-03,  7.5718e-03,  7.1393e-03,\n",
              "                       -3.7699e-03,  3.9710e-03, -8.5737e-03, -3.1705e-03, -8.6210e-04,\n",
              "                        4.5720e-03,  4.4358e-02, -9.2620e-03,  1.8503e-02, -6.2592e-03,\n",
              "                       -3.1019e-02, -6.8933e-04, -2.5527e-02, -6.6332e-03, -9.6860e-03,\n",
              "                       -1.7312e-03,  2.2393e-02,  4.4599e-04,  1.9388e-04,  1.8258e-02,\n",
              "                        3.1357e-02,  1.7683e-03,  7.0704e-03, -1.9243e-02, -5.9242e-04,\n",
              "                        1.4693e-03, -7.3266e-03, -5.8133e-04, -5.4936e-03, -1.3152e-02,\n",
              "                       -1.2181e-03, -4.6603e-03, -5.3448e-03,  1.0293e-02, -2.2200e-02,\n",
              "                        1.0747e-02,  1.0279e-02, -3.0759e-02,  1.4779e-03, -1.1618e-02,\n",
              "                       -7.3237e-03, -9.8349e-04,  1.1547e-02, -6.6664e-03, -3.3714e-03,\n",
              "                        1.9486e-02, -7.2867e-03,  4.9137e-03,  3.7280e-03,  1.0319e-03,\n",
              "                        2.5273e-02,  1.0497e-04, -8.3050e-03,  4.2569e-03,  2.9153e-03,\n",
              "                       -1.9529e-02,  6.6486e-03,  2.2111e-02,  4.4593e-03, -1.0130e-02,\n",
              "                       -7.3360e-03, -4.5389e-03,  3.1128e-03, -6.0901e-03,  1.4082e-02,\n",
              "                       -1.1320e-03,  6.4742e-04,  5.3794e-03,  1.6975e-03, -2.4410e-02,\n",
              "                        2.5022e-02, -6.8869e-03, -2.3872e-02, -1.2534e-02,  1.1815e-02,\n",
              "                       -1.4655e-02,  1.4339e-02, -4.3470e-03,  8.4819e-03, -1.4973e-02,\n",
              "                       -1.1675e-02,  6.0049e-04,  3.4632e-02, -1.6280e-02,  6.5770e-03,\n",
              "                        4.7427e-03, -6.1497e-03, -8.1279e-03,  2.3570e-02,  3.6316e-03,\n",
              "                       -2.6394e-03,  6.5070e-03,  4.2186e-03, -2.0259e-02, -1.8994e-03,\n",
              "                       -9.4470e-03, -5.4784e-03,  2.0702e-02, -9.4861e-03,  1.9804e-02,\n",
              "                       -3.6029e-04, -7.3186e-03, -1.4060e-02,  1.2985e-02, -1.7466e-02,\n",
              "                       -1.8636e-02, -1.1771e-02,  9.5569e-05,  8.0864e-03,  6.2882e-03,\n",
              "                       -1.6314e-03, -1.9279e-02, -1.8920e-02, -9.4971e-03,  1.6116e-03,\n",
              "                       -8.6501e-03,  1.9794e-03,  1.3492e-02,  3.4231e-03,  1.2314e-02,\n",
              "                        5.3046e-03,  5.4604e-03,  3.6583e-03,  2.7763e-03,  1.7161e-03,\n",
              "                       -2.9072e-02, -4.3353e-02,  1.3515e-02, -2.8834e-02, -8.8289e-03,\n",
              "                       -1.4391e-02,  4.9085e-03,  6.4232e-03, -3.9634e-03, -2.5165e-03,\n",
              "                        4.3005e-02,  3.0849e-02,  7.5783e-03,  6.5610e-03,  1.2286e-02,\n",
              "                        6.2507e-03,  2.9508e-03, -1.1912e-02,  5.8767e-03, -6.1620e-03,\n",
              "                        1.4363e-03, -1.0664e-02, -5.1356e-03, -4.9744e-02, -9.0060e-04,\n",
              "                       -4.3941e-02, -1.7947e-02,  9.6083e-04, -2.0445e-03, -1.3040e-04,\n",
              "                       -1.0108e-02, -1.3003e-02, -1.4976e-04, -3.3103e-03,  6.6026e-03,\n",
              "                       -2.2342e-02,  1.5525e-03,  3.9015e-03, -1.2166e-02, -3.4390e-03,\n",
              "                       -8.3949e-04,  5.3900e-03,  1.3213e-02, -1.2064e-02, -2.0715e-02,\n",
              "                       -1.2058e-04,  5.0011e-03, -9.7624e-03, -4.6580e-04,  3.4535e-02,\n",
              "                        2.5934e-03,  1.7008e-03, -1.3965e-02,  2.7415e-03, -2.5735e-02,\n",
              "                        1.5195e-03, -6.5942e-03,  3.3589e-03, -3.4901e-03,  2.2687e-02,\n",
              "                       -3.8914e-04,  2.9477e-02,  1.8203e-03,  5.0977e-03,  2.4803e-02,\n",
              "                        3.3662e-03, -5.4020e-03, -2.8148e-03,  9.7923e-03,  4.2913e-03,\n",
              "                        6.2917e-03, -1.7630e-02, -3.1662e-03,  1.7756e-02,  1.9670e-02,\n",
              "                        2.6151e-03, -2.3316e-02,  2.3352e-03,  7.5427e-03, -5.7918e-03,\n",
              "                        4.3531e-03,  1.2643e-03, -3.7792e-03, -4.8438e-04, -4.4774e-03,\n",
              "                       -5.3911e-03,  5.1009e-03, -1.8801e-02, -8.7291e-03,  7.4478e-03,\n",
              "                       -7.0488e-03, -3.1881e-03,  2.4363e-02,  7.0837e-03,  9.0128e-03,\n",
              "                        1.6311e-02,  3.2182e-02, -1.8754e-02, -1.9376e-02], device='cuda:0'),\n",
              "               'exp_avg_sq': tensor([5.2282e-06, 2.2017e-04, 1.1966e-05, 4.2592e-06, 3.9008e-04, 7.7696e-04,\n",
              "                       1.2166e-05, 4.1781e-07, 3.9976e-05, 8.0014e-05, 6.3658e-05, 1.5126e-04,\n",
              "                       2.2995e-04, 4.9558e-04, 1.6631e-05, 1.5691e-05, 4.3959e-04, 2.9646e-05,\n",
              "                       8.5422e-05, 8.6124e-05, 5.1313e-06, 2.3298e-04, 3.8269e-05, 2.1215e-05,\n",
              "                       8.4764e-04, 7.8072e-05, 9.4922e-04, 5.9656e-05, 4.6471e-05, 8.3446e-05,\n",
              "                       3.1032e-05, 8.6447e-06, 5.3313e-06, 1.3892e-06, 1.4530e-04, 2.9466e-05,\n",
              "                       2.3191e-04, 5.7105e-05, 1.0090e-05, 8.2943e-06, 3.1271e-06, 5.1762e-05,\n",
              "                       4.1618e-04, 6.7568e-04, 2.9378e-06, 3.0446e-04, 3.1966e-04, 1.4410e-04,\n",
              "                       1.2190e-04, 5.3139e-05, 3.1455e-05, 1.9845e-05, 2.5004e-04, 1.9957e-05,\n",
              "                       7.8256e-04, 6.3815e-04, 2.3315e-05, 4.2299e-06, 3.9798e-05, 2.8962e-05,\n",
              "                       2.2886e-04, 3.1895e-05, 1.1106e-05, 1.8313e-04, 2.2874e-04, 5.8202e-06,\n",
              "                       1.2310e-05, 2.0878e-05, 1.3752e-03, 4.3992e-04, 4.3053e-05, 3.0245e-06,\n",
              "                       1.5032e-05, 2.3212e-05, 7.6411e-05, 7.3193e-06, 2.0593e-04, 1.5121e-04,\n",
              "                       2.3986e-04, 6.0356e-05, 9.6099e-05, 3.6366e-05, 2.2790e-04, 4.0589e-07,\n",
              "                       8.4543e-05, 1.7042e-04, 3.9773e-05, 8.0222e-05, 2.1823e-04, 4.1848e-05,\n",
              "                       2.5504e-04, 2.1712e-05, 7.6000e-06, 2.0138e-04, 6.0515e-04, 1.7686e-03,\n",
              "                       6.4219e-05, 1.3904e-05, 1.0554e-06, 9.6009e-05, 3.9632e-05, 1.0649e-04,\n",
              "                       1.2070e-04, 1.0791e-05, 9.6091e-06, 2.9347e-06, 1.9542e-04, 1.6691e-04,\n",
              "                       4.5498e-04, 4.0830e-04, 2.0523e-05, 2.2141e-06, 3.6391e-05, 2.2870e-05,\n",
              "                       3.5537e-05, 8.3305e-05, 1.7174e-05, 1.0358e-04, 1.2450e-03, 1.4960e-05,\n",
              "                       9.1385e-06, 7.2881e-06, 1.8248e-04, 1.1920e-05, 2.4656e-04, 2.6270e-04,\n",
              "                       9.7678e-05, 4.6908e-04, 4.2525e-04, 5.8301e-05, 1.0109e-04, 2.1044e-03,\n",
              "                       7.7314e-05, 2.5129e-06, 4.1799e-04, 5.9578e-07, 5.6648e-04, 1.0312e-04,\n",
              "                       2.6488e-07, 5.2261e-06, 4.4522e-06, 1.7741e-05, 1.1540e-04, 5.8586e-07,\n",
              "                       2.9199e-05, 5.8836e-05, 9.7565e-04, 3.2574e-04, 3.1331e-04, 7.1396e-05,\n",
              "                       9.0652e-04, 1.9080e-06, 6.2673e-05, 3.3244e-04, 8.6336e-05, 1.9363e-04,\n",
              "                       4.8800e-04, 4.8984e-04, 2.4968e-04, 1.7139e-04, 1.3166e-04, 3.7374e-05,\n",
              "                       1.2249e-04, 2.4056e-05, 5.4453e-05, 1.8130e-03, 3.4743e-04, 2.0448e-04,\n",
              "                       8.8279e-04, 6.2456e-05, 4.0764e-05, 8.7644e-06, 1.9967e-05, 1.0377e-03,\n",
              "                       1.5417e-04, 2.9142e-05, 2.7282e-04, 3.1891e-04, 3.3122e-05, 4.7631e-04,\n",
              "                       2.6857e-04, 1.0144e-05, 1.6355e-04, 2.2034e-04, 2.8448e-04, 7.1961e-05,\n",
              "                       9.7921e-06, 4.0169e-06, 1.1843e-05, 8.8748e-06, 8.6139e-06, 2.7519e-05,\n",
              "                       1.8751e-04, 8.8566e-05, 1.4192e-04, 2.6707e-04, 1.2228e-05, 3.7879e-04,\n",
              "                       2.8984e-05, 2.1843e-04, 1.2493e-03, 1.1856e-04, 5.2543e-05, 8.8635e-06,\n",
              "                       1.1671e-04, 4.7341e-05, 1.4869e-05, 4.7726e-06, 1.1819e-04, 4.7521e-05,\n",
              "                       2.1171e-05, 2.8420e-05, 1.1487e-04, 9.1664e-06, 2.2471e-06, 1.1727e-05,\n",
              "                       1.9282e-03, 2.3556e-05, 4.4455e-04, 1.3923e-04, 1.3158e-03, 6.3681e-06,\n",
              "                       7.5225e-04, 5.1367e-05, 1.3472e-04, 1.0925e-05, 8.4592e-04, 1.8214e-06,\n",
              "                       3.2385e-06, 2.7036e-04, 1.0254e-03, 4.2230e-06, 8.1831e-05, 3.9265e-04,\n",
              "                       4.0569e-06, 1.4993e-05, 4.4845e-05, 2.0606e-06, 2.0021e-05, 2.3142e-04,\n",
              "                       3.7202e-06, 1.7832e-05, 3.4149e-05, 1.0950e-04, 5.2165e-04, 1.9370e-04,\n",
              "                       1.7518e-04, 9.5934e-04, 1.6362e-05, 1.2249e-04, 4.0696e-05, 3.0118e-05,\n",
              "                       1.1082e-04, 1.6594e-05, 2.6567e-05, 3.5896e-04, 2.0357e-04, 2.1169e-05,\n",
              "                       2.1861e-05, 2.9464e-06, 7.3895e-04, 2.2332e-06, 6.6278e-05, 2.3060e-05,\n",
              "                       7.1922e-06, 6.0943e-04, 4.9187e-05, 4.3238e-04, 2.1421e-05, 1.2481e-04,\n",
              "                       6.2691e-05, 5.0889e-05, 2.0641e-05, 6.8911e-05, 2.5495e-04, 2.5083e-06,\n",
              "                       7.8761e-07, 3.1082e-05, 7.4265e-06, 7.6342e-04, 5.7656e-04, 2.1275e-05,\n",
              "                       5.4832e-04, 1.4476e-04, 1.4341e-04, 1.2355e-04, 2.0623e-04, 2.4392e-05,\n",
              "                       9.7784e-05, 9.7696e-05, 1.3462e-04, 1.2086e-05, 1.3467e-03, 2.0917e-04,\n",
              "                       5.2028e-05, 2.8464e-05, 3.7760e-05, 1.5294e-04, 6.1715e-04, 7.0245e-06,\n",
              "                       2.7360e-05, 3.3273e-05, 1.4439e-05, 3.3939e-04, 1.2224e-05, 9.3455e-05,\n",
              "                       2.1265e-05, 4.5736e-04, 9.5953e-05, 3.1908e-04, 3.6085e-05, 5.4559e-05,\n",
              "                       3.2461e-04, 1.8910e-04, 3.6492e-04, 4.6320e-04, 2.5882e-04, 2.5112e-06,\n",
              "                       1.1076e-04, 3.4548e-05, 5.4762e-06, 2.9314e-04, 4.2531e-04, 1.1816e-04,\n",
              "                       3.8565e-06, 3.6448e-05, 4.5043e-06, 1.7918e-04, 1.8926e-05, 1.6052e-04,\n",
              "                       2.7620e-05, 2.8648e-05, 8.9603e-06, 1.3119e-05, 5.8508e-06, 1.3285e-03,\n",
              "                       1.6601e-03, 2.6809e-04, 9.7495e-04, 4.4447e-05, 1.6136e-04, 2.7352e-05,\n",
              "                       4.0402e-05, 4.7850e-06, 1.0046e-05, 1.2593e-03, 1.0052e-03, 8.9642e-05,\n",
              "                       5.2666e-05, 1.6861e-04, 5.8628e-05, 2.2073e-05, 1.9029e-04, 3.9997e-05,\n",
              "                       2.7524e-05, 3.0186e-06, 1.8409e-04, 4.2444e-05, 2.7591e-03, 2.7859e-06,\n",
              "                       2.0398e-03, 2.6960e-04, 1.0409e-06, 4.0804e-06, 1.3247e-06, 5.5425e-05,\n",
              "                       2.1173e-04, 1.0673e-06, 9.2733e-06, 3.3815e-05, 6.0676e-04, 4.8820e-06,\n",
              "                       3.6759e-05, 3.2416e-04, 1.9434e-05, 3.1197e-06, 2.6357e-05, 1.7368e-04,\n",
              "                       2.9348e-04, 4.3255e-04, 1.2063e-06, 2.4806e-05, 8.7750e-05, 1.0568e-05,\n",
              "                       9.7583e-04, 1.7493e-05, 3.5155e-06, 2.1333e-04, 8.2711e-06, 6.2552e-04,\n",
              "                       1.6911e-05, 2.7453e-05, 1.4324e-05, 5.5725e-06, 4.1379e-04, 2.6487e-06,\n",
              "                       6.6012e-04, 3.2666e-06, 1.9219e-05, 7.4997e-04, 2.5488e-05, 2.5111e-05,\n",
              "                       1.0844e-05, 8.3796e-05, 3.9580e-05, 6.2868e-05, 3.8462e-04, 5.6542e-06,\n",
              "                       2.3498e-04, 3.5553e-04, 1.0996e-05, 6.5322e-04, 1.7650e-05, 8.3987e-05,\n",
              "                       6.9365e-05, 2.1161e-05, 1.9503e-06, 2.8786e-05, 1.9568e-05, 4.5508e-05,\n",
              "                       3.4871e-05, 3.4934e-05, 2.3108e-04, 6.7743e-05, 6.2140e-05, 3.1710e-05,\n",
              "                       4.6573e-05, 5.3652e-04, 5.2819e-05, 1.2022e-04, 2.6750e-04, 1.1362e-03,\n",
              "                       4.4592e-04, 5.9534e-04], device='cuda:0')},\n",
              "              Parameter containing:\n",
              "              tensor([[-0.0464,  0.0133,  0.0376,  ...,  0.0441,  0.0363, -0.0459],\n",
              "                      [-0.0325, -0.0073,  0.0480,  ...,  0.0304,  0.0183, -0.0236],\n",
              "                      [-0.0188,  0.0226,  0.0366,  ...,  0.0452, -0.0340, -0.0122],\n",
              "                      ...,\n",
              "                      [-0.0085,  0.0367, -0.0133,  ...,  0.0387,  0.0002,  0.0095],\n",
              "                      [ 0.0381,  0.0094, -0.0364,  ..., -0.0415,  0.0039,  0.0432],\n",
              "                      [-0.0355, -0.0332,  0.0135,  ..., -0.0234,  0.0435,  0.0176]],\n",
              "                     device='cuda:0', requires_grad=True): {'step': tensor(12012.),\n",
              "               'exp_avg': tensor([[-0.0432,  0.1030,  0.1191,  ..., -0.0427, -0.2586, -0.2451],\n",
              "                       [ 0.1577, -0.2336, -0.2627,  ...,  0.2361,  0.5986,  0.5696],\n",
              "                       [ 0.0595, -0.0145, -0.0367,  ...,  0.0663,  0.1228,  0.0858],\n",
              "                       ...,\n",
              "                       [-0.0398,  0.1095,  0.1551,  ..., -0.2000, -0.2920, -0.4088],\n",
              "                       [-0.0082, -0.0882, -0.0392,  ..., -0.0629,  0.1195,  0.0372],\n",
              "                       [ 0.1266, -0.0439, -0.0572,  ...,  0.0250,  0.2511,  0.0436]],\n",
              "                      device='cuda:0'),\n",
              "               'exp_avg_sq': tensor([[20020.8945,  4457.1445, 31863.5176,  ..., 17705.4570, 55822.8594,\n",
              "                        21870.6113],\n",
              "                       [10046.9600,  1508.0900,  6112.7520,  ...,  9214.8740, 12932.0635,\n",
              "                        16198.4668],\n",
              "                       [ 2792.6350,   310.1691,  2061.7722,  ...,  4157.9995,  2280.5286,\n",
              "                         7454.0205],\n",
              "                       ...,\n",
              "                       [  919.9092,   350.0287,  2919.6306,  ...,  2494.6755,  4557.1079,\n",
              "                         4128.1631],\n",
              "                       [30460.6641,  2071.8811,  9251.9727,  ..., 10105.5234, 29422.8672,\n",
              "                        38008.0273],\n",
              "                       [ 5820.1030,   666.6512,  4495.2900,  ...,  5863.8296,  8412.5781,\n",
              "                         9256.8643]], device='cuda:0')},\n",
              "              Parameter containing:\n",
              "              tensor([-1.7810e-02,  4.3928e-02, -4.6167e-02, -5.7844e-02,  2.5607e-02,\n",
              "                      -3.6344e-02, -5.7275e-02,  5.6901e-02, -3.5760e-02,  2.2000e-02,\n",
              "                       2.4882e-02,  1.2746e-02, -2.6100e-02, -1.1631e-02,  3.7805e-02,\n",
              "                       4.2290e-02,  1.3360e-02, -5.8672e-03,  1.9543e-03,  3.9945e-02,\n",
              "                       2.1227e-02,  4.6635e-03,  1.9416e-02,  1.7688e-02, -1.9803e-02,\n",
              "                       2.9280e-02,  5.5964e-02, -1.3996e-02,  7.2230e-03,  6.7366e-03,\n",
              "                      -1.7436e-02, -3.0400e-02,  7.3804e-03,  1.2622e-02, -3.2260e-02,\n",
              "                      -3.0313e-03, -3.1447e-02,  1.5321e-02, -5.1065e-03,  2.8720e-02,\n",
              "                      -2.8811e-02,  4.4901e-03,  4.2810e-02, -1.5304e-03,  3.2660e-02,\n",
              "                      -3.2740e-02, -3.2036e-02,  2.2104e-02, -4.2276e-02, -1.0148e-03,\n",
              "                       7.7917e-03,  4.0385e-02, -2.6359e-02,  1.1877e-02,  2.9785e-02,\n",
              "                      -3.9445e-02, -3.6864e-02,  2.7061e-02, -1.0523e-03,  2.2353e-02,\n",
              "                      -3.2761e-02, -3.8154e-03, -3.6965e-02, -3.3676e-02,  3.6247e-02,\n",
              "                      -4.2055e-02, -6.3837e-03,  2.7117e-02,  1.7869e-02,  5.2881e-04,\n",
              "                       5.2587e-02, -1.3209e-03, -2.9531e-02,  3.2885e-02, -3.4076e-02,\n",
              "                       1.7658e-02, -2.6877e-02,  3.8775e-02,  3.2376e-02, -5.5967e-02,\n",
              "                       4.0872e-02,  2.3486e-02, -5.7503e-03,  5.6345e-02,  1.9224e-02,\n",
              "                      -1.6338e-02,  9.3231e-03,  3.7065e-02,  2.8037e-02,  2.6753e-02,\n",
              "                       4.2873e-02, -6.8517e-03, -5.6712e-02, -2.3173e-03,  2.5100e-02,\n",
              "                       3.3572e-02,  1.7750e-02,  1.1553e-02, -2.3382e-02, -2.0777e-02,\n",
              "                       3.1719e-02, -4.7914e-02,  2.4772e-02,  2.6637e-02, -3.3452e-02,\n",
              "                       1.5422e-02, -1.7943e-02,  3.9847e-02,  4.5218e-03, -1.7480e-02,\n",
              "                      -3.3284e-02, -1.2470e-02, -3.5548e-02, -1.1512e-02, -1.2315e-02,\n",
              "                       5.5333e-02, -4.9137e-02,  7.2149e-03,  3.4070e-02, -5.0438e-02,\n",
              "                      -8.2175e-03, -2.4857e-02, -2.6482e-02,  5.9669e-03,  3.9256e-02,\n",
              "                      -1.7506e-02,  1.3802e-02,  6.3889e-04, -6.0112e-02,  1.1416e-02,\n",
              "                      -2.4013e-02,  4.1903e-02,  3.9561e-02, -2.1764e-02,  3.5490e-02,\n",
              "                      -4.9130e-02, -4.7470e-02, -1.8963e-02,  3.1622e-02,  1.6098e-02,\n",
              "                      -2.3001e-03,  1.9195e-02, -1.4975e-02,  3.5293e-02, -3.9127e-03,\n",
              "                      -3.9241e-02, -5.8383e-04, -5.1505e-02,  3.6242e-02,  2.2060e-02,\n",
              "                       4.5033e-02, -4.5195e-02, -5.7196e-02,  1.7795e-02,  3.8578e-02,\n",
              "                      -3.1794e-02,  6.8693e-03, -4.3592e-03,  6.4483e-03, -2.4904e-02,\n",
              "                      -3.8008e-02, -1.0330e-02,  7.5716e-04,  2.9490e-02, -1.7882e-02,\n",
              "                      -2.5430e-02, -9.9079e-03,  4.6341e-02, -3.0572e-05,  2.8945e-02,\n",
              "                       4.1468e-02, -3.3616e-02,  2.8422e-02, -1.4021e-03, -2.9983e-03,\n",
              "                      -2.0896e-02,  2.2347e-02,  3.1889e-02, -3.8535e-03,  1.2989e-03,\n",
              "                      -5.6935e-02,  1.6116e-02,  2.7690e-02, -1.5700e-02,  2.3980e-02,\n",
              "                       1.8666e-02, -2.0226e-02,  2.7883e-02, -2.1903e-02, -4.9673e-02,\n",
              "                       2.5380e-02, -3.2188e-02,  3.0799e-02, -1.5222e-02, -2.5142e-02,\n",
              "                      -3.9524e-02, -2.8643e-02, -1.8366e-03, -3.8581e-02,  1.0627e-02,\n",
              "                       4.8672e-02,  4.1579e-02, -1.0490e-02,  3.8122e-02,  3.0120e-03,\n",
              "                       8.3475e-04, -1.2183e-03,  5.6835e-02,  3.7499e-02,  1.4778e-02,\n",
              "                      -3.5433e-02,  2.7066e-02,  5.1789e-02, -3.2648e-02, -1.6261e-02,\n",
              "                      -2.6893e-02, -1.8389e-02,  1.6791e-02, -3.2481e-02, -5.0878e-02,\n",
              "                      -3.1422e-02,  3.5886e-02,  5.2147e-02, -2.3809e-02, -7.6526e-04,\n",
              "                      -3.6575e-02, -4.8783e-02,  6.0105e-02,  9.2847e-03, -4.9846e-02,\n",
              "                      -2.6556e-02, -2.4201e-02,  4.5232e-03, -5.6516e-02,  1.3692e-02,\n",
              "                      -4.5855e-02,  9.4091e-03,  4.6285e-02, -6.2000e-03], device='cuda:0',\n",
              "                     requires_grad=True): {'step': tensor(12012.),\n",
              "               'exp_avg': tensor([ 4.7952e-02, -8.3826e-02,  3.9878e-03,  5.1307e-02, -1.6456e-02,\n",
              "                        2.5829e-02,  2.6613e-02, -1.2871e-02,  2.9598e-02, -3.5918e-03,\n",
              "                       -1.8488e-02, -6.1923e-02, -1.4156e-01,  2.4108e-03, -2.3133e-02,\n",
              "                       -4.0155e-02,  2.0189e-02,  1.3405e-02,  1.9971e-02, -1.4764e-01,\n",
              "                       -9.5316e-02, -3.8314e-02,  1.9239e-02,  7.5305e-02, -2.7645e-02,\n",
              "                        1.0964e-02, -4.7011e-02, -9.6133e-03, -7.3674e-02,  1.6152e-02,\n",
              "                       -2.1344e-02,  3.2013e-02, -1.5932e-02,  1.8838e-02,  3.5230e-02,\n",
              "                        1.6193e-02,  2.3706e-02,  2.2088e-03,  4.7266e-02,  1.0104e-04,\n",
              "                        1.6930e-02, -2.9527e-02, -2.2648e-02,  6.4180e-02, -5.6753e-02,\n",
              "                       -3.6426e-02,  9.5224e-02,  2.5437e-03,  9.1267e-02, -5.4279e-02,\n",
              "                       -2.1518e-02, -1.2587e-01,  1.9323e-02,  6.2545e-02, -1.1191e-02,\n",
              "                        4.1834e-02,  1.3876e-01, -3.6250e-02,  1.3232e-02, -2.9535e-03,\n",
              "                        9.4956e-03,  3.9606e-03,  9.0129e-02,  4.7290e-02, -1.3510e-01,\n",
              "                        2.8025e-02, -3.6873e-02, -2.0216e-02,  1.0266e-02,  2.0605e-02,\n",
              "                       -9.8835e-03,  4.2174e-02, -1.3746e-02, -2.0648e-02, -2.8214e-02,\n",
              "                       -1.6166e-02, -7.2994e-02, -1.7255e-02,  4.6600e-02,  5.6805e-02,\n",
              "                       -8.1595e-02,  1.2920e-02, -8.2655e-03, -1.5174e-03,  3.2424e-02,\n",
              "                       -9.4969e-03, -1.2370e-02, -4.3073e-02,  2.6994e-02, -7.3837e-04,\n",
              "                       -1.2608e-01,  1.2751e-01,  1.2360e-02, -4.3109e-02,  2.7027e-02,\n",
              "                       -1.1688e-02,  9.6204e-03,  3.7798e-02,  5.3103e-02, -4.0110e-03,\n",
              "                       -5.9012e-03,  4.9633e-02,  5.0752e-02, -1.3577e-01,  3.4373e-02,\n",
              "                       -3.4578e-02,  3.4663e-02,  2.8318e-03,  2.7985e-02, -2.6497e-02,\n",
              "                        1.6401e-02,  1.0021e-01, -2.2275e-03,  9.3971e-02, -3.5001e-02,\n",
              "                       -2.9338e-02,  2.3282e-02, -2.5447e-02, -4.2500e-02,  3.7476e-02,\n",
              "                        3.6772e-02,  2.8841e-02,  4.7376e-02, -7.0005e-03,  8.8971e-04,\n",
              "                       -8.2308e-02,  7.1323e-03, -6.9319e-02,  9.8969e-02, -2.0689e-02,\n",
              "                       -3.7791e-02, -2.3577e-02, -2.3060e-03,  6.5022e-02, -3.9705e-02,\n",
              "                        1.6546e-02,  2.6907e-02, -5.1717e-02, -2.1753e-02, -8.6577e-03,\n",
              "                        2.5792e-03, -5.4929e-02, -2.2966e-03, -7.4882e-02,  1.0260e-02,\n",
              "                        1.1475e-01,  6.6147e-02,  1.6136e-02, -2.3932e-02,  2.9821e-02,\n",
              "                       -6.0317e-03,  5.2227e-02,  1.8337e-02,  1.4726e-01, -4.7862e-02,\n",
              "                        1.2891e-02, -2.6232e-03,  1.0133e-02,  5.1223e-02, -1.0265e-01,\n",
              "                        4.9109e-02,  1.7759e-02,  1.0748e-02,  4.0750e-02,  7.0795e-03,\n",
              "                        2.3045e-02,  9.2354e-02, -1.9664e-02, -1.5007e-01,  2.7098e-02,\n",
              "                       -6.4136e-02, -6.1029e-02, -5.9563e-02,  2.7074e-02, -7.7566e-02,\n",
              "                        9.2823e-03, -4.7462e-02,  2.3274e-02, -1.5403e-02,  8.4030e-02,\n",
              "                        7.5590e-02,  4.3278e-02, -4.7045e-02, -8.8863e-03,  3.8666e-02,\n",
              "                        5.4677e-02, -7.6463e-03,  1.4120e-02, -2.7119e-02,  6.9045e-02,\n",
              "                        1.5120e-02,  5.1848e-02,  1.1190e-02, -3.9946e-02, -2.9587e-02,\n",
              "                        5.8872e-03,  5.2469e-03,  1.8650e-02,  1.0957e-01, -4.2997e-02,\n",
              "                       -1.0385e-02,  7.4060e-03, -4.4723e-02, -2.3359e-02, -1.1004e-01,\n",
              "                       -2.0059e-02,  2.1084e-02, -9.2850e-02,  1.8558e-03,  5.3555e-03,\n",
              "                        2.7675e-03,  6.0833e-02, -1.1855e-02, -1.7618e-02, -1.6790e-01,\n",
              "                       -1.3939e-02, -1.2090e-02, -4.6321e-02, -4.1988e-02,  3.2626e-04,\n",
              "                       -2.5316e-01, -9.0977e-02, -5.1140e-02,  1.7722e-02, -7.6595e-02,\n",
              "                        5.3505e-02,  1.2981e-02, -1.5272e-02, -3.9309e-02,  1.7532e-02,\n",
              "                        2.0043e-02, -1.0755e-01,  6.6881e-02,  2.0469e-02, -9.9682e-02,\n",
              "                        3.5458e-02,  1.5739e-02, -8.9861e-02, -3.8941e-02], device='cuda:0'),\n",
              "               'exp_avg_sq': tensor([3.7415e-03, 4.3399e-03, 8.8548e-05, 2.9323e-03, 2.7390e-04, 6.7469e-04,\n",
              "                       6.5594e-04, 3.3451e-04, 6.6281e-04, 6.8849e-05, 6.4095e-04, 3.8153e-03,\n",
              "                       2.1707e-02, 1.2938e-04, 3.8657e-04, 1.5500e-03, 2.4223e-04, 8.4116e-05,\n",
              "                       2.9763e-04, 2.0372e-02, 1.3435e-02, 1.8110e-03, 3.5855e-04, 6.6455e-03,\n",
              "                       6.5283e-04, 2.5063e-04, 2.2537e-03, 1.1181e-04, 4.1620e-03, 4.9053e-04,\n",
              "                       3.6079e-04, 9.8123e-04, 2.8300e-04, 6.5562e-04, 2.7438e-03, 2.1921e-04,\n",
              "                       3.0171e-04, 1.5661e-04, 2.3630e-03, 1.1544e-05, 1.5660e-04, 1.5410e-03,\n",
              "                       5.7868e-04, 4.9819e-03, 3.0512e-03, 1.8328e-03, 9.7004e-03, 8.4958e-06,\n",
              "                       8.5327e-03, 2.0902e-03, 3.5023e-04, 1.2626e-02, 3.9478e-04, 4.6489e-03,\n",
              "                       1.9202e-04, 2.4548e-03, 1.7723e-02, 1.1832e-03, 4.8486e-04, 2.4028e-04,\n",
              "                       1.6243e-04, 8.6399e-06, 9.9609e-03, 2.4497e-03, 1.8650e-02, 1.0687e-03,\n",
              "                       2.0072e-03, 3.0981e-04, 7.9811e-05, 3.8692e-04, 2.0877e-04, 2.8911e-03,\n",
              "                       1.1254e-04, 5.7297e-04, 1.2761e-03, 2.1056e-04, 4.2237e-03, 3.3811e-04,\n",
              "                       2.0338e-03, 3.0849e-03, 5.7963e-03, 1.3321e-04, 1.7847e-04, 6.9053e-04,\n",
              "                       9.0741e-04, 5.4932e-05, 2.0227e-04, 1.7549e-03, 7.6315e-04, 6.5928e-05,\n",
              "                       1.3490e-02, 1.4347e-02, 1.4996e-04, 2.3058e-03, 9.1303e-04, 2.2523e-04,\n",
              "                       1.0446e-04, 1.4428e-03, 3.2772e-03, 1.0739e-05, 5.5671e-05, 1.9199e-03,\n",
              "                       1.7884e-03, 1.6193e-02, 1.1375e-03, 1.6156e-03, 1.0724e-03, 1.4616e-05,\n",
              "                       5.7078e-04, 7.7976e-04, 5.8777e-04, 9.7064e-03, 7.4297e-06, 8.9625e-03,\n",
              "                       9.3776e-04, 6.9784e-04, 4.6349e-04, 4.6567e-04, 1.7165e-03, 1.5510e-03,\n",
              "                       1.0491e-03, 6.5202e-04, 3.4099e-03, 3.9208e-05, 6.6837e-06, 3.3482e-03,\n",
              "                       6.9305e-05, 5.4872e-03, 7.7311e-03, 1.1060e-03, 1.5089e-03, 1.1221e-03,\n",
              "                       1.5635e-05, 3.4941e-03, 1.5287e-03, 2.6055e-04, 1.8305e-03, 2.8723e-03,\n",
              "                       6.5478e-04, 1.8226e-04, 5.7544e-06, 3.1191e-03, 5.7372e-05, 7.1351e-03,\n",
              "                       9.7621e-05, 1.2776e-02, 3.1683e-03, 2.4080e-04, 6.3894e-04, 1.5214e-03,\n",
              "                       1.6967e-05, 3.1415e-03, 3.3516e-04, 2.1218e-02, 2.3047e-03, 2.1620e-04,\n",
              "                       4.4593e-05, 1.4020e-04, 1.7348e-03, 1.1507e-02, 2.0323e-03, 2.4353e-04,\n",
              "                       1.7849e-04, 1.7733e-03, 3.9444e-04, 4.1490e-04, 8.6726e-03, 7.1335e-04,\n",
              "                       1.9987e-02, 1.2409e-03, 3.2203e-03, 4.1784e-03, 4.5459e-03, 6.0806e-04,\n",
              "                       8.1107e-03, 1.0862e-04, 1.9808e-03, 8.6494e-04, 3.7736e-04, 8.1475e-03,\n",
              "                       6.2011e-03, 2.7680e-03, 3.2441e-03, 4.5907e-04, 1.2217e-03, 2.9896e-03,\n",
              "                       5.1797e-05, 2.2063e-04, 9.0636e-04, 3.6344e-03, 2.0240e-04, 2.9883e-03,\n",
              "                       1.2564e-04, 1.3891e-03, 1.6089e-03, 1.6790e-05, 6.6351e-05, 3.3284e-04,\n",
              "                       1.1971e-02, 3.4866e-03, 1.3414e-04, 9.2832e-05, 1.4833e-03, 5.0254e-04,\n",
              "                       8.6273e-03, 3.5685e-04, 4.2367e-04, 7.4334e-03, 1.9191e-05, 2.6944e-04,\n",
              "                       5.5656e-05, 3.5606e-03, 4.2658e-04, 2.7333e-04, 2.9785e-02, 3.6389e-04,\n",
              "                       9.9516e-05, 1.3890e-03, 1.8112e-03, 1.2717e-04, 8.3977e-02, 6.5422e-03,\n",
              "                       2.4160e-03, 3.0985e-04, 7.3055e-03, 2.1031e-03, 2.2099e-04, 2.1064e-04,\n",
              "                       1.4488e-03, 2.0449e-04, 3.9737e-04, 1.1541e-02, 2.9497e-03, 4.4219e-04,\n",
              "                       8.2155e-03, 1.8960e-03, 2.9856e-04, 1.1788e-02, 1.9420e-03],\n",
              "                      device='cuda:0')},\n",
              "              Parameter containing:\n",
              "              tensor([[ 0.0511, -0.0240, -0.0515,  ...,  0.0092, -0.0358,  0.0257],\n",
              "                      [-0.0436, -0.0326, -0.0154,  ..., -0.0225,  0.0041, -0.0105],\n",
              "                      [ 0.0262,  0.0642, -0.0537,  ..., -0.0453,  0.0381,  0.0045],\n",
              "                      ...,\n",
              "                      [-0.0170, -0.0535, -0.0527,  ..., -0.0440, -0.0114, -0.0065],\n",
              "                      [-0.0266,  0.0033,  0.0398,  ...,  0.0001, -0.0328, -0.0451],\n",
              "                      [-0.0162, -0.0302, -0.0502,  ..., -0.0144, -0.0255,  0.0290]],\n",
              "                     device='cuda:0', requires_grad=True): {'step': tensor(12012.),\n",
              "               'exp_avg': tensor([[-1.1705e-02,  1.5742e-01, -2.3634e-02,  ...,  1.7519e-01,\n",
              "                        -9.2600e-02,  5.4381e-01],\n",
              "                       [-2.7785e-04,  6.7771e-04, -3.7252e-04,  ...,  1.6519e-03,\n",
              "                         8.1129e-04, -2.0187e-03],\n",
              "                       [-1.9372e-02,  2.1701e-01, -2.0015e-02,  ...,  1.3674e-01,\n",
              "                        -8.0972e-02,  5.4250e-01],\n",
              "                       ...,\n",
              "                       [-1.4627e-05, -4.7860e-05, -5.7319e-05,  ..., -3.2468e-05,\n",
              "                        -2.0815e-05,  3.5789e-05],\n",
              "                       [ 4.6670e-03, -1.3057e-02,  3.6418e-03,  ..., -3.0610e-02,\n",
              "                        -1.4641e-02,  3.6077e-02],\n",
              "                       [ 2.8522e-04, -7.3016e-03, -9.6113e-05,  ..., -4.5885e-03,\n",
              "                         3.6040e-03, -2.1835e-02]], device='cuda:0'),\n",
              "               'exp_avg_sq': tensor([[1.6836e+03, 6.1618e+04, 1.3262e+04,  ..., 3.7543e+03, 1.7941e+03,\n",
              "                        7.6758e+04],\n",
              "                       [1.1275e+00, 4.2393e+01, 9.5522e+00,  ..., 2.1228e+00, 1.5534e+00,\n",
              "                        5.0724e+01],\n",
              "                       [3.2431e+02, 3.3056e+04, 3.2015e+03,  ..., 2.2856e+03, 1.4594e+03,\n",
              "                        2.3394e+04],\n",
              "                       ...,\n",
              "                       [2.0132e-04, 4.2080e-03, 8.9834e-04,  ..., 1.9721e-04, 8.5770e-05,\n",
              "                        5.2671e-03],\n",
              "                       [3.1397e+02, 1.1723e+04, 2.8176e+03,  ..., 5.4632e+02, 3.0195e+02,\n",
              "                        1.2658e+04],\n",
              "                       [2.5430e+00, 1.0855e+02, 2.3649e+01,  ..., 5.7359e+00, 3.8556e+00,\n",
              "                        1.3212e+02]], device='cuda:0')},\n",
              "              Parameter containing:\n",
              "              tensor([ 0.0076,  0.0378, -0.0146,  ...,  0.0550,  0.0526,  0.0182],\n",
              "                     device='cuda:0', requires_grad=True): {'step': tensor(12012.),\n",
              "               'exp_avg': tensor([-1.3881e-01,  3.7582e-03, -1.0974e-01,  ...,  2.7022e-05,\n",
              "                       -6.7511e-02,  6.3270e-03], device='cuda:0'),\n",
              "               'exp_avg_sq': tensor([1.7840e-02, 1.4911e-05, 1.1329e-02,  ..., 8.2794e-10, 4.4913e-03,\n",
              "                       3.8624e-05], device='cuda:0')},\n",
              "              Parameter containing:\n",
              "              tensor([[ 2.1837e-02, -5.2224e-04,  1.9225e-02,  ...,  1.6239e-05,\n",
              "                        9.3617e-03, -4.1860e-03]], device='cuda:0', requires_grad=True): {'step': tensor(12012.),\n",
              "               'exp_avg': tensor([[ 1.9275,  0.0560,  3.1352,  ..., -0.4629, -1.4378, -0.9419]],\n",
              "                      device='cuda:0'),\n",
              "               'exp_avg_sq': tensor([[ 8302609.0000, 20093488.0000,  2923217.2500,  ...,\n",
              "                         2213533.5000,  6101427.5000,  1978096.5000]], device='cuda:0')},\n",
              "              Parameter containing:\n",
              "              tensor([0.0346], device='cuda:0', requires_grad=True): {'step': tensor(12012.),\n",
              "               'exp_avg': tensor([-7.3540], device='cuda:0'),\n",
              "               'exp_avg_sq': tensor([53.3415], device='cuda:0')}}),\n",
              " 'param_groups': [{'lr': 1.0435149104684462e-06,\n",
              "   'betas': (0.939104753876406, 0.9991856135686923),\n",
              "   'eps': 1e-08,\n",
              "   'weight_decay': 0.0009246275300019842,\n",
              "   'amsgrad': False,\n",
              "   'maximize': False,\n",
              "   'foreach': None,\n",
              "   'capturable': False,\n",
              "   'differentiable': False,\n",
              "   'fused': None,\n",
              "   'params': [Parameter containing:\n",
              "    tensor([0.2017], device='cuda:0', requires_grad=True),\n",
              "    Parameter containing:\n",
              "    tensor([[ 0.1714, -0.2086, -0.0275,  ..., -0.0289,  0.2055,  0.0674],\n",
              "            [-0.1348,  0.1487,  0.1684,  ..., -0.0689, -0.1292,  0.2476],\n",
              "            [ 0.0141, -0.2602, -0.0598,  ..., -0.0233, -0.1510, -0.1562],\n",
              "            ...,\n",
              "            [ 0.0950,  0.2630,  0.2416,  ...,  0.0394,  0.1964,  0.0878],\n",
              "            [ 0.0915, -0.0232,  0.2064,  ..., -0.1801,  0.0730,  0.1871],\n",
              "            [ 0.0377, -0.2164, -0.0513,  ..., -0.1552,  0.0815,  0.1792]],\n",
              "           device='cuda:0', requires_grad=True),\n",
              "    Parameter containing:\n",
              "    tensor([-0.0751, -0.2434,  0.0402,  ...,  0.1357, -0.0321, -0.1180],\n",
              "           device='cuda:0', requires_grad=True),\n",
              "    Parameter containing:\n",
              "    tensor([[-0.0106,  0.0039, -0.0041,  ..., -0.0065,  0.0139, -0.0117],\n",
              "            [-0.0014,  0.0015,  0.0052,  ...,  0.0156,  0.0045,  0.0103],\n",
              "            [ 0.0160, -0.0106,  0.0124,  ...,  0.0112,  0.0148,  0.0165],\n",
              "            ...,\n",
              "            [-0.0047,  0.0093, -0.0130,  ..., -0.0110, -0.0068, -0.0029],\n",
              "            [ 0.0148,  0.0095, -0.0126,  ..., -0.0108, -0.0073,  0.0041],\n",
              "            [ 0.0092, -0.0091, -0.0096,  ...,  0.0011, -0.0060,  0.0016]],\n",
              "           device='cuda:0', requires_grad=True),\n",
              "    Parameter containing:\n",
              "    tensor([-0.0088, -0.0120, -0.0203,  0.0120,  0.0028,  0.0135,  0.0199, -0.0234,\n",
              "             0.0095,  0.0150,  0.0055,  0.0213, -0.0063,  0.0171, -0.0009,  0.0216,\n",
              "            -0.0017, -0.0179,  0.0171, -0.0160,  0.0194, -0.0197,  0.0034, -0.0215,\n",
              "            -0.0241,  0.0031,  0.0063, -0.0204,  0.0042,  0.0061,  0.0248,  0.0085,\n",
              "             0.0207,  0.0006, -0.0094, -0.0177, -0.0045, -0.0162, -0.0062, -0.0017,\n",
              "             0.0017,  0.0127, -0.0273,  0.0057,  0.0036, -0.0268, -0.0224,  0.0032,\n",
              "            -0.0073,  0.0132,  0.0221,  0.0085,  0.0077,  0.0144,  0.0211, -0.0036,\n",
              "             0.0152, -0.0072,  0.0016, -0.0064, -0.0176,  0.0154,  0.0016,  0.0166,\n",
              "             0.0051, -0.0153,  0.0106,  0.0081, -0.0186, -0.0081,  0.0119, -0.0081,\n",
              "             0.0034, -0.0049,  0.0007,  0.0181,  0.0035, -0.0021, -0.0017,  0.0201,\n",
              "            -0.0048,  0.0198, -0.0153,  0.0036,  0.0087,  0.0131, -0.0165, -0.0017,\n",
              "             0.0188,  0.0120, -0.0249,  0.0127, -0.0035,  0.0219,  0.0053,  0.0078,\n",
              "             0.0205, -0.0078, -0.0035,  0.0124,  0.0008,  0.0255,  0.0073,  0.0007,\n",
              "             0.0018, -0.0190,  0.0145, -0.0150,  0.0136, -0.0152,  0.0039, -0.0001,\n",
              "            -0.0053,  0.0060,  0.0133, -0.0175,  0.0040, -0.0051, -0.0111,  0.0063,\n",
              "             0.0042,  0.0117, -0.0055, -0.0130, -0.0064,  0.0092,  0.0009, -0.0125,\n",
              "            -0.0197, -0.0169,  0.0042, -0.0083,  0.0057,  0.0211,  0.0142, -0.0072,\n",
              "            -0.0219, -0.0241,  0.0171,  0.0041,  0.0232,  0.0049,  0.0160, -0.0143,\n",
              "            -0.0226,  0.0076, -0.0076, -0.0014,  0.0044, -0.0098, -0.0075, -0.0019,\n",
              "             0.0061, -0.0059,  0.0211, -0.0086, -0.0138, -0.0035,  0.0104, -0.0025,\n",
              "            -0.0007, -0.0189, -0.0014,  0.0025,  0.0040], device='cuda:0',\n",
              "           requires_grad=True),\n",
              "    Parameter containing:\n",
              "    tensor([[ 0.0728,  0.0024,  0.0557,  ..., -0.0147, -0.0591, -0.0098],\n",
              "            [ 0.0333, -0.0365, -0.0048,  ..., -0.0680,  0.0217,  0.0010],\n",
              "            [ 0.0679,  0.0396,  0.0450,  ..., -0.0432,  0.0165,  0.0549],\n",
              "            ...,\n",
              "            [ 0.0465,  0.0120,  0.0610,  ...,  0.0627,  0.0060,  0.0742],\n",
              "            [ 0.0102, -0.0165, -0.0463,  ...,  0.0601, -0.0433,  0.0691],\n",
              "            [ 0.0358,  0.0011,  0.0296,  ...,  0.0675,  0.0149, -0.0330]],\n",
              "           device='cuda:0', requires_grad=True),\n",
              "    Parameter containing:\n",
              "    tensor([ 0.0293,  0.0403,  0.0065,  ...,  0.0763, -0.0358,  0.0399],\n",
              "           device='cuda:0', requires_grad=True),\n",
              "    Parameter containing:\n",
              "    tensor([[ 0.0048,  0.0068, -0.0156,  ..., -0.0128, -0.0045,  0.0130],\n",
              "            [-0.0050,  0.0063,  0.0004,  ...,  0.0096, -0.0056, -0.0016],\n",
              "            [ 0.0127, -0.0076, -0.0152,  ...,  0.0049, -0.0079,  0.0032],\n",
              "            ...,\n",
              "            [ 0.0131, -0.0054,  0.0022,  ..., -0.0082,  0.0106, -0.0127],\n",
              "            [-0.0097, -0.0015,  0.0150,  ..., -0.0139,  0.0082,  0.0113],\n",
              "            [ 0.0072, -0.0033, -0.0096,  ..., -0.0082, -0.0090, -0.0142]],\n",
              "           device='cuda:0', requires_grad=True),\n",
              "    Parameter containing:\n",
              "    tensor([-1.4644e-02,  1.3843e-02, -1.2527e-02, -1.8397e-02,  2.4651e-02,\n",
              "            -2.2901e-03,  1.0966e-02, -7.8560e-03,  6.8127e-04,  2.2343e-03,\n",
              "             8.6920e-03,  3.6325e-04,  4.7929e-04,  1.9350e-02,  2.5472e-02,\n",
              "             1.3570e-02, -2.2760e-02, -3.0134e-03, -1.3062e-02, -2.5668e-02,\n",
              "            -1.2565e-02,  2.2170e-02, -2.1142e-02,  1.6898e-02,  9.7861e-03,\n",
              "            -2.1066e-03, -1.6608e-02,  1.4650e-02,  2.2596e-03, -1.8276e-02,\n",
              "             2.4887e-03,  1.5826e-03,  1.6918e-02,  2.1985e-02, -5.6207e-03,\n",
              "             2.0825e-03,  1.7231e-02,  1.8756e-02,  5.6741e-03,  1.3048e-02,\n",
              "            -1.3364e-02,  2.4034e-02, -1.7756e-02, -9.4156e-04,  1.5844e-02,\n",
              "             3.6329e-03, -1.7657e-02,  1.0409e-02,  6.7563e-03, -6.1903e-03,\n",
              "            -6.3573e-04, -2.4493e-02, -3.6173e-03,  2.3673e-02,  1.4898e-02,\n",
              "            -2.3911e-03, -1.5265e-02,  4.2672e-03, -7.7612e-04,  2.1112e-02,\n",
              "            -7.0015e-03, -1.1139e-02,  1.2268e-03, -1.6741e-02,  1.7541e-02,\n",
              "            -1.4529e-02,  2.5273e-02,  1.2298e-02,  2.2361e-02, -1.5622e-02,\n",
              "            -1.1946e-02, -1.1218e-02, -5.4795e-03, -1.6717e-02,  1.5478e-02,\n",
              "             1.6456e-02,  5.5651e-03,  1.0748e-02,  7.2967e-03,  2.4066e-02,\n",
              "             1.7235e-02, -1.9173e-02, -2.2804e-02,  6.4362e-03, -1.4290e-02,\n",
              "             2.4794e-02,  1.6925e-02,  2.6937e-03,  1.7403e-02, -7.6603e-03,\n",
              "             2.2308e-02,  1.6655e-02,  1.3902e-02, -7.3759e-03, -2.5543e-02,\n",
              "            -2.4332e-03, -4.4513e-03,  1.6072e-02, -6.7764e-04, -2.5473e-02,\n",
              "             3.1916e-04,  4.4499e-03, -1.5601e-02, -2.0498e-02,  1.1919e-03,\n",
              "            -1.9543e-04,  2.7326e-02,  4.6801e-04, -2.6475e-02,  4.2807e-03,\n",
              "             1.7919e-02, -9.9845e-03, -2.7001e-02, -2.3675e-02,  1.9171e-03,\n",
              "            -5.6959e-03, -8.3362e-03, -1.7565e-03,  2.5014e-02, -1.5443e-02,\n",
              "            -1.5901e-02, -3.5966e-03,  1.9060e-02, -8.9553e-03,  1.6295e-03,\n",
              "            -1.6863e-03, -2.2393e-02, -2.3305e-02, -8.2628e-03,  1.4140e-03,\n",
              "            -1.0867e-03, -3.2346e-03,  2.1182e-02, -1.9241e-03,  7.4982e-03,\n",
              "            -3.3342e-03, -1.7012e-02,  1.1298e-02, -9.5925e-03,  1.5533e-02,\n",
              "            -1.3206e-03, -1.9074e-02, -2.6056e-02, -1.4066e-02, -1.9910e-03,\n",
              "             2.3553e-04,  1.7014e-02,  5.2460e-03,  8.8764e-03, -1.5260e-02,\n",
              "            -2.6359e-02,  5.9548e-03,  1.2944e-03,  2.5762e-02,  1.6239e-02,\n",
              "             1.1412e-02,  3.7344e-03,  6.9325e-03,  1.3500e-02, -8.3098e-03,\n",
              "             1.7460e-02,  2.2192e-02, -9.8030e-03,  1.1987e-02, -8.8472e-03,\n",
              "             5.9974e-04,  3.2075e-03, -2.7225e-03, -1.3326e-02,  2.2168e-02,\n",
              "            -2.0909e-02, -9.5618e-03, -1.4464e-02, -2.5885e-02, -1.3820e-02,\n",
              "             8.3289e-03,  5.7389e-03, -2.1209e-02,  9.1357e-03,  1.4905e-03,\n",
              "            -7.4595e-03,  1.0407e-02,  1.2859e-03,  1.3825e-02,  3.2070e-03,\n",
              "             8.8182e-03,  7.8281e-03,  1.2127e-03,  1.1035e-02,  8.5492e-03,\n",
              "            -4.9211e-03, -9.8871e-03,  2.6144e-02, -1.3497e-02,  2.4007e-02,\n",
              "            -1.9702e-03, -7.2920e-03,  2.2792e-02,  2.0227e-02,  7.7653e-03,\n",
              "             2.7562e-03, -1.3269e-02,  8.4357e-03,  1.7608e-02, -5.5362e-03,\n",
              "             2.7428e-02, -4.9257e-03, -9.1144e-03, -6.1610e-03,  3.6534e-03,\n",
              "            -7.9441e-03, -8.0316e-03,  1.9137e-02,  2.3530e-02, -9.7611e-03,\n",
              "             9.5943e-04, -4.3924e-03, -5.9950e-03, -2.0498e-02,  1.0096e-02,\n",
              "             1.0628e-02,  5.8918e-03,  7.0928e-03, -2.3847e-03,  2.6356e-02,\n",
              "             2.0929e-02, -2.6767e-03,  1.6108e-02, -8.9011e-03, -9.6668e-03,\n",
              "             2.7426e-03, -8.4323e-03, -5.5784e-03,  1.5348e-02,  9.6576e-03,\n",
              "             1.9061e-02,  6.4060e-03, -1.5861e-03,  6.4374e-04,  2.1010e-02,\n",
              "            -1.0807e-02,  9.3299e-03,  9.3520e-03,  3.0722e-03,  1.0389e-02,\n",
              "             1.7838e-03, -5.8128e-03,  1.1658e-02, -2.7595e-03,  2.1957e-03,\n",
              "            -2.3064e-03, -4.3018e-03, -1.5722e-02,  5.6121e-03,  8.5441e-04,\n",
              "            -7.5549e-04,  2.3234e-02, -5.3248e-03,  1.7665e-03, -1.0592e-02,\n",
              "            -1.2120e-02, -1.3246e-02,  1.0791e-02, -3.1659e-03, -6.5286e-03,\n",
              "             1.4590e-02, -1.1002e-02, -2.7125e-02, -7.3621e-03,  1.4300e-02,\n",
              "             1.3117e-02,  1.1664e-02, -2.1498e-02,  9.1874e-03,  2.1436e-03,\n",
              "            -1.6381e-03, -6.4482e-03, -1.7519e-02,  1.5859e-03,  3.8960e-03,\n",
              "            -2.2561e-02,  8.3653e-03,  1.1313e-02,  2.6038e-02, -1.1439e-02,\n",
              "             2.0768e-02, -7.4530e-03,  1.5476e-02, -1.9201e-02,  6.0592e-03,\n",
              "             3.9559e-03, -1.0961e-02, -2.6849e-02, -1.4753e-03, -9.2095e-03,\n",
              "             7.9068e-04, -3.0674e-03,  2.4397e-02, -7.3466e-03,  1.4752e-03,\n",
              "             1.4022e-02,  1.2814e-03, -1.8170e-02,  1.6697e-03,  1.3434e-03,\n",
              "             2.8009e-02, -6.2157e-03, -2.3876e-03,  2.0979e-03, -2.5089e-02,\n",
              "             2.5124e-02,  1.7657e-02,  6.8489e-03, -1.7483e-02,  1.3389e-02,\n",
              "             1.8967e-02,  2.5338e-02, -1.2681e-02, -2.4966e-02, -1.6271e-02,\n",
              "             1.1686e-02,  2.1885e-02,  9.6003e-03,  2.3377e-02,  5.4437e-03,\n",
              "             2.3863e-02, -1.2017e-02, -1.9796e-02,  1.1716e-03, -1.0416e-02,\n",
              "            -2.0184e-02,  2.5519e-04,  7.9147e-03,  3.9813e-03,  1.4158e-02,\n",
              "             2.5038e-02, -3.9819e-03, -1.7372e-02,  1.1258e-02, -9.5821e-03,\n",
              "             2.0847e-02, -2.4019e-02, -3.4895e-05, -1.1505e-02, -3.5578e-03,\n",
              "            -1.3804e-02, -1.3102e-02, -1.6777e-02, -2.3434e-02, -6.4262e-04,\n",
              "            -1.1724e-02,  2.2873e-02,  7.0825e-03, -4.6588e-03,  1.6240e-02,\n",
              "            -3.4072e-03, -3.4415e-03, -3.5931e-03, -2.6664e-03, -1.4400e-02,\n",
              "             1.1790e-02,  7.0499e-03,  8.7596e-03,  2.2459e-02,  3.8535e-03,\n",
              "             1.1919e-02, -4.3202e-03,  1.5887e-02,  3.1076e-03, -2.0931e-02,\n",
              "            -3.7907e-03,  8.5344e-03, -2.9826e-03, -4.1587e-03,  6.8159e-03,\n",
              "             6.7377e-03, -1.5817e-02, -2.2048e-03,  3.2126e-03, -3.0469e-03,\n",
              "             1.7571e-02, -1.4804e-02,  1.5351e-02,  6.9015e-03, -9.0027e-03,\n",
              "            -2.0431e-02,  1.9026e-03,  1.7629e-03, -2.3028e-02,  2.2354e-02,\n",
              "            -5.1609e-03, -5.2296e-04,  8.2431e-03,  1.3441e-02, -1.9158e-02,\n",
              "            -2.3097e-03, -3.0997e-03, -1.8904e-02,  3.0504e-03, -1.0565e-02,\n",
              "            -6.8210e-03,  1.3265e-02,  4.5539e-03, -2.6578e-02, -5.3818e-03,\n",
              "            -1.0341e-02,  1.7870e-02, -1.2073e-02, -2.0424e-02, -1.8491e-03,\n",
              "            -1.4946e-02,  1.7958e-02,  1.0010e-02, -1.6775e-02,  4.7408e-03,\n",
              "            -1.9533e-02, -2.2132e-02,  9.5863e-03, -6.1018e-03,  2.7769e-03,\n",
              "             2.2510e-02, -4.1825e-03,  1.9992e-02,  9.9069e-03, -1.5425e-02,\n",
              "             7.4287e-03,  2.1685e-02, -1.4970e-02, -1.8587e-02, -8.3793e-03,\n",
              "            -2.0370e-03, -1.9584e-02,  4.5874e-05,  2.4449e-02], device='cuda:0',\n",
              "           requires_grad=True),\n",
              "    Parameter containing:\n",
              "    tensor([[-0.0464,  0.0133,  0.0376,  ...,  0.0441,  0.0363, -0.0459],\n",
              "            [-0.0325, -0.0073,  0.0480,  ...,  0.0304,  0.0183, -0.0236],\n",
              "            [-0.0188,  0.0226,  0.0366,  ...,  0.0452, -0.0340, -0.0122],\n",
              "            ...,\n",
              "            [-0.0085,  0.0367, -0.0133,  ...,  0.0387,  0.0002,  0.0095],\n",
              "            [ 0.0381,  0.0094, -0.0364,  ..., -0.0415,  0.0039,  0.0432],\n",
              "            [-0.0355, -0.0332,  0.0135,  ..., -0.0234,  0.0435,  0.0176]],\n",
              "           device='cuda:0', requires_grad=True),\n",
              "    Parameter containing:\n",
              "    tensor([-1.7810e-02,  4.3928e-02, -4.6167e-02, -5.7844e-02,  2.5607e-02,\n",
              "            -3.6344e-02, -5.7275e-02,  5.6901e-02, -3.5760e-02,  2.2000e-02,\n",
              "             2.4882e-02,  1.2746e-02, -2.6100e-02, -1.1631e-02,  3.7805e-02,\n",
              "             4.2290e-02,  1.3360e-02, -5.8672e-03,  1.9543e-03,  3.9945e-02,\n",
              "             2.1227e-02,  4.6635e-03,  1.9416e-02,  1.7688e-02, -1.9803e-02,\n",
              "             2.9280e-02,  5.5964e-02, -1.3996e-02,  7.2230e-03,  6.7366e-03,\n",
              "            -1.7436e-02, -3.0400e-02,  7.3804e-03,  1.2622e-02, -3.2260e-02,\n",
              "            -3.0313e-03, -3.1447e-02,  1.5321e-02, -5.1065e-03,  2.8720e-02,\n",
              "            -2.8811e-02,  4.4901e-03,  4.2810e-02, -1.5304e-03,  3.2660e-02,\n",
              "            -3.2740e-02, -3.2036e-02,  2.2104e-02, -4.2276e-02, -1.0148e-03,\n",
              "             7.7917e-03,  4.0385e-02, -2.6359e-02,  1.1877e-02,  2.9785e-02,\n",
              "            -3.9445e-02, -3.6864e-02,  2.7061e-02, -1.0523e-03,  2.2353e-02,\n",
              "            -3.2761e-02, -3.8154e-03, -3.6965e-02, -3.3676e-02,  3.6247e-02,\n",
              "            -4.2055e-02, -6.3837e-03,  2.7117e-02,  1.7869e-02,  5.2881e-04,\n",
              "             5.2587e-02, -1.3209e-03, -2.9531e-02,  3.2885e-02, -3.4076e-02,\n",
              "             1.7658e-02, -2.6877e-02,  3.8775e-02,  3.2376e-02, -5.5967e-02,\n",
              "             4.0872e-02,  2.3486e-02, -5.7503e-03,  5.6345e-02,  1.9224e-02,\n",
              "            -1.6338e-02,  9.3231e-03,  3.7065e-02,  2.8037e-02,  2.6753e-02,\n",
              "             4.2873e-02, -6.8517e-03, -5.6712e-02, -2.3173e-03,  2.5100e-02,\n",
              "             3.3572e-02,  1.7750e-02,  1.1553e-02, -2.3382e-02, -2.0777e-02,\n",
              "             3.1719e-02, -4.7914e-02,  2.4772e-02,  2.6637e-02, -3.3452e-02,\n",
              "             1.5422e-02, -1.7943e-02,  3.9847e-02,  4.5218e-03, -1.7480e-02,\n",
              "            -3.3284e-02, -1.2470e-02, -3.5548e-02, -1.1512e-02, -1.2315e-02,\n",
              "             5.5333e-02, -4.9137e-02,  7.2149e-03,  3.4070e-02, -5.0438e-02,\n",
              "            -8.2175e-03, -2.4857e-02, -2.6482e-02,  5.9669e-03,  3.9256e-02,\n",
              "            -1.7506e-02,  1.3802e-02,  6.3889e-04, -6.0112e-02,  1.1416e-02,\n",
              "            -2.4013e-02,  4.1903e-02,  3.9561e-02, -2.1764e-02,  3.5490e-02,\n",
              "            -4.9130e-02, -4.7470e-02, -1.8963e-02,  3.1622e-02,  1.6098e-02,\n",
              "            -2.3001e-03,  1.9195e-02, -1.4975e-02,  3.5293e-02, -3.9127e-03,\n",
              "            -3.9241e-02, -5.8383e-04, -5.1505e-02,  3.6242e-02,  2.2060e-02,\n",
              "             4.5033e-02, -4.5195e-02, -5.7196e-02,  1.7795e-02,  3.8578e-02,\n",
              "            -3.1794e-02,  6.8693e-03, -4.3592e-03,  6.4483e-03, -2.4904e-02,\n",
              "            -3.8008e-02, -1.0330e-02,  7.5716e-04,  2.9490e-02, -1.7882e-02,\n",
              "            -2.5430e-02, -9.9079e-03,  4.6341e-02, -3.0572e-05,  2.8945e-02,\n",
              "             4.1468e-02, -3.3616e-02,  2.8422e-02, -1.4021e-03, -2.9983e-03,\n",
              "            -2.0896e-02,  2.2347e-02,  3.1889e-02, -3.8535e-03,  1.2989e-03,\n",
              "            -5.6935e-02,  1.6116e-02,  2.7690e-02, -1.5700e-02,  2.3980e-02,\n",
              "             1.8666e-02, -2.0226e-02,  2.7883e-02, -2.1903e-02, -4.9673e-02,\n",
              "             2.5380e-02, -3.2188e-02,  3.0799e-02, -1.5222e-02, -2.5142e-02,\n",
              "            -3.9524e-02, -2.8643e-02, -1.8366e-03, -3.8581e-02,  1.0627e-02,\n",
              "             4.8672e-02,  4.1579e-02, -1.0490e-02,  3.8122e-02,  3.0120e-03,\n",
              "             8.3475e-04, -1.2183e-03,  5.6835e-02,  3.7499e-02,  1.4778e-02,\n",
              "            -3.5433e-02,  2.7066e-02,  5.1789e-02, -3.2648e-02, -1.6261e-02,\n",
              "            -2.6893e-02, -1.8389e-02,  1.6791e-02, -3.2481e-02, -5.0878e-02,\n",
              "            -3.1422e-02,  3.5886e-02,  5.2147e-02, -2.3809e-02, -7.6526e-04,\n",
              "            -3.6575e-02, -4.8783e-02,  6.0105e-02,  9.2847e-03, -4.9846e-02,\n",
              "            -2.6556e-02, -2.4201e-02,  4.5232e-03, -5.6516e-02,  1.3692e-02,\n",
              "            -4.5855e-02,  9.4091e-03,  4.6285e-02, -6.2000e-03], device='cuda:0',\n",
              "           requires_grad=True),\n",
              "    Parameter containing:\n",
              "    tensor([[ 0.0511, -0.0240, -0.0515,  ...,  0.0092, -0.0358,  0.0257],\n",
              "            [-0.0436, -0.0326, -0.0154,  ..., -0.0225,  0.0041, -0.0105],\n",
              "            [ 0.0262,  0.0642, -0.0537,  ..., -0.0453,  0.0381,  0.0045],\n",
              "            ...,\n",
              "            [-0.0170, -0.0535, -0.0527,  ..., -0.0440, -0.0114, -0.0065],\n",
              "            [-0.0266,  0.0033,  0.0398,  ...,  0.0001, -0.0328, -0.0451],\n",
              "            [-0.0162, -0.0302, -0.0502,  ..., -0.0144, -0.0255,  0.0290]],\n",
              "           device='cuda:0', requires_grad=True),\n",
              "    Parameter containing:\n",
              "    tensor([ 0.0076,  0.0378, -0.0146,  ...,  0.0550,  0.0526,  0.0182],\n",
              "           device='cuda:0', requires_grad=True),\n",
              "    Parameter containing:\n",
              "    tensor([[ 2.1837e-02, -5.2224e-04,  1.9225e-02,  ...,  1.6239e-05,\n",
              "              9.3617e-03, -4.1860e-03]], device='cuda:0', requires_grad=True),\n",
              "    Parameter containing:\n",
              "    tensor([0.0346], device='cuda:0', requires_grad=True)]}],\n",
              " '_warned_capturable_if_run_uncaptured': True}"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.optim.lr_scheduler.ReduceLROnPlateau at 0x7ff7f6ba61a0>"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'factor': 0.11351038537346028,\n",
              " 'optimizer': Adam (\n",
              " Parameter Group 0\n",
              "     amsgrad: False\n",
              "     betas: (0.939104753876406, 0.9991856135686923)\n",
              "     capturable: False\n",
              "     differentiable: False\n",
              "     eps: 1e-08\n",
              "     foreach: None\n",
              "     fused: None\n",
              "     lr: 1.0435149104684462e-06\n",
              "     maximize: False\n",
              "     weight_decay: 0.0009246275300019842\n",
              " ),\n",
              " 'min_lrs': [0],\n",
              " 'patience': 6,\n",
              " 'verbose': False,\n",
              " 'cooldown': 0,\n",
              " 'cooldown_counter': 0,\n",
              " 'mode': 'min',\n",
              " 'threshold': 0.0024087472803278587,\n",
              " 'threshold_mode': 'rel',\n",
              " 'best': inf,\n",
              " 'num_bad_epochs': 0,\n",
              " 'mode_worse': inf,\n",
              " 'eps': 1e-08,\n",
              " 'last_epoch': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[329.7682469941981,\n",
              " 60.670210743533865,\n",
              " 214.82536590971105,\n",
              " 254.8235335215849,\n",
              " 260.79753624554803,\n",
              " 175.55626383200251,\n",
              " 86.4886414346134,\n",
              " 224.84707958697152,\n",
              " 267.4306979967903,\n",
              " 60.80470394897461,\n",
              " 109.20048194651883,\n",
              " 127.78668296042048,\n",
              " 89.88581229750689,\n",
              " 271.3847599761963,\n",
              " 84.37362592629826,\n",
              " 169.27452567084816,\n",
              " 105.81718339412912,\n",
              " 98.01889168270336,\n",
              " 99.98805096399644,\n",
              " 89.01658562945198,\n",
              " 69.45862861902573,\n",
              " 85.75294198823816,\n",
              " 76.17052950116326,\n",
              " 62.06362083058077,\n",
              " 270.6012640138514,\n",
              " 127.1333292695887,\n",
              " 115.46500750732422,\n",
              " 551.4420475068933,\n",
              " 241.1380550235524,\n",
              " 70.30089705559226,\n",
              " 128.4908230501063,\n",
              " 120.6162562640022,\n",
              " 118.83837587639304,\n",
              " 60.5539454959645,\n",
              " 294.43668231237075,\n",
              " 59.91180491189396,\n",
              " 342.2578929127413,\n",
              " 61.0743972491096,\n",
              " 563.453007717716,\n",
              " 406.1808843563304,\n",
              " 109.09669203150132,\n",
              " 60.09493388671875,\n",
              " 80.60267702564913,\n",
              " 119.16066909610524,\n",
              " 102.10882005354938,\n",
              " 83.34829251870548,\n",
              " 63.48556739573759,\n",
              " 59.2889790624282,\n",
              " 119.14343903305952,\n",
              " 390.5235665161133,\n",
              " 60.725736037848975,\n",
              " 138.55651302813362,\n",
              " 226.4354114846622,\n",
              " 153.62015693574793,\n",
              " 65.70022944991167,\n",
              " 64.71399918428308,\n",
              " 60.23339509600471,\n",
              " 108.73411009162454,\n",
              " 126.08609990485697,\n",
              " 140.58802285802506,\n",
              " 67.60850392788157,\n",
              " 241.0824483060949,\n",
              " 271.2877443075741,\n",
              " 287.19909845186123,\n",
              " 152.50759600973691,\n",
              " 95.1943511435116,\n",
              " 251.87697995066924,\n",
              " 60.38381917563046,\n",
              " 174.34210771161247,\n",
              " 171.08199997486787,\n",
              " 148.81953520436005,\n",
              " 78.97994977524701,\n",
              " 84.20401040541705,\n",
              " 96.90097429773668,\n",
              " 414.6130210452809,\n",
              " 187.20533214901195,\n",
              " 58.46305235308479,\n",
              " 58.30016341947668,\n",
              " 109.28189306317498,\n",
              " 188.4451036434398,\n",
              " 69.55269395787856,\n",
              " 171.2507282765108,\n",
              " 117.14268938490925,\n",
              " 69.67123152070887,\n",
              " 354.39763802167107,\n",
              " 76.5021133842917,\n",
              " 102.42335972842048,\n",
              " 184.2549398782169,\n",
              " 60.1116830480239,\n",
              " 60.46045998624914,\n",
              " 61.39461052246094,\n",
              " 84.8643321780934,\n",
              " 57.99584855023552,\n",
              " 79.52908711224724,\n",
              " 79.3855202460794,\n",
              " 114.44997990516214,\n",
              " 63.64446973589729,\n",
              " 62.69805384449678,\n",
              " 682.9913757948932,\n",
              " 130.50618358513327,\n",
              " 75.13513224092371,\n",
              " 85.55770418054917,\n",
              " 63.313190329159006,\n",
              " 90.28544288150565,\n",
              " 115.94552365722656,\n",
              " 59.60084201696059,\n",
              " 155.1035019093233,\n",
              " 130.26081545984604,\n",
              " 282.1796539005055,\n",
              " 76.55414637917912,\n",
              " 90.081884067311,\n",
              " 116.04006894495348,\n",
              " 79.38569998164458,\n",
              " 99.305031167064,\n",
              " 86.41104303889556,\n",
              " 92.45100669304342,\n",
              " 68.55414141648237,\n",
              " 143.37360260799633,\n",
              " 136.67910690307616,\n",
              " 106.20986585262524,\n",
              " 65.65070502642463,\n",
              " 87.79719914802102,\n",
              " 73.89898707490809,\n",
              " 61.79027293503705,\n",
              " 58.68526369126264,\n",
              " 71.08662841473748,\n",
              " 67.35480087100758,\n",
              " 66.4721449111041,\n",
              " 109.30360605935488,\n",
              " 94.185397810274,\n",
              " 58.45150937105067,\n",
              " 133.95284477996827,\n",
              " 130.64142880751666,\n",
              " 65.16389890818877,\n",
              " 58.09027768806008,\n",
              " 61.7128553456026,\n",
              " 85.31095307572309,\n",
              " 111.71994527094223,\n",
              " 59.06515631139418,\n",
              " 97.29240790189856,\n",
              " 135.1352318513758,\n",
              " 59.49087421875,\n",
              " 64.69356279512293]"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[48.52852320149739,\n",
              " 48.9826415649414,\n",
              " 49.63599255777995,\n",
              " 50.30754013264974,\n",
              " 49.88135396321615,\n",
              " 50.35551092529297,\n",
              " 51.019969673665365,\n",
              " 50.362748970540366,\n",
              " 49.90369322102865,\n",
              " 49.16101432291666,\n",
              " 49.20761100260417,\n",
              " 49.7406426961263,\n",
              " 49.66824965820312,\n",
              " 50.29563568929037,\n",
              " 50.882777473958335,\n",
              " 50.07050185953776,\n",
              " 49.60693638102214,\n",
              " 49.903725272623696,\n",
              " 49.4396,\n",
              " 49.90802005208333,\n",
              " 49.684298315429686,\n",
              " 50.08499295247396,\n",
              " 49.72314844563802,\n",
              " 49.54006092122396,\n",
              " 48.70808123779297,\n",
              " 49.03478438313802,\n",
              " 49.49044652913411,\n",
              " 50.789546048990886,\n",
              " 49.98793509521485,\n",
              " 50.17488321533203,\n",
              " 49.73143297932943,\n",
              " 50.178494283040365,\n",
              " 49.73317892252605,\n",
              " 49.67396096598307,\n",
              " 50.31385185953776,\n",
              " 50.7211615234375,\n",
              " 49.6556697631836,\n",
              " 49.66030877278646,\n",
              " 50.97781237386068,\n",
              " 49.82410281982422,\n",
              " 49.63029313557943,\n",
              " 49.35670391845703,\n",
              " 49.481733984375,\n",
              " 49.25704972737631,\n",
              " 49.55820872395833,\n",
              " 49.83234582926432,\n",
              " 49.70316097005208,\n",
              " 49.62724474690755,\n",
              " 49.22343535563151,\n",
              " 49.71524131673177,\n",
              " 50.20495547281901,\n",
              " 49.72341239827474,\n",
              " 50.03693751627604,\n",
              " 50.27026454671224,\n",
              " 49.52396755371094,\n",
              " 49.61620374348959,\n",
              " 49.56705234781901,\n",
              " 49.94658787841797,\n",
              " 49.547928283691405,\n",
              " 49.97905541992188,\n",
              " 50.24364090576172,\n",
              " 49.44411665039063,\n",
              " 49.69091632486979,\n",
              " 49.30925075683594,\n",
              " 49.71257808024089,\n",
              " 49.37517787272135,\n",
              " 48.91460804036458,\n",
              " 48.91328328857422,\n",
              " 49.49965594889323,\n",
              " 48.99075867919922,\n",
              " 49.269125427246095,\n",
              " 49.13237247314453,\n",
              " 49.24822761230469,\n",
              " 49.59179902750651,\n",
              " 48.82445791829427,\n",
              " 49.23872244873047,\n",
              " 49.36285411376953,\n",
              " 49.33958542480469,\n",
              " 49.7835020711263,\n",
              " 49.23665766601562,\n",
              " 49.21973633219401,\n",
              " 49.10653017985026,\n",
              " 49.123732503255205,\n",
              " 48.97669221598307,\n",
              " 49.685408565266926,\n",
              " 49.83970347493489,\n",
              " 49.51290683186849,\n",
              " 49.16654254557292,\n",
              " 49.05324731445312,\n",
              " 49.00779370930989,\n",
              " 49.0710168741862,\n",
              " 48.86845166015625,\n",
              " 48.78570233154297,\n",
              " 48.995095271809895,\n",
              " 49.28817360432943,\n",
              " 49.64573686930338,\n",
              " 49.46400516764323,\n",
              " 49.27335232747396,\n",
              " 48.4402846069336,\n",
              " 48.75808723958333,\n",
              " 48.8048694539388,\n",
              " 48.673641365559895,\n",
              " 48.7506620320638,\n",
              " 48.50924128417969,\n",
              " 48.14496092936198,\n",
              " 48.13988106689453,\n",
              " 48.5687002726237,\n",
              " 48.208383833821614,\n",
              " 48.651507063802086,\n",
              " 48.490669152832034,\n",
              " 48.24914638671875,\n",
              " 48.50536184895833,\n",
              " 48.355947424316405,\n",
              " 48.654969921875,\n",
              " 48.49108402506511,\n",
              " 48.58745308024088,\n",
              " 48.4937128133138,\n",
              " 48.87853012288412,\n",
              " 48.59049736735026,\n",
              " 48.63525475260417,\n",
              " 48.6676962117513,\n",
              " 48.407510099283854,\n",
              " 48.22903098551432,\n",
              " 48.30382889811198,\n",
              " 48.32068372395833,\n",
              " 48.18849168701172,\n",
              " 48.32340022379557,\n",
              " 48.23030470377604,\n",
              " 48.53174258626302,\n",
              " 48.80562029622396,\n",
              " 48.76169762369792,\n",
              " 49.2144584391276,\n",
              " 48.7950400390625,\n",
              " 48.61519736735026,\n",
              " 48.54269322102864,\n",
              " 48.42650794677734,\n",
              " 48.692369661458336,\n",
              " 48.38030979003906,\n",
              " 48.37582344563802,\n",
              " 48.78764684651693,\n",
              " 48.62803771158854,\n",
              " 48.3941068766276,\n",
              " 48.5566469523112]"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'l1_norm': 2.462433651015338, 'linf_norm': 1471.0911383085588},\n",
              " {'l1_norm': 4.505640866790098, 'linf_norm': 3560.750384835451},\n",
              " {'l1_norm': 3.3745522125019747, 'linf_norm': 2411.0259789586908},\n",
              " {'l1_norm': 2.069184050032672, 'linf_norm': 1078.049683562447},\n",
              " {'l1_norm': 2.081270943484587, 'linf_norm': 1092.550133956135},\n",
              " {'l1_norm': 3.09893632441128, 'linf_norm': 2132.9980466103048},\n",
              " {'l1_norm': 2.917815266093086, 'linf_norm': 1951.53434892577},\n",
              " {'l1_norm': 1.490851698572495, 'linf_norm': 488.5837541184033},\n",
              " {'l1_norm': 1.5546375295526842, 'linf_norm': 554.0419568942463},\n",
              " {'l1_norm': 4.302632949279336, 'linf_norm': 3358.6246256677964},\n",
              " {'l1_norm': 3.392723059957168, 'linf_norm': 2423.1418104261734},\n",
              " {'l1_norm': 2.0947805477591124, 'linf_norm': 1097.888804274054},\n",
              " {'l1_norm': 4.624693239458869, 'linf_norm': 3696.5623760130825},\n",
              " {'l1_norm': 3.6214763282102695, 'linf_norm': 2666.720595997081},\n",
              " {'l1_norm': 2.6795040133083567, 'linf_norm': 1707.2449429077597},\n",
              " {'l1_norm': 1.0962472787857056, 'linf_norm': 86.20514553450415},\n",
              " {'l1_norm': 1.1709137069589952, 'linf_norm': 158.66346238672594},\n",
              " {'l1_norm': 2.740275912812177, 'linf_norm': 1761.5270436847463},\n",
              " {'l1_norm': 2.783965091638004, 'linf_norm': 1807.5919738166808},\n",
              " {'l1_norm': 2.0188640102835262, 'linf_norm': 1023.1302686523212},\n",
              " {'l1_norm': 2.2792289834864, 'linf_norm': 1293.801538021189},\n",
              " {'l1_norm': 2.5385255622302783, 'linf_norm': 1557.1943730016371},\n",
              " {'l1_norm': 1.385838911942875, 'linf_norm': 376.5018815755732},\n",
              " {'l1_norm': 4.529495228369096, 'linf_norm': 3594.720275970616},\n",
              " {'l1_norm': 5.207042423697079, 'linf_norm': 4272.248345902746},\n",
              " {'l1_norm': 4.168592472166174, 'linf_norm': 3207.492105733445},\n",
              " {'l1_norm': 3.482794677734375, 'linf_norm': 2514.062448850632},\n",
              " {'l1_norm': 4.276261981358248, 'linf_norm': 3332.659566514565},\n",
              " {'l1_norm': 3.840849444804472, 'linf_norm': 2893.4148338147893},\n",
              " {'l1_norm': 2.8163603514054243, 'linf_norm': 1841.779279743172},\n",
              " {'l1_norm': 4.974315304212009, 'linf_norm': 4047.4211569362415},\n",
              " {'l1_norm': 2.3722517287198235, 'linf_norm': 1384.4237862854004},\n",
              " {'l1_norm': 1.926986324130788, 'linf_norm': 928.2212679913576},\n",
              " {'l1_norm': 4.609392176841287, 'linf_norm': 3667.965293858887},\n",
              " {'l1_norm': 2.8944563620847816, 'linf_norm': 1917.0901388270656},\n",
              " {'l1_norm': 2.338206043658537, 'linf_norm': 1342.175517163905},\n",
              " {'l1_norm': 2.062625412307066, 'linf_norm': 1069.6128969272163},\n",
              " {'l1_norm': 4.032254012163947, 'linf_norm': 3073.304120347225},\n",
              " {'l1_norm': 1.4189009082681991, 'linf_norm': 412.47664003754784},\n",
              " {'l1_norm': 3.5749792072969324, 'linf_norm': 2611.1392206851287},\n",
              " {'l1_norm': 2.3431555193396174, 'linf_norm': 1352.2670761848783},\n",
              " {'l1_norm': 5.246709856044545, 'linf_norm': 4319.000076336659},\n",
              " {'l1_norm': 1.825363645991157, 'linf_norm': 810.3336459701762},\n",
              " {'l1_norm': 3.2816270815456616, 'linf_norm': 2305.688454904983},\n",
              " {'l1_norm': 3.718255072722716, 'linf_norm': 2751.6266245289635},\n",
              " {'l1_norm': 4.129154097781462, 'linf_norm': 3179.762200100484},\n",
              " {'l1_norm': 2.928552569798862, 'linf_norm': 1949.239057845688},\n",
              " {'l1_norm': 4.034457626196917, 'linf_norm': 3079.4278891376557},\n",
              " {'l1_norm': 2.9376726440205294, 'linf_norm': 1921.7468701760688},\n",
              " {'l1_norm': 3.7575947352914247, 'linf_norm': 2788.2454221362955},\n",
              " {'l1_norm': 2.6423007748435525, 'linf_norm': 1650.3897991777308},\n",
              " {'l1_norm': 2.502980637191324, 'linf_norm': 1512.7903079169778},\n",
              " {'l1_norm': 2.92656228909212, 'linf_norm': 1939.8283534106872},\n",
              " {'l1_norm': 3.5351544395895567, 'linf_norm': 2575.018174662119},\n",
              " {'l1_norm': 3.220857930879032, 'linf_norm': 2241.2390582830653},\n",
              " {'l1_norm': 3.3411322244756363, 'linf_norm': 2363.822008965644},\n",
              " {'l1_norm': 3.0431105607425466, 'linf_norm': 2060.934952628977},\n",
              " {'l1_norm': 2.839531654436448, 'linf_norm': 1850.9847803051607},\n",
              " {'l1_norm': 4.24772025866789, 'linf_norm': 3295.570780644002},\n",
              " {'l1_norm': 3.77010391962388, 'linf_norm': 2801.7516300213083},\n",
              " {'l1_norm': 3.2434714671191047, 'linf_norm': 2265.870168878017},\n",
              " {'l1_norm': 3.467023417113809, 'linf_norm': 2495.3780698764917},\n",
              " {'l1_norm': 2.4919338803571813, 'linf_norm': 1488.6264292431438},\n",
              " {'l1_norm': 3.579319799058577, 'linf_norm': 2610.8924054283816},\n",
              " {'l1_norm': 3.624878188178119, 'linf_norm': 2653.3675928379844},\n",
              " {'l1_norm': 4.421043439803404, 'linf_norm': 3466.792099539678},\n",
              " {'l1_norm': 3.063083748334997, 'linf_norm': 2067.614312348949},\n",
              " {'l1_norm': 4.655102437961803, 'linf_norm': 3693.782684744128},\n",
              " {'l1_norm': 3.209080131255879, 'linf_norm': 2218.9816937062656},\n",
              " {'l1_norm': 3.924818767177357, 'linf_norm': 2945.35631645105},\n",
              " {'l1_norm': 4.398326398120207, 'linf_norm': 3431.223711718548},\n",
              " {'l1_norm': 4.685499310179318, 'linf_norm': 3733.304230373383},\n",
              " {'l1_norm': 3.3244404349719776, 'linf_norm': 2336.452027562344},\n",
              " {'l1_norm': 3.479639557288675, 'linf_norm': 2498.517743053986},\n",
              " {'l1_norm': 3.434184421898337, 'linf_norm': 2441.239735703165},\n",
              " {'l1_norm': 3.2799885049034567, 'linf_norm': 2281.974634605172},\n",
              " {'l1_norm': 4.023355508916518, 'linf_norm': 3055.138864862139},\n",
              " {'l1_norm': 4.219442948223563, 'linf_norm': 3253.703693612088},\n",
              " {'l1_norm': 3.207980731829475, 'linf_norm': 2220.3444572247786},\n",
              " {'l1_norm': 3.3463932862786687, 'linf_norm': 2359.296488202869},\n",
              " {'l1_norm': 4.768051802803488, 'linf_norm': 3813.427443258308},\n",
              " {'l1_norm': 5.2301896792692295, 'linf_norm': 4279.159495236239},\n",
              " {'l1_norm': 4.155072369081834, 'linf_norm': 3182.991798280492},\n",
              " {'l1_norm': 3.759782496059642, 'linf_norm': 2771.2164336682827},\n",
              " {'l1_norm': 3.901193742146212, 'linf_norm': 2929.8565819732557},\n",
              " {'l1_norm': 4.341771721323799, 'linf_norm': 3383.489352882082},\n",
              " {'l1_norm': 4.665532016305362, 'linf_norm': 3711.561254564353},\n",
              " {'l1_norm': 3.7272499197455016, 'linf_norm': 2735.633313424278},\n",
              " {'l1_norm': 3.1330227907236883, 'linf_norm': 2134.423415311342},\n",
              " {'l1_norm': 3.807514019326603, 'linf_norm': 2817.4008552622067},\n",
              " {'l1_norm': 5.427916802608266, 'linf_norm': 4473.733813824014},\n",
              " {'l1_norm': 4.311446790381039, 'linf_norm': 3335.2135309479154},\n",
              " {'l1_norm': 5.27818904653437, 'linf_norm': 4323.971792810855},\n",
              " {'l1_norm': 5.01728485176984, 'linf_norm': 4061.015172812664},\n",
              " {'l1_norm': 4.430087107361064, 'linf_norm': 3462.719988198449},\n",
              " {'l1_norm': 4.522680240485247, 'linf_norm': 3562.55556235199},\n",
              " {'l1_norm': 4.012448862109465, 'linf_norm': 3038.9257808602724},\n",
              " {'l1_norm': 4.610894855920006, 'linf_norm': 3649.9617801304985},\n",
              " {'l1_norm': 4.20725210609436, 'linf_norm': 3214.031404216755},\n",
              " {'l1_norm': 4.236973236285939, 'linf_norm': 3252.5215070085524},\n",
              " {'l1_norm': 5.654996071175968, 'linf_norm': 4701.849582845598},\n",
              " {'l1_norm': 5.024997836438347, 'linf_norm': 4061.6583009614833},\n",
              " {'l1_norm': 5.180252853012085, 'linf_norm': 4219.100635722844},\n",
              " {'l1_norm': 5.322514560654584, 'linf_norm': 4362.769661487961},\n",
              " {'l1_norm': 4.990481280225866, 'linf_norm': 4009.0180931899126},\n",
              " {'l1_norm': 5.252119262470918, 'linf_norm': 4269.651655507043},\n",
              " {'l1_norm': 4.828947202962987, 'linf_norm': 3851.016350152812},\n",
              " {'l1_norm': 3.883445202614279, 'linf_norm': 2867.140747060854},\n",
              " {'l1_norm': 4.958285808439816, 'linf_norm': 3981.329439020774},\n",
              " {'l1_norm': 3.3049833527060115, 'linf_norm': 2294.4158233307894},\n",
              " {'l1_norm': 4.490048114529778, 'linf_norm': 3500.0567454768347},\n",
              " {'l1_norm': 5.897086358328426, 'linf_norm': 4932.654991521588},\n",
              " {'l1_norm': 5.286807176904118, 'linf_norm': 4310.497249897856},\n",
              " {'l1_norm': 5.646219129023833, 'linf_norm': 4684.867032171048},\n",
              " {'l1_norm': 5.205508256171731, 'linf_norm': 4237.933829194888},\n",
              " {'l1_norm': 5.240742172095355, 'linf_norm': 4272.253588800251},\n",
              " {'l1_norm': 6.110406686603322, 'linf_norm': 5153.2449214754},\n",
              " {'l1_norm': 4.558258299782697, 'linf_norm': 3578.71663409591},\n",
              " {'l1_norm': 5.320116897386663, 'linf_norm': 4360.271089641089},\n",
              " {'l1_norm': 5.508162760740168, 'linf_norm': 4543.728260450408},\n",
              " {'l1_norm': 4.111704105680129, 'linf_norm': 3121.8617681682585},\n",
              " {'l1_norm': 3.4358939059201408, 'linf_norm': 2419.7768832448846},\n",
              " {'l1_norm': 6.297375399735395, 'linf_norm': 5342.16057232896},\n",
              " {'l1_norm': 5.558086243752872, 'linf_norm': 4587.882579073154},\n",
              " {'l1_norm': 5.319614742559545, 'linf_norm': 4346.960121614299},\n",
              " {'l1_norm': 5.902195251644359, 'linf_norm': 4938.457831748918},\n",
              " {'l1_norm': 5.582344041397993, 'linf_norm': 4606.606172709207},\n",
              " {'l1_norm': 4.934633139475654, 'linf_norm': 3949.7590613972834},\n",
              " {'l1_norm': 5.381347799413344, 'linf_norm': 4408.652413400773},\n",
              " {'l1_norm': 4.983665080592211, 'linf_norm': 4006.584286609784},\n",
              " {'l1_norm': 4.202733252788993, 'linf_norm': 3209.802022033366},\n",
              " {'l1_norm': 4.492853011894226, 'linf_norm': 3504.050745492812},\n",
              " {'l1_norm': 4.661343129999497, 'linf_norm': 3669.352887151292},\n",
              " {'l1_norm': 5.6157926552043245, 'linf_norm': 4654.279478331263},\n",
              " {'l1_norm': 4.105603854852564, 'linf_norm': 3102.2656275485097},\n",
              " {'l1_norm': 4.783608278061362, 'linf_norm': 3794.194188042147},\n",
              " {'l1_norm': 3.878663193725137, 'linf_norm': 2869.280707052971},\n",
              " {'l1_norm': 5.740803191016702, 'linf_norm': 4777.175278220648},\n",
              " {'l1_norm': 3.729979377970976, 'linf_norm': 2708.7566257118674},\n",
              " {'l1_norm': 5.438241700060227, 'linf_norm': 4463.908424611911},\n",
              " {'l1_norm': 3.666706697929607, 'linf_norm': 2650.557433378259},\n",
              " {'l1_norm': 4.655623925029531, 'linf_norm': 3658.978307669045},\n",
              " {'l1_norm': 6.385269577172223, 'linf_norm': 5435.109447862592}]"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'l1_norm': 1.096243870608012, 'linf_norm': 60.96088713480631},\n",
              " {'l1_norm': 1.0696745104471843, 'linf_norm': 48.35170460205078},\n",
              " {'l1_norm': 1.0483041390736898, 'linf_norm': 37.28235263875325},\n",
              " {'l1_norm': 1.034365457089742, 'linf_norm': 29.58243853963216},\n",
              " {'l1_norm': 1.0437649065653485, 'linf_norm': 35.538838480122884},\n",
              " {'l1_norm': 1.0362086419423422, 'linf_norm': 30.843678192138672},\n",
              " {'l1_norm': 1.030736967086792, 'linf_norm': 25.750377920532227},\n",
              " {'l1_norm': 1.0382170139312743, 'linf_norm': 32.25114989878337},\n",
              " {'l1_norm': 1.047505692545573, 'linf_norm': 38.10431731414795},\n",
              " {'l1_norm': 1.0712038208643595, 'linf_norm': 50.252415264892576},\n",
              " {'l1_norm': 1.070287463315328, 'linf_norm': 49.93542903035482},\n",
              " {'l1_norm': 1.0538532957077027, 'linf_norm': 41.546618448384606},\n",
              " {'l1_norm': 1.0561909530003866, 'linf_norm': 42.85725467020671},\n",
              " {'l1_norm': 1.042140861574809, 'linf_norm': 34.97467788340251},\n",
              " {'l1_norm': 1.0367469018300377, 'linf_norm': 29.99719788029989},\n",
              " {'l1_norm': 1.0475697584788004, 'linf_norm': 38.63791677551269},\n",
              " {'l1_norm': 1.060003595352173, 'linf_norm': 45.00681438446045},\n",
              " {'l1_norm': 1.0525015874226888, 'linf_norm': 41.3713078608195},\n",
              " {'l1_norm': 1.0665662530263265, 'linf_norm': 48.434547498575846},\n",
              " {'l1_norm': 1.054416421063741, 'linf_norm': 42.74950555267334},\n",
              " {'l1_norm': 1.061451059214274, 'linf_norm': 46.3229273534139},\n",
              " {'l1_norm': 1.0528167815526326, 'linf_norm': 42.23456236877441},\n",
              " {'l1_norm': 1.0624619788487752, 'linf_norm': 47.28687773335775},\n",
              " {'l1_norm': 1.0686429103851318, 'linf_norm': 50.42415806020101},\n",
              " {'l1_norm': 1.1030053317387898, 'linf_norm': 67.03214162597656},\n",
              " {'l1_norm': 1.0865671069463094, 'linf_norm': 59.48521319681804},\n",
              " {'l1_norm': 1.0712121868133544, 'linf_norm': 52.06573041941325},\n",
              " {'l1_norm': 1.0456343327204387, 'linf_norm': 36.49969958648682},\n",
              " {'l1_norm': 1.0588117813746134, 'linf_norm': 46.23554562937419},\n",
              " {'l1_norm': 1.0557966037750244, 'linf_norm': 44.65881901448568},\n",
              " {'l1_norm': 1.0680667632420855, 'linf_norm': 51.07842188415528},\n",
              " {'l1_norm': 1.057376709493001, 'linf_norm': 45.68130628712972},\n",
              " {'l1_norm': 1.0697521394093832, 'linf_norm': 51.96666826833089},\n",
              " {'l1_norm': 1.07246172580719, 'linf_norm': 53.527600156148274},\n",
              " {'l1_norm': 1.0580037086486815, 'linf_norm': 46.10825624262492},\n",
              " {'l1_norm': 1.0532209793726603, 'linf_norm': 42.16966154352824},\n",
              " {'l1_norm': 1.0755325656255086, 'linf_norm': 55.41830772857666},\n",
              " {'l1_norm': 1.0764000590642293, 'linf_norm': 55.989432820129394},\n",
              " {'l1_norm': 1.0524986413955688, 'linf_norm': 41.36524575246175},\n",
              " {'l1_norm': 1.072609373283386, 'linf_norm': 53.94554364674886},\n",
              " {'l1_norm': 1.0789018445332843, 'linf_norm': 57.42767744344076},\n",
              " {'l1_norm': 1.0882459391911825, 'linf_norm': 62.43300061645508},\n",
              " {'l1_norm': 1.0850979153315226, 'linf_norm': 60.97690765228271},\n",
              " {'l1_norm': 1.0935102181752523, 'linf_norm': 65.52593141377767},\n",
              " {'l1_norm': 1.0844783047993978, 'linf_norm': 60.93201266988118},\n",
              " {'l1_norm': 1.0779724190394084, 'linf_norm': 57.77366150563558},\n",
              " {'l1_norm': 1.0823962242762248, 'linf_norm': 60.085524611409504},\n",
              " {'l1_norm': 1.085444025103251, 'linf_norm': 61.79665225270589},\n",
              " {'l1_norm': 1.0989273293813069, 'linf_norm': 68.92911766103109},\n",
              " {'l1_norm': 1.084908274714152, 'linf_norm': 61.9380764058431},\n",
              " {'l1_norm': 1.0740184806823732, 'linf_norm': 56.59415890401205},\n",
              " {'l1_norm': 1.0861508837382, 'linf_norm': 62.890743982950845},\n",
              " {'l1_norm': 1.07890200659434, 'linf_norm': 59.07034691009522},\n",
              " {'l1_norm': 1.0743869997024535, 'linf_norm': 56.95761788508097},\n",
              " {'l1_norm': 1.093265143585205, 'linf_norm': 66.8555909886678},\n",
              " {'l1_norm': 1.0916446338017782, 'linf_norm': 66.1491562189738},\n",
              " {'l1_norm': 1.0939818221410116, 'linf_norm': 67.4764176905314},\n",
              " {'l1_norm': 1.0853033753077188, 'linf_norm': 63.01678925628662},\n",
              " {'l1_norm': 1.0969066983540854, 'linf_norm': 69.25168280029297},\n",
              " {'l1_norm': 1.0860263917922974, 'linf_norm': 63.67091318817139},\n",
              " {'l1_norm': 1.081012056541443, 'linf_norm': 61.3259343111674},\n",
              " {'l1_norm': 1.102760815302531, 'linf_norm': 72.68509333496094},\n",
              " {'l1_norm': 1.0964051860173545, 'linf_norm': 69.65948868459066},\n",
              " {'l1_norm': 1.1079570045471192, 'linf_norm': 75.67331372273763},\n",
              " {'l1_norm': 1.095914248975118, 'linf_norm': 69.6062973490397},\n",
              " {'l1_norm': 1.1069105674107869, 'linf_norm': 75.35762569630941},\n",
              " {'l1_norm': 1.1255148486455282, 'linf_norm': 84.38436004231771},\n",
              " {'l1_norm': 1.126495593070984, 'linf_norm': 84.99190034586589},\n",
              " {'l1_norm': 1.1070214836120609, 'linf_norm': 75.70145703125},\n",
              " {'l1_norm': 1.1255752918879192, 'linf_norm': 84.65646578267415},\n",
              " {'l1_norm': 1.1149008786519368, 'linf_norm': 79.57917391764323},\n",
              " {'l1_norm': 1.1207400236765543, 'linf_norm': 82.5654703898112},\n",
              " {'l1_norm': 1.117414262708028, 'linf_norm': 81.14544353790284},\n",
              " {'l1_norm': 1.1079332681655885, 'linf_norm': 76.61925716501872},\n",
              " {'l1_norm': 1.1347959102630616, 'linf_norm': 89.77283846842448},\n",
              " {'l1_norm': 1.1207257318496704, 'linf_norm': 83.22205860137939},\n",
              " {'l1_norm': 1.117842785580953, 'linf_norm': 81.95276622009277},\n",
              " {'l1_norm': 1.11956945660909, 'linf_norm': 82.94084516245525},\n",
              " {'l1_norm': 1.1085747299194335, 'linf_norm': 77.5968699198405},\n",
              " {'l1_norm': 1.1245943834940593, 'linf_norm': 85.78214365183513},\n",
              " {'l1_norm': 1.1259598799387611, 'linf_norm': 86.56945763041179},\n",
              " {'l1_norm': 1.1299833983739218, 'linf_norm': 88.55142515614827},\n",
              " {'l1_norm': 1.1292898237864175, 'linf_norm': 88.1621797861735},\n",
              " {'l1_norm': 1.1358866174062092, 'linf_norm': 91.4530682973226},\n",
              " {'l1_norm': 1.1149601203918458, 'linf_norm': 81.60400503184},\n",
              " {'l1_norm': 1.1121156617482504, 'linf_norm': 80.3413714978536},\n",
              " {'l1_norm': 1.1217607716242473, 'linf_norm': 85.36433238627116},\n",
              " {'l1_norm': 1.1336459674199422, 'linf_norm': 91.3493557551066},\n",
              " {'l1_norm': 1.1385964097340902, 'linf_norm': 93.87079058532714},\n",
              " {'l1_norm': 1.1411609261830649, 'linf_norm': 95.23393248189292},\n",
              " {'l1_norm': 1.1400109816233317, 'linf_norm': 94.82086866760254},\n",
              " {'l1_norm': 1.148508369064331, 'linf_norm': 98.85979677174886},\n",
              " {'l1_norm': 1.1527404505411785, 'linf_norm': 100.94626348368328},\n",
              " {'l1_norm': 1.1460614055633545, 'linf_norm': 98.08686467081706},\n",
              " {'l1_norm': 1.1381045729955035, 'linf_norm': 94.35618542277018},\n",
              " {'l1_norm': 1.1290920820236203, 'linf_norm': 90.21713300628662},\n",
              " {'l1_norm': 1.1349500900268554, 'linf_norm': 93.26637597503662},\n",
              " {'l1_norm': 1.141540218925476, 'linf_norm': 96.5720995997111},\n",
              " {'l1_norm': 1.174418173090617, 'linf_norm': 111.91810542399088},\n",
              " {'l1_norm': 1.161475137837728, 'linf_norm': 106.18973945566812},\n",
              " {'l1_norm': 1.1606264883677164, 'linf_norm': 105.93875031789143},\n",
              " {'l1_norm': 1.1661035713831585, 'linf_norm': 108.54160181681316},\n",
              " {'l1_norm': 1.1638148504257202, 'linf_norm': 107.6213585586548},\n",
              " {'l1_norm': 1.1750617485682169, 'linf_norm': 112.86519986114502},\n",
              " {'l1_norm': 1.194313180033366, 'linf_norm': 121.6731561147054},\n",
              " {'l1_norm': 1.1954183884938558, 'linf_norm': 122.32899573262532},\n",
              " {'l1_norm': 1.1747205125172933, 'linf_norm': 113.30658898824056},\n",
              " {'l1_norm': 1.1928207355499267, 'linf_norm': 121.44077298278808},\n",
              " {'l1_norm': 1.1726121731440229, 'linf_norm': 112.7636620457967},\n",
              " {'l1_norm': 1.18043800462087, 'linf_norm': 116.38366086680094},\n",
              " {'l1_norm': 1.1930966766993203, 'linf_norm': 122.04723408304852},\n",
              " {'l1_norm': 1.1818953497568767, 'linf_norm': 117.18547964986163},\n",
              " {'l1_norm': 1.1898823289235434, 'linf_norm': 120.86904417622884},\n",
              " {'l1_norm': 1.1770757102330525, 'linf_norm': 115.20548655293784},\n",
              " {'l1_norm': 1.1846645167032877, 'linf_norm': 118.80179070943196},\n",
              " {'l1_norm': 1.1810644692738852, 'linf_norm': 117.32016307779948},\n",
              " {'l1_norm': 1.1860867663065593, 'linf_norm': 119.69332685343424},\n",
              " {'l1_norm': 1.1712937482833865, 'linf_norm': 113.22854490712484},\n",
              " {'l1_norm': 1.18301585744222, 'linf_norm': 118.91134089152018},\n",
              " {'l1_norm': 1.181882242711385, 'linf_norm': 118.59782640126546},\n",
              " {'l1_norm': 1.1813507060368855, 'linf_norm': 118.52636567382812},\n",
              " {'l1_norm': 1.193294654337565, 'linf_norm': 124.0742220082601},\n",
              " {'l1_norm': 1.2026886936187744, 'linf_norm': 128.41369096679688},\n",
              " {'l1_norm': 1.1999570156733197, 'linf_norm': 127.39521586507162},\n",
              " {'l1_norm': 1.200033598136902, 'linf_norm': 127.57085423431396},\n",
              " {'l1_norm': 1.2075447640101116, 'linf_norm': 131.10749627888998},\n",
              " {'l1_norm': 1.2018097416559854, 'linf_norm': 128.73696750539145},\n",
              " {'l1_norm': 1.2073839413324992, 'linf_norm': 131.4287403467814},\n",
              " {'l1_norm': 1.1944340393066406, 'linf_norm': 125.69631120707194},\n",
              " {'l1_norm': 1.1844790733973185, 'linf_norm': 121.29582012125653},\n",
              " {'l1_norm': 1.1870733296076457, 'linf_norm': 122.61657004394532},\n",
              " {'l1_norm': 1.1732243552525838, 'linf_norm': 116.4708039683024},\n",
              " {'l1_norm': 1.1870254978815715, 'linf_norm': 123.00263632303874},\n",
              " {'l1_norm': 1.1947779900868734, 'linf_norm': 126.74881779327391},\n",
              " {'l1_norm': 1.1987287178675334, 'linf_norm': 128.70239515889486},\n",
              " {'l1_norm': 1.2042113408406576, 'linf_norm': 131.31010094807942},\n",
              " {'l1_norm': 1.1950534103393555, 'linf_norm': 127.2181216506958},\n",
              " {'l1_norm': 1.2087394320805869, 'linf_norm': 133.3460787190755},\n",
              " {'l1_norm': 1.2099866366704306, 'linf_norm': 134.01672421976724},\n",
              " {'l1_norm': 1.1949844096501667, 'linf_norm': 127.31451531219484},\n",
              " {'l1_norm': 1.2017009065628053, 'linf_norm': 130.51029783274333},\n",
              " {'l1_norm': 1.2116423011779784, 'linf_norm': 135.0306331797282},\n",
              " {'l1_norm': 1.2060399401982624, 'linf_norm': 132.6225849258423}]"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "# %%script echo skipping\n",
        "\n",
        "if hidden_activation_name_loaded == \"LeakyReLU\":\n",
        "  var_dict_loaded[\"leakyrelu_slope\"]\n",
        "if hidden_activation_name_loaded == \"PReLU\":\n",
        "  var_dict_loaded[\"prelu_init\"]\n",
        "elif hidden_activation_name_loaded == \"SoftPlus\":\n",
        "  var_dict_loaded[\"softplus_beta\"]\n",
        "\n",
        "batch_size_loaded\n",
        "n_epochs_loaded\n",
        "loss_name_loaded\n",
        "optimizer_name_loaded\n",
        "scheduler_name_loaded\n",
        "n_units_loaded\n",
        "n_layers_loaded\n",
        "hidden_activation_name_loaded\n",
        "output_activation_name_loaded\n",
        "lr_loaded\n",
        "dropout_rate_loaded\n",
        "hidden_activation_loaded\n",
        "output_activation_loaded\n",
        "net_loaded\n",
        "net_loaded.__dict__ # print the subparameters of the network\n",
        "loss_fn_loaded\n",
        "optimizer_loaded\n",
        "optimizer_loaded.__dict__ # print the subparameters of the optimizer\n",
        "if scheduler_loaded is not None:\n",
        "  scheduler_loaded\n",
        "  scheduler_loaded.__dict__ # print the subparameters of the scheduler\n",
        "train_losses_loaded\n",
        "test_losses_loaded\n",
        "train_metrics_loaded\n",
        "test_metrics_loaded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTLhVihYyUfh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f10cc413-d9db-47c2-867d-31156e3e85a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64.69356279512293"
            ]
          },
          "metadata": {},
          "execution_count": 94
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48.5566469523112"
            ]
          },
          "metadata": {},
          "execution_count": 94
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.2060399401982624"
            ]
          },
          "metadata": {},
          "execution_count": 94
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "132.6225849258423"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "train_losses_loaded[-1]\n",
        "test_losses_loaded[-1]\n",
        "test_metrics_loaded[-1]['l1_norm']\n",
        "test_metrics_loaded[-1]['linf_norm']\n",
        "# print(f'Error is {test_metrics_loaded[-1][\"l1_norm\"] / (3.84e-4)} times bigger than in Dieselhorst et al.')\n",
        "# print(f'Error is {test_metrics_loaded[-1][\"linf_norm\"] / (8.14e-3)} times bigger than in Dieselhorst et al.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTRl12RVra2L"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ICitDHLyUfh"
      },
      "source": [
        "### Visualize loaded results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwLGR1aSUZik"
      },
      "source": [
        "Let us verify correct loading of the train and test metrics by visualizing them again but now through the loaded values. Likewise for the train and test losses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXiNgLsmUZil"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"last_expr_or_assign\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgq4WfSiUZil",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b83d1877-e375-45ec-f2af-d07d0bc2c800"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plotting the losses and metrics for the best network plt.figure(figsize=(12, \n",
        "#plt.subplot(2, 2, 1)\n",
        "#plt.plot(train_losses_loaded, label=\"Train Loss\")\n",
        "#plt.plot(test_losses_loaded, label=\"Test Loss\")\n",
        "#plt.xlabel(\"Epoch\")\n",
        "#plt.ylabel(\"Loss\")\n",
        "#plt.legend()\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot([m[\"l1_norm\"] for m in train_metrics_loaded], label=\"Train L1 Norm\")\n",
        "plt.plot([m[\"l1_norm\"] for m in test_metrics_loaded], label=\"Test L1 Norm\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"L1 Norm\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-3, 1e2)\n",
        "plt.legend()\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot([m[\"linf_norm\"] for m in train_metrics_loaded], label=\"Train Linf Norm\")\n",
        "plt.plot([m[\"linf_norm\"] for m in test_metrics_loaded], label=\"Test Linf Norm\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Linf Norm\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-3, 1e2)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Added plotting MSE of training data and MSE of test data in one plot \n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(train_losses_loaded,label=\"training data\")\n",
        "plt.plot(test_losses_loaded,label=\"test data\")\n",
        "#if scheduler is not None:\n",
        "#    plt.plot([scheduler.get_last_lr()[0] for _ in range(n_epochs)], label=\"Learning rate\") \n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(f\"{loss_name_loaded} Loss\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-7, 1e0)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkgLqJ_UUZim"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwOy5TMuyUfi"
      },
      "source": [
        "## Counting the number of parameters in the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaawajExyUfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7115862-1e7c-4224-c9c3-6a04c56adf30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (hidden_activation): PReLU(num_parameters=1)\n",
              "  (output_activation): Identity()\n",
              "  (layers): ModuleList(\n",
              "    (0): Linear(in_features=14, out_features=3558, bias=True)\n",
              "    (1): Linear(in_features=3558, out_features=165, bias=True)\n",
              "    (2): Linear(in_features=165, out_features=3906, bias=True)\n",
              "    (3): Linear(in_features=3906, out_features=434, bias=True)\n",
              "    (4): Linear(in_features=434, out_features=239, bias=True)\n",
              "    (5): Linear(in_features=239, out_features=1274, bias=True)\n",
              "    (6): Linear(in_features=1274, out_features=1, bias=True)\n",
              "  )\n",
              "  (dropouts): ModuleList(\n",
              "    (0-5): 6 x Dropout(p=0.06388872518840581, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 98
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 3395640 parameters.\n"
          ]
        }
      ],
      "source": [
        "net_loaded.eval()\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {count_parameters(net_loaded)} parameters.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxuzVSnlUZin"
      },
      "source": [
        "## Evaluating the network on arbirary input\n",
        "### Comparing `net` and `net_loaded`\n",
        "\n",
        "We compare `net` and `net_loaded` to confirm correct loading of the network. Note that `net` is only available if we have trained the model in this session."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BTJjqz9DxBJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQxEmL6SyUfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c00cf797-6320-47c2-b5d5-10020c2a5da4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (hidden_activation): PReLU(num_parameters=1)\n",
              "  (output_activation): Identity()\n",
              "  (layers): ModuleList(\n",
              "    (0): Linear(in_features=14, out_features=3558, bias=True)\n",
              "    (1): Linear(in_features=3558, out_features=165, bias=True)\n",
              "    (2): Linear(in_features=165, out_features=3906, bias=True)\n",
              "    (3): Linear(in_features=3906, out_features=434, bias=True)\n",
              "    (4): Linear(in_features=434, out_features=239, bias=True)\n",
              "    (5): Linear(in_features=239, out_features=1274, bias=True)\n",
              "    (6): Linear(in_features=1274, out_features=1, bias=True)\n",
              "  )\n",
              "  (dropouts): ModuleList(\n",
              "    (0-5): 6 x Dropout(p=0.06388872518840581, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "#%%script echo skipping\n",
        "\n",
        "# Set the network to evaluation mode\n",
        "net.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_YG8acPyUfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fadec66c-6ee3-464a-ea72-47c6836ebda1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.eval of Net(\n",
              "  (hidden_activation): PReLU(num_parameters=1)\n",
              "  (output_activation): Identity()\n",
              "  (layers): ModuleList(\n",
              "    (0): Linear(in_features=14, out_features=3558, bias=True)\n",
              "    (1): Linear(in_features=3558, out_features=165, bias=True)\n",
              "    (2): Linear(in_features=165, out_features=3906, bias=True)\n",
              "    (3): Linear(in_features=3906, out_features=434, bias=True)\n",
              "    (4): Linear(in_features=434, out_features=239, bias=True)\n",
              "    (5): Linear(in_features=239, out_features=1274, bias=True)\n",
              "    (6): Linear(in_features=1274, out_features=1, bias=True)\n",
              "  )\n",
              "  (dropouts): ModuleList(\n",
              "    (0-5): 6 x Dropout(p=0.06388872518840581, inplace=False)\n",
              "  )\n",
              ")>"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "# Set the network to evaluation mode\n",
        "net_loaded.eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKhTNVrpyUfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42db7e26-ecdf-41a1-a972-17c0b8b64710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters are the same.\n",
            "Net device: cuda:0\n",
            "Net_loaded device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "for p1, p2 in zip(net.parameters(), net_loaded.parameters()):\n",
        "    if p1.data.ne(p2.data).sum() > 0:\n",
        "        print(\"Parameters are NOT the same.\")\n",
        "        break\n",
        "else:\n",
        "    print(\"Parameters are the same.\")\n",
        "\n",
        "print(\"Net device:\", next(net.parameters()).device)\n",
        "print(\"Net_loaded device:\", next(net_loaded.parameters()).device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0PLAA0DUZin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bd52a37-88e5-4c49-a43c-f1aeee885350"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor([0.2017], device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.1714, -0.2086, -0.0275,  ..., -0.0289,  0.2055,  0.0674],\n",
            "        [-0.1348,  0.1487,  0.1684,  ..., -0.0689, -0.1292,  0.2476],\n",
            "        [ 0.0141, -0.2602, -0.0598,  ..., -0.0233, -0.1510, -0.1562],\n",
            "        ...,\n",
            "        [ 0.0950,  0.2630,  0.2416,  ...,  0.0394,  0.1964,  0.0878],\n",
            "        [ 0.0915, -0.0232,  0.2064,  ..., -0.1801,  0.0730,  0.1871],\n",
            "        [ 0.0377, -0.2164, -0.0513,  ..., -0.1552,  0.0815,  0.1792]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([-0.0751, -0.2434,  0.0402,  ...,  0.1357, -0.0321, -0.1180],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([[-0.0106,  0.0039, -0.0041,  ..., -0.0065,  0.0139, -0.0117],\n",
            "        [-0.0014,  0.0015,  0.0052,  ...,  0.0156,  0.0045,  0.0103],\n",
            "        [ 0.0160, -0.0106,  0.0124,  ...,  0.0112,  0.0148,  0.0165],\n",
            "        ...,\n",
            "        [-0.0047,  0.0093, -0.0130,  ..., -0.0110, -0.0068, -0.0029],\n",
            "        [ 0.0148,  0.0095, -0.0126,  ..., -0.0108, -0.0073,  0.0041],\n",
            "        [ 0.0092, -0.0091, -0.0096,  ...,  0.0011, -0.0060,  0.0016]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([-0.0088, -0.0120, -0.0203,  0.0120,  0.0028,  0.0135,  0.0199, -0.0234,\n",
            "         0.0095,  0.0150,  0.0055,  0.0213, -0.0063,  0.0171, -0.0009,  0.0216,\n",
            "        -0.0017, -0.0179,  0.0171, -0.0160,  0.0194, -0.0197,  0.0034, -0.0215,\n",
            "        -0.0241,  0.0031,  0.0063, -0.0204,  0.0042,  0.0061,  0.0248,  0.0085,\n",
            "         0.0207,  0.0006, -0.0094, -0.0177, -0.0045, -0.0162, -0.0062, -0.0017,\n",
            "         0.0017,  0.0127, -0.0273,  0.0057,  0.0036, -0.0268, -0.0224,  0.0032,\n",
            "        -0.0073,  0.0132,  0.0221,  0.0085,  0.0077,  0.0144,  0.0211, -0.0036,\n",
            "         0.0152, -0.0072,  0.0016, -0.0064, -0.0176,  0.0154,  0.0016,  0.0166,\n",
            "         0.0051, -0.0153,  0.0106,  0.0081, -0.0186, -0.0081,  0.0119, -0.0081,\n",
            "         0.0034, -0.0049,  0.0007,  0.0181,  0.0035, -0.0021, -0.0017,  0.0201,\n",
            "        -0.0048,  0.0198, -0.0153,  0.0036,  0.0087,  0.0131, -0.0165, -0.0017,\n",
            "         0.0188,  0.0120, -0.0249,  0.0127, -0.0035,  0.0219,  0.0053,  0.0078,\n",
            "         0.0205, -0.0078, -0.0035,  0.0124,  0.0008,  0.0255,  0.0073,  0.0007,\n",
            "         0.0018, -0.0190,  0.0145, -0.0150,  0.0136, -0.0152,  0.0039, -0.0001,\n",
            "        -0.0053,  0.0060,  0.0133, -0.0175,  0.0040, -0.0051, -0.0111,  0.0063,\n",
            "         0.0042,  0.0117, -0.0055, -0.0130, -0.0064,  0.0092,  0.0009, -0.0125,\n",
            "        -0.0197, -0.0169,  0.0042, -0.0083,  0.0057,  0.0211,  0.0142, -0.0072,\n",
            "        -0.0219, -0.0241,  0.0171,  0.0041,  0.0232,  0.0049,  0.0160, -0.0143,\n",
            "        -0.0226,  0.0076, -0.0076, -0.0014,  0.0044, -0.0098, -0.0075, -0.0019,\n",
            "         0.0061, -0.0059,  0.0211, -0.0086, -0.0138, -0.0035,  0.0104, -0.0025,\n",
            "        -0.0007, -0.0189, -0.0014,  0.0025,  0.0040], device='cuda:0',\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.0728,  0.0024,  0.0557,  ..., -0.0147, -0.0591, -0.0098],\n",
            "        [ 0.0333, -0.0365, -0.0048,  ..., -0.0680,  0.0217,  0.0010],\n",
            "        [ 0.0679,  0.0396,  0.0450,  ..., -0.0432,  0.0165,  0.0549],\n",
            "        ...,\n",
            "        [ 0.0465,  0.0120,  0.0610,  ...,  0.0627,  0.0060,  0.0742],\n",
            "        [ 0.0102, -0.0165, -0.0463,  ...,  0.0601, -0.0433,  0.0691],\n",
            "        [ 0.0358,  0.0011,  0.0296,  ...,  0.0675,  0.0149, -0.0330]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([ 0.0293,  0.0403,  0.0065,  ...,  0.0763, -0.0358,  0.0399],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.0048,  0.0068, -0.0156,  ..., -0.0128, -0.0045,  0.0130],\n",
            "        [-0.0050,  0.0063,  0.0004,  ...,  0.0096, -0.0056, -0.0016],\n",
            "        [ 0.0127, -0.0076, -0.0152,  ...,  0.0049, -0.0079,  0.0032],\n",
            "        ...,\n",
            "        [ 0.0131, -0.0054,  0.0022,  ..., -0.0082,  0.0106, -0.0127],\n",
            "        [-0.0097, -0.0015,  0.0150,  ..., -0.0139,  0.0082,  0.0113],\n",
            "        [ 0.0072, -0.0033, -0.0096,  ..., -0.0082, -0.0090, -0.0142]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([-1.4644e-02,  1.3843e-02, -1.2527e-02, -1.8397e-02,  2.4651e-02,\n",
            "        -2.2901e-03,  1.0966e-02, -7.8560e-03,  6.8127e-04,  2.2343e-03,\n",
            "         8.6920e-03,  3.6325e-04,  4.7929e-04,  1.9350e-02,  2.5472e-02,\n",
            "         1.3570e-02, -2.2760e-02, -3.0134e-03, -1.3062e-02, -2.5668e-02,\n",
            "        -1.2565e-02,  2.2170e-02, -2.1142e-02,  1.6898e-02,  9.7861e-03,\n",
            "        -2.1066e-03, -1.6608e-02,  1.4650e-02,  2.2596e-03, -1.8276e-02,\n",
            "         2.4887e-03,  1.5826e-03,  1.6918e-02,  2.1985e-02, -5.6207e-03,\n",
            "         2.0825e-03,  1.7231e-02,  1.8756e-02,  5.6741e-03,  1.3048e-02,\n",
            "        -1.3364e-02,  2.4034e-02, -1.7756e-02, -9.4156e-04,  1.5844e-02,\n",
            "         3.6329e-03, -1.7657e-02,  1.0409e-02,  6.7563e-03, -6.1903e-03,\n",
            "        -6.3573e-04, -2.4493e-02, -3.6173e-03,  2.3673e-02,  1.4898e-02,\n",
            "        -2.3911e-03, -1.5265e-02,  4.2672e-03, -7.7612e-04,  2.1112e-02,\n",
            "        -7.0015e-03, -1.1139e-02,  1.2268e-03, -1.6741e-02,  1.7541e-02,\n",
            "        -1.4529e-02,  2.5273e-02,  1.2298e-02,  2.2361e-02, -1.5622e-02,\n",
            "        -1.1946e-02, -1.1218e-02, -5.4795e-03, -1.6717e-02,  1.5478e-02,\n",
            "         1.6456e-02,  5.5651e-03,  1.0748e-02,  7.2967e-03,  2.4066e-02,\n",
            "         1.7235e-02, -1.9173e-02, -2.2804e-02,  6.4362e-03, -1.4290e-02,\n",
            "         2.4794e-02,  1.6925e-02,  2.6937e-03,  1.7403e-02, -7.6603e-03,\n",
            "         2.2308e-02,  1.6655e-02,  1.3902e-02, -7.3759e-03, -2.5543e-02,\n",
            "        -2.4332e-03, -4.4513e-03,  1.6072e-02, -6.7764e-04, -2.5473e-02,\n",
            "         3.1916e-04,  4.4499e-03, -1.5601e-02, -2.0498e-02,  1.1919e-03,\n",
            "        -1.9543e-04,  2.7326e-02,  4.6801e-04, -2.6475e-02,  4.2807e-03,\n",
            "         1.7919e-02, -9.9845e-03, -2.7001e-02, -2.3675e-02,  1.9171e-03,\n",
            "        -5.6959e-03, -8.3362e-03, -1.7565e-03,  2.5014e-02, -1.5443e-02,\n",
            "        -1.5901e-02, -3.5966e-03,  1.9060e-02, -8.9553e-03,  1.6295e-03,\n",
            "        -1.6863e-03, -2.2393e-02, -2.3305e-02, -8.2628e-03,  1.4140e-03,\n",
            "        -1.0867e-03, -3.2346e-03,  2.1182e-02, -1.9241e-03,  7.4982e-03,\n",
            "        -3.3342e-03, -1.7012e-02,  1.1298e-02, -9.5925e-03,  1.5533e-02,\n",
            "        -1.3206e-03, -1.9074e-02, -2.6056e-02, -1.4066e-02, -1.9910e-03,\n",
            "         2.3553e-04,  1.7014e-02,  5.2460e-03,  8.8764e-03, -1.5260e-02,\n",
            "        -2.6359e-02,  5.9548e-03,  1.2944e-03,  2.5762e-02,  1.6239e-02,\n",
            "         1.1412e-02,  3.7344e-03,  6.9325e-03,  1.3500e-02, -8.3098e-03,\n",
            "         1.7460e-02,  2.2192e-02, -9.8030e-03,  1.1987e-02, -8.8472e-03,\n",
            "         5.9974e-04,  3.2075e-03, -2.7225e-03, -1.3326e-02,  2.2168e-02,\n",
            "        -2.0909e-02, -9.5618e-03, -1.4464e-02, -2.5885e-02, -1.3820e-02,\n",
            "         8.3289e-03,  5.7389e-03, -2.1209e-02,  9.1357e-03,  1.4905e-03,\n",
            "        -7.4595e-03,  1.0407e-02,  1.2859e-03,  1.3825e-02,  3.2070e-03,\n",
            "         8.8182e-03,  7.8281e-03,  1.2127e-03,  1.1035e-02,  8.5492e-03,\n",
            "        -4.9211e-03, -9.8871e-03,  2.6144e-02, -1.3497e-02,  2.4007e-02,\n",
            "        -1.9702e-03, -7.2920e-03,  2.2792e-02,  2.0227e-02,  7.7653e-03,\n",
            "         2.7562e-03, -1.3269e-02,  8.4357e-03,  1.7608e-02, -5.5362e-03,\n",
            "         2.7428e-02, -4.9257e-03, -9.1144e-03, -6.1610e-03,  3.6534e-03,\n",
            "        -7.9441e-03, -8.0316e-03,  1.9137e-02,  2.3530e-02, -9.7611e-03,\n",
            "         9.5943e-04, -4.3924e-03, -5.9950e-03, -2.0498e-02,  1.0096e-02,\n",
            "         1.0628e-02,  5.8918e-03,  7.0928e-03, -2.3847e-03,  2.6356e-02,\n",
            "         2.0929e-02, -2.6767e-03,  1.6108e-02, -8.9011e-03, -9.6668e-03,\n",
            "         2.7426e-03, -8.4323e-03, -5.5784e-03,  1.5348e-02,  9.6576e-03,\n",
            "         1.9061e-02,  6.4060e-03, -1.5861e-03,  6.4374e-04,  2.1010e-02,\n",
            "        -1.0807e-02,  9.3299e-03,  9.3520e-03,  3.0722e-03,  1.0389e-02,\n",
            "         1.7838e-03, -5.8128e-03,  1.1658e-02, -2.7595e-03,  2.1957e-03,\n",
            "        -2.3064e-03, -4.3018e-03, -1.5722e-02,  5.6121e-03,  8.5441e-04,\n",
            "        -7.5549e-04,  2.3234e-02, -5.3248e-03,  1.7665e-03, -1.0592e-02,\n",
            "        -1.2120e-02, -1.3246e-02,  1.0791e-02, -3.1659e-03, -6.5286e-03,\n",
            "         1.4590e-02, -1.1002e-02, -2.7125e-02, -7.3621e-03,  1.4300e-02,\n",
            "         1.3117e-02,  1.1664e-02, -2.1498e-02,  9.1874e-03,  2.1436e-03,\n",
            "        -1.6381e-03, -6.4482e-03, -1.7519e-02,  1.5859e-03,  3.8960e-03,\n",
            "        -2.2561e-02,  8.3653e-03,  1.1313e-02,  2.6038e-02, -1.1439e-02,\n",
            "         2.0768e-02, -7.4530e-03,  1.5476e-02, -1.9201e-02,  6.0592e-03,\n",
            "         3.9559e-03, -1.0961e-02, -2.6849e-02, -1.4753e-03, -9.2095e-03,\n",
            "         7.9068e-04, -3.0674e-03,  2.4397e-02, -7.3466e-03,  1.4752e-03,\n",
            "         1.4022e-02,  1.2814e-03, -1.8170e-02,  1.6697e-03,  1.3434e-03,\n",
            "         2.8009e-02, -6.2157e-03, -2.3876e-03,  2.0979e-03, -2.5089e-02,\n",
            "         2.5124e-02,  1.7657e-02,  6.8489e-03, -1.7483e-02,  1.3389e-02,\n",
            "         1.8967e-02,  2.5338e-02, -1.2681e-02, -2.4966e-02, -1.6271e-02,\n",
            "         1.1686e-02,  2.1885e-02,  9.6003e-03,  2.3377e-02,  5.4437e-03,\n",
            "         2.3863e-02, -1.2017e-02, -1.9796e-02,  1.1716e-03, -1.0416e-02,\n",
            "        -2.0184e-02,  2.5519e-04,  7.9147e-03,  3.9813e-03,  1.4158e-02,\n",
            "         2.5038e-02, -3.9819e-03, -1.7372e-02,  1.1258e-02, -9.5821e-03,\n",
            "         2.0847e-02, -2.4019e-02, -3.4895e-05, -1.1505e-02, -3.5578e-03,\n",
            "        -1.3804e-02, -1.3102e-02, -1.6777e-02, -2.3434e-02, -6.4262e-04,\n",
            "        -1.1724e-02,  2.2873e-02,  7.0825e-03, -4.6588e-03,  1.6240e-02,\n",
            "        -3.4072e-03, -3.4415e-03, -3.5931e-03, -2.6664e-03, -1.4400e-02,\n",
            "         1.1790e-02,  7.0499e-03,  8.7596e-03,  2.2459e-02,  3.8535e-03,\n",
            "         1.1919e-02, -4.3202e-03,  1.5887e-02,  3.1076e-03, -2.0931e-02,\n",
            "        -3.7907e-03,  8.5344e-03, -2.9826e-03, -4.1587e-03,  6.8159e-03,\n",
            "         6.7377e-03, -1.5817e-02, -2.2048e-03,  3.2126e-03, -3.0469e-03,\n",
            "         1.7571e-02, -1.4804e-02,  1.5351e-02,  6.9015e-03, -9.0027e-03,\n",
            "        -2.0431e-02,  1.9026e-03,  1.7629e-03, -2.3028e-02,  2.2354e-02,\n",
            "        -5.1609e-03, -5.2296e-04,  8.2431e-03,  1.3441e-02, -1.9158e-02,\n",
            "        -2.3097e-03, -3.0997e-03, -1.8904e-02,  3.0504e-03, -1.0565e-02,\n",
            "        -6.8210e-03,  1.3265e-02,  4.5539e-03, -2.6578e-02, -5.3818e-03,\n",
            "        -1.0341e-02,  1.7870e-02, -1.2073e-02, -2.0424e-02, -1.8491e-03,\n",
            "        -1.4946e-02,  1.7958e-02,  1.0010e-02, -1.6775e-02,  4.7408e-03,\n",
            "        -1.9533e-02, -2.2132e-02,  9.5863e-03, -6.1018e-03,  2.7769e-03,\n",
            "         2.2510e-02, -4.1825e-03,  1.9992e-02,  9.9069e-03, -1.5425e-02,\n",
            "         7.4287e-03,  2.1685e-02, -1.4970e-02, -1.8587e-02, -8.3793e-03,\n",
            "        -2.0370e-03, -1.9584e-02,  4.5874e-05,  2.4449e-02], device='cuda:0',\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[-0.0464,  0.0133,  0.0376,  ...,  0.0441,  0.0363, -0.0459],\n",
            "        [-0.0325, -0.0073,  0.0480,  ...,  0.0304,  0.0183, -0.0236],\n",
            "        [-0.0188,  0.0226,  0.0366,  ...,  0.0452, -0.0340, -0.0122],\n",
            "        ...,\n",
            "        [-0.0085,  0.0367, -0.0133,  ...,  0.0387,  0.0002,  0.0095],\n",
            "        [ 0.0381,  0.0094, -0.0364,  ..., -0.0415,  0.0039,  0.0432],\n",
            "        [-0.0355, -0.0332,  0.0135,  ..., -0.0234,  0.0435,  0.0176]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([-1.7810e-02,  4.3928e-02, -4.6167e-02, -5.7844e-02,  2.5607e-02,\n",
            "        -3.6344e-02, -5.7275e-02,  5.6901e-02, -3.5760e-02,  2.2000e-02,\n",
            "         2.4882e-02,  1.2746e-02, -2.6100e-02, -1.1631e-02,  3.7805e-02,\n",
            "         4.2290e-02,  1.3360e-02, -5.8672e-03,  1.9543e-03,  3.9945e-02,\n",
            "         2.1227e-02,  4.6635e-03,  1.9416e-02,  1.7688e-02, -1.9803e-02,\n",
            "         2.9280e-02,  5.5964e-02, -1.3996e-02,  7.2230e-03,  6.7366e-03,\n",
            "        -1.7436e-02, -3.0400e-02,  7.3804e-03,  1.2622e-02, -3.2260e-02,\n",
            "        -3.0313e-03, -3.1447e-02,  1.5321e-02, -5.1065e-03,  2.8720e-02,\n",
            "        -2.8811e-02,  4.4901e-03,  4.2810e-02, -1.5304e-03,  3.2660e-02,\n",
            "        -3.2740e-02, -3.2036e-02,  2.2104e-02, -4.2276e-02, -1.0148e-03,\n",
            "         7.7917e-03,  4.0385e-02, -2.6359e-02,  1.1877e-02,  2.9785e-02,\n",
            "        -3.9445e-02, -3.6864e-02,  2.7061e-02, -1.0523e-03,  2.2353e-02,\n",
            "        -3.2761e-02, -3.8154e-03, -3.6965e-02, -3.3676e-02,  3.6247e-02,\n",
            "        -4.2055e-02, -6.3837e-03,  2.7117e-02,  1.7869e-02,  5.2881e-04,\n",
            "         5.2587e-02, -1.3209e-03, -2.9531e-02,  3.2885e-02, -3.4076e-02,\n",
            "         1.7658e-02, -2.6877e-02,  3.8775e-02,  3.2376e-02, -5.5967e-02,\n",
            "         4.0872e-02,  2.3486e-02, -5.7503e-03,  5.6345e-02,  1.9224e-02,\n",
            "        -1.6338e-02,  9.3231e-03,  3.7065e-02,  2.8037e-02,  2.6753e-02,\n",
            "         4.2873e-02, -6.8517e-03, -5.6712e-02, -2.3173e-03,  2.5100e-02,\n",
            "         3.3572e-02,  1.7750e-02,  1.1553e-02, -2.3382e-02, -2.0777e-02,\n",
            "         3.1719e-02, -4.7914e-02,  2.4772e-02,  2.6637e-02, -3.3452e-02,\n",
            "         1.5422e-02, -1.7943e-02,  3.9847e-02,  4.5218e-03, -1.7480e-02,\n",
            "        -3.3284e-02, -1.2470e-02, -3.5548e-02, -1.1512e-02, -1.2315e-02,\n",
            "         5.5333e-02, -4.9137e-02,  7.2149e-03,  3.4070e-02, -5.0438e-02,\n",
            "        -8.2175e-03, -2.4857e-02, -2.6482e-02,  5.9669e-03,  3.9256e-02,\n",
            "        -1.7506e-02,  1.3802e-02,  6.3889e-04, -6.0112e-02,  1.1416e-02,\n",
            "        -2.4013e-02,  4.1903e-02,  3.9561e-02, -2.1764e-02,  3.5490e-02,\n",
            "        -4.9130e-02, -4.7470e-02, -1.8963e-02,  3.1622e-02,  1.6098e-02,\n",
            "        -2.3001e-03,  1.9195e-02, -1.4975e-02,  3.5293e-02, -3.9127e-03,\n",
            "        -3.9241e-02, -5.8383e-04, -5.1505e-02,  3.6242e-02,  2.2060e-02,\n",
            "         4.5033e-02, -4.5195e-02, -5.7196e-02,  1.7795e-02,  3.8578e-02,\n",
            "        -3.1794e-02,  6.8693e-03, -4.3592e-03,  6.4483e-03, -2.4904e-02,\n",
            "        -3.8008e-02, -1.0330e-02,  7.5716e-04,  2.9490e-02, -1.7882e-02,\n",
            "        -2.5430e-02, -9.9079e-03,  4.6341e-02, -3.0572e-05,  2.8945e-02,\n",
            "         4.1468e-02, -3.3616e-02,  2.8422e-02, -1.4021e-03, -2.9983e-03,\n",
            "        -2.0896e-02,  2.2347e-02,  3.1889e-02, -3.8535e-03,  1.2989e-03,\n",
            "        -5.6935e-02,  1.6116e-02,  2.7690e-02, -1.5700e-02,  2.3980e-02,\n",
            "         1.8666e-02, -2.0226e-02,  2.7883e-02, -2.1903e-02, -4.9673e-02,\n",
            "         2.5380e-02, -3.2188e-02,  3.0799e-02, -1.5222e-02, -2.5142e-02,\n",
            "        -3.9524e-02, -2.8643e-02, -1.8366e-03, -3.8581e-02,  1.0627e-02,\n",
            "         4.8672e-02,  4.1579e-02, -1.0490e-02,  3.8122e-02,  3.0120e-03,\n",
            "         8.3475e-04, -1.2183e-03,  5.6835e-02,  3.7499e-02,  1.4778e-02,\n",
            "        -3.5433e-02,  2.7066e-02,  5.1789e-02, -3.2648e-02, -1.6261e-02,\n",
            "        -2.6893e-02, -1.8389e-02,  1.6791e-02, -3.2481e-02, -5.0878e-02,\n",
            "        -3.1422e-02,  3.5886e-02,  5.2147e-02, -2.3809e-02, -7.6526e-04,\n",
            "        -3.6575e-02, -4.8783e-02,  6.0105e-02,  9.2847e-03, -4.9846e-02,\n",
            "        -2.6556e-02, -2.4201e-02,  4.5232e-03, -5.6516e-02,  1.3692e-02,\n",
            "        -4.5855e-02,  9.4091e-03,  4.6285e-02, -6.2000e-03], device='cuda:0',\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.0511, -0.0240, -0.0515,  ...,  0.0092, -0.0358,  0.0257],\n",
            "        [-0.0436, -0.0326, -0.0154,  ..., -0.0225,  0.0041, -0.0105],\n",
            "        [ 0.0262,  0.0642, -0.0537,  ..., -0.0453,  0.0381,  0.0045],\n",
            "        ...,\n",
            "        [-0.0170, -0.0535, -0.0527,  ..., -0.0440, -0.0114, -0.0065],\n",
            "        [-0.0266,  0.0033,  0.0398,  ...,  0.0001, -0.0328, -0.0451],\n",
            "        [-0.0162, -0.0302, -0.0502,  ..., -0.0144, -0.0255,  0.0290]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([ 0.0076,  0.0378, -0.0146,  ...,  0.0550,  0.0526,  0.0182],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([[ 2.1837e-02, -5.2224e-04,  1.9225e-02,  ...,  1.6239e-05,\n",
            "          9.3617e-03, -4.1860e-03]], device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([0.0346], device='cuda:0', requires_grad=True)]\n"
          ]
        }
      ],
      "source": [
        "#%%script echo skipping\n",
        "\n",
        "print(list(net.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NZ8iVA7UZio",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0f98d10-ce6e-4157-c5fb-15a96de964f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor([0.2017], device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.1714, -0.2086, -0.0275,  ..., -0.0289,  0.2055,  0.0674],\n",
            "        [-0.1348,  0.1487,  0.1684,  ..., -0.0689, -0.1292,  0.2476],\n",
            "        [ 0.0141, -0.2602, -0.0598,  ..., -0.0233, -0.1510, -0.1562],\n",
            "        ...,\n",
            "        [ 0.0950,  0.2630,  0.2416,  ...,  0.0394,  0.1964,  0.0878],\n",
            "        [ 0.0915, -0.0232,  0.2064,  ..., -0.1801,  0.0730,  0.1871],\n",
            "        [ 0.0377, -0.2164, -0.0513,  ..., -0.1552,  0.0815,  0.1792]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([-0.0751, -0.2434,  0.0402,  ...,  0.1357, -0.0321, -0.1180],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([[-0.0106,  0.0039, -0.0041,  ..., -0.0065,  0.0139, -0.0117],\n",
            "        [-0.0014,  0.0015,  0.0052,  ...,  0.0156,  0.0045,  0.0103],\n",
            "        [ 0.0160, -0.0106,  0.0124,  ...,  0.0112,  0.0148,  0.0165],\n",
            "        ...,\n",
            "        [-0.0047,  0.0093, -0.0130,  ..., -0.0110, -0.0068, -0.0029],\n",
            "        [ 0.0148,  0.0095, -0.0126,  ..., -0.0108, -0.0073,  0.0041],\n",
            "        [ 0.0092, -0.0091, -0.0096,  ...,  0.0011, -0.0060,  0.0016]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([-0.0088, -0.0120, -0.0203,  0.0120,  0.0028,  0.0135,  0.0199, -0.0234,\n",
            "         0.0095,  0.0150,  0.0055,  0.0213, -0.0063,  0.0171, -0.0009,  0.0216,\n",
            "        -0.0017, -0.0179,  0.0171, -0.0160,  0.0194, -0.0197,  0.0034, -0.0215,\n",
            "        -0.0241,  0.0031,  0.0063, -0.0204,  0.0042,  0.0061,  0.0248,  0.0085,\n",
            "         0.0207,  0.0006, -0.0094, -0.0177, -0.0045, -0.0162, -0.0062, -0.0017,\n",
            "         0.0017,  0.0127, -0.0273,  0.0057,  0.0036, -0.0268, -0.0224,  0.0032,\n",
            "        -0.0073,  0.0132,  0.0221,  0.0085,  0.0077,  0.0144,  0.0211, -0.0036,\n",
            "         0.0152, -0.0072,  0.0016, -0.0064, -0.0176,  0.0154,  0.0016,  0.0166,\n",
            "         0.0051, -0.0153,  0.0106,  0.0081, -0.0186, -0.0081,  0.0119, -0.0081,\n",
            "         0.0034, -0.0049,  0.0007,  0.0181,  0.0035, -0.0021, -0.0017,  0.0201,\n",
            "        -0.0048,  0.0198, -0.0153,  0.0036,  0.0087,  0.0131, -0.0165, -0.0017,\n",
            "         0.0188,  0.0120, -0.0249,  0.0127, -0.0035,  0.0219,  0.0053,  0.0078,\n",
            "         0.0205, -0.0078, -0.0035,  0.0124,  0.0008,  0.0255,  0.0073,  0.0007,\n",
            "         0.0018, -0.0190,  0.0145, -0.0150,  0.0136, -0.0152,  0.0039, -0.0001,\n",
            "        -0.0053,  0.0060,  0.0133, -0.0175,  0.0040, -0.0051, -0.0111,  0.0063,\n",
            "         0.0042,  0.0117, -0.0055, -0.0130, -0.0064,  0.0092,  0.0009, -0.0125,\n",
            "        -0.0197, -0.0169,  0.0042, -0.0083,  0.0057,  0.0211,  0.0142, -0.0072,\n",
            "        -0.0219, -0.0241,  0.0171,  0.0041,  0.0232,  0.0049,  0.0160, -0.0143,\n",
            "        -0.0226,  0.0076, -0.0076, -0.0014,  0.0044, -0.0098, -0.0075, -0.0019,\n",
            "         0.0061, -0.0059,  0.0211, -0.0086, -0.0138, -0.0035,  0.0104, -0.0025,\n",
            "        -0.0007, -0.0189, -0.0014,  0.0025,  0.0040], device='cuda:0',\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.0728,  0.0024,  0.0557,  ..., -0.0147, -0.0591, -0.0098],\n",
            "        [ 0.0333, -0.0365, -0.0048,  ..., -0.0680,  0.0217,  0.0010],\n",
            "        [ 0.0679,  0.0396,  0.0450,  ..., -0.0432,  0.0165,  0.0549],\n",
            "        ...,\n",
            "        [ 0.0465,  0.0120,  0.0610,  ...,  0.0627,  0.0060,  0.0742],\n",
            "        [ 0.0102, -0.0165, -0.0463,  ...,  0.0601, -0.0433,  0.0691],\n",
            "        [ 0.0358,  0.0011,  0.0296,  ...,  0.0675,  0.0149, -0.0330]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([ 0.0293,  0.0403,  0.0065,  ...,  0.0763, -0.0358,  0.0399],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.0048,  0.0068, -0.0156,  ..., -0.0128, -0.0045,  0.0130],\n",
            "        [-0.0050,  0.0063,  0.0004,  ...,  0.0096, -0.0056, -0.0016],\n",
            "        [ 0.0127, -0.0076, -0.0152,  ...,  0.0049, -0.0079,  0.0032],\n",
            "        ...,\n",
            "        [ 0.0131, -0.0054,  0.0022,  ..., -0.0082,  0.0106, -0.0127],\n",
            "        [-0.0097, -0.0015,  0.0150,  ..., -0.0139,  0.0082,  0.0113],\n",
            "        [ 0.0072, -0.0033, -0.0096,  ..., -0.0082, -0.0090, -0.0142]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([-1.4644e-02,  1.3843e-02, -1.2527e-02, -1.8397e-02,  2.4651e-02,\n",
            "        -2.2901e-03,  1.0966e-02, -7.8560e-03,  6.8127e-04,  2.2343e-03,\n",
            "         8.6920e-03,  3.6325e-04,  4.7929e-04,  1.9350e-02,  2.5472e-02,\n",
            "         1.3570e-02, -2.2760e-02, -3.0134e-03, -1.3062e-02, -2.5668e-02,\n",
            "        -1.2565e-02,  2.2170e-02, -2.1142e-02,  1.6898e-02,  9.7861e-03,\n",
            "        -2.1066e-03, -1.6608e-02,  1.4650e-02,  2.2596e-03, -1.8276e-02,\n",
            "         2.4887e-03,  1.5826e-03,  1.6918e-02,  2.1985e-02, -5.6207e-03,\n",
            "         2.0825e-03,  1.7231e-02,  1.8756e-02,  5.6741e-03,  1.3048e-02,\n",
            "        -1.3364e-02,  2.4034e-02, -1.7756e-02, -9.4156e-04,  1.5844e-02,\n",
            "         3.6329e-03, -1.7657e-02,  1.0409e-02,  6.7563e-03, -6.1903e-03,\n",
            "        -6.3573e-04, -2.4493e-02, -3.6173e-03,  2.3673e-02,  1.4898e-02,\n",
            "        -2.3911e-03, -1.5265e-02,  4.2672e-03, -7.7612e-04,  2.1112e-02,\n",
            "        -7.0015e-03, -1.1139e-02,  1.2268e-03, -1.6741e-02,  1.7541e-02,\n",
            "        -1.4529e-02,  2.5273e-02,  1.2298e-02,  2.2361e-02, -1.5622e-02,\n",
            "        -1.1946e-02, -1.1218e-02, -5.4795e-03, -1.6717e-02,  1.5478e-02,\n",
            "         1.6456e-02,  5.5651e-03,  1.0748e-02,  7.2967e-03,  2.4066e-02,\n",
            "         1.7235e-02, -1.9173e-02, -2.2804e-02,  6.4362e-03, -1.4290e-02,\n",
            "         2.4794e-02,  1.6925e-02,  2.6937e-03,  1.7403e-02, -7.6603e-03,\n",
            "         2.2308e-02,  1.6655e-02,  1.3902e-02, -7.3759e-03, -2.5543e-02,\n",
            "        -2.4332e-03, -4.4513e-03,  1.6072e-02, -6.7764e-04, -2.5473e-02,\n",
            "         3.1916e-04,  4.4499e-03, -1.5601e-02, -2.0498e-02,  1.1919e-03,\n",
            "        -1.9543e-04,  2.7326e-02,  4.6801e-04, -2.6475e-02,  4.2807e-03,\n",
            "         1.7919e-02, -9.9845e-03, -2.7001e-02, -2.3675e-02,  1.9171e-03,\n",
            "        -5.6959e-03, -8.3362e-03, -1.7565e-03,  2.5014e-02, -1.5443e-02,\n",
            "        -1.5901e-02, -3.5966e-03,  1.9060e-02, -8.9553e-03,  1.6295e-03,\n",
            "        -1.6863e-03, -2.2393e-02, -2.3305e-02, -8.2628e-03,  1.4140e-03,\n",
            "        -1.0867e-03, -3.2346e-03,  2.1182e-02, -1.9241e-03,  7.4982e-03,\n",
            "        -3.3342e-03, -1.7012e-02,  1.1298e-02, -9.5925e-03,  1.5533e-02,\n",
            "        -1.3206e-03, -1.9074e-02, -2.6056e-02, -1.4066e-02, -1.9910e-03,\n",
            "         2.3553e-04,  1.7014e-02,  5.2460e-03,  8.8764e-03, -1.5260e-02,\n",
            "        -2.6359e-02,  5.9548e-03,  1.2944e-03,  2.5762e-02,  1.6239e-02,\n",
            "         1.1412e-02,  3.7344e-03,  6.9325e-03,  1.3500e-02, -8.3098e-03,\n",
            "         1.7460e-02,  2.2192e-02, -9.8030e-03,  1.1987e-02, -8.8472e-03,\n",
            "         5.9974e-04,  3.2075e-03, -2.7225e-03, -1.3326e-02,  2.2168e-02,\n",
            "        -2.0909e-02, -9.5618e-03, -1.4464e-02, -2.5885e-02, -1.3820e-02,\n",
            "         8.3289e-03,  5.7389e-03, -2.1209e-02,  9.1357e-03,  1.4905e-03,\n",
            "        -7.4595e-03,  1.0407e-02,  1.2859e-03,  1.3825e-02,  3.2070e-03,\n",
            "         8.8182e-03,  7.8281e-03,  1.2127e-03,  1.1035e-02,  8.5492e-03,\n",
            "        -4.9211e-03, -9.8871e-03,  2.6144e-02, -1.3497e-02,  2.4007e-02,\n",
            "        -1.9702e-03, -7.2920e-03,  2.2792e-02,  2.0227e-02,  7.7653e-03,\n",
            "         2.7562e-03, -1.3269e-02,  8.4357e-03,  1.7608e-02, -5.5362e-03,\n",
            "         2.7428e-02, -4.9257e-03, -9.1144e-03, -6.1610e-03,  3.6534e-03,\n",
            "        -7.9441e-03, -8.0316e-03,  1.9137e-02,  2.3530e-02, -9.7611e-03,\n",
            "         9.5943e-04, -4.3924e-03, -5.9950e-03, -2.0498e-02,  1.0096e-02,\n",
            "         1.0628e-02,  5.8918e-03,  7.0928e-03, -2.3847e-03,  2.6356e-02,\n",
            "         2.0929e-02, -2.6767e-03,  1.6108e-02, -8.9011e-03, -9.6668e-03,\n",
            "         2.7426e-03, -8.4323e-03, -5.5784e-03,  1.5348e-02,  9.6576e-03,\n",
            "         1.9061e-02,  6.4060e-03, -1.5861e-03,  6.4374e-04,  2.1010e-02,\n",
            "        -1.0807e-02,  9.3299e-03,  9.3520e-03,  3.0722e-03,  1.0389e-02,\n",
            "         1.7838e-03, -5.8128e-03,  1.1658e-02, -2.7595e-03,  2.1957e-03,\n",
            "        -2.3064e-03, -4.3018e-03, -1.5722e-02,  5.6121e-03,  8.5441e-04,\n",
            "        -7.5549e-04,  2.3234e-02, -5.3248e-03,  1.7665e-03, -1.0592e-02,\n",
            "        -1.2120e-02, -1.3246e-02,  1.0791e-02, -3.1659e-03, -6.5286e-03,\n",
            "         1.4590e-02, -1.1002e-02, -2.7125e-02, -7.3621e-03,  1.4300e-02,\n",
            "         1.3117e-02,  1.1664e-02, -2.1498e-02,  9.1874e-03,  2.1436e-03,\n",
            "        -1.6381e-03, -6.4482e-03, -1.7519e-02,  1.5859e-03,  3.8960e-03,\n",
            "        -2.2561e-02,  8.3653e-03,  1.1313e-02,  2.6038e-02, -1.1439e-02,\n",
            "         2.0768e-02, -7.4530e-03,  1.5476e-02, -1.9201e-02,  6.0592e-03,\n",
            "         3.9559e-03, -1.0961e-02, -2.6849e-02, -1.4753e-03, -9.2095e-03,\n",
            "         7.9068e-04, -3.0674e-03,  2.4397e-02, -7.3466e-03,  1.4752e-03,\n",
            "         1.4022e-02,  1.2814e-03, -1.8170e-02,  1.6697e-03,  1.3434e-03,\n",
            "         2.8009e-02, -6.2157e-03, -2.3876e-03,  2.0979e-03, -2.5089e-02,\n",
            "         2.5124e-02,  1.7657e-02,  6.8489e-03, -1.7483e-02,  1.3389e-02,\n",
            "         1.8967e-02,  2.5338e-02, -1.2681e-02, -2.4966e-02, -1.6271e-02,\n",
            "         1.1686e-02,  2.1885e-02,  9.6003e-03,  2.3377e-02,  5.4437e-03,\n",
            "         2.3863e-02, -1.2017e-02, -1.9796e-02,  1.1716e-03, -1.0416e-02,\n",
            "        -2.0184e-02,  2.5519e-04,  7.9147e-03,  3.9813e-03,  1.4158e-02,\n",
            "         2.5038e-02, -3.9819e-03, -1.7372e-02,  1.1258e-02, -9.5821e-03,\n",
            "         2.0847e-02, -2.4019e-02, -3.4895e-05, -1.1505e-02, -3.5578e-03,\n",
            "        -1.3804e-02, -1.3102e-02, -1.6777e-02, -2.3434e-02, -6.4262e-04,\n",
            "        -1.1724e-02,  2.2873e-02,  7.0825e-03, -4.6588e-03,  1.6240e-02,\n",
            "        -3.4072e-03, -3.4415e-03, -3.5931e-03, -2.6664e-03, -1.4400e-02,\n",
            "         1.1790e-02,  7.0499e-03,  8.7596e-03,  2.2459e-02,  3.8535e-03,\n",
            "         1.1919e-02, -4.3202e-03,  1.5887e-02,  3.1076e-03, -2.0931e-02,\n",
            "        -3.7907e-03,  8.5344e-03, -2.9826e-03, -4.1587e-03,  6.8159e-03,\n",
            "         6.7377e-03, -1.5817e-02, -2.2048e-03,  3.2126e-03, -3.0469e-03,\n",
            "         1.7571e-02, -1.4804e-02,  1.5351e-02,  6.9015e-03, -9.0027e-03,\n",
            "        -2.0431e-02,  1.9026e-03,  1.7629e-03, -2.3028e-02,  2.2354e-02,\n",
            "        -5.1609e-03, -5.2296e-04,  8.2431e-03,  1.3441e-02, -1.9158e-02,\n",
            "        -2.3097e-03, -3.0997e-03, -1.8904e-02,  3.0504e-03, -1.0565e-02,\n",
            "        -6.8210e-03,  1.3265e-02,  4.5539e-03, -2.6578e-02, -5.3818e-03,\n",
            "        -1.0341e-02,  1.7870e-02, -1.2073e-02, -2.0424e-02, -1.8491e-03,\n",
            "        -1.4946e-02,  1.7958e-02,  1.0010e-02, -1.6775e-02,  4.7408e-03,\n",
            "        -1.9533e-02, -2.2132e-02,  9.5863e-03, -6.1018e-03,  2.7769e-03,\n",
            "         2.2510e-02, -4.1825e-03,  1.9992e-02,  9.9069e-03, -1.5425e-02,\n",
            "         7.4287e-03,  2.1685e-02, -1.4970e-02, -1.8587e-02, -8.3793e-03,\n",
            "        -2.0370e-03, -1.9584e-02,  4.5874e-05,  2.4449e-02], device='cuda:0',\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[-0.0464,  0.0133,  0.0376,  ...,  0.0441,  0.0363, -0.0459],\n",
            "        [-0.0325, -0.0073,  0.0480,  ...,  0.0304,  0.0183, -0.0236],\n",
            "        [-0.0188,  0.0226,  0.0366,  ...,  0.0452, -0.0340, -0.0122],\n",
            "        ...,\n",
            "        [-0.0085,  0.0367, -0.0133,  ...,  0.0387,  0.0002,  0.0095],\n",
            "        [ 0.0381,  0.0094, -0.0364,  ..., -0.0415,  0.0039,  0.0432],\n",
            "        [-0.0355, -0.0332,  0.0135,  ..., -0.0234,  0.0435,  0.0176]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([-1.7810e-02,  4.3928e-02, -4.6167e-02, -5.7844e-02,  2.5607e-02,\n",
            "        -3.6344e-02, -5.7275e-02,  5.6901e-02, -3.5760e-02,  2.2000e-02,\n",
            "         2.4882e-02,  1.2746e-02, -2.6100e-02, -1.1631e-02,  3.7805e-02,\n",
            "         4.2290e-02,  1.3360e-02, -5.8672e-03,  1.9543e-03,  3.9945e-02,\n",
            "         2.1227e-02,  4.6635e-03,  1.9416e-02,  1.7688e-02, -1.9803e-02,\n",
            "         2.9280e-02,  5.5964e-02, -1.3996e-02,  7.2230e-03,  6.7366e-03,\n",
            "        -1.7436e-02, -3.0400e-02,  7.3804e-03,  1.2622e-02, -3.2260e-02,\n",
            "        -3.0313e-03, -3.1447e-02,  1.5321e-02, -5.1065e-03,  2.8720e-02,\n",
            "        -2.8811e-02,  4.4901e-03,  4.2810e-02, -1.5304e-03,  3.2660e-02,\n",
            "        -3.2740e-02, -3.2036e-02,  2.2104e-02, -4.2276e-02, -1.0148e-03,\n",
            "         7.7917e-03,  4.0385e-02, -2.6359e-02,  1.1877e-02,  2.9785e-02,\n",
            "        -3.9445e-02, -3.6864e-02,  2.7061e-02, -1.0523e-03,  2.2353e-02,\n",
            "        -3.2761e-02, -3.8154e-03, -3.6965e-02, -3.3676e-02,  3.6247e-02,\n",
            "        -4.2055e-02, -6.3837e-03,  2.7117e-02,  1.7869e-02,  5.2881e-04,\n",
            "         5.2587e-02, -1.3209e-03, -2.9531e-02,  3.2885e-02, -3.4076e-02,\n",
            "         1.7658e-02, -2.6877e-02,  3.8775e-02,  3.2376e-02, -5.5967e-02,\n",
            "         4.0872e-02,  2.3486e-02, -5.7503e-03,  5.6345e-02,  1.9224e-02,\n",
            "        -1.6338e-02,  9.3231e-03,  3.7065e-02,  2.8037e-02,  2.6753e-02,\n",
            "         4.2873e-02, -6.8517e-03, -5.6712e-02, -2.3173e-03,  2.5100e-02,\n",
            "         3.3572e-02,  1.7750e-02,  1.1553e-02, -2.3382e-02, -2.0777e-02,\n",
            "         3.1719e-02, -4.7914e-02,  2.4772e-02,  2.6637e-02, -3.3452e-02,\n",
            "         1.5422e-02, -1.7943e-02,  3.9847e-02,  4.5218e-03, -1.7480e-02,\n",
            "        -3.3284e-02, -1.2470e-02, -3.5548e-02, -1.1512e-02, -1.2315e-02,\n",
            "         5.5333e-02, -4.9137e-02,  7.2149e-03,  3.4070e-02, -5.0438e-02,\n",
            "        -8.2175e-03, -2.4857e-02, -2.6482e-02,  5.9669e-03,  3.9256e-02,\n",
            "        -1.7506e-02,  1.3802e-02,  6.3889e-04, -6.0112e-02,  1.1416e-02,\n",
            "        -2.4013e-02,  4.1903e-02,  3.9561e-02, -2.1764e-02,  3.5490e-02,\n",
            "        -4.9130e-02, -4.7470e-02, -1.8963e-02,  3.1622e-02,  1.6098e-02,\n",
            "        -2.3001e-03,  1.9195e-02, -1.4975e-02,  3.5293e-02, -3.9127e-03,\n",
            "        -3.9241e-02, -5.8383e-04, -5.1505e-02,  3.6242e-02,  2.2060e-02,\n",
            "         4.5033e-02, -4.5195e-02, -5.7196e-02,  1.7795e-02,  3.8578e-02,\n",
            "        -3.1794e-02,  6.8693e-03, -4.3592e-03,  6.4483e-03, -2.4904e-02,\n",
            "        -3.8008e-02, -1.0330e-02,  7.5716e-04,  2.9490e-02, -1.7882e-02,\n",
            "        -2.5430e-02, -9.9079e-03,  4.6341e-02, -3.0572e-05,  2.8945e-02,\n",
            "         4.1468e-02, -3.3616e-02,  2.8422e-02, -1.4021e-03, -2.9983e-03,\n",
            "        -2.0896e-02,  2.2347e-02,  3.1889e-02, -3.8535e-03,  1.2989e-03,\n",
            "        -5.6935e-02,  1.6116e-02,  2.7690e-02, -1.5700e-02,  2.3980e-02,\n",
            "         1.8666e-02, -2.0226e-02,  2.7883e-02, -2.1903e-02, -4.9673e-02,\n",
            "         2.5380e-02, -3.2188e-02,  3.0799e-02, -1.5222e-02, -2.5142e-02,\n",
            "        -3.9524e-02, -2.8643e-02, -1.8366e-03, -3.8581e-02,  1.0627e-02,\n",
            "         4.8672e-02,  4.1579e-02, -1.0490e-02,  3.8122e-02,  3.0120e-03,\n",
            "         8.3475e-04, -1.2183e-03,  5.6835e-02,  3.7499e-02,  1.4778e-02,\n",
            "        -3.5433e-02,  2.7066e-02,  5.1789e-02, -3.2648e-02, -1.6261e-02,\n",
            "        -2.6893e-02, -1.8389e-02,  1.6791e-02, -3.2481e-02, -5.0878e-02,\n",
            "        -3.1422e-02,  3.5886e-02,  5.2147e-02, -2.3809e-02, -7.6526e-04,\n",
            "        -3.6575e-02, -4.8783e-02,  6.0105e-02,  9.2847e-03, -4.9846e-02,\n",
            "        -2.6556e-02, -2.4201e-02,  4.5232e-03, -5.6516e-02,  1.3692e-02,\n",
            "        -4.5855e-02,  9.4091e-03,  4.6285e-02, -6.2000e-03], device='cuda:0',\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.0511, -0.0240, -0.0515,  ...,  0.0092, -0.0358,  0.0257],\n",
            "        [-0.0436, -0.0326, -0.0154,  ..., -0.0225,  0.0041, -0.0105],\n",
            "        [ 0.0262,  0.0642, -0.0537,  ..., -0.0453,  0.0381,  0.0045],\n",
            "        ...,\n",
            "        [-0.0170, -0.0535, -0.0527,  ..., -0.0440, -0.0114, -0.0065],\n",
            "        [-0.0266,  0.0033,  0.0398,  ...,  0.0001, -0.0328, -0.0451],\n",
            "        [-0.0162, -0.0302, -0.0502,  ..., -0.0144, -0.0255,  0.0290]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([ 0.0076,  0.0378, -0.0146,  ...,  0.0550,  0.0526,  0.0182],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([[ 2.1837e-02, -5.2224e-04,  1.9225e-02,  ...,  1.6239e-05,\n",
            "          9.3617e-03, -4.1860e-03]], device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([0.0346], device='cuda:0', requires_grad=True)]\n"
          ]
        }
      ],
      "source": [
        "print(list(net_loaded.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InGW0Xq6UZip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a665ae2-7598-4066-8b73-e9fcf1ced50b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rho.shape:  torch.Size([20])\n",
            "epsilon.shape:  torch.Size([20])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5.8377e-01,  3.5042e+01,  2.4076e+01,  5.7846e+01,  7.4569e+01,\n",
              "         -8.5106e+00, -1.5336e+00,  5.7988e+00,  1.0729e+00,  4.8974e-02,\n",
              "          7.0864e-02,  9.2059e-01,  8.6702e-02,  1.0328e+00],\n",
              "        [ 2.6172e+00,  2.4673e+01,  5.5416e+01,  8.9492e+00,  7.5616e+01,\n",
              "         -9.3517e+00,  4.0517e+00, -4.2435e+00,  9.5271e-01,  7.5467e-02,\n",
              "          3.8309e-02,  9.7772e-01,  6.4975e-02,  1.0279e+00],\n",
              "        [ 6.3902e+00,  4.1409e+01,  3.7410e+01, -3.5789e+01,  8.7587e+01,\n",
              "         -3.0872e+00, -8.3299e+00, -5.5289e+00,  1.0909e+00,  7.3099e-02,\n",
              "          4.3896e-02,  1.0323e+00,  3.1160e-02,  9.6139e-01],\n",
              "        [ 6.6427e+00, -2.3341e+01,  1.5950e+02,  5.5226e+01,  2.2358e+02,\n",
              "          6.2581e+00,  8.2670e+00,  6.5763e+00,  9.6335e-01,  3.7827e-02,\n",
              "          8.1243e-02,  9.9831e-01,  8.3793e-02,  1.0077e+00],\n",
              "        [ 1.5121e+00,  1.0021e+02,  6.7436e+00,  6.4227e+01,  1.1664e+02,\n",
              "         -4.8554e+00, -7.2420e+00,  6.4374e+00,  1.0282e+00,  4.8482e-02,\n",
              "          8.3963e-02,  1.0565e+00,  7.4893e-02,  9.8567e-01],\n",
              "        [ 5.7512e+00,  4.3222e+01,  2.1950e+01,  3.5323e+01,  4.7973e+01,\n",
              "         -7.6748e+00,  5.0884e+00,  3.1853e+00,  1.0437e+00,  7.7207e-02,\n",
              "          4.8375e-02,  1.0540e+00,  9.6864e-02,  9.7652e-01],\n",
              "        [ 2.7837e+00,  1.9270e+01,  3.7615e+01,  5.9570e+01,  7.0049e+01,\n",
              "         -1.5326e+00, -7.6817e+00,  5.3490e+00,  9.7067e-01,  8.3424e-02,\n",
              "          6.3331e-02,  9.7535e-01,  6.9460e-02,  9.4015e-01],\n",
              "        [ 3.0579e+00,  1.4203e+02,  2.8549e+01,  9.4715e+01,  1.6778e+02,\n",
              "         -3.9336e+00, -9.0791e+00,  8.1323e+00,  9.2625e-01,  1.4362e-02,\n",
              "          9.7974e-02,  1.0602e+00,  4.7402e-02,  1.0905e+00],\n",
              "        [ 2.8763e+00,  3.3664e+01,  2.7970e+01,  2.3158e+01,  4.5487e+01,\n",
              "          4.3326e+00,  8.1458e-03, -6.3743e+00,  1.0506e+00,  1.4861e-02,\n",
              "          6.1688e-02,  1.0154e+00,  3.4440e-05,  1.0866e+00],\n",
              "        [ 1.7881e+00, -2.2412e+01,  2.0476e+01,  3.5014e+01,  6.5852e+01,\n",
              "          9.5042e+00,  2.8718e+00,  4.0540e+00,  9.0355e-01,  8.5844e-02,\n",
              "          3.1554e-02,  9.7717e-01,  5.7381e-02,  1.0868e+00],\n",
              "        [ 2.5335e+00,  3.3902e+01,  3.8248e+01, -3.8741e+01,  8.6328e+01,\n",
              "         -8.1244e-01,  8.1626e+00,  8.7058e+00,  9.4669e-01,  6.8988e-02,\n",
              "          6.1792e-02,  9.3396e-01,  6.9287e-02,  1.0817e+00],\n",
              "        [ 1.0190e+00,  1.4953e+00,  1.6723e+01,  1.1232e+01,  2.2157e+01,\n",
              "         -2.1019e-01,  3.2672e+00, -4.7604e+00,  9.0257e-01,  1.6882e-02,\n",
              "          8.9972e-02,  1.0335e+00,  8.1534e-02,  9.3464e-01],\n",
              "        [ 5.4018e-01,  3.0875e+01,  2.3680e+01, -3.2117e+00,  5.8157e+01,\n",
              "          3.1581e+00, -2.3900e+00,  9.0963e+00,  9.8874e-01,  4.6540e-02,\n",
              "          2.7140e-02,  9.3215e-01,  4.7964e-02,  9.4033e-01],\n",
              "        [ 1.1779e+00,  2.2733e+01,  1.2946e+01,  1.3494e+01,  2.8870e+01,\n",
              "          3.9744e+00, -3.4903e-01, -5.4219e+00,  1.0131e+00,  3.0226e-02,\n",
              "          6.8431e-02,  9.0421e-01,  6.5416e-02,  1.0644e+00],\n",
              "        [ 9.7427e+00,  5.4020e+01,  1.8680e+02,  1.6321e+02,  2.3315e+02,\n",
              "         -4.7966e-01,  2.0455e+00, -1.7463e-01,  1.0413e+00,  6.4614e-03,\n",
              "          5.0919e-02,  1.0991e+00,  1.1191e-02,  1.0883e+00],\n",
              "        [ 2.7622e-01, -1.5288e+00, -2.2750e+00,  5.6582e+00,  2.1393e+01,\n",
              "          3.4338e+00,  4.3835e+00,  2.6005e+00,  9.9888e-01,  8.0051e-02,\n",
              "          4.1004e-02,  9.9714e-01,  3.5526e-02,  9.5893e-01],\n",
              "        [ 3.2224e+00,  7.3073e+01, -4.1187e+01,  7.6636e+01,  1.3171e+02,\n",
              "          8.1205e+00,  9.3642e+00,  3.3348e-01,  9.9445e-01,  6.6610e-02,\n",
              "          3.9262e-02,  1.0329e+00,  5.8211e-02,  1.0761e+00],\n",
              "        [ 4.0029e-01,  2.0407e+01,  1.5296e+01,  6.6522e+00,  3.0296e+01,\n",
              "          3.7005e+00, -2.4887e+00, -5.5295e+00,  9.9369e-01,  2.8811e-02,\n",
              "          9.1364e-02,  9.3627e-01,  3.7993e-04,  1.0110e+00],\n",
              "        [ 2.1247e+00,  9.4990e+00,  2.1780e+01,  8.7052e-01,  2.3981e+01,\n",
              "          1.9450e+00,  6.9929e-01,  3.4542e+00,  9.3014e-01,  9.3181e-02,\n",
              "          7.2097e-03,  1.0956e+00,  5.2762e-02,  1.0578e+00],\n",
              "        [ 8.2863e-01,  1.1925e+01, -3.7884e+00,  5.0949e+00,  1.9714e+01,\n",
              "          1.6236e+00,  5.3208e+00,  8.5778e-01,  9.4574e-01,  4.9860e-02,\n",
              "          2.9984e-02,  9.5927e-01,  1.8922e-02,  1.0004e+00]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ],
      "source": [
        "rho_example, epsilon_example, vx_example, vy_example, vz_example, Bx_example, By_example, Bz_example, gxx_example, gxy_example, gxz_example, gyy_example, gyz_example, gzz_example = generate_samples(20)\n",
        "\n",
        "inputs =  generate_input_data(rho_example, epsilon_example, vx_example, vy_example, vz_example, Bx_example, By_example, Bz_example, gxx_example, gxy_example, gxz_example, gyy_example, gyz_example, gzz_example)\n",
        "inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVa1upmFUZip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "113e242d-7987-4ff2-e39e-8f9806140470"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[0.3023]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.1900]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.2093]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.1500]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.4054]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.2677]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.2610]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.4729]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.2256]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.2286]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.2008]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.1993]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.2208]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.2230]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.3046]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.2329]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.4824]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.2140]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.1914]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.2360]], device='cuda:0', grad_fn=<AddmmBackward0>)]"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "#%%script echo skipping\n",
        "\n",
        "# Pass the inputs to the network and get the outputs\n",
        "outputs = [net(input.unsqueeze(0)) for input in inputs]\n",
        "# Print the outputs\n",
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-Xjfo7VUZir",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5255e8b-2717-42ae-c72e-3dcceea2f7d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[0.3023]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.1900]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.2093]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.1500]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.4054]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.2677]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.2610]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.4729]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.2256]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.2286]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.2008]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.1993]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.2208]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.2230]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.3046]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.2329]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.4824]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.2140]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.1914]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[0.2360]], device='cuda:0', grad_fn=<AddmmBackward0>)]"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ],
      "source": [
        "\n",
        "# Pass the inputs to the network and get the outputs\n",
        "outputs = [net_loaded(input.unsqueeze(0)) for input in inputs]\n",
        "# Print the outputs\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjpIvdybUZis"
      },
      "source": [
        "## Porting the model to C++"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMlEd4RoUZis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff7f3045-afd5-46d8-fe1b-d47ec6182772"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (hidden_activation): PReLU(num_parameters=1)\n",
              "  (output_activation): Identity()\n",
              "  (layers): ModuleList(\n",
              "    (0): Linear(in_features=14, out_features=3558, bias=True)\n",
              "    (1): Linear(in_features=3558, out_features=165, bias=True)\n",
              "    (2): Linear(in_features=165, out_features=3906, bias=True)\n",
              "    (3): Linear(in_features=3906, out_features=434, bias=True)\n",
              "    (4): Linear(in_features=434, out_features=239, bias=True)\n",
              "    (5): Linear(in_features=239, out_features=1274, bias=True)\n",
              "    (6): Linear(in_features=1274, out_features=1, bias=True)\n",
              "  )\n",
              "  (dropouts): ModuleList(\n",
              "    (0-5): 6 x Dropout(p=0.06388872518840581, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 107
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-74-61389beeeffb>:62: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert x.shape[1] == N_INPUTS, f\"x must have shape (batch_size, {N_INPUTS})\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rho.shape:  torch.Size([1])\n",
            "epsilon.shape:  torch.Size([1])\n",
            "input shape:  torch.Size([1, 14])\n",
            "input:  tensor([[ 1.0234e+00,  1.8287e+01,  4.0764e+01,  4.6277e+01,  7.8273e+01,\n",
            "          5.8426e+00, -7.7507e+00,  4.7959e+00,  1.0050e+00,  4.0137e-02,\n",
            "          9.3720e-02,  9.3847e-01,  2.5888e-02,  9.8740e-01]], device='cuda:0')\n",
            "Output: tensor([[0.2368]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch.jit\n",
        "\n",
        "# Creating a dummy input tensor of shape (1, 5) to trace the model\n",
        "dummy_input = torch.randn(1, N_INPUTS).to(device)\n",
        "\n",
        "# Ensure that net_loaded is in evaluation mode.\n",
        "net_loaded.eval()\n",
        "\n",
        "# Tracing the model using the torch.jit.trace function\n",
        "traced_model = torch.jit.trace(net_loaded, dummy_input)\n",
        "\n",
        "# Saving the traced model to a file named \"net.pt\"\n",
        "traced_model.save(\"net.pt\")\n",
        "save_file(\"net.pt\")\n",
        "\n",
        "example_input_to_validate_correct_export_and_import = generate_input_data(*generate_samples(1))\n",
        "print(\"input shape: \", example_input_to_validate_correct_export_and_import.shape)\n",
        "print(\"input: \", example_input_to_validate_correct_export_and_import)\n",
        "print(\"Output:\", net_loaded(example_input_to_validate_correct_export_and_import))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x4BxmMpXvnIf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPvB1xoSUZhR"
      },
      "source": [
        "# Neural network to learn conservative-to-primitive conversion in relativistic hydrodynamics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eKD484DvExD"
      },
      "source": [
        "## How to use this notebook\n",
        "\n",
        "### Local installation\n",
        "\n",
        "1. Install required packages with `pip install -r requirements.txt` to your desired environment.\n",
        "2. If a script version of this notebook is desired, comment (not uncomment) the first line of `nbconvert` cell.\n",
        "\n",
        "### Colab installation\n",
        "\n",
        "1.  Comment (not uncomment) the first line of the drive mounting cell.\n",
        "2.  Comment (not uncomment) the first line of the `pip install` cell.\n",
        "\n",
        "<!-- - For colab we also want to set the runtime to GPU by clicking _Change runtime_ in the _Runtime_ menu, and -->\n",
        "<!-- - We want to wait for the google drive connection popup to appear and follow the instructions. -->\n",
        "\n",
        "### Training without optimization\n",
        "\n",
        "3. Set `OPTIMIZE = False` in section _Constants and flags to set_.\n",
        "4. Run the entire notebook.\n",
        "\n",
        "### Training with optimization\n",
        "\n",
        "3. Set `OPTIMIZE = True` in section _Constants and flags to set_.\n",
        "4. Run the entire notebook.\n",
        "\n",
        "### Loading an already trained model\n",
        "\n",
        "3. Run cells in section _Initialization_.\n",
        "4. Run cells with definitions in section _Generating the data_.\n",
        "5. Run cell with the definition of _Net_ in section _Defining the neural network_.\n",
        "6. Make sure the `net.pth`, `optimizer.pth`, `scheduler.pth`, `var_dict.json` and `train_output.csv` files are in the directory containing this notebook.\n",
        "7. Run the cells in section _Loading_ and continue from there.\n",
        "\n",
        "### Generating the C++ model\n",
        "\n",
        "8. Run section _Porting the model to C++_, this requires a model to be loaded.\n",
        "9. Set the path to the `net.pt` file in the C++ source file.\n",
        "10. `mkdir build && cd build`,\n",
        "11. `cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch/ ..`,\n",
        "10. Compile and run, e.g. `cmake --build . --config release && ./executable`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYPfIIglvExD"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsv90vHWvExE"
      },
      "source": [
        "\n",
        "Use this first cell to **convert this notebook** to a python script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqdgdNLHUZhV",
        "outputId": "e8f516a3-bd25-4dbf-bb35-1236a2bdfde8",
        "tags": [
          "remove_cell"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "!jupyter nbconvert five_parameter_run_1.ipynb --TagRemovePreprocessor.enabled=True --TagRemovePreprocessor.remove_cell_tags='{\"remove_cell\"}' --to script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzcUr0LnUZhw"
      },
      "source": [
        "Next some cells for working on **google colab**,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Jm_7_2r1vExG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# check if the drive is mounted\n",
        "drive_mounted = os.path.exists(\"/content/drive\")\n",
        "# change this to your desired folder\n",
        "drive_folder = \"/content/drive/My Drive/bsc/con2prim_towards_GRMHD/five_parameter_run_1\"\n",
        "\n",
        "# define a function to save a file to the drive or the current directory\n",
        "def save_file(file_name):\n",
        "  if drive_mounted:\n",
        "    # save the file to the drive folder\n",
        "    file_path = os.path.join(drive_folder, file_name)\n",
        "    # copy the file from the current directory to the drive folder\n",
        "    shutil.copyfile(file_name, file_path)\n",
        "  else:\n",
        "    # do nothing as the file is already in the current directory\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecHw2_xlUZhx",
        "outputId": "90c6837b-6e9c-4ffe-b473-4ca47a26748d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#%%script echo skipping\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1rcStMLUZhy",
        "outputId": "60b02bf7-6410-41e9-de40-d068373eb45c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.1.1-py3-none-any.whl (365 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.12.2)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes>=0.9.1 (from optuna)\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.4.3)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.40.0)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n",
            "Installing collected packages: tensorboardX, Mako, colorlog, cmaes, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.11.1 cmaes-0.9.1 colorlog-6.7.0 optuna-3.1.1 tensorboardX-2.6\n"
          ]
        }
      ],
      "source": [
        "#%%script echo skipping\n",
        "\n",
        "!pip install optuna tensorboard tensorboardX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDsq9gG1vExH"
      },
      "source": [
        "Importing the **libraries** and setting the **device**,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tREdWQUVUZhz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import optuna\n",
        "import tensorboardX as tbx\n",
        "\n",
        "# Checking if GPU is available and setting the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38GvmerjUZhz"
      },
      "source": [
        "### Constants and flags to set\n",
        "Defining some constants and parameters for convenience.\n",
        "\n",
        "**NOTE**: Some **subparameters** still need to be adjusted in the `create_model` function itself as of (Tue May 16 07:42:45 AM CEST 2023)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ei6VZDYKUZh0"
      },
      "outputs": [],
      "source": [
        "\n",
        "N_TRIALS = 150 # Number of trials for hyperparameter optimization\n",
        "OPTIMIZE = True # Whether to optimize the hyperparameters or to use predetermined values from Dieseldorst et al..\n",
        "ZSCORE_NORMALIZATION = False # Whether to z-score normalize the input data.\n",
        "\n",
        "# I try out here the values as obtained in Optuna run 5, but I will increase the number of epochs.\n",
        "# N_LAYERS_NO_OPT = 3\n",
        "# N_UNITS_NO_OPT = [78, 193, 99]\n",
        "# HIDDEN_ACTIVATION_NAME_NO_OPT = \"ReLU\"\n",
        "# OUTPUT_ACTIVATION_NAME_NO_OPT = \"Linear\"\n",
        "# LOSS_NAME_NO_OPT = \"MSE\"\n",
        "# OPTIMIZER_NAME_NO_OPT = \"Adam\"\n",
        "# LR_NO_OPT = 0.00036516467819506355\n",
        "# BATCH_SIZE_NO_OPT = 170\n",
        "# N_EPOCHS_NO_OPT = 400\n",
        "# SCHEDULER_NAME_NO_OPT = \"ReduceLROnPlateau\"\n",
        "\n",
        "N_LAYERS_NO_OPT = 3\n",
        "N_UNITS_NO_OPT = [555, 458, 115]\n",
        "HIDDEN_ACTIVATION_NAME_NO_OPT = \"ReLU\"\n",
        "OUTPUT_ACTIVATION_NAME_NO_OPT = \"ReLU\"\n",
        "LOSS_NAME_NO_OPT = \"Huber\"\n",
        "OPTIMIZER_NAME_NO_OPT = \"RMSprop\"\n",
        "LR_NO_OPT = 0.000122770896701404\n",
        "BATCH_SIZE_NO_OPT = 49\n",
        "N_EPOCHS_NO_OPT = 400\n",
        "SCHEDULER_NAME_NO_OPT = \"ReduceLROnPlateau\"\n",
        "\n",
        "c = 1  # Speed of light (used in compute_conserved_variables and sample_primitive_variables functions)\n",
        "gamma = 5 / 3  # Adiabatic index (used in eos_analytic function)\n",
        "n_train_samples = 80000 # Number of training samples (used in generate_input_data and generate_labels functions)\n",
        "n_test_samples = 10000 # Number of test samples (used in generate_input_data and generate_labels functions)\n",
        "rho_interval = (0, 10.1) # Sampling interval for rest-mass density (used in sample_primitive_variables function)\n",
        "vx_interval = (0, .57 * c) # Sampling interval for velocity in x-direction (used in sample_primitive_variables function)\n",
        "vy_interval = (0, .57 * c) # Sampling interval for velocity in y-direction (used in sample_primitive_variables function)\n",
        "vz_interval = (0, .57 * c) # Sampling interval for velocity in z-direction (used in sample_primitive_variables function)\n",
        "epsilon_interval = (0, 2.02) # Sampling interval for specific internal energy (used in sample_primitive_variables function)\n",
        "\n",
        "np.random.seed(43) # Uncomment for pseudorandom data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlaP5UL2UZh1"
      },
      "source": [
        "## Generating the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "s_EvGFZcUZh1"
      },
      "outputs": [],
      "source": [
        "# Defining an analytic equation of state (EOS) for an ideal gas\n",
        "def eos_analytic(rho, epsilon):\n",
        "    \"\"\"Computes the pressure from rest-mass density and specific internal energy using an analytic EOS.\n",
        "\n",
        "    Args:\n",
        "        rho (torch.Tensor): The rest-mass density tensor of shape (n_samples,).\n",
        "        epsilon (torch.Tensor): The specific internal energy tensor of shape (n_samples,).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The pressure tensor of shape (n_samples,).\n",
        "    \"\"\"\n",
        "    # Adding some assertions to check that the input tensors are valid and have \n",
        "    # the expected shape and type \n",
        "    assert isinstance(rho, torch.Tensor), \"rho must be a torch.Tensor\"\n",
        "    assert isinstance(epsilon, torch.Tensor), \"epsilon must be a torch.Tensor\"\n",
        "    assert rho.shape == epsilon.shape, \"rho and epsilon must have the same shape\"\n",
        "    assert rho.ndim == 1, \"rho and epsilon must be one-dimensional tensors\"\n",
        "    assert rho.dtype == torch.float32, \"rho and epsilon must have dtype torch.float32\"\n",
        "\n",
        "    return (gamma - 1) * rho * epsilon\n",
        "\n",
        "\n",
        "\n",
        "# Defining a function that samples primitive variables from uniform distributions\n",
        "def sample_primitive_variables(n_samples):\n",
        "    \"\"\"Samples primitive variables from uniform distributions.\n",
        "\n",
        "    Args:\n",
        "        n_samples (int): The number of samples to generate.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple of (rho, vx, vy, vz, epsilon), where rho is rest-mass density,\n",
        "            vx is velocity in x-direction,\n",
        "            vy is velocity in y-direction,\n",
        "            vz is velocity in z-direction,\n",
        "            epsilon is specific internal energy,\n",
        "            each being a numpy array of shape (n_samples,).\n",
        "    \"\"\"\n",
        "    # Sampling from uniform distributions with intervals matching Dieseldorst \n",
        "    # et al.\n",
        "    rho = np.random.uniform(*rho_interval, size=n_samples)  # Rest-mass density\n",
        "    vx = np.random.uniform(*vx_interval, size=n_samples)  # Velocity in x-direction\n",
        "    vy = np.random.uniform(*vy_interval, size=n_samples)  # Velocity in y-direction\n",
        "    vz = np.random.uniform(*vz_interval, size=n_samples)  # Velocity in z-direction \n",
        "    epsilon = np.random.uniform(*epsilon_interval, size=n_samples)  # Specific internal energy\n",
        "\n",
        "    # Returning the primitive variables\n",
        "    return rho, vx, vy, vz, epsilon\n",
        "\n",
        "\n",
        "\n",
        "# Defining a function that computes conserved variables from primitive variables\n",
        "def compute_conserved_variables(rho, vx, vy, vz, epsilon):\n",
        "    \"\"\"Computes conserved variables from primitive variables.\n",
        "\n",
        "    Args:\n",
        "        rho (torch.Tensor): The rest-mass density tensor of shape (n_samples,).\n",
        "        vx (torch.Tensor): The velocity in x-direction tensor of shape (n_samples,)\n",
        "        vy (torch.Tensor): The velocity in y-direction tensor of shape (n_samples,)\n",
        "        vz (torch.Tensor): The velocity in z-direction tensor of shape (n_samples,)\n",
        "        epsilon (torch.Tensor): The specific internal energy tensor of shape (n_samples,).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple of (D, Sx, Sy, Sz, tau), where D is conserved density,\n",
        "            Sx is conserved momentum in x-direction,\n",
        "            Sy is conserved momentum in y-direction,\n",
        "            Sz is conserved momentum in z-direction,\n",
        "            tau is conserved energy density,\n",
        "            each being a torch tensor of shape (n_samples,).\n",
        "    \"\"\"\n",
        "\n",
        "  # Computing the pressure from the primitive variables using the EOS\n",
        "    p = eos_analytic(rho, epsilon)\n",
        "    # Computing the Lorentz factor from the velocity.\n",
        "    v2 = vx ** 2 + vy ** 2 + vz ** 2\n",
        "    W = 1 / torch.sqrt(1 - v2 / c ** 2)\n",
        "    # Specific enthalpy\n",
        "    h = 1 + epsilon + p / rho  \n",
        "\n",
        "    # Computing the conserved variables from the primitive variables\n",
        "    D = rho * W  # Conserved density\n",
        "    Sx = rho * h * W ** 2 * vx  # Conserved momentum in x-direction\n",
        "    Sy = rho * h * W ** 2 * vy  # Conserved momentum in y-direction\n",
        "    Sz = rho * h * W ** 2 * vz  # Conserved momentum in z-direction\n",
        "    tau = rho * h * W ** 2 - p - D  # Conserved energy density\n",
        "\n",
        "    # Returning the conserved variables\n",
        "    return D, Sx, Sy, Sz, tau\n",
        "\n",
        "# Defining a function that generates input data (conserved variables) from given samples of primitive variables\n",
        "def generate_input_data(rho, vx, vy, vz, epsilon):\n",
        "    # Converting the numpy arrays to torch tensors and moving them to the device\n",
        "    rho = torch.tensor(rho, dtype=torch.float32).to(device)\n",
        "    vx = torch.tensor(vx, dtype=torch.float32).to(device)\n",
        "    vy = torch.tensor(vy, dtype=torch.float32).to(device)\n",
        "    vz = torch.tensor(vz, dtype=torch.float32).to(device)\n",
        "    epsilon = torch.tensor(epsilon, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Computing the conserved variables using the compute_conserved_variables function\n",
        "    D, Sx, Sy, Sz, tau = compute_conserved_variables(rho, vx, vy, vz, epsilon) \n",
        "\n",
        "    # Stacking the conserved variables into a torch tensor\n",
        "    x = torch.stack([D, Sx, Sy, Sz, tau], axis=1)\n",
        "\n",
        "    # Returning the input data tensor\n",
        "    return x\n",
        "\n",
        "# Defining a function that generates output data (labels) from given samples of primitive variables\n",
        "def generate_labels(rho, epsilon):\n",
        "    # Converting the numpy arrays to torch tensors and moving them to the device\n",
        "    rho = torch.tensor(rho, dtype=torch.float32).to(device)\n",
        "    epsilon = torch.tensor(epsilon, dtype=torch.float32).to(device)\n",
        "   \n",
        "    # Computing the pressure from the primitive variables using the EOS\n",
        "    p = eos_analytic(rho, epsilon)\n",
        "\n",
        "    # Returning the output data tensor\n",
        "    return p\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dczop2rUZh3"
      },
      "source": [
        "Sampling the primitive variables using the sample_primitive_variables function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cKubR6C8UZh4"
      },
      "outputs": [],
      "source": [
        "rho_train, vx_train, vy_train, vz_train ,epsilon_train = sample_primitive_variables(n_train_samples)\n",
        "rho_test, vx_test ,vy_test ,vz_test ,epsilon_test = sample_primitive_variables(n_test_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zhbYH5HkvExJ",
        "outputId": "4eb8079e-c5a6-4909-d1e6-0964c0c1f149",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.28762072, 1.9590914 , 0.58522533, ..., 0.06985293, 0.82055397,\n",
              "       1.51444467])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "rho_train\n",
        "vx_train\n",
        "vy_train\n",
        "vz_train \n",
        "epsilon_train\n",
        "rho_test\n",
        "vx_test \n",
        "vy_test \n",
        "vz_test \n",
        "epsilon_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VMp6XJ6RUZh4"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"last_expr_or_assign\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "E5YFdqKjUZh5",
        "outputId": "dd4d6965-71ce-4d19-9c84-ee5875a04df5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x400 with 5 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABjUAAAGMCAYAAACS4SA6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABnyElEQVR4nO3de3iMd/7/8VccEoTEqRIqJaXOp9KWKF3FCrKW0pbWIU61fKN1amm6StFuVItqS3VbFYo67NKDtIhD2BKlKXUqRSktiW6RcUxI7t8f/WXWSMIkJpm5534+rmuudu77M/d87pnk5T33O3PfPoZhGAIAAAAAAAAAAPBwRdw9AQAAAAAAAAAAAGfQ1AAAAAAAAAAAAKZAUwMAAAAAAAAAAJgCTQ0AAAAAAAAAAGAKNDUAAAAAAAAAAIAp0NQAAAAAAAAAAACmQFMDAAAAAAAAAACYAk0NAAAAAAAAAABgCjQ1AAAAAAAAAACAKdDUAAAAAApR//79Vb16dZdus02bNmrTpo3bnt/T5OX1uNHx48fl4+OjN99887ZjX3nlFfn4+ORjdgAAAADuBE0NAAAAwEmxsbHy8fGx30qUKKFatWpp+PDhSklJcff07E6dOqVXXnlFu3fvdvdUAAAAAMClirl7AgAAAIDZTJ48WaGhobp69aq+/vprvffee/ryyy+1b98+lSpV6paP/eCDD5SZmenS+axbt87h/qlTpzRp0iRVr15dTZo0KfDn9zQ3vx4AAAAAvAdNDQAAACCPOnXqpAceeECSNHjwYFWoUEEzZszQZ599pqeeeirHx1y6dEn+/v4qXry4y+fj6+vr9NiCeH5PcfnyZZUqVSpPrwcAAAAAc+H0UwAAAMAdatu2rSTp2LFjkv64bkXp0qV19OhRde7cWWXKlFHv3r3t6268psWN13GYPXu27r33XpUqVUodOnTQyZMnZRiGpkyZoqpVq6pkyZLq2rWrzp496/D8N15DIiEhQQ8++KAkacCAAfZTZcXGxmZ7/mvXrql8+fIaMGBAtn2y2WwqUaKEnn/+efuytLQ0TZw4UTVr1pSfn59CQkI0duxYpaWl3fL1GT58uEqXLq3Lly9nW/fUU08pODhYGRkZkqTPPvtMERERqlKlivz8/FSjRg1NmTLFvv7GfW7QoIGSkpL0yCOPqFSpUnrppZeyvR6SlJ6ergkTJqhZs2YKDAyUv7+/WrdurU2bNuU655kzZ6patWoqWbKk/vSnP2nfvn233McsixYtUrNmzVSyZEmVL19evXr10smTJx3GHD58WD169FBwcLBKlCihqlWrqlevXkpNTXXqOQAAAAAr45saAAAAwB06evSoJKlChQr2ZdevX1d4eLhatWqlN99887anpVq8eLHS09P17LPP6uzZs5o2bZqefPJJtW3bVgkJCRo3bpyOHDmid955R88//7w++uijHLdTt25dTZ48WRMmTNCQIUPUunVrSVLLli2zjS1evLgee+wxrVy5Uu+//77DNxw+/fRTpaWlqVevXpKkzMxM/fWvf9XXX3+tIUOGqG7dutq7d69mzpypH3/8UZ9++mmu+9azZ0/Nnj1bcXFxeuKJJ+zLL1++rC+++EL9+/dX0aJFJf1x3ZLSpUtr9OjRKl26tDZu3KgJEybIZrPpjTfecNju77//rk6dOqlXr17q06ePgoKCcnx+m82mDz/8UE899ZSeeeYZXbhwQfPmzVN4eLh27NiR7RRdCxcu1IULFxQVFaWrV69q1qxZatu2rfbu3Zvrc0jSa6+9ppdffllPPvmkBg8erN9++03vvPOOHnnkEe3atUtly5ZVenq6wsPDlZaWpmeffVbBwcH69ddftXr1ap0/f16BgYG5bh8AAAAATQ0AAAAgz1JTU/Xf//5XV69e1datWzV58mSVLFlSf/nLX+xj0tLS9MQTTygmJsapbf766686fPiw/aB2RkaGYmJidOXKFX377bcqVuyP0v23337T4sWL9d5778nPzy/bdoKCgtSpUydNmDBBYWFh6tOnzy2ft2fPnvroo4+0bt06h/kvW7ZM9957r/00W0uWLNH69eu1efNmtWrVyj6uQYMGGjp0qLZt25Zj40SSWrVqpbvvvlvLli1zaGrExcXp0qVL6tmzp33ZkiVLVLJkSfv9oUOHaujQoZozZ45effVVh31OTk7W3Llz9be//e2W+1iuXDkdP37coWnzzDPPqE6dOnrnnXc0b948h/FHjhzR4cOHdffdd0uSOnbsqObNm+v111/XjBkzcnyOn3/+WRMnTtSrr75q/8aIJHXv3l3333+/5syZo5deekkHDhzQsWPHtGLFCj3++OP2cRMmTLjlPgAAAAD4A6efAgAAAPKoffv2uuuuuxQSEqJevXqpdOnSWrVqlf0geJZhw4Y5vc0nnnjC4a/0mzdvLknq06ePvaGRtTw9PV2//vrrHe7FH9q2bauKFStq2bJl9mXnzp1TfHy8Q7NhxYoVqlu3rurUqaP//ve/9lvWqbdudSonHx8fPfHEE/ryyy918eJF+/Jly5bp7rvvdmiS3NjQuHDhgv773/+qdevWunz5sg4ePOiwXT8/vxxPnXWzokWL2hsamZmZOnv2rK5fv64HHnhA3333Xbbx3bp1c3gvH3roITVv3lxffvllrs+xcuVKZWZm6sknn3R4fYKDg3XffffZX5+s93jt2rU5no4LAAAAwK3xTQ0AAAAgj2bPnq1atWqpWLFiCgoKUu3atVWkiOPfCxUrVkxVq1Z1epv33HOPw/2sg98hISE5Lj937lx+pp5NsWLF1KNHDy1ZskRpaWny8/PTypUrde3aNYemxuHDh/XDDz/orrvuynE7Z86cueXz9OzZU2+99ZY+//xzPf3007p48aK+/PJL/e1vf5OPj4993P79+zV+/Hht3LhRNpvNYRs3X3Pi7rvvdvqi4AsWLND06dN18OBBXbt2zb48NDQ029j77rsv27JatWpp+fLluW7/8OHDMgwjx8dK/7tAe2hoqEaPHq0ZM2Zo8eLFat26tf7617+qT58+nHoKAAAAcAJNDQAAACCPHnroIftpmXLj5+eXrdFxK1nXlHB2uWEYTm/7dnr16qX3339fX331lbp166bly5erTp06aty4sX1MZmamGjZsmOvpl25uvtysRYsWql69upYvX66nn35aX3zxha5cueLQODl//rz+9Kc/KSAgQJMnT1aNGjVUokQJfffddxo3bpwyMzMdtnnjtzpuZdGiRerfv7+6deumF154QZUqVVLRokUVExNjvx7KncrMzJSPj4+++uqrHN+z0qVL2/9/+vTp6t+/vz777DOtW7dOzz33nGJiYrR9+/Y8NcIAAAAAK6KpAQAAAHiZG7/54IxHHnlElStX1rJly9SqVStt3LhRf//73x3G1KhRQ99//73atWuX5+1nefLJJzVr1izZbDYtW7ZM1atXV4sWLezrExIS9Pvvv2vlypV65JFH7MuPHTuWr+fL8q9//Uv33nuvVq5c6TD3iRMn5jj+8OHD2Zb9+OOPql69eq7PUaNGDRmGodDQUNWqVeu2c2rYsKEaNmyo8ePHa9u2bXr44Yc1d+5cvfrqq7ffIQAAAMDCuKYGAAAA4GX8/f0l/fHNB2cUKVJEjz/+uL744gt9/PHHun79usM3KKQ/GhK//vqrPvjgg2yPv3Llii5dunTb5+nZs6fS0tK0YMECrVmzRk8++aTD+qxvONz4LZT09HTNmTPHqf3ITU7b/eabb5SYmJjj+E8//dThmiU7duzQN998o06dOuX6HN27d1fRokU1adKkbN+iMQxDv//+uyTJZrPp+vXrDusbNmyoIkWKKC0tLW87BgAAAFgQ39QAAAAAvEyNGjVUtmxZzZ07V2XKlJG/v7+aN2+e4/UjsvTs2VPvvPOOJk6cqIYNG6pu3boO6/v27avly5dr6NCh2rRpkx5++GFlZGTo4MGDWr58udauXXvbU3I1bdpUNWvW1N///nelpaVla5y0bNlS5cqVU2RkpJ577jn5+Pjo448/vuNTbf3lL3/RypUr9dhjjykiIkLHjh3T3LlzVa9ePYcLl2epWbOmWrVqpWHDhiktLU1vvfWWKlSooLFjx+b6HDVq1NCrr76q6OhoHT9+XN26dVOZMmV07NgxrVq1SkOGDNHzzz+vjRs3avjw4XriiSdUq1YtXb9+XR9//LGKFi2qHj163NF+AgAAAFZAUwMAAADwMsWLF9eCBQsUHR2toUOH6vr165o/f/4tmxotW7ZUSEiITp48ma3ZIP3xbY5PP/1UM2fO1MKFC7Vq1SqVKlVK9957r0aMGOHUKZekP5onr732mmrWrKmmTZs6rKtQoYJWr16tMWPGaPz48SpXrpz69Omjdu3aKTw8PG8vwg369++v5ORkvf/++1q7dq3q1aunRYsWacWKFUpISMg2vl+/fipSpIjeeustnTlzRg899JDeffddVa5c+ZbP8+KLL6pWrVqaOXOmJk2aJOmPa4106NBBf/3rXyVJjRs3Vnh4uL744gv9+uuvKlWqlBo3bqyvvvrK4VRcAAAAAHLmY7jyCoMAAAAAAAAAAAAFhGtqAAAAAAAAAAAAU6CpAQAAAAAAAAAATIGmBgAAAAAAAAAAMAWaGgAAAAAAAAAAwBRoagAAAAAAAAAAAFOgqQEAAAAAAAAAAEyBpgYAAAAAAAAAADAFmhoAAAAAAAAAAMAUaGoAAAAAAAAAAABToKkBAAAAAAAAAABMgaYGAAAAAAAAAAAwBZoaAAAAAAAAAADAFGhqAAAAAAAAAAAAU6CpAQAAAAAAAAAATIGmBgAAAAAAAAAAMAWaGgAAAAAAAAAAwBRoagAAAAAAAAAAAFOgqQEAAAAAAAAAAEyBpgYAAAAAAAAAADAFmhoAAAAAAAAAAMAUaGoAAAAAAAAAAABToKkBAAAAAAAAAABMgaYGAAAAAAAAAAAwhWLunoAZZGZm6tSpUypTpox8fHzcPR0AuTAMQxcuXFCVKlVUpAg92ztB7gHmQO65DrkHmAO55zrkHuD5yDzXIfMAc3A292hqOOHUqVMKCQlx9zQAOOnkyZOqWrWqu6dhauQeYC7k3p0j9wBzIffuHLkHmAeZd+fIPMBcbpd7NDWcUKZMGUl/vJgBAQFung2A3NhsNoWEhNh/Z5F/5B5gDuSe65B7gDmQe65D7gGej8xzHTIPMAdnc4+mhhOyvpYWEBBA8AEmwFdJ7xy5B5gLuXfnyD3AXMi9O0fuAeZB5t05Mg8wl9vlHifkAwAAAAAAAAAApkBTAwAAAAAAAAAAmAJNDQAAAAAAAAAAYAo0NQAAAAAAAAAAgCnQ1AAAAAAAAAAAAKZAUwMAAAAAAAAAAJgCTQ0AAAAAAAAAAGAKNDUAAAAAAAAAAIAp0NQAAAAAAAAAAACmQFMDAAAAAAAAAACYAk0NAAAAAAAAAABgCsXcPQHAGdVfjHN67PGpEQU4EwAwL7IUALyDs3lOlgNA7shSFDY+j8EbuStLaWrArfIS6DAH/pEGAKDgFcSHB6v/G271/QcAAADMgqYGAADATTi4CQAAChrfFACQGz6PwFlW/beEpgbghIL6RklBBAr/8AEAADOihnEvXn8AwJ2aOnWqoqOjNWLECL311luSpKtXr2rMmDFaunSp0tLSFB4erjlz5igoKMj+uBMnTmjYsGHatGmTSpcurcjISMXExKhYsf8dtkxISNDo0aO1f/9+hYSEaPz48erfv38h72HOOAuJe1n1oL7VeUxTw6rBB9fjAxkAb0CWwRtZsd7jdxkAAFjBzp079f7776tRo0YOy0eNGqW4uDitWLFCgYGBGj58uLp3766tW7dKkjIyMhQREaHg4GBt27ZNp0+fVr9+/VS8eHH94x//kCQdO3ZMERERGjp0qBYvXqwNGzZo8ODBqly5ssLDwwt9XwG4n0c0NQg+wD3oZruXFQ/umUlBHIg001/wcCAWrka9d3tmyggAzqHegysUVF3G9ZngKhcvXlTv3r31wQcf6NVXX7UvT01N1bx587RkyRK1bdtWkjR//nzVrVtX27dvV4sWLbRu3TodOHBA69evV1BQkJo0aaIpU6Zo3LhxeuWVV+Tr66u5c+cqNDRU06dPlyTVrVtXX3/9tWbOnGmqWs/d+P20Nm97/93e1CD44E4cPIC7cHAPgJV4Yr2XlpamtLQ0+32bzVaAr4D7eGutwx9mwAyo94DC563/7nm6qKgoRUREqH379g61XlJSkq5du6b27dvbl9WpU0f33HOPEhMT1aJFCyUmJqphw4YOjd3w8HANGzZM+/fv1/3336/ExESHbWSNGTlyZK5zskqtBxQEM2Sp25saBB9we2YIk/yw6gEJTzy4d6fM8l566+8S4Ok8sd6LiYnRpEmTXLeTAHADT6z37uRzrrf9dScA11m6dKm+++477dy5M9u65ORk+fr6qmzZsg7Lg4KClJycbB9zY52XtT5r3a3G2Gw2XblyRSVLlsz23Faq9Qricy6573q8pq7l1qYGweedOGgI3JonHtyjmYvCQBFnTZ5a70VHR2v06NH2+zabTSEhIXnfQQDIgSfWe3zOzT8z1TB8HncvM/2suMLJkyc1YsQIxcfHq0SJEu6ejgNqvcJjlj9yhHdxW1OD4ANgRZ56cI8PuebmjR9erfaB0Ft5cr3n5+cnPz8/d0/D63ljPoGDF7fjqfWeJ37OdffPEhkF3JmkpCSdOXNGTZs2tS/LyMjQli1b9O6772rt2rVKT0/X+fPnHXIvJSVFwcHBkqTg4GDt2LHDYbspKSn2dVn/zVp245iAgIAc806i1jM7vn2C23FbU4PgA2A1nnxwzxM/5AIwP0+u9wBP440HV6148MCT673C+pzr7p9ldz8/XI/31HO1a9dOe/fudVg2YMAA1alTR+PGjVNISIiKFy+uDRs2qEePHpKkQ4cO6cSJEwoLC5MkhYWF6bXXXtOZM2dUqVIlSVJ8fLwCAgJUr149+5gvv/zS4Xni4+Pt2wC8Dbl3e25rahB8AKzGkw/umb2Z6+5/8N39/O5mlv03yzy9CfUe3I3fexQ2T673zIzfZe/E+2p+ZcqUUYMGDRyW+fv7q0KFCvblgwYN0ujRo1W+fHkFBATo2WefVVhYmFq0aCFJ6tChg+rVq6e+fftq2rRpSk5O1vjx4xUVFWX/jDp06FC9++67Gjt2rAYOHKiNGzdq+fLliosruJ8hfj4Bz+a2poY3B19emOXrtt7yl1Oehn8krYWDe9b8i00UPLLUc3lzvcfPnbVRQyM31Hvg3wfvw3t6Z2bOnKkiRYqoR48eSktLU3h4uObMmWNfX7RoUa1evVrDhg1TWFiY/P39FRkZqcmTJ9vHhIaGKi4uTqNGjdKsWbNUtWpVffjhhwoPD3fHLgEOyAj3cOuFwm/HrMHHed8A5MSbD+4BQH6Ztd6zOj68ATmj3oM3IvORFwkJCQ73S5QoodmzZ2v27Nm5PqZatWrZGrU3a9OmjXbt2uWKKcJNyBK4kkc1NQg+z0XwWBvvf+Hh4B4Ab0e9B8DqqPcAAADujEc1NQDAaji4lzuaaQAA3Bn+LfUM1HsAAJgLNZTno6nhhfjFAwDA83AqScB6qMsBAAAA1yvi7gkAAAAAAAAAAAA4g29qAAAA3AH+EhsAAAAAgMLDNzUAAAAAAAAAAIAp0NQAAAAAAAAAAACmQFMDAAAAAAAAAACYAk0NAAAAAAAAAABgCjQ1AAAAAAAAAACAKdDUAAAAAAAAAAAApkBTAwAAAAAAAAAAmAJNDQAAAAAAAAAAYAo0NQAAAAAAAAAAgCnQ1AAAAAAAAAAAAKZAUwMAAAAAAAAAAJgCTQ0AAAAAAAAAAGAKNDUAAAAAAAAAAIAp0NQAAAAAAAAAAACmQFMDAAAAAAAAAACYAk0NAAAAAAAAAABgCjQ1AAAAAAAAAACAKdDUAAAAAAAAAAAApkBTAwAAAAAAAECevffee2rUqJECAgIUEBCgsLAwffXVV/b1bdq0kY+Pj8Nt6NChDts4ceKEIiIiVKpUKVWqVEkvvPCCrl+/7jAmISFBTZs2lZ+fn2rWrKnY2NjC2D0AHsqtTQ2CDwAAwHtR6wEAAHi3qlWraurUqUpKStK3336rtm3bqmvXrtq/f799zDPPPKPTp0/bb9OmTbOvy8jIUEREhNLT07Vt2zYtWLBAsbGxmjBhgn3MsWPHFBERoUcffVS7d+/WyJEjNXjwYK1du7ZQ9xWA53BrU4PgA2AlHNwDYDXUegCshnoPgNV06dJFnTt31n333adatWrptddeU+nSpbV9+3b7mFKlSik4ONh+CwgIsK9bt26dDhw4oEWLFqlJkybq1KmTpkyZotmzZys9PV2SNHfuXIWGhmr69OmqW7euhg8frscff1wzZ84s9P0F4Bnc2tQg+ABYCQf3AFiNJ9d6aWlpstlsDjcAuFPUewCsLCMjQ0uXLtWlS5cUFhZmX7548WJVrFhRDRo0UHR0tC5fvmxfl5iYqIYNGyooKMi+LDw8XDabzZ6diYmJat++vcNzhYeHKzExMde5UOsB3s1jrqlB8AHwdp58cA8ACpon1XqSFBMTo8DAQPstJCTEFbsJwOI8ud7jcy6AgrJ3716VLl1afn5+Gjp0qFatWqV69epJkp5++mktWrRImzZtUnR0tD7++GP16dPH/tjk5GSHWk+S/X5ycvItx9hsNl25ciXHOVHrAd7N7U0Ngg+AFXnawT0+5AIoKJ5Y60lSdHS0UlNT7beTJ0+6ZH8BIIun1Xt8zgVQUGrXrq3du3frm2++0bBhwxQZGakDBw5IkoYMGaLw8HA1bNhQvXv31sKFC7Vq1SodPXq0QOdErQd4t2LunkBW8KWmpupf//qXIiMjtXnzZtWrV09Dhgyxj2vYsKEqV66sdu3a6ejRo6pRo0aBzSk6OlqjR4+237fZbBR8AFxi7969CgsL09WrV1W6dOlsB/eqVaumKlWqaM+ePRo3bpwOHTqklStXSnLNwb2SJUvmOK+YmBhNmjTJpfsKAJJn1nqS5OfnJz8/vwJ9DgDW5Kn1Hp9zARQUX19f1axZU5LUrFkz7dy5U7NmzdL777+fbWzz5s0lSUeOHFGNGjUUHBysHTt2OIxJSUmRJAUHB9v/m7XsxjEBAQG5Zh61HuDd3N7UIPgAWImnHtzjQy6AguKJtR4AFCRPrff4nAugsGRmZiotLS3Hdbt375YkVa5cWZIUFham1157TWfOnFGlSpUkSfHx8QoICLA3hMPCwvTll186bCc+Pt7hW3AArMXtp5+6WV6Db+/evTpz5ox9TE7Bt2HDBoftEHwA3CXr4F6zZs0UExOjxo0ba9asWTmOvfHgnpT7gbusdbcac7uDe35+fgoICHC4AUBBoNYD4O08td4DgIIQHR2tLVu26Pjx49q7d6+io6OVkJCg3r176+jRo5oyZYqSkpJ0/Phxff755+rXr58eeeQRNWrUSJLUoUMH1atXT3379tX333+vtWvXavz48YqKirI3YocOHaqffvpJY8eO1cGDBzVnzhwtX75co0aNcueuA3AjtzY1CD4AVsfBPQDejFoPAKj3AHi3M2fOqF+/fqpdu7batWunnTt3au3atfrzn/8sX19frV+/Xh06dFCdOnU0ZswY9ejRQ1988YX98UWLFtXq1atVtGhRhYWFqU+fPurXr58mT55sHxMaGqq4uDjFx8ercePGmj59uj788EOFh4e7Y5cBeAC3nn4qK/hOnz6twMBANWrUyB58J0+e1Pr16/XWW2/p0qVLCgkJUY8ePTR+/Hj747OCb9iwYQoLC5O/v78iIyNzDL5Ro0Zp1qxZqlq1KsEHwC2io6PVqVMn3XPPPbpw4YKWLFmihIQErV27VkePHtWSJUvUuXNnVahQQXv27NGoUaNyPbg3bdo0JScn53hw791339XYsWM1cOBAbdy4UcuXL1dcXJw7dx2ARVHrAbAa6j0AVjNv3rxc14WEhGjz5s233Ua1atWynV7qZm3atNGuXbvyPD8A3smtTQ2CD4CVcHAPgNVQ6wGwGuo9AACAguf2C4UDgFVwcA8AAMC7Ue8BAAAUPI+7UDgAAAAAAAAAAEBOaGoAAAAAAAAAAABToKkBAAAAAAAAAABMgaYGAAAAAAAAAAAwBZoaAAAAAAAAAADAFGhqAAAAAAAAAAAAU6CpAQAAAAAAAAAATIGmBgAAAAAAAAAAMAWaGgAAAAAAAAAAwBRoagAAAAAAAAAAAFOgqQEAAAAAAAAAAEyBpgYAAAAAAAAAADAFmhoAAAAAAAAAAMAUaGoAAAAAAAAAAABToKkBAAAAAAAAAABMgaYGAAAAAAAAAAAwBZoaAAAAAAAAAADAFGhqAAAAAAAAAAAAU6CpAQAAAAAAAAAATIGmBgAAAAAAAAAAMAWaGgAAAAAAAAAAwBRoagAAAAAAAADIs/fee0+NGjVSQECAAgICFBYWpq+++sq+/urVq4qKilKFChVUunRp9ejRQykpKQ7bOHHihCIiIlSqVClVqlRJL7zwgq5fv+4wJiEhQU2bNpWfn59q1qyp2NjYwtg9AB7KrU0Ngg8AAMB7UesBsBpyD4DVVK1aVVOnTlVSUpK+/fZbtW3bVl27dtX+/fslSaNGjdIXX3yhFStWaPPmzTp16pS6d+9uf3xGRoYiIiKUnp6ubdu2acGCBYqNjdWECRPsY44dO6aIiAg9+uij2r17t0aOHKnBgwdr7dq1hb6/ADyDW5saBB8AK+FDLgCrodYDYDXkHgCr6dKlizp37qz77rtPtWrV0muvvabSpUtr+/btSk1N1bx58zRjxgy1bdtWzZo10/z587Vt2zZt375dkrRu3TodOHBAixYtUpMmTdSpUydNmTJFs2fPVnp6uiRp7ty5Cg0N1fTp01W3bl0NHz5cjz/+uGbOnOnOXQfgRm5tahB8AKyED7kArMaTa720tDTZbDaHGwDcKU/OPQAoaBkZGVq6dKkuXbqksLAwJSUl6dq1a2rfvr19TJ06dXTPPfcoMTFRkpSYmKiGDRsqKCjIPiY8PFw2m83+WTkxMdFhG1ljsraRE2o9wLt5zDU1CD4A3o4PuQCszJNqPUmKiYlRYGCg/RYSEuKqXQUASZ6Xe3zOBVBQ9u7dq9KlS8vPz09Dhw7VqlWrVK9ePSUnJ8vX11dly5Z1GB8UFKTk5GRJUnJyskPmZa3PWnerMTabTVeuXMlxTtR6gHdze1OD4ANgRXzIBWAVnljrSVJ0dLRSU1Ptt5MnT97prgKAJM/NPT7nAigotWvX1u7du/XNN99o2LBhioyM1IEDB9w6J2o9wLsVc/cEsoIvNTVV//rXvxQZGanNmze7dU7R0dEaPXq0/b7NZqPgA+ASe/fuVVhYmK5evarSpUvbP+Tu3r27UD7klixZMsd5xcTEaNKkSa7YRQBw4Im1niT5+fnJz8/P3dMA4IU8Nff4nAugoPj6+qpmzZqSpGbNmmnnzp2aNWuWevbsqfT0dJ0/f97hs25KSoqCg4MlScHBwdqxY4fD9rKuLXnjmJuvN5mSkqKAgIBcP+NS6wHeze3f1MgKvmbNmikmJkaNGzfWrFmzFBwcbA++G90cfDmFWta6W425XfBlXcg36wYAruCJf8Ei8VcsAAqOJ9Z6AFCQPDX3+JwLoLBkZmYqLS1NzZo1U/HixbVhwwb7ukOHDunEiRMKCwuTJIWFhWnv3r06c+aMfUx8fLwCAgJUr149+5gbt5E1JmsbAKzH7U2NmxF8ALwZH3IBWB21HgCrIfcAeLPo6Ght2bJFx48f1969exUdHa2EhAT17t1bgYGBGjRokEaPHq1NmzYpKSlJAwYMUFhYmFq0aCFJ6tChg+rVq6e+ffvq+++/19q1azV+/HhFRUXZv2kxdOhQ/fTTTxo7dqwOHjyoOXPmaPny5Ro1apQ7dx2AG7n19FPR0dHq1KmT7rnnHl24cEFLlixRQkKC1q5d6xB85cuXV0BAgJ599tlcg2/atGlKTk7OMfjeffddjR07VgMHDtTGjRu1fPlyxcXFuXPXAUBSzh9ye/ToISnnD7mvvfaazpw5o0qVKknK+UPul19+6fAcfMgF4C7UegCshtwDYDVnzpxRv379dPr0aQUGBqpRo0Zau3at/vznP0uSZs6cqSJFiqhHjx5KS0tTeHi45syZY3980aJFtXr1ag0bNkxhYWHy9/dXZGSkJk+ebB8TGhqquLg4jRo1SrNmzVLVqlX14YcfKjw8vND3F4BncGtTg+ADYCV8yAVgNdR6AKyG3ANgNfPmzbvl+hIlSmj27NmaPXt2rmOqVauW7Y/zbtamTRvt2rUrX3ME4H18DMMw3D0JT2ez2RQYGKjU1FSnTslS/UUOHgKudHxqhFPj8vq7WtgGDRqkDRs2OHzIHTdunP1D7tWrVzVmzBh98sknDh9ys04tJUk///yzhg0bpoSEBPuH3KlTp6pYsf/1qBMSEjRq1CgdOHBAVatW1csvv6z+/fvnaa7kHuBe3pJ7ZkLuAe5F7hW+vLyWZB7ges7kHpnnOtR6gHu5utZz6zc1AMBK+AsWAAAAAAAA4M543IXCAQAAAAAAAAAAckJTAwAAAAAAAAAAmAJNDQAAAAAAAAAAYAo0NQAAAAAAAAAAgCnQ1AAAAAAAAAAAAKZAUwMAAAAAAAAAAJgCTQ0AAAAAAAAAAGAKNDUAAAAAAAAAAIAp0NQAAAAAAAAAAACmQFMDAAAAAAAAAACYAk0NAAAAAAAAAABgCjQ1AAAAAAAAAACAKdDUAAAAAAAAAAAApkBTAwAAAAAAAAAAmAJNDQAAAAAAAAAAYAo0NQAAAAAAAAAAgCnQ1AAAAAAAAAAAAKaQr6bGTz/95Op5AIBHI/cAWA25B8BqyD0AVkLmATCzfDU1atasqUcffVSLFi3S1atXXT0nAPA45B4AqyH3AFgNuQfASsg8AGaWr6bGd999p0aNGmn06NEKDg7W3/72N+3YscPVcwMAj0HuAbAacg+A1ZB7AKyEzANgZvlqajRp0kSzZs3SqVOn9NFHH+n06dNq1aqVGjRooBkzZui3335z9TwBwK3IPQBWQ+4BsBpyD4CVkHkAzOyOLhRerFgxde/eXStWrNDrr7+uI0eO6Pnnn1dISIj69eun06dPu2qeAOARyD0AVkPuAbAacg+AlZB5AMzojpoa3377rf7v//5PlStX1owZM/T888/r6NGjio+P16lTp9S1a9dbPj4mJkYPPvigypQpo0qVKqlbt246dOiQw5g2bdrIx8fH4TZ06FCHMSdOnFBERIRKlSqlSpUq6YUXXtD169cdxiQkJKhp06by8/NTzZo1FRsbeye7DsCi7jT3AMBs7iT3qPUAmBG5B8BKOLYHwIzy1dSYMWOGGjZsqJYtW+rUqVNauHChfv75Z7366qsKDQ1V69atFRsbq+++++6W29m8ebOioqK0fft2xcfH69q1a+rQoYMuXbrkMO6ZZ57R6dOn7bdp06bZ12VkZCgiIkLp6enatm2bFixYoNjYWE2YMME+5tixY4qIiNCjjz6q3bt3a+TIkRo8eLDWrl2bn90HYEGuyD2KPQBm4orco9YDYCbkHgAr4dgeADMrlp8Hvffeexo4cKD69++vypUr5zimUqVKmjdv3i23s2bNGof7sbGxqlSpkpKSkvTII4/Yl5cqVUrBwcE5bmPdunU6cOCA1q9fr6CgIDVp0kRTpkzRuHHj9Morr8jX11dz585VaGiopk+fLkmqW7euvv76a82cOVPh4eF52XUAFuWK3Msq9h588EFdv35dL730kjp06KADBw7I39/fPu6ZZ57R5MmT7fdLlSpl//+sYi84OFjbtm3T6dOn1a9fPxUvXlz/+Mc/JP2v2Bs6dKgWL16sDRs2aPDgwapcuTKZB8Bprsg9T6710tLSlJaWZr9vs9ly3Q8A1uDtuQcAN/L2Y3vUeoB3y9c3NQ4fPqzo6OhcQ0+SfH19FRkZmaftpqamSpLKly/vsHzx4sWqWLGiGjRooOjoaF2+fNm+LjExUQ0bNlRQUJB9WXh4uGw2m/bv328f0759e4dthoeHKzExMcd5pKWlyWazOdwAWJsrcm/NmjXq37+/6tevr8aNGys2NlYnTpxQUlKSw7isYi/rFhAQYF+XVewtWrRITZo0UadOnTRlyhTNnj1b6enpkuRQ7NWtW1fDhw/X448/rpkzZ+Y6N3IPwM0Kot7zlFpP+uPbc4GBgfZbSEiI0/sBwDt5e+5R7wG4kbcf26PWA7xbvpoa8+fP14oVK7ItX7FihRYsWJCviWRmZmrkyJF6+OGH1aBBA/vyp59+WosWLdKmTZsUHR2tjz/+WH369LGvT05Odgg9Sfb7ycnJtxxjs9l05cqVbHMh+ADcrCByz1OKPYncA5Cdq3PPk2o9SYqOjlZqaqr9dvLkyTzvEwDv4u25R70H4EbefmyPWg/wbvk6/VRMTIzef//9bMsrVaqkIUOG5LmLK0lRUVHat2+fvv76a4flQ4YMsf9/w4YNVblyZbVr105Hjx5VjRo18j55J0RHR2v06NH2+zabjYIPsDhX596tir1q1aqpSpUq2rNnj8aNG6dDhw5p5cqVklxT7JUsWTLbfMg9ADdzde55Uq0nSX5+fvLz8yuw7QMwH2/PPeo9ADfy9mN71HqAd8tXU+PEiRMKDQ3NtrxatWo6ceJEnrc3fPhwrV69Wlu2bFHVqlVvObZ58+aSpCNHjqhGjRoKDg7Wjh07HMakpKRIkv1cfcHBwfZlN44JCAjI8eAewQfgZq7OPU8q9iRyD0B2rsw9T6v1ACAn3p571HsAbuTtx/YAeLd8nX6qUqVK2rNnT7bl33//vSpUqOD0dgzD0PDhw7Vq1Spt3LgxxzC92e7duyXJfs6/sLAw7d27V2fOnLGPiY+PV0BAgOrVq2cfs2HDBoftxMfHKywszOm5ArA2V+We9L9ib9OmTXkq9qTcC7msdbcaQ7EHIC9ckXvUegDMhNwDYCUc2wNgZvlqajz11FN67rnntGnTJmVkZCgjI0MbN27UiBEj1KtXL6e3ExUVpUWLFmnJkiUqU6aMkpOTlZycbD8X3tGjRzVlyhQlJSXp+PHj+vzzz9WvXz898sgjatSokSSpQ4cOqlevnvr27avvv/9ea9eu1fjx4xUVFWX/K5ShQ4fqp59+0tixY3Xw4EHNmTNHy5cv16hRo/Kz+wAsyBW5R7EHwExckXvUegDMhNwDYCUc2wNgZj6GYRh5fVB6err69u2rFStWqFixP85glZmZqX79+mnu3Lny9fV17sl9fHJcPn/+fPXv318nT55Unz59tG/fPl26dEkhISF67LHHNH78eAUEBNjH//zzzxo2bJgSEhLk7++vyMhITZ061T43SUpISNCoUaN04MABVa1aVS+//LL69+/v1DxtNpsCAwOVmprq8Ly5qf5inFPbBeCc41MjnBqX19/VvHBF7v3f//2flixZos8++0y1a9e2Lw8MDFTJkiV19OhRLVmyRJ07d1aFChW0Z88ejRo1SlWrVtXmzZslSRkZGWrSpImqVKmiadOmKTk5WX379tXgwYP1j3/8Q5J07NgxNWjQQFFRURo4cKA2btyo5557TnFxcQoPD3dqf8k9wL28JffMUutJ5B7gbuSeZ+cemQe4njO55+mZJ5kn96j1APdyda2Xr6ZGlh9//FHff/+9SpYsqYYNG6patWr53ZRHI/gA9/KED7lZ7iT3zFLsSeQe4G7ekntmQu4B7kXuFT6aGoB7ubupkYXMyxm5B7iWq2u9fF0oPEutWrVUq1atO9kEAJjKneTe7XrIISEh9m9k3Eq1atX05Zdf3nJMmzZttGvXrjzNDwByQr0HwGrIPQBWQuYBMKN8NTUyMjIUGxurDRs26MyZM8rMzHRYv3HjRpdMDgA8BbkHwGrIPQBWQ+4BsBIyD4CZ5aupMWLECMXGxioiIkINGjTI9ZQqAOAtyD0AVkPuAbAacg+AlZB5AMwsX02NpUuXavny5ercubOr5wMAHoncA2A15B4AqyH3AFgJmQfAzIrk50G+vr6qWbOmq+cCAB6L3ANgNeQeAKsh9wBYCZkHwMzy1dQYM2aMZs2adduL3gKAtyD3AFgNuQfAasg9AFZC5gEws3ydfurrr7/Wpk2b9NVXX6l+/foqXry4w/qVK1e6ZHIA4CnIPQBWQ+4BsBpyD4CVkHkAzCxfTY2yZcvqsccec/VcAMBjkXsArIbcA2A15B4AKyHzAJhZvpoa8+fPd/U8AMCjkXsArIbcA2A15B4AKyHzAJhZvq6pIUnXr1/X+vXr9f777+vChQuSpFOnTunixYsumxwAeBJyD4DVkHsArIbcA2AlZB4As8rXNzV+/vlndezYUSdOnFBaWpr+/Oc/q0yZMnr99deVlpamuXPnunqeAOBW5B4AqyH3AFgNuQfASsg8AGaWr29qjBgxQg888IDOnTunkiVL2pc/9thj2rBhg8smBwCegtwDYDXkHgCrIfcAWAmZB8DM8vVNjf/85z/atm2bfH19HZZXr15dv/76q0smBgCehNwDYDXkHgCrIfcAWAmZB8DM8vVNjczMTGVkZGRb/ssvv6hMmTJ3PCkA8DTkHgCrIfcAWA25B8BKyDwAZpavpkaHDh301ltv2e/7+Pjo4sWLmjhxojp37uyquQGAxyD3AFgNuQfAasg9AFZC5gEws3ydfmr69OkKDw9XvXr1dPXqVT399NM6fPiwKlasqE8++cTVcwQAtyP3AFgNuQfAasg9AFZC5gEws3w1NapWrarvv/9eS5cu1Z49e3Tx4kUNGjRIvXv3dri4EAB4C3IPgNWQewCshtwDYCVkHgAzy1dTQ5KKFSumPn36uHIuAODRyD0AVkPuAbAacg+AlZB5AMwqX02NhQsX3nJ9v3798jUZAPBU5B4AqyH3AFgNuQfASsg8AGaWr6bGiBEjHO5fu3ZNly9flq+vr0qVKkXwAfA65B4AqyH3AFgNuQfASsg8AGZWJD8POnfunMPt4sWLOnTokFq1asXFhAB4JXIPgNWQewCshtwDYCVkHgAzy1dTIyf33Xefpk6dmq3TCwDeitwDYDXkHgCrIfcAWAmZB8AsXNbUkP64wNCpU6dcuUkA8GjkHgCrIfcAWA25B8BKyDwAZpCvpsbnn3/ucPvss880d+5c9enTRw8//LDT24mJidGDDz6oMmXKqFKlSurWrZsOHTrkMObq1auKiopShQoVVLp0afXo0UMpKSkOY06cOKGIiAiVKlVKlSpV0gsvvKDr1687jElISFDTpk3l5+enmjVrKjY2Nj+7DsCiXJV7AGAWrsg9aj0AZkLuAbASju0BMLN8XSi8W7duDvd9fHx01113qW3btpo+fbrT29m8ebOioqL04IMP6vr163rppZfUoUMHHThwQP7+/pKkUaNGKS4uTitWrFBgYKCGDx+u7t27a+vWrZKkjIwMRUREKDg4WNu2bdPp06fVr18/FS9eXP/4xz8kSceOHVNERISGDh2qxYsXa8OGDRo8eLAqV66s8PDw/LwEACzGFbkXExOjlStX6uDBgypZsqRatmyp119/XbVr17aPuXr1qsaMGaOlS5cqLS1N4eHhmjNnjoKCguxjTpw4oWHDhmnTpk0qXbq0IiMjFRMTo2LF/hfpCQkJGj16tPbv36+QkBCNHz9e/fv3v6PXAIC1uCL3qPUAmAm5B8BKOLYHwMx8DMMw3D2JLL/99psqVaqkzZs365FHHlFqaqruuusuLVmyRI8//rgk6eDBg6pbt64SExPVokULffXVV/rLX/6iU6dO2Q/6zZ07V+PGjdNvv/0mX19fjRs3TnFxcdq3b5/9uXr16qXz589rzZo1t52XzWZTYGCgUlNTFRAQcNvx1V+My+crACAnx6dGODUur7+rha1jx47q1auXQ7G3b98+h2Jv2LBhiouLU2xsrL3YK1KkiEOx16RJEwUHB+uNN96wF3vPPPOMQ7HXoEEDDR06VIMHD9aGDRs0cuRIxcXFOV3skXuAe3lL7t3MU2s9idwD3I3c8+zcI/MA13Mm98yWeZLn5F5aWprS0tLs9202m0JCQqj1ADdxda3n0mtq3KnU1FRJUvny5SVJSUlJunbtmtq3b28fU6dOHd1zzz1KTEyUJCUmJqphw4YOf8UcHh4um82m/fv328fcuI2sMVnbuFlaWppsNpvDDQDu1Jo1a9S/f3/Vr19fjRs3VmxsrE6cOKGkpCRJf2TgvHnzNGPGDLVt21bNmjXT/PnztW3bNm3fvl2StG7dOh04cECLFi1SkyZN1KlTJ02ZMkWzZ89Wenq6pD+Kv9DQUE2fPl1169bV8OHD9fjjj2vmzJm5zo3cA1AYPKXWk8g9AIWD3ANgNZ6SezExMQoMDLTfQkJCXLeTANwuX6efGj16tNNjZ8yY4dS4zMxMjRw5Ug8//LAaNGggSUpOTpavr6/Kli3rMDYoKEjJycn2MTeGXtb6rHW3GmOz2XTlyhWVLFnSYV1MTIwmTZrk3A4CsISCyL28FnstWrTItdgbNmyY9u/fr/vvvz/XYm/kyJG5zoXcA3AzV+eeJ9V6ErkHIDtyD4CVePuxvejoaId9zPqmBgDvkK+mxq5du7Rr1y5du3bNfi74H3/8UUWLFlXTpk3t43x8fJzeZlRUlPbt26evv/46P1NyKYIPwM1cnXueVOxJ5B6A7Fyde55U60nkHoDsyD0AVuLtx/b8/Pzk5+fn7mkAKCD5amp06dJFZcqU0YIFC1SuXDlJ0rlz5zRgwAC1bt1aY8aMydP2hg8frtWrV2vLli2qWrWqfXlwcLDS09N1/vx5h4N8KSkpCg4Oto/ZsWOHw/ZSUlLs67L+m7XsxjEBAQE5Htwj+ADczNW550nFnkTuAcjOlbnnabWeRO4ByI7cA2Al3n5sD4B3y9c1NaZPn66YmBh76ElSuXLl9Oqrr2r69OlOb8cwDA0fPlyrVq3Sxo0bFRoa6rC+WbNmKl68uDZs2GBfdujQIZ04cUJhYWGSpLCwMO3du1dnzpyxj4mPj1dAQIDq1atnH3PjNrLGZG0DAG7HVbkn/a/Y27RpU67F3o1uLvZyKuSy1t1qDMUegLxwRe5R6wEwE3IPgJVwbA+AmeWrqWGz2fTbb79lW/7bb7/pwoULTm8nKipKixYt0pIlS1SmTBklJycrOTlZV65ckSQFBgZq0KBBGj16tDZt2qSkpCQNGDBAYWFhatGihSSpQ4cOqlevnvr27avvv/9ea9eu1fjx4xUVFWX/K5ShQ4fqp59+0tixY3Xw4EHNmTNHy5cv16hRo/Kz+wAsyBW5R7EHwExckXvUegDMhNwDYCUc2wNgZvlqajz22GMaMGCAVq5cqV9++UW//PKL/v3vf2vQoEHq3r2709t57733lJqaqjZt2qhy5cr227Jly+xjZs6cqb/85S/q0aOHHnnkEQUHB2vlypX29UWLFtXq1atVtGhRhYWFqU+fPurXr58mT55sHxMaGqq4uDjFx8ercePGmj59uj788EOFh4fnZ/cBWJArco9iD4CZuCL3qPUAmAm5B8BKOLYHwMx8DMMw8vqgy5cv6/nnn9dHH32ka9euSZKKFSumQYMG6Y033pC/v7/LJ+pONptNgYGBSk1NVUBAwG3HV38xrhBmBVjH8akRTo3L6+9qXrgi93K7wNr8+fPVv39/SdLVq1c1ZswYffLJJ0pLS1N4eLjmzJljP7WUJP38888aNmyYEhIS5O/vr8jISE2dOlXFiv3vMkkJCQkaNWqUDhw4oKpVq+rll1+2P4czyD3Avbwl98yE3APci9wrfHl5Lck8wPWcyT0yz3Wo9QD3cnWtl6+mRpZLly7p6NGjkqQaNWp4XeBlIfgA9/KED7lZyL2ckXuAa5F7hY/cA9yL3Ct8NDUA93J3UyMLmZczcg9wLVfXevk6/VSW06dP6/Tp07rvvvvk7++vO+iPAIApkHsArIbcA2A15B4AKyHzAJhRvpoav//+u9q1a6datWqpc+fOOn36tCRp0KBBGjNmjEsnCACegNwDYDXkHgCrIfcAWAmZB8DM8tXUGDVqlIoXL64TJ06oVKlS9uU9e/bUmjVrXDY5APAU5B4AqyH3AFgNuQfASsg8AGZW7PZDslu3bp3Wrl2rqlWrOiy/77779PPPP7tkYgDgScg9AFZD7gGwGnIPgJWQeQDMLF/f1Lh06ZJDFzfL2bNn5efnd8eTAgBPQ+4BsBpyD4DVkHsArITMA2Bm+WpqtG7dWgsXLrTf9/HxUWZmpqZNm6ZHH33UZZMDAE9B7gGwGnIPgNWQewCshMwDYGb5Ov3UtGnT1K5dO3377bdKT0/X2LFjtX//fp09e1Zbt2519RwBwO3IPQBWQ+4BsBpyD4CVkHkAzCxf39Ro0KCBfvzxR7Vq1Updu3bVpUuX1L17d+3atUs1atRw9RwBwO3IPQBWQ+4BsBpyD4CVkHkAzCzP39S4du2aOnbsqLlz5+rvf/97QcwJADwKuQfAasg9AFZD7gGwEjIPgNnl+ZsaxYsX1549ewpiLgDgkcg9AFZD7gGwGnIPgJWQeQDMLl+nn+rTp4/mzZvn6rkAgMci9wBYDbkHwGrIPQBWQuYBMLN8XSj8+vXr+uijj7R+/Xo1a9ZM/v7+DutnzJjhkskBgKcg9wBYDbkHwGrIPQBWQuYBMLM8NTV++uknVa9eXfv27VPTpk0lST/++KPDGB8fH9fNDgDcjNwDYDXkHgCrIfcAWAmZB8Ab5Kmpcd999+n06dPatGmTJKlnz556++23FRQUVCCTAwB3I/cAWA25B8BqyD0AVkLmAfAGebqmhmEYDve/+uorXbp0yaUTAgBPQu4BsBpyD4DVkHsArITMA+AN8nWh8Cw3ByEAeDtyD4DVkHsArIbcA2AlZB4AM8pTU8PHxyfbefU4zx4Ab0buAbAacg+A1ZB7AKyEzAPgDfJ0TQ3DMNS/f3/5+flJkq5evaqhQ4fK39/fYdzKlStdN0MAcCNyD4DVkHsArIbcA2AlZB4Ab5CnpkZkZKTD/T59+rh0MgDgacg9AFZD7gGwGnIPgJWQeQC8QZ6aGvPnzy+oeQCARyL3AFgNuQfAasg9AFZC5gHwBnd0oXAAAAAAAAAAAIDCQlMDAAAAAAAAAACYglubGlu2bFGXLl1UpUoV+fj46NNPP3VY379/f/n4+DjcOnbs6DDm7Nmz6t27twICAlS2bFkNGjRIFy9edBizZ88etW7dWiVKlFBISIimTZtW0LsGAAAAUe8BsBYyD4DVkHsA3MGtTY1Lly6pcePGmj17dq5jOnbsqNOnT9tvn3zyicP63r17a//+/YqPj9fq1au1ZcsWDRkyxL7eZrOpQ4cOqlatmpKSkvTGG2/olVde0T//+c8C2y8AyA0FHwCrod4DYCVkHgCrIfcAuEOeLhTuap06dVKnTp1uOcbPz0/BwcE5rvvhhx+0Zs0a7dy5Uw888IAk6Z133lHnzp315ptvqkqVKlq8eLHS09P10UcfydfXV/Xr19fu3bs1Y8YMh4AEgMKQVfANHDhQ3bt3z3FMx44dHS7e5ufn57C+d+/eOn36tOLj43Xt2jUNGDBAQ4YM0ZIlSyT9r+Br37695s6dq71792rgwIEqW7YsuQeg0FHvAbASMg+A1Xhq7qWlpSktLc1+32az5XMPAXgij7+mRkJCgipVqqTatWtr2LBh+v333+3rEhMTVbZsWXvoSVL79u1VpEgRffPNN/YxjzzyiHx9fe1jwsPDdejQIZ07dy7H50xLS5PNZnO4AYArdOrUSa+++qoee+yxXMdkFXxZt3LlytnXZRV8H374oZo3b65WrVrpnXfe0dKlS3Xq1ClJcij46tevr169eum5557TjBkzcn1Ocg+AO1HvAbASd2SeRO4BcB935F5MTIwCAwPtt5CQkALaOwDu4NFNjY4dO2rhwoXasGGDXn/9dW3evFmdOnVSRkaGJCk5OVmVKlVyeEyxYsVUvnx5JScn28cEBQU5jMm6nzXmZgQfAHei4ANgJdR7AKzEXZknkXsA3MNduRcdHa3U1FT77eTJk67eNQBu5NbTT91Or1697P/fsGFDNWrUSDVq1FBCQoLatWtXYM8bHR2t0aNH2+/bbDYKPgCFomPHjurevbtCQ0N19OhRvfTSS+rUqZMSExNVtGhRpwu+0NBQhzE3Fnw3fvMjC7kHwF2o9wBYibsyTyL3ALiHu3LPz88v26mcAXgPj25q3Ozee+9VxYoVdeTIEbVr107BwcE6c+aMw5jr16/r7Nmz9nP1BQcHKyUlxWFM1v3czudH8AFwFwo+AFZHvQfASgor8yRyD4BnKMzcA+C9PPr0Uzf75Zdf9Pvvv6ty5cqSpLCwMJ0/f15JSUn2MRs3blRmZqaaN29uH7NlyxZdu3bNPiY+Pl61a9fO8a+VAcCT3FjwSaLgA+D1qPcAWAmZB8BqyD0AruDWpsbFixe1e/du7d69W5J07Ngx7d69WydOnNDFixf1wgsvaPv27Tp+/Lg2bNigrl27qmbNmgoPD5ck1a1bVx07dtQzzzyjHTt2aOvWrRo+fLh69eqlKlWqSJKefvpp+fr6atCgQdq/f7+WLVumWbNmOXztFgA8FQUfALOj3gNgJWQeAKsh9wC4g1ubGt9++63uv/9+3X///ZKk0aNH6/7779eECRNUtGhR7dmzR3/9619Vq1YtDRo0SM2aNdN//vMfh6/MLl68WHXq1FG7du3UuXNntWrVSv/85z/t6wMDA7Vu3TodO3ZMzZo105gxYzRhwgQNGTKk0PcXACj4AFgN9R4AKyHzAFgNuQfAHXwMwzDcPQlPZ7PZFBgYqNTUVAUEBNx2fPUX4wphVoB1HJ8a4dS4vP6uukNCQoIeffTRbMsjIyP13nvvqVu3btq1a5fOnz+vKlWqqEOHDpoyZYr9Qt+SdPbsWQ0fPlxffPGFihQpoh49eujtt99W6dKl7WP27NmjqKgo7dy5UxUrVtSzzz6rcePGOT1Pcg9wL2/KPbMg9wD3IvcKX15eSzIPcD1nco/Mcx1qPcC9XF3rmepC4QBgdm3atNGteslr16697TbKly+vJUuW3HJMo0aN9J///CfP8wMAAAAAAAA8makuFA4AAAAAAAAAAKyLpgYAAAAAAAAAADAFmhoAAAAAAAAAAMAUaGoAAAAAAAAAAABToKkBAAAAAAAAAABMgaYGAAAAAAAAAAAwBZoaAAAAAAAAAADAFGhqAAAAAAAAAAAAU6CpAQAAAAAAAAAATIGmBgAAAAAAAAAAMAWaGgAAAAAAAAAAwBRoagAAAAAAAAAAAFOgqQEAAAAAAAAAAEyBpgYAAAAAAAAAADAFmhoAAAAAAAAAAMAUaGoAAAAAAAAAAABToKkBAAAAAAAAAABMgaYGAAAAAAAAAAAwBZoaAAAAAAAAAADAFGhqAAAAAAAAAAAAU6CpAQAAAAAAAAAATIGmBgAAAAAAAAAAMAW3NjW2bNmiLl26qEqVKvLx8dGnn37qsN4wDE2YMEGVK1dWyZIl1b59ex0+fNhhzNmzZ9W7d28FBASobNmyGjRokC5evOgwZs+ePWrdurVKlCihkJAQTZs2raB3DQAAAKLeA2AtZB4AqyH3ALiDW5saly5dUuPGjTV79uwc10+bNk1vv/225s6dq2+++Ub+/v4KDw/X1atX7WN69+6t/fv3Kz4+XqtXr9aWLVs0ZMgQ+3qbzaYOHTqoWrVqSkpK0htvvKFXXnlF//znPwt8/wDgZhR8AKyGeg+AlZB5AKyG3APgDsXc+eSdOnVSp06dclxnGIbeeustjR8/Xl27dpUkLVy4UEFBQfr000/Vq1cv/fDDD1qzZo127typBx54QJL0zjvvqHPnznrzzTdVpUoVLV68WOnp6froo4/k6+ur+vXra/fu3ZoxY4ZDQAJAYcgq+AYOHKju3btnW59V8C1YsEChoaF6+eWXFR4ergMHDqhEiRKS/ij4Tp8+rfj4eF27dk0DBgzQkCFDtGTJEkn/K/jat2+vuXPnau/evRo4cKDKli1L7gEodNR7AKyEzANgNZ6ae2lpaUpLS7Pft9lsLt5zAO7ksdfUOHbsmJKTk9W+fXv7ssDAQDVv3lyJiYmSpMTERJUtW9YeepLUvn17FSlSRN988419zCOPPCJfX1/7mPDwcB06dEjnzp3L8bnT0tJks9kcbgDgCp06ddKrr76qxx57LNu6mwu+Ro0aaeHChTp16pT9Gx1ZBd+HH36o5s2bq1WrVnrnnXe0dOlSnTp1SpIcCr769eurV69eeu655zRjxoxc50XuAXAH6j0AVuLOzJPIPQCFz525FxMTo8DAQPstJCSkIHYRgJt4bFMjOTlZkhQUFOSwPCgoyL4uOTlZlSpVclhfrFgxlS9f3mFMTtu48TluRvABcAcKPgBWQ70HwErcmXkSuQeg8Lkz96Kjo5Wammq/nTx58s53CIDH8NimhjsRfADcgYIPAAoPuQfAasg9AFbi5+engIAAhxsA7+GxTY3g4GBJUkpKisPylJQU+7rg4GCdOXPGYf3169d19uxZhzE5bePG57gZwQfAasg9AO5AvQfAStyZeRK5B6DwuTv3AHgvj21qhIaGKjg4WBs2bLAvs9ls+uabbxQWFiZJCgsL0/nz55WUlGQfs3HjRmVmZqp58+b2MVu2bNG1a9fsY+Lj41W7dm2VK1eukPYGAG6Pgg+A1VDvAbASMg+A1ZB7AAqKW5saFy9e1O7du7V7925Jf5xPfvfu3Tpx4oR8fHw0cuRIvfrqq/r888+1d+9e9evXT1WqVFG3bt0kSXXr1lXHjh31zDPPaMeOHdq6dauGDx+uXr16qUqVKpKkp59+Wr6+vho0aJD279+vZcuWadasWRo9erSb9hoAckbBB8AbUe8BsBIyD4DVkHsA3KGYO5/822+/1aOPPmq/nxVGkZGRio2N1dixY3Xp0iUNGTJE58+fV6tWrbRmzRqVKFHC/pjFixdr+PDhateunYoUKaIePXro7bfftq8PDAzUunXrFBUVpWbNmqlixYqaMGGChgwZUng7CgD/38WLF3XkyBH7/ayCr3z58rrnnnvsBd99992n0NBQvfzyy7kWfHPnztW1a9dyLPgmTZqkQYMGady4cdq3b59mzZqlmTNnumOXAVgc9R4AKyHzAFgNuQfAHXwMwzDcPQlPZ7PZFBgYqNTUVKfOO1r9xbhCmBVgHcenRjg1Lq+/q+6QkJDgUPBlySr4DMPQxIkT9c9//tNe8M2ZM0e1atWyjz179qyGDx+uL774wqHgK126tH3Mnj17FBUVpZ07d6pixYp69tlnNW7cOKfnSe4B7uVNuWcW5B7gXuRe4cvLa0nmAa7nTO6Rea5DrQe4l6trPbd+UwMArKZNmza6VS/Zx8dHkydP1uTJk3MdU758eS1ZsuSWz9OoUSP95z//yfc8AQAAAAAAAE/ksRcKBwAAAAAAAAAAuBFNDQAAAAAAAAAAYAo0NQAAAAAAAAAAgCnQ1AAAAAAAAAAAAKZAUwMAAAAAAAAAAJgCTQ0AAAAAAAAAAGAKNDUAAAAAAAAAAIAp0NQAAAAAAAAAAACmQFMDAAAAAAAAAACYAk0NAAAAAAAAAABgCjQ1AAAAAAAAAACAKdDUAAAAAAAAAAAApkBTAwAAAAAAAAAAmAJNDQAAAAAAAAAAYAo0NQAAAAAAAAAAgCnQ1AAAAAAAAAAAAKZAUwMAAAAAAAAAAJgCTQ0AAAAAAAAAAGAKNDUAAAAAAAAAAIAp0NQAAAAAAAAAAACmQFMDAAAAAAAAAACYAk0NAAAAAAAAAABgCh7d1HjllVfk4+PjcKtTp459/dWrVxUVFaUKFSqodOnS6tGjh1JSUhy2ceLECUVERKhUqVKqVKmSXnjhBV2/fr2wdwUAnELuAbAacg+A1ZB7AKyEzANQEIq5ewK3U79+fa1fv95+v1ix/0151KhRiouL04oVKxQYGKjhw4ere/fu2rp1qyQpIyNDERERCg4O1rZt23T69Gn169dPxYsX1z/+8Y9C3xcAcAa5B8BqyD0AVkPuAbASMg+Aq3l8U6NYsWIKDg7Otjw1NVXz5s3TkiVL1LZtW0nS/PnzVbduXW3fvl0tWrTQunXrdODAAa1fv15BQUFq0qSJpkyZonHjxumVV16Rr69vYe8OANwWuQfAasg9AFZD7gGwEjIPgKt59OmnJOnw4cOqUqWK7r33XvXu3VsnTpyQJCUlJenatWtq3769fWydOnV0zz33KDExUZKUmJiohg0bKigoyD4mPDxcNptN+/fvz/U509LSZLPZHG4AUFjIPQBWQ+4BsBpyD4CVkHkAXM2jmxrNmzdXbGys1qxZo/fee0/Hjh1T69atdeHCBSUnJ8vX11dly5Z1eExQUJCSk5MlScnJyQ6hl7U+a11uYmJiFBgYaL+FhIS4dscAIBfkHgCrIfcAWA25B8BKyDwABcGjTz/VqVMn+/83atRIzZs3V7Vq1bR8+XKVLFmywJ43Ojpao0ePtt+32WyEH4BCQe4BsBpyD4DVkHsArITMA1AQPPqbGjcrW7asatWqpSNHjig4OFjp6ek6f/68w5iUlBT7efqCg4OVkpKSbX3Wutz4+fkpICDA4QYA7kDuAbAacg+A1ZB7AKyEzAPgCqZqaly8eFFHjx5V5cqV1axZMxUvXlwbNmywrz906JBOnDihsLAwSVJYWJj27t2rM2fO2MfEx8crICBA9erVK/T5A0BekXsArIbcA2A15B4AKyHzALiCR59+6vnnn1eXLl1UrVo1nTp1ShMnTlTRokX11FNPKTAwUIMGDdLo0aNVvnx5BQQE6Nlnn1VYWJhatGghSerQoYPq1aunvn37atq0aUpOTtb48eMVFRUlPz8/N+8dAGRH7gGwGnIPgNWQewCshMwDUBA8uqnxyy+/6KmnntLvv/+uu+66S61atdL27dt11113SZJmzpypIkWKqEePHkpLS1N4eLjmzJljf3zRokW1evVqDRs2TGFhYfL391dkZKQmT57srl0CgFsi9wBYDbkHwGrIPQBWQuYBKAg+hmEY7p6Ep7PZbAoMDFRqaqpT5+Cr/mJcIcwKsI7jUyOcGpfX31XkjtwD3IvcK3zkHuBe5F7hy8trSeYBrudM7pF5rkOtB7iXq2s9U11TAwAAAAAAAAAAWBdNDQAAAAAAAAAAYAo0NQAAAAAAAAAAgCnQ1AAAAAAAAAAAAKZAUwMAAAAAAAAAAJgCTQ0AAAAAAAAAAGAKNDUAAAAAAAAAAIAp0NQAAAAAAAAAAACmQFMDAAAAAAAAAACYAk0NAAAAAAAAAABgCjQ1AAAAAAAAAACAKdDUAAAAAAAAAAAApkBTAwAAAAAAAAAAmAJNDQAAAAAAAAAAYAo0NQAAAAAAAAAAgCnQ1AAAAAAAAAAAAKZAUwMAAAAAAAAAAJgCTQ0AAAAAAAAAAGAKNDUAAAAAAAAAAIAp0NQAAAAAAAAAAACmQFMDAAAAAAAAAACYAk0NAAAAAAAAAABgCjQ1AAAAAAAAAACAKViqqTF79mxVr15dJUqUUPPmzbVjxw53TwkAChS5B8BKyDwAVkPuAbAacg+AZKGmxrJlyzR69GhNnDhR3333nRo3bqzw8HCdOXPG3VMDgAJB7gGwEjIPgNWQewCshtwDkMUyTY0ZM2bomWee0YABA1SvXj3NnTtXpUqV0kcffeTuqQFAgSD3AFgJmQfAasg9AFZD7gHIUszdEygM6enpSkpKUnR0tH1ZkSJF1L59eyUmJmYbn5aWprS0NPv91NRUSZLNZnPq+TLTLt/hjAHcyNnfvaxxhmEU5HRMgdwDzI3cy5u8Zp5E7gGehtzLm8LOPTIPcD1nfvfIvP/hMy5gbq6u9SzR1Pjvf/+rjIwMBQUFOSwPCgrSwYMHs42PiYnRpEmTsi0PCQkpsDkCyF3gW3kbf+HCBQUGBhbIXMyC3APMjdzLm7xmnkTuAZ6G3Msbcg8wv7zkntUzT+IzLmB2rq71LNHUyKvo6GiNHj3afj8zM1Nnz55VhQoV5OPjc8vH2mw2hYSE6OTJkwoICCjoqRYK9snzedv+SPnbJ8MwdOHCBVWpUqWAZ+d9yL3CwWvlHF4n55F7+UfuFTxeJ+fxWjmP3Mu//OYeP5/O47VyHq+Vc8i8/KPWc8Q+eT5v2x+pYI/tWaKpUbFiRRUtWlQpKSkOy1NSUhQcHJxtvJ+fn/z8/ByWlS1bNk/PGRAQ4DU/gFnYJ8/nbfsj5X2frP7XK1nIPc/Ga+UcXifnkHt5zzyJ3CtMvE7O47VyDrnnntzj59N5vFbO47W6PTLvD3zGdQ32yfN52/5IBXNszxIXCvf19VWzZs20YcMG+7LMzExt2LBBYWFhbpwZABQMcg+AlZB5AKyG3ANgNeQegBtZ4psakjR69GhFRkbqgQce0EMPPaS33npLly5d0oABA9w9NQAoEOQeACsh8wBYDbkHwGrIPQBZLNPU6Nmzp3777TdNmDBBycnJatKkidasWZPtAkN3ys/PTxMnTsz2FTczY588n7ftj+Sd+1TYyD3Pw2vlHF4n5EdhZZ7Ez6izeJ2cx2uF/KDW8zy8Vs7jtUJ+kHv5xz55Pm/bH6lg98nHMAzD5VsFAAAAAAAAAABwMUtcUwMAAAAAAAAAAJgfTQ0AAAAAAAAAAGAKNDUAAAAAAAAAAIAp0NQAAAAAAAAAAACmQFMjH2bPnq3q1aurRIkSat68uXbs2HHL8StWrFCdOnVUokQJNWzYUF9++WUhzfT2YmJi9OCDD6pMmTKqVKmSunXrpkOHDt3yMbGxsfLx8XG4lShRopBmfHuvvPJKtvnVqVPnlo/x5PdIkqpXr55tn3x8fBQVFZXjeE97j7Zs2aIuXbqoSpUq8vHx0aeffuqw3jAMTZgwQZUrV1bJkiXVvn17HT58+LbbzevvIvLPm3KvoOXltfrggw/UunVrlStXTuXKlVP79u0t83Oc39/fpUuXysfHR926dSvYCcLSyDznkXnOI/fgycg955F7ziP34Mm8Mffysk+edtzoZrc7jpSThIQENW3aVH5+fqpZs6ZiY2MLfJ55kdd9SkhIyPFYYHJycuFM+Dbyc0xZct3vEk2NPFq2bJlGjx6tiRMn6rvvvlPjxo0VHh6uM2fO5Dh+27ZteuqppzRo0CDt2rVL3bp1U7du3bRv375CnnnONm/erKioKG3fvl3x8fG6du2aOnTooEuXLt3ycQEBATp9+rT99vPPPxfSjJ1Tv359h/l9/fXXuY719PdIknbu3OmwP/Hx8ZKkJ554ItfHeNJ7dOnSJTVu3FizZ8/Ocf20adP09ttva+7cufrmm2/k7++v8PBwXb16Nddt5vV3EfnnbblXkPL6WiUkJOipp57Spk2blJiYqJCQEHXo0EG//vprIc+8cOX39/f48eN6/vnn1bp160KaKayIzHMemec8cg+ejNxzHrnnPHIPnswbcy8/v3OedNzoZrc7jnSzY8eOKSIiQo8++qh2796tkSNHavDgwVq7dm0Bz9R5ed2nLIcOHXJ4nypVqlRAM8yb/BxTdunvkoE8eeihh4yoqCj7/YyMDKNKlSpGTExMjuOffPJJIyIiwmFZ8+bNjb/97W8FOs/8OnPmjCHJ2Lx5c65j5s+fbwQGBhbepPJo4sSJRuPGjZ0eb7b3yDAMY8SIEUaNGjWMzMzMHNd78nskyVi1apX9fmZmphEcHGy88cYb9mXnz583/Pz8jE8++STX7eT1dxH55+2550p3+nN5/fp1o0yZMsaCBQsKaooeIT+v0/Xr142WLVsaH374oREZGWl07dq1EGYKKyLznEfmOY/cgycj95xH7jmP3IMn88bcy+s+efJxo5vdfBwpJ2PHjjXq16/vsKxnz55GeHh4Ac4s/5zZp02bNhmSjHPnzhXKnO6UM8eUXfm7xDc18iA9PV1JSUlq3769fVmRIkXUvn17JSYm5viYxMREh/GSFB4enut4d0tNTZUklS9f/pbjLl68qGrVqikkJERdu3bV/v37C2N6Tjt8+LCqVKmie++9V71799aJEydyHWu29yg9PV2LFi3SwIED5ePjk+s4T3+Pshw7dkzJyckO70FgYKCaN2+e63uQn99F5I8Vcs9VXPFzefnyZV27du22GWxm+X2dJk+erEqVKmnQoEGFMU1YFJnnPDLPeeQePBm55zxyz3nkHjyZN+Zefn/nzHLcyBme/h7diSZNmqhy5cr685//rK1bt7p7Orly5piyK98nmhp58N///lcZGRkKCgpyWB4UFJTr+cySk5PzNN6dMjMzNXLkSD388MNq0KBBruNq166tjz76SJ999pkWLVqkzMxMtWzZUr/88kshzjZ3zZs3V2xsrNasWaP33ntPx44dU+vWrXXhwoUcx5vpPZKkTz/9VOfPn1f//v1zHePp79GNsl7nvLwH+fldRP54e+65kit+LseNG6cqVapk+0fem+Tndfr66681b948ffDBB4UxRVgYmec8Ms955B48GbnnPHLPeeQePJk35l5+9slMx42ckdt7ZLPZdOXKFTfN6s5UrlxZc+fO1b///W/9+9//VkhIiNq0aaPvvvvO3VPLxtljyq78XSqW50fAa0VFRWnfvn23vP6EJIWFhSksLMx+v2XLlqpbt67ef/99TZkypaCneVudOnWy/3+jRo3UvHlzVatWTcuXL/eKv/iYN2+eOnXqpCpVquQ6xtPfIwDZTZ06VUuXLlVCQoJHXaDN3S5cuKC+ffvqgw8+UMWKFd09HQAuQubljtwDvBO5lztyDyh8HDfyfLVr11bt2rXt91u2bKmjR49q5syZ+vjjj904s+ycPabsSjQ18qBixYoqWrSoUlJSHJanpKQoODg4x8cEBwfnaby7DB8+XKtXr9aWLVtUtWrVPD22ePHiuv/++3XkyJECmt2dKVu2rGrVqpXr/MzyHknSzz//rPXr12vlypV5epwnv0dZr3NKSooqV65sX56SkqImTZrk+Jj8/C4if7w591ztTn4u33zzTU2dOlXr169Xo0aNCnKabpfX1+no0aM6fvy4unTpYl+WmZkpSSpWrJgOHTqkGjVqFOykYRlknvPIPOeRe/Bk5J7zyD3nkXvwZN6Ye644RuLJx42ckdt7FBAQoJIlS7ppVq730EMPFWrjwBl5Oabsyt8lTj+VB76+vmrWrJk2bNhgX5aZmakNGzY4dDdvFBYW5jBekuLj43MdX9gMw9Dw4cO1atUqbdy4UaGhoXneRkZGhvbu3etwQNqTXLx4UUePHs11fp7+Ht1o/vz5qlSpkiIiIvL0OE9+j0JDQxUcHOzwHthsNn3zzTe5vgf5+V1E/nhj7hWU/P5cTps2TVOmTNGaNWv0wAMPFMZU3Sqvr1OdOnW0d+9e7d69237761//qkcffVS7d+9WSEhIYU4fXo7Mcx6Z5zxyD56M3HMeuec8cg+ezBtzzxXHSDz5uJEzPP09cpXdu3d7zHuUn2PKLn2f8nxpcYtbunSp4efnZ8TGxhoHDhwwhgwZYpQtW9ZITk42DMMw+vbta7z44ov28Vu3bjWKFStmvPnmm8YPP/xgTJw40ShevLixd+9ed+2Cg2HDhhmBgYFGQkKCcfr0afvt8uXL9jE379OkSZOMtWvXGkePHjWSkpKMXr16GSVKlDD279/vjl3IZsyYMUZCQoJx7NgxY+vWrUb79u2NihUrGmfOnDEMw3zvUZaMjAzjnnvuMcaNG5dtnae/RxcuXDB27dpl7Nq1y5BkzJgxw9i1a5fx888/G4ZhGFOnTjXKli1rfPbZZ8aePXuMrl27GqGhocaVK1fs22jbtq3xzjvv2O/f7ncRruNtuVeQ8vpaTZ061fD19TX+9a9/OWTwhQsX3LULhSKvr9PNIiMjja5duxbSbGE1ZJ7zyDznkXvwZOSe88g955F78GTemHt53SdPO250s9sdR3rxxReNvn372sf/9NNPRqlSpYwXXnjB+OGHH4zZs2cbRYsWNdasWeOuXcgmr/s0c+ZM49NPPzUOHz5s7N271xgxYoRRpEgRY/369e7aBQf5Oabsyt8lmhr58M477xj33HOP4evrazz00EPG9u3b7ev+9Kc/GZGRkQ7jly9fbtSqVcvw9fU16tevb8TFxRXyjHMnKcfb/Pnz7WNu3qeRI0fa9z8oKMjo3Lmz8d133xX+5HPRs2dPo3Llyoavr69x9913Gz179jSOHDliX2+29yjL2rVrDUnGoUOHsq3z9Pdo06ZNOf6cZc05MzPTePnll42goCDDz8/PaNeuXbb9rFatmjFx4kSHZbf6XYRreVPuFbS8vFbVqlXL8Xfj5p91b5TXn6kb8SEXBY3Mcx6Z5zxyD56M3HMeuec8cg+ezBtzLy/75GnHjW52u+NIkZGRxp/+9Kdsj2nSpInh6+tr3HvvvQ7HNj1BXvfp9ddfN2rUqGGUKFHCKF++vNGmTRtj48aN7pl8DvJzTNkwXPe75PP/JwEAAAAAAAAAAODRuKYGAAAAAAAAAAAwBZoaAAAAAAAAAADAFGhqAAAAAAAAAAAAU6CpAQAAAAAAAAAATIGmBgAAAAAAAAAAMAWaGgAAAAAAAAAAwBRoagAAAAAAAAAAAFOgqQEAAAAAAAAAAEyBpgYsISEhQT4+Pjp//ry7pwIAAAAAAADABW4+5hcbG6uyZcu6dU4oeDQ1AAAAAAAAAACm07JlS50+fVqBgYHungoKUTF3TwAoDOnp6e6eAgAAAAAAAAAX8vX1VXBwsLungULGNzXgldq0aaPhw4dr5MiRqlixosLDwyVJSUlJeuCBB1SqVCm1bNlShw4dcnjce++9pxo1asjX11e1a9fWxx9/7I7pA0C+/fOf/1SVKlWUmZnpsLxr164aMGCA2rdvr/DwcBmGIUk6e/asqlatqgkTJrhjugBwR26VeW3btlWRIkX07bffOqx76623VK1atWyPAQCzuFX2DRw4UNWrV5ePj0+2GwC4S2ZmpmJiYhQaGqqSJUuqcePG+te//iXpf6ePiouLU6NGjVSiRAm1aNFC+/btsz/+559/VpcuXVSuXDn5+/urfv36+vLLLx0ef6tTzt/ueJ+Pj48+/PBDPfbYYypVqpTuu+8+ff75565/IeAyNDXgtRYsWCBfX19t3bpVc+fOlST9/e9/1/Tp0/Xtt9+qWLFiGjhwoH38qlWrNGLECI0ZM0b79u3T3/72Nw0YMECbNm1y1y4AQJ498cQT+v333x2y6+zZs1qzZo369OmjBQsWaOfOnXr77bclSUOHDtXdd99NUwOAKd0q8/7+97+rffv2mj9/vsNj5s+fr/79+6tIET4KATCnW2Vf7969tXPnTp0+fVqnT5/WL7/8ohYtWqh169ZunDEAq4uJidHChQs1d+5c7d+/X6NGjVKfPn20efNm+5gXXnhB06dP186dO3XXXXepS5cuunbtmiQpKipKaWlp2rJli/bu3avXX39dpUuXduq5nT3eN2nSJD355JPas2ePOnfurN69e+vs2bOuexHgWgbghf70pz8Z999/v/3+pk2bDEnG+vXr7cvi4uIMScaVK1cMwzCMli1bGs8884zDdp544gmjc+fOhTNpAHCRrl27GgMHDrTff//9940qVaoYGRkZhmEYxvLly40SJUoYL774ouHv72/8+OOP7poqANyxW2XesmXLjHLlyhlXr141DMMwkpKSDB8fH+PYsWNumi0AuMbt6r0szz33nFGtWjXjzJkzhT1FADAMwzCuXr1qlCpVyti2bZvD8kGDBhlPPfWU/Zjd0qVL7et+//13o2TJksayZcsMwzCMhg0bGq+88kqO2896/Llz5wzDMIz58+cbgYGB9vXOHO+TZIwfP95+/+LFi4Yk46uvvsrXPqPg8edJ8FrNmjXLtqxRo0b2/69cubIk6cyZM5KkH374QQ8//LDD+Icfflg//PBDAc4SAFyvd+/e+ve//620tDRJ0uLFi9WrVy/7XyU/8cQTeuyxxzR16lS9+eabuu+++9w5XQC4I7fKvG7duqlo0aJatWqVJCk2NlaPPvqoqlev7sYZA8Cdu129J/1xmqp58+bp888/11133eWuqQKwuCNHjujy5cv685//rNKlS9tvCxcu1NGjR+3jwsLC7P9fvnx51a5d235M7rnnntOrr76qhx9+WBMnTtSePXucfn5nj/fdeMzQ399fAQEB9mOG8Dw0NeC1/P39sy0rXry4/f+zzinK+ZQBeJsuXbrIMAzFxcXp5MmT+s9//qPevXvb11++fFlJSUkqWrSoDh8+7MaZAsCdu1Xm+fr6ql+/fpo/f77S09O1ZMkSh9OPAoBZ3a7e27Rpk5599lktXLjQ4UAdABS2ixcvSpLi4uK0e/du++3AgQP262rczuDBg/XTTz+pb9++2rt3rx544AG98847Lp3njccMpT+OG3LM0HPR1AD+v7p162rr1q0Oy7Zu3ap69eq5aUYAkD8lSpRQ9+7dtXjxYn3yySeqXbu2mjZtal8/ZswYFSlSRF999ZXefvttbdy40Y2zBYA7c7vMGzx4sNavX685c+bo+vXr6t69uxtnCwCucavsO3LkiB5//HG99NJLZB4At6tXr578/Px04sQJ1axZ0+EWEhJiH7d9+3b7/587d04//vij6tata18WEhKioUOHauXKlRozZow++OADp56f433eqZi7JwB4ihdeeEFPPvmk7r//frVv315ffPGFVq5cqfXr17t7agCQZ71799Zf/vIX7d+/X3369LEvj4uL00cffaTExEQ1bdpUL7zwgiIjI7Vnzx6VK1fOjTMGgPzLLfOkPz7ItmjRQuPGjdPAgQNVsmRJN80SAFwrp+y7cuWKunTpovvvv19DhgxRcnKyfXxwcLC7pgrAwsqUKaPnn39eo0aNUmZmplq1aqXU1FRt3bpVAQEBqlatmiRp8uTJqlChgoKCgvT3v/9dFStWVLdu3SRJI0eOVKdOnVSrVi2dO3dOmzZtcmh43ArH+7wTTQ3g/+vWrZtmzZqlN998UyNGjFBoaKjmz5+vNm3auHtqAJBnbdu2Vfny5XXo0CE9/fTTkqTffvtNgwYN0iuvvGL/S75JkyZp3bp1Gjp0qJYtW+bOKQNAvuWUeTcaNGiQtm3bxqmnAHiVnLIvJSVFBw8e1MGDB1WlShWH8YZhuGOaAKApU6borrvuUkxMjH766SeVLVtWTZs21UsvvWQ/xdPUqVM1YsQIHT58WE2aNNEXX3whX19fSVJGRoaioqL0yy+/KCAgQB07dtTMmTOdem6O93knH4N/1QAAAAB4sSlTpmjFihV5uqgkAAAACl5CQoIeffRRnTt3TmXLlnX3dGASXFMDAAAAgFe6ePGi9u3bp3fffVfPPvusu6cDAAAAwAVoagAAAADwSsOHD1ezZs3Upk0bTj0FAAAAeAlOPwUAAAAAAAAAAEyBb2oAAAAAAAAAAABToKkBAAAAAAAAAABMgaYGAAAAAAAAAAAwBZoaAAAAAAAAAADAFGhqAAAAAAAAAAAAU6CpAQAAAAAAAAAATIGmBgAAAAAAAAAAMAWaGgAAAAAAAAAAwBT+HxrdOixOCrN3AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plotting the histograms of rho, vx and epsilon\n",
        "plt.figure(figsize=(16, 4))\n",
        "plt.subplot(1, 5, 1)\n",
        "plt.hist(rho_train, bins=20)\n",
        "plt.xlabel(\"rho\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "#plt.yscale(\"log\")\n",
        "plt.subplot(1, 5, 2)\n",
        "plt.hist(vx_train, bins=20)\n",
        "plt.xlabel(\"vx\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "#plt.yscale(\"log\")\n",
        "plt.subplot(1, 5, 3)\n",
        "plt.hist(vy_train, bins=20)\n",
        "plt.xlabel(\"vy\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "#plt.yscale(\"log\")\n",
        "plt.subplot(1, 5, 4)\n",
        "plt.hist(vz_train, bins=20)\n",
        "plt.xlabel(\"vz\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "#plt.yscale(\"log\")\n",
        "plt.subplot(1, 5, 5)\n",
        "plt.hist(epsilon_train, bins=20)\n",
        "plt.xlabel(\"epsilon\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "#plt.yscale(\"log\")\n",
        "plt.suptitle(\"Primitive variables\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1fsekS7zvExL"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7ubtOXVgUZh6"
      },
      "outputs": [],
      "source": [
        "# Generating the input and output data for train and test sets.\n",
        "x_train = generate_input_data(rho_train, vx_train ,vy_train, vz_train, epsilon_train)\n",
        "y_train = generate_labels(rho_train, epsilon_train) \n",
        "x_test = generate_input_data(rho_test, vx_test, vy_test, vz_test, epsilon_test)\n",
        "y_test = generate_labels(rho_test, epsilon_test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lG_10Fx0vExM",
        "outputId": "b615afd6-ed6c-4302-8836-7074b5c67251",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.2253,  1.2439,  0.5428,  0.9811,  2.6203],\n",
              "        [ 8.6760, 26.3492,  1.3128, 22.2189, 32.8612],\n",
              "        [ 1.6644,  1.8987,  1.2941,  0.6595,  1.8787],\n",
              "        ...,\n",
              "        [ 0.8806,  0.3053,  0.6187,  1.1567,  1.7092],\n",
              "        [ 9.4032,  9.6825,  9.5324, 16.1767, 22.2331],\n",
              "        [ 4.6567,  2.9289,  0.3831,  2.7460,  8.2808]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.4350, 7.3753, 0.5281,  ..., 0.8335, 8.3240, 4.9954], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0324e+01, 7.6037e+00, 4.1150e+00, 7.3877e+00, 1.5725e+01],\n",
              "        [6.1274e+00, 8.8058e+00, 1.2547e+01, 1.0761e+01, 1.9520e+01],\n",
              "        [2.8945e+00, 7.2054e-01, 2.6138e+00, 1.5800e+00, 2.6378e+00],\n",
              "        ...,\n",
              "        [1.0343e+00, 9.6165e-03, 8.6076e-02, 6.1373e-02, 7.7322e-02],\n",
              "        [8.6855e+00, 1.3404e+00, 1.4288e+01, 9.4060e+00, 1.4448e+01],\n",
              "        [1.1708e+01, 1.1230e+01, 2.6180e+01, 2.4258e+00, 2.8773e+01]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([8.3647, 6.5048, 0.9900,  ..., 0.0480, 3.6481, 9.7164], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "x_train\n",
        "y_train\n",
        "x_test\n",
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CqQxpKTYvExM"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"last_expr_or_assign\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "T2bsgZyzvExM",
        "outputId": "026f6436-9429-4102-da85-16cce95a7577",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 950
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Note how we are only plotting train and not test here. \n",
        "# Plotting histograms of the input variables before z-score normalization\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.suptitle('Histograms of input variables before z-score normalization')\n",
        "for i in range(5):\n",
        "    plt.subplot(3, 2, i+1)\n",
        "    plt.hist(x_train[:, i].cpu(), bins=50) # Must be converted to cpu() for plotting.\n",
        "    plt.xlabel(f'Variable {i}')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QrN95284vExM",
        "outputId": "d2992735-6b0c-4aa9-e264-0c9a1cd18628",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary statistics of input variables before z-score normalization\n",
            "tensor([[2.0201e-04, 3.6966e+01, 6.3568e+00, 6.2493e+00, 3.8162e+00],\n",
            "        [1.7208e-05, 2.1647e+02, 6.8448e+00, 3.8932e+00, 8.7775e+00],\n",
            "        [2.1043e-05, 2.0937e+02, 6.8447e+00, 3.8528e+00, 8.7732e+00],\n",
            "        [3.4876e-05, 2.1949e+02, 6.8660e+00, 3.8964e+00, 8.7893e+00],\n",
            "        [6.3357e-05, 3.5165e+02, 1.2107e+01, 8.2963e+00, 1.3245e+01]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# print('Summary statistics of input variables before z-score normalization')\n",
        "# print(torch.stack([torch.min(x_train, dim=0).values,\n",
        "#                 torch.max(x_train, dim=0).values,\n",
        "#                 torch.mean(x_train, dim=0),\n",
        "#                 torch.median(x_train, dim=0).values,\n",
        "#                 torch.std(x_train, dim=0)], dim=1))\n",
        "\n",
        "# Computing summary statistics of the input variables before and after z-score normalization\n",
        "print('Summary statistics of input variables before z-score normalization')\n",
        "print(torch.stack([torch.min(x_train, dim=0).values, torch.max(x_train, dim=0).values, torch.nanmean(x_train, dim=0), torch.median(x_train, dim=0).values, torch.std(x_train, dim=0)], dim=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTEmkR1SUZh7"
      },
      "source": [
        "Perform z-score normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kqUmp1VVvExN"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yPOv6DxhUZh7"
      },
      "outputs": [],
      "source": [
        "if ZSCORE_NORMALIZATION:\n",
        "    \n",
        "    # # Computing the median of each input variable from the training set using torch.nanmedian function\n",
        "    # D_median = torch.nanmedian(x_train[:, 0])\n",
        "    # Sx_median = torch.nanmedian(x_train[:, 1])\n",
        "    # Sy_median = torch.nanmedian(x_train[:, 2])\n",
        "    # Sz_median = torch.nanmedian(x_train[:, 3])\n",
        "    # tau_median = torch.nanmedian(x_train[:, 4])\n",
        "\n",
        "    # # Computing the standard deviation of each input variable from the training set using torch.std function with a boolean mask to ignore nan values\n",
        "    # D_std = torch.std(x_train[~torch.isnan(x_train[:, 0]), 0])\n",
        "    # Sx_std = torch.std(x_train[~torch.isnan(x_train[:, 1]), 1])\n",
        "    # Sy_std = torch.std(x_train[~torch.isnan(x_train[:, 2]), 2])\n",
        "    # Sz_std = torch.std(x_train[~torch.isnan(x_train[:, 3]), 3])\n",
        "    # tau_std = torch.std(x_train[~torch.isnan(x_train[:, 4]), 4])\n",
        "\n",
        "\n",
        "    # # Applying z-score normalization to both train and test sets using the statistics from the training set\n",
        "    # x_train[:, 0] = torch.sub(x_train[:, 0], D_median).div(D_std)\n",
        "    # x_train[:, 1] = torch.sub(x_train[:, 1], Sx_median).div(Sx_std)\n",
        "    # x_train[:, 2] = torch.sub(x_train[:, 2], Sy_median).div(Sy_std)\n",
        "    # x_train[:, 3] = torch.sub(x_train[:, 3], Sz_median).div(Sz_std)\n",
        "    # x_train[:, 4] = torch.sub(x_train[:, 4], tau_median).div(tau_std)\n",
        "\n",
        "    # x_test[:, 0] = torch.sub(x_test[:, 0], D_median).div(D_std)\n",
        "    # x_test[:, 1] = torch.sub(x_test[:, 1], Sx_median).div(Sx_std)\n",
        "    # x_test[:, 2] = torch.sub(x_test[:, 2], Sy_median).div(Sy_std)\n",
        "    # x_test[:, 3] = torch.sub(x_test[:, 3], Sz_median).div(Sz_std)\n",
        "    # x_test[:, 4] = torch.sub(x_test[:, 4], tau_median).div(tau_std)\n",
        "\n",
        "    # Computing the mean and standard deviation of each column\n",
        "    mean = x_train.mean(dim=0)\n",
        "    std = x_train.std(dim=0)\n",
        "\n",
        "    # Applying z-score normalization\n",
        "    x_train = (x_train - mean) / std\n",
        "    # Use the same mean and std from the training data as we don't want test data leakage.\n",
        "    x_test = (x_test - mean) / std\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "boC-8KmXvExN",
        "outputId": "47df6c92-5fa4-48cc-bff3-ead1a42965e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.1+cu118'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6J7rG-OevExN",
        "outputId": "514f29d9-c53b-47d8-9692-25966477f0bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.2253, 8.6760, 1.6644,  ..., 0.8806, 9.4032, 4.6567], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([80000, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.2253,  1.2439,  0.5428,  0.9811,  2.6203],\n",
              "        [ 8.6760, 26.3492,  1.3128, 22.2189, 32.8612],\n",
              "        [ 1.6644,  1.8987,  1.2941,  0.6595,  1.8787],\n",
              "        ...,\n",
              "        [ 0.8806,  0.3053,  0.6187,  1.1567,  1.7092],\n",
              "        [ 9.4032,  9.6825,  9.5324, 16.1767, 22.2331],\n",
              "        [ 4.6567,  2.9289,  0.3831,  2.7460,  8.2808]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0324e+01, 7.6037e+00, 4.1150e+00, 7.3877e+00, 1.5725e+01],\n",
              "        [6.1274e+00, 8.8058e+00, 1.2547e+01, 1.0761e+01, 1.9520e+01],\n",
              "        [2.8945e+00, 7.2054e-01, 2.6138e+00, 1.5800e+00, 2.6378e+00],\n",
              "        ...,\n",
              "        [1.0343e+00, 9.6165e-03, 8.6076e-02, 6.1373e-02, 7.7322e-02],\n",
              "        [8.6855e+00, 1.3404e+00, 1.4288e+01, 9.4060e+00, 1.4448e+01],\n",
              "        [1.1708e+01, 1.1230e+01, 2.6180e+01, 2.4258e+00, 2.8773e+01]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "x_train[:, 0]\n",
        "x_train.shape\n",
        "x_train\n",
        "x_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmdr204ZvExO"
      },
      "source": [
        "Plotting the histograms of the input data after normalization if z-score normalization was performed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "yE_qe_eLvExO"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"last_expr_or_assign\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bHw7bNaRvExO"
      },
      "outputs": [],
      "source": [
        "if ZSCORE_NORMALIZATION: \n",
        "    # Note how we are only plotting train and not test here.\n",
        "    # Plotting histograms of the input variables after z-score normalization\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.suptitle('Histograms of input variables after z-score normalization')\n",
        "    for i in range(5):\n",
        "        plt.subplot(3, 2, i+1)\n",
        "        plt.hist(x_train[:, i].cpu(), bins=50) # Must be convertedhere to cpu() for plotting.\n",
        "        plt.xlabel(f'Variable {i}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5_7RTKmuvExO"
      },
      "outputs": [],
      "source": [
        "if ZSCORE_NORMALIZATION:\n",
        "    # print('Summary statistics of input variables after z-score normalization')\n",
        "    # print(torch.stack([torch.min(x_train, dim=0).values,\n",
        "    #                 torch.max(x_train, dim=0).values,\n",
        "    #                 torch.mean(x_train, dim=0),\n",
        "    #                 torch.median(x_train, dim=0).values,\n",
        "    #                 torch.std(x_train, dim=0)], dim=1))\n",
        "    # Computing summary statistics of the input variables after z-score normalization\n",
        "    print('Summary statistics of input variables after z-score normalization')\n",
        "    print(torch.stack([torch.min(x_train, dim=0).values, torch.max(x_train, dim=0).values, torch.mean(x_train, dim=0), torch.median(x_train, dim=0).values, torch.std(x_train, dim=0)], dim=1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E96p_MsOUZh9",
        "outputId": "4b95bad0-8f3a-4364-eed1-008e0ce2a5e3"
      },
      "source": [
        "Checking if our output is always positive by plotting a histogram of y_train and y_test tensors "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vhyJwr4nvExO",
        "outputId": "b17fd799-2aaa-4e1c-d10c-c265677c9248",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxUAAAGGCAYAAAANcKzOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAtDUlEQVR4nO3de3RU5b3/8c9ASLg0hEIklxIIxxJsuCQKgQOVFiQVAwsFWoyiEiKibSctbaQVT5cEj+0BQbPwMkesS4jUVbmco9h1UrEYoSiihHDxkopAQwDJBUQICRIgs39/+GOaSBIysyeZ2bPfr7WylrPn2TvfzU7m6yfP7GcchmEYAgAAAAAfdQp0AQAAAACsjVABAAAAwBRCBQAAAABTCBUAAAAATCFUAAAAADCFUAEAAADAFEIFAAAAAFMIFQAAAABMCQt0AYHmdrt1/PhxRUZGyuFwBLocAAgIwzB09uxZxcfHq1Mn/t4k0R8AQGp7f7B9qDh+/LgSEhICXQYABIWjR4+qX79+gS4jKNAfAOBfrtYfbBsqXC6XXC6XLl26JOnrf6iePXsGuCoACIyamholJCQoMjIy0KUEjcv/FvQHAHbW1v7gMAzD6KCaglJNTY2ioqJ05swZmgYA2+K18Er8mwBA218LeeMsAAAAAFMIFQAAAABMIVQAAAAAMIVQAQAAAMAUQgUAAAAAUwgVAAAAAEwhVAAAAAAwhVABAAAAwBRCBQAAAABTCBUAAAAATCFUAADQiMvlUnJystLS0gJdCgBYBqECAIBGnE6nSktLVVxcHOhSAMAywgJdgJUlLiz0ed/DS6f4sRIAQLDxtUfQHwBYETMVAAAAAEwhVAAAAAAwhVABAAAAwBRCBQAAAABTCBUAAAAATCFUAAAAADCFUAEAAADAFEIFAAAAAFMIFQAAAABMIVQAAAAAMIVQAQAAAMAUQgUAAAAAUwgVAAAAAEwhVAAAAAAwhVABAAAAwBRCBQAAAABTCBUAAAAATCFUAAAAADCFUAEAAADAFEIFAAAAAFMIFQAAAABMCZlQce7cOQ0YMEALFiwIdCkAAACArYRMqPjDH/6gf//3fw90GQAAAIDthESoOHDggD799FNlZGQEuhQAAADAdgIeKrZt26apU6cqPj5eDodDGzduvGKMy+VSYmKiunbtqtGjR2vnzp1Nnl+wYIGWLFnSQRUDAAAAaCzgoaKurk4pKSlyuVzNPr9u3Trl5uYqLy9Pu3fvVkpKiiZNmqTq6mpJ0uuvv66kpCQlJSV1ZNkAAAAA/r+wQBeQkZHR6tuW8vPzNW/ePGVnZ0uSVq5cqcLCQq1atUoLFy7U+++/r7Vr12rDhg2qra3VxYsX1bNnTy1atKjZ49XX16u+vt7zuKamxr8n1EaJCwt93vfw0il+rAQAAAAwJ+AzFa25cOGCSkpKlJ6e7tnWqVMnpaena8eOHZKkJUuW6OjRozp8+LCeeOIJzZs3r8VAcXl8VFSU5yshIaHdzwMAAAAIZUEdKk6ePKmGhgbFxMQ02R4TE6PKykqfjvnwww/rzJkznq+jR4/6o1QAAADAtgL+9id/mjNnzlXHREREKCIiov2LAQAAAGwiqENFdHS0OnfurKqqqibbq6qqFBsbG6CqAABoP9xzB8CKgvrtT+Hh4RoxYoSKioo829xut4qKijRmzJgAVgYAAADgsoDPVNTW1urgwYOex2VlZdq7d6969+6t/v37Kzc3V1lZWRo5cqRGjRqlFStWqK6uzrMaFAAAAIDACnio2LVrlyZMmOB5nJubK0nKyspSQUGBMjMzdeLECS1atEiVlZVKTU3Vpk2brrh5GwAAAEBgBDxUjB8/XoZhtDomJydHOTk5fv2+LpdLLpdLDQ0Nfj0uAAAAYDdBfU9Fe3I6nSotLVVxcXGgSwEAAAAszbahAgAAAIB/ECoAAAAAmBLweyrgPV/XMGf9cgAAALQHZioAAAAAmEKoAACEpNOnT2vkyJFKTU3V0KFD9cILLwS6JAAIWbZ9+xNLygJAaIuMjNS2bdvUvXt31dXVaejQoZoxY4b69OkT6NIAIOTYdqaCJWUBILR17txZ3bt3lyTV19fLMIyrfi4SAMA3tp2pAAAEt23btmn58uUqKSlRRUWFXnvtNU2bNq3JGJfLpeXLl6uyslIpKSl65plnNGrUKM/zp0+f1g9/+EMdOHBAy5cvV3R0dAefRcfydSEPicU8AJhj25kKAEBwq6urU0pKilwuV7PPr1u3Trm5ucrLy9Pu3buVkpKiSZMmqbq62jOmV69e2rdvn8rKyvTnP/9ZVVVVHVU+ANgKoQIAEJQyMjL0+9//XtOnT2/2+fz8fM2bN0/Z2dlKTk7WypUr1b17d61ateqKsTExMUpJSdE777zT3mUDgC0RKgAAlnPhwgWVlJQoPT3ds61Tp05KT0/Xjh07JElVVVU6e/asJOnMmTPatm2bBg8e3OIx6+vrVVNT0+QLANA2hAoAgOWcPHlSDQ0NiomJabI9JiZGlZWVkqTy8nKNGzdOKSkpGjdunH7xi19o2LBhLR5zyZIlioqK8nwlJCS06zkAQCjhRm0AQEgaNWqU9u7d2+bxDz/8sHJzcz2Pa2pqCBYA0EaECgCA5URHR6tz585X3HhdVVWl2NhYn44ZERGhiIgIf5QHALZj27c/uVwuJScnKy0tLdClAAC8FB4erhEjRqioqMizze12q6ioSGPGjAlgZQBgT7adqXA6nXI6naqpqVFUVFSgy+kQrF8OwEpqa2t18OBBz+OysjLt3btXvXv3Vv/+/ZWbm6usrCyNHDlSo0aN0ooVK1RXV6fs7OwAVg0A9mTbUAEACG67du3ShAkTPI8v3++QlZWlgoICZWZm6sSJE1q0aJEqKyuVmpqqTZs2XXHzNgCg/REqAABBafz48TIMo9UxOTk5ysnJ6aCKAAAtse09FQAANId77gDAe4QKAAAacTqdKi0tVXFxcaBLAQDL4O1PAADA58U8WMgDgMRMBQAAAACTmKlAm7AcLQAAAFpi25kKbsQDAAAA/MO2oYIb8QAAAAD/sG2oAAAAAOAfhAoAAAAAphAqAABohHvuAMB7hAoAABrhnjsA8B6hAgAAAIAphAoAAAAApvDhd2h3vn5wHh+aBwAAYA3MVAAAAAAwhVABAAAAwBTbhgqWDAQAAAD8w7ahgiUDAQAAAP+wbagAAKA5zGQDgPdY/QkAgEacTqecTqdqamoUFRUV6HKCnq8r/Ems8geEEmYqAAAAAJhCqAAAAABgCm9/QtBiSh0AAMAamKkAAAAAYAqhAgAAAIAphAoAAAAAphAqAAAAAJhCqAAAAABgCqs/AQCAgGCVPyB0MFMBAEAjLpdLycnJSktLC3QpAGAZtg0VNA0AQHOcTqdKS0tVXFwc6FIAwDJsGypoGgAAAIB/2DZUAAAAAPAPbtRGSOLmPwAAgI7DTAUAAAAAUwgVAAAAAEwhVAAAAAAwhVABAAAAwBRCBQAAAABTWP0J+AZfV45i1SgA6DhmVvnzFa/zQMuYqQAAAABgCqECAAAAgCmECgAAGnG5XEpOTlZaWlqgSwEAyyBUAADQiNPpVGlpqYqLiwNdCgBYBqECAAAAgCmECgAAAACmECoAAAAAmEKoAAAAAGCKbUMFq3sAAAAA/mHbUMHqHgAAAIB/hAW6ACBUJC4s9Hnfw0un+LESAEB74HUeaJltZyoAAAAA+AehAgAAAIAphAoAAAAAphAqAAAAAJhCqAAAAABgCqs/AUGAFUUAAICVMVMBAAAAwBRmKgAAaMTlcsnlcqmhoSHQpSCEMCONUMdMBQAAjTidTpWWlqq4uDjQpQCAZfgUKv75z3/6uw4AQIigRwCA/fgUKr773e9qwoQJevnll3X+/Hl/1wQAsDB6BADYj0+hYvfu3Ro+fLhyc3MVGxurBx54QDt37vR3bQAAC6JHAID9+BQqUlNT9dRTT+n48eNatWqVKioqdOONN2ro0KHKz8/XiRMn/F0nAMAi6BEAYD+mbtQOCwvTjBkztGHDBj3++OM6ePCgFixYoISEBM2ePVsVFRX+qhMAYDH0CACwD1OhYteuXfr5z3+uuLg45efna8GCBTp06JA2b96s48eP67bbbvNXnQAAi6FHAIB9+PQ5Ffn5+Vq9erX279+vyZMna82aNZo8ebI6dfo6owwcOFAFBQVKTEz0Z60AAAugRwCA/fgUKp577jnde++9mjNnjuLi4pod07dvX7344oumigMAWA89AgDsx6dQceDAgauOCQ8PV1ZWli+HBwBYGD0CAOzHp3sqVq9erQ0bNlyxfcOGDXrppZdMFwUAsC56BADYj08zFUuWLNHzzz9/xfa+ffvq/vvv569PQAdKXFjo036Hl07xcyXA1+gRgH/xOg8r8Gmm4siRIxo4cOAV2wcMGKAjR46YLgoAYF30CACwH59CRd++ffXhhx9esX3fvn3q06eP6aIAANZFjwAA+/EpVNx555365S9/qS1btqihoUENDQ16++23NX/+fN1xxx3+rrFduFwuJScnKy0tLdClAEBICYUeAQDwjk/3VDz22GM6fPiwJk6cqLCwrw/hdrs1e/Zs/dd//ZdfC2wvTqdTTqdTNTU1ioqKCnQ5ABAyQqFHAAC841OoCA8P17p16/TYY49p37596tatm4YNG6YBAwb4uz4AgMXQIwDAfnwKFZclJSUpKSnJX7UAAEIIPQIA7MOnUNHQ0KCCggIVFRWpurpabre7yfNvv/22X4oD0H58XaJQYplCtI4eAQD241OomD9/vgoKCjRlyhQNHTpUDofD33UBACyKHgEA9uNTqFi7dq3Wr1+vyZMn+7seAIDFWb1HuFwuuVwuNTQ0BLoUALAMn5aUDQ8P13e/+11/1wIACAFW7xFOp1OlpaUqLi4OdCkAYBk+hYoHH3xQTz31lAzD8Hc9AACLo0cAgP349Pand999V1u2bNEbb7yhIUOGqEuXLk2ef/XVV/1SHADAeugRAGA/PoWKXr16afr06f6uBQAQAugRAGA/PoWK1atX+7sOAECIoEcAwYGlw9GRfLqnQpIuXbqkt956S88//7zOnj0rSTp+/Lhqa2v9VhwAwJroEQBgLz7NVJSXl+uWW27RkSNHVF9frx/96EeKjIzU448/rvr6eq1cudLfdQIIIvz1C62hRwCA/fg0UzF//nyNHDlSX375pbp16+bZPn36dBUVFfmtOACA9dAjAMB+fJqpeOedd/Tee+8pPDy8yfbExER9/vnnfikMAGBN9AgAsB+fZircbneznzR67NgxRUZGmi4KAGBd9AgAsB+fQsXNN9+sFStWeB47HA7V1tYqLy9PkydP9ldtAAALokcAgP349PanJ598UpMmTVJycrLOnz+vWbNm6cCBA4qOjtYrr7zi7xoBABZCjwAA+/EpVPTr10/79u3T2rVr9eGHH6q2tlZz587VXXfd1eSmPACA/dAjAMB+fAoVkhQWFqa7777bn7UAAEIEPQIA7MWnULFmzZpWn589e7ZPxQAArI8eAVgfn0cEb/kUKubPn9/k8cWLF3Xu3DmFh4ere/fuNAwAsDF6BADYj0+rP3355ZdNvmpra7V//37deOON3IQHADZHjwAA+/H5nopvGjRokJYuXaq7775bn376qb8OCyDEMKVuT/QIAAhtPs1UtCQsLEzHjx/35yEBACGCHgEAocunmYq//OUvTR4bhqGKigo9++yz+v73v++XwgAA1kSPAAD78SlUTJs2rcljh8Oha665RjfddJOefPJJf9QFALAoegQA2I9PocLtdvu7DgBAiKBHAID9+O1GbQAAAMDXBTlYjMPafAoVubm5bR6bn5/vy7cAAFgUPQIA7MenULFnzx7t2bNHFy9e1ODBgyVJn332mTp37qwbbrjBM87hcPinSgCAZdAjAMB+fAoVU6dOVWRkpF566SV9+9vflvT1hx1lZ2dr3LhxevDBB/1aJADAOugRAGA/Pn1OxZNPPqklS5Z4moUkffvb39bvf/97VvYAAJujRwCA/fgUKmpqanTixIkrtp84cUJnz541XRQAwLroEQBgPz6FiunTpys7O1uvvvqqjh07pmPHjul///d/NXfuXM2YMcPfNQIALMTqPcLlcik5OVlpaWmBLgUALMOneypWrlypBQsWaNasWbp48eLXBwoL09y5c7V8+XK/FggAsBar9win0ymn06mamhpFRUUFuhwAsASfQkX37t313//931q+fLkOHTokSbr22mvVo0cPvxbXFqdPn1Z6erouXbqkS5cuaf78+Zo3b16H1wEA+Fow9QgAQMcw9eF3FRUVqqio0A9+8AN169ZNhmF0+BKBkZGR2rZtm7p37666ujoNHTpUM2bMUJ8+fTq0DgBAU8HQIwAAHcOneyq++OILTZw4UUlJSZo8ebIqKiokSXPnzu3wpQI7d+6s7t27S5Lq6+tlGIYMw+jQGgAA/xJMPQIA0DF8ChW//vWv1aVLFx05csTzP/SSlJmZqU2bNnl1rG3btmnq1KmKj4+Xw+HQxo0brxjjcrmUmJiorl27avTo0dq5c2eT50+fPq2UlBT169dPv/nNbxQdHe3LaQEA/MCfPQIAYA0+hYq//e1vevzxx9WvX78m2wcNGqTy8nKvjlVXV6eUlBS5XK5mn1+3bp1yc3OVl5en3bt3KyUlRZMmTVJ1dbVnTK9evbRv3z6VlZXpz3/+s6qqqrw/KQCAX/izRwAArMGneyrq6uqa/PXpslOnTikiIsKrY2VkZCgjI6PF5/Pz8zVv3jxlZ2dL+npVkcLCQq1atUoLFy5sMjYmJkYpKSl655139JOf/KTZ49XX16u+vt7zuKamxqt6AQRO4sJCn/Y7vHSKnytBa/zZIwAA1uDTTMW4ceO0Zs0az2OHwyG3261ly5ZpwoQJfivuwoULKikpUXp6umdbp06dlJ6erh07dkiSqqqqPB+mdObMGW3btk2DBw9u8ZhLlixRVFSU5yshIcFv9QIAOq5HAACCh08zFcuWLdPEiRO1a9cuXbhwQb/97W/1ySef6NSpU9q+fbvfijt58qQaGhoUExPTZHtMTIw+/fRTSVJ5ebnuv/9+zw3av/jFLzRs2LAWj/nwww8rNzfX87impoZgAQB+1FE9AgAQPHwKFUOHDtVnn32mZ599VpGRkaqtrdWMGTPkdDoVFxfn7xpbNWrUKO3du7fN4yMiIph+B4B2FEw9AgDQMbwOFRcvXtQtt9yilStX6ne/+1171OQRHR2tzp07X3HjdVVVlWJjY9v1ewMAvNeRPQIAEDy8vqeiS5cu+vDDD9ujliuEh4drxIgRKioq8mxzu90qKirSmDFjOqQGAEDbdWSPAAAED59u1L777rv14osv+qWA2tpa7d271/MWprKyMu3du1dHjhyRJOXm5uqFF17QSy+9pH/84x/62c9+prq6Os9qUACA4OLPHgEAsAaf7qm4dOmSVq1apbfeeksjRoxQjx49mjyfn5/f5mPt2rWryWogl2+izsrKUkFBgTIzM3XixAktWrRIlZWVSk1N1aZNm664eRsAEBz82SMA2Ievy4ZLLB0eDLwKFf/85z+VmJiojz/+WDfccIMk6bPPPmsyxuFweFXA+PHjZRhGq2NycnKUk5Pj1XGvxuVyyeVyqaGhwa/HBQC7ao8eAQCwBq9CxaBBg1RRUaEtW7ZIkjIzM/X0009bctbA6XTK6XSqpqZGUVFRgS4HACwvlHoEAMA7Xt1T8c0ZhTfeeEN1dXV+LQgAYE30CACwL59u1L7sam9bAgDYFz0CAOzDq1DhcDiueD8s748FAEj0CACwM6/uqTAMQ3PmzPF8IvX58+f105/+9IqVPV599VX/VQgAsAR6BADYl1ehIisrq8nju+++26/FAACsix4BAPblVahYvXp1e9XR4VhSFgD8K5R6BADAOz59+F0oYElZwD74QCUAANqXqdWfAAAAAIBQAQAAAMAUQgUAAAAAU2x7TwUAAABCA/fOBR4zFQAAAABMIVQAAAAAMMW2ocLlcik5OVlpaWmBLgUAAACwNNuGCqfTqdLSUhUXFwe6FAAAAMDSbBsqAAAAAPgHoQIAAACAKYQKAAAAAKYQKgAAAACYQqgAAAAAYAqhAgAAAIAphAoAAAAAphAqAAAAAJhi21DBJ2oDAAAA/mHbUMEnagMAAAD+YdtQAQAAAMA/wgJdAAAAABAoiQsLfdrv8NIpfq7E2pipAACEpKNHj2r8+PFKTk7W8OHDtWHDhkCXBAAhi5kKAGiFr3/BkvgrVqCFhYVpxYoVSk1NVWVlpUaMGKHJkyerR48egS4NAEIOoQIAEJLi4uIUFxcnSYqNjVV0dLROnTpFqACAdsDbnwAAQWnbtm2aOnWq4uPj5XA4tHHjxivGuFwuJSYmqmvXrho9erR27tzZ7LFKSkrU0NCghISEdq4aAOyJUAEACEp1dXVKSUmRy+Vq9vl169YpNzdXeXl52r17t1JSUjRp0iRVV1c3GXfq1CnNnj1bf/zjHzuibACwJd7+BAAIShkZGcrIyGjx+fz8fM2bN0/Z2dmSpJUrV6qwsFCrVq3SwoULJUn19fWaNm2aFi5cqLFjx7b6/err61VfX+95XFNT44ezAAB7YKYCAGA5Fy5cUElJidLT0z3bOnXqpPT0dO3YsUOSZBiG5syZo5tuukn33HPPVY+5ZMkSRUVFeb54qxQAtJ1tQ4XL5VJycrLS0tICXQoAwEsnT55UQ0ODYmJimmyPiYlRZWWlJGn79u1at26dNm7cqNTUVKWmpuqjjz5q8ZgPP/ywzpw54/k6evRou54DAIQS2779yel0yul0qqamRlFRUYEuBwDgZzfeeKPcbnebx0dERCgiIqIdKwKA0GXbmQoAgHVFR0erc+fOqqqqarK9qqpKsbGxAaoKAOyLUAEAsJzw8HCNGDFCRUVFnm1ut1tFRUUaM2ZMACsDAHuy7dufAADBrba2VgcPHvQ8Lisr0969e9W7d2/1799fubm5ysrK0siRIzVq1CitWLFCdXV1ntWgAAAdh1ABAAhKu3bt0oQJEzyPc3NzJUlZWVkqKChQZmamTpw4oUWLFqmyslKpqanatGnTFTdvAwDaH6ECABCUxo8fL8MwWh2Tk5OjnJwcv35fl8sll8ulhoYGvx4XAEIZ91QAANCI0+lUaWmpiouLA10KAFgGoQIAAACAKYQKAAAAAKYQKgAAAACYQqgAAAAAYAqhAgAAAIAphAoAAAAAptg2VLhcLiUnJystLS3QpQAAAACWZttQwTrkAIDm8EcnAPCebUMFAADN4Y9OAOA9QgUAAAAAU8ICXQAAAABgNYkLC33e9/DSKX6sJDgwUwEAAADAFEIFAAAAAFMIFQAAAABMIVQAAAAAMIVQAQAAAMAUVn8CgHbi68ogobgqiJW4XC65XC41NDQEuhQAsAxmKgAAaIQPvwMA7xEqAAAAAJhCqAAAAABgCqECAAAAgCmECgAAAACmECoAAAAAmEKoAAAAAGCKbUOFy+VScnKy0tLSAl0KAAAAYGm2DRWsQw4AAAD4h21DBQAAAAD/IFQAANAIb48FAO8RKgAAaIS3xwKA9wgVAAAAAEwhVAAAAAAwhVABAAAAwBRCBQAAAABTCBUAAAAATAkLdAEAAACAnSQuLPR538NLp/ixEv9hpgIAAACAKYQKAAAAAKYQKgAAAACYQqgAAAAAYAqhAgAAAIAphAoAABpxuVxKTk5WWlpaoEsBAMsgVAAA0IjT6VRpaamKi4sDXQoAWAahAgAAAIAphAoAAAAAphAqAAAAAJhCqAAAAABgCqECAAAAgCmECgAAAACmECoAAAAAmGLbUMGHGwEAAAD+YdtQwYcbAQAAAP5h21ABAAAAwD8IFQAAAABMIVQAAAAAMIVQAQAAAMAUQgUAAAAAUwgVAAA0wpLjAOA9QgUAAI2w5DgAeI9QAQAAAMAUQgUAAAAAUwgVAAAAAEwhVAAAAAAwhVABAAAAwBRCBQAAAABTCBUAAAAATCFUAAAAADCFUAEAAADAFEIFAAAAAFMIFQAAAABMIVQAAAAAMIVQAQAAAMAUQgUAAAAAUwgVAAAAAEwhVAAAAAAwhVABAAAAwJSwQBcAAGgqcWGhz/seXjrFj5UAAIKNrz2ivfsDMxUAAAAATCFUAADQiMvlUnJystLS0gJdCgBYBqECAIBGnE6nSktLVVxcHOhSAMAyCBUAAAAATCFUAAAAADCFUAEAAADAFEIFAAAAAFMIFQAAAABMIVQAAAAAMIVQAQAAAMAUQgUAAAAAUwgVAAAAAEwhVAAAAAAwJSzQBQSaYRiSpJqaGq/3ddef83c5AGCKL69ljfe7/JoIc/1BokcACC7t3R9sHyrOnj0rSUpISAhwJQBgXtQKc/ufPXtWUVFRfqnF6ugPAEJJe/cHh2HzP0u53W4dP35ckZGRcjgcbd6vpqZGCQkJOnr0qHr27NmOFXa8UD03zstaQvW8pOA8N8MwdPbsWcXHx6tTJ94ZK/neH6TgvMb+wrlZE+dmTcFwbm3tD7afqejUqZP69evn8/49e/YMuR/gy0L13DgvawnV85KC79yYoWjKbH+Qgu8a+xPnZk2cmzUF+tza0h/4cxQAAAAAUwgVAAAAAEwhVPgoIiJCeXl5ioiICHQpfheq58Z5WUuonpcU2ueGr4XyNebcrIlzsyYrnZvtb9QGAAAAYA4zFQAAAABMIVQAAAAAMIVQAQAAAMAUQkUrXC6XEhMT1bVrV40ePVo7d+5sdfyGDRt03XXXqWvXrho2bJj++te/dlClbbdkyRKlpaUpMjJSffv21bRp07R///5W9ykoKJDD4Wjy1bVr1w6quG0WL158RY3XXXddq/tY4XolJiZecV4Oh0NOp7PZ8cF8rbZt26apU6cqPj5eDodDGzdubPK8YRhatGiR4uLi1K1bN6Wnp+vAgQNXPa63v6f+1tp5Xbx4UQ899JCGDRumHj16KD4+XrNnz9bx48dbPaYvP8/oePSIrwXz605jodonJHqFFXqFFPr9glDRgnXr1ik3N1d5eXnavXu3UlJSNGnSJFVXVzc7/r333tOdd96puXPnas+ePZo2bZqmTZumjz/+uIMrb93f//53OZ1Ovf/++9q8ebMuXryom2++WXV1da3u17NnT1VUVHi+ysvLO6jithsyZEiTGt99990Wx1rlehUXFzc5p82bN0uSZs6c2eI+wXqt6urqlJKSIpfL1ezzy5Yt09NPP62VK1fqgw8+UI8ePTRp0iSdP3++xWN6+3vaHlo7r3Pnzmn37t165JFHtHv3br366qvav3+/br311qse15ufZ3Q8ekRTwfq6802h2CckeoUVeoVkg35hoFmjRo0ynE6n53FDQ4MRHx9vLFmypNnxt99+uzFlypQm20aPHm088MAD7VqnWdXV1YYk4+9//3uLY1avXm1ERUV1XFE+yMvLM1JSUto83qrXa/78+ca1115ruN3uZp+3wrUyDMOQZLz22muex26324iNjTWWL1/u2Xb69GkjIiLCeOWVV1o8jre/p+3tm+fVnJ07dxqSjPLy8hbHePvzjI5Hj/gXq7zu2KVPGAa94puCrVcYRmj2C2YqmnHhwgWVlJQoPT3ds61Tp05KT0/Xjh07mt1nx44dTcZL0qRJk1ocHyzOnDkjSerdu3er42prazVgwAAlJCTotttu0yeffNIR5XnlwIEDio+P17/927/prrvu0pEjR1oca8XrdeHCBb388su699575XA4WhxnhWv1TWVlZaqsrGxyTaKiojR69OgWr4kvv6fB4MyZM3I4HOrVq1er47z5eUbHokdcySqvO6HeJyR6xTdZtVdI1usXhIpmnDx5Ug0NDYqJiWmyPSYmRpWVlc3uU1lZ6dX4YOB2u/WrX/1K3//+9zV06NAWxw0ePFirVq3S66+/rpdffllut1tjx47VsWPHOrDa1o0ePVoFBQXatGmTnnvuOZWVlWncuHE6e/Zss+OteL02btyo06dPa86cOS2OscK1as7lf3dvrokvv6eBdv78eT300EO688471bNnzxbHefvzjI5Fj2jKKq87dugTEr3im6zYKyRr9ouwDv+OCBpOp1Mff/zxVd97N2bMGI0ZM8bzeOzYsfre976n559/Xo899lh7l9kmGRkZnv8ePny4Ro8erQEDBmj9+vWaO3duACvznxdffFEZGRmKj49vcYwVrpVdXbx4UbfffrsMw9Bzzz3X6lg7/Dwj+IVSj5Ds83tFr7A+q/YLZiqaER0drc6dO6uqqqrJ9qqqKsXGxja7T2xsrFfjAy0nJ0f/93//py1btqhfv35e7dulSxddf/31OnjwYDtVZ16vXr2UlJTUYo1Wu17l5eV66623dN9993m1nxWulSTPv7s318SX39NAudwgysvLtXnz5lb/6tScq/08o2PRI1pnldedUOsTEr2iOVbqFZK1+wWhohnh4eEaMWKEioqKPNvcbreKioqaJPvGxowZ02S8JG3evLnF8YFiGIZycnL02muv6e2339bAgQO9PkZDQ4M++ugjxcXFtUOF/lFbW6tDhw61WKNVrtdlq1evVt++fTVlyhSv9rPCtZKkgQMHKjY2tsk1qamp0QcffNDiNfHl9zQQLjeIAwcO6K233lKfPn28PsbVfp7RsegRrbPK606o9QmJXtEcq/QKKQT6RWDvEw9ea9euNSIiIoyCggKjtLTUuP/++41evXoZlZWVhmEYxj333GMsXLjQM3779u1GWFiY8cQTTxj/+Mc/jLy8PKNLly7GRx99FKhTaNbPfvYzIyoqyti6datRUVHh+Tp37pxnzDfP7dFHHzXefPNN49ChQ0ZJSYlxxx13GF27djU++eSTQJxCsx588EFj69atRllZmbF9+3YjPT3diI6ONqqrqw3DsO71MoyvV6no37+/8dBDD13xnJWu1dmzZ409e/YYe/bsMSQZ+fn5xp49ezyrWixdutTo1auX8frrrxsffvihcdtttxkDBw40vvrqK88xbrrpJuOZZ57xPL7a72mgz+vChQvGrbfeavTr18/Yu3dvk9+5+vr6Fs/raj/PCDx6hDVedxoL5T5hGPSKYO8VVzu3UOgXhIpWPPPMM0b//v2N8PBwY9SoUcb777/vee6HP/yhkZWV1WT8+vXrjaSkJCM8PNwYMmSIUVhY2MEVX52kZr9Wr17tGfPNc/vVr37l+XeIiYkxJk+ebOzevbvji29FZmamERcXZ4SHhxvf+c53jMzMTOPgwYOe5616vQzDMN58801DkrF///4rnrPStdqyZUuzP3uX63e73cYjjzxixMTEGBEREcbEiROvOOcBAwYYeXl5Tba19nvaEVo7r7KyshZ/57Zs2dLieV3t5xnBgR7xtWB+3WkslPuEYdArGgvGXmEYod8vHIZhGP6e/QAAAABgH9xTAQAAAMAUQgUAAAAAUwgVAAAAAEwhVAAAAAAwhVABAAAAwBRCBQAAAABTCBUAAAAATCFUAAAAADCFUAFY0Jw5czRt2rRAlwEAACCJUAF0iMWLFys1NdVvx3vqqadUUFDgt+MBAALP371CkgoKCtSrVy+/HhNoTligCwDwLxcvXlSXLl2uOi4qKqoDqgEAAGgbZiqANlqzZo369Omj+vr6JtunTZume+65p8X9CgoK9Oijj2rfvn1yOBxyOByeWQaHw6HnnntOt956q3r06KE//OEPamho0Ny5czVw4EB169ZNgwcP1lNPPdXkmN98+9P48eP1y1/+Ur/97W/Vu3dvxcbGavHixf46dQBAG7VHrzh9+rTuu+8+XXPNNerZs6duuukm7du3z7Pvvn37NGHCBEVGRqpnz54aMWKEdu3apa1btyo7O1tnzpzxHJPegPZCqADaaObMmWpoaNBf/vIXz7bq6moVFhbq3nvvbXG/zMxMPfjggxoyZIgqKipUUVGhzMxMz/OLFy/W9OnT9dFHH+nee++V2+1Wv379tGHDBpWWlmrRokX6j//4D61fv77V+l566SX16NFDH3zwgZYtW6b//M//1ObNm82fOACgzdqjV8ycOVPV1dV64403VFJSohtuuEETJ07UqVOnJEl33XWX+vXrp+LiYpWUlGjhwoXq0qWLxo4dqxUrVqhnz56eYy5YsKB9/wFgW7z9CWijbt26adasWVq9erVmzpwpSXr55ZfVv39/jR8/vtX9vvWtbyksLEyxsbFXPD9r1ixlZ2c32fboo496/nvgwIHasWOH1q9fr9tvv73F7zN8+HDl5eVJkgYNGqRnn31WRUVF+tGPfuTNaQIATPB3r3j33Xe1c+dOVVdXKyIiQpL0xBNPaOPGjfqf//kf3X///Tpy5Ih+85vf6LrrrpP0dQ+4LCoqSg6Ho9n+A/gToQLwwrx585SWlqbPP/9c3/nOd1RQUKA5c+bI4XD4fMyRI0desc3lcmnVqlU6cuSIvvrqK124cOGqN+8NHz68yeO4uDhVV1f7XBcAwDf+7BX79u1TbW2t+vTp02T7V199pUOHDkmScnNzdd999+lPf/qT0tPTNXPmTF177bV+ORegrQgVgBeuv/56paSkaM2aNbr55pv1ySefqLCw0NQxe/To0eTx2rVrtWDBAj355JMaM2aMIiMjtXz5cn3wwQetHuebN3g7HA653W5TtQEAvOfPXlFbW6u4uDht3br1iucur+q0ePFizZo1S4WFhXrjjTeUl5entWvXavr06SbOAvAOoQLw0n333acVK1bo888/V3p6uhISEq66T3h4uBoaGtp0/O3bt2vs2LH6+c9/7tl2+a9RAABr8FevuOGGG1RZWamwsDAlJia2uG9SUpKSkpL061//WnfeeadWr16t6dOne9V/ADO4URvw0qxZs3Ts2DG98MILrd5011hiYqLKysq0d+9enTx58opVQRobNGiQdu3apTfffFOfffaZHnnkERUXF/urfABAB/BXr0hPT9eYMWM0bdo0/e1vf9Phw4f13nvv6Xe/+5127dqlr776Sjk5Odq6davKy8u1fft2FRcX63vf+57nmLW1tSoqKtLJkyd17ty59jxt2BihAvBSVFSUfvzjH+tb3/pWmz/V+sc//rFuueUWTZgwQddcc41eeeWVFsc+8MADmjFjhjIzMzV69Gh98cUXTWYtAADBz1+9wuFw6K9//at+8IMfKDs7W0lJSbrjjjtUXl6umJgYde7cWV988YVmz56tpKQk3X777crIyPAs+DF27Fj99Kc/VWZmpq655hotW7asHc8aduYwDMMIdBGA1UycOFFDhgzR008/HehSAABBil4BOyFUAF748ssvtXXrVv3kJz9RaWmpBg8eHOiSAABBhl4BO+JGbcAL119/vb788ks9/vjjTZrEkCFDVF5e3uw+zz//vO66666OKhEAEGD0CtgRMxWAH5SXl+vixYvNPhcTE6PIyMgOrggAEGzoFQhlhAoAAAAAprD6EwAAAABTCBUAAAAATCFUAAAAADCFUAEAAADAFEIFAAAAAFMIFQAAAABMIVQAAAAAMIVQAQAAAMCU/wcD15fZBa3qWgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(y_train.cpu().numpy(), bins=20) # must be cpu here.\n",
        "plt.xlabel(\"y_train\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.yscale(\"log\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(y_test.cpu().numpy(), bins=20) # must be cpu here\n",
        "plt.xlabel(\"y_test\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.yscale(\"log\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "FEgjk--AUZh9"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2b9GecHUZh9"
      },
      "source": [
        "## Defining the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Iv8HA-ZXUZh-"
      },
      "outputs": [],
      "source": [
        "# Defining a class for the network\n",
        "class Net(nn.Module):\n",
        "    \"\"\"A class for creating a network with a\n",
        "    variable number of hidden layers and units.\n",
        "\n",
        "    Attributes:\n",
        "        n_layers (int): The number of hidden layers in the network.\n",
        "        n_units (list): A list of integers representing the number of units in each hidden layer.\n",
        "        hidden_activation (torch.nn.Module): The activation function for the hidden layers.\n",
        "        output_activation (torch.nn.Module): The activation function for the output layer.\n",
        "        layers (torch.nn.ModuleList): A list of linear layers in the network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_layers, n_units, hidden_activation, output_activation):\n",
        "        \"\"\"Initializes the network with the given hyperparameters.\n",
        "\n",
        "        Args:\n",
        "            n_layers (int): The number of hidden layers in the network.\n",
        "            n_units (list): A list of integers representing the number of units in each hidden layer.\n",
        "            hidden_activation (torch.nn.Module): The activation function for the hidden layers.\n",
        "            output_activation (torch.nn.Module): The activation function for the output layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.n_units = n_units\n",
        "        self.hidden_activation = hidden_activation\n",
        "        self.output_activation = output_activation\n",
        "\n",
        "        # Creating a list of linear layers with different numbers of units for each layer\n",
        "        self.layers = nn.ModuleList([nn.Linear(5, n_units[0])])\n",
        "        for i in range(1, n_layers):\n",
        "            self.layers.append(nn.Linear(n_units[i - 1], n_units[i]))\n",
        "        self.layers.append(nn.Linear(n_units[-1], 1))\n",
        "\n",
        "        # Adding some assertions to check that the input arguments are valid\n",
        "        assert isinstance(n_layers, int) and n_layers > 0, \"n_layers must be a positive integer\"\n",
        "        assert isinstance(n_units, list) and len(n_units) == n_layers, \"n_units must be a list of length n_layers\"\n",
        "        assert all(isinstance(n, int) and n > 0 for n in n_units), \"n_units must contain positive integers\"\n",
        "        assert isinstance(hidden_activation, nn.Module), \"hidden_activation must be a torch.nn.Module\"\n",
        "        assert isinstance(output_activation, nn.Module), \"output_activation must be a torch.nn.Module\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Performs a forward pass on the input tensor.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input tensor of shape (batch_size, 5).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The output tensor of shape (batch_size, 1).\n",
        "        \"\"\"\n",
        "        # Adding an assertion to check that the input tensor has the expected shape and type\n",
        "        assert isinstance(x, torch.Tensor), \"x must be a torch.Tensor\"\n",
        "        assert x.shape[1] == 5, \"x must have shape (batch_size, 5)\"\n",
        "\n",
        "        # Looping over the hidden layers and applying the linear transformation and the activation function\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = self.hidden_activation(layer(x))\n",
        "        # Applying the linear transformation and the activation function on the output layer\n",
        "        x = self.output_activation(self.layers[-1](x))\n",
        "\n",
        "        # Returning the output tensor\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GNvp55PUZh_"
      },
      "source": [
        "## Defining the model and search space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "9a1opluOUZh_"
      },
      "outputs": [],
      "source": [
        "# Defining a function to create a trial network and optimizer\n",
        "def create_model(trial, optimize):\n",
        "    \"\"\"Creates a trial network and optimizer based on the sampled hyperparameters.\n",
        "\n",
        "    Args:\n",
        "        trial (optuna.trial.Trial): The trial object that contains the hyperparameters.\n",
        "        optimize (boolean): Whether to optimize the hyperparameters or to use predefined values.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple of (net, loss_fn, optimizer, batch_size, n_epochs,\n",
        "            scheduler, loss_name, optimizer_name, scheduler_name,\n",
        "            n_units, n_layers, hidden_activation, output_activation),\n",
        "            where net is the trial network,\n",
        "            loss_fn is the loss function,\n",
        "            optimizer is the optimizer,\n",
        "            batch_size is the batch size,\n",
        "            n_epochs is the number of epochs,\n",
        "            scheduler is the learning rate scheduler,\n",
        "            loss_name is the name of the loss function,\n",
        "            optimizer_name is the name of the optimizer,\n",
        "            scheduler_name is the name of the scheduler,\n",
        "            n_units is a list of integers representing\n",
        "            the number of units in each hidden layer,\n",
        "            n_layers is an integer representing the number of hidden layers in the network,\n",
        "            hidden_activation is a torch.nn.Module representing the activation function for the hidden layers,\n",
        "            output_activation is a torch.nn.Module representing the activation function for the output layer,\n",
        "            lr is the (initial) learning rate.\n",
        "    \"\"\"\n",
        "    # If optimize is True, sample the hyperparameters from the search space\n",
        "    if optimize:\n",
        "        # Sampling the hyperparameters from the search space\n",
        "        n_layers = trial.suggest_int(\"n_layers\", 2, 6)\n",
        "        n_units = [trial.suggest_int(f\"n_units_{i}\", 16, 2048) for i in range(n_layers)] \n",
        "        hidden_activation_name = trial.suggest_categorical(\n",
        "            #\"hidden_activation\", [\"ReLU\", \"LeakyReLU\", \"ELU\", \"Tanh\", \"Sigmoid\"]\n",
        "            #\"hidden_activation\", [\"ReLU\", \"LeakyReLU\"]\n",
        "            \"hidden_activation\", [\"ReLU\", \"LeakyReLU\", \"ELU\"]\n",
        "        )\n",
        "        output_activation_name = trial.suggest_categorical(\n",
        "            #\"output_activation\", [\"Linear\", \"ReLU\", \"Softplus\"]\n",
        "            # Assuming pressure cannot be negative, linear output activation is not an option.\n",
        "            #\"output_activation\", [\"ReLU\", \"Softplus\", \"Linear\"]\n",
        "            \"output_activation\", [\"ReLU\", \"Linear\"]\n",
        "        ) \n",
        "        loss_name = trial.suggest_categorical(\n",
        "            #\"loss\", [\"MSE\", \"MAE\", \"Huber\", \"LogCosh\"] \n",
        "            \"loss\", [\"MSE\", \"MAE\", \"Huber\"] \n",
        "        )\n",
        "        optimizer_name = trial.suggest_categorical(\n",
        "            \"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\", \"Adagrad\"] \n",
        "        )\n",
        "        lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2) \n",
        "\n",
        "        batch_size_list = [32, 48, 64, 96, 128, 256, 512, 1048]\n",
        "        batch_size = trial.suggest_categorical(\"batch_size\", batch_size_list)\n",
        "        #batch_size = trial.suggest_int(\"batch_size\", 64, 1048)\n",
        "        n_epochs = trial.suggest_int(\"n_epochs\", 100, 300) \n",
        "        scheduler_name = trial.suggest_categorical(\n",
        "            \"scheduler\",\n",
        "            # [\"None\", \"CosineAnnealingLR\", \"ReduceLROnPlateau\", \"StepLR\", \"ExponentialLR\"],\n",
        "            [\"CosineAnnealingLR\", \"ReduceLROnPlateau\", \"StepLR\"],\n",
        "        )\n",
        "\n",
        "    # If optimize is False, use the predefined values\n",
        "    else:\n",
        "        # Setting the hyperparameters to the predefined values\n",
        "        n_layers = N_LAYERS_NO_OPT\n",
        "        n_units = N_UNITS_NO_OPT\n",
        "        hidden_activation_name = HIDDEN_ACTIVATION_NAME_NO_OPT\n",
        "        output_activation_name = OUTPUT_ACTIVATION_NAME_NO_OPT\n",
        "        loss_name = LOSS_NAME_NO_OPT\n",
        "        optimizer_name = OPTIMIZER_NAME_NO_OPT\n",
        "        lr = LR_NO_OPT\n",
        "        batch_size = BATCH_SIZE_NO_OPT\n",
        "        n_epochs = N_EPOCHS_NO_OPT\n",
        "        scheduler_name = SCHEDULER_NAME_NO_OPT\n",
        "\n",
        "\n",
        "    # Creating the activation functions from their names\n",
        "    if hidden_activation_name == \"ReLU\":\n",
        "        hidden_activation = nn.ReLU()\n",
        "    elif hidden_activation_name == \"LeakyReLU\":\n",
        "        hidden_activation = nn.LeakyReLU() \n",
        "    elif hidden_activation_name == \"ELU\":\n",
        "        hidden_activation = nn.ELU() \n",
        "    elif hidden_activation_name == \"Tanh\":\n",
        "        hidden_activation = nn.Tanh()\n",
        "    else:\n",
        "        hidden_activation = nn.Sigmoid()\n",
        "\n",
        "    if output_activation_name == \"ReLU\":\n",
        "        output_activation = nn.ReLU()\n",
        "    elif output_activation_name == \"Softplus\":\n",
        "        output_activation = nn.Softplus()\n",
        "    else:\n",
        "        output_activation = nn.Identity()\n",
        "\n",
        "    # Creating the loss function from its name\n",
        "    if loss_name == \"MSE\":\n",
        "        loss_fn = nn.MSELoss()\n",
        "    elif loss_name == \"MAE\":\n",
        "        loss_fn = nn.L1Loss()\n",
        "    elif loss_name == \"Huber\":\n",
        "        loss_fn = nn.SmoothL1Loss() \n",
        "    else:\n",
        "        # Creating the log-cosh loss function\n",
        "        def log_cosh_loss(y_pred, y_true):\n",
        "            return torch.mean(torch.log(torch.cosh(y_pred - y_true)))\n",
        "            \n",
        "        loss_fn = log_cosh_loss\n",
        "\n",
        "    # Creating the network with the sampled hyperparameters\n",
        "    net = Net(\n",
        "        n_layers, n_units, hidden_activation, output_activation\n",
        "    ).to(device)\n",
        "\n",
        "    if optimize:\n",
        "        # Creating the optimizer from its name\n",
        "        if optimizer_name == \"SGD\":\n",
        "            # Added sampling the weight decay and momentum for SGD\n",
        "            weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
        "            momentum = trial.suggest_uniform(\"momentum\", 0.0, 0.99)\n",
        "            optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
        "        elif optimizer_name == \"Adam\":\n",
        "            # Added sampling the weight decay and beta parameters for Adam\n",
        "            weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
        "            beta1 = trial.suggest_uniform(\"beta1\", 0.9, 0.999)\n",
        "            beta2 = trial.suggest_uniform(\"beta2\", 0.999, 0.9999)\n",
        "            optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay, betas=(beta1, beta2))\n",
        "        elif optimizer_name == \"RMSprop\":\n",
        "            optimizer = optim.RMSprop(net.parameters(), lr=lr)\n",
        "        else:\n",
        "            # Added creating the Adagrad optimizer\n",
        "            optimizer = optim.Adagrad(net.parameters(), lr=lr)\n",
        "\n",
        "        # Creating the learning rate scheduler from its name\n",
        "        if scheduler_name == \"StepLR\":\n",
        "            step_size = trial.suggest_int(\"step_size\", 5, 15)\n",
        "            gamma = trial.suggest_uniform(\"gamma\", 0.1, 0.5)\n",
        "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "        elif scheduler_name == \"ExponentialLR\":\n",
        "            gamma = trial.suggest_uniform(\"gamma\", 0.8, 0.99)\n",
        "            scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
        "        elif scheduler_name == \"CosineAnnealingLR\":\n",
        "            if n_epochs < 150:\n",
        "                t_max_fraction = trial.suggest_uniform('t_max_fraction', 0.1, 0.3)\n",
        "            elif n_epochs > 250:\n",
        "                t_max_fraction = trial.suggest_uniform('t_max_fraction', 0.05, 0.1)\n",
        "            else:\n",
        "                t_max_fraction = trial.suggest_uniform('t_max_fraction', 0.1, 0.2)\n",
        "\n",
        "            T_max = int(n_epochs * t_max_fraction)\n",
        "            eta_min = trial.suggest_loguniform(\"eta_min\", 1e-7, 1e-2)\n",
        "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)\n",
        "        elif scheduler_name == \"ReduceLROnPlateau\":\n",
        "            # Added sampling the factor, patience and threshold for ReduceLROnPlateau\n",
        "            factor = trial.suggest_uniform(\"factor\", 0.1, 0.5)\n",
        "            patience = trial.suggest_int(\"patience\", 5, 10)\n",
        "            threshold = trial.suggest_loguniform(\"threshold\", 1e-4, 1e-2)\n",
        "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                optimizer, mode=\"min\", factor=factor, patience=patience, threshold=threshold\n",
        "            )\n",
        "        # # Added using OneCycleLR scheduler as an option\n",
        "        # elif scheduler_name == \"OneCycleLR\":\n",
        "        #         # Added sampling the max_lr and pct_start for OneCycleLR\n",
        "        #         max_lr = trial.suggest_loguniform(\"max_lr\", lr, 10 * lr) \n",
        "        #         pct_start = trial.suggest_uniform(\"pct_start\", 0.1, 0.9)\n",
        "        #         scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "        #             optimizer,\n",
        "        #             max_lr=max_lr,\n",
        "        #             epochs=n_epochs,\n",
        "        #             steps_per_epoch=len(train_loader),\n",
        "        #             pct_start=pct_start,\n",
        "        #         )\n",
        "        else:\n",
        "            scheduler = None\n",
        "    else:\n",
        "        # Creating the optimizer from its name\n",
        "        if optimizer_name == \"SGD\":\n",
        "            optimizer = optim.SGD(net.parameters(), lr=lr)\n",
        "        elif optimizer_name == \"Adam\":\n",
        "            optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "        elif optimizer_name == \"RMSprop\":\n",
        "            optimizer = optim.RMSprop(net.parameters(), lr=lr)\n",
        "        else:\n",
        "            optimizer = optim.Adagrad(net.parameters(), lr=lr)\n",
        "\n",
        "        # Creating the learning rate scheduler from its name\n",
        "        if scheduler_name == \"StepLR\":\n",
        "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "        elif scheduler_name == \"ExponentialLR\":\n",
        "            scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "        elif scheduler_name == \"CosineAnnealingLR\":\n",
        "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer)\n",
        "        elif scheduler_name == \"ReduceLROnPlateau\":\n",
        "            # Creating the ReduceLROnPlateau scheduler with a threshold value of 0.01\n",
        "            #scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            #    optimizer, mode=\"min\", factor=0.1, patience=10, threshold=0.01\n",
        "            #)\n",
        "            # Use Dieseldorst et al. settings and add to that a minimum lr.\n",
        "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                        optimizer, mode=\"min\", factor=0.18979341786654758, patience=11, threshold=0.0017197466122611932 #, min_lr=1e-6\n",
        "                    )\n",
        "        else:\n",
        "            scheduler = None\n",
        "\n",
        "    # Returning all variables needed for saving and loading\n",
        "    return net, loss_fn, optimizer, batch_size, n_epochs, scheduler, loss_name, optimizer_name, scheduler_name, n_units, n_layers, hidden_activation, output_activation, lr\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-czA7VvUZiD"
      },
      "source": [
        " ## The training and evaluation loop\n",
        "\n",
        " We first define a couple of functions used in the training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "aD6FQNmxUZiD"
      },
      "outputs": [],
      "source": [
        "# Defining a function that computes loss and metrics for a given batch\n",
        "def compute_loss_and_metrics(y_pred, y_true, loss_fn):\n",
        "    \"\"\"Computes loss and metrics for a given batch.\n",
        "\n",
        "    Args:\n",
        "        y_pred (torch.Tensor): The predicted pressure tensor of shape (batch_size, 1).\n",
        "        y_true (torch.Tensor): The true pressure tensor of shape (batch_size,).\n",
        "        loss_fn (torch.nn.Module or function): The loss function to use.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple of (loss, l1_norm), where loss is a scalar tensor,\n",
        "            l1_norm is L1 norm for relative error of pressure,\n",
        "            each being a scalar tensor.\n",
        "            linf_norm is Linf norm for relative error of pressure.\n",
        "    \"\"\"\n",
        "    # Reshaping the target tensor to match the input tensor\n",
        "    y_true = y_true.view(-1, 1)\n",
        "\n",
        "    # Computing the loss using the loss function\n",
        "    loss = loss_fn(y_pred, y_true)\n",
        "\n",
        "    # Computing the relative error of pressure\n",
        "    rel_error = torch.abs((y_pred - y_true) / y_true)\n",
        "\n",
        "    # Computing the L1 norm for the relative error of pressure\n",
        "    l1_norm = torch.mean(rel_error) \n",
        "    # Computing the Linf norm for the relative error of pressure\n",
        "    linf_norm = torch.max(rel_error) \n",
        "\n",
        "    # Returning the loss and metrics\n",
        "    return loss, l1_norm, linf_norm\n",
        "\n",
        "\n",
        "# Defining a function that updates the learning rate scheduler with validation loss if applicable\n",
        "def update_scheduler(scheduler, test_loss):\n",
        "    \"\"\"Updates the learning rate scheduler with validation loss if applicable.\n",
        "\n",
        "    Args:\n",
        "        scheduler (torch.optim.lr_scheduler._LRScheduler or None): The learning rate scheduler to use.\n",
        "        test_loss (float): The validation loss to use.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Checking if scheduler is not None\n",
        "    if scheduler is not None:\n",
        "        # Checking if scheduler is ReduceLROnPlateau\n",
        "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
        "            # Updating the scheduler with test_loss\n",
        "            scheduler.step(test_loss)\n",
        "        else:\n",
        "            # Updating the scheduler without test_loss\n",
        "            scheduler.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1nE662UUZiE"
      },
      "source": [
        "Now for the actual training and evaluation loop,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "YAOjgKW3UZiF"
      },
      "outputs": [],
      "source": [
        "# Defining a function to train and evaluate a network\n",
        "def train_and_eval(net, loss_fn, optimizer, batch_size, n_epochs, scheduler, trial=None):\n",
        "    \"\"\"Trains and evaluates a network.\n",
        "\n",
        "    Args:\n",
        "        net (torch.nn.Module): The network to train and evaluate.\n",
        "        loss_fn (torch.nn.Module or function): The loss function.\n",
        "        optimizer (torch.optim.Optimizer): The optimizer.\n",
        "        batch_size (int): The batch size.\n",
        "        n_epochs (int): The number of epochs.\n",
        "        scheduler (torch.optim.lr_scheduler._LRScheduler or None): The learning rate scheduler.\n",
        "    Returns:\n",
        "        tuple: A tuple of (train_losses, test_losses, train_metrics, test_metrics), where\n",
        "            train_losses is a list of training losses for each epoch,\n",
        "            test_losses is a list of validation losses for each epoch,\n",
        "            train_metrics is a list of dictionaries containing training metrics for each epoch,\n",
        "            test_metrics is a list of dictionaries containing validation metrics for each epoch.\n",
        "    \"\"\"\n",
        "    # Creating data loaders for train and test sets\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.TensorDataset(x_test, y_test), batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Initializing lists to store the losses and metrics for each epoch\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    train_metrics = []\n",
        "    test_metrics = []\n",
        "\n",
        "    # Creating a SummaryWriter object to log data for tensorboard\n",
        "    writer = tbx.SummaryWriter()\n",
        "\n",
        "    # Looping over the epochs\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # Setting the network to training mode\n",
        "        net.train()\n",
        "\n",
        "        # Initializing variables to store the total loss and metrics for the train set\n",
        "        train_loss = 0.0\n",
        "        train_l1_norm = 0.0\n",
        "        train_linf_norm = 0.0\n",
        "\n",
        "        # Looping over the batches in the train set\n",
        "        for x_batch, y_batch in train_loader:\n",
        "\n",
        "            # Moving the batch tensors to the device\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            # Zeroing the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Performing a forward pass and computing the loss and metrics\n",
        "            y_pred = net(x_batch)\n",
        "            loss, l1_norm, linf_norm = compute_loss_and_metrics(\n",
        "                y_pred, y_batch, loss_fn\n",
        "            )\n",
        "\n",
        "\n",
        "            # Performing a backward pass and updating the weights\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Updating the total loss and metrics for the train set\n",
        "            train_loss += loss.item() * x_batch.size(0)\n",
        "            train_l1_norm += l1_norm.item() * x_batch.size(0)\n",
        "            train_linf_norm += linf_norm.item() * x_batch.size(0)\n",
        "\n",
        "        # Computing the average loss and metrics for the train set\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_l1_norm /= len(train_loader.dataset)\n",
        "        train_linf_norm /= len(train_loader.dataset)\n",
        "\n",
        "        # Appending the average loss and metrics for the train set to the lists\n",
        "        train_losses.append(train_loss)\n",
        "        train_metrics.append(\n",
        "            {\n",
        "                \"l1_norm\": train_l1_norm,\n",
        "                \"linf_norm\": train_linf_norm,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Logging the average loss and metrics for the train set to tensorboard\n",
        "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
        "        writer.add_scalar(\"L1 norm/train\", train_l1_norm, epoch)\n",
        "        writer.add_scalar(\"Linf norm/train\", train_linf_norm, epoch)\n",
        "\n",
        "        # Setting the network to evaluation mode\n",
        "        net.eval()\n",
        "\n",
        "        # Initializing variables to store the total loss and metrics for the test set\n",
        "        test_loss = 0.0\n",
        "        test_l1_norm = 0.0\n",
        "        test_linf_norm = 0.0\n",
        "\n",
        "        # Looping over the batches in the test set\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in test_loader:\n",
        "\n",
        "                # Moving the batch tensors to the device\n",
        "                x_batch = x_batch.to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "\n",
        "                # Performing a forward pass and computing the loss and metrics\n",
        "                y_pred = net(x_batch)\n",
        "                loss, l1_norm, linf_norm = compute_loss_and_metrics(\n",
        "                    y_pred, y_batch, loss_fn\n",
        "                )\n",
        "\n",
        "\n",
        "                # Updating the total loss and metrics for the test set\n",
        "                test_loss += loss.item() * x_batch.size(0)\n",
        "                test_l1_norm += l1_norm.item() * x_batch.size(0)\n",
        "                test_linf_norm += linf_norm.item() * x_batch.size(0)\n",
        "\n",
        "        # Computing the average loss and metrics for the test set\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        test_l1_norm /= len(test_loader.dataset)\n",
        "        test_linf_norm /= len(test_loader.dataset)\n",
        "\n",
        "        # Appending the average loss and metrics for the test set to the lists\n",
        "        test_losses.append(test_loss)\n",
        "        test_metrics.append(\n",
        "            {\n",
        "                \"l1_norm\": test_l1_norm,\n",
        "                \"linf_norm\": test_linf_norm,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Logging the average loss and metrics for the test set to tensorboard\n",
        "        writer.add_scalar(\"Loss/test\", test_loss, epoch)\n",
        "        writer.add_scalar(\"L1 norm/test\", test_l1_norm, epoch)\n",
        "        writer.add_scalar(\"Linf norm/test\", test_linf_norm, epoch)\n",
        "\n",
        "        # Printing the average loss and metrics for both sets for this epoch\n",
        "        print(\n",
        "            f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, \"\n",
        "            f\"Train L1 Norm: {train_l1_norm:.4f}, Test L1 Norm: {test_l1_norm:.4f}, \"\n",
        "            f\"Train Linf Norm: {train_linf_norm:.4f}, Test Linf Norm: {test_linf_norm:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Updating the learning rate scheduler with validation loss if applicable\n",
        "        update_scheduler(scheduler, test_loss)\n",
        "\n",
        "        # Reporting the intermediate metric value to Optuna if trial is not None\n",
        "        if trial is not None:\n",
        "            trial.report(test_metrics[-1][\"l1_norm\"], epoch)\n",
        "\n",
        "            # Checking if the trial should be pruned based on the intermediate value if trial is not None\n",
        "            if trial.should_prune():\n",
        "                raise optuna.TrialPruned()\n",
        "\n",
        "    # Closing the SummaryWriter object\n",
        "    writer.close()\n",
        "\n",
        "    # Returning the losses and metrics lists\n",
        "    return train_losses, test_losses, train_metrics, test_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg9jz0SvUZiQ"
      },
      "source": [
        "## The objective function and hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "fmRncQPuUZiR"
      },
      "outputs": [],
      "source": [
        "# Defining an objective function for Optuna to minimize\n",
        "def objective(trial):\n",
        "    \"\"\"Defines an objective function for Optuna to minimize.\n",
        "\n",
        "    Args:\n",
        "        trial (optuna.trial.Trial): The trial object that contains the hyperparameters.\n",
        "\n",
        "    Returns:\n",
        "        float: The validation L1 norm to minimize.\n",
        "    \"\"\"\n",
        "    # Creating a trial network and optimizer using the create_model function\n",
        "    net, \\\n",
        "    loss_fn, \\\n",
        "    optimizer, \\\n",
        "    batch_size, \\\n",
        "    n_epochs, \\\n",
        "    scheduler, \\\n",
        "    loss_name, \\\n",
        "    optimizer_name, \\\n",
        "    scheduler_name, \\\n",
        "    n_units, \\\n",
        "    n_layers, \\\n",
        "    hidden_activation, \\\n",
        "    output_activation, \\\n",
        "    lr = create_model(trial, optimize=True)\n",
        "\n",
        "    # Training and evaluating the network using the train_and_eval function\n",
        "    _, _, _, test_metrics = train_and_eval(\n",
        "        net, loss_fn, optimizer, batch_size, n_epochs, scheduler, trial\n",
        "    )\n",
        "\n",
        "    # Returning the last validation L1 norm as the objective value to minimize\n",
        "    return test_metrics[-1][\"l1_norm\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyES4NAyUZiS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "495922a7-d5e7-4cbd-8d4c-8831f8ff3a51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 15:32:15,049]\u001b[0m A new study created in memory with name: no-name-b54c3748-34c2-4f3a-b23d-23d6641209e5\u001b[0m\n",
            "<ipython-input-28-e8f6ba3279cc>:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "<ipython-input-28-e8f6ba3279cc>:121: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
            "<ipython-input-28-e8f6ba3279cc>:122: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  momentum = trial.suggest_uniform(\"momentum\", 0.0, 0.99)\n",
            "<ipython-input-28-e8f6ba3279cc>:150: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  t_max_fraction = trial.suggest_uniform('t_max_fraction', 0.1, 0.2)\n",
            "<ipython-input-28-e8f6ba3279cc>:153: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  eta_min = trial.suggest_loguniform(\"eta_min\", 1e-7, 1e-2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.3165, Test Loss: 0.0184, Train L1 Norm: 2.3224, Test L1 Norm: 0.3861, Train Linf Norm: 874.1108, Test Linf Norm: 79.0132\n",
            "Epoch 2: Train Loss: 0.0156, Test Loss: 0.0268, Train L1 Norm: 0.7670, Test L1 Norm: 0.2097, Train Linf Norm: 305.5545, Test Linf Norm: 39.8496\n",
            "Epoch 3: Train Loss: 0.0142, Test Loss: 0.0022, Train L1 Norm: 0.7132, Test L1 Norm: 0.1725, Train Linf Norm: 236.9826, Test Linf Norm: 37.1178\n",
            "Epoch 4: Train Loss: 0.0083, Test Loss: 0.0027, Train L1 Norm: 0.6105, Test L1 Norm: 0.1885, Train Linf Norm: 253.8516, Test Linf Norm: 41.8405\n",
            "Epoch 5: Train Loss: 0.0048, Test Loss: 0.0050, Train L1 Norm: 0.5645, Test L1 Norm: 0.1641, Train Linf Norm: 235.7162, Test Linf Norm: 36.3796\n",
            "Epoch 6: Train Loss: 0.0089, Test Loss: 0.0027, Train L1 Norm: 0.5473, Test L1 Norm: 0.1560, Train Linf Norm: 224.1416, Test Linf Norm: 35.3218\n",
            "Epoch 7: Train Loss: 0.0081, Test Loss: 0.0018, Train L1 Norm: 0.5274, Test L1 Norm: 0.1618, Train Linf Norm: 214.8692, Test Linf Norm: 35.9466\n",
            "Epoch 8: Train Loss: 0.0043, Test Loss: 0.0035, Train L1 Norm: 0.4857, Test L1 Norm: 0.1726, Train Linf Norm: 200.2131, Test Linf Norm: 37.1618\n",
            "Epoch 9: Train Loss: 0.0050, Test Loss: 0.0152, Train L1 Norm: 0.4949, Test L1 Norm: 0.1701, Train Linf Norm: 203.3937, Test Linf Norm: 32.9492\n",
            "Epoch 10: Train Loss: 0.0040, Test Loss: 0.0005, Train L1 Norm: 0.4689, Test L1 Norm: 0.1326, Train Linf Norm: 197.1341, Test Linf Norm: 30.7243\n",
            "Epoch 11: Train Loss: 0.0015, Test Loss: 0.0037, Train L1 Norm: 0.4137, Test L1 Norm: 0.1440, Train Linf Norm: 168.4183, Test Linf Norm: 30.4714\n",
            "Epoch 12: Train Loss: 0.0019, Test Loss: 0.0045, Train L1 Norm: 0.4208, Test L1 Norm: 0.1552, Train Linf Norm: 174.1296, Test Linf Norm: 31.9736\n",
            "Epoch 13: Train Loss: 0.0034, Test Loss: 0.0030, Train L1 Norm: 0.3992, Test L1 Norm: 0.1230, Train Linf Norm: 162.3960, Test Linf Norm: 27.9304\n",
            "Epoch 14: Train Loss: 0.0031, Test Loss: 0.0024, Train L1 Norm: 0.3921, Test L1 Norm: 0.1205, Train Linf Norm: 159.5835, Test Linf Norm: 27.5086\n",
            "Epoch 15: Train Loss: 0.0023, Test Loss: 0.0056, Train L1 Norm: 0.3846, Test L1 Norm: 0.1244, Train Linf Norm: 159.7212, Test Linf Norm: 26.3919\n",
            "Epoch 16: Train Loss: 0.0014, Test Loss: 0.0015, Train L1 Norm: 0.3534, Test L1 Norm: 0.1175, Train Linf Norm: 147.2262, Test Linf Norm: 27.2569\n",
            "Epoch 17: Train Loss: 0.0012, Test Loss: 0.0005, Train L1 Norm: 0.3586, Test L1 Norm: 0.1088, Train Linf Norm: 151.5841, Test Linf Norm: 25.5391\n",
            "Epoch 18: Train Loss: 0.0012, Test Loss: 0.0004, Train L1 Norm: 0.3382, Test L1 Norm: 0.1062, Train Linf Norm: 140.4251, Test Linf Norm: 24.9117\n",
            "Epoch 19: Train Loss: 0.0010, Test Loss: 0.0003, Train L1 Norm: 0.3319, Test L1 Norm: 0.1038, Train Linf Norm: 138.5352, Test Linf Norm: 24.4089\n",
            "Epoch 20: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.3180, Test L1 Norm: 0.1001, Train Linf Norm: 133.9460, Test Linf Norm: 23.6025\n",
            "Epoch 21: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.3000, Test L1 Norm: 0.0982, Train Linf Norm: 125.8177, Test Linf Norm: 23.1773\n",
            "Epoch 22: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.2981, Test L1 Norm: 0.0959, Train Linf Norm: 124.8589, Test Linf Norm: 22.7055\n",
            "Epoch 23: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.2928, Test L1 Norm: 0.0947, Train Linf Norm: 121.3442, Test Linf Norm: 22.4476\n",
            "Epoch 24: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.2887, Test L1 Norm: 0.0940, Train Linf Norm: 120.0681, Test Linf Norm: 22.2529\n",
            "Epoch 25: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.2863, Test L1 Norm: 0.0919, Train Linf Norm: 95.8087, Test Linf Norm: 21.7964\n",
            "Epoch 26: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.2802, Test L1 Norm: 0.0907, Train Linf Norm: 118.4151, Test Linf Norm: 21.5413\n",
            "Epoch 27: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.2783, Test L1 Norm: 0.0922, Train Linf Norm: 115.3728, Test Linf Norm: 21.7014\n",
            "Epoch 28: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.2736, Test L1 Norm: 0.0901, Train Linf Norm: 111.5398, Test Linf Norm: 21.3553\n",
            "Epoch 29: Train Loss: 0.0002, Test Loss: 0.0003, Train L1 Norm: 0.2729, Test L1 Norm: 0.0899, Train Linf Norm: 114.1068, Test Linf Norm: 21.2317\n",
            "Epoch 30: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.2709, Test L1 Norm: 0.0889, Train Linf Norm: 113.2022, Test Linf Norm: 21.1244\n",
            "Epoch 31: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.2692, Test L1 Norm: 0.0885, Train Linf Norm: 110.6961, Test Linf Norm: 21.0119\n",
            "Epoch 32: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.2677, Test L1 Norm: 0.0897, Train Linf Norm: 110.4204, Test Linf Norm: 21.1318\n",
            "Epoch 33: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.2684, Test L1 Norm: 0.0880, Train Linf Norm: 112.8423, Test Linf Norm: 20.9070\n",
            "Epoch 34: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.2674, Test L1 Norm: 0.0881, Train Linf Norm: 111.8865, Test Linf Norm: 20.9027\n",
            "Epoch 35: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.2659, Test L1 Norm: 0.0880, Train Linf Norm: 109.1427, Test Linf Norm: 20.8763\n",
            "Epoch 36: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.2647, Test L1 Norm: 0.0876, Train Linf Norm: 109.9809, Test Linf Norm: 20.7728\n",
            "Epoch 37: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.2644, Test L1 Norm: 0.0874, Train Linf Norm: 109.9383, Test Linf Norm: 20.7513\n",
            "Epoch 38: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.2640, Test L1 Norm: 0.0871, Train Linf Norm: 109.1871, Test Linf Norm: 20.6717\n",
            "Epoch 39: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.2629, Test L1 Norm: 0.0866, Train Linf Norm: 110.4302, Test Linf Norm: 20.5653\n",
            "Epoch 40: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.2614, Test L1 Norm: 0.0867, Train Linf Norm: 109.1667, Test Linf Norm: 20.5130\n",
            "Epoch 41: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.2593, Test L1 Norm: 0.0866, Train Linf Norm: 107.1535, Test Linf Norm: 20.4999\n",
            "Epoch 42: Train Loss: 0.0002, Test Loss: 0.0007, Train L1 Norm: 0.2589, Test L1 Norm: 0.0895, Train Linf Norm: 107.3351, Test Linf Norm: 20.5808\n",
            "Epoch 43: Train Loss: 0.0007, Test Loss: 0.0003, Train L1 Norm: 0.2614, Test L1 Norm: 0.0855, Train Linf Norm: 107.3550, Test Linf Norm: 20.1458\n",
            "Epoch 44: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.2637, Test L1 Norm: 0.0841, Train Linf Norm: 108.4589, Test Linf Norm: 19.9742\n",
            "Epoch 45: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.2485, Test L1 Norm: 0.0852, Train Linf Norm: 102.7823, Test Linf Norm: 20.1187\n",
            "Epoch 46: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.2488, Test L1 Norm: 0.0823, Train Linf Norm: 104.5942, Test Linf Norm: 19.6063\n",
            "Epoch 47: Train Loss: 0.0002, Test Loss: 0.0008, Train L1 Norm: 0.2462, Test L1 Norm: 0.0834, Train Linf Norm: 102.9084, Test Linf Norm: 19.0275\n",
            "Epoch 48: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.2383, Test L1 Norm: 0.0823, Train Linf Norm: 97.2129, Test Linf Norm: 19.2433\n",
            "Epoch 49: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.2380, Test L1 Norm: 0.0795, Train Linf Norm: 98.7972, Test Linf Norm: 18.6426\n",
            "Epoch 50: Train Loss: 0.0009, Test Loss: 0.0003, Train L1 Norm: 0.2357, Test L1 Norm: 0.0798, Train Linf Norm: 95.3106, Test Linf Norm: 18.7747\n",
            "Epoch 51: Train Loss: 0.0008, Test Loss: 0.0003, Train L1 Norm: 0.2294, Test L1 Norm: 0.0826, Train Linf Norm: 93.4504, Test Linf Norm: 19.2203\n",
            "Epoch 52: Train Loss: 0.0022, Test Loss: 0.0009, Train L1 Norm: 0.2307, Test L1 Norm: 0.0837, Train Linf Norm: 91.8109, Test Linf Norm: 19.4084\n",
            "Epoch 53: Train Loss: 0.0013, Test Loss: 0.0009, Train L1 Norm: 0.2325, Test L1 Norm: 0.0774, Train Linf Norm: 94.2212, Test Linf Norm: 18.0524\n",
            "Epoch 54: Train Loss: 0.0008, Test Loss: 0.0004, Train L1 Norm: 0.2233, Test L1 Norm: 0.0798, Train Linf Norm: 91.4062, Test Linf Norm: 18.2810\n",
            "Epoch 55: Train Loss: 0.0029, Test Loss: 0.0023, Train L1 Norm: 0.2072, Test L1 Norm: 0.0944, Train Linf Norm: 79.7799, Test Linf Norm: 19.6980\n",
            "Epoch 56: Train Loss: 0.0024, Test Loss: 0.0024, Train L1 Norm: 0.2694, Test L1 Norm: 0.0893, Train Linf Norm: 111.0007, Test Linf Norm: 18.4272\n",
            "Epoch 57: Train Loss: 0.0013, Test Loss: 0.0182, Train L1 Norm: 0.2187, Test L1 Norm: 0.1001, Train Linf Norm: 88.3775, Test Linf Norm: 15.7315\n",
            "Epoch 58: Train Loss: 0.0042, Test Loss: 0.0023, Train L1 Norm: 0.2479, Test L1 Norm: 0.0910, Train Linf Norm: 99.2329, Test Linf Norm: 19.6210\n",
            "Epoch 59: Train Loss: 0.0025, Test Loss: 0.0028, Train L1 Norm: 0.2440, Test L1 Norm: 0.0798, Train Linf Norm: 96.7367, Test Linf Norm: 16.1759\n",
            "Epoch 60: Train Loss: 0.0039, Test Loss: 0.0005, Train L1 Norm: 0.2495, Test L1 Norm: 0.0761, Train Linf Norm: 97.0626, Test Linf Norm: 17.6567\n",
            "Epoch 61: Train Loss: 0.0034, Test Loss: 0.0019, Train L1 Norm: 0.2190, Test L1 Norm: 0.0828, Train Linf Norm: 86.0987, Test Linf Norm: 17.9155\n",
            "Epoch 62: Train Loss: 0.0059, Test Loss: 0.0079, Train L1 Norm: 0.2137, Test L1 Norm: 0.0857, Train Linf Norm: 79.5645, Test Linf Norm: 15.7232\n",
            "Epoch 63: Train Loss: 0.0035, Test Loss: 0.0115, Train L1 Norm: 0.2186, Test L1 Norm: 0.1140, Train Linf Norm: 83.1356, Test Linf Norm: 20.2904\n",
            "Epoch 64: Train Loss: 0.0037, Test Loss: 0.0009, Train L1 Norm: 0.2166, Test L1 Norm: 0.0725, Train Linf Norm: 82.5226, Test Linf Norm: 16.1981\n",
            "Epoch 65: Train Loss: 0.0010, Test Loss: 0.0003, Train L1 Norm: 0.2116, Test L1 Norm: 0.0713, Train Linf Norm: 86.2698, Test Linf Norm: 16.2012\n",
            "Epoch 66: Train Loss: 0.0010, Test Loss: 0.0003, Train L1 Norm: 0.1889, Test L1 Norm: 0.0697, Train Linf Norm: 74.1230, Test Linf Norm: 16.0125\n",
            "Epoch 67: Train Loss: 0.0024, Test Loss: 0.0007, Train L1 Norm: 0.2014, Test L1 Norm: 0.0699, Train Linf Norm: 79.4563, Test Linf Norm: 15.7187\n",
            "Epoch 68: Train Loss: 0.0040, Test Loss: 0.0021, Train L1 Norm: 0.2289, Test L1 Norm: 0.0714, Train Linf Norm: 85.0627, Test Linf Norm: 14.2874\n",
            "Epoch 69: Train Loss: 0.0016, Test Loss: 0.0012, Train L1 Norm: 0.1990, Test L1 Norm: 0.0660, Train Linf Norm: 78.8273, Test Linf Norm: 14.7941\n",
            "Epoch 70: Train Loss: 0.0038, Test Loss: 0.0021, Train L1 Norm: 0.1873, Test L1 Norm: 0.0706, Train Linf Norm: 69.5372, Test Linf Norm: 14.9884\n",
            "Epoch 71: Train Loss: 0.0009, Test Loss: 0.0002, Train L1 Norm: 0.1690, Test L1 Norm: 0.0610, Train Linf Norm: 65.6338, Test Linf Norm: 13.9759\n",
            "Epoch 72: Train Loss: 0.0026, Test Loss: 0.0015, Train L1 Norm: 0.1797, Test L1 Norm: 0.0661, Train Linf Norm: 69.5053, Test Linf Norm: 13.4483\n",
            "Epoch 73: Train Loss: 0.0018, Test Loss: 0.0005, Train L1 Norm: 0.1675, Test L1 Norm: 0.0620, Train Linf Norm: 63.6827, Test Linf Norm: 13.8556\n",
            "Epoch 74: Train Loss: 0.0055, Test Loss: 0.0009, Train L1 Norm: 0.1847, Test L1 Norm: 0.0637, Train Linf Norm: 66.5231, Test Linf Norm: 13.8061\n",
            "Epoch 75: Train Loss: 0.0058, Test Loss: 0.0015, Train L1 Norm: 0.2341, Test L1 Norm: 0.0769, Train Linf Norm: 90.5521, Test Linf Norm: 16.8680\n",
            "Epoch 76: Train Loss: 0.0028, Test Loss: 0.0016, Train L1 Norm: 0.1977, Test L1 Norm: 0.0763, Train Linf Norm: 77.7449, Test Linf Norm: 16.8784\n",
            "Epoch 77: Train Loss: 0.0007, Test Loss: 0.0004, Train L1 Norm: 0.2003, Test L1 Norm: 0.0644, Train Linf Norm: 81.2503, Test Linf Norm: 14.6832\n",
            "Epoch 78: Train Loss: 0.0012, Test Loss: 0.0016, Train L1 Norm: 0.1990, Test L1 Norm: 0.0773, Train Linf Norm: 80.4455, Test Linf Norm: 16.0149\n",
            "Epoch 79: Train Loss: 0.0014, Test Loss: 0.0009, Train L1 Norm: 0.2044, Test L1 Norm: 0.0642, Train Linf Norm: 82.9954, Test Linf Norm: 14.0173\n",
            "Epoch 80: Train Loss: 0.0008, Test Loss: 0.0013, Train L1 Norm: 0.1825, Test L1 Norm: 0.0630, Train Linf Norm: 73.4096, Test Linf Norm: 13.2767\n",
            "Epoch 81: Train Loss: 0.0005, Test Loss: 0.0013, Train L1 Norm: 0.1649, Test L1 Norm: 0.0656, Train Linf Norm: 63.3632, Test Linf Norm: 13.9986\n",
            "Epoch 82: Train Loss: 0.0022, Test Loss: 0.0003, Train L1 Norm: 0.1909, Test L1 Norm: 0.0601, Train Linf Norm: 75.9818, Test Linf Norm: 13.9310\n",
            "Epoch 83: Train Loss: 0.0007, Test Loss: 0.0004, Train L1 Norm: 0.1629, Test L1 Norm: 0.0611, Train Linf Norm: 65.0391, Test Linf Norm: 13.8311\n",
            "Epoch 84: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.1655, Test L1 Norm: 0.0571, Train Linf Norm: 66.6644, Test Linf Norm: 12.9067\n",
            "Epoch 85: Train Loss: 0.0005, Test Loss: 0.0002, Train L1 Norm: 0.1650, Test L1 Norm: 0.0568, Train Linf Norm: 66.6607, Test Linf Norm: 12.9271\n",
            "Epoch 86: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.1511, Test L1 Norm: 0.0548, Train Linf Norm: 59.5288, Test Linf Norm: 12.5340\n",
            "Epoch 87: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.1495, Test L1 Norm: 0.0532, Train Linf Norm: 57.9470, Test Linf Norm: 12.1075\n",
            "Epoch 88: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.1470, Test L1 Norm: 0.0531, Train Linf Norm: 59.0466, Test Linf Norm: 12.0348\n",
            "Epoch 89: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.1457, Test L1 Norm: 0.0528, Train Linf Norm: 58.6638, Test Linf Norm: 12.0355\n",
            "Epoch 90: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1443, Test L1 Norm: 0.0521, Train Linf Norm: 57.2827, Test Linf Norm: 11.8565\n",
            "Epoch 91: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1400, Test L1 Norm: 0.0521, Train Linf Norm: 55.3338, Test Linf Norm: 11.8396\n",
            "Epoch 92: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1397, Test L1 Norm: 0.0514, Train Linf Norm: 55.5958, Test Linf Norm: 11.7133\n",
            "Epoch 93: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1390, Test L1 Norm: 0.0517, Train Linf Norm: 55.7649, Test Linf Norm: 11.7697\n",
            "Epoch 94: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1376, Test L1 Norm: 0.0512, Train Linf Norm: 54.5295, Test Linf Norm: 11.6484\n",
            "Epoch 95: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1362, Test L1 Norm: 0.0509, Train Linf Norm: 53.9694, Test Linf Norm: 11.5878\n",
            "Epoch 96: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.1371, Test L1 Norm: 0.0508, Train Linf Norm: 55.2753, Test Linf Norm: 11.5318\n",
            "Epoch 97: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1343, Test L1 Norm: 0.0505, Train Linf Norm: 53.0871, Test Linf Norm: 11.4670\n",
            "Epoch 98: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1351, Test L1 Norm: 0.0504, Train Linf Norm: 53.9229, Test Linf Norm: 11.4576\n",
            "Epoch 99: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1350, Test L1 Norm: 0.0503, Train Linf Norm: 54.0424, Test Linf Norm: 11.4171\n",
            "Epoch 100: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1346, Test L1 Norm: 0.0504, Train Linf Norm: 54.6337, Test Linf Norm: 11.4437\n",
            "Epoch 101: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1341, Test L1 Norm: 0.0502, Train Linf Norm: 53.6207, Test Linf Norm: 11.4144\n",
            "Epoch 102: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1341, Test L1 Norm: 0.0501, Train Linf Norm: 53.9306, Test Linf Norm: 11.3625\n",
            "Epoch 103: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1337, Test L1 Norm: 0.0504, Train Linf Norm: 52.8958, Test Linf Norm: 11.4290\n",
            "Epoch 104: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1332, Test L1 Norm: 0.0501, Train Linf Norm: 52.9264, Test Linf Norm: 11.3666\n",
            "Epoch 105: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1331, Test L1 Norm: 0.0500, Train Linf Norm: 53.4255, Test Linf Norm: 11.3555\n",
            "Epoch 106: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1335, Test L1 Norm: 0.0501, Train Linf Norm: 52.8896, Test Linf Norm: 11.3793\n",
            "Epoch 107: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1333, Test L1 Norm: 0.0502, Train Linf Norm: 53.1956, Test Linf Norm: 11.3967\n",
            "Epoch 108: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1329, Test L1 Norm: 0.0498, Train Linf Norm: 52.7850, Test Linf Norm: 11.2961\n",
            "Epoch 109: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1323, Test L1 Norm: 0.0496, Train Linf Norm: 47.0346, Test Linf Norm: 11.2391\n",
            "Epoch 110: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1318, Test L1 Norm: 0.0495, Train Linf Norm: 52.2011, Test Linf Norm: 11.2157\n",
            "Epoch 111: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1323, Test L1 Norm: 0.0494, Train Linf Norm: 50.9183, Test Linf Norm: 11.1790\n",
            "Epoch 112: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1322, Test L1 Norm: 0.0493, Train Linf Norm: 52.7908, Test Linf Norm: 11.1425\n",
            "Epoch 113: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1298, Test L1 Norm: 0.0491, Train Linf Norm: 52.1288, Test Linf Norm: 11.0793\n",
            "Epoch 114: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1289, Test L1 Norm: 0.0490, Train Linf Norm: 49.2281, Test Linf Norm: 11.1026\n",
            "Epoch 115: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.1289, Test L1 Norm: 0.0488, Train Linf Norm: 51.8565, Test Linf Norm: 10.9908\n",
            "Epoch 116: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1264, Test L1 Norm: 0.0486, Train Linf Norm: 49.3890, Test Linf Norm: 10.9508\n",
            "Epoch 117: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1270, Test L1 Norm: 0.0475, Train Linf Norm: 50.3809, Test Linf Norm: 10.6506\n",
            "Epoch 118: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1239, Test L1 Norm: 0.0474, Train Linf Norm: 49.0118, Test Linf Norm: 10.6285\n",
            "Epoch 119: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.1304, Test L1 Norm: 0.0479, Train Linf Norm: 50.2286, Test Linf Norm: 10.4846\n",
            "Epoch 120: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.1161, Test L1 Norm: 0.0478, Train Linf Norm: 44.2702, Test Linf Norm: 10.7952\n",
            "Epoch 121: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.1233, Test L1 Norm: 0.0475, Train Linf Norm: 48.3667, Test Linf Norm: 10.6112\n",
            "Epoch 122: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.1219, Test L1 Norm: 0.0473, Train Linf Norm: 47.1972, Test Linf Norm: 10.6202\n",
            "Epoch 123: Train Loss: 0.0016, Test Loss: 0.0097, Train L1 Norm: 0.1299, Test L1 Norm: 0.0721, Train Linf Norm: 49.0274, Test Linf Norm: 11.4025\n",
            "Epoch 124: Train Loss: 0.0020, Test Loss: 0.0016, Train L1 Norm: 0.1573, Test L1 Norm: 0.0666, Train Linf Norm: 61.2940, Test Linf Norm: 13.1696\n",
            "Epoch 125: Train Loss: 0.0010, Test Loss: 0.0004, Train L1 Norm: 0.1281, Test L1 Norm: 0.0499, Train Linf Norm: 49.3867, Test Linf Norm: 11.0024\n",
            "Epoch 126: Train Loss: 0.0024, Test Loss: 0.0048, Train L1 Norm: 0.1537, Test L1 Norm: 0.0851, Train Linf Norm: 58.3424, Test Linf Norm: 14.8709\n",
            "Epoch 127: Train Loss: 0.0038, Test Loss: 0.0004, Train L1 Norm: 0.2058, Test L1 Norm: 0.0507, Train Linf Norm: 83.0755, Test Linf Norm: 10.9887\n",
            "Epoch 128: Train Loss: 0.0017, Test Loss: 0.0008, Train L1 Norm: 0.1521, Test L1 Norm: 0.0623, Train Linf Norm: 57.9942, Test Linf Norm: 12.8903\n",
            "Epoch 129: Train Loss: 0.0013, Test Loss: 0.0018, Train L1 Norm: 0.1347, Test L1 Norm: 0.0542, Train Linf Norm: 46.3758, Test Linf Norm: 10.7876\n",
            "Epoch 130: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.1326, Test L1 Norm: 0.0496, Train Linf Norm: 47.7297, Test Linf Norm: 11.0210\n",
            "Epoch 131: Train Loss: 0.0003, Test Loss: 0.0040, Train L1 Norm: 0.1332, Test L1 Norm: 0.0695, Train Linf Norm: 53.0675, Test Linf Norm: 12.5184\n",
            "Epoch 132: Train Loss: 0.0010, Test Loss: 0.0006, Train L1 Norm: 0.1279, Test L1 Norm: 0.0517, Train Linf Norm: 48.3567, Test Linf Norm: 10.9720\n",
            "Epoch 133: Train Loss: 0.0020, Test Loss: 0.0041, Train L1 Norm: 0.1394, Test L1 Norm: 0.0623, Train Linf Norm: 48.6184, Test Linf Norm: 10.9745\n",
            "Epoch 134: Train Loss: 0.0033, Test Loss: 0.0010, Train L1 Norm: 0.1534, Test L1 Norm: 0.0569, Train Linf Norm: 57.9117, Test Linf Norm: 12.6320\n",
            "Epoch 135: Train Loss: 0.0009, Test Loss: 0.0005, Train L1 Norm: 0.1376, Test L1 Norm: 0.0497, Train Linf Norm: 53.6448, Test Linf Norm: 10.6923\n",
            "Epoch 136: Train Loss: 0.0024, Test Loss: 0.0007, Train L1 Norm: 0.1477, Test L1 Norm: 0.0531, Train Linf Norm: 54.8733, Test Linf Norm: 11.1612\n",
            "Epoch 137: Train Loss: 0.0022, Test Loss: 0.0004, Train L1 Norm: 0.1310, Test L1 Norm: 0.0503, Train Linf Norm: 48.7482, Test Linf Norm: 11.1078\n",
            "Epoch 138: Train Loss: 0.0007, Test Loss: 0.0009, Train L1 Norm: 0.1336, Test L1 Norm: 0.0502, Train Linf Norm: 51.3203, Test Linf Norm: 10.6473\n",
            "Epoch 139: Train Loss: 0.0031, Test Loss: 0.0005, Train L1 Norm: 0.1440, Test L1 Norm: 0.0541, Train Linf Norm: 54.4958, Test Linf Norm: 12.1272\n",
            "Epoch 140: Train Loss: 0.0007, Test Loss: 0.0006, Train L1 Norm: 0.1295, Test L1 Norm: 0.0483, Train Linf Norm: 48.9324, Test Linf Norm: 10.1286\n",
            "Epoch 141: Train Loss: 0.0012, Test Loss: 0.0004, Train L1 Norm: 0.1371, Test L1 Norm: 0.0485, Train Linf Norm: 52.7744, Test Linf Norm: 10.8425\n",
            "Epoch 142: Train Loss: 0.0004, Test Loss: 0.0048, Train L1 Norm: 0.1335, Test L1 Norm: 0.0647, Train Linf Norm: 54.1293, Test Linf Norm: 11.5458\n",
            "Epoch 143: Train Loss: 0.0017, Test Loss: 0.0032, Train L1 Norm: 0.1370, Test L1 Norm: 0.0722, Train Linf Norm: 51.2268, Test Linf Norm: 13.8870\n",
            "Epoch 144: Train Loss: 0.0007, Test Loss: 0.0002, Train L1 Norm: 0.1328, Test L1 Norm: 0.0502, Train Linf Norm: 51.7922, Test Linf Norm: 11.3993\n",
            "Epoch 145: Train Loss: 0.0031, Test Loss: 0.0046, Train L1 Norm: 0.1503, Test L1 Norm: 0.0647, Train Linf Norm: 58.1243, Test Linf Norm: 12.2988\n",
            "Epoch 146: Train Loss: 0.0006, Test Loss: 0.0002, Train L1 Norm: 0.1280, Test L1 Norm: 0.0471, Train Linf Norm: 49.3316, Test Linf Norm: 10.5344\n",
            "Epoch 147: Train Loss: 0.0003, Test Loss: 0.0004, Train L1 Norm: 0.1315, Test L1 Norm: 0.0462, Train Linf Norm: 51.4921, Test Linf Norm: 10.2669\n",
            "Epoch 148: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.1197, Test L1 Norm: 0.0448, Train Linf Norm: 47.0540, Test Linf Norm: 10.1684\n",
            "Epoch 149: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.1192, Test L1 Norm: 0.0469, Train Linf Norm: 47.3256, Test Linf Norm: 10.3923\n",
            "Epoch 150: Train Loss: 0.0007, Test Loss: 0.0001, Train L1 Norm: 0.1187, Test L1 Norm: 0.0452, Train Linf Norm: 45.5167, Test Linf Norm: 10.2481\n",
            "Epoch 151: Train Loss: 0.0004, Test Loss: 0.0138, Train L1 Norm: 0.1227, Test L1 Norm: 0.0713, Train Linf Norm: 48.9838, Test Linf Norm: 9.9254\n",
            "Epoch 152: Train Loss: 0.0033, Test Loss: 0.0010, Train L1 Norm: 0.1343, Test L1 Norm: 0.0525, Train Linf Norm: 49.9297, Test Linf Norm: 11.2044\n",
            "Epoch 153: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.1244, Test L1 Norm: 0.0494, Train Linf Norm: 47.9681, Test Linf Norm: 10.8966\n",
            "Epoch 154: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.1248, Test L1 Norm: 0.0472, Train Linf Norm: 50.0282, Test Linf Norm: 10.6455\n",
            "Epoch 155: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.1242, Test L1 Norm: 0.0440, Train Linf Norm: 50.5800, Test Linf Norm: 9.9881\n",
            "Epoch 156: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1180, Test L1 Norm: 0.0444, Train Linf Norm: 46.7147, Test Linf Norm: 10.1156\n",
            "Epoch 157: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1179, Test L1 Norm: 0.0427, Train Linf Norm: 45.9665, Test Linf Norm: 9.6170\n",
            "Epoch 158: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.1141, Test L1 Norm: 0.0426, Train Linf Norm: 45.2447, Test Linf Norm: 9.6151\n",
            "Epoch 159: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1105, Test L1 Norm: 0.0423, Train Linf Norm: 44.0393, Test Linf Norm: 9.4807\n",
            "Epoch 160: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1104, Test L1 Norm: 0.0422, Train Linf Norm: 43.2115, Test Linf Norm: 9.5215\n",
            "Epoch 161: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1092, Test L1 Norm: 0.0419, Train Linf Norm: 41.8746, Test Linf Norm: 9.4359\n",
            "Epoch 162: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1089, Test L1 Norm: 0.0417, Train Linf Norm: 43.3456, Test Linf Norm: 9.3858\n",
            "Epoch 163: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1078, Test L1 Norm: 0.0417, Train Linf Norm: 42.9841, Test Linf Norm: 9.4083\n",
            "Epoch 164: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1082, Test L1 Norm: 0.0418, Train Linf Norm: 42.7474, Test Linf Norm: 9.4529\n",
            "Epoch 165: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1071, Test L1 Norm: 0.0419, Train Linf Norm: 42.7886, Test Linf Norm: 9.4437\n",
            "Epoch 166: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1083, Test L1 Norm: 0.0415, Train Linf Norm: 42.4697, Test Linf Norm: 9.3228\n",
            "Epoch 167: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1073, Test L1 Norm: 0.0416, Train Linf Norm: 42.7018, Test Linf Norm: 9.3570\n",
            "Epoch 168: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1067, Test L1 Norm: 0.0415, Train Linf Norm: 41.6212, Test Linf Norm: 9.3432\n",
            "Epoch 169: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1070, Test L1 Norm: 0.0414, Train Linf Norm: 41.5720, Test Linf Norm: 9.3166\n",
            "Epoch 170: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1072, Test L1 Norm: 0.0414, Train Linf Norm: 42.8475, Test Linf Norm: 9.3291\n",
            "Epoch 171: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1066, Test L1 Norm: 0.0414, Train Linf Norm: 42.2317, Test Linf Norm: 9.3240\n",
            "Epoch 172: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1068, Test L1 Norm: 0.0413, Train Linf Norm: 42.8110, Test Linf Norm: 9.3015\n",
            "Epoch 173: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1072, Test L1 Norm: 0.0414, Train Linf Norm: 42.5819, Test Linf Norm: 9.3271\n",
            "Epoch 174: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1063, Test L1 Norm: 0.0413, Train Linf Norm: 39.4417, Test Linf Norm: 9.3039\n",
            "Epoch 175: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1067, Test L1 Norm: 0.0412, Train Linf Norm: 42.0124, Test Linf Norm: 9.2477\n",
            "Epoch 176: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1062, Test L1 Norm: 0.0412, Train Linf Norm: 41.6047, Test Linf Norm: 9.2474\n",
            "Epoch 177: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1048, Test L1 Norm: 0.0418, Train Linf Norm: 42.2627, Test Linf Norm: 9.3663\n",
            "Epoch 178: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1060, Test L1 Norm: 0.0411, Train Linf Norm: 41.0442, Test Linf Norm: 9.2531\n",
            "Epoch 179: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1057, Test L1 Norm: 0.0413, Train Linf Norm: 41.2649, Test Linf Norm: 9.3217\n",
            "Epoch 180: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1040, Test L1 Norm: 0.0406, Train Linf Norm: 41.1385, Test Linf Norm: 9.1140\n",
            "Epoch 181: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.1049, Test L1 Norm: 0.0409, Train Linf Norm: 41.9089, Test Linf Norm: 9.0478\n",
            "Epoch 182: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1051, Test L1 Norm: 0.0410, Train Linf Norm: 41.5532, Test Linf Norm: 9.1974\n",
            "Epoch 183: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1042, Test L1 Norm: 0.0410, Train Linf Norm: 37.1123, Test Linf Norm: 9.1783\n",
            "Epoch 184: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1029, Test L1 Norm: 0.0402, Train Linf Norm: 38.0053, Test Linf Norm: 9.0091\n",
            "Epoch 185: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1041, Test L1 Norm: 0.0403, Train Linf Norm: 40.9831, Test Linf Norm: 9.0453\n",
            "Epoch 186: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.1054, Test L1 Norm: 0.0403, Train Linf Norm: 41.6475, Test Linf Norm: 8.9090\n",
            "Epoch 187: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1006, Test L1 Norm: 0.0396, Train Linf Norm: 31.7599, Test Linf Norm: 8.8923\n",
            "Epoch 188: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0972, Test L1 Norm: 0.0390, Train Linf Norm: 33.0532, Test Linf Norm: 8.5552\n",
            "Epoch 189: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.1014, Test L1 Norm: 0.0411, Train Linf Norm: 37.8022, Test Linf Norm: 9.0806\n",
            "Epoch 190: Train Loss: 0.0001, Test Loss: 0.0004, Train L1 Norm: 0.1028, Test L1 Norm: 0.0394, Train Linf Norm: 39.8066, Test Linf Norm: 8.5857\n",
            "Epoch 191: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0984, Test L1 Norm: 0.0395, Train Linf Norm: 37.8304, Test Linf Norm: 8.7287\n",
            "Epoch 192: Train Loss: 0.0001, Test Loss: 0.0003, Train L1 Norm: 0.0992, Test L1 Norm: 0.0401, Train Linf Norm: 38.6252, Test Linf Norm: 8.5106\n",
            "Epoch 193: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.1029, Test L1 Norm: 0.0405, Train Linf Norm: 38.2236, Test Linf Norm: 8.8837\n",
            "Epoch 194: Train Loss: 0.0002, Test Loss: 0.0006, Train L1 Norm: 0.0986, Test L1 Norm: 0.0413, Train Linf Norm: 37.1361, Test Linf Norm: 8.8296\n",
            "Epoch 195: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0985, Test L1 Norm: 0.0389, Train Linf Norm: 37.7255, Test Linf Norm: 8.5078\n",
            "Epoch 196: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1006, Test L1 Norm: 0.0395, Train Linf Norm: 39.9934, Test Linf Norm: 8.7628\n",
            "Epoch 197: Train Loss: 0.0013, Test Loss: 0.0009, Train L1 Norm: 0.1133, Test L1 Norm: 0.0438, Train Linf Norm: 43.2023, Test Linf Norm: 8.7063\n",
            "Epoch 198: Train Loss: 0.0013, Test Loss: 0.0001, Train L1 Norm: 0.1186, Test L1 Norm: 0.0411, Train Linf Norm: 44.8665, Test Linf Norm: 9.1015\n",
            "Epoch 199: Train Loss: 0.0002, Test Loss: 0.0003, Train L1 Norm: 0.1020, Test L1 Norm: 0.0409, Train Linf Norm: 40.0038, Test Linf Norm: 8.7379\n",
            "Epoch 200: Train Loss: 0.0009, Test Loss: 0.0036, Train L1 Norm: 0.1178, Test L1 Norm: 0.0554, Train Linf Norm: 45.5486, Test Linf Norm: 9.8481\n",
            "Epoch 201: Train Loss: 0.0009, Test Loss: 0.0020, Train L1 Norm: 0.1224, Test L1 Norm: 0.0511, Train Linf Norm: 43.8760, Test Linf Norm: 10.0690\n",
            "Epoch 202: Train Loss: 0.0015, Test Loss: 0.0003, Train L1 Norm: 0.1077, Test L1 Norm: 0.0412, Train Linf Norm: 39.9819, Test Linf Norm: 8.8659\n",
            "Epoch 203: Train Loss: 0.0006, Test Loss: 0.0002, Train L1 Norm: 0.1145, Test L1 Norm: 0.0368, Train Linf Norm: 44.4839, Test Linf Norm: 8.0183\n",
            "Epoch 204: Train Loss: 0.0010, Test Loss: 0.0044, Train L1 Norm: 0.1047, Test L1 Norm: 0.0492, Train Linf Norm: 39.6282, Test Linf Norm: 8.0965\n",
            "Epoch 205: Train Loss: 0.0022, Test Loss: 0.0005, Train L1 Norm: 0.1463, Test L1 Norm: 0.0442, Train Linf Norm: 53.6208, Test Linf Norm: 9.4835\n",
            "Epoch 206: Train Loss: 0.0015, Test Loss: 0.0019, Train L1 Norm: 0.1078, Test L1 Norm: 0.0447, Train Linf Norm: 40.5622, Test Linf Norm: 8.3642\n",
            "Epoch 207: Train Loss: 0.0007, Test Loss: 0.0004, Train L1 Norm: 0.1287, Test L1 Norm: 0.0441, Train Linf Norm: 51.4684, Test Linf Norm: 9.4876\n",
            "Epoch 208: Train Loss: 0.0009, Test Loss: 0.0007, Train L1 Norm: 0.1107, Test L1 Norm: 0.0409, Train Linf Norm: 41.9571, Test Linf Norm: 8.6932\n",
            "Epoch 209: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.1161, Test L1 Norm: 0.0430, Train Linf Norm: 45.4191, Test Linf Norm: 9.0151\n",
            "Epoch 210: Train Loss: 0.0013, Test Loss: 0.0003, Train L1 Norm: 0.1099, Test L1 Norm: 0.0410, Train Linf Norm: 41.2877, Test Linf Norm: 9.0766\n",
            "Epoch 211: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.1080, Test L1 Norm: 0.0397, Train Linf Norm: 42.5709, Test Linf Norm: 8.7669\n",
            "Epoch 212: Train Loss: 0.0002, Test Loss: 0.0004, Train L1 Norm: 0.1078, Test L1 Norm: 0.0382, Train Linf Norm: 42.5706, Test Linf Norm: 8.1591\n",
            "Epoch 213: Train Loss: 0.0006, Test Loss: 0.0001, Train L1 Norm: 0.1112, Test L1 Norm: 0.0388, Train Linf Norm: 43.4377, Test Linf Norm: 8.6300\n",
            "Epoch 214: Train Loss: 0.0012, Test Loss: 0.0002, Train L1 Norm: 0.1178, Test L1 Norm: 0.0399, Train Linf Norm: 45.7996, Test Linf Norm: 8.7618\n",
            "Epoch 215: Train Loss: 0.0002, Test Loss: 0.0004, Train L1 Norm: 0.1092, Test L1 Norm: 0.0398, Train Linf Norm: 37.2172, Test Linf Norm: 8.6529\n",
            "Epoch 216: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.1078, Test L1 Norm: 0.0368, Train Linf Norm: 42.4564, Test Linf Norm: 8.0301\n",
            "Epoch 217: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.1111, Test L1 Norm: 0.0380, Train Linf Norm: 44.5242, Test Linf Norm: 8.4464\n",
            "Epoch 218: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.1026, Test L1 Norm: 0.0406, Train Linf Norm: 35.7725, Test Linf Norm: 8.7493\n",
            "Epoch 219: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.1041, Test L1 Norm: 0.0373, Train Linf Norm: 41.6859, Test Linf Norm: 8.2743\n",
            "Epoch 220: Train Loss: 0.0002, Test Loss: 0.0009, Train L1 Norm: 0.1043, Test L1 Norm: 0.0458, Train Linf Norm: 42.1513, Test Linf Norm: 8.8823\n",
            "Epoch 221: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0979, Test L1 Norm: 0.0367, Train Linf Norm: 37.7750, Test Linf Norm: 8.0852\n",
            "Epoch 222: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0976, Test L1 Norm: 0.0366, Train Linf Norm: 38.7147, Test Linf Norm: 8.0760\n",
            "Epoch 223: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0962, Test L1 Norm: 0.0357, Train Linf Norm: 37.7389, Test Linf Norm: 7.9643\n",
            "Epoch 224: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0953, Test L1 Norm: 0.0352, Train Linf Norm: 37.6800, Test Linf Norm: 7.8368\n",
            "Epoch 225: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0947, Test L1 Norm: 0.0361, Train Linf Norm: 37.7936, Test Linf Norm: 8.0093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 15:36:35,812]\u001b[0m Trial 0 finished with value: 0.03574055362939835 and parameters: {'n_layers': 3, 'n_units_0': 365, 'n_units_1': 854, 'n_units_2': 1151, 'hidden_activation': 'ReLU', 'output_activation': 'Linear', 'loss': 'Huber', 'optimizer': 'SGD', 'lr': 0.0032542528502947916, 'batch_size': 512, 'n_epochs': 226, 'scheduler': 'CosineAnnealingLR', 'weight_decay': 1.2725271593905619e-05, 'momentum': 0.9437574684803857, 't_max_fraction': 0.1509152001515959, 'eta_min': 0.00013935524070238977}. Best is trial 0 with value: 0.03574055362939835.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 226: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0960, Test L1 Norm: 0.0357, Train Linf Norm: 37.5839, Test Linf Norm: 7.9498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-e8f6ba3279cc>:139: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  gamma = trial.suggest_uniform(\"gamma\", 0.1, 0.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 114.3778, Test Loss: 3.4549, Train L1 Norm: 100.0016, Test L1 Norm: 1.0000, Train Linf Norm: 11473.8515, Test Linf Norm: 1.0000\n",
            "Epoch 2: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 3: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 4: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 5: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 6: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 7: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 8: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 9: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 10: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 11: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 12: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 13: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 14: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 15: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 16: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 17: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 18: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 19: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 20: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 21: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 22: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 23: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 24: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 25: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 26: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 27: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 28: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 29: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 30: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 31: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 32: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 33: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 34: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 35: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 36: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 37: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 38: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 39: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 40: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 41: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 42: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 43: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 44: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 45: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 46: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 47: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 48: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 49: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 50: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 51: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 52: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 53: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 54: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 55: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 56: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 57: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 58: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 59: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 60: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 61: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 62: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 63: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 64: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 65: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 66: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 67: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 68: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 69: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 70: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 71: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 72: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 73: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 74: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 75: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 76: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 77: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 78: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 79: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 80: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 81: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 82: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 83: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 84: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 85: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 86: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 87: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 88: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 89: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 90: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 91: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 92: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 93: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 94: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 95: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 96: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 97: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 98: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 99: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 100: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 101: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 102: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 103: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 104: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 105: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 106: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 107: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 108: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 109: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 110: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 111: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 112: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 113: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 114: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 115: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 116: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 117: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 118: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 119: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 120: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 121: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 122: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 123: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 124: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 125: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 126: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 127: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 128: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 129: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 130: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 131: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 132: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 133: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 134: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 135: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 136: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 137: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 138: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 139: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 140: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 141: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 142: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 143: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 144: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 145: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 146: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 147: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 148: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 149: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 150: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 151: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 152: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 153: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 154: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 155: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 156: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 157: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 158: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 159: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 160: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 161: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 162: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 163: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 164: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 165: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 166: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 167: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 168: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 169: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 170: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 171: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 172: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 173: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 174: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 175: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 176: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 177: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 178: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 179: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 180: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 181: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 182: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 183: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 184: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 185: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 186: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 187: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 188: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 189: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 190: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 191: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 192: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 193: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 194: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 195: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 196: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 197: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 198: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 199: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 200: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 201: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 202: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 203: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 204: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 205: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 206: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 207: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 208: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 15:40:21,400]\u001b[0m Trial 1 finished with value: 1.0 and parameters: {'n_layers': 3, 'n_units_0': 1076, 'n_units_1': 773, 'n_units_2': 86, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'MAE', 'optimizer': 'RMSprop', 'lr': 0.006365015122318576, 'batch_size': 1048, 'n_epochs': 209, 'scheduler': 'StepLR', 'step_size': 9, 'gamma': 0.4842299174922722}. Best is trial 0 with value: 0.03574055362939835.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 209: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-e8f6ba3279cc>:126: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
            "<ipython-input-28-e8f6ba3279cc>:127: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  beta1 = trial.suggest_uniform(\"beta1\", 0.9, 0.999)\n",
            "<ipython-input-28-e8f6ba3279cc>:128: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  beta2 = trial.suggest_uniform(\"beta2\", 0.999, 0.9999)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 4.1978, Test Loss: 3.0087, Train L1 Norm: 18.5197, Test L1 Norm: 1.0000, Train Linf Norm: 1076.0963, Test Linf Norm: 1.0000\n",
            "Epoch 2: Train Loss: 2.9638, Test Loss: 3.0087, Train L1 Norm: 1.0211, Test L1 Norm: 1.0000, Train Linf Norm: 2.3518, Test Linf Norm: 1.0000\n",
            "Epoch 3: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 4: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 5: Train Loss: 2.9641, Test Loss: 3.0087, Train L1 Norm: 1.0033, Test L1 Norm: 1.0000, Train Linf Norm: 1.2137, Test Linf Norm: 1.0000\n",
            "Epoch 6: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 7: Train Loss: 2.9637, Test Loss: 3.0087, Train L1 Norm: 1.0001, Test L1 Norm: 1.0000, Train Linf Norm: 1.0092, Test Linf Norm: 1.0000\n",
            "Epoch 8: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0001, Test L1 Norm: 1.0000, Train Linf Norm: 1.0072, Test Linf Norm: 1.0000\n",
            "Epoch 9: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 10: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 11: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 12: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 13: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 14: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 15: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 16: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 17: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 18: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 19: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 20: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 21: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 22: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 23: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 24: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 25: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 26: Train Loss: 2.9357, Test Loss: 3.0087, Train L1 Norm: 1.7387, Test L1 Norm: 1.0000, Train Linf Norm: 45.3669, Test Linf Norm: 1.0000\n",
            "Epoch 27: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.1312, Test L1 Norm: 1.0000, Train Linf Norm: 9.3959, Test Linf Norm: 1.0000\n",
            "Epoch 28: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0011, Test L1 Norm: 1.0000, Train Linf Norm: 1.0609, Test Linf Norm: 1.0000\n",
            "Epoch 29: Train Loss: 2.9525, Test Loss: 3.0087, Train L1 Norm: 1.1285, Test L1 Norm: 1.0000, Train Linf Norm: 8.1859, Test Linf Norm: 1.0000\n",
            "Epoch 30: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 31: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 32: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 33: Train Loss: 2.9626, Test Loss: 3.0087, Train L1 Norm: 1.0476, Test L1 Norm: 1.0000, Train Linf Norm: 2.7541, Test Linf Norm: 1.0000\n",
            "Epoch 34: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 35: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 36: Train Loss: 2.9628, Test Loss: 3.0087, Train L1 Norm: 1.0030, Test L1 Norm: 1.0000, Train Linf Norm: 1.0366, Test Linf Norm: 1.0000\n",
            "Epoch 37: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 38: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 39: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 40: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 41: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 42: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 43: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 44: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 45: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 46: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 47: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 48: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 49: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 50: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 51: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 52: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 53: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 54: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 55: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 56: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 57: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 58: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 59: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 60: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 61: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 62: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 63: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 64: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 65: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 66: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 67: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 68: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 69: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 70: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 71: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 72: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 73: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 74: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 75: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 76: Train Loss: 1.9868, Test Loss: 0.2521, Train L1 Norm: 1.5269, Test L1 Norm: 0.3722, Train Linf Norm: 42.4660, Test Linf Norm: 3.7650\n",
            "Epoch 77: Train Loss: 0.2224, Test Loss: 0.2442, Train L1 Norm: 0.3744, Test L1 Norm: 0.3490, Train Linf Norm: 6.6589, Test Linf Norm: 5.6389\n",
            "Epoch 78: Train Loss: 0.2116, Test Loss: 0.2114, Train L1 Norm: 0.3625, Test L1 Norm: 0.3522, Train Linf Norm: 7.3151, Test Linf Norm: 7.1590\n",
            "Epoch 79: Train Loss: 0.2092, Test Loss: 0.2097, Train L1 Norm: 0.3615, Test L1 Norm: 0.3559, Train Linf Norm: 7.5023, Test Linf Norm: 7.7232\n",
            "Epoch 80: Train Loss: 0.2064, Test Loss: 0.2051, Train L1 Norm: 0.3620, Test L1 Norm: 0.3394, Train Linf Norm: 7.7517, Test Linf Norm: 6.7889\n",
            "Epoch 81: Train Loss: 0.2014, Test Loss: 0.1982, Train L1 Norm: 0.3523, Test L1 Norm: 0.3368, Train Linf Norm: 7.2214, Test Linf Norm: 6.5897\n",
            "Epoch 82: Train Loss: 0.1946, Test Loss: 0.1915, Train L1 Norm: 0.3421, Test L1 Norm: 0.3337, Train Linf Norm: 6.6377, Test Linf Norm: 6.4094\n",
            "Epoch 83: Train Loss: 0.1874, Test Loss: 0.1880, Train L1 Norm: 0.3310, Test L1 Norm: 0.3222, Train Linf Norm: 6.0354, Test Linf Norm: 5.8671\n",
            "Epoch 84: Train Loss: 0.1833, Test Loss: 0.1838, Train L1 Norm: 0.3198, Test L1 Norm: 0.3113, Train Linf Norm: 5.3849, Test Linf Norm: 5.2613\n",
            "Epoch 85: Train Loss: 0.1816, Test Loss: 0.1834, Train L1 Norm: 0.3124, Test L1 Norm: 0.3076, Train Linf Norm: 5.1176, Test Linf Norm: 5.0665\n",
            "Epoch 86: Train Loss: 0.1804, Test Loss: 0.1821, Train L1 Norm: 0.3119, Test L1 Norm: 0.3003, Train Linf Norm: 5.2068, Test Linf Norm: 4.9810\n",
            "Epoch 87: Train Loss: 0.1804, Test Loss: 0.1861, Train L1 Norm: 0.3100, Test L1 Norm: 0.3113, Train Linf Norm: 5.2399, Test Linf Norm: 5.5791\n",
            "Epoch 88: Train Loss: 0.1798, Test Loss: 0.1817, Train L1 Norm: 0.3083, Test L1 Norm: 0.2965, Train Linf Norm: 5.1863, Test Linf Norm: 4.9769\n",
            "Epoch 89: Train Loss: 0.1782, Test Loss: 0.1803, Train L1 Norm: 0.3074, Test L1 Norm: 0.3015, Train Linf Norm: 5.2943, Test Linf Norm: 5.1534\n",
            "Epoch 90: Train Loss: 0.1779, Test Loss: 0.1804, Train L1 Norm: 0.3084, Test L1 Norm: 0.2985, Train Linf Norm: 5.2487, Test Linf Norm: 5.0375\n",
            "Epoch 91: Train Loss: 0.1780, Test Loss: 0.1807, Train L1 Norm: 0.3063, Test L1 Norm: 0.2974, Train Linf Norm: 5.3147, Test Linf Norm: 4.9011\n",
            "Epoch 92: Train Loss: 0.1777, Test Loss: 0.1798, Train L1 Norm: 0.3069, Test L1 Norm: 0.2993, Train Linf Norm: 5.3361, Test Linf Norm: 5.1282\n",
            "Epoch 93: Train Loss: 0.1777, Test Loss: 0.1801, Train L1 Norm: 0.3063, Test L1 Norm: 0.3018, Train Linf Norm: 5.3286, Test Linf Norm: 5.2172\n",
            "Epoch 94: Train Loss: 0.1775, Test Loss: 0.1795, Train L1 Norm: 0.3064, Test L1 Norm: 0.2980, Train Linf Norm: 5.3487, Test Linf Norm: 5.0675\n",
            "Epoch 95: Train Loss: 0.1773, Test Loss: 0.1795, Train L1 Norm: 0.3063, Test L1 Norm: 0.2981, Train Linf Norm: 5.3340, Test Linf Norm: 5.2374\n",
            "Epoch 96: Train Loss: 0.1774, Test Loss: 0.1796, Train L1 Norm: 0.3079, Test L1 Norm: 0.2962, Train Linf Norm: 5.5556, Test Linf Norm: 5.0477\n",
            "Epoch 97: Train Loss: 0.1772, Test Loss: 0.1799, Train L1 Norm: 0.3063, Test L1 Norm: 0.2968, Train Linf Norm: 5.4741, Test Linf Norm: 5.0578\n",
            "Epoch 98: Train Loss: 0.1772, Test Loss: 0.1799, Train L1 Norm: 0.3059, Test L1 Norm: 0.2963, Train Linf Norm: 5.4068, Test Linf Norm: 5.0148\n",
            "Epoch 99: Train Loss: 0.1773, Test Loss: 0.1791, Train L1 Norm: 0.3050, Test L1 Norm: 0.2966, Train Linf Norm: 5.4145, Test Linf Norm: 5.1198\n",
            "Epoch 100: Train Loss: 0.1767, Test Loss: 0.1799, Train L1 Norm: 0.3046, Test L1 Norm: 0.2956, Train Linf Norm: 5.3846, Test Linf Norm: 5.0613\n",
            "Epoch 101: Train Loss: 0.1767, Test Loss: 0.1790, Train L1 Norm: 0.3072, Test L1 Norm: 0.2961, Train Linf Norm: 5.5701, Test Linf Norm: 5.1366\n",
            "Epoch 102: Train Loss: 0.1767, Test Loss: 0.1790, Train L1 Norm: 0.3076, Test L1 Norm: 0.2962, Train Linf Norm: 5.6005, Test Linf Norm: 5.1203\n",
            "Epoch 103: Train Loss: 0.1767, Test Loss: 0.1790, Train L1 Norm: 0.3043, Test L1 Norm: 0.2959, Train Linf Norm: 5.3729, Test Linf Norm: 5.1286\n",
            "Epoch 104: Train Loss: 0.1766, Test Loss: 0.1790, Train L1 Norm: 0.3062, Test L1 Norm: 0.2958, Train Linf Norm: 5.5074, Test Linf Norm: 5.1284\n",
            "Epoch 105: Train Loss: 0.1766, Test Loss: 0.1791, Train L1 Norm: 0.3055, Test L1 Norm: 0.2956, Train Linf Norm: 5.5072, Test Linf Norm: 5.1126\n",
            "Epoch 106: Train Loss: 0.1765, Test Loss: 0.1792, Train L1 Norm: 0.3075, Test L1 Norm: 0.2950, Train Linf Norm: 5.6612, Test Linf Norm: 5.0493\n",
            "Epoch 107: Train Loss: 0.1765, Test Loss: 0.1792, Train L1 Norm: 0.3054, Test L1 Norm: 0.2948, Train Linf Norm: 5.4911, Test Linf Norm: 5.0517\n",
            "Epoch 108: Train Loss: 0.1765, Test Loss: 0.1792, Train L1 Norm: 0.3057, Test L1 Norm: 0.2958, Train Linf Norm: 5.4894, Test Linf Norm: 5.1374\n",
            "Epoch 109: Train Loss: 0.1764, Test Loss: 0.1789, Train L1 Norm: 0.3053, Test L1 Norm: 0.2962, Train Linf Norm: 5.5103, Test Linf Norm: 5.1972\n",
            "Epoch 110: Train Loss: 0.1764, Test Loss: 0.1794, Train L1 Norm: 0.3057, Test L1 Norm: 0.2948, Train Linf Norm: 5.5332, Test Linf Norm: 5.0698\n",
            "Epoch 111: Train Loss: 0.1763, Test Loss: 0.1788, Train L1 Norm: 0.3055, Test L1 Norm: 0.2955, Train Linf Norm: 5.5368, Test Linf Norm: 5.1625\n",
            "Epoch 112: Train Loss: 0.1763, Test Loss: 0.1787, Train L1 Norm: 0.3056, Test L1 Norm: 0.2962, Train Linf Norm: 5.5277, Test Linf Norm: 5.1942\n",
            "Epoch 113: Train Loss: 0.1763, Test Loss: 0.1788, Train L1 Norm: 0.3055, Test L1 Norm: 0.2959, Train Linf Norm: 5.5582, Test Linf Norm: 5.1774\n",
            "Epoch 114: Train Loss: 0.1762, Test Loss: 0.1788, Train L1 Norm: 0.3059, Test L1 Norm: 0.2953, Train Linf Norm: 5.4995, Test Linf Norm: 5.1471\n",
            "Epoch 115: Train Loss: 0.1763, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2956, Train Linf Norm: 5.5285, Test Linf Norm: 5.1788\n",
            "Epoch 116: Train Loss: 0.1762, Test Loss: 0.1787, Train L1 Norm: 0.3063, Test L1 Norm: 0.2958, Train Linf Norm: 5.6432, Test Linf Norm: 5.1837\n",
            "Epoch 117: Train Loss: 0.1762, Test Loss: 0.1788, Train L1 Norm: 0.3052, Test L1 Norm: 0.2967, Train Linf Norm: 5.5478, Test Linf Norm: 5.2312\n",
            "Epoch 118: Train Loss: 0.1762, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2953, Train Linf Norm: 5.5708, Test Linf Norm: 5.1572\n",
            "Epoch 119: Train Loss: 0.1762, Test Loss: 0.1787, Train L1 Norm: 0.3055, Test L1 Norm: 0.2955, Train Linf Norm: 5.5129, Test Linf Norm: 5.1684\n",
            "Epoch 120: Train Loss: 0.1762, Test Loss: 0.1787, Train L1 Norm: 0.3055, Test L1 Norm: 0.2953, Train Linf Norm: 5.5331, Test Linf Norm: 5.1502\n",
            "Epoch 121: Train Loss: 0.1762, Test Loss: 0.1787, Train L1 Norm: 0.3058, Test L1 Norm: 0.2952, Train Linf Norm: 5.5994, Test Linf Norm: 5.1542\n",
            "Epoch 122: Train Loss: 0.1761, Test Loss: 0.1789, Train L1 Norm: 0.3057, Test L1 Norm: 0.2949, Train Linf Norm: 5.5580, Test Linf Norm: 5.1259\n",
            "Epoch 123: Train Loss: 0.1762, Test Loss: 0.1787, Train L1 Norm: 0.3053, Test L1 Norm: 0.2953, Train Linf Norm: 5.5501, Test Linf Norm: 5.1619\n",
            "Epoch 124: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2953, Train Linf Norm: 5.5597, Test Linf Norm: 5.1612\n",
            "Epoch 125: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3052, Test L1 Norm: 0.2954, Train Linf Norm: 5.5585, Test Linf Norm: 5.1670\n",
            "Epoch 126: Train Loss: 0.1761, Test Loss: 0.1789, Train L1 Norm: 0.3054, Test L1 Norm: 0.2947, Train Linf Norm: 5.5787, Test Linf Norm: 5.1168\n",
            "Epoch 127: Train Loss: 0.1761, Test Loss: 0.1788, Train L1 Norm: 0.3054, Test L1 Norm: 0.2950, Train Linf Norm: 5.5748, Test Linf Norm: 5.1377\n",
            "Epoch 128: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2952, Train Linf Norm: 5.5727, Test Linf Norm: 5.1532\n",
            "Epoch 129: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3056, Test L1 Norm: 0.2951, Train Linf Norm: 5.4946, Test Linf Norm: 5.1534\n",
            "Epoch 130: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3052, Test L1 Norm: 0.2953, Train Linf Norm: 5.5171, Test Linf Norm: 5.1696\n",
            "Epoch 131: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3055, Test L1 Norm: 0.2951, Train Linf Norm: 5.5715, Test Linf Norm: 5.1558\n",
            "Epoch 132: Train Loss: 0.1761, Test Loss: 0.1786, Train L1 Norm: 0.3057, Test L1 Norm: 0.2954, Train Linf Norm: 5.6177, Test Linf Norm: 5.1797\n",
            "Epoch 133: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3058, Test L1 Norm: 0.2951, Train Linf Norm: 5.5591, Test Linf Norm: 5.1554\n",
            "Epoch 134: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3053, Test L1 Norm: 0.2951, Train Linf Norm: 5.5182, Test Linf Norm: 5.1531\n",
            "Epoch 135: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3056, Test L1 Norm: 0.2950, Train Linf Norm: 5.6113, Test Linf Norm: 5.1465\n",
            "Epoch 136: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3053, Test L1 Norm: 0.2951, Train Linf Norm: 5.5502, Test Linf Norm: 5.1552\n",
            "Epoch 137: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3053, Test L1 Norm: 0.2951, Train Linf Norm: 5.5752, Test Linf Norm: 5.1547\n",
            "Epoch 138: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5691, Test Linf Norm: 5.1581\n",
            "Epoch 139: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3055, Test L1 Norm: 0.2950, Train Linf Norm: 5.4352, Test Linf Norm: 5.1473\n",
            "Epoch 140: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3053, Test L1 Norm: 0.2951, Train Linf Norm: 5.5762, Test Linf Norm: 5.1550\n",
            "Epoch 141: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3053, Test L1 Norm: 0.2951, Train Linf Norm: 5.5496, Test Linf Norm: 5.1598\n",
            "Epoch 142: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3055, Test L1 Norm: 0.2952, Train Linf Norm: 5.5591, Test Linf Norm: 5.1626\n",
            "Epoch 143: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3055, Test L1 Norm: 0.2951, Train Linf Norm: 5.5576, Test Linf Norm: 5.1587\n",
            "Epoch 144: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3055, Test L1 Norm: 0.2951, Train Linf Norm: 5.6141, Test Linf Norm: 5.1563\n",
            "Epoch 145: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.4468, Test Linf Norm: 5.1550\n",
            "Epoch 146: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5281, Test Linf Norm: 5.1561\n",
            "Epoch 147: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5425, Test Linf Norm: 5.1543\n",
            "Epoch 148: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5407, Test Linf Norm: 5.1553\n",
            "Epoch 149: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3053, Test L1 Norm: 0.2951, Train Linf Norm: 5.5511, Test Linf Norm: 5.1557\n",
            "Epoch 150: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5759, Test Linf Norm: 5.1568\n",
            "Epoch 151: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3053, Test L1 Norm: 0.2951, Train Linf Norm: 5.5344, Test Linf Norm: 5.1565\n",
            "Epoch 152: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5872, Test Linf Norm: 5.1554\n",
            "Epoch 153: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5219, Test Linf Norm: 5.1564\n",
            "Epoch 154: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5848, Test Linf Norm: 5.1554\n",
            "Epoch 155: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.6081, Test Linf Norm: 5.1556\n",
            "Epoch 156: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5561, Test Linf Norm: 5.1552\n",
            "Epoch 157: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.6116, Test Linf Norm: 5.1552\n",
            "Epoch 158: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5071, Test Linf Norm: 5.1550\n",
            "Epoch 159: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5555, Test Linf Norm: 5.1553\n",
            "Epoch 160: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5959, Test Linf Norm: 5.1551\n",
            "Epoch 161: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5599, Test Linf Norm: 5.1553\n",
            "Epoch 162: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5519, Test Linf Norm: 5.1552\n",
            "Epoch 163: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.4999, Test Linf Norm: 5.1545\n",
            "Epoch 164: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5774, Test Linf Norm: 5.1550\n",
            "Epoch 165: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2950, Train Linf Norm: 5.5353, Test Linf Norm: 5.1544\n",
            "Epoch 166: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2950, Train Linf Norm: 5.5513, Test Linf Norm: 5.1545\n",
            "Epoch 167: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2950, Train Linf Norm: 5.5671, Test Linf Norm: 5.1545\n",
            "Epoch 168: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5226, Test Linf Norm: 5.1546\n",
            "Epoch 169: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5638, Test Linf Norm: 5.1547\n",
            "Epoch 170: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5232, Test Linf Norm: 5.1546\n",
            "Epoch 171: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5291, Test Linf Norm: 5.1548\n",
            "Epoch 172: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5913, Test Linf Norm: 5.1550\n",
            "Epoch 173: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5707, Test Linf Norm: 5.1549\n",
            "Epoch 174: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.4116, Test Linf Norm: 5.1547\n",
            "Epoch 175: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5761, Test Linf Norm: 5.1550\n",
            "Epoch 176: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5857, Test Linf Norm: 5.1550\n",
            "Epoch 177: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5895, Test Linf Norm: 5.1550\n",
            "Epoch 178: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5354, Test Linf Norm: 5.1550\n",
            "Epoch 179: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5985, Test Linf Norm: 5.1550\n",
            "Epoch 180: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5327, Test Linf Norm: 5.1551\n",
            "Epoch 181: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5829, Test Linf Norm: 5.1551\n",
            "Epoch 182: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5275, Test Linf Norm: 5.1551\n",
            "Epoch 183: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5789, Test Linf Norm: 5.1551\n",
            "Epoch 184: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5586, Test Linf Norm: 5.1551\n",
            "Epoch 185: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5857, Test Linf Norm: 5.1551\n",
            "Epoch 186: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5778, Test Linf Norm: 5.1551\n",
            "Epoch 187: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5118, Test Linf Norm: 5.1552\n",
            "Epoch 188: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5918, Test Linf Norm: 5.1552\n",
            "Epoch 189: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5895, Test Linf Norm: 5.1552\n",
            "Epoch 190: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5877, Test Linf Norm: 5.1552\n",
            "Epoch 191: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5561, Test Linf Norm: 5.1552\n",
            "Epoch 192: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5007, Test Linf Norm: 5.1552\n",
            "Epoch 193: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5884, Test Linf Norm: 5.1552\n",
            "Epoch 194: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5647, Test Linf Norm: 5.1552\n",
            "Epoch 195: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5457, Test Linf Norm: 5.1552\n",
            "Epoch 196: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5746, Test Linf Norm: 5.1552\n",
            "Epoch 197: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5806, Test Linf Norm: 5.1552\n",
            "Epoch 198: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5941, Test Linf Norm: 5.1552\n",
            "Epoch 199: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5887, Test Linf Norm: 5.1552\n",
            "Epoch 200: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5686, Test Linf Norm: 5.1552\n",
            "Epoch 201: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5416, Test Linf Norm: 5.1552\n",
            "Epoch 202: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5625, Test Linf Norm: 5.1552\n",
            "Epoch 203: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5640, Test Linf Norm: 5.1552\n",
            "Epoch 204: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5835, Test Linf Norm: 5.1552\n",
            "Epoch 205: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5499, Test Linf Norm: 5.1552\n",
            "Epoch 206: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5674, Test Linf Norm: 5.1552\n",
            "Epoch 207: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.4812, Test Linf Norm: 5.1552\n",
            "Epoch 208: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5742, Test Linf Norm: 5.1552\n",
            "Epoch 209: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5477, Test Linf Norm: 5.1552\n",
            "Epoch 210: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5664, Test Linf Norm: 5.1552\n",
            "Epoch 211: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5291, Test Linf Norm: 5.1552\n",
            "Epoch 212: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5748, Test Linf Norm: 5.1552\n",
            "Epoch 213: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5500, Test Linf Norm: 5.1552\n",
            "Epoch 214: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5932, Test Linf Norm: 5.1552\n",
            "Epoch 215: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5699, Test Linf Norm: 5.1552\n",
            "Epoch 216: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5640, Test Linf Norm: 5.1552\n",
            "Epoch 217: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5812, Test Linf Norm: 5.1552\n",
            "Epoch 218: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5716, Test Linf Norm: 5.1552\n",
            "Epoch 219: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.4913, Test Linf Norm: 5.1552\n",
            "Epoch 220: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5988, Test Linf Norm: 5.1552\n",
            "Epoch 221: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5781, Test Linf Norm: 5.1552\n",
            "Epoch 222: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5644, Test Linf Norm: 5.1552\n",
            "Epoch 223: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5807, Test Linf Norm: 5.1552\n",
            "Epoch 224: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.6096, Test Linf Norm: 5.1552\n",
            "Epoch 225: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5911, Test Linf Norm: 5.1552\n",
            "Epoch 226: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5829, Test Linf Norm: 5.1552\n",
            "Epoch 227: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5752, Test Linf Norm: 5.1552\n",
            "Epoch 228: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5683, Test Linf Norm: 5.1552\n",
            "Epoch 229: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5877, Test Linf Norm: 5.1552\n",
            "Epoch 230: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.4821, Test Linf Norm: 5.1552\n",
            "Epoch 231: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5673, Test Linf Norm: 5.1552\n",
            "Epoch 232: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5635, Test Linf Norm: 5.1552\n",
            "Epoch 233: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5867, Test Linf Norm: 5.1552\n",
            "Epoch 234: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5134, Test Linf Norm: 5.1552\n",
            "Epoch 235: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5829, Test Linf Norm: 5.1552\n",
            "Epoch 236: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5184, Test Linf Norm: 5.1552\n",
            "Epoch 237: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5933, Test Linf Norm: 5.1552\n",
            "Epoch 238: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5910, Test Linf Norm: 5.1552\n",
            "Epoch 239: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5821, Test Linf Norm: 5.1552\n",
            "Epoch 240: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5901, Test Linf Norm: 5.1552\n",
            "Epoch 241: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5542, Test Linf Norm: 5.1552\n",
            "Epoch 242: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5654, Test Linf Norm: 5.1552\n",
            "Epoch 243: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5918, Test Linf Norm: 5.1552\n",
            "Epoch 244: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.4825, Test Linf Norm: 5.1552\n",
            "Epoch 245: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5881, Test Linf Norm: 5.1552\n",
            "Epoch 246: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5713, Test Linf Norm: 5.1552\n",
            "Epoch 247: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.6054, Test Linf Norm: 5.1552\n",
            "Epoch 248: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5864, Test Linf Norm: 5.1552\n",
            "Epoch 249: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5260, Test Linf Norm: 5.1552\n",
            "Epoch 250: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5463, Test Linf Norm: 5.1552\n",
            "Epoch 251: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5460, Test Linf Norm: 5.1552\n",
            "Epoch 252: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5812, Test Linf Norm: 5.1552\n",
            "Epoch 253: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5693, Test Linf Norm: 5.1552\n",
            "Epoch 254: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.6138, Test Linf Norm: 5.1552\n",
            "Epoch 255: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5343, Test Linf Norm: 5.1552\n",
            "Epoch 256: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5565, Test Linf Norm: 5.1552\n",
            "Epoch 257: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5726, Test Linf Norm: 5.1552\n",
            "Epoch 258: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5398, Test Linf Norm: 5.1552\n",
            "Epoch 259: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5679, Test Linf Norm: 5.1552\n",
            "Epoch 260: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5917, Test Linf Norm: 5.1552\n",
            "Epoch 261: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5599, Test Linf Norm: 5.1552\n",
            "Epoch 262: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5308, Test Linf Norm: 5.1552\n",
            "Epoch 263: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5764, Test Linf Norm: 5.1552\n",
            "Epoch 264: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5294, Test Linf Norm: 5.1552\n",
            "Epoch 265: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5149, Test Linf Norm: 5.1552\n",
            "Epoch 266: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.4994, Test Linf Norm: 5.1552\n",
            "Epoch 267: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5725, Test Linf Norm: 5.1552\n",
            "Epoch 268: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5488, Test Linf Norm: 5.1552\n",
            "Epoch 269: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.4817, Test Linf Norm: 5.1552\n",
            "Epoch 270: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5376, Test Linf Norm: 5.1552\n",
            "Epoch 271: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5824, Test Linf Norm: 5.1552\n",
            "Epoch 272: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5040, Test Linf Norm: 5.1552\n",
            "Epoch 273: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5407, Test Linf Norm: 5.1552\n",
            "Epoch 274: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5056, Test Linf Norm: 5.1552\n",
            "Epoch 275: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5626, Test Linf Norm: 5.1552\n",
            "Epoch 276: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.6107, Test Linf Norm: 5.1552\n",
            "Epoch 277: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5543, Test Linf Norm: 5.1552\n",
            "Epoch 278: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5671, Test Linf Norm: 5.1552\n",
            "Epoch 279: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5581, Test Linf Norm: 5.1552\n",
            "Epoch 280: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5310, Test Linf Norm: 5.1552\n",
            "Epoch 281: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5523, Test Linf Norm: 5.1552\n",
            "Epoch 282: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.4939, Test Linf Norm: 5.1552\n",
            "Epoch 283: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5176, Test Linf Norm: 5.1552\n",
            "Epoch 284: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5802, Test Linf Norm: 5.1552\n",
            "Epoch 285: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5742, Test Linf Norm: 5.1552\n",
            "Epoch 286: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5710, Test Linf Norm: 5.1552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 16:14:31,316]\u001b[0m Trial 2 finished with value: 0.29505610167980195 and parameters: {'n_layers': 4, 'n_units_0': 1667, 'n_units_1': 1807, 'n_units_2': 1576, 'n_units_3': 857, 'hidden_activation': 'ELU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adam', 'lr': 0.005313437330538165, 'batch_size': 64, 'n_epochs': 287, 'scheduler': 'StepLR', 'weight_decay': 0.00018220191095773226, 'beta1': 0.9769970678001817, 'beta2': 0.9995286378678427, 'step_size': 11, 'gamma': 0.3384850447450514}. Best is trial 0 with value: 0.03574055362939835.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 287: Train Loss: 0.1761, Test Loss: 0.1787, Train L1 Norm: 0.3054, Test L1 Norm: 0.2951, Train Linf Norm: 5.5402, Test Linf Norm: 5.1552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-e8f6ba3279cc>:157: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  factor = trial.suggest_uniform(\"factor\", 0.1, 0.5)\n",
            "<ipython-input-28-e8f6ba3279cc>:159: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  threshold = trial.suggest_loguniform(\"threshold\", 1e-4, 1e-2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.0995, Test Loss: 0.1855, Train L1 Norm: 0.2442, Test L1 Norm: 0.2235, Train Linf Norm: 19.6973, Test Linf Norm: 2.0883\n",
            "Epoch 2: Train Loss: 0.0156, Test Loss: 0.0377, Train L1 Norm: 0.1074, Test L1 Norm: 0.1222, Train Linf Norm: 6.2133, Test Linf Norm: 2.8156\n",
            "Epoch 3: Train Loss: 0.0103, Test Loss: 0.0075, Train L1 Norm: 0.0982, Test L1 Norm: 0.0714, Train Linf Norm: 7.5515, Test Linf Norm: 5.6198\n",
            "Epoch 4: Train Loss: 0.0133, Test Loss: 0.0054, Train L1 Norm: 0.0959, Test L1 Norm: 0.0670, Train Linf Norm: 7.2946, Test Linf Norm: 3.9030\n",
            "Epoch 5: Train Loss: 0.0065, Test Loss: 0.0038, Train L1 Norm: 0.1503, Test L1 Norm: 0.0749, Train Linf Norm: 25.8867, Test Linf Norm: 8.0659\n",
            "Epoch 6: Train Loss: 0.0071, Test Loss: 0.0037, Train L1 Norm: 0.1052, Test L1 Norm: 0.0523, Train Linf Norm: 14.8369, Test Linf Norm: 4.9674\n",
            "Epoch 7: Train Loss: 0.0079, Test Loss: 0.0774, Train L1 Norm: 0.0845, Test L1 Norm: 0.2218, Train Linf Norm: 10.2916, Test Linf Norm: 16.1360\n",
            "Epoch 8: Train Loss: 0.0089, Test Loss: 0.1032, Train L1 Norm: 0.0671, Test L1 Norm: 0.2292, Train Linf Norm: 6.1534, Test Linf Norm: 16.7490\n",
            "Epoch 9: Train Loss: 0.0126, Test Loss: 0.0069, Train L1 Norm: 0.1076, Test L1 Norm: 0.0590, Train Linf Norm: 14.8227, Test Linf Norm: 5.6641\n",
            "Epoch 10: Train Loss: 0.0064, Test Loss: 0.0064, Train L1 Norm: 0.0641, Test L1 Norm: 0.0515, Train Linf Norm: 6.5931, Test Linf Norm: 5.2152\n",
            "Epoch 11: Train Loss: 0.0089, Test Loss: 0.0118, Train L1 Norm: 0.0674, Test L1 Norm: 0.0773, Train Linf Norm: 6.4404, Test Linf Norm: 7.4755\n",
            "Epoch 12: Train Loss: 0.0051, Test Loss: 0.0172, Train L1 Norm: 0.0606, Test L1 Norm: 0.0689, Train Linf Norm: 6.4208, Test Linf Norm: 4.0254\n",
            "Epoch 13: Train Loss: 0.0067, Test Loss: 0.0053, Train L1 Norm: 0.0654, Test L1 Norm: 0.0514, Train Linf Norm: 7.5074, Test Linf Norm: 5.3971\n",
            "Epoch 14: Train Loss: 0.0063, Test Loss: 0.0043, Train L1 Norm: 0.0605, Test L1 Norm: 0.0557, Train Linf Norm: 6.1901, Test Linf Norm: 6.0884\n",
            "Epoch 15: Train Loss: 0.0051, Test Loss: 0.0057, Train L1 Norm: 0.0600, Test L1 Norm: 0.0536, Train Linf Norm: 6.9333, Test Linf Norm: 5.8200\n",
            "Epoch 16: Train Loss: 0.0071, Test Loss: 0.0096, Train L1 Norm: 0.1356, Test L1 Norm: 0.0688, Train Linf Norm: 25.1778, Test Linf Norm: 6.1901\n",
            "Epoch 17: Train Loss: 0.0048, Test Loss: 0.0034, Train L1 Norm: 0.0623, Test L1 Norm: 0.0482, Train Linf Norm: 6.8195, Test Linf Norm: 5.3515\n",
            "Epoch 18: Train Loss: 0.0040, Test Loss: 0.0040, Train L1 Norm: 0.0600, Test L1 Norm: 0.0668, Train Linf Norm: 7.0223, Test Linf Norm: 7.4811\n",
            "Epoch 19: Train Loss: 0.0073, Test Loss: 0.0066, Train L1 Norm: 0.0670, Test L1 Norm: 0.0576, Train Linf Norm: 7.4744, Test Linf Norm: 4.3013\n",
            "Epoch 20: Train Loss: 0.0044, Test Loss: 0.0061, Train L1 Norm: 0.0622, Test L1 Norm: 0.0503, Train Linf Norm: 7.6138, Test Linf Norm: 4.8489\n",
            "Epoch 21: Train Loss: 0.0044, Test Loss: 0.0042, Train L1 Norm: 0.0626, Test L1 Norm: 0.0467, Train Linf Norm: 7.3513, Test Linf Norm: 4.9958\n",
            "Epoch 22: Train Loss: 0.0064, Test Loss: 0.0019, Train L1 Norm: 0.0649, Test L1 Norm: 0.0438, Train Linf Norm: 7.5220, Test Linf Norm: 5.2446\n",
            "Epoch 23: Train Loss: 0.0054, Test Loss: 0.0033, Train L1 Norm: 0.0610, Test L1 Norm: 0.0473, Train Linf Norm: 6.7205, Test Linf Norm: 5.2155\n",
            "Epoch 24: Train Loss: 0.0036, Test Loss: 0.0040, Train L1 Norm: 0.0578, Test L1 Norm: 0.0557, Train Linf Norm: 6.9565, Test Linf Norm: 6.0549\n",
            "Epoch 25: Train Loss: 0.0058, Test Loss: 0.0022, Train L1 Norm: 0.0623, Test L1 Norm: 0.0460, Train Linf Norm: 7.4046, Test Linf Norm: 5.4511\n",
            "Epoch 26: Train Loss: 0.0052, Test Loss: 0.0037, Train L1 Norm: 0.0574, Test L1 Norm: 0.0493, Train Linf Norm: 6.1067, Test Linf Norm: 5.2796\n",
            "Epoch 27: Train Loss: 0.0056, Test Loss: 0.0019, Train L1 Norm: 0.0609, Test L1 Norm: 0.0442, Train Linf Norm: 6.8777, Test Linf Norm: 5.5035\n",
            "Epoch 28: Train Loss: 0.0036, Test Loss: 0.0034, Train L1 Norm: 0.0564, Test L1 Norm: 0.0610, Train Linf Norm: 6.3326, Test Linf Norm: 6.6762\n",
            "Epoch 29: Train Loss: 0.0052, Test Loss: 0.0022, Train L1 Norm: 0.0591, Test L1 Norm: 0.0446, Train Linf Norm: 6.3267, Test Linf Norm: 5.4167\n",
            "Epoch 30: Train Loss: 0.0044, Test Loss: 0.0053, Train L1 Norm: 0.0568, Test L1 Norm: 0.0630, Train Linf Norm: 6.2635, Test Linf Norm: 6.3377\n",
            "Epoch 31: Train Loss: 0.0038, Test Loss: 0.0025, Train L1 Norm: 0.0569, Test L1 Norm: 0.0473, Train Linf Norm: 6.6518, Test Linf Norm: 4.7135\n",
            "Epoch 32: Train Loss: 0.0028, Test Loss: 0.0019, Train L1 Norm: 0.0547, Test L1 Norm: 0.0456, Train Linf Norm: 6.4287, Test Linf Norm: 5.5296\n",
            "Epoch 33: Train Loss: 0.0044, Test Loss: 0.0044, Train L1 Norm: 0.0572, Test L1 Norm: 0.0556, Train Linf Norm: 6.6827, Test Linf Norm: 3.8649\n",
            "Epoch 34: Train Loss: 0.0039, Test Loss: 0.0030, Train L1 Norm: 0.0556, Test L1 Norm: 0.0480, Train Linf Norm: 6.3265, Test Linf Norm: 4.1483\n",
            "Epoch 35: Train Loss: 0.0052, Test Loss: 0.0041, Train L1 Norm: 0.0603, Test L1 Norm: 0.0524, Train Linf Norm: 6.5630, Test Linf Norm: 4.1828\n",
            "Epoch 36: Train Loss: 0.0033, Test Loss: 0.0045, Train L1 Norm: 0.0549, Test L1 Norm: 0.0618, Train Linf Norm: 6.4052, Test Linf Norm: 6.7501\n",
            "Epoch 37: Train Loss: 0.0031, Test Loss: 0.0018, Train L1 Norm: 0.0563, Test L1 Norm: 0.0431, Train Linf Norm: 6.9935, Test Linf Norm: 5.2581\n",
            "Epoch 38: Train Loss: 0.0045, Test Loss: 0.0027, Train L1 Norm: 0.0576, Test L1 Norm: 0.0470, Train Linf Norm: 6.6559, Test Linf Norm: 4.8196\n",
            "Epoch 39: Train Loss: 0.0046, Test Loss: 0.0019, Train L1 Norm: 0.0558, Test L1 Norm: 0.0432, Train Linf Norm: 6.1778, Test Linf Norm: 5.0074\n",
            "Epoch 40: Train Loss: 0.0035, Test Loss: 0.0143, Train L1 Norm: 0.0563, Test L1 Norm: 0.1001, Train Linf Norm: 6.8703, Test Linf Norm: 8.8686\n",
            "Epoch 41: Train Loss: 0.0055, Test Loss: 0.0028, Train L1 Norm: 0.0582, Test L1 Norm: 0.0472, Train Linf Norm: 6.5346, Test Linf Norm: 4.3068\n",
            "Epoch 42: Train Loss: 0.0022, Test Loss: 0.0133, Train L1 Norm: 0.0526, Test L1 Norm: 0.0682, Train Linf Norm: 6.4943, Test Linf Norm: 5.9989\n",
            "Epoch 43: Train Loss: 0.0045, Test Loss: 0.0021, Train L1 Norm: 0.0574, Test L1 Norm: 0.0441, Train Linf Norm: 6.4197, Test Linf Norm: 4.9069\n",
            "Epoch 44: Train Loss: 0.0031, Test Loss: 0.0034, Train L1 Norm: 0.0545, Test L1 Norm: 0.0497, Train Linf Norm: 6.5292, Test Linf Norm: 4.7769\n",
            "Epoch 45: Train Loss: 0.0031, Test Loss: 0.0018, Train L1 Norm: 0.0605, Test L1 Norm: 0.0415, Train Linf Norm: 8.1977, Test Linf Norm: 4.7329\n",
            "Epoch 46: Train Loss: 0.0029, Test Loss: 0.0019, Train L1 Norm: 0.0542, Test L1 Norm: 0.0423, Train Linf Norm: 6.6291, Test Linf Norm: 4.8866\n",
            "Epoch 47: Train Loss: 0.0042, Test Loss: 0.0087, Train L1 Norm: 0.0596, Test L1 Norm: 0.0751, Train Linf Norm: 7.1729, Test Linf Norm: 6.8857\n",
            "Epoch 48: Train Loss: 0.0047, Test Loss: 0.0023, Train L1 Norm: 0.0588, Test L1 Norm: 0.0462, Train Linf Norm: 6.7283, Test Linf Norm: 5.2315\n",
            "Epoch 49: Train Loss: 0.0033, Test Loss: 0.0016, Train L1 Norm: 0.0947, Test L1 Norm: 0.0414, Train Linf Norm: 16.6511, Test Linf Norm: 4.5427\n",
            "Epoch 50: Train Loss: 0.0026, Test Loss: 0.0018, Train L1 Norm: 0.0541, Test L1 Norm: 0.0429, Train Linf Norm: 6.6841, Test Linf Norm: 5.1823\n",
            "Epoch 51: Train Loss: 0.0063, Test Loss: 0.0021, Train L1 Norm: 0.0594, Test L1 Norm: 0.0453, Train Linf Norm: 6.5074, Test Linf Norm: 3.9724\n",
            "Epoch 52: Train Loss: 0.0019, Test Loss: 0.0017, Train L1 Norm: 0.0528, Test L1 Norm: 0.0424, Train Linf Norm: 6.5387, Test Linf Norm: 5.0540\n",
            "Epoch 53: Train Loss: 0.0036, Test Loss: 0.0038, Train L1 Norm: 0.0700, Test L1 Norm: 0.0480, Train Linf Norm: 10.5441, Test Linf Norm: 4.8972\n",
            "Epoch 54: Train Loss: 0.0026, Test Loss: 0.0029, Train L1 Norm: 0.0613, Test L1 Norm: 0.0475, Train Linf Norm: 8.5872, Test Linf Norm: 4.3335\n",
            "Epoch 55: Train Loss: 0.0034, Test Loss: 0.0096, Train L1 Norm: 0.0567, Test L1 Norm: 0.0635, Train Linf Norm: 6.9186, Test Linf Norm: 5.8353\n",
            "Epoch 56: Train Loss: 0.0027, Test Loss: 0.0015, Train L1 Norm: 0.0558, Test L1 Norm: 0.0412, Train Linf Norm: 6.8414, Test Linf Norm: 4.2409\n",
            "Epoch 57: Train Loss: 0.0026, Test Loss: 0.0016, Train L1 Norm: 0.0550, Test L1 Norm: 0.0404, Train Linf Norm: 6.9238, Test Linf Norm: 4.6934\n",
            "Epoch 58: Train Loss: 0.0021, Test Loss: 0.0063, Train L1 Norm: 0.0538, Test L1 Norm: 0.0498, Train Linf Norm: 6.6947, Test Linf Norm: 4.7673\n",
            "Epoch 59: Train Loss: 0.0035, Test Loss: 0.0055, Train L1 Norm: 0.0590, Test L1 Norm: 0.0556, Train Linf Norm: 7.4562, Test Linf Norm: 5.1213\n",
            "Epoch 60: Train Loss: 0.0035, Test Loss: 0.0036, Train L1 Norm: 0.0555, Test L1 Norm: 0.0485, Train Linf Norm: 6.6433, Test Linf Norm: 3.7415\n",
            "Epoch 61: Train Loss: 0.0028, Test Loss: 0.0038, Train L1 Norm: 0.0533, Test L1 Norm: 0.0446, Train Linf Norm: 6.2037, Test Linf Norm: 4.4638\n",
            "Epoch 62: Train Loss: 0.0036, Test Loss: 0.0018, Train L1 Norm: 0.0566, Test L1 Norm: 0.0404, Train Linf Norm: 7.0106, Test Linf Norm: 4.3989\n",
            "Epoch 63: Train Loss: 0.0033, Test Loss: 0.0017, Train L1 Norm: 0.0595, Test L1 Norm: 0.0391, Train Linf Norm: 7.2459, Test Linf Norm: 4.5833\n",
            "Epoch 64: Train Loss: 0.0035, Test Loss: 0.0078, Train L1 Norm: 0.0561, Test L1 Norm: 0.0538, Train Linf Norm: 6.8783, Test Linf Norm: 3.5801\n",
            "Epoch 65: Train Loss: 0.0029, Test Loss: 0.0155, Train L1 Norm: 0.0554, Test L1 Norm: 0.0740, Train Linf Norm: 7.0485, Test Linf Norm: 5.9126\n",
            "Epoch 66: Train Loss: 0.0036, Test Loss: 0.0015, Train L1 Norm: 0.0616, Test L1 Norm: 0.0395, Train Linf Norm: 7.8970, Test Linf Norm: 4.6272\n",
            "Epoch 67: Train Loss: 0.0024, Test Loss: 0.0030, Train L1 Norm: 0.0714, Test L1 Norm: 0.0426, Train Linf Norm: 10.8765, Test Linf Norm: 4.2738\n",
            "Epoch 68: Train Loss: 0.0024, Test Loss: 0.0023, Train L1 Norm: 0.0525, Test L1 Norm: 0.0411, Train Linf Norm: 6.6251, Test Linf Norm: 4.4693\n",
            "Epoch 69: Train Loss: 0.0030, Test Loss: 0.0016, Train L1 Norm: 0.0592, Test L1 Norm: 0.0378, Train Linf Norm: 8.0635, Test Linf Norm: 4.1965\n",
            "Epoch 70: Train Loss: 0.0023, Test Loss: 0.0015, Train L1 Norm: 0.0517, Test L1 Norm: 0.0371, Train Linf Norm: 6.6289, Test Linf Norm: 3.9227\n",
            "Epoch 71: Train Loss: 0.0026, Test Loss: 0.0021, Train L1 Norm: 0.0533, Test L1 Norm: 0.0388, Train Linf Norm: 6.8514, Test Linf Norm: 3.9733\n",
            "Epoch 72: Train Loss: 0.0022, Test Loss: 0.0025, Train L1 Norm: 0.0516, Test L1 Norm: 0.0395, Train Linf Norm: 6.3340, Test Linf Norm: 3.9212\n",
            "Epoch 73: Train Loss: 0.0038, Test Loss: 0.0206, Train L1 Norm: 0.0540, Test L1 Norm: 0.0695, Train Linf Norm: 6.5737, Test Linf Norm: 3.1218\n",
            "Epoch 74: Train Loss: 0.0032, Test Loss: 0.0015, Train L1 Norm: 0.0541, Test L1 Norm: 0.0363, Train Linf Norm: 6.7281, Test Linf Norm: 3.6933\n",
            "Epoch 75: Train Loss: 0.0031, Test Loss: 0.0018, Train L1 Norm: 0.0532, Test L1 Norm: 0.0378, Train Linf Norm: 6.1750, Test Linf Norm: 3.5699\n",
            "Epoch 76: Train Loss: 0.0039, Test Loss: 0.0026, Train L1 Norm: 0.0594, Test L1 Norm: 0.0409, Train Linf Norm: 7.5232, Test Linf Norm: 3.7956\n",
            "Epoch 77: Train Loss: 0.0023, Test Loss: 0.0026, Train L1 Norm: 0.0662, Test L1 Norm: 0.0401, Train Linf Norm: 10.4442, Test Linf Norm: 4.3737\n",
            "Epoch 78: Train Loss: 0.0034, Test Loss: 0.0016, Train L1 Norm: 0.0574, Test L1 Norm: 0.0362, Train Linf Norm: 7.5910, Test Linf Norm: 3.8599\n",
            "Epoch 79: Train Loss: 0.0026, Test Loss: 0.0014, Train L1 Norm: 0.0552, Test L1 Norm: 0.0347, Train Linf Norm: 7.5028, Test Linf Norm: 3.5605\n",
            "Epoch 80: Train Loss: 0.0040, Test Loss: 0.0033, Train L1 Norm: 0.0594, Test L1 Norm: 0.0526, Train Linf Norm: 7.4047, Test Linf Norm: 5.4087\n",
            "Epoch 81: Train Loss: 0.0023, Test Loss: 0.0016, Train L1 Norm: 0.0531, Test L1 Norm: 0.0366, Train Linf Norm: 7.0251, Test Linf Norm: 3.7239\n",
            "Epoch 82: Train Loss: 0.0024, Test Loss: 0.0035, Train L1 Norm: 0.0544, Test L1 Norm: 0.0455, Train Linf Norm: 7.2891, Test Linf Norm: 4.5420\n",
            "Epoch 83: Train Loss: 0.0031, Test Loss: 0.0018, Train L1 Norm: 0.0542, Test L1 Norm: 0.0355, Train Linf Norm: 6.8708, Test Linf Norm: 3.6036\n",
            "Epoch 84: Train Loss: 0.0021, Test Loss: 0.0031, Train L1 Norm: 0.0542, Test L1 Norm: 0.0435, Train Linf Norm: 7.5161, Test Linf Norm: 4.3528\n",
            "Epoch 85: Train Loss: 0.0022, Test Loss: 0.0015, Train L1 Norm: 0.0533, Test L1 Norm: 0.0385, Train Linf Norm: 7.1310, Test Linf Norm: 4.2651\n",
            "Epoch 86: Train Loss: 0.0027, Test Loss: 0.0079, Train L1 Norm: 0.0589, Test L1 Norm: 0.0415, Train Linf Norm: 8.2180, Test Linf Norm: 3.3229\n",
            "Epoch 87: Train Loss: 0.0021, Test Loss: 0.0014, Train L1 Norm: 0.0529, Test L1 Norm: 0.0335, Train Linf Norm: 6.9258, Test Linf Norm: 3.4097\n",
            "Epoch 88: Train Loss: 0.0022, Test Loss: 0.0028, Train L1 Norm: 0.0598, Test L1 Norm: 0.0462, Train Linf Norm: 8.5744, Test Linf Norm: 4.6577\n",
            "Epoch 89: Train Loss: 0.0026, Test Loss: 0.0014, Train L1 Norm: 0.0579, Test L1 Norm: 0.0357, Train Linf Norm: 8.1224, Test Linf Norm: 3.9484\n",
            "Epoch 90: Train Loss: 0.0026, Test Loss: 0.0034, Train L1 Norm: 0.0552, Test L1 Norm: 0.0461, Train Linf Norm: 7.6008, Test Linf Norm: 4.4106\n",
            "Epoch 91: Train Loss: 0.0012, Test Loss: 0.0014, Train L1 Norm: 0.0502, Test L1 Norm: 0.0350, Train Linf Norm: 7.5112, Test Linf Norm: 3.7242\n",
            "Epoch 92: Train Loss: 0.0013, Test Loss: 0.0025, Train L1 Norm: 0.0499, Test L1 Norm: 0.0349, Train Linf Norm: 7.4698, Test Linf Norm: 3.0350\n",
            "Epoch 93: Train Loss: 0.0013, Test Loss: 0.0015, Train L1 Norm: 0.0493, Test L1 Norm: 0.0365, Train Linf Norm: 6.8814, Test Linf Norm: 3.8952\n",
            "Epoch 94: Train Loss: 0.0012, Test Loss: 0.0058, Train L1 Norm: 0.0476, Test L1 Norm: 0.0470, Train Linf Norm: 6.9282, Test Linf Norm: 4.1311\n",
            "Epoch 95: Train Loss: 0.0013, Test Loss: 0.0013, Train L1 Norm: 0.0573, Test L1 Norm: 0.0325, Train Linf Norm: 9.1738, Test Linf Norm: 3.3332\n",
            "Epoch 96: Train Loss: 0.0013, Test Loss: 0.0018, Train L1 Norm: 0.0497, Test L1 Norm: 0.0371, Train Linf Norm: 7.2406, Test Linf Norm: 3.7486\n",
            "Epoch 97: Train Loss: 0.0011, Test Loss: 0.0014, Train L1 Norm: 0.0484, Test L1 Norm: 0.0354, Train Linf Norm: 7.2274, Test Linf Norm: 3.8164\n",
            "Epoch 98: Train Loss: 0.0012, Test Loss: 0.0013, Train L1 Norm: 0.0494, Test L1 Norm: 0.0317, Train Linf Norm: 7.4655, Test Linf Norm: 3.0564\n",
            "Epoch 99: Train Loss: 0.0012, Test Loss: 0.0016, Train L1 Norm: 0.0478, Test L1 Norm: 0.0344, Train Linf Norm: 6.7731, Test Linf Norm: 3.6449\n",
            "Epoch 100: Train Loss: 0.0013, Test Loss: 0.0012, Train L1 Norm: 0.0492, Test L1 Norm: 0.0318, Train Linf Norm: 7.1616, Test Linf Norm: 3.3450\n",
            "Epoch 101: Train Loss: 0.0013, Test Loss: 0.0018, Train L1 Norm: 0.0491, Test L1 Norm: 0.0335, Train Linf Norm: 7.2525, Test Linf Norm: 3.3475\n",
            "Epoch 102: Train Loss: 0.0013, Test Loss: 0.0012, Train L1 Norm: 0.0486, Test L1 Norm: 0.0303, Train Linf Norm: 7.0336, Test Linf Norm: 3.1054\n",
            "Epoch 103: Train Loss: 0.0014, Test Loss: 0.0020, Train L1 Norm: 0.0631, Test L1 Norm: 0.0367, Train Linf Norm: 10.6926, Test Linf Norm: 3.8014\n",
            "Epoch 104: Train Loss: 0.0012, Test Loss: 0.0012, Train L1 Norm: 0.0490, Test L1 Norm: 0.0314, Train Linf Norm: 7.3817, Test Linf Norm: 3.1821\n",
            "Epoch 105: Train Loss: 0.0013, Test Loss: 0.0030, Train L1 Norm: 0.0481, Test L1 Norm: 0.0452, Train Linf Norm: 7.0501, Test Linf Norm: 4.2961\n",
            "Epoch 106: Train Loss: 0.0013, Test Loss: 0.0012, Train L1 Norm: 0.0466, Test L1 Norm: 0.0293, Train Linf Norm: 6.2457, Test Linf Norm: 2.8874\n",
            "Epoch 107: Train Loss: 0.0014, Test Loss: 0.0015, Train L1 Norm: 0.0481, Test L1 Norm: 0.0324, Train Linf Norm: 7.1982, Test Linf Norm: 3.0177\n",
            "Epoch 108: Train Loss: 0.0011, Test Loss: 0.0018, Train L1 Norm: 0.0478, Test L1 Norm: 0.0399, Train Linf Norm: 7.3369, Test Linf Norm: 3.9666\n",
            "Epoch 109: Train Loss: 0.0013, Test Loss: 0.0013, Train L1 Norm: 0.0503, Test L1 Norm: 0.0313, Train Linf Norm: 7.6467, Test Linf Norm: 2.9799\n",
            "Epoch 110: Train Loss: 0.0014, Test Loss: 0.0023, Train L1 Norm: 0.0497, Test L1 Norm: 0.0353, Train Linf Norm: 7.4112, Test Linf Norm: 3.3035\n",
            "Epoch 111: Train Loss: 0.0013, Test Loss: 0.0036, Train L1 Norm: 0.0477, Test L1 Norm: 0.0407, Train Linf Norm: 7.0049, Test Linf Norm: 3.6768\n",
            "Epoch 112: Train Loss: 0.0012, Test Loss: 0.0011, Train L1 Norm: 0.0463, Test L1 Norm: 0.0300, Train Linf Norm: 6.7402, Test Linf Norm: 3.2630\n",
            "Epoch 113: Train Loss: 0.0012, Test Loss: 0.0044, Train L1 Norm: 0.0474, Test L1 Norm: 0.0366, Train Linf Norm: 7.0316, Test Linf Norm: 3.9236\n",
            "Epoch 114: Train Loss: 0.0017, Test Loss: 0.0012, Train L1 Norm: 0.0507, Test L1 Norm: 0.0319, Train Linf Norm: 7.4509, Test Linf Norm: 3.5506\n",
            "Epoch 115: Train Loss: 0.0015, Test Loss: 0.0013, Train L1 Norm: 0.0472, Test L1 Norm: 0.0285, Train Linf Norm: 6.7670, Test Linf Norm: 2.8261\n",
            "Epoch 116: Train Loss: 0.0014, Test Loss: 0.0094, Train L1 Norm: 0.0512, Test L1 Norm: 0.0765, Train Linf Norm: 7.1901, Test Linf Norm: 5.9357\n",
            "Epoch 117: Train Loss: 0.0019, Test Loss: 0.0015, Train L1 Norm: 0.0488, Test L1 Norm: 0.0314, Train Linf Norm: 6.7121, Test Linf Norm: 3.3348\n",
            "Epoch 118: Train Loss: 0.0014, Test Loss: 0.0013, Train L1 Norm: 0.0484, Test L1 Norm: 0.0308, Train Linf Norm: 7.1228, Test Linf Norm: 3.2265\n",
            "Epoch 119: Train Loss: 0.0012, Test Loss: 0.0017, Train L1 Norm: 0.0485, Test L1 Norm: 0.0321, Train Linf Norm: 7.3457, Test Linf Norm: 2.4211\n",
            "Epoch 120: Train Loss: 0.0013, Test Loss: 0.0012, Train L1 Norm: 0.0447, Test L1 Norm: 0.0295, Train Linf Norm: 6.1989, Test Linf Norm: 2.8787\n",
            "Epoch 121: Train Loss: 0.0013, Test Loss: 0.0015, Train L1 Norm: 0.0473, Test L1 Norm: 0.0315, Train Linf Norm: 6.8769, Test Linf Norm: 3.2551\n",
            "Epoch 122: Train Loss: 0.0014, Test Loss: 0.0021, Train L1 Norm: 0.0483, Test L1 Norm: 0.0324, Train Linf Norm: 7.0212, Test Linf Norm: 2.8884\n",
            "Epoch 123: Train Loss: 0.0015, Test Loss: 0.0012, Train L1 Norm: 0.0470, Test L1 Norm: 0.0288, Train Linf Norm: 6.5350, Test Linf Norm: 3.1115\n",
            "Epoch 124: Train Loss: 0.0010, Test Loss: 0.0012, Train L1 Norm: 0.0457, Test L1 Norm: 0.0310, Train Linf Norm: 6.6679, Test Linf Norm: 3.3008\n",
            "Epoch 125: Train Loss: 0.0009, Test Loss: 0.0011, Train L1 Norm: 0.0454, Test L1 Norm: 0.0294, Train Linf Norm: 6.9658, Test Linf Norm: 3.1554\n",
            "Epoch 126: Train Loss: 0.0010, Test Loss: 0.0012, Train L1 Norm: 0.0460, Test L1 Norm: 0.0310, Train Linf Norm: 7.1424, Test Linf Norm: 3.2740\n",
            "Epoch 127: Train Loss: 0.0009, Test Loss: 0.0013, Train L1 Norm: 0.0446, Test L1 Norm: 0.0298, Train Linf Norm: 6.6216, Test Linf Norm: 3.0846\n",
            "Epoch 128: Train Loss: 0.0009, Test Loss: 0.0011, Train L1 Norm: 0.0451, Test L1 Norm: 0.0287, Train Linf Norm: 6.9403, Test Linf Norm: 2.8765\n",
            "Epoch 129: Train Loss: 0.0010, Test Loss: 0.0012, Train L1 Norm: 0.0453, Test L1 Norm: 0.0287, Train Linf Norm: 6.9114, Test Linf Norm: 2.7651\n",
            "Epoch 130: Train Loss: 0.0009, Test Loss: 0.0011, Train L1 Norm: 0.0446, Test L1 Norm: 0.0289, Train Linf Norm: 6.6744, Test Linf Norm: 2.8670\n",
            "Epoch 131: Train Loss: 0.0009, Test Loss: 0.0013, Train L1 Norm: 0.0457, Test L1 Norm: 0.0299, Train Linf Norm: 7.1015, Test Linf Norm: 3.1024\n",
            "Epoch 132: Train Loss: 0.0009, Test Loss: 0.0015, Train L1 Norm: 0.0442, Test L1 Norm: 0.0297, Train Linf Norm: 6.3679, Test Linf Norm: 2.7344\n",
            "Epoch 133: Train Loss: 0.0010, Test Loss: 0.0012, Train L1 Norm: 0.0463, Test L1 Norm: 0.0287, Train Linf Norm: 7.2506, Test Linf Norm: 2.7291\n",
            "Epoch 134: Train Loss: 0.0009, Test Loss: 0.0011, Train L1 Norm: 0.0443, Test L1 Norm: 0.0281, Train Linf Norm: 6.3829, Test Linf Norm: 2.7068\n",
            "Epoch 135: Train Loss: 0.0010, Test Loss: 0.0011, Train L1 Norm: 0.0450, Test L1 Norm: 0.0286, Train Linf Norm: 6.7780, Test Linf Norm: 2.9306\n",
            "Epoch 136: Train Loss: 0.0009, Test Loss: 0.0012, Train L1 Norm: 0.0441, Test L1 Norm: 0.0313, Train Linf Norm: 6.5582, Test Linf Norm: 3.5530\n",
            "Epoch 137: Train Loss: 0.0010, Test Loss: 0.0010, Train L1 Norm: 0.0462, Test L1 Norm: 0.0290, Train Linf Norm: 7.2658, Test Linf Norm: 3.0224\n",
            "Epoch 138: Train Loss: 0.0009, Test Loss: 0.0011, Train L1 Norm: 0.0448, Test L1 Norm: 0.0293, Train Linf Norm: 6.9042, Test Linf Norm: 3.1488\n",
            "Epoch 139: Train Loss: 0.0010, Test Loss: 0.0011, Train L1 Norm: 0.0457, Test L1 Norm: 0.0282, Train Linf Norm: 7.0560, Test Linf Norm: 2.9307\n",
            "Epoch 140: Train Loss: 0.0009, Test Loss: 0.0019, Train L1 Norm: 0.0445, Test L1 Norm: 0.0330, Train Linf Norm: 6.8856, Test Linf Norm: 3.6934\n",
            "Epoch 141: Train Loss: 0.0009, Test Loss: 0.0011, Train L1 Norm: 0.0455, Test L1 Norm: 0.0282, Train Linf Norm: 7.0208, Test Linf Norm: 2.8301\n",
            "Epoch 142: Train Loss: 0.0009, Test Loss: 0.0014, Train L1 Norm: 0.0466, Test L1 Norm: 0.0284, Train Linf Norm: 7.2685, Test Linf Norm: 2.9652\n",
            "Epoch 143: Train Loss: 0.0009, Test Loss: 0.0011, Train L1 Norm: 0.0466, Test L1 Norm: 0.0278, Train Linf Norm: 7.2613, Test Linf Norm: 2.8513\n",
            "Epoch 144: Train Loss: 0.0010, Test Loss: 0.0012, Train L1 Norm: 0.0440, Test L1 Norm: 0.0302, Train Linf Norm: 6.5388, Test Linf Norm: 3.1366\n",
            "Epoch 145: Train Loss: 0.0009, Test Loss: 0.0014, Train L1 Norm: 0.0446, Test L1 Norm: 0.0290, Train Linf Norm: 6.9049, Test Linf Norm: 2.8801\n",
            "Epoch 146: Train Loss: 0.0009, Test Loss: 0.0014, Train L1 Norm: 0.0436, Test L1 Norm: 0.0294, Train Linf Norm: 6.4956, Test Linf Norm: 2.5782\n",
            "Epoch 147: Train Loss: 0.0011, Test Loss: 0.0014, Train L1 Norm: 0.0624, Test L1 Norm: 0.0281, Train Linf Norm: 11.3712, Test Linf Norm: 2.7582\n",
            "Epoch 148: Train Loss: 0.0009, Test Loss: 0.0012, Train L1 Norm: 0.0466, Test L1 Norm: 0.0302, Train Linf Norm: 7.2074, Test Linf Norm: 3.2672\n",
            "Epoch 149: Train Loss: 0.0008, Test Loss: 0.0010, Train L1 Norm: 0.0450, Test L1 Norm: 0.0281, Train Linf Norm: 7.0480, Test Linf Norm: 2.9434\n",
            "Epoch 150: Train Loss: 0.0008, Test Loss: 0.0011, Train L1 Norm: 0.0440, Test L1 Norm: 0.0293, Train Linf Norm: 6.5668, Test Linf Norm: 3.1643\n",
            "Epoch 151: Train Loss: 0.0008, Test Loss: 0.0011, Train L1 Norm: 0.0444, Test L1 Norm: 0.0279, Train Linf Norm: 6.9723, Test Linf Norm: 2.8735\n",
            "Epoch 152: Train Loss: 0.0008, Test Loss: 0.0011, Train L1 Norm: 0.0435, Test L1 Norm: 0.0283, Train Linf Norm: 6.2615, Test Linf Norm: 2.9995\n",
            "Epoch 153: Train Loss: 0.0008, Test Loss: 0.0014, Train L1 Norm: 0.0443, Test L1 Norm: 0.0288, Train Linf Norm: 6.7810, Test Linf Norm: 2.8181\n",
            "Epoch 154: Train Loss: 0.0008, Test Loss: 0.0010, Train L1 Norm: 0.0440, Test L1 Norm: 0.0278, Train Linf Norm: 6.9688, Test Linf Norm: 2.9028\n",
            "Epoch 155: Train Loss: 0.0008, Test Loss: 0.0013, Train L1 Norm: 0.0442, Test L1 Norm: 0.0316, Train Linf Norm: 6.9178, Test Linf Norm: 3.3568\n",
            "Epoch 156: Train Loss: 0.0009, Test Loss: 0.0012, Train L1 Norm: 0.0440, Test L1 Norm: 0.0293, Train Linf Norm: 6.7428, Test Linf Norm: 3.1075\n",
            "Epoch 157: Train Loss: 0.0008, Test Loss: 0.0010, Train L1 Norm: 0.0438, Test L1 Norm: 0.0280, Train Linf Norm: 6.6757, Test Linf Norm: 3.0304\n",
            "Epoch 158: Train Loss: 0.0008, Test Loss: 0.0011, Train L1 Norm: 0.0492, Test L1 Norm: 0.0298, Train Linf Norm: 8.2464, Test Linf Norm: 3.2511\n",
            "Epoch 159: Train Loss: 0.0008, Test Loss: 0.0010, Train L1 Norm: 0.0438, Test L1 Norm: 0.0279, Train Linf Norm: 6.8087, Test Linf Norm: 2.8948\n",
            "Epoch 160: Train Loss: 0.0008, Test Loss: 0.0012, Train L1 Norm: 0.0436, Test L1 Norm: 0.0330, Train Linf Norm: 6.7570, Test Linf Norm: 3.6395\n",
            "Epoch 161: Train Loss: 0.0008, Test Loss: 0.0014, Train L1 Norm: 0.0432, Test L1 Norm: 0.0342, Train Linf Norm: 6.6332, Test Linf Norm: 3.6347\n",
            "Epoch 162: Train Loss: 0.0008, Test Loss: 0.0010, Train L1 Norm: 0.0449, Test L1 Norm: 0.0284, Train Linf Norm: 7.1305, Test Linf Norm: 3.0128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 16:18:40,213]\u001b[0m Trial 3 finished with value: 0.028729132837057112 and parameters: {'n_layers': 3, 'n_units_0': 1148, 'n_units_1': 855, 'n_units_2': 42, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adam', 'lr': 0.00037336346060584007, 'batch_size': 256, 'n_epochs': 163, 'scheduler': 'ReduceLROnPlateau', 'weight_decay': 0.0030629480995270843, 'beta1': 0.9134952507504207, 'beta2': 0.9997704130618683, 'factor': 0.34962558510865055, 'patience': 10, 'threshold': 0.00412574764640008}. Best is trial 3 with value: 0.028729132837057112.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 163: Train Loss: 0.0008, Test Loss: 0.0011, Train L1 Norm: 0.0434, Test L1 Norm: 0.0287, Train Linf Norm: 6.7534, Test Linf Norm: 3.0556\n",
            "Epoch 1: Train Loss: 1.3407, Test Loss: 0.9171, Train L1 Norm: 4.6354, Test L1 Norm: 1.4682, Train Linf Norm: 1714.2988, Test Linf Norm: 226.7664\n",
            "Epoch 2: Train Loss: 0.4724, Test Loss: 0.2700, Train L1 Norm: 2.6557, Test L1 Norm: 0.3676, Train Linf Norm: 1073.2926, Test Linf Norm: 70.4574\n",
            "Epoch 3: Train Loss: 0.2268, Test Loss: 0.1511, Train L1 Norm: 0.8008, Test L1 Norm: 0.2791, Train Linf Norm: 293.5904, Test Linf Norm: 58.0147\n",
            "Epoch 4: Train Loss: 0.2520, Test Loss: 0.2520, Train L1 Norm: 0.6730, Test L1 Norm: 0.2614, Train Linf Norm: 225.5951, Test Linf Norm: 48.8976\n",
            "Epoch 5: Train Loss: 0.2111, Test Loss: 0.2134, Train L1 Norm: 0.5827, Test L1 Norm: 0.3044, Train Linf Norm: 202.2112, Test Linf Norm: 54.9352\n",
            "Epoch 6: Train Loss: 0.2194, Test Loss: 0.1260, Train L1 Norm: 0.5651, Test L1 Norm: 0.2347, Train Linf Norm: 187.4056, Test Linf Norm: 46.3212\n",
            "Epoch 7: Train Loss: 0.1923, Test Loss: 0.2756, Train L1 Norm: 0.5473, Test L1 Norm: 0.2091, Train Linf Norm: 201.6139, Test Linf Norm: 34.8958\n",
            "Epoch 8: Train Loss: 0.1992, Test Loss: 0.2263, Train L1 Norm: 0.4779, Test L1 Norm: 0.1911, Train Linf Norm: 162.4429, Test Linf Norm: 32.6566\n",
            "Epoch 9: Train Loss: 0.1953, Test Loss: 0.0883, Train L1 Norm: 0.6187, Test L1 Norm: 0.1867, Train Linf Norm: 241.1371, Test Linf Norm: 37.4304\n",
            "Epoch 10: Train Loss: 0.1482, Test Loss: 0.2991, Train L1 Norm: 0.4749, Test L1 Norm: 0.2874, Train Linf Norm: 166.5867, Test Linf Norm: 42.7154\n",
            "Epoch 11: Train Loss: 0.1771, Test Loss: 0.0794, Train L1 Norm: 0.4285, Test L1 Norm: 0.1361, Train Linf Norm: 152.8253, Test Linf Norm: 28.4495\n",
            "Epoch 12: Train Loss: 0.1687, Test Loss: 0.0841, Train L1 Norm: 0.4016, Test L1 Norm: 0.1791, Train Linf Norm: 140.1340, Test Linf Norm: 34.1041\n",
            "Epoch 13: Train Loss: 0.1733, Test Loss: 0.0639, Train L1 Norm: 0.3784, Test L1 Norm: 0.1404, Train Linf Norm: 123.0344, Test Linf Norm: 29.1991\n",
            "Epoch 14: Train Loss: 0.0560, Test Loss: 0.0575, Train L1 Norm: 0.2732, Test L1 Norm: 0.1193, Train Linf Norm: 87.7836, Test Linf Norm: 24.0531\n",
            "Epoch 15: Train Loss: 0.0591, Test Loss: 0.1065, Train L1 Norm: 0.2369, Test L1 Norm: 0.1126, Train Linf Norm: 85.8480, Test Linf Norm: 20.8519\n",
            "Epoch 16: Train Loss: 0.0633, Test Loss: 0.0443, Train L1 Norm: 0.2397, Test L1 Norm: 0.1128, Train Linf Norm: 87.5715, Test Linf Norm: 23.3206\n",
            "Epoch 17: Train Loss: 0.0609, Test Loss: 0.0392, Train L1 Norm: 0.2641, Test L1 Norm: 0.0990, Train Linf Norm: 98.1217, Test Linf Norm: 21.6536\n",
            "Epoch 18: Train Loss: 0.1134, Test Loss: 0.0859, Train L1 Norm: 0.3291, Test L1 Norm: 0.1107, Train Linf Norm: 124.1196, Test Linf Norm: 22.1413\n",
            "Epoch 19: Train Loss: 0.0704, Test Loss: 0.0751, Train L1 Norm: 0.2499, Test L1 Norm: 0.1221, Train Linf Norm: 90.4181, Test Linf Norm: 23.1842\n",
            "Epoch 20: Train Loss: 0.0608, Test Loss: 0.0426, Train L1 Norm: 0.2594, Test L1 Norm: 0.1019, Train Linf Norm: 98.7709, Test Linf Norm: 21.3658\n",
            "Epoch 21: Train Loss: 0.0816, Test Loss: 0.0373, Train L1 Norm: 0.2593, Test L1 Norm: 0.1046, Train Linf Norm: 94.9025, Test Linf Norm: 21.9754\n",
            "Epoch 22: Train Loss: 0.0574, Test Loss: 0.1119, Train L1 Norm: 0.2286, Test L1 Norm: 0.1055, Train Linf Norm: 74.9394, Test Linf Norm: 18.8655\n",
            "Epoch 23: Train Loss: 0.0926, Test Loss: 0.0350, Train L1 Norm: 0.2816, Test L1 Norm: 0.0918, Train Linf Norm: 103.6446, Test Linf Norm: 20.2154\n",
            "Epoch 24: Train Loss: 0.0633, Test Loss: 0.1363, Train L1 Norm: 0.2493, Test L1 Norm: 0.1536, Train Linf Norm: 94.6068, Test Linf Norm: 24.7363\n",
            "Epoch 25: Train Loss: 0.0647, Test Loss: 0.0955, Train L1 Norm: 0.2337, Test L1 Norm: 0.1204, Train Linf Norm: 87.0475, Test Linf Norm: 21.3424\n",
            "Epoch 26: Train Loss: 0.0668, Test Loss: 0.1941, Train L1 Norm: 0.2120, Test L1 Norm: 0.1238, Train Linf Norm: 75.9449, Test Linf Norm: 16.0243\n",
            "Epoch 27: Train Loss: 0.0319, Test Loss: 0.0248, Train L1 Norm: 0.1922, Test L1 Norm: 0.0793, Train Linf Norm: 66.8027, Test Linf Norm: 17.3262\n",
            "Epoch 28: Train Loss: 0.0281, Test Loss: 0.0235, Train L1 Norm: 0.1772, Test L1 Norm: 0.0776, Train Linf Norm: 67.3690, Test Linf Norm: 16.9839\n",
            "Epoch 29: Train Loss: 0.0269, Test Loss: 0.0234, Train L1 Norm: 0.1812, Test L1 Norm: 0.0748, Train Linf Norm: 68.2645, Test Linf Norm: 16.4822\n",
            "Epoch 30: Train Loss: 0.0279, Test Loss: 0.0352, Train L1 Norm: 0.1849, Test L1 Norm: 0.0836, Train Linf Norm: 69.7189, Test Linf Norm: 17.2874\n",
            "Epoch 31: Train Loss: 0.0277, Test Loss: 0.0256, Train L1 Norm: 0.1752, Test L1 Norm: 0.0773, Train Linf Norm: 65.8572, Test Linf Norm: 16.6403\n",
            "Epoch 32: Train Loss: 0.0284, Test Loss: 0.0252, Train L1 Norm: 0.1713, Test L1 Norm: 0.0765, Train Linf Norm: 63.3809, Test Linf Norm: 16.5227\n",
            "Epoch 33: Train Loss: 0.0284, Test Loss: 0.0217, Train L1 Norm: 0.1814, Test L1 Norm: 0.0718, Train Linf Norm: 68.6729, Test Linf Norm: 15.9122\n",
            "Epoch 34: Train Loss: 0.0279, Test Loss: 0.0219, Train L1 Norm: 0.1800, Test L1 Norm: 0.0704, Train Linf Norm: 69.9653, Test Linf Norm: 15.5826\n",
            "Epoch 35: Train Loss: 0.0285, Test Loss: 0.0259, Train L1 Norm: 0.1823, Test L1 Norm: 0.0714, Train Linf Norm: 70.3830, Test Linf Norm: 15.7126\n",
            "Epoch 36: Train Loss: 0.0287, Test Loss: 0.0303, Train L1 Norm: 0.1827, Test L1 Norm: 0.0780, Train Linf Norm: 71.4399, Test Linf Norm: 16.2279\n",
            "Epoch 37: Train Loss: 0.0366, Test Loss: 0.0522, Train L1 Norm: 0.1784, Test L1 Norm: 0.0745, Train Linf Norm: 66.3546, Test Linf Norm: 14.9880\n",
            "Epoch 38: Train Loss: 0.0422, Test Loss: 0.0343, Train L1 Norm: 0.1915, Test L1 Norm: 0.0704, Train Linf Norm: 71.9924, Test Linf Norm: 15.0971\n",
            "Epoch 39: Train Loss: 0.0289, Test Loss: 0.0183, Train L1 Norm: 0.1644, Test L1 Norm: 0.0689, Train Linf Norm: 61.5982, Test Linf Norm: 15.3208\n",
            "Epoch 40: Train Loss: 0.0178, Test Loss: 0.0218, Train L1 Norm: 0.1634, Test L1 Norm: 0.0652, Train Linf Norm: 62.8871, Test Linf Norm: 14.3965\n",
            "Epoch 41: Train Loss: 0.0173, Test Loss: 0.0190, Train L1 Norm: 0.1569, Test L1 Norm: 0.0657, Train Linf Norm: 58.8540, Test Linf Norm: 14.3513\n",
            "Epoch 42: Train Loss: 0.0171, Test Loss: 0.0171, Train L1 Norm: 0.1551, Test L1 Norm: 0.0644, Train Linf Norm: 59.1532, Test Linf Norm: 14.2477\n",
            "Epoch 43: Train Loss: 0.0169, Test Loss: 0.0169, Train L1 Norm: 0.1528, Test L1 Norm: 0.0641, Train Linf Norm: 58.8308, Test Linf Norm: 14.1521\n",
            "Epoch 44: Train Loss: 0.0167, Test Loss: 0.0168, Train L1 Norm: 0.1488, Test L1 Norm: 0.0632, Train Linf Norm: 57.2757, Test Linf Norm: 14.0439\n",
            "Epoch 45: Train Loss: 0.0166, Test Loss: 0.0174, Train L1 Norm: 0.1528, Test L1 Norm: 0.0623, Train Linf Norm: 58.7649, Test Linf Norm: 13.8238\n",
            "Epoch 46: Train Loss: 0.0163, Test Loss: 0.0305, Train L1 Norm: 0.1505, Test L1 Norm: 0.0702, Train Linf Norm: 58.1567, Test Linf Norm: 14.3119\n",
            "Epoch 47: Train Loss: 0.0164, Test Loss: 0.0177, Train L1 Norm: 0.1440, Test L1 Norm: 0.0616, Train Linf Norm: 53.0275, Test Linf Norm: 13.6337\n",
            "Epoch 48: Train Loss: 0.0160, Test Loss: 0.0157, Train L1 Norm: 0.1492, Test L1 Norm: 0.0617, Train Linf Norm: 58.2180, Test Linf Norm: 13.6751\n",
            "Epoch 49: Train Loss: 0.0160, Test Loss: 0.0218, Train L1 Norm: 0.1497, Test L1 Norm: 0.0651, Train Linf Norm: 55.8751, Test Linf Norm: 13.8309\n",
            "Epoch 50: Train Loss: 0.0161, Test Loss: 0.0154, Train L1 Norm: 0.1456, Test L1 Norm: 0.0611, Train Linf Norm: 55.6664, Test Linf Norm: 13.5580\n",
            "Epoch 51: Train Loss: 0.0155, Test Loss: 0.0159, Train L1 Norm: 0.1455, Test L1 Norm: 0.0610, Train Linf Norm: 55.8606, Test Linf Norm: 13.4643\n",
            "Epoch 52: Train Loss: 0.0159, Test Loss: 0.0154, Train L1 Norm: 0.1460, Test L1 Norm: 0.0595, Train Linf Norm: 56.4274, Test Linf Norm: 13.2797\n",
            "Epoch 53: Train Loss: 0.0143, Test Loss: 0.0148, Train L1 Norm: 0.1434, Test L1 Norm: 0.0593, Train Linf Norm: 56.0828, Test Linf Norm: 13.2080\n",
            "Epoch 54: Train Loss: 0.0142, Test Loss: 0.0157, Train L1 Norm: 0.1409, Test L1 Norm: 0.0589, Train Linf Norm: 53.0518, Test Linf Norm: 13.1003\n",
            "Epoch 55: Train Loss: 0.0141, Test Loss: 0.0147, Train L1 Norm: 0.1403, Test L1 Norm: 0.0594, Train Linf Norm: 51.9224, Test Linf Norm: 13.1665\n",
            "Epoch 56: Train Loss: 0.0140, Test Loss: 0.0146, Train L1 Norm: 0.1398, Test L1 Norm: 0.0591, Train Linf Norm: 53.4932, Test Linf Norm: 13.0955\n",
            "Epoch 57: Train Loss: 0.0139, Test Loss: 0.0146, Train L1 Norm: 0.1389, Test L1 Norm: 0.0589, Train Linf Norm: 52.2104, Test Linf Norm: 13.0678\n",
            "Epoch 58: Train Loss: 0.0138, Test Loss: 0.0148, Train L1 Norm: 0.1371, Test L1 Norm: 0.0585, Train Linf Norm: 52.1236, Test Linf Norm: 12.9592\n",
            "Epoch 59: Train Loss: 0.0138, Test Loss: 0.0157, Train L1 Norm: 0.1367, Test L1 Norm: 0.0578, Train Linf Norm: 51.5285, Test Linf Norm: 12.8541\n",
            "Epoch 60: Train Loss: 0.0136, Test Loss: 0.0147, Train L1 Norm: 0.1357, Test L1 Norm: 0.0587, Train Linf Norm: 52.1604, Test Linf Norm: 12.9789\n",
            "Epoch 61: Train Loss: 0.0137, Test Loss: 0.0141, Train L1 Norm: 0.1352, Test L1 Norm: 0.0581, Train Linf Norm: 49.5727, Test Linf Norm: 12.9043\n",
            "Epoch 62: Train Loss: 0.0135, Test Loss: 0.0140, Train L1 Norm: 0.1358, Test L1 Norm: 0.0578, Train Linf Norm: 50.7142, Test Linf Norm: 12.8408\n",
            "Epoch 63: Train Loss: 0.0135, Test Loss: 0.0140, Train L1 Norm: 0.1356, Test L1 Norm: 0.0574, Train Linf Norm: 50.9834, Test Linf Norm: 12.7741\n",
            "Epoch 64: Train Loss: 0.0134, Test Loss: 0.0141, Train L1 Norm: 0.1335, Test L1 Norm: 0.0574, Train Linf Norm: 51.0790, Test Linf Norm: 12.7538\n",
            "Epoch 65: Train Loss: 0.0133, Test Loss: 0.0152, Train L1 Norm: 0.1324, Test L1 Norm: 0.0586, Train Linf Norm: 50.7947, Test Linf Norm: 12.8541\n",
            "Epoch 66: Train Loss: 0.0132, Test Loss: 0.0139, Train L1 Norm: 0.1332, Test L1 Norm: 0.0570, Train Linf Norm: 50.5830, Test Linf Norm: 12.6892\n",
            "Epoch 67: Train Loss: 0.0131, Test Loss: 0.0138, Train L1 Norm: 0.1330, Test L1 Norm: 0.0568, Train Linf Norm: 51.6480, Test Linf Norm: 12.6519\n",
            "Epoch 68: Train Loss: 0.0131, Test Loss: 0.0137, Train L1 Norm: 0.1334, Test L1 Norm: 0.0567, Train Linf Norm: 51.0042, Test Linf Norm: 12.6216\n",
            "Epoch 69: Train Loss: 0.0130, Test Loss: 0.0140, Train L1 Norm: 0.1330, Test L1 Norm: 0.0565, Train Linf Norm: 50.9470, Test Linf Norm: 12.5751\n",
            "Epoch 70: Train Loss: 0.0130, Test Loss: 0.0137, Train L1 Norm: 0.1327, Test L1 Norm: 0.0566, Train Linf Norm: 50.5014, Test Linf Norm: 12.5910\n",
            "Epoch 71: Train Loss: 0.0130, Test Loss: 0.0137, Train L1 Norm: 0.1328, Test L1 Norm: 0.0561, Train Linf Norm: 51.0747, Test Linf Norm: 12.5014\n",
            "Epoch 72: Train Loss: 0.0129, Test Loss: 0.0136, Train L1 Norm: 0.1322, Test L1 Norm: 0.0563, Train Linf Norm: 51.6087, Test Linf Norm: 12.5386\n",
            "Epoch 73: Train Loss: 0.0129, Test Loss: 0.0136, Train L1 Norm: 0.1318, Test L1 Norm: 0.0561, Train Linf Norm: 49.3068, Test Linf Norm: 12.4937\n",
            "Epoch 74: Train Loss: 0.0129, Test Loss: 0.0135, Train L1 Norm: 0.1312, Test L1 Norm: 0.0562, Train Linf Norm: 51.0587, Test Linf Norm: 12.5076\n",
            "Epoch 75: Train Loss: 0.0129, Test Loss: 0.0137, Train L1 Norm: 0.1315, Test L1 Norm: 0.0558, Train Linf Norm: 46.1832, Test Linf Norm: 12.4284\n",
            "Epoch 76: Train Loss: 0.0129, Test Loss: 0.0135, Train L1 Norm: 0.1300, Test L1 Norm: 0.0562, Train Linf Norm: 49.7802, Test Linf Norm: 12.4918\n",
            "Epoch 77: Train Loss: 0.0128, Test Loss: 0.0135, Train L1 Norm: 0.1300, Test L1 Norm: 0.0561, Train Linf Norm: 48.3803, Test Linf Norm: 12.4657\n",
            "Epoch 78: Train Loss: 0.0128, Test Loss: 0.0138, Train L1 Norm: 0.1308, Test L1 Norm: 0.0556, Train Linf Norm: 49.3643, Test Linf Norm: 12.3881\n",
            "Epoch 79: Train Loss: 0.0127, Test Loss: 0.0134, Train L1 Norm: 0.1307, Test L1 Norm: 0.0558, Train Linf Norm: 50.3884, Test Linf Norm: 12.4165\n",
            "Epoch 80: Train Loss: 0.0127, Test Loss: 0.0134, Train L1 Norm: 0.1302, Test L1 Norm: 0.0559, Train Linf Norm: 49.3378, Test Linf Norm: 12.4476\n",
            "Epoch 81: Train Loss: 0.0127, Test Loss: 0.0134, Train L1 Norm: 0.1297, Test L1 Norm: 0.0558, Train Linf Norm: 48.9972, Test Linf Norm: 12.4265\n",
            "Epoch 82: Train Loss: 0.0127, Test Loss: 0.0133, Train L1 Norm: 0.1297, Test L1 Norm: 0.0557, Train Linf Norm: 49.4867, Test Linf Norm: 12.4071\n",
            "Epoch 83: Train Loss: 0.0126, Test Loss: 0.0133, Train L1 Norm: 0.1294, Test L1 Norm: 0.0558, Train Linf Norm: 49.1174, Test Linf Norm: 12.4128\n",
            "Epoch 84: Train Loss: 0.0126, Test Loss: 0.0133, Train L1 Norm: 0.1292, Test L1 Norm: 0.0556, Train Linf Norm: 48.3477, Test Linf Norm: 12.3874\n",
            "Epoch 85: Train Loss: 0.0126, Test Loss: 0.0133, Train L1 Norm: 0.1291, Test L1 Norm: 0.0557, Train Linf Norm: 49.5835, Test Linf Norm: 12.3843\n",
            "Epoch 86: Train Loss: 0.0126, Test Loss: 0.0134, Train L1 Norm: 0.1292, Test L1 Norm: 0.0558, Train Linf Norm: 49.3708, Test Linf Norm: 12.4084\n",
            "Epoch 87: Train Loss: 0.0126, Test Loss: 0.0133, Train L1 Norm: 0.1286, Test L1 Norm: 0.0555, Train Linf Norm: 49.0076, Test Linf Norm: 12.3581\n",
            "Epoch 88: Train Loss: 0.0126, Test Loss: 0.0133, Train L1 Norm: 0.1286, Test L1 Norm: 0.0554, Train Linf Norm: 48.2515, Test Linf Norm: 12.3389\n",
            "Epoch 89: Train Loss: 0.0126, Test Loss: 0.0133, Train L1 Norm: 0.1286, Test L1 Norm: 0.0554, Train Linf Norm: 49.3490, Test Linf Norm: 12.3382\n",
            "Epoch 90: Train Loss: 0.0126, Test Loss: 0.0133, Train L1 Norm: 0.1287, Test L1 Norm: 0.0556, Train Linf Norm: 49.6325, Test Linf Norm: 12.3606\n",
            "Epoch 91: Train Loss: 0.0126, Test Loss: 0.0132, Train L1 Norm: 0.1285, Test L1 Norm: 0.0555, Train Linf Norm: 49.6670, Test Linf Norm: 12.3444\n",
            "Epoch 92: Train Loss: 0.0125, Test Loss: 0.0132, Train L1 Norm: 0.1285, Test L1 Norm: 0.0553, Train Linf Norm: 48.6848, Test Linf Norm: 12.3221\n",
            "Epoch 93: Train Loss: 0.0125, Test Loss: 0.0132, Train L1 Norm: 0.1282, Test L1 Norm: 0.0554, Train Linf Norm: 48.4163, Test Linf Norm: 12.3284\n",
            "Epoch 94: Train Loss: 0.0125, Test Loss: 0.0132, Train L1 Norm: 0.1284, Test L1 Norm: 0.0553, Train Linf Norm: 48.9282, Test Linf Norm: 12.3115\n",
            "Epoch 95: Train Loss: 0.0125, Test Loss: 0.0132, Train L1 Norm: 0.1284, Test L1 Norm: 0.0553, Train Linf Norm: 48.5962, Test Linf Norm: 12.3099\n",
            "Epoch 96: Train Loss: 0.0125, Test Loss: 0.0132, Train L1 Norm: 0.1283, Test L1 Norm: 0.0553, Train Linf Norm: 48.5187, Test Linf Norm: 12.3169\n",
            "Epoch 97: Train Loss: 0.0125, Test Loss: 0.0132, Train L1 Norm: 0.1282, Test L1 Norm: 0.0552, Train Linf Norm: 48.7479, Test Linf Norm: 12.3012\n",
            "Epoch 98: Train Loss: 0.0125, Test Loss: 0.0132, Train L1 Norm: 0.1281, Test L1 Norm: 0.0553, Train Linf Norm: 47.9351, Test Linf Norm: 12.3074\n",
            "Epoch 99: Train Loss: 0.0125, Test Loss: 0.0132, Train L1 Norm: 0.1281, Test L1 Norm: 0.0552, Train Linf Norm: 47.6786, Test Linf Norm: 12.2966\n",
            "Epoch 100: Train Loss: 0.0125, Test Loss: 0.0132, Train L1 Norm: 0.1275, Test L1 Norm: 0.0552, Train Linf Norm: 49.2013, Test Linf Norm: 12.2915\n",
            "Epoch 101: Train Loss: 0.0125, Test Loss: 0.0132, Train L1 Norm: 0.1278, Test L1 Norm: 0.0552, Train Linf Norm: 48.7280, Test Linf Norm: 12.2835\n",
            "Epoch 102: Train Loss: 0.0125, Test Loss: 0.0132, Train L1 Norm: 0.1279, Test L1 Norm: 0.0551, Train Linf Norm: 49.0455, Test Linf Norm: 12.2680\n",
            "Epoch 103: Train Loss: 0.0125, Test Loss: 0.0131, Train L1 Norm: 0.1278, Test L1 Norm: 0.0552, Train Linf Norm: 49.1716, Test Linf Norm: 12.2829\n",
            "Epoch 104: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1280, Test L1 Norm: 0.0552, Train Linf Norm: 48.9949, Test Linf Norm: 12.2814\n",
            "Epoch 105: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1276, Test L1 Norm: 0.0552, Train Linf Norm: 49.4178, Test Linf Norm: 12.2792\n",
            "Epoch 106: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1275, Test L1 Norm: 0.0551, Train Linf Norm: 49.0273, Test Linf Norm: 12.2711\n",
            "Epoch 107: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1278, Test L1 Norm: 0.0551, Train Linf Norm: 49.1332, Test Linf Norm: 12.2714\n",
            "Epoch 108: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1277, Test L1 Norm: 0.0551, Train Linf Norm: 48.1457, Test Linf Norm: 12.2687\n",
            "Epoch 109: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1277, Test L1 Norm: 0.0551, Train Linf Norm: 48.1370, Test Linf Norm: 12.2673\n",
            "Epoch 110: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1277, Test L1 Norm: 0.0550, Train Linf Norm: 48.4678, Test Linf Norm: 12.2583\n",
            "Epoch 111: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1276, Test L1 Norm: 0.0550, Train Linf Norm: 48.9655, Test Linf Norm: 12.2546\n",
            "Epoch 112: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1276, Test L1 Norm: 0.0551, Train Linf Norm: 49.0999, Test Linf Norm: 12.2612\n",
            "Epoch 113: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1277, Test L1 Norm: 0.0551, Train Linf Norm: 46.8445, Test Linf Norm: 12.2613\n",
            "Epoch 114: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1275, Test L1 Norm: 0.0550, Train Linf Norm: 48.3784, Test Linf Norm: 12.2443\n",
            "Epoch 115: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1276, Test L1 Norm: 0.0551, Train Linf Norm: 44.3499, Test Linf Norm: 12.2604\n",
            "Epoch 116: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1276, Test L1 Norm: 0.0550, Train Linf Norm: 46.6969, Test Linf Norm: 12.2516\n",
            "Epoch 117: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1276, Test L1 Norm: 0.0550, Train Linf Norm: 48.8630, Test Linf Norm: 12.2442\n",
            "Epoch 118: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1275, Test L1 Norm: 0.0550, Train Linf Norm: 48.8027, Test Linf Norm: 12.2532\n",
            "Epoch 119: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1275, Test L1 Norm: 0.0550, Train Linf Norm: 46.6073, Test Linf Norm: 12.2476\n",
            "Epoch 120: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1275, Test L1 Norm: 0.0550, Train Linf Norm: 48.6977, Test Linf Norm: 12.2442\n",
            "Epoch 121: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1275, Test L1 Norm: 0.0550, Train Linf Norm: 46.9892, Test Linf Norm: 12.2518\n",
            "Epoch 122: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1275, Test L1 Norm: 0.0550, Train Linf Norm: 48.0239, Test Linf Norm: 12.2456\n",
            "Epoch 123: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1274, Test L1 Norm: 0.0550, Train Linf Norm: 47.9782, Test Linf Norm: 12.2461\n",
            "Epoch 124: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0551, Train Linf Norm: 43.6241, Test Linf Norm: 12.2554\n",
            "Epoch 125: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1274, Test L1 Norm: 0.0550, Train Linf Norm: 46.8717, Test Linf Norm: 12.2403\n",
            "Epoch 126: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 48.8706, Test Linf Norm: 12.2466\n",
            "Epoch 127: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1274, Test L1 Norm: 0.0550, Train Linf Norm: 48.7353, Test Linf Norm: 12.2436\n",
            "Epoch 128: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1274, Test L1 Norm: 0.0550, Train Linf Norm: 48.7103, Test Linf Norm: 12.2398\n",
            "Epoch 129: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.5620, Test Linf Norm: 12.2362\n",
            "Epoch 130: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 49.0960, Test Linf Norm: 12.2419\n",
            "Epoch 131: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 49.0494, Test Linf Norm: 12.2369\n",
            "Epoch 132: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 48.5653, Test Linf Norm: 12.2433\n",
            "Epoch 133: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 48.4735, Test Linf Norm: 12.2374\n",
            "Epoch 134: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 48.9006, Test Linf Norm: 12.2408\n",
            "Epoch 135: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0549, Train Linf Norm: 42.7614, Test Linf Norm: 12.2346\n",
            "Epoch 136: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 47.9802, Test Linf Norm: 12.2404\n",
            "Epoch 137: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 48.9768, Test Linf Norm: 12.2372\n",
            "Epoch 138: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 49.1812, Test Linf Norm: 12.2361\n",
            "Epoch 139: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 48.4863, Test Linf Norm: 12.2404\n",
            "Epoch 140: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 49.2944, Test Linf Norm: 12.2382\n",
            "Epoch 141: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 46.0989, Test Linf Norm: 12.2366\n",
            "Epoch 142: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.4534, Test Linf Norm: 12.2395\n",
            "Epoch 143: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 47.9059, Test Linf Norm: 12.2401\n",
            "Epoch 144: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 49.1509, Test Linf Norm: 12.2360\n",
            "Epoch 145: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 48.2316, Test Linf Norm: 12.2359\n",
            "Epoch 146: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 44.6857, Test Linf Norm: 12.2365\n",
            "Epoch 147: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.5587, Test Linf Norm: 12.2373\n",
            "Epoch 148: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 47.4709, Test Linf Norm: 12.2361\n",
            "Epoch 149: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.3123, Test Linf Norm: 12.2353\n",
            "Epoch 150: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.7108, Test Linf Norm: 12.2367\n",
            "Epoch 151: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 47.8408, Test Linf Norm: 12.2394\n",
            "Epoch 152: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 43.1385, Test Linf Norm: 12.2373\n",
            "Epoch 153: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 48.6796, Test Linf Norm: 12.2364\n",
            "Epoch 154: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 48.6769, Test Linf Norm: 12.2354\n",
            "Epoch 155: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 49.0807, Test Linf Norm: 12.2357\n",
            "Epoch 156: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.5708, Test Linf Norm: 12.2375\n",
            "Epoch 157: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 47.8965, Test Linf Norm: 12.2374\n",
            "Epoch 158: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 49.5995, Test Linf Norm: 12.2369\n",
            "Epoch 159: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 46.2262, Test Linf Norm: 12.2366\n",
            "Epoch 160: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 48.2933, Test Linf Norm: 12.2363\n",
            "Epoch 161: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.8480, Test Linf Norm: 12.2364\n",
            "Epoch 162: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 48.8094, Test Linf Norm: 12.2369\n",
            "Epoch 163: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 48.6540, Test Linf Norm: 12.2366\n",
            "Epoch 164: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.5800, Test Linf Norm: 12.2364\n",
            "Epoch 165: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 48.4350, Test Linf Norm: 12.2359\n",
            "Epoch 166: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 49.4188, Test Linf Norm: 12.2362\n",
            "Epoch 167: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 48.6206, Test Linf Norm: 12.2359\n",
            "Epoch 168: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 48.2624, Test Linf Norm: 12.2361\n",
            "Epoch 169: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.6192, Test Linf Norm: 12.2362\n",
            "Epoch 170: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 48.5972, Test Linf Norm: 12.2362\n",
            "Epoch 171: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 47.4541, Test Linf Norm: 12.2362\n",
            "Epoch 172: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 48.1111, Test Linf Norm: 12.2361\n",
            "Epoch 173: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 48.8844, Test Linf Norm: 12.2362\n",
            "Epoch 174: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 45.6125, Test Linf Norm: 12.2362\n",
            "Epoch 175: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 48.0465, Test Linf Norm: 12.2362\n",
            "Epoch 176: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 48.2879, Test Linf Norm: 12.2361\n",
            "Epoch 177: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.3695, Test Linf Norm: 12.2360\n",
            "Epoch 178: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 49.3136, Test Linf Norm: 12.2360\n",
            "Epoch 179: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 47.9869, Test Linf Norm: 12.2361\n",
            "Epoch 180: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 49.0720, Test Linf Norm: 12.2361\n",
            "Epoch 181: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1273, Test L1 Norm: 0.0550, Train Linf Norm: 48.5441, Test Linf Norm: 12.2360\n",
            "Epoch 182: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.4238, Test Linf Norm: 12.2360\n",
            "Epoch 183: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 43.6423, Test Linf Norm: 12.2360\n",
            "Epoch 184: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.2992, Test Linf Norm: 12.2360\n",
            "Epoch 185: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.3007, Test Linf Norm: 12.2360\n",
            "Epoch 186: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.3829, Test Linf Norm: 12.2360\n",
            "Epoch 187: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.9014, Test Linf Norm: 12.2360\n",
            "Epoch 188: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.9737, Test Linf Norm: 12.2360\n",
            "Epoch 189: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.7794, Test Linf Norm: 12.2360\n",
            "Epoch 190: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.9243, Test Linf Norm: 12.2360\n",
            "Epoch 191: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.0540, Test Linf Norm: 12.2360\n",
            "Epoch 192: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.1561, Test Linf Norm: 12.2360\n",
            "Epoch 193: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.7712, Test Linf Norm: 12.2360\n",
            "Epoch 194: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 46.9430, Test Linf Norm: 12.2360\n",
            "Epoch 195: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.6182, Test Linf Norm: 12.2360\n",
            "Epoch 196: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.5394, Test Linf Norm: 12.2360\n",
            "Epoch 197: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.8090, Test Linf Norm: 12.2360\n",
            "Epoch 198: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.2544, Test Linf Norm: 12.2360\n",
            "Epoch 199: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 46.6178, Test Linf Norm: 12.2360\n",
            "Epoch 200: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.3004, Test Linf Norm: 12.2360\n",
            "Epoch 201: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.6541, Test Linf Norm: 12.2360\n",
            "Epoch 202: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 46.8026, Test Linf Norm: 12.2360\n",
            "Epoch 203: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.2618, Test Linf Norm: 12.2360\n",
            "Epoch 204: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.5660, Test Linf Norm: 12.2360\n",
            "Epoch 205: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.5553, Test Linf Norm: 12.2360\n",
            "Epoch 206: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.7758, Test Linf Norm: 12.2360\n",
            "Epoch 207: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.4589, Test Linf Norm: 12.2360\n",
            "Epoch 208: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.5123, Test Linf Norm: 12.2360\n",
            "Epoch 209: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 46.7849, Test Linf Norm: 12.2360\n",
            "Epoch 210: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 45.5221, Test Linf Norm: 12.2360\n",
            "Epoch 211: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.2579, Test Linf Norm: 12.2360\n",
            "Epoch 212: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.6937, Test Linf Norm: 12.2360\n",
            "Epoch 213: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.8661, Test Linf Norm: 12.2360\n",
            "Epoch 214: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 45.4625, Test Linf Norm: 12.2360\n",
            "Epoch 215: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.6530, Test Linf Norm: 12.2360\n",
            "Epoch 216: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 49.3033, Test Linf Norm: 12.2360\n",
            "Epoch 217: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.8438, Test Linf Norm: 12.2360\n",
            "Epoch 218: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.5044, Test Linf Norm: 12.2360\n",
            "Epoch 219: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.3024, Test Linf Norm: 12.2360\n",
            "Epoch 220: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.4782, Test Linf Norm: 12.2360\n",
            "Epoch 221: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.7203, Test Linf Norm: 12.2360\n",
            "Epoch 222: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.6570, Test Linf Norm: 12.2360\n",
            "Epoch 223: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.7207, Test Linf Norm: 12.2360\n",
            "Epoch 224: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.5803, Test Linf Norm: 12.2360\n",
            "Epoch 225: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.4541, Test Linf Norm: 12.2360\n",
            "Epoch 226: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.5170, Test Linf Norm: 12.2360\n",
            "Epoch 227: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.1150, Test Linf Norm: 12.2360\n",
            "Epoch 228: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.4244, Test Linf Norm: 12.2360\n",
            "Epoch 229: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.8825, Test Linf Norm: 12.2360\n",
            "Epoch 230: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.1004, Test Linf Norm: 12.2360\n",
            "Epoch 231: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.2627, Test Linf Norm: 12.2360\n",
            "Epoch 232: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 49.2030, Test Linf Norm: 12.2360\n",
            "Epoch 233: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.2854, Test Linf Norm: 12.2360\n",
            "Epoch 234: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.7485, Test Linf Norm: 12.2360\n",
            "Epoch 235: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.4874, Test Linf Norm: 12.2360\n",
            "Epoch 236: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.6603, Test Linf Norm: 12.2360\n",
            "Epoch 237: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.0817, Test Linf Norm: 12.2360\n",
            "Epoch 238: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.7514, Test Linf Norm: 12.2360\n",
            "Epoch 239: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.7826, Test Linf Norm: 12.2360\n",
            "Epoch 240: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 49.2250, Test Linf Norm: 12.2360\n",
            "Epoch 241: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.9370, Test Linf Norm: 12.2360\n",
            "Epoch 242: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 49.4066, Test Linf Norm: 12.2360\n",
            "Epoch 243: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.1802, Test Linf Norm: 12.2360\n",
            "Epoch 244: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 49.1604, Test Linf Norm: 12.2360\n",
            "Epoch 245: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 46.8573, Test Linf Norm: 12.2360\n",
            "Epoch 246: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.6232, Test Linf Norm: 12.2360\n",
            "Epoch 247: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.4552, Test Linf Norm: 12.2360\n",
            "Epoch 248: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.2067, Test Linf Norm: 12.2360\n",
            "Epoch 249: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.7349, Test Linf Norm: 12.2360\n",
            "Epoch 250: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.9098, Test Linf Norm: 12.2360\n",
            "Epoch 251: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.6860, Test Linf Norm: 12.2360\n",
            "Epoch 252: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.6346, Test Linf Norm: 12.2360\n",
            "Epoch 253: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 49.4026, Test Linf Norm: 12.2360\n",
            "Epoch 254: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.9221, Test Linf Norm: 12.2360\n",
            "Epoch 255: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.5470, Test Linf Norm: 12.2360\n",
            "Epoch 256: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.2685, Test Linf Norm: 12.2360\n",
            "Epoch 257: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 49.0598, Test Linf Norm: 12.2360\n",
            "Epoch 258: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 49.2689, Test Linf Norm: 12.2360\n",
            "Epoch 259: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.2543, Test Linf Norm: 12.2360\n",
            "Epoch 260: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.7366, Test Linf Norm: 12.2360\n",
            "Epoch 261: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.0198, Test Linf Norm: 12.2360\n",
            "Epoch 262: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 46.8869, Test Linf Norm: 12.2360\n",
            "Epoch 263: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.5440, Test Linf Norm: 12.2360\n",
            "Epoch 264: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.3061, Test Linf Norm: 12.2360\n",
            "Epoch 265: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.4083, Test Linf Norm: 12.2360\n",
            "Epoch 266: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 49.4812, Test Linf Norm: 12.2360\n",
            "Epoch 267: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 49.4450, Test Linf Norm: 12.2360\n",
            "Epoch 268: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.1584, Test Linf Norm: 12.2360\n",
            "Epoch 269: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.6484, Test Linf Norm: 12.2360\n",
            "Epoch 270: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.9522, Test Linf Norm: 12.2360\n",
            "Epoch 271: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.2606, Test Linf Norm: 12.2360\n",
            "Epoch 272: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 44.9437, Test Linf Norm: 12.2360\n",
            "Epoch 273: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 49.1640, Test Linf Norm: 12.2360\n",
            "Epoch 274: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.3244, Test Linf Norm: 12.2360\n",
            "Epoch 275: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.3074, Test Linf Norm: 12.2360\n",
            "Epoch 276: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.4391, Test Linf Norm: 12.2360\n",
            "Epoch 277: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.7837, Test Linf Norm: 12.2360\n",
            "Epoch 278: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.9329, Test Linf Norm: 12.2360\n",
            "Epoch 279: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 46.4766, Test Linf Norm: 12.2360\n",
            "Epoch 280: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 49.2003, Test Linf Norm: 12.2360\n",
            "Epoch 281: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.8756, Test Linf Norm: 12.2360\n",
            "Epoch 282: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 49.0392, Test Linf Norm: 12.2360\n",
            "Epoch 283: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 49.0268, Test Linf Norm: 12.2360\n",
            "Epoch 284: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.5420, Test Linf Norm: 12.2360\n",
            "Epoch 285: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.5651, Test Linf Norm: 12.2360\n",
            "Epoch 286: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.1554, Test Linf Norm: 12.2360\n",
            "Epoch 287: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 49.4431, Test Linf Norm: 12.2360\n",
            "Epoch 288: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.5835, Test Linf Norm: 12.2360\n",
            "Epoch 289: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 49.0069, Test Linf Norm: 12.2360\n",
            "Epoch 290: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 47.3028, Test Linf Norm: 12.2360\n",
            "Epoch 291: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.0834, Test Linf Norm: 12.2360\n",
            "Epoch 292: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.1793, Test Linf Norm: 12.2360\n",
            "Epoch 293: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 49.0445, Test Linf Norm: 12.2360\n",
            "Epoch 294: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.7245, Test Linf Norm: 12.2360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 16:25:30,717]\u001b[0m Trial 4 finished with value: 0.05496781919002533 and parameters: {'n_layers': 4, 'n_units_0': 972, 'n_units_1': 1712, 'n_units_2': 1095, 'n_units_3': 714, 'hidden_activation': 'LeakyReLU', 'output_activation': 'ReLU', 'loss': 'MAE', 'optimizer': 'SGD', 'lr': 0.0004510263165822516, 'batch_size': 512, 'n_epochs': 295, 'scheduler': 'StepLR', 'weight_decay': 0.009818278377634463, 'momentum': 0.6691393537369812, 'step_size': 13, 'gamma': 0.4612246456460004}. Best is trial 3 with value: 0.028729132837057112.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 295: Train Loss: 0.0124, Test Loss: 0.0131, Train L1 Norm: 0.1272, Test L1 Norm: 0.0550, Train Linf Norm: 48.2084, Test Linf Norm: 12.2360\n",
            "Epoch 1: Train Loss: 3.0184, Test Loss: 3.0087, Train L1 Norm: 1.0213, Test L1 Norm: 1.0000, Train Linf Norm: 1.1076, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 16:25:41,807]\u001b[0m Trial 5 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 2.9636, Test Loss: 3.0087, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.4383, Test Loss: 0.0124, Train L1 Norm: 2.5217, Test L1 Norm: 0.2758, Train Linf Norm: 1811.5481, Test Linf Norm: 90.3496\n",
            "Epoch 2: Train Loss: 0.0063, Test Loss: 0.0067, Train L1 Norm: 0.7244, Test L1 Norm: 0.2141, Train Linf Norm: 557.0939, Test Linf Norm: 78.4897\n",
            "Epoch 3: Train Loss: 0.0047, Test Loss: 0.0019, Train L1 Norm: 0.6308, Test L1 Norm: 0.1658, Train Linf Norm: 510.7414, Test Linf Norm: 63.1042\n",
            "Epoch 4: Train Loss: 0.0037, Test Loss: 0.0041, Train L1 Norm: 0.5429, Test L1 Norm: 0.1723, Train Linf Norm: 431.7121, Test Linf Norm: 63.6761\n",
            "Epoch 5: Train Loss: 0.0051, Test Loss: 0.0065, Train L1 Norm: 0.5677, Test L1 Norm: 0.1886, Train Linf Norm: 458.2724, Test Linf Norm: 68.7944\n",
            "Epoch 6: Train Loss: 0.0062, Test Loss: 0.0010, Train L1 Norm: 0.6019, Test L1 Norm: 0.1531, Train Linf Norm: 497.1396, Test Linf Norm: 60.8969\n",
            "Epoch 7: Train Loss: 0.0036, Test Loss: 0.0028, Train L1 Norm: 0.5633, Test L1 Norm: 0.1637, Train Linf Norm: 457.9416, Test Linf Norm: 64.9147\n",
            "Epoch 8: Train Loss: 0.0030, Test Loss: 0.0073, Train L1 Norm: 0.5517, Test L1 Norm: 0.1668, Train Linf Norm: 449.2017, Test Linf Norm: 61.5978\n",
            "Epoch 9: Train Loss: 0.0067, Test Loss: 0.0047, Train L1 Norm: 0.5745, Test L1 Norm: 0.1736, Train Linf Norm: 456.1840, Test Linf Norm: 63.7528\n",
            "Epoch 10: Train Loss: 0.0091, Test Loss: 0.0038, Train L1 Norm: 0.6256, Test L1 Norm: 0.1736, Train Linf Norm: 506.3719, Test Linf Norm: 66.5048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 16:26:02,466]\u001b[0m Trial 6 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: Train Loss: 0.0025, Test Loss: 0.0018, Train L1 Norm: 0.5606, Test L1 Norm: 0.1672, Train Linf Norm: 366.0774, Test Linf Norm: 66.0232\n",
            "Epoch 1: Train Loss: 0.3187, Test Loss: 0.0390, Train L1 Norm: 1.2747, Test L1 Norm: 0.2800, Train Linf Norm: 34.0333, Test Linf Norm: 5.7621\n",
            "Epoch 2: Train Loss: 0.0595, Test Loss: 0.0339, Train L1 Norm: 0.6312, Test L1 Norm: 0.1775, Train Linf Norm: 17.4347, Test Linf Norm: 3.2373\n",
            "Epoch 3: Train Loss: 0.0397, Test Loss: 0.0142, Train L1 Norm: 0.4660, Test L1 Norm: 0.1156, Train Linf Norm: 12.8877, Test Linf Norm: 2.1836\n",
            "Epoch 4: Train Loss: 0.0243, Test Loss: 0.0121, Train L1 Norm: 0.3546, Test L1 Norm: 0.1131, Train Linf Norm: 9.5643, Test Linf Norm: 2.1302\n",
            "Epoch 5: Train Loss: 0.0249, Test Loss: 0.0082, Train L1 Norm: 0.4132, Test L1 Norm: 0.1016, Train Linf Norm: 11.3554, Test Linf Norm: 1.9380\n",
            "Epoch 6: Train Loss: 0.0182, Test Loss: 0.0068, Train L1 Norm: 0.4242, Test L1 Norm: 0.0964, Train Linf Norm: 12.0465, Test Linf Norm: 1.8788\n",
            "Epoch 7: Train Loss: 0.0072, Test Loss: 0.0065, Train L1 Norm: 0.3546, Test L1 Norm: 0.0952, Train Linf Norm: 10.1002, Test Linf Norm: 1.8535\n",
            "Epoch 8: Train Loss: 0.0067, Test Loss: 0.0055, Train L1 Norm: 0.3744, Test L1 Norm: 0.0954, Train Linf Norm: 10.7561, Test Linf Norm: 1.8686\n",
            "Epoch 9: Train Loss: 0.0064, Test Loss: 0.0053, Train L1 Norm: 0.3524, Test L1 Norm: 0.0920, Train Linf Norm: 10.0706, Test Linf Norm: 1.7909\n",
            "Epoch 10: Train Loss: 0.0057, Test Loss: 0.0050, Train L1 Norm: 0.3633, Test L1 Norm: 0.0943, Train Linf Norm: 10.4569, Test Linf Norm: 1.8593\n",
            "Epoch 11: Train Loss: 0.0059, Test Loss: 0.0049, Train L1 Norm: 0.3847, Test L1 Norm: 0.0960, Train Linf Norm: 11.1394, Test Linf Norm: 1.9052\n",
            "Epoch 12: Train Loss: 0.0053, Test Loss: 0.0051, Train L1 Norm: 0.3498, Test L1 Norm: 0.0905, Train Linf Norm: 10.0589, Test Linf Norm: 1.7675\n",
            "Epoch 13: Train Loss: 0.0045, Test Loss: 0.0043, Train L1 Norm: 0.3533, Test L1 Norm: 0.0909, Train Linf Norm: 10.1912, Test Linf Norm: 1.8050\n",
            "Epoch 14: Train Loss: 0.0044, Test Loss: 0.0047, Train L1 Norm: 0.3652, Test L1 Norm: 0.0906, Train Linf Norm: 10.5795, Test Linf Norm: 1.7710\n",
            "Epoch 15: Train Loss: 0.0044, Test Loss: 0.0042, Train L1 Norm: 0.3569, Test L1 Norm: 0.0912, Train Linf Norm: 10.2799, Test Linf Norm: 1.8135\n",
            "Epoch 16: Train Loss: 0.0044, Test Loss: 0.0044, Train L1 Norm: 0.3517, Test L1 Norm: 0.0914, Train Linf Norm: 10.1493, Test Linf Norm: 1.8213\n",
            "Epoch 17: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.3511, Test L1 Norm: 0.0895, Train Linf Norm: 10.1299, Test Linf Norm: 1.7643\n",
            "Epoch 18: Train Loss: 0.0042, Test Loss: 0.0041, Train L1 Norm: 0.3596, Test L1 Norm: 0.0904, Train Linf Norm: 10.3174, Test Linf Norm: 1.8058\n",
            "Epoch 19: Train Loss: 0.0041, Test Loss: 0.0040, Train L1 Norm: 0.3458, Test L1 Norm: 0.0900, Train Linf Norm: 9.9815, Test Linf Norm: 1.7957\n",
            "Epoch 20: Train Loss: 0.0041, Test Loss: 0.0041, Train L1 Norm: 0.3509, Test L1 Norm: 0.0903, Train Linf Norm: 10.1318, Test Linf Norm: 1.8042\n",
            "Epoch 21: Train Loss: 0.0041, Test Loss: 0.0040, Train L1 Norm: 0.3495, Test L1 Norm: 0.0902, Train Linf Norm: 10.0857, Test Linf Norm: 1.8010\n",
            "Epoch 22: Train Loss: 0.0040, Test Loss: 0.0040, Train L1 Norm: 0.3486, Test L1 Norm: 0.0903, Train Linf Norm: 10.0441, Test Linf Norm: 1.8049\n",
            "Epoch 23: Train Loss: 0.0040, Test Loss: 0.0040, Train L1 Norm: 0.3502, Test L1 Norm: 0.0895, Train Linf Norm: 10.1209, Test Linf Norm: 1.7875\n",
            "Epoch 24: Train Loss: 0.0040, Test Loss: 0.0040, Train L1 Norm: 0.3480, Test L1 Norm: 0.0894, Train Linf Norm: 10.0620, Test Linf Norm: 1.7827\n",
            "Epoch 25: Train Loss: 0.0040, Test Loss: 0.0040, Train L1 Norm: 0.3499, Test L1 Norm: 0.0899, Train Linf Norm: 10.0967, Test Linf Norm: 1.7969\n",
            "Epoch 26: Train Loss: 0.0040, Test Loss: 0.0040, Train L1 Norm: 0.3490, Test L1 Norm: 0.0900, Train Linf Norm: 10.0918, Test Linf Norm: 1.8005\n",
            "Epoch 27: Train Loss: 0.0040, Test Loss: 0.0040, Train L1 Norm: 0.3504, Test L1 Norm: 0.0899, Train Linf Norm: 10.1124, Test Linf Norm: 1.7970\n",
            "Epoch 28: Train Loss: 0.0040, Test Loss: 0.0040, Train L1 Norm: 0.3519, Test L1 Norm: 0.0897, Train Linf Norm: 10.1630, Test Linf Norm: 1.7924\n",
            "Epoch 29: Train Loss: 0.0040, Test Loss: 0.0039, Train L1 Norm: 0.3503, Test L1 Norm: 0.0899, Train Linf Norm: 10.1377, Test Linf Norm: 1.7967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 16:30:29,043]\u001b[0m Trial 7 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30: Train Loss: 0.0040, Test Loss: 0.0040, Train L1 Norm: 0.3509, Test L1 Norm: 0.0898, Train Linf Norm: 10.1486, Test Linf Norm: 1.7958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-e8f6ba3279cc>:148: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  t_max_fraction = trial.suggest_uniform('t_max_fraction', 0.05, 0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.3429, Test Loss: 0.0481, Train L1 Norm: 0.7170, Test L1 Norm: 0.1558, Train Linf Norm: 34.4950, Test Linf Norm: 7.0103\n",
            "Epoch 2: Train Loss: 0.0597, Test Loss: 0.0994, Train L1 Norm: 0.2729, Test L1 Norm: 0.1074, Train Linf Norm: 15.1596, Test Linf Norm: 3.9288\n",
            "Epoch 3: Train Loss: 0.0549, Test Loss: 0.1197, Train L1 Norm: 0.3068, Test L1 Norm: 0.0763, Train Linf Norm: 17.6044, Test Linf Norm: 1.3947\n",
            "Epoch 4: Train Loss: 0.0500, Test Loss: 0.0586, Train L1 Norm: 0.1580, Test L1 Norm: 0.0731, Train Linf Norm: 8.2252, Test Linf Norm: 2.6138\n",
            "Epoch 5: Train Loss: 0.0504, Test Loss: 0.0341, Train L1 Norm: 0.1686, Test L1 Norm: 0.0600, Train Linf Norm: 9.0444, Test Linf Norm: 2.5244\n",
            "Epoch 6: Train Loss: 0.0335, Test Loss: 0.0258, Train L1 Norm: 0.1032, Test L1 Norm: 0.0666, Train Linf Norm: 5.3626, Test Linf Norm: 2.9249\n",
            "Epoch 7: Train Loss: 0.0352, Test Loss: 0.0323, Train L1 Norm: 0.1068, Test L1 Norm: 0.0355, Train Linf Norm: 5.5720, Test Linf Norm: 1.1477\n",
            "Epoch 8: Train Loss: 0.0360, Test Loss: 0.0311, Train L1 Norm: 0.0909, Test L1 Norm: 0.0304, Train Linf Norm: 4.4767, Test Linf Norm: 0.7902\n",
            "Epoch 9: Train Loss: 0.0290, Test Loss: 0.0238, Train L1 Norm: 0.0811, Test L1 Norm: 0.0456, Train Linf Norm: 4.0771, Test Linf Norm: 1.8800\n",
            "Epoch 10: Train Loss: 0.0229, Test Loss: 0.0195, Train L1 Norm: 0.1022, Test L1 Norm: 0.0211, Train Linf Norm: 5.6935, Test Linf Norm: 0.7332\n",
            "Epoch 11: Train Loss: 0.0173, Test Loss: 0.0156, Train L1 Norm: 0.0973, Test L1 Norm: 0.0290, Train Linf Norm: 5.5884, Test Linf Norm: 1.2711\n",
            "Epoch 12: Train Loss: 0.0146, Test Loss: 0.0134, Train L1 Norm: 0.0556, Test L1 Norm: 0.0238, Train Linf Norm: 3.0385, Test Linf Norm: 1.0197\n",
            "Epoch 13: Train Loss: 0.0108, Test Loss: 0.0107, Train L1 Norm: 0.0362, Test L1 Norm: 0.0134, Train Linf Norm: 1.9221, Test Linf Norm: 0.4913\n",
            "Epoch 14: Train Loss: 0.0094, Test Loss: 0.0100, Train L1 Norm: 0.0272, Test L1 Norm: 0.0136, Train Linf Norm: 1.3965, Test Linf Norm: 0.5401\n",
            "Epoch 15: Train Loss: 0.0106, Test Loss: 0.0112, Train L1 Norm: 0.0323, Test L1 Norm: 0.0129, Train Linf Norm: 1.6741, Test Linf Norm: 0.4654\n",
            "Epoch 16: Train Loss: 0.0123, Test Loss: 0.0182, Train L1 Norm: 0.0368, Test L1 Norm: 0.0153, Train Linf Norm: 1.9017, Test Linf Norm: 0.4341\n",
            "Epoch 17: Train Loss: 0.0155, Test Loss: 0.0152, Train L1 Norm: 0.0364, Test L1 Norm: 0.0223, Train Linf Norm: 1.6949, Test Linf Norm: 0.9097\n",
            "Epoch 18: Train Loss: 0.0173, Test Loss: 0.0212, Train L1 Norm: 0.0879, Test L1 Norm: 0.0276, Train Linf Norm: 4.9915, Test Linf Norm: 1.0728\n",
            "Epoch 19: Train Loss: 0.0213, Test Loss: 0.0214, Train L1 Norm: 0.0983, Test L1 Norm: 0.0206, Train Linf Norm: 5.4924, Test Linf Norm: 0.6591\n",
            "Epoch 20: Train Loss: 0.0285, Test Loss: 0.0238, Train L1 Norm: 0.1010, Test L1 Norm: 0.0430, Train Linf Norm: 5.4361, Test Linf Norm: 1.8611\n",
            "Epoch 21: Train Loss: 0.0338, Test Loss: 0.0201, Train L1 Norm: 0.1457, Test L1 Norm: 0.0325, Train Linf Norm: 8.0983, Test Linf Norm: 0.9885\n",
            "Epoch 22: Train Loss: 0.0354, Test Loss: 0.0244, Train L1 Norm: 0.1638, Test L1 Norm: 0.0417, Train Linf Norm: 9.0541, Test Linf Norm: 1.7883\n",
            "Epoch 23: Train Loss: 0.0333, Test Loss: 0.0342, Train L1 Norm: 0.1545, Test L1 Norm: 0.0281, Train Linf Norm: 8.5292, Test Linf Norm: 0.7970\n",
            "Epoch 24: Train Loss: 0.0322, Test Loss: 0.0315, Train L1 Norm: 0.1476, Test L1 Norm: 0.0568, Train Linf Norm: 8.2889, Test Linf Norm: 2.4705\n",
            "Epoch 25: Train Loss: 0.0366, Test Loss: 0.0490, Train L1 Norm: 0.1453, Test L1 Norm: 0.0416, Train Linf Norm: 7.9684, Test Linf Norm: 1.2601\n",
            "Epoch 26: Train Loss: 0.0411, Test Loss: 0.0265, Train L1 Norm: 0.0965, Test L1 Norm: 0.0338, Train Linf Norm: 4.6947, Test Linf Norm: 1.2172\n",
            "Epoch 27: Train Loss: 0.0312, Test Loss: 0.0460, Train L1 Norm: 0.1593, Test L1 Norm: 0.0361, Train Linf Norm: 9.0254, Test Linf Norm: 0.9498\n",
            "Epoch 28: Train Loss: 0.0362, Test Loss: 0.0404, Train L1 Norm: 0.1569, Test L1 Norm: 0.0568, Train Linf Norm: 8.5656, Test Linf Norm: 2.2761\n",
            "Epoch 29: Train Loss: 0.0325, Test Loss: 0.0355, Train L1 Norm: 0.1036, Test L1 Norm: 0.0506, Train Linf Norm: 5.4239, Test Linf Norm: 1.8864\n",
            "Epoch 30: Train Loss: 0.0352, Test Loss: 0.0396, Train L1 Norm: 0.1005, Test L1 Norm: 0.0486, Train Linf Norm: 5.1489, Test Linf Norm: 1.6580\n",
            "Epoch 31: Train Loss: 0.0291, Test Loss: 0.0336, Train L1 Norm: 0.1067, Test L1 Norm: 0.0311, Train Linf Norm: 5.7033, Test Linf Norm: 0.9839\n",
            "Epoch 32: Train Loss: 0.0299, Test Loss: 0.0299, Train L1 Norm: 0.1098, Test L1 Norm: 0.0310, Train Linf Norm: 5.9201, Test Linf Norm: 0.8550\n",
            "Epoch 33: Train Loss: 0.0251, Test Loss: 0.0266, Train L1 Norm: 0.1078, Test L1 Norm: 0.0364, Train Linf Norm: 5.9347, Test Linf Norm: 1.2209\n",
            "Epoch 34: Train Loss: 0.0232, Test Loss: 0.0180, Train L1 Norm: 0.1035, Test L1 Norm: 0.0286, Train Linf Norm: 5.7697, Test Linf Norm: 1.1917\n",
            "Epoch 35: Train Loss: 0.0190, Test Loss: 0.0230, Train L1 Norm: 0.0881, Test L1 Norm: 0.0215, Train Linf Norm: 4.9639, Test Linf Norm: 0.6464\n",
            "Epoch 36: Train Loss: 0.0183, Test Loss: 0.0117, Train L1 Norm: 0.0831, Test L1 Norm: 0.0157, Train Linf Norm: 4.6677, Test Linf Norm: 0.5165\n",
            "Epoch 37: Train Loss: 0.0135, Test Loss: 0.0222, Train L1 Norm: 0.0726, Test L1 Norm: 0.0284, Train Linf Norm: 4.0853, Test Linf Norm: 1.1554\n",
            "Epoch 38: Train Loss: 0.0132, Test Loss: 0.0109, Train L1 Norm: 0.0584, Test L1 Norm: 0.0168, Train Linf Norm: 3.2692, Test Linf Norm: 0.7027\n",
            "Epoch 39: Train Loss: 0.0089, Test Loss: 0.0089, Train L1 Norm: 0.0433, Test L1 Norm: 0.0149, Train Linf Norm: 2.4179, Test Linf Norm: 0.6334\n",
            "Epoch 40: Train Loss: 0.0082, Test Loss: 0.0086, Train L1 Norm: 0.0461, Test L1 Norm: 0.0136, Train Linf Norm: 2.6477, Test Linf Norm: 0.5795\n",
            "Epoch 41: Train Loss: 0.0091, Test Loss: 0.0087, Train L1 Norm: 0.0505, Test L1 Norm: 0.0131, Train Linf Norm: 2.9102, Test Linf Norm: 0.5321\n",
            "Epoch 42: Train Loss: 0.0115, Test Loss: 0.0113, Train L1 Norm: 0.0594, Test L1 Norm: 0.0168, Train Linf Norm: 3.3821, Test Linf Norm: 0.7086\n",
            "Epoch 43: Train Loss: 0.0114, Test Loss: 0.0226, Train L1 Norm: 0.0444, Test L1 Norm: 0.0196, Train Linf Norm: 2.4162, Test Linf Norm: 0.5796\n",
            "Epoch 44: Train Loss: 0.0146, Test Loss: 0.0212, Train L1 Norm: 0.0602, Test L1 Norm: 0.0242, Train Linf Norm: 3.3301, Test Linf Norm: 0.9575\n",
            "Epoch 45: Train Loss: 0.0186, Test Loss: 0.0204, Train L1 Norm: 0.0847, Test L1 Norm: 0.0326, Train Linf Norm: 4.7715, Test Linf Norm: 1.3865\n",
            "Epoch 46: Train Loss: 0.0213, Test Loss: 0.0166, Train L1 Norm: 0.0918, Test L1 Norm: 0.0328, Train Linf Norm: 5.0921, Test Linf Norm: 1.4920\n",
            "Epoch 47: Train Loss: 0.0205, Test Loss: 0.0232, Train L1 Norm: 0.0786, Test L1 Norm: 0.0255, Train Linf Norm: 4.2380, Test Linf Norm: 0.8181\n",
            "Epoch 48: Train Loss: 0.0236, Test Loss: 0.0247, Train L1 Norm: 0.1257, Test L1 Norm: 0.0367, Train Linf Norm: 7.2091, Test Linf Norm: 1.4274\n",
            "Epoch 49: Train Loss: 0.0253, Test Loss: 0.0175, Train L1 Norm: 0.1399, Test L1 Norm: 0.0290, Train Linf Norm: 8.0038, Test Linf Norm: 1.2409\n",
            "Epoch 50: Train Loss: 0.0263, Test Loss: 0.0324, Train L1 Norm: 0.1479, Test L1 Norm: 0.0529, Train Linf Norm: 8.5094, Test Linf Norm: 2.2337\n",
            "Epoch 51: Train Loss: 0.0304, Test Loss: 0.0243, Train L1 Norm: 0.1229, Test L1 Norm: 0.0390, Train Linf Norm: 6.8027, Test Linf Norm: 1.6264\n",
            "Epoch 52: Train Loss: 0.0285, Test Loss: 0.0633, Train L1 Norm: 0.1252, Test L1 Norm: 0.0449, Train Linf Norm: 6.9528, Test Linf Norm: 0.6535\n",
            "Epoch 53: Train Loss: 0.0310, Test Loss: 0.0234, Train L1 Norm: 0.1292, Test L1 Norm: 0.0411, Train Linf Norm: 7.0761, Test Linf Norm: 1.7107\n",
            "Epoch 54: Train Loss: 0.0310, Test Loss: 0.0481, Train L1 Norm: 0.0923, Test L1 Norm: 0.0332, Train Linf Norm: 4.7869, Test Linf Norm: 0.8072\n",
            "Epoch 55: Train Loss: 0.0329, Test Loss: 0.0228, Train L1 Norm: 0.1346, Test L1 Norm: 0.0555, Train Linf Norm: 7.4316, Test Linf Norm: 2.5115\n",
            "Epoch 56: Train Loss: 0.0275, Test Loss: 0.0477, Train L1 Norm: 0.0964, Test L1 Norm: 0.0537, Train Linf Norm: 5.1278, Test Linf Norm: 1.8988\n",
            "Epoch 57: Train Loss: 0.0335, Test Loss: 0.0294, Train L1 Norm: 0.0914, Test L1 Norm: 0.0340, Train Linf Norm: 4.6977, Test Linf Norm: 1.0639\n",
            "Epoch 58: Train Loss: 0.0239, Test Loss: 0.0256, Train L1 Norm: 0.0793, Test L1 Norm: 0.0510, Train Linf Norm: 4.2136, Test Linf Norm: 2.0790\n",
            "Epoch 59: Train Loss: 0.0226, Test Loss: 0.0246, Train L1 Norm: 0.1025, Test L1 Norm: 0.0258, Train Linf Norm: 5.7353, Test Linf Norm: 0.7491\n",
            "Epoch 60: Train Loss: 0.0224, Test Loss: 0.0302, Train L1 Norm: 0.0718, Test L1 Norm: 0.0297, Train Linf Norm: 3.8054, Test Linf Norm: 0.8218\n",
            "Epoch 61: Train Loss: 0.0203, Test Loss: 0.0164, Train L1 Norm: 0.0741, Test L1 Norm: 0.0270, Train Linf Norm: 4.0627, Test Linf Norm: 1.0643\n",
            "Epoch 62: Train Loss: 0.0158, Test Loss: 0.0142, Train L1 Norm: 0.0492, Test L1 Norm: 0.0150, Train Linf Norm: 2.5517, Test Linf Norm: 0.5436\n",
            "Epoch 63: Train Loss: 0.0136, Test Loss: 0.0187, Train L1 Norm: 0.0381, Test L1 Norm: 0.0165, Train Linf Norm: 1.9706, Test Linf Norm: 0.4674\n",
            "Epoch 64: Train Loss: 0.0135, Test Loss: 0.0163, Train L1 Norm: 0.0364, Test L1 Norm: 0.0171, Train Linf Norm: 1.8896, Test Linf Norm: 0.6203\n",
            "Epoch 65: Train Loss: 0.0113, Test Loss: 0.0135, Train L1 Norm: 0.0344, Test L1 Norm: 0.0167, Train Linf Norm: 1.8259, Test Linf Norm: 0.6325\n",
            "Epoch 66: Train Loss: 0.0103, Test Loss: 0.0107, Train L1 Norm: 0.0389, Test L1 Norm: 0.0137, Train Linf Norm: 2.1346, Test Linf Norm: 0.5243\n",
            "Epoch 67: Train Loss: 0.0107, Test Loss: 0.0109, Train L1 Norm: 0.0333, Test L1 Norm: 0.0123, Train Linf Norm: 1.7755, Test Linf Norm: 0.4438\n",
            "Epoch 68: Train Loss: 0.0125, Test Loss: 0.0120, Train L1 Norm: 0.0387, Test L1 Norm: 0.0116, Train Linf Norm: 2.0593, Test Linf Norm: 0.3860\n",
            "Epoch 69: Train Loss: 0.0136, Test Loss: 0.0121, Train L1 Norm: 0.0243, Test L1 Norm: 0.0213, Train Linf Norm: 1.0941, Test Linf Norm: 0.9094\n",
            "Epoch 70: Train Loss: 0.0151, Test Loss: 0.0178, Train L1 Norm: 0.0580, Test L1 Norm: 0.0193, Train Linf Norm: 3.1905, Test Linf Norm: 0.6423\n",
            "Epoch 71: Train Loss: 0.0170, Test Loss: 0.0221, Train L1 Norm: 0.0534, Test L1 Norm: 0.0229, Train Linf Norm: 2.8210, Test Linf Norm: 0.7800\n",
            "Epoch 72: Train Loss: 0.0210, Test Loss: 0.0219, Train L1 Norm: 0.0551, Test L1 Norm: 0.0304, Train Linf Norm: 2.7902, Test Linf Norm: 1.1627\n",
            "Epoch 73: Train Loss: 0.0205, Test Loss: 0.0215, Train L1 Norm: 0.0662, Test L1 Norm: 0.0361, Train Linf Norm: 3.4927, Test Linf Norm: 1.5479\n",
            "Epoch 74: Train Loss: 0.0243, Test Loss: 0.0226, Train L1 Norm: 0.0822, Test L1 Norm: 0.0242, Train Linf Norm: 4.3571, Test Linf Norm: 0.8814\n",
            "Epoch 75: Train Loss: 0.0242, Test Loss: 0.0184, Train L1 Norm: 0.0703, Test L1 Norm: 0.0288, Train Linf Norm: 3.6118, Test Linf Norm: 1.2006\n",
            "Epoch 76: Train Loss: 0.0247, Test Loss: 0.0168, Train L1 Norm: 0.0803, Test L1 Norm: 0.0409, Train Linf Norm: 4.1998, Test Linf Norm: 1.8084\n",
            "Epoch 77: Train Loss: 0.0281, Test Loss: 0.0224, Train L1 Norm: 0.1266, Test L1 Norm: 0.0400, Train Linf Norm: 7.1026, Test Linf Norm: 1.7033\n",
            "Epoch 78: Train Loss: 0.0264, Test Loss: 0.0192, Train L1 Norm: 0.1336, Test L1 Norm: 0.0369, Train Linf Norm: 7.5324, Test Linf Norm: 1.5655\n",
            "Epoch 79: Train Loss: 0.0278, Test Loss: 0.0266, Train L1 Norm: 0.0981, Test L1 Norm: 0.0326, Train Linf Norm: 5.2281, Test Linf Norm: 1.2572\n",
            "Epoch 80: Train Loss: 0.0291, Test Loss: 0.0210, Train L1 Norm: 0.1125, Test L1 Norm: 0.0684, Train Linf Norm: 6.1714, Test Linf Norm: 3.3464\n",
            "Epoch 81: Train Loss: 0.0280, Test Loss: 0.0199, Train L1 Norm: 0.1205, Test L1 Norm: 0.0309, Train Linf Norm: 6.6994, Test Linf Norm: 1.1970\n",
            "Epoch 82: Train Loss: 0.0250, Test Loss: 0.0200, Train L1 Norm: 0.1280, Test L1 Norm: 0.0377, Train Linf Norm: 7.2401, Test Linf Norm: 1.6451\n",
            "Epoch 83: Train Loss: 0.0289, Test Loss: 0.0332, Train L1 Norm: 0.0731, Test L1 Norm: 0.0507, Train Linf Norm: 3.6330, Test Linf Norm: 1.9609\n",
            "Epoch 84: Train Loss: 0.0246, Test Loss: 0.0249, Train L1 Norm: 0.1022, Test L1 Norm: 0.0206, Train Linf Norm: 5.6340, Test Linf Norm: 0.6289\n",
            "Epoch 85: Train Loss: 0.0235, Test Loss: 0.0270, Train L1 Norm: 0.0759, Test L1 Norm: 0.0346, Train Linf Norm: 3.9995, Test Linf Norm: 1.3264\n",
            "Epoch 86: Train Loss: 0.0216, Test Loss: 0.0190, Train L1 Norm: 0.0776, Test L1 Norm: 0.0188, Train Linf Norm: 4.1752, Test Linf Norm: 0.5716\n",
            "Epoch 87: Train Loss: 0.0203, Test Loss: 0.0208, Train L1 Norm: 0.0425, Test L1 Norm: 0.0253, Train Linf Norm: 2.0284, Test Linf Norm: 0.9540\n",
            "Epoch 88: Train Loss: 0.0212, Test Loss: 0.0131, Train L1 Norm: 0.0614, Test L1 Norm: 0.0186, Train Linf Norm: 3.2489, Test Linf Norm: 0.7229\n",
            "Epoch 89: Train Loss: 0.0153, Test Loss: 0.0127, Train L1 Norm: 0.0597, Test L1 Norm: 0.0175, Train Linf Norm: 3.3017, Test Linf Norm: 0.6889\n",
            "Epoch 90: Train Loss: 0.0130, Test Loss: 0.0123, Train L1 Norm: 0.0337, Test L1 Norm: 0.0144, Train Linf Norm: 1.5187, Test Linf Norm: 0.5493\n",
            "Epoch 91: Train Loss: 0.0117, Test Loss: 0.0127, Train L1 Norm: 0.0382, Test L1 Norm: 0.0137, Train Linf Norm: 2.0629, Test Linf Norm: 0.5090\n",
            "Epoch 92: Train Loss: 0.0112, Test Loss: 0.0118, Train L1 Norm: 0.0342, Test L1 Norm: 0.0129, Train Linf Norm: 1.7866, Test Linf Norm: 0.4774\n",
            "Epoch 93: Train Loss: 0.0120, Test Loss: 0.0118, Train L1 Norm: 0.0387, Test L1 Norm: 0.0128, Train Linf Norm: 2.0773, Test Linf Norm: 0.4631\n",
            "Epoch 94: Train Loss: 0.0135, Test Loss: 0.0121, Train L1 Norm: 0.0304, Test L1 Norm: 0.0125, Train Linf Norm: 1.4992, Test Linf Norm: 0.4435\n",
            "Epoch 95: Train Loss: 0.0137, Test Loss: 0.0137, Train L1 Norm: 0.0416, Test L1 Norm: 0.0149, Train Linf Norm: 2.1988, Test Linf Norm: 0.4700\n",
            "Epoch 96: Train Loss: 0.0161, Test Loss: 0.0142, Train L1 Norm: 0.0360, Test L1 Norm: 0.0201, Train Linf Norm: 1.7418, Test Linf Norm: 0.5394\n",
            "Epoch 97: Train Loss: 0.0166, Test Loss: 0.0259, Train L1 Norm: 0.0703, Test L1 Norm: 0.0257, Train Linf Norm: 3.9175, Test Linf Norm: 0.8996\n",
            "Epoch 98: Train Loss: 0.0192, Test Loss: 0.0195, Train L1 Norm: 0.0642, Test L1 Norm: 0.0200, Train Linf Norm: 3.4377, Test Linf Norm: 0.6042\n",
            "Epoch 99: Train Loss: 0.0196, Test Loss: 0.0157, Train L1 Norm: 0.0758, Test L1 Norm: 0.0183, Train Linf Norm: 4.1420, Test Linf Norm: 0.6706\n",
            "Epoch 100: Train Loss: 0.0245, Test Loss: 0.0285, Train L1 Norm: 0.0887, Test L1 Norm: 0.0291, Train Linf Norm: 4.7896, Test Linf Norm: 0.9764\n",
            "Epoch 101: Train Loss: 0.0237, Test Loss: 0.0213, Train L1 Norm: 0.0922, Test L1 Norm: 0.0235, Train Linf Norm: 5.0434, Test Linf Norm: 0.7440\n",
            "Epoch 102: Train Loss: 0.0246, Test Loss: 0.0204, Train L1 Norm: 0.1055, Test L1 Norm: 0.0247, Train Linf Norm: 5.8011, Test Linf Norm: 0.8354\n",
            "Epoch 103: Train Loss: 0.0274, Test Loss: 0.0264, Train L1 Norm: 0.0835, Test L1 Norm: 0.0382, Train Linf Norm: 4.2724, Test Linf Norm: 1.3972\n",
            "Epoch 104: Train Loss: 0.0266, Test Loss: 0.0340, Train L1 Norm: 0.1048, Test L1 Norm: 0.0301, Train Linf Norm: 5.6774, Test Linf Norm: 0.8899\n",
            "Epoch 105: Train Loss: 0.0258, Test Loss: 0.0297, Train L1 Norm: 0.0907, Test L1 Norm: 0.0317, Train Linf Norm: 4.8293, Test Linf Norm: 1.0307\n",
            "Epoch 106: Train Loss: 0.0283, Test Loss: 0.0241, Train L1 Norm: 0.1108, Test L1 Norm: 0.0312, Train Linf Norm: 6.0268, Test Linf Norm: 0.9500\n",
            "Epoch 107: Train Loss: 0.0267, Test Loss: 0.0257, Train L1 Norm: 0.1252, Test L1 Norm: 0.0510, Train Linf Norm: 6.8776, Test Linf Norm: 2.0475\n",
            "Epoch 108: Train Loss: 0.0262, Test Loss: 0.0299, Train L1 Norm: 0.1343, Test L1 Norm: 0.0463, Train Linf Norm: 7.5956, Test Linf Norm: 1.6236\n",
            "Epoch 109: Train Loss: 0.0247, Test Loss: 0.0245, Train L1 Norm: 0.1007, Test L1 Norm: 0.0430, Train Linf Norm: 5.5187, Test Linf Norm: 1.7567\n",
            "Epoch 110: Train Loss: 0.0218, Test Loss: 0.0234, Train L1 Norm: 0.0884, Test L1 Norm: 0.0213, Train Linf Norm: 4.8482, Test Linf Norm: 0.5966\n",
            "Epoch 111: Train Loss: 0.0229, Test Loss: 0.0258, Train L1 Norm: 0.0772, Test L1 Norm: 0.0478, Train Linf Norm: 4.1494, Test Linf Norm: 1.7320\n",
            "Epoch 112: Train Loss: 0.0216, Test Loss: 0.0201, Train L1 Norm: 0.0716, Test L1 Norm: 0.0245, Train Linf Norm: 3.7946, Test Linf Norm: 0.5989\n",
            "Epoch 113: Train Loss: 0.0171, Test Loss: 0.0191, Train L1 Norm: 0.0642, Test L1 Norm: 0.0209, Train Linf Norm: 3.5034, Test Linf Norm: 0.6950\n",
            "Epoch 114: Train Loss: 0.0177, Test Loss: 0.0191, Train L1 Norm: 0.0482, Test L1 Norm: 0.0166, Train Linf Norm: 2.4901, Test Linf Norm: 0.5754\n",
            "Epoch 115: Train Loss: 0.0149, Test Loss: 0.0149, Train L1 Norm: 0.0378, Test L1 Norm: 0.0133, Train Linf Norm: 1.9269, Test Linf Norm: 0.4161\n",
            "Epoch 116: Train Loss: 0.0132, Test Loss: 0.0120, Train L1 Norm: 0.0312, Test L1 Norm: 0.0152, Train Linf Norm: 1.5473, Test Linf Norm: 0.5196\n",
            "Epoch 117: Train Loss: 0.0113, Test Loss: 0.0112, Train L1 Norm: 0.0292, Test L1 Norm: 0.0126, Train Linf Norm: 1.4392, Test Linf Norm: 0.4527\n",
            "Epoch 118: Train Loss: 0.0106, Test Loss: 0.0110, Train L1 Norm: 0.0273, Test L1 Norm: 0.0121, Train Linf Norm: 1.3939, Test Linf Norm: 0.4335\n",
            "Epoch 119: Train Loss: 0.0114, Test Loss: 0.0121, Train L1 Norm: 0.0280, Test L1 Norm: 0.0133, Train Linf Norm: 1.4061, Test Linf Norm: 0.4806\n",
            "Epoch 120: Train Loss: 0.0123, Test Loss: 0.0132, Train L1 Norm: 0.0276, Test L1 Norm: 0.0175, Train Linf Norm: 1.3407, Test Linf Norm: 0.6307\n",
            "Epoch 121: Train Loss: 0.0129, Test Loss: 0.0127, Train L1 Norm: 0.0398, Test L1 Norm: 0.0208, Train Linf Norm: 2.0851, Test Linf Norm: 0.8139\n",
            "Epoch 122: Train Loss: 0.0154, Test Loss: 0.0179, Train L1 Norm: 0.0469, Test L1 Norm: 0.0162, Train Linf Norm: 2.4605, Test Linf Norm: 0.5025\n",
            "Epoch 123: Train Loss: 0.0177, Test Loss: 0.0194, Train L1 Norm: 0.0666, Test L1 Norm: 0.0175, Train Linf Norm: 3.6073, Test Linf Norm: 0.4864\n",
            "Epoch 124: Train Loss: 0.0179, Test Loss: 0.0217, Train L1 Norm: 0.0522, Test L1 Norm: 0.0397, Train Linf Norm: 2.6585, Test Linf Norm: 1.5056\n",
            "Epoch 125: Train Loss: 0.0215, Test Loss: 0.0225, Train L1 Norm: 0.0714, Test L1 Norm: 0.0270, Train Linf Norm: 3.7929, Test Linf Norm: 1.0247\n",
            "Epoch 126: Train Loss: 0.0206, Test Loss: 0.0286, Train L1 Norm: 0.0785, Test L1 Norm: 0.0353, Train Linf Norm: 4.2351, Test Linf Norm: 1.4293\n",
            "Epoch 127: Train Loss: 0.0233, Test Loss: 0.0192, Train L1 Norm: 0.0767, Test L1 Norm: 0.0313, Train Linf Norm: 3.9927, Test Linf Norm: 1.1399\n",
            "Epoch 128: Train Loss: 0.0235, Test Loss: 0.0171, Train L1 Norm: 0.0894, Test L1 Norm: 0.0241, Train Linf Norm: 4.8400, Test Linf Norm: 0.7542\n",
            "Epoch 129: Train Loss: 0.0246, Test Loss: 0.0231, Train L1 Norm: 0.0765, Test L1 Norm: 0.0279, Train Linf Norm: 3.9597, Test Linf Norm: 1.0750\n",
            "Epoch 130: Train Loss: 0.0244, Test Loss: 0.0237, Train L1 Norm: 0.1204, Test L1 Norm: 0.0295, Train Linf Norm: 6.7638, Test Linf Norm: 1.0984\n",
            "Epoch 131: Train Loss: 0.0276, Test Loss: 0.0222, Train L1 Norm: 0.0958, Test L1 Norm: 0.0602, Train Linf Norm: 5.0572, Test Linf Norm: 2.2016\n",
            "Epoch 132: Train Loss: 0.0255, Test Loss: 0.0348, Train L1 Norm: 0.1361, Test L1 Norm: 0.0384, Train Linf Norm: 7.7336, Test Linf Norm: 1.0122\n",
            "Epoch 133: Train Loss: 0.0256, Test Loss: 0.0274, Train L1 Norm: 0.0940, Test L1 Norm: 0.0278, Train Linf Norm: 5.0232, Test Linf Norm: 0.8788\n",
            "Epoch 134: Train Loss: 0.0237, Test Loss: 0.0353, Train L1 Norm: 0.0990, Test L1 Norm: 0.0494, Train Linf Norm: 5.4282, Test Linf Norm: 1.6651\n",
            "Epoch 135: Train Loss: 0.0248, Test Loss: 0.0169, Train L1 Norm: 0.0901, Test L1 Norm: 0.0171, Train Linf Norm: 4.8461, Test Linf Norm: 0.5187\n",
            "Epoch 136: Train Loss: 0.0234, Test Loss: 0.0373, Train L1 Norm: 0.0842, Test L1 Norm: 0.0262, Train Linf Norm: 4.5363, Test Linf Norm: 0.7251\n",
            "Epoch 137: Train Loss: 0.0226, Test Loss: 0.0310, Train L1 Norm: 0.0780, Test L1 Norm: 0.0239, Train Linf Norm: 4.1594, Test Linf Norm: 0.7244\n",
            "Epoch 138: Train Loss: 0.0199, Test Loss: 0.0179, Train L1 Norm: 0.0542, Test L1 Norm: 0.0189, Train Linf Norm: 2.7479, Test Linf Norm: 0.5548\n",
            "Epoch 139: Train Loss: 0.0165, Test Loss: 0.0162, Train L1 Norm: 0.0444, Test L1 Norm: 0.0132, Train Linf Norm: 2.2515, Test Linf Norm: 0.3592\n",
            "Epoch 140: Train Loss: 0.0164, Test Loss: 0.0146, Train L1 Norm: 0.0572, Test L1 Norm: 0.0139, Train Linf Norm: 3.1022, Test Linf Norm: 0.3510\n",
            "Epoch 141: Train Loss: 0.0143, Test Loss: 0.0122, Train L1 Norm: 0.0321, Test L1 Norm: 0.0123, Train Linf Norm: 1.5294, Test Linf Norm: 0.4024\n",
            "Epoch 142: Train Loss: 0.0123, Test Loss: 0.0163, Train L1 Norm: 0.0341, Test L1 Norm: 0.0136, Train Linf Norm: 1.7605, Test Linf Norm: 0.3665\n",
            "Epoch 143: Train Loss: 0.0110, Test Loss: 0.0111, Train L1 Norm: 0.0296, Test L1 Norm: 0.0104, Train Linf Norm: 1.5306, Test Linf Norm: 0.3216\n",
            "Epoch 144: Train Loss: 0.0104, Test Loss: 0.0106, Train L1 Norm: 0.0282, Test L1 Norm: 0.0104, Train Linf Norm: 1.4449, Test Linf Norm: 0.3291\n",
            "Epoch 145: Train Loss: 0.0108, Test Loss: 0.0121, Train L1 Norm: 0.0279, Test L1 Norm: 0.0105, Train Linf Norm: 1.4190, Test Linf Norm: 0.3178\n",
            "Epoch 146: Train Loss: 0.0122, Test Loss: 0.0129, Train L1 Norm: 0.0295, Test L1 Norm: 0.0135, Train Linf Norm: 1.4732, Test Linf Norm: 0.3517\n",
            "Epoch 147: Train Loss: 0.0129, Test Loss: 0.0126, Train L1 Norm: 0.0338, Test L1 Norm: 0.0135, Train Linf Norm: 1.7167, Test Linf Norm: 0.3769\n",
            "Epoch 148: Train Loss: 0.0141, Test Loss: 0.0131, Train L1 Norm: 0.0382, Test L1 Norm: 0.0134, Train Linf Norm: 1.9387, Test Linf Norm: 0.3258\n",
            "Epoch 149: Train Loss: 0.0173, Test Loss: 0.0158, Train L1 Norm: 0.0450, Test L1 Norm: 0.0146, Train Linf Norm: 2.2731, Test Linf Norm: 0.4100\n",
            "Epoch 150: Train Loss: 0.0189, Test Loss: 0.0172, Train L1 Norm: 0.0714, Test L1 Norm: 0.0166, Train Linf Norm: 3.8606, Test Linf Norm: 0.5415\n",
            "Epoch 151: Train Loss: 0.0182, Test Loss: 0.0156, Train L1 Norm: 0.0702, Test L1 Norm: 0.0239, Train Linf Norm: 3.8050, Test Linf Norm: 0.9471\n",
            "Epoch 152: Train Loss: 0.0209, Test Loss: 0.0303, Train L1 Norm: 0.0854, Test L1 Norm: 0.0337, Train Linf Norm: 4.6471, Test Linf Norm: 1.0732\n",
            "Epoch 153: Train Loss: 0.0235, Test Loss: 0.0305, Train L1 Norm: 0.0803, Test L1 Norm: 0.0363, Train Linf Norm: 4.2346, Test Linf Norm: 1.1319\n",
            "Epoch 154: Train Loss: 0.0220, Test Loss: 0.0266, Train L1 Norm: 0.0910, Test L1 Norm: 0.0396, Train Linf Norm: 4.9906, Test Linf Norm: 1.1519\n",
            "Epoch 155: Train Loss: 0.0244, Test Loss: 0.0296, Train L1 Norm: 0.1138, Test L1 Norm: 0.0401, Train Linf Norm: 6.3476, Test Linf Norm: 1.5126\n",
            "Epoch 156: Train Loss: 0.0253, Test Loss: 0.0309, Train L1 Norm: 0.1212, Test L1 Norm: 0.0444, Train Linf Norm: 6.7293, Test Linf Norm: 1.8179\n",
            "Epoch 157: Train Loss: 0.0246, Test Loss: 0.0268, Train L1 Norm: 0.1135, Test L1 Norm: 0.0338, Train Linf Norm: 6.3356, Test Linf Norm: 1.2756\n",
            "Epoch 158: Train Loss: 0.0244, Test Loss: 0.0227, Train L1 Norm: 0.0940, Test L1 Norm: 0.0261, Train Linf Norm: 5.0760, Test Linf Norm: 0.9917\n",
            "Epoch 159: Train Loss: 0.0245, Test Loss: 0.0258, Train L1 Norm: 0.0919, Test L1 Norm: 0.0284, Train Linf Norm: 4.9409, Test Linf Norm: 0.6960\n",
            "Epoch 160: Train Loss: 0.0210, Test Loss: 0.0418, Train L1 Norm: 0.0918, Test L1 Norm: 0.0314, Train Linf Norm: 5.0147, Test Linf Norm: 0.7163\n",
            "Epoch 161: Train Loss: 0.0229, Test Loss: 0.0255, Train L1 Norm: 0.0770, Test L1 Norm: 0.0273, Train Linf Norm: 4.0952, Test Linf Norm: 0.9437\n",
            "Epoch 162: Train Loss: 0.0205, Test Loss: 0.0172, Train L1 Norm: 0.0601, Test L1 Norm: 0.0222, Train Linf Norm: 3.0748, Test Linf Norm: 0.8041\n",
            "Epoch 163: Train Loss: 0.0226, Test Loss: 0.0168, Train L1 Norm: 0.0591, Test L1 Norm: 0.0263, Train Linf Norm: 2.9471, Test Linf Norm: 0.8389\n",
            "Epoch 164: Train Loss: 0.0178, Test Loss: 0.0175, Train L1 Norm: 0.0600, Test L1 Norm: 0.0320, Train Linf Norm: 3.1507, Test Linf Norm: 1.2468\n",
            "Epoch 165: Train Loss: 0.0193, Test Loss: 0.0139, Train L1 Norm: 0.0517, Test L1 Norm: 0.0201, Train Linf Norm: 2.6432, Test Linf Norm: 0.7329\n",
            "Epoch 166: Train Loss: 0.0150, Test Loss: 0.0171, Train L1 Norm: 0.0294, Test L1 Norm: 0.0158, Train Linf Norm: 1.3729, Test Linf Norm: 0.4802\n",
            "Epoch 167: Train Loss: 0.0139, Test Loss: 0.0188, Train L1 Norm: 0.0352, Test L1 Norm: 0.0155, Train Linf Norm: 1.7889, Test Linf Norm: 0.4506\n",
            "Epoch 168: Train Loss: 0.0132, Test Loss: 0.0121, Train L1 Norm: 0.0358, Test L1 Norm: 0.0112, Train Linf Norm: 1.8587, Test Linf Norm: 0.3450\n",
            "Epoch 169: Train Loss: 0.0107, Test Loss: 0.0109, Train L1 Norm: 0.0273, Test L1 Norm: 0.0108, Train Linf Norm: 1.3880, Test Linf Norm: 0.3460\n",
            "Epoch 170: Train Loss: 0.0103, Test Loss: 0.0105, Train L1 Norm: 0.0271, Test L1 Norm: 0.0107, Train Linf Norm: 1.3922, Test Linf Norm: 0.3527\n",
            "Epoch 171: Train Loss: 0.0111, Test Loss: 0.0116, Train L1 Norm: 0.0282, Test L1 Norm: 0.0107, Train Linf Norm: 1.4430, Test Linf Norm: 0.3197\n",
            "Epoch 172: Train Loss: 0.0122, Test Loss: 0.0123, Train L1 Norm: 0.0281, Test L1 Norm: 0.0130, Train Linf Norm: 1.3890, Test Linf Norm: 0.4355\n",
            "Epoch 173: Train Loss: 0.0128, Test Loss: 0.0117, Train L1 Norm: 0.0236, Test L1 Norm: 0.0110, Train Linf Norm: 1.0606, Test Linf Norm: 0.3298\n",
            "Epoch 174: Train Loss: 0.0148, Test Loss: 0.0130, Train L1 Norm: 0.0386, Test L1 Norm: 0.0238, Train Linf Norm: 1.9539, Test Linf Norm: 0.9775\n",
            "Epoch 175: Train Loss: 0.0158, Test Loss: 0.0208, Train L1 Norm: 0.0504, Test L1 Norm: 0.0182, Train Linf Norm: 2.6389, Test Linf Norm: 0.5257\n",
            "Epoch 176: Train Loss: 0.0172, Test Loss: 0.0174, Train L1 Norm: 0.0568, Test L1 Norm: 0.0194, Train Linf Norm: 3.0016, Test Linf Norm: 0.6168\n",
            "Epoch 177: Train Loss: 0.0188, Test Loss: 0.0157, Train L1 Norm: 0.0806, Test L1 Norm: 0.0238, Train Linf Norm: 4.4672, Test Linf Norm: 0.9565\n",
            "Epoch 178: Train Loss: 0.0215, Test Loss: 0.0267, Train L1 Norm: 0.0706, Test L1 Norm: 0.0262, Train Linf Norm: 3.6986, Test Linf Norm: 0.6692\n",
            "Epoch 179: Train Loss: 0.0241, Test Loss: 0.0185, Train L1 Norm: 0.0606, Test L1 Norm: 0.0264, Train Linf Norm: 2.9955, Test Linf Norm: 0.9052\n",
            "Epoch 180: Train Loss: 0.0234, Test Loss: 0.0209, Train L1 Norm: 0.1033, Test L1 Norm: 0.0268, Train Linf Norm: 5.7245, Test Linf Norm: 0.9670\n",
            "Epoch 181: Train Loss: 0.0217, Test Loss: 0.0168, Train L1 Norm: 0.0784, Test L1 Norm: 0.0237, Train Linf Norm: 4.1946, Test Linf Norm: 0.8730\n",
            "Epoch 182: Train Loss: 0.0255, Test Loss: 0.0189, Train L1 Norm: 0.0848, Test L1 Norm: 0.0239, Train Linf Norm: 4.4707, Test Linf Norm: 0.8951\n",
            "Epoch 183: Train Loss: 0.0237, Test Loss: 0.0221, Train L1 Norm: 0.0863, Test L1 Norm: 0.0319, Train Linf Norm: 4.5698, Test Linf Norm: 1.3340\n",
            "Epoch 184: Train Loss: 0.0231, Test Loss: 0.0273, Train L1 Norm: 0.1294, Test L1 Norm: 0.0307, Train Linf Norm: 7.4035, Test Linf Norm: 0.9771\n",
            "Epoch 185: Train Loss: 0.0221, Test Loss: 0.0236, Train L1 Norm: 0.1098, Test L1 Norm: 0.0378, Train Linf Norm: 6.1533, Test Linf Norm: 1.4779\n",
            "Epoch 186: Train Loss: 0.0233, Test Loss: 0.0208, Train L1 Norm: 0.1081, Test L1 Norm: 0.0214, Train Linf Norm: 6.0144, Test Linf Norm: 0.6070\n",
            "Epoch 187: Train Loss: 0.0229, Test Loss: 0.0174, Train L1 Norm: 0.0700, Test L1 Norm: 0.0286, Train Linf Norm: 3.5943, Test Linf Norm: 1.1313\n",
            "Epoch 188: Train Loss: 0.0212, Test Loss: 0.0175, Train L1 Norm: 0.0778, Test L1 Norm: 0.0210, Train Linf Norm: 4.1774, Test Linf Norm: 0.7461\n",
            "Epoch 189: Train Loss: 0.0197, Test Loss: 0.0162, Train L1 Norm: 0.0721, Test L1 Norm: 0.0223, Train Linf Norm: 3.8791, Test Linf Norm: 0.8045\n",
            "Epoch 190: Train Loss: 0.0186, Test Loss: 0.0205, Train L1 Norm: 0.0593, Test L1 Norm: 0.0214, Train Linf Norm: 3.1042, Test Linf Norm: 0.7533\n",
            "Epoch 191: Train Loss: 0.0172, Test Loss: 0.0132, Train L1 Norm: 0.0393, Test L1 Norm: 0.0215, Train Linf Norm: 1.8888, Test Linf Norm: 0.8936\n",
            "Epoch 192: Train Loss: 0.0149, Test Loss: 0.0142, Train L1 Norm: 0.0471, Test L1 Norm: 0.0148, Train Linf Norm: 2.4477, Test Linf Norm: 0.5249\n",
            "Epoch 193: Train Loss: 0.0128, Test Loss: 0.0136, Train L1 Norm: 0.0319, Test L1 Norm: 0.0146, Train Linf Norm: 1.5933, Test Linf Norm: 0.4346\n",
            "Epoch 194: Train Loss: 0.0117, Test Loss: 0.0121, Train L1 Norm: 0.0265, Test L1 Norm: 0.0134, Train Linf Norm: 1.2847, Test Linf Norm: 0.4509\n",
            "Epoch 195: Train Loss: 0.0105, Test Loss: 0.0112, Train L1 Norm: 0.0247, Test L1 Norm: 0.0110, Train Linf Norm: 1.2326, Test Linf Norm: 0.3683\n",
            "Epoch 196: Train Loss: 0.0101, Test Loss: 0.0103, Train L1 Norm: 0.0246, Test L1 Norm: 0.0110, Train Linf Norm: 1.2426, Test Linf Norm: 0.3858\n",
            "Epoch 197: Train Loss: 0.0104, Test Loss: 0.0109, Train L1 Norm: 0.0250, Test L1 Norm: 0.0113, Train Linf Norm: 1.2469, Test Linf Norm: 0.3959\n",
            "Epoch 198: Train Loss: 0.0119, Test Loss: 0.0133, Train L1 Norm: 0.0236, Test L1 Norm: 0.0140, Train Linf Norm: 1.0788, Test Linf Norm: 0.4807\n",
            "Epoch 199: Train Loss: 0.0132, Test Loss: 0.0132, Train L1 Norm: 0.0250, Test L1 Norm: 0.0162, Train Linf Norm: 1.1350, Test Linf Norm: 0.4808\n",
            "Epoch 200: Train Loss: 0.0133, Test Loss: 0.0132, Train L1 Norm: 0.0403, Test L1 Norm: 0.0184, Train Linf Norm: 2.0828, Test Linf Norm: 0.6542\n",
            "Epoch 201: Train Loss: 0.0178, Test Loss: 0.0127, Train L1 Norm: 0.0418, Test L1 Norm: 0.0245, Train Linf Norm: 2.0419, Test Linf Norm: 1.0522\n",
            "Epoch 202: Train Loss: 0.0179, Test Loss: 0.0187, Train L1 Norm: 0.0484, Test L1 Norm: 0.0208, Train Linf Norm: 2.4415, Test Linf Norm: 0.7237\n",
            "Epoch 203: Train Loss: 0.0183, Test Loss: 0.0162, Train L1 Norm: 0.0672, Test L1 Norm: 0.0238, Train Linf Norm: 3.6127, Test Linf Norm: 0.9666\n",
            "Epoch 204: Train Loss: 0.0198, Test Loss: 0.0172, Train L1 Norm: 0.0964, Test L1 Norm: 0.0246, Train Linf Norm: 5.4316, Test Linf Norm: 0.9633\n",
            "Epoch 205: Train Loss: 0.0207, Test Loss: 0.0191, Train L1 Norm: 0.0984, Test L1 Norm: 0.0332, Train Linf Norm: 5.4307, Test Linf Norm: 1.3214\n",
            "Epoch 206: Train Loss: 0.0226, Test Loss: 0.0238, Train L1 Norm: 0.0761, Test L1 Norm: 0.0299, Train Linf Norm: 3.9903, Test Linf Norm: 1.1739\n",
            "Epoch 207: Train Loss: 0.0220, Test Loss: 0.0208, Train L1 Norm: 0.1267, Test L1 Norm: 0.0231, Train Linf Norm: 7.2511, Test Linf Norm: 0.8017\n",
            "Epoch 208: Train Loss: 0.0255, Test Loss: 0.0237, Train L1 Norm: 0.0877, Test L1 Norm: 0.0262, Train Linf Norm: 4.6210, Test Linf Norm: 0.6675\n",
            "Epoch 209: Train Loss: 0.0244, Test Loss: 0.0411, Train L1 Norm: 0.1020, Test L1 Norm: 0.0687, Train Linf Norm: 5.6060, Test Linf Norm: 3.0010\n",
            "Epoch 210: Train Loss: 0.0237, Test Loss: 0.0179, Train L1 Norm: 0.0977, Test L1 Norm: 0.0525, Train Linf Norm: 5.3608, Test Linf Norm: 2.2905\n",
            "Epoch 211: Train Loss: 0.0228, Test Loss: 0.0332, Train L1 Norm: 0.0761, Test L1 Norm: 0.0307, Train Linf Norm: 4.0235, Test Linf Norm: 0.7140\n",
            "Epoch 212: Train Loss: 0.0256, Test Loss: 0.0289, Train L1 Norm: 0.0631, Test L1 Norm: 0.0235, Train Linf Norm: 3.0963, Test Linf Norm: 0.5591\n",
            "Epoch 213: Train Loss: 0.0227, Test Loss: 0.0483, Train L1 Norm: 0.0638, Test L1 Norm: 0.0425, Train Linf Norm: 3.2499, Test Linf Norm: 1.2543\n",
            "Epoch 214: Train Loss: 0.0212, Test Loss: 0.0313, Train L1 Norm: 0.0732, Test L1 Norm: 0.0261, Train Linf Norm: 3.8948, Test Linf Norm: 0.8662\n",
            "Epoch 215: Train Loss: 0.0201, Test Loss: 0.0170, Train L1 Norm: 0.0873, Test L1 Norm: 0.0192, Train Linf Norm: 4.8524, Test Linf Norm: 0.6143\n",
            "Epoch 216: Train Loss: 0.0185, Test Loss: 0.0162, Train L1 Norm: 0.0782, Test L1 Norm: 0.0155, Train Linf Norm: 4.3314, Test Linf Norm: 0.4428\n",
            "Epoch 217: Train Loss: 0.0164, Test Loss: 0.0147, Train L1 Norm: 0.0522, Test L1 Norm: 0.0161, Train Linf Norm: 2.7300, Test Linf Norm: 0.5056\n",
            "Epoch 218: Train Loss: 0.0147, Test Loss: 0.0167, Train L1 Norm: 0.0459, Test L1 Norm: 0.0255, Train Linf Norm: 2.4258, Test Linf Norm: 0.9916\n",
            "Epoch 219: Train Loss: 0.0155, Test Loss: 0.0219, Train L1 Norm: 0.0424, Test L1 Norm: 0.0184, Train Linf Norm: 2.1818, Test Linf Norm: 0.5630\n",
            "Epoch 220: Train Loss: 0.0123, Test Loss: 0.0146, Train L1 Norm: 0.0389, Test L1 Norm: 0.0169, Train Linf Norm: 2.0714, Test Linf Norm: 0.5881\n",
            "Epoch 221: Train Loss: 0.0110, Test Loss: 0.0111, Train L1 Norm: 0.0365, Test L1 Norm: 0.0139, Train Linf Norm: 1.9579, Test Linf Norm: 0.5136\n",
            "Epoch 222: Train Loss: 0.0102, Test Loss: 0.0104, Train L1 Norm: 0.0314, Test L1 Norm: 0.0129, Train Linf Norm: 1.6079, Test Linf Norm: 0.4805\n",
            "Epoch 223: Train Loss: 0.0105, Test Loss: 0.0121, Train L1 Norm: 0.0293, Test L1 Norm: 0.0143, Train Linf Norm: 1.5164, Test Linf Norm: 0.4627\n",
            "Epoch 224: Train Loss: 0.0121, Test Loss: 0.0116, Train L1 Norm: 0.0348, Test L1 Norm: 0.0133, Train Linf Norm: 1.7996, Test Linf Norm: 0.4590\n",
            "Epoch 225: Train Loss: 0.0131, Test Loss: 0.0126, Train L1 Norm: 0.0435, Test L1 Norm: 0.0144, Train Linf Norm: 2.3106, Test Linf Norm: 0.4941\n",
            "Epoch 226: Train Loss: 0.0141, Test Loss: 0.0127, Train L1 Norm: 0.0420, Test L1 Norm: 0.0140, Train Linf Norm: 2.1677, Test Linf Norm: 0.4611\n",
            "Epoch 227: Train Loss: 0.0150, Test Loss: 0.0133, Train L1 Norm: 0.0406, Test L1 Norm: 0.0216, Train Linf Norm: 2.0364, Test Linf Norm: 0.8885\n",
            "Epoch 228: Train Loss: 0.0170, Test Loss: 0.0146, Train L1 Norm: 0.0482, Test L1 Norm: 0.0222, Train Linf Norm: 2.4563, Test Linf Norm: 0.8728\n",
            "Epoch 229: Train Loss: 0.0195, Test Loss: 0.0170, Train L1 Norm: 0.1017, Test L1 Norm: 0.0209, Train Linf Norm: 5.7237, Test Linf Norm: 0.7983\n",
            "Epoch 230: Train Loss: 0.0198, Test Loss: 0.0185, Train L1 Norm: 0.0699, Test L1 Norm: 0.0314, Train Linf Norm: 3.7233, Test Linf Norm: 1.2971\n",
            "Epoch 231: Train Loss: 0.0210, Test Loss: 0.0207, Train L1 Norm: 0.0977, Test L1 Norm: 0.0316, Train Linf Norm: 5.4028, Test Linf Norm: 1.2437\n",
            "Epoch 232: Train Loss: 0.0217, Test Loss: 0.0166, Train L1 Norm: 0.1106, Test L1 Norm: 0.0376, Train Linf Norm: 6.2503, Test Linf Norm: 1.5759\n",
            "Epoch 233: Train Loss: 0.0227, Test Loss: 0.0200, Train L1 Norm: 0.0913, Test L1 Norm: 0.0284, Train Linf Norm: 4.9370, Test Linf Norm: 1.1553\n",
            "Epoch 234: Train Loss: 0.0239, Test Loss: 0.0212, Train L1 Norm: 0.0801, Test L1 Norm: 0.0290, Train Linf Norm: 4.1668, Test Linf Norm: 1.1615\n",
            "Epoch 235: Train Loss: 0.0244, Test Loss: 0.0178, Train L1 Norm: 0.1228, Test L1 Norm: 0.0221, Train Linf Norm: 6.8991, Test Linf Norm: 0.7048\n",
            "Epoch 236: Train Loss: 0.0214, Test Loss: 0.0222, Train L1 Norm: 0.0561, Test L1 Norm: 0.0285, Train Linf Norm: 2.7277, Test Linf Norm: 0.8524\n",
            "Epoch 237: Train Loss: 0.0230, Test Loss: 0.0211, Train L1 Norm: 0.1072, Test L1 Norm: 0.0558, Train Linf Norm: 5.9829, Test Linf Norm: 2.4770\n",
            "Epoch 238: Train Loss: 0.0239, Test Loss: 0.0187, Train L1 Norm: 0.1236, Test L1 Norm: 0.0315, Train Linf Norm: 6.9821, Test Linf Norm: 1.2596\n",
            "Epoch 239: Train Loss: 0.0212, Test Loss: 0.0185, Train L1 Norm: 0.0610, Test L1 Norm: 0.0335, Train Linf Norm: 3.0906, Test Linf Norm: 1.4398\n",
            "Epoch 240: Train Loss: 0.0251, Test Loss: 0.0254, Train L1 Norm: 0.0862, Test L1 Norm: 0.0413, Train Linf Norm: 4.6200, Test Linf Norm: 1.8483\n",
            "Epoch 241: Train Loss: 0.0206, Test Loss: 0.0233, Train L1 Norm: 0.0916, Test L1 Norm: 0.0276, Train Linf Norm: 5.0728, Test Linf Norm: 1.0705\n",
            "Epoch 242: Train Loss: 0.0197, Test Loss: 0.0175, Train L1 Norm: 0.0825, Test L1 Norm: 0.0267, Train Linf Norm: 4.5362, Test Linf Norm: 1.0216\n",
            "Epoch 243: Train Loss: 0.0194, Test Loss: 0.0196, Train L1 Norm: 0.0639, Test L1 Norm: 0.0222, Train Linf Norm: 3.4077, Test Linf Norm: 0.7969\n",
            "Epoch 244: Train Loss: 0.0155, Test Loss: 0.0135, Train L1 Norm: 0.0487, Test L1 Norm: 0.0160, Train Linf Norm: 2.5670, Test Linf Norm: 0.5808\n",
            "Epoch 245: Train Loss: 0.0137, Test Loss: 0.0139, Train L1 Norm: 0.0423, Test L1 Norm: 0.0152, Train Linf Norm: 2.1511, Test Linf Norm: 0.5309\n",
            "Epoch 246: Train Loss: 0.0128, Test Loss: 0.0114, Train L1 Norm: 0.0294, Test L1 Norm: 0.0129, Train Linf Norm: 1.4503, Test Linf Norm: 0.4566\n",
            "Epoch 247: Train Loss: 0.0110, Test Loss: 0.0112, Train L1 Norm: 0.0268, Test L1 Norm: 0.0124, Train Linf Norm: 1.3453, Test Linf Norm: 0.4405\n",
            "Epoch 248: Train Loss: 0.0104, Test Loss: 0.0106, Train L1 Norm: 0.0242, Test L1 Norm: 0.0122, Train Linf Norm: 1.1918, Test Linf Norm: 0.4446\n",
            "Epoch 249: Train Loss: 0.0110, Test Loss: 0.0114, Train L1 Norm: 0.0248, Test L1 Norm: 0.0140, Train Linf Norm: 1.2119, Test Linf Norm: 0.5023\n",
            "Epoch 250: Train Loss: 0.0119, Test Loss: 0.0112, Train L1 Norm: 0.0270, Test L1 Norm: 0.0132, Train Linf Norm: 1.3223, Test Linf Norm: 0.4643\n",
            "Epoch 251: Train Loss: 0.0125, Test Loss: 0.0119, Train L1 Norm: 0.0296, Test L1 Norm: 0.0124, Train Linf Norm: 1.4608, Test Linf Norm: 0.4334\n",
            "Epoch 252: Train Loss: 0.0143, Test Loss: 0.0128, Train L1 Norm: 0.0447, Test L1 Norm: 0.0138, Train Linf Norm: 2.3451, Test Linf Norm: 0.4227\n",
            "Epoch 253: Train Loss: 0.0151, Test Loss: 0.0146, Train L1 Norm: 0.0311, Test L1 Norm: 0.0125, Train Linf Norm: 1.4469, Test Linf Norm: 0.3542\n",
            "Epoch 254: Train Loss: 0.0166, Test Loss: 0.0145, Train L1 Norm: 0.0407, Test L1 Norm: 0.0171, Train Linf Norm: 1.9984, Test Linf Norm: 0.5664\n",
            "Epoch 255: Train Loss: 0.0185, Test Loss: 0.0172, Train L1 Norm: 0.0678, Test L1 Norm: 0.0152, Train Linf Norm: 3.6513, Test Linf Norm: 0.4304\n",
            "Epoch 256: Train Loss: 0.0210, Test Loss: 0.0309, Train L1 Norm: 0.0927, Test L1 Norm: 0.0265, Train Linf Norm: 5.1753, Test Linf Norm: 0.8216\n",
            "Epoch 257: Train Loss: 0.0205, Test Loss: 0.0367, Train L1 Norm: 0.1135, Test L1 Norm: 0.0301, Train Linf Norm: 6.4733, Test Linf Norm: 0.8510\n",
            "Epoch 258: Train Loss: 0.0217, Test Loss: 0.0213, Train L1 Norm: 0.1104, Test L1 Norm: 0.0360, Train Linf Norm: 6.2224, Test Linf Norm: 1.3491\n",
            "Epoch 259: Train Loss: 0.0213, Test Loss: 0.0177, Train L1 Norm: 0.1283, Test L1 Norm: 0.0242, Train Linf Norm: 7.3341, Test Linf Norm: 0.9551\n",
            "Epoch 260: Train Loss: 0.0238, Test Loss: 0.0294, Train L1 Norm: 0.1048, Test L1 Norm: 0.0410, Train Linf Norm: 5.8047, Test Linf Norm: 1.6010\n",
            "Epoch 261: Train Loss: 0.0232, Test Loss: 0.0483, Train L1 Norm: 0.0946, Test L1 Norm: 0.0441, Train Linf Norm: 5.1219, Test Linf Norm: 1.2217\n",
            "Epoch 262: Train Loss: 0.0242, Test Loss: 0.0233, Train L1 Norm: 0.1311, Test L1 Norm: 0.0243, Train Linf Norm: 7.4562, Test Linf Norm: 0.5600\n",
            "Epoch 263: Train Loss: 0.0213, Test Loss: 0.0186, Train L1 Norm: 0.0681, Test L1 Norm: 0.0312, Train Linf Norm: 3.5079, Test Linf Norm: 1.2642\n",
            "Epoch 264: Train Loss: 0.0217, Test Loss: 0.0229, Train L1 Norm: 0.0657, Test L1 Norm: 0.0272, Train Linf Norm: 3.3538, Test Linf Norm: 0.9502\n",
            "Epoch 265: Train Loss: 0.0223, Test Loss: 0.0187, Train L1 Norm: 0.0895, Test L1 Norm: 0.0217, Train Linf Norm: 4.8990, Test Linf Norm: 0.7143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 16:49:25,616]\u001b[0m Trial 8 finished with value: 0.03349379635155201 and parameters: {'n_layers': 5, 'n_units_0': 2035, 'n_units_1': 430, 'n_units_2': 85, 'n_units_3': 249, 'n_units_4': 1180, 'hidden_activation': 'ReLU', 'output_activation': 'Linear', 'loss': 'MAE', 'optimizer': 'Adam', 'lr': 0.007043416804254222, 'batch_size': 64, 'n_epochs': 266, 'scheduler': 'CosineAnnealingLR', 'weight_decay': 0.0003493490424421317, 'beta1': 0.9933071892518038, 'beta2': 0.9993400777217661, 't_max_fraction': 0.05198176896720075, 'eta_min': 1.7980843712441667e-06}. Best is trial 3 with value: 0.028729132837057112.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 266: Train Loss: 0.0212, Test Loss: 0.0175, Train L1 Norm: 0.0666, Test L1 Norm: 0.0335, Train Linf Norm: 3.4968, Test Linf Norm: 1.3741\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 16:49:29,485]\u001b[0m Trial 9 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 21.4836, Test Loss: 21.1527, Train L1 Norm: 1.0222, Test L1 Norm: 1.0000, Train Linf Norm: 1.3500, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.1769, Test Loss: 0.0323, Train L1 Norm: 0.5229, Test L1 Norm: 0.1229, Train Linf Norm: 73.8835, Test Linf Norm: 9.0341\n",
            "Epoch 2: Train Loss: 0.0202, Test Loss: 0.0095, Train L1 Norm: 0.2233, Test L1 Norm: 0.0784, Train Linf Norm: 37.2328, Test Linf Norm: 6.1008\n",
            "Epoch 3: Train Loss: 0.0107, Test Loss: 0.0064, Train L1 Norm: 0.2382, Test L1 Norm: 0.0590, Train Linf Norm: 46.5801, Test Linf Norm: 4.4207\n",
            "Epoch 4: Train Loss: 0.0083, Test Loss: 0.0051, Train L1 Norm: 0.1599, Test L1 Norm: 0.0510, Train Linf Norm: 28.3492, Test Linf Norm: 3.5964\n",
            "Epoch 5: Train Loss: 0.0056, Test Loss: 0.0044, Train L1 Norm: 0.1360, Test L1 Norm: 0.0581, Train Linf Norm: 24.0297, Test Linf Norm: 5.2185\n",
            "Epoch 6: Train Loss: 0.0042, Test Loss: 0.0039, Train L1 Norm: 0.1907, Test L1 Norm: 0.0482, Train Linf Norm: 39.1278, Test Linf Norm: 3.7825\n",
            "Epoch 7: Train Loss: 0.0040, Test Loss: 0.0034, Train L1 Norm: 0.1767, Test L1 Norm: 0.0485, Train Linf Norm: 35.9175, Test Linf Norm: 3.5969\n",
            "Epoch 8: Train Loss: 0.0038, Test Loss: 0.0061, Train L1 Norm: 0.1563, Test L1 Norm: 0.0483, Train Linf Norm: 30.6121, Test Linf Norm: 3.4328\n",
            "Epoch 9: Train Loss: 0.0033, Test Loss: 0.0030, Train L1 Norm: 0.1889, Test L1 Norm: 0.0436, Train Linf Norm: 39.3100, Test Linf Norm: 3.3928\n",
            "Epoch 10: Train Loss: 0.0028, Test Loss: 0.0034, Train L1 Norm: 0.1512, Test L1 Norm: 0.0506, Train Linf Norm: 30.4609, Test Linf Norm: 3.9349\n",
            "Epoch 11: Train Loss: 0.0029, Test Loss: 0.0037, Train L1 Norm: 0.1851, Test L1 Norm: 0.0581, Train Linf Norm: 38.9244, Test Linf Norm: 5.0641\n",
            "Epoch 12: Train Loss: 0.0028, Test Loss: 0.0067, Train L1 Norm: 0.1375, Test L1 Norm: 0.0721, Train Linf Norm: 27.0970, Test Linf Norm: 5.3497\n",
            "Epoch 13: Train Loss: 0.0026, Test Loss: 0.0021, Train L1 Norm: 0.1156, Test L1 Norm: 0.0399, Train Linf Norm: 21.3653, Test Linf Norm: 3.3318\n",
            "Epoch 14: Train Loss: 0.0025, Test Loss: 0.0020, Train L1 Norm: 0.1376, Test L1 Norm: 0.0439, Train Linf Norm: 27.2741, Test Linf Norm: 4.0474\n",
            "Epoch 15: Train Loss: 0.0021, Test Loss: 0.0028, Train L1 Norm: 0.1305, Test L1 Norm: 0.0434, Train Linf Norm: 26.0155, Test Linf Norm: 3.4061\n",
            "Epoch 16: Train Loss: 0.0020, Test Loss: 0.0099, Train L1 Norm: 0.1351, Test L1 Norm: 0.0639, Train Linf Norm: 27.3987, Test Linf Norm: 4.1503\n",
            "Epoch 17: Train Loss: 0.0020, Test Loss: 0.0031, Train L1 Norm: 0.1435, Test L1 Norm: 0.0422, Train Linf Norm: 29.6297, Test Linf Norm: 3.1755\n",
            "Epoch 18: Train Loss: 0.0019, Test Loss: 0.0017, Train L1 Norm: 0.1364, Test L1 Norm: 0.0370, Train Linf Norm: 27.9591, Test Linf Norm: 3.1946\n",
            "Epoch 19: Train Loss: 0.0017, Test Loss: 0.0019, Train L1 Norm: 0.1266, Test L1 Norm: 0.0396, Train Linf Norm: 25.5834, Test Linf Norm: 3.5046\n",
            "Epoch 20: Train Loss: 0.0016, Test Loss: 0.0015, Train L1 Norm: 0.1271, Test L1 Norm: 0.0361, Train Linf Norm: 25.9829, Test Linf Norm: 3.1119\n",
            "Epoch 21: Train Loss: 0.0016, Test Loss: 0.0017, Train L1 Norm: 0.1097, Test L1 Norm: 0.0362, Train Linf Norm: 21.4872, Test Linf Norm: 2.8351\n",
            "Epoch 22: Train Loss: 0.0015, Test Loss: 0.0015, Train L1 Norm: 0.1121, Test L1 Norm: 0.0351, Train Linf Norm: 22.3461, Test Linf Norm: 2.8793\n",
            "Epoch 23: Train Loss: 0.0014, Test Loss: 0.0016, Train L1 Norm: 0.1076, Test L1 Norm: 0.0344, Train Linf Norm: 21.2162, Test Linf Norm: 2.9294\n",
            "Epoch 24: Train Loss: 0.0015, Test Loss: 0.0017, Train L1 Norm: 0.1178, Test L1 Norm: 0.0399, Train Linf Norm: 23.5606, Test Linf Norm: 3.5032\n",
            "Epoch 25: Train Loss: 0.0014, Test Loss: 0.0035, Train L1 Norm: 0.1155, Test L1 Norm: 0.0454, Train Linf Norm: 23.3455, Test Linf Norm: 3.5677\n",
            "Epoch 26: Train Loss: 0.0017, Test Loss: 0.0014, Train L1 Norm: 0.1000, Test L1 Norm: 0.0377, Train Linf Norm: 19.1825, Test Linf Norm: 3.4274\n",
            "Epoch 27: Train Loss: 0.0012, Test Loss: 0.0012, Train L1 Norm: 0.1076, Test L1 Norm: 0.0332, Train Linf Norm: 21.3551, Test Linf Norm: 2.9723\n",
            "Epoch 28: Train Loss: 0.0012, Test Loss: 0.0012, Train L1 Norm: 0.1115, Test L1 Norm: 0.0337, Train Linf Norm: 22.5335, Test Linf Norm: 3.0903\n",
            "Epoch 29: Train Loss: 0.0013, Test Loss: 0.0011, Train L1 Norm: 0.1183, Test L1 Norm: 0.0332, Train Linf Norm: 24.2752, Test Linf Norm: 3.0012\n",
            "Epoch 30: Train Loss: 0.0012, Test Loss: 0.0012, Train L1 Norm: 0.0930, Test L1 Norm: 0.0346, Train Linf Norm: 17.7714, Test Linf Norm: 3.1008\n",
            "Epoch 31: Train Loss: 0.0013, Test Loss: 0.0010, Train L1 Norm: 0.1066, Test L1 Norm: 0.0318, Train Linf Norm: 21.2721, Test Linf Norm: 2.7619\n",
            "Epoch 32: Train Loss: 0.0011, Test Loss: 0.0011, Train L1 Norm: 0.1031, Test L1 Norm: 0.0331, Train Linf Norm: 20.4501, Test Linf Norm: 2.9976\n",
            "Epoch 33: Train Loss: 0.0011, Test Loss: 0.0010, Train L1 Norm: 0.1101, Test L1 Norm: 0.0318, Train Linf Norm: 22.2469, Test Linf Norm: 2.8675\n",
            "Epoch 34: Train Loss: 0.0011, Test Loss: 0.0012, Train L1 Norm: 0.1091, Test L1 Norm: 0.0326, Train Linf Norm: 22.0131, Test Linf Norm: 2.7585\n",
            "Epoch 35: Train Loss: 0.0010, Test Loss: 0.0011, Train L1 Norm: 0.1038, Test L1 Norm: 0.0320, Train Linf Norm: 20.9848, Test Linf Norm: 2.7057\n",
            "Epoch 36: Train Loss: 0.0010, Test Loss: 0.0009, Train L1 Norm: 0.0996, Test L1 Norm: 0.0334, Train Linf Norm: 19.9420, Test Linf Norm: 3.0116\n",
            "Epoch 37: Train Loss: 0.0010, Test Loss: 0.0009, Train L1 Norm: 0.1115, Test L1 Norm: 0.0301, Train Linf Norm: 22.6251, Test Linf Norm: 2.5500\n",
            "Epoch 38: Train Loss: 0.0010, Test Loss: 0.0041, Train L1 Norm: 0.1110, Test L1 Norm: 0.0401, Train Linf Norm: 22.9339, Test Linf Norm: 2.9785\n",
            "Epoch 39: Train Loss: 0.0010, Test Loss: 0.0010, Train L1 Norm: 0.1060, Test L1 Norm: 0.0307, Train Linf Norm: 21.5352, Test Linf Norm: 2.5211\n",
            "Epoch 40: Train Loss: 0.0009, Test Loss: 0.0008, Train L1 Norm: 0.1043, Test L1 Norm: 0.0301, Train Linf Norm: 21.1988, Test Linf Norm: 2.5976\n",
            "Epoch 41: Train Loss: 0.0009, Test Loss: 0.0009, Train L1 Norm: 0.1236, Test L1 Norm: 0.0308, Train Linf Norm: 26.1094, Test Linf Norm: 2.7059\n",
            "Epoch 42: Train Loss: 0.0009, Test Loss: 0.0022, Train L1 Norm: 0.1070, Test L1 Norm: 0.0339, Train Linf Norm: 22.1086, Test Linf Norm: 2.7331\n",
            "Epoch 43: Train Loss: 0.0009, Test Loss: 0.0016, Train L1 Norm: 0.0984, Test L1 Norm: 0.0378, Train Linf Norm: 19.8119, Test Linf Norm: 3.0907\n",
            "Epoch 44: Train Loss: 0.0009, Test Loss: 0.0008, Train L1 Norm: 0.0959, Test L1 Norm: 0.0295, Train Linf Norm: 19.2270, Test Linf Norm: 2.5855\n",
            "Epoch 45: Train Loss: 0.0009, Test Loss: 0.0009, Train L1 Norm: 0.1011, Test L1 Norm: 0.0298, Train Linf Norm: 20.5012, Test Linf Norm: 2.7376\n",
            "Epoch 46: Train Loss: 0.0009, Test Loss: 0.0008, Train L1 Norm: 0.1067, Test L1 Norm: 0.0291, Train Linf Norm: 22.0233, Test Linf Norm: 2.5719\n",
            "Epoch 47: Train Loss: 0.0008, Test Loss: 0.0008, Train L1 Norm: 0.1062, Test L1 Norm: 0.0297, Train Linf Norm: 21.8666, Test Linf Norm: 2.7457\n",
            "Epoch 48: Train Loss: 0.0009, Test Loss: 0.0014, Train L1 Norm: 0.1043, Test L1 Norm: 0.0347, Train Linf Norm: 21.4323, Test Linf Norm: 2.7929\n",
            "Epoch 49: Train Loss: 0.0008, Test Loss: 0.0008, Train L1 Norm: 0.1035, Test L1 Norm: 0.0284, Train Linf Norm: 21.2174, Test Linf Norm: 2.4182\n",
            "Epoch 50: Train Loss: 0.0008, Test Loss: 0.0006, Train L1 Norm: 0.0974, Test L1 Norm: 0.0279, Train Linf Norm: 19.9571, Test Linf Norm: 2.4419\n",
            "Epoch 51: Train Loss: 0.0008, Test Loss: 0.0008, Train L1 Norm: 0.0956, Test L1 Norm: 0.0277, Train Linf Norm: 19.3016, Test Linf Norm: 2.3259\n",
            "Epoch 52: Train Loss: 0.0008, Test Loss: 0.0007, Train L1 Norm: 0.1044, Test L1 Norm: 0.0304, Train Linf Norm: 21.4814, Test Linf Norm: 2.6976\n",
            "Epoch 53: Train Loss: 0.0008, Test Loss: 0.0007, Train L1 Norm: 0.1014, Test L1 Norm: 0.0271, Train Linf Norm: 20.8781, Test Linf Norm: 2.2034\n",
            "Epoch 54: Train Loss: 0.0007, Test Loss: 0.0006, Train L1 Norm: 0.0929, Test L1 Norm: 0.0278, Train Linf Norm: 18.7336, Test Linf Norm: 2.5474\n",
            "Epoch 55: Train Loss: 0.0007, Test Loss: 0.0009, Train L1 Norm: 0.1059, Test L1 Norm: 0.0292, Train Linf Norm: 22.1061, Test Linf Norm: 2.3017\n",
            "Epoch 56: Train Loss: 0.0007, Test Loss: 0.0006, Train L1 Norm: 0.0985, Test L1 Norm: 0.0271, Train Linf Norm: 20.2899, Test Linf Norm: 2.3412\n",
            "Epoch 57: Train Loss: 0.0008, Test Loss: 0.0011, Train L1 Norm: 0.1043, Test L1 Norm: 0.0280, Train Linf Norm: 21.8107, Test Linf Norm: 2.2537\n",
            "Epoch 58: Train Loss: 0.0007, Test Loss: 0.0008, Train L1 Norm: 0.0993, Test L1 Norm: 0.0264, Train Linf Norm: 20.4538, Test Linf Norm: 2.1234\n",
            "Epoch 59: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0958, Test L1 Norm: 0.0256, Train Linf Norm: 19.7196, Test Linf Norm: 2.0776\n",
            "Epoch 60: Train Loss: 0.0007, Test Loss: 0.0007, Train L1 Norm: 0.0973, Test L1 Norm: 0.0265, Train Linf Norm: 20.0756, Test Linf Norm: 2.3321\n",
            "Epoch 61: Train Loss: 0.0007, Test Loss: 0.0005, Train L1 Norm: 0.0966, Test L1 Norm: 0.0255, Train Linf Norm: 19.9924, Test Linf Norm: 2.0988\n",
            "Epoch 62: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0997, Test L1 Norm: 0.0256, Train Linf Norm: 20.6735, Test Linf Norm: 2.1298\n",
            "Epoch 63: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0931, Test L1 Norm: 0.0261, Train Linf Norm: 19.0546, Test Linf Norm: 2.2604\n",
            "Epoch 64: Train Loss: 0.0006, Test Loss: 0.0007, Train L1 Norm: 0.0964, Test L1 Norm: 0.0292, Train Linf Norm: 19.9464, Test Linf Norm: 2.4826\n",
            "Epoch 65: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.1035, Test L1 Norm: 0.0249, Train Linf Norm: 21.8937, Test Linf Norm: 2.0817\n",
            "Epoch 66: Train Loss: 0.0007, Test Loss: 0.0005, Train L1 Norm: 0.0969, Test L1 Norm: 0.0251, Train Linf Norm: 20.0344, Test Linf Norm: 2.0863\n",
            "Epoch 67: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0968, Test L1 Norm: 0.0256, Train Linf Norm: 20.1703, Test Linf Norm: 2.2612\n",
            "Epoch 68: Train Loss: 0.0006, Test Loss: 0.0031, Train L1 Norm: 0.0939, Test L1 Norm: 0.0325, Train Linf Norm: 19.1397, Test Linf Norm: 2.3311\n",
            "Epoch 69: Train Loss: 0.0007, Test Loss: 0.0005, Train L1 Norm: 0.0920, Test L1 Norm: 0.0248, Train Linf Norm: 18.7674, Test Linf Norm: 2.0427\n",
            "Epoch 70: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0940, Test L1 Norm: 0.0245, Train Linf Norm: 19.4279, Test Linf Norm: 2.0411\n",
            "Epoch 71: Train Loss: 0.0006, Test Loss: 0.0007, Train L1 Norm: 0.0929, Test L1 Norm: 0.0257, Train Linf Norm: 19.1841, Test Linf Norm: 2.1085\n",
            "Epoch 72: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0908, Test L1 Norm: 0.0249, Train Linf Norm: 18.7997, Test Linf Norm: 2.1372\n",
            "Epoch 73: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0895, Test L1 Norm: 0.0299, Train Linf Norm: 18.2923, Test Linf Norm: 2.6017\n",
            "Epoch 74: Train Loss: 0.0006, Test Loss: 0.0031, Train L1 Norm: 0.0936, Test L1 Norm: 0.0374, Train Linf Norm: 19.3774, Test Linf Norm: 2.5845\n",
            "Epoch 75: Train Loss: 0.0006, Test Loss: 0.0004, Train L1 Norm: 0.0973, Test L1 Norm: 0.0242, Train Linf Norm: 20.2290, Test Linf Norm: 2.0405\n",
            "Epoch 76: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0921, Test L1 Norm: 0.0263, Train Linf Norm: 19.1636, Test Linf Norm: 2.3029\n",
            "Epoch 77: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0918, Test L1 Norm: 0.0241, Train Linf Norm: 19.0914, Test Linf Norm: 1.9716\n",
            "Epoch 78: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0946, Test L1 Norm: 0.0244, Train Linf Norm: 19.6058, Test Linf Norm: 2.1331\n",
            "Epoch 79: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0900, Test L1 Norm: 0.0240, Train Linf Norm: 18.7335, Test Linf Norm: 2.1085\n",
            "Epoch 80: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.0946, Test L1 Norm: 0.0267, Train Linf Norm: 19.7901, Test Linf Norm: 2.2059\n",
            "Epoch 81: Train Loss: 0.0006, Test Loss: 0.0004, Train L1 Norm: 0.0899, Test L1 Norm: 0.0237, Train Linf Norm: 18.4535, Test Linf Norm: 1.9984\n",
            "Epoch 82: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0918, Test L1 Norm: 0.0242, Train Linf Norm: 19.0742, Test Linf Norm: 2.1530\n",
            "Epoch 83: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0840, Test L1 Norm: 0.0233, Train Linf Norm: 17.0764, Test Linf Norm: 1.9888\n",
            "Epoch 84: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0967, Test L1 Norm: 0.0235, Train Linf Norm: 20.4038, Test Linf Norm: 2.0088\n",
            "Epoch 85: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0909, Test L1 Norm: 0.0238, Train Linf Norm: 18.7210, Test Linf Norm: 1.9893\n",
            "Epoch 86: Train Loss: 0.0004, Test Loss: 0.0006, Train L1 Norm: 0.0959, Test L1 Norm: 0.0240, Train Linf Norm: 20.4412, Test Linf Norm: 1.9583\n",
            "Epoch 87: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0942, Test L1 Norm: 0.0232, Train Linf Norm: 19.9800, Test Linf Norm: 1.9634\n",
            "Epoch 88: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0942, Test L1 Norm: 0.0235, Train Linf Norm: 19.8896, Test Linf Norm: 2.0093\n",
            "Epoch 89: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0949, Test L1 Norm: 0.0235, Train Linf Norm: 20.1751, Test Linf Norm: 1.9950\n",
            "Epoch 90: Train Loss: 0.0004, Test Loss: 0.0005, Train L1 Norm: 0.0964, Test L1 Norm: 0.0236, Train Linf Norm: 20.4753, Test Linf Norm: 2.0096\n",
            "Epoch 91: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0956, Test L1 Norm: 0.0234, Train Linf Norm: 20.3531, Test Linf Norm: 1.9852\n",
            "Epoch 92: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0951, Test L1 Norm: 0.0234, Train Linf Norm: 20.2740, Test Linf Norm: 1.9946\n",
            "Epoch 93: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0956, Test L1 Norm: 0.0233, Train Linf Norm: 20.3117, Test Linf Norm: 1.9868\n",
            "Epoch 94: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0939, Test L1 Norm: 0.0233, Train Linf Norm: 19.8493, Test Linf Norm: 2.0065\n",
            "Epoch 95: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0934, Test L1 Norm: 0.0235, Train Linf Norm: 19.5984, Test Linf Norm: 1.9964\n",
            "Epoch 96: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0899, Test L1 Norm: 0.0233, Train Linf Norm: 18.9738, Test Linf Norm: 1.9776\n",
            "Epoch 97: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0940, Test L1 Norm: 0.0232, Train Linf Norm: 19.9149, Test Linf Norm: 1.9952\n",
            "Epoch 98: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0944, Test L1 Norm: 0.0232, Train Linf Norm: 20.0513, Test Linf Norm: 1.9849\n",
            "Epoch 99: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0928, Test L1 Norm: 0.0232, Train Linf Norm: 19.7248, Test Linf Norm: 2.0031\n",
            "Epoch 100: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0921, Test L1 Norm: 0.0231, Train Linf Norm: 19.2873, Test Linf Norm: 1.9819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 16:51:49,142]\u001b[0m Trial 10 finished with value: 0.023221840736269952 and parameters: {'n_layers': 2, 'n_units_0': 800, 'n_units_1': 98, 'hidden_activation': 'ReLU', 'output_activation': 'Linear', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.0013332181796177327, 'batch_size': 256, 'n_epochs': 101, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.19643578862292976, 'patience': 5, 'threshold': 0.008455502280580186}. Best is trial 10 with value: 0.023221840736269952.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 101: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0929, Test L1 Norm: 0.0232, Train Linf Norm: 19.6568, Test Linf Norm: 1.9857\n",
            "Epoch 1: Train Loss: 0.1966, Test Loss: 0.0325, Train L1 Norm: 1.0383, Test L1 Norm: 0.1932, Train Linf Norm: 198.8742, Test Linf Norm: 20.0102\n",
            "Epoch 2: Train Loss: 0.0227, Test Loss: 0.0147, Train L1 Norm: 0.3375, Test L1 Norm: 0.1141, Train Linf Norm: 63.1087, Test Linf Norm: 10.3718\n",
            "Epoch 3: Train Loss: 0.0131, Test Loss: 0.0110, Train L1 Norm: 0.1775, Test L1 Norm: 0.0808, Train Linf Norm: 28.5769, Test Linf Norm: 6.5234\n",
            "Epoch 4: Train Loss: 0.0086, Test Loss: 0.0098, Train L1 Norm: 0.1546, Test L1 Norm: 0.0734, Train Linf Norm: 25.9636, Test Linf Norm: 5.4000\n",
            "Epoch 5: Train Loss: 0.0072, Test Loss: 0.0080, Train L1 Norm: 0.1181, Test L1 Norm: 0.0668, Train Linf Norm: 18.6639, Test Linf Norm: 5.1548\n",
            "Epoch 6: Train Loss: 0.0062, Test Loss: 0.0048, Train L1 Norm: 0.0969, Test L1 Norm: 0.0515, Train Linf Norm: 14.1159, Test Linf Norm: 3.7479\n",
            "Epoch 7: Train Loss: 0.0045, Test Loss: 0.0057, Train L1 Norm: 0.0840, Test L1 Norm: 0.0534, Train Linf Norm: 11.0844, Test Linf Norm: 3.5982\n",
            "Epoch 8: Train Loss: 0.0039, Test Loss: 0.0038, Train L1 Norm: 0.1002, Test L1 Norm: 0.0448, Train Linf Norm: 16.3380, Test Linf Norm: 2.9309\n",
            "Epoch 9: Train Loss: 0.0035, Test Loss: 0.0079, Train L1 Norm: 0.0938, Test L1 Norm: 0.0543, Train Linf Norm: 14.9911, Test Linf Norm: 3.7732\n",
            "Epoch 10: Train Loss: 0.0032, Test Loss: 0.0034, Train L1 Norm: 0.0789, Test L1 Norm: 0.0458, Train Linf Norm: 11.4192, Test Linf Norm: 3.5093\n",
            "Epoch 11: Train Loss: 0.0030, Test Loss: 0.0031, Train L1 Norm: 0.0855, Test L1 Norm: 0.0420, Train Linf Norm: 13.5105, Test Linf Norm: 2.9667\n",
            "Epoch 12: Train Loss: 0.0026, Test Loss: 0.0028, Train L1 Norm: 0.0861, Test L1 Norm: 0.0386, Train Linf Norm: 13.9246, Test Linf Norm: 2.3429\n",
            "Epoch 13: Train Loss: 0.0026, Test Loss: 0.0025, Train L1 Norm: 0.0868, Test L1 Norm: 0.0367, Train Linf Norm: 13.8146, Test Linf Norm: 2.2591\n",
            "Epoch 14: Train Loss: 0.0025, Test Loss: 0.0100, Train L1 Norm: 0.0760, Test L1 Norm: 0.0655, Train Linf Norm: 11.5528, Test Linf Norm: 4.6662\n",
            "Epoch 15: Train Loss: 0.0023, Test Loss: 0.0023, Train L1 Norm: 0.0792, Test L1 Norm: 0.0365, Train Linf Norm: 12.6394, Test Linf Norm: 2.2096\n",
            "Epoch 16: Train Loss: 0.0021, Test Loss: 0.0022, Train L1 Norm: 0.0862, Test L1 Norm: 0.0364, Train Linf Norm: 14.6742, Test Linf Norm: 2.3811\n",
            "Epoch 17: Train Loss: 0.0020, Test Loss: 0.0026, Train L1 Norm: 0.0872, Test L1 Norm: 0.0469, Train Linf Norm: 14.5806, Test Linf Norm: 3.0701\n",
            "Epoch 18: Train Loss: 0.0019, Test Loss: 0.0106, Train L1 Norm: 0.0693, Test L1 Norm: 0.0571, Train Linf Norm: 10.3851, Test Linf Norm: 3.5833\n",
            "Epoch 19: Train Loss: 0.0019, Test Loss: 0.0019, Train L1 Norm: 0.0767, Test L1 Norm: 0.0357, Train Linf Norm: 12.4045, Test Linf Norm: 2.2978\n",
            "Epoch 20: Train Loss: 0.0018, Test Loss: 0.0017, Train L1 Norm: 0.0646, Test L1 Norm: 0.0334, Train Linf Norm: 9.3836, Test Linf Norm: 2.1786\n",
            "Epoch 21: Train Loss: 0.0017, Test Loss: 0.0016, Train L1 Norm: 0.0661, Test L1 Norm: 0.0328, Train Linf Norm: 9.8525, Test Linf Norm: 2.0954\n",
            "Epoch 22: Train Loss: 0.0016, Test Loss: 0.0023, Train L1 Norm: 0.0601, Test L1 Norm: 0.0379, Train Linf Norm: 8.3283, Test Linf Norm: 2.3287\n",
            "Epoch 23: Train Loss: 0.0015, Test Loss: 0.0014, Train L1 Norm: 0.0872, Test L1 Norm: 0.0322, Train Linf Norm: 15.4706, Test Linf Norm: 2.0854\n",
            "Epoch 24: Train Loss: 0.0013, Test Loss: 0.0013, Train L1 Norm: 0.0561, Test L1 Norm: 0.0335, Train Linf Norm: 7.8234, Test Linf Norm: 2.4478\n",
            "Epoch 25: Train Loss: 0.0016, Test Loss: 0.0013, Train L1 Norm: 0.0650, Test L1 Norm: 0.0314, Train Linf Norm: 9.9827, Test Linf Norm: 2.0274\n",
            "Epoch 26: Train Loss: 0.0013, Test Loss: 0.0013, Train L1 Norm: 0.0606, Test L1 Norm: 0.0321, Train Linf Norm: 9.0122, Test Linf Norm: 2.1648\n",
            "Epoch 27: Train Loss: 0.0013, Test Loss: 0.0027, Train L1 Norm: 0.1006, Test L1 Norm: 0.0382, Train Linf Norm: 19.4299, Test Linf Norm: 2.7019\n",
            "Epoch 28: Train Loss: 0.0014, Test Loss: 0.0012, Train L1 Norm: 0.0563, Test L1 Norm: 0.0308, Train Linf Norm: 8.0394, Test Linf Norm: 2.0078\n",
            "Epoch 29: Train Loss: 0.0013, Test Loss: 0.0014, Train L1 Norm: 0.0529, Test L1 Norm: 0.0310, Train Linf Norm: 7.1567, Test Linf Norm: 1.8278\n",
            "Epoch 30: Train Loss: 0.0011, Test Loss: 0.0014, Train L1 Norm: 0.0646, Test L1 Norm: 0.0327, Train Linf Norm: 10.4299, Test Linf Norm: 2.0754\n",
            "Epoch 31: Train Loss: 0.0012, Test Loss: 0.0012, Train L1 Norm: 0.0555, Test L1 Norm: 0.0303, Train Linf Norm: 7.6727, Test Linf Norm: 1.8726\n",
            "Epoch 32: Train Loss: 0.0011, Test Loss: 0.0011, Train L1 Norm: 0.0518, Test L1 Norm: 0.0298, Train Linf Norm: 7.4035, Test Linf Norm: 1.8487\n",
            "Epoch 33: Train Loss: 0.0011, Test Loss: 0.0010, Train L1 Norm: 0.0741, Test L1 Norm: 0.0299, Train Linf Norm: 12.7849, Test Linf Norm: 2.1265\n",
            "Epoch 34: Train Loss: 0.0010, Test Loss: 0.0011, Train L1 Norm: 0.0503, Test L1 Norm: 0.0289, Train Linf Norm: 7.1115, Test Linf Norm: 1.7961\n",
            "Epoch 35: Train Loss: 0.0010, Test Loss: 0.0011, Train L1 Norm: 0.0474, Test L1 Norm: 0.0315, Train Linf Norm: 6.4328, Test Linf Norm: 2.4177\n",
            "Epoch 36: Train Loss: 0.0009, Test Loss: 0.0010, Train L1 Norm: 0.0570, Test L1 Norm: 0.0290, Train Linf Norm: 8.6681, Test Linf Norm: 1.7926\n",
            "Epoch 37: Train Loss: 0.0009, Test Loss: 0.0012, Train L1 Norm: 0.0633, Test L1 Norm: 0.0296, Train Linf Norm: 10.4181, Test Linf Norm: 1.8382\n",
            "Epoch 38: Train Loss: 0.0011, Test Loss: 0.0010, Train L1 Norm: 0.0510, Test L1 Norm: 0.0283, Train Linf Norm: 7.3028, Test Linf Norm: 1.8742\n",
            "Epoch 39: Train Loss: 0.0010, Test Loss: 0.0009, Train L1 Norm: 0.0529, Test L1 Norm: 0.0280, Train Linf Norm: 7.9372, Test Linf Norm: 1.8456\n",
            "Epoch 40: Train Loss: 0.0009, Test Loss: 0.0011, Train L1 Norm: 0.0480, Test L1 Norm: 0.0285, Train Linf Norm: 6.3819, Test Linf Norm: 1.7716\n",
            "Epoch 41: Train Loss: 0.0009, Test Loss: 0.0017, Train L1 Norm: 0.0507, Test L1 Norm: 0.0308, Train Linf Norm: 7.4491, Test Linf Norm: 2.1067\n",
            "Epoch 42: Train Loss: 0.0009, Test Loss: 0.0009, Train L1 Norm: 0.0542, Test L1 Norm: 0.0278, Train Linf Norm: 8.2946, Test Linf Norm: 1.8781\n",
            "Epoch 43: Train Loss: 0.0009, Test Loss: 0.0009, Train L1 Norm: 0.0528, Test L1 Norm: 0.0274, Train Linf Norm: 7.9497, Test Linf Norm: 1.8442\n",
            "Epoch 44: Train Loss: 0.0009, Test Loss: 0.0008, Train L1 Norm: 0.0621, Test L1 Norm: 0.0284, Train Linf Norm: 10.3245, Test Linf Norm: 2.0665\n",
            "Epoch 45: Train Loss: 0.0008, Test Loss: 0.0008, Train L1 Norm: 0.0466, Test L1 Norm: 0.0273, Train Linf Norm: 6.6015, Test Linf Norm: 1.8931\n",
            "Epoch 46: Train Loss: 0.0008, Test Loss: 0.0011, Train L1 Norm: 0.0448, Test L1 Norm: 0.0284, Train Linf Norm: 6.0543, Test Linf Norm: 2.0491\n",
            "Epoch 47: Train Loss: 0.0008, Test Loss: 0.0008, Train L1 Norm: 0.0445, Test L1 Norm: 0.0275, Train Linf Norm: 6.1418, Test Linf Norm: 2.0386\n",
            "Epoch 48: Train Loss: 0.0008, Test Loss: 0.0008, Train L1 Norm: 0.0595, Test L1 Norm: 0.0271, Train Linf Norm: 10.0096, Test Linf Norm: 1.8946\n",
            "Epoch 49: Train Loss: 0.0008, Test Loss: 0.0013, Train L1 Norm: 0.0512, Test L1 Norm: 0.0372, Train Linf Norm: 7.4411, Test Linf Norm: 2.8787\n",
            "Epoch 50: Train Loss: 0.0008, Test Loss: 0.0008, Train L1 Norm: 0.0518, Test L1 Norm: 0.0269, Train Linf Norm: 7.9575, Test Linf Norm: 1.9456\n",
            "Epoch 51: Train Loss: 0.0007, Test Loss: 0.0011, Train L1 Norm: 0.0487, Test L1 Norm: 0.0271, Train Linf Norm: 7.3550, Test Linf Norm: 1.8531\n",
            "Epoch 52: Train Loss: 0.0008, Test Loss: 0.0009, Train L1 Norm: 0.0521, Test L1 Norm: 0.0291, Train Linf Norm: 8.1943, Test Linf Norm: 2.1566\n",
            "Epoch 53: Train Loss: 0.0008, Test Loss: 0.0009, Train L1 Norm: 0.0471, Test L1 Norm: 0.0290, Train Linf Norm: 6.7842, Test Linf Norm: 2.1455\n",
            "Epoch 54: Train Loss: 0.0008, Test Loss: 0.0007, Train L1 Norm: 0.0485, Test L1 Norm: 0.0268, Train Linf Norm: 7.1787, Test Linf Norm: 1.9289\n",
            "Epoch 55: Train Loss: 0.0007, Test Loss: 0.0007, Train L1 Norm: 0.0459, Test L1 Norm: 0.0260, Train Linf Norm: 6.6853, Test Linf Norm: 1.8642\n",
            "Epoch 56: Train Loss: 0.0007, Test Loss: 0.0007, Train L1 Norm: 0.0505, Test L1 Norm: 0.0264, Train Linf Norm: 7.8024, Test Linf Norm: 1.9407\n",
            "Epoch 57: Train Loss: 0.0007, Test Loss: 0.0007, Train L1 Norm: 0.0547, Test L1 Norm: 0.0268, Train Linf Norm: 8.9597, Test Linf Norm: 1.9919\n",
            "Epoch 58: Train Loss: 0.0007, Test Loss: 0.0007, Train L1 Norm: 0.0565, Test L1 Norm: 0.0263, Train Linf Norm: 7.8098, Test Linf Norm: 2.0066\n",
            "Epoch 59: Train Loss: 0.0007, Test Loss: 0.0007, Train L1 Norm: 0.0625, Test L1 Norm: 0.0261, Train Linf Norm: 10.9259, Test Linf Norm: 1.9603\n",
            "Epoch 60: Train Loss: 0.0006, Test Loss: 0.0007, Train L1 Norm: 0.0546, Test L1 Norm: 0.0265, Train Linf Norm: 9.0789, Test Linf Norm: 1.9626\n",
            "Epoch 61: Train Loss: 0.0007, Test Loss: 0.0006, Train L1 Norm: 0.0538, Test L1 Norm: 0.0263, Train Linf Norm: 8.7471, Test Linf Norm: 2.0570\n",
            "Epoch 62: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0508, Test L1 Norm: 0.0263, Train Linf Norm: 8.1580, Test Linf Norm: 2.0147\n",
            "Epoch 63: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0432, Test L1 Norm: 0.0258, Train Linf Norm: 6.2095, Test Linf Norm: 1.9935\n",
            "Epoch 64: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0494, Test L1 Norm: 0.0263, Train Linf Norm: 7.7847, Test Linf Norm: 2.0300\n",
            "Epoch 65: Train Loss: 0.0007, Test Loss: 0.0006, Train L1 Norm: 0.0489, Test L1 Norm: 0.0258, Train Linf Norm: 7.7392, Test Linf Norm: 2.0027\n",
            "Epoch 66: Train Loss: 0.0006, Test Loss: 0.0009, Train L1 Norm: 0.0517, Test L1 Norm: 0.0292, Train Linf Norm: 8.4601, Test Linf Norm: 2.3582\n",
            "Epoch 67: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0523, Test L1 Norm: 0.0263, Train Linf Norm: 8.6493, Test Linf Norm: 2.1442\n",
            "Epoch 68: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0523, Test L1 Norm: 0.0258, Train Linf Norm: 8.6344, Test Linf Norm: 2.0346\n",
            "Epoch 69: Train Loss: 0.0007, Test Loss: 0.0006, Train L1 Norm: 0.0489, Test L1 Norm: 0.0259, Train Linf Norm: 7.6907, Test Linf Norm: 2.0769\n",
            "Epoch 70: Train Loss: 0.0006, Test Loss: 0.0014, Train L1 Norm: 0.0518, Test L1 Norm: 0.0300, Train Linf Norm: 8.6343, Test Linf Norm: 2.2285\n",
            "Epoch 71: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0605, Test L1 Norm: 0.0274, Train Linf Norm: 10.3964, Test Linf Norm: 2.2120\n",
            "Epoch 72: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0541, Test L1 Norm: 0.0259, Train Linf Norm: 9.1301, Test Linf Norm: 2.1134\n",
            "Epoch 73: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0551, Test L1 Norm: 0.0260, Train Linf Norm: 9.4195, Test Linf Norm: 2.0854\n",
            "Epoch 74: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0503, Test L1 Norm: 0.0267, Train Linf Norm: 8.1018, Test Linf Norm: 2.1697\n",
            "Epoch 75: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0487, Test L1 Norm: 0.0257, Train Linf Norm: 7.7404, Test Linf Norm: 2.0821\n",
            "Epoch 76: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0486, Test L1 Norm: 0.0256, Train Linf Norm: 7.5990, Test Linf Norm: 2.0885\n",
            "Epoch 77: Train Loss: 0.0006, Test Loss: 0.0008, Train L1 Norm: 0.0531, Test L1 Norm: 0.0270, Train Linf Norm: 8.8678, Test Linf Norm: 2.1088\n",
            "Epoch 78: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0538, Test L1 Norm: 0.0256, Train Linf Norm: 9.1489, Test Linf Norm: 2.1033\n",
            "Epoch 79: Train Loss: 0.0005, Test Loss: 0.0025, Train L1 Norm: 0.0619, Test L1 Norm: 0.0322, Train Linf Norm: 11.3542, Test Linf Norm: 2.3615\n",
            "Epoch 80: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0540, Test L1 Norm: 0.0260, Train Linf Norm: 9.1590, Test Linf Norm: 2.1355\n",
            "Epoch 81: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0513, Test L1 Norm: 0.0258, Train Linf Norm: 8.5507, Test Linf Norm: 2.1509\n",
            "Epoch 82: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0517, Test L1 Norm: 0.0255, Train Linf Norm: 8.4624, Test Linf Norm: 2.1134\n",
            "Epoch 83: Train Loss: 0.0005, Test Loss: 0.0015, Train L1 Norm: 0.0533, Test L1 Norm: 0.0307, Train Linf Norm: 9.0877, Test Linf Norm: 2.3833\n",
            "Epoch 84: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0592, Test L1 Norm: 0.0263, Train Linf Norm: 10.5838, Test Linf Norm: 2.2313\n",
            "Epoch 85: Train Loss: 0.0005, Test Loss: 0.0007, Train L1 Norm: 0.0448, Test L1 Norm: 0.0259, Train Linf Norm: 6.7523, Test Linf Norm: 2.1883\n",
            "Epoch 86: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.0481, Test L1 Norm: 0.0256, Train Linf Norm: 7.8614, Test Linf Norm: 2.1507\n",
            "Epoch 87: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0572, Test L1 Norm: 0.0254, Train Linf Norm: 9.8192, Test Linf Norm: 2.1364\n",
            "Epoch 88: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0483, Test L1 Norm: 0.0252, Train Linf Norm: 7.6332, Test Linf Norm: 2.1427\n",
            "Epoch 89: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0540, Test L1 Norm: 0.0259, Train Linf Norm: 9.3483, Test Linf Norm: 2.2095\n",
            "Epoch 90: Train Loss: 0.0005, Test Loss: 0.0125, Train L1 Norm: 0.0527, Test L1 Norm: 0.0598, Train Linf Norm: 9.0113, Test Linf Norm: 3.6830\n",
            "Epoch 91: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0493, Test L1 Norm: 0.0254, Train Linf Norm: 8.0364, Test Linf Norm: 2.2029\n",
            "Epoch 92: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0480, Test L1 Norm: 0.0253, Train Linf Norm: 7.7696, Test Linf Norm: 2.1550\n",
            "Epoch 93: Train Loss: 0.0005, Test Loss: 0.0023, Train L1 Norm: 0.0533, Test L1 Norm: 0.0326, Train Linf Norm: 9.1639, Test Linf Norm: 2.4561\n",
            "Epoch 94: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.0529, Test L1 Norm: 0.0262, Train Linf Norm: 9.0447, Test Linf Norm: 2.1750\n",
            "Epoch 95: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0519, Test L1 Norm: 0.0267, Train Linf Norm: 8.7005, Test Linf Norm: 2.2938\n",
            "Epoch 96: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0484, Test L1 Norm: 0.0252, Train Linf Norm: 8.1026, Test Linf Norm: 2.1795\n",
            "Epoch 97: Train Loss: 0.0004, Test Loss: 0.0005, Train L1 Norm: 0.0461, Test L1 Norm: 0.0250, Train Linf Norm: 7.5881, Test Linf Norm: 2.1487\n",
            "Epoch 98: Train Loss: 0.0004, Test Loss: 0.0005, Train L1 Norm: 0.0475, Test L1 Norm: 0.0254, Train Linf Norm: 7.8658, Test Linf Norm: 2.1751\n",
            "Epoch 99: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0484, Test L1 Norm: 0.0254, Train Linf Norm: 8.0249, Test Linf Norm: 2.1850\n",
            "Epoch 100: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0460, Test L1 Norm: 0.0252, Train Linf Norm: 7.5768, Test Linf Norm: 2.1811\n",
            "Epoch 101: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0472, Test L1 Norm: 0.0251, Train Linf Norm: 7.8073, Test Linf Norm: 2.1587\n",
            "Epoch 102: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0485, Test L1 Norm: 0.0252, Train Linf Norm: 8.0108, Test Linf Norm: 2.1653\n",
            "Epoch 103: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0460, Test L1 Norm: 0.0251, Train Linf Norm: 7.5001, Test Linf Norm: 2.1662\n",
            "Epoch 104: Train Loss: 0.0004, Test Loss: 0.0005, Train L1 Norm: 0.0478, Test L1 Norm: 0.0252, Train Linf Norm: 7.8799, Test Linf Norm: 2.1794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 16:54:12,425]\u001b[0m Trial 11 finished with value: 0.025123866349458693 and parameters: {'n_layers': 2, 'n_units_0': 827, 'n_units_1': 63, 'hidden_activation': 'ReLU', 'output_activation': 'Linear', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.00137772584226203, 'batch_size': 256, 'n_epochs': 105, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.19073745483393878, 'patience': 5, 'threshold': 0.009017892774159462}. Best is trial 10 with value: 0.023221840736269952.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 105: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0466, Test L1 Norm: 0.0251, Train Linf Norm: 7.6260, Test Linf Norm: 2.1565\n",
            "Epoch 1: Train Loss: 0.1586, Test Loss: 0.0199, Train L1 Norm: 0.4521, Test L1 Norm: 0.1135, Train Linf Norm: 59.4399, Test Linf Norm: 11.5630\n",
            "Epoch 2: Train Loss: 0.0202, Test Loss: 0.0106, Train L1 Norm: 0.1902, Test L1 Norm: 0.0928, Train Linf Norm: 27.7440, Test Linf Norm: 8.8949\n",
            "Epoch 3: Train Loss: 0.0106, Test Loss: 0.0088, Train L1 Norm: 0.1748, Test L1 Norm: 0.1015, Train Linf Norm: 29.9309, Test Linf Norm: 9.1101\n",
            "Epoch 4: Train Loss: 0.0071, Test Loss: 0.0049, Train L1 Norm: 0.1131, Test L1 Norm: 0.0564, Train Linf Norm: 17.0507, Test Linf Norm: 4.7043\n",
            "Epoch 5: Train Loss: 0.0055, Test Loss: 0.0061, Train L1 Norm: 0.0994, Test L1 Norm: 0.0665, Train Linf Norm: 14.3460, Test Linf Norm: 5.7706\n",
            "Epoch 6: Train Loss: 0.0047, Test Loss: 0.0040, Train L1 Norm: 0.1011, Test L1 Norm: 0.0510, Train Linf Norm: 15.1657, Test Linf Norm: 4.4708\n",
            "Epoch 7: Train Loss: 0.0039, Test Loss: 0.0032, Train L1 Norm: 0.0921, Test L1 Norm: 0.0481, Train Linf Norm: 14.2749, Test Linf Norm: 4.2873\n",
            "Epoch 8: Train Loss: 0.0033, Test Loss: 0.0056, Train L1 Norm: 0.1053, Test L1 Norm: 0.0488, Train Linf Norm: 17.8218, Test Linf Norm: 3.8724\n",
            "Epoch 9: Train Loss: 0.0027, Test Loss: 0.0024, Train L1 Norm: 0.0914, Test L1 Norm: 0.0441, Train Linf Norm: 14.9137, Test Linf Norm: 3.7499\n",
            "Epoch 10: Train Loss: 0.0028, Test Loss: 0.0023, Train L1 Norm: 0.0967, Test L1 Norm: 0.0495, Train Linf Norm: 15.9242, Test Linf Norm: 4.6222\n",
            "Epoch 11: Train Loss: 0.0024, Test Loss: 0.0023, Train L1 Norm: 0.0719, Test L1 Norm: 0.0403, Train Linf Norm: 10.2710, Test Linf Norm: 2.9726\n",
            "Epoch 12: Train Loss: 0.0022, Test Loss: 0.0040, Train L1 Norm: 0.0741, Test L1 Norm: 0.0440, Train Linf Norm: 11.2708, Test Linf Norm: 3.4093\n",
            "Epoch 13: Train Loss: 0.0021, Test Loss: 0.0017, Train L1 Norm: 0.0719, Test L1 Norm: 0.0402, Train Linf Norm: 10.7624, Test Linf Norm: 3.4585\n",
            "Epoch 14: Train Loss: 0.0019, Test Loss: 0.0021, Train L1 Norm: 0.0842, Test L1 Norm: 0.0405, Train Linf Norm: 14.1044, Test Linf Norm: 3.2103\n",
            "Epoch 15: Train Loss: 0.0019, Test Loss: 0.0020, Train L1 Norm: 0.0766, Test L1 Norm: 0.0381, Train Linf Norm: 12.1686, Test Linf Norm: 2.8589\n",
            "Epoch 16: Train Loss: 0.0019, Test Loss: 0.0017, Train L1 Norm: 0.0591, Test L1 Norm: 0.0376, Train Linf Norm: 8.0813, Test Linf Norm: 3.1214\n",
            "Epoch 17: Train Loss: 0.0015, Test Loss: 0.0018, Train L1 Norm: 0.0791, Test L1 Norm: 0.0415, Train Linf Norm: 13.3684, Test Linf Norm: 3.6156\n",
            "Epoch 18: Train Loss: 0.0018, Test Loss: 0.0018, Train L1 Norm: 0.0589, Test L1 Norm: 0.0410, Train Linf Norm: 8.1292, Test Linf Norm: 3.5089\n",
            "Epoch 19: Train Loss: 0.0016, Test Loss: 0.0013, Train L1 Norm: 0.0581, Test L1 Norm: 0.0351, Train Linf Norm: 7.8401, Test Linf Norm: 2.9058\n",
            "Epoch 20: Train Loss: 0.0015, Test Loss: 0.0013, Train L1 Norm: 0.0795, Test L1 Norm: 0.0372, Train Linf Norm: 13.6215, Test Linf Norm: 3.3596\n",
            "Epoch 21: Train Loss: 0.0014, Test Loss: 0.0014, Train L1 Norm: 0.0666, Test L1 Norm: 0.0398, Train Linf Norm: 10.6012, Test Linf Norm: 3.7109\n",
            "Epoch 22: Train Loss: 0.0013, Test Loss: 0.0012, Train L1 Norm: 0.0682, Test L1 Norm: 0.0337, Train Linf Norm: 11.1465, Test Linf Norm: 2.7619\n",
            "Epoch 23: Train Loss: 0.0013, Test Loss: 0.0011, Train L1 Norm: 0.0755, Test L1 Norm: 0.0340, Train Linf Norm: 12.7289, Test Linf Norm: 2.8500\n",
            "Epoch 24: Train Loss: 0.0013, Test Loss: 0.0015, Train L1 Norm: 0.0875, Test L1 Norm: 0.0397, Train Linf Norm: 16.0678, Test Linf Norm: 3.7679\n",
            "Epoch 25: Train Loss: 0.0013, Test Loss: 0.0010, Train L1 Norm: 0.0739, Test L1 Norm: 0.0368, Train Linf Norm: 12.6302, Test Linf Norm: 3.5771\n",
            "Epoch 26: Train Loss: 0.0012, Test Loss: 0.0010, Train L1 Norm: 0.0832, Test L1 Norm: 0.0330, Train Linf Norm: 15.0388, Test Linf Norm: 2.8024\n",
            "Epoch 27: Train Loss: 0.0012, Test Loss: 0.0036, Train L1 Norm: 0.0624, Test L1 Norm: 0.0489, Train Linf Norm: 9.6937, Test Linf Norm: 4.2916\n",
            "Epoch 28: Train Loss: 0.0011, Test Loss: 0.0012, Train L1 Norm: 0.0950, Test L1 Norm: 0.0336, Train Linf Norm: 18.0842, Test Linf Norm: 2.7000\n",
            "Epoch 29: Train Loss: 0.0011, Test Loss: 0.0009, Train L1 Norm: 0.0744, Test L1 Norm: 0.0346, Train Linf Norm: 12.9243, Test Linf Norm: 3.3119\n",
            "Epoch 30: Train Loss: 0.0010, Test Loss: 0.0009, Train L1 Norm: 0.0778, Test L1 Norm: 0.0339, Train Linf Norm: 13.3397, Test Linf Norm: 3.1716\n",
            "Epoch 31: Train Loss: 0.0010, Test Loss: 0.0010, Train L1 Norm: 0.0726, Test L1 Norm: 0.0343, Train Linf Norm: 12.6516, Test Linf Norm: 3.3028\n",
            "Epoch 32: Train Loss: 0.0010, Test Loss: 0.0008, Train L1 Norm: 0.0863, Test L1 Norm: 0.0347, Train Linf Norm: 16.1470, Test Linf Norm: 3.4416\n",
            "Epoch 33: Train Loss: 0.0010, Test Loss: 0.0009, Train L1 Norm: 0.0795, Test L1 Norm: 0.0325, Train Linf Norm: 14.2555, Test Linf Norm: 3.0030\n",
            "Epoch 34: Train Loss: 0.0010, Test Loss: 0.0008, Train L1 Norm: 0.0873, Test L1 Norm: 0.0326, Train Linf Norm: 16.4573, Test Linf Norm: 3.1127\n",
            "Epoch 35: Train Loss: 0.0009, Test Loss: 0.0009, Train L1 Norm: 0.0553, Test L1 Norm: 0.0393, Train Linf Norm: 8.3599, Test Linf Norm: 4.1652\n",
            "Epoch 36: Train Loss: 0.0008, Test Loss: 0.0030, Train L1 Norm: 0.0949, Test L1 Norm: 0.0414, Train Linf Norm: 18.5749, Test Linf Norm: 3.5315\n",
            "Epoch 37: Train Loss: 0.0009, Test Loss: 0.0008, Train L1 Norm: 0.0709, Test L1 Norm: 0.0313, Train Linf Norm: 12.3920, Test Linf Norm: 2.8938\n",
            "Epoch 38: Train Loss: 0.0008, Test Loss: 0.0008, Train L1 Norm: 0.0763, Test L1 Norm: 0.0320, Train Linf Norm: 13.9305, Test Linf Norm: 3.0498\n",
            "Epoch 39: Train Loss: 0.0009, Test Loss: 0.0010, Train L1 Norm: 0.0800, Test L1 Norm: 0.0368, Train Linf Norm: 14.5481, Test Linf Norm: 3.6352\n",
            "Epoch 40: Train Loss: 0.0008, Test Loss: 0.0007, Train L1 Norm: 0.0844, Test L1 Norm: 0.0325, Train Linf Norm: 15.8664, Test Linf Norm: 3.0278\n",
            "Epoch 41: Train Loss: 0.0008, Test Loss: 0.0009, Train L1 Norm: 0.0876, Test L1 Norm: 0.0325, Train Linf Norm: 16.8049, Test Linf Norm: 2.9175\n",
            "Epoch 42: Train Loss: 0.0008, Test Loss: 0.0007, Train L1 Norm: 0.0666, Test L1 Norm: 0.0326, Train Linf Norm: 11.3870, Test Linf Norm: 3.1962\n",
            "Epoch 43: Train Loss: 0.0008, Test Loss: 0.0007, Train L1 Norm: 0.0819, Test L1 Norm: 0.0318, Train Linf Norm: 15.3102, Test Linf Norm: 3.1250\n",
            "Epoch 44: Train Loss: 0.0008, Test Loss: 0.0007, Train L1 Norm: 0.0734, Test L1 Norm: 0.0319, Train Linf Norm: 13.3665, Test Linf Norm: 3.1096\n",
            "Epoch 45: Train Loss: 0.0008, Test Loss: 0.0007, Train L1 Norm: 0.0773, Test L1 Norm: 0.0325, Train Linf Norm: 14.2712, Test Linf Norm: 3.1533\n",
            "Epoch 46: Train Loss: 0.0008, Test Loss: 0.0007, Train L1 Norm: 0.0706, Test L1 Norm: 0.0314, Train Linf Norm: 12.3888, Test Linf Norm: 2.9987\n",
            "Epoch 47: Train Loss: 0.0008, Test Loss: 0.0008, Train L1 Norm: 0.0721, Test L1 Norm: 0.0332, Train Linf Norm: 13.1419, Test Linf Norm: 3.0962\n",
            "Epoch 48: Train Loss: 0.0007, Test Loss: 0.0006, Train L1 Norm: 0.0674, Test L1 Norm: 0.0305, Train Linf Norm: 11.8273, Test Linf Norm: 2.8457\n",
            "Epoch 49: Train Loss: 0.0007, Test Loss: 0.0006, Train L1 Norm: 0.0747, Test L1 Norm: 0.0330, Train Linf Norm: 13.8698, Test Linf Norm: 3.3239\n",
            "Epoch 50: Train Loss: 0.0007, Test Loss: 0.0006, Train L1 Norm: 0.0495, Test L1 Norm: 0.0315, Train Linf Norm: 7.3351, Test Linf Norm: 3.0372\n",
            "Epoch 51: Train Loss: 0.0007, Test Loss: 0.0007, Train L1 Norm: 0.0764, Test L1 Norm: 0.0307, Train Linf Norm: 14.0225, Test Linf Norm: 2.7909\n",
            "Epoch 52: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0670, Test L1 Norm: 0.0307, Train Linf Norm: 11.9850, Test Linf Norm: 2.9748\n",
            "Epoch 53: Train Loss: 0.0006, Test Loss: 0.0007, Train L1 Norm: 0.0653, Test L1 Norm: 0.0346, Train Linf Norm: 11.4488, Test Linf Norm: 3.5705\n",
            "Epoch 54: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0687, Test L1 Norm: 0.0324, Train Linf Norm: 12.3696, Test Linf Norm: 3.2763\n",
            "Epoch 55: Train Loss: 0.0007, Test Loss: 0.0065, Train L1 Norm: 0.0649, Test L1 Norm: 0.0511, Train Linf Norm: 11.3836, Test Linf Norm: 4.0308\n",
            "Epoch 56: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0644, Test L1 Norm: 0.0309, Train Linf Norm: 11.3800, Test Linf Norm: 3.0489\n",
            "Epoch 57: Train Loss: 0.0007, Test Loss: 0.0005, Train L1 Norm: 0.0639, Test L1 Norm: 0.0307, Train Linf Norm: 10.9595, Test Linf Norm: 2.9188\n",
            "Epoch 58: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0666, Test L1 Norm: 0.0338, Train Linf Norm: 11.9141, Test Linf Norm: 3.3971\n",
            "Epoch 59: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0634, Test L1 Norm: 0.0315, Train Linf Norm: 11.0681, Test Linf Norm: 3.1353\n",
            "Epoch 60: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0613, Test L1 Norm: 0.0306, Train Linf Norm: 10.4706, Test Linf Norm: 3.0289\n",
            "Epoch 61: Train Loss: 0.0007, Test Loss: 0.0005, Train L1 Norm: 0.0635, Test L1 Norm: 0.0303, Train Linf Norm: 11.1974, Test Linf Norm: 3.0142\n",
            "Epoch 62: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0599, Test L1 Norm: 0.0298, Train Linf Norm: 10.3154, Test Linf Norm: 2.9134\n",
            "Epoch 63: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0538, Test L1 Norm: 0.0297, Train Linf Norm: 8.7289, Test Linf Norm: 2.8726\n",
            "Epoch 64: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0640, Test L1 Norm: 0.0302, Train Linf Norm: 11.2284, Test Linf Norm: 3.0245\n",
            "Epoch 65: Train Loss: 0.0005, Test Loss: 0.0010, Train L1 Norm: 0.0559, Test L1 Norm: 0.0317, Train Linf Norm: 9.3535, Test Linf Norm: 3.0418\n",
            "Epoch 66: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.0517, Test L1 Norm: 0.0314, Train Linf Norm: 7.8195, Test Linf Norm: 3.1758\n",
            "Epoch 67: Train Loss: 0.0006, Test Loss: 0.0014, Train L1 Norm: 0.0515, Test L1 Norm: 0.0351, Train Linf Norm: 8.3323, Test Linf Norm: 3.3194\n",
            "Epoch 68: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0620, Test L1 Norm: 0.0300, Train Linf Norm: 10.8973, Test Linf Norm: 3.0501\n",
            "Epoch 69: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0559, Test L1 Norm: 0.0292, Train Linf Norm: 9.4569, Test Linf Norm: 2.7873\n",
            "Epoch 70: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0645, Test L1 Norm: 0.0293, Train Linf Norm: 11.4451, Test Linf Norm: 2.8474\n",
            "Epoch 71: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0636, Test L1 Norm: 0.0284, Train Linf Norm: 11.3836, Test Linf Norm: 2.6143\n",
            "Epoch 72: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0532, Test L1 Norm: 0.0311, Train Linf Norm: 8.7966, Test Linf Norm: 3.2809\n",
            "Epoch 73: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0525, Test L1 Norm: 0.0289, Train Linf Norm: 8.5835, Test Linf Norm: 2.7818\n",
            "Epoch 74: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0578, Test L1 Norm: 0.0296, Train Linf Norm: 9.8947, Test Linf Norm: 3.0550\n",
            "Epoch 75: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0511, Test L1 Norm: 0.0288, Train Linf Norm: 8.3736, Test Linf Norm: 2.8940\n",
            "Epoch 76: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0553, Test L1 Norm: 0.0292, Train Linf Norm: 9.2775, Test Linf Norm: 2.9459\n",
            "Epoch 77: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0538, Test L1 Norm: 0.0292, Train Linf Norm: 8.9515, Test Linf Norm: 3.0055\n",
            "Epoch 78: Train Loss: 0.0005, Test Loss: 0.0010, Train L1 Norm: 0.0533, Test L1 Norm: 0.0305, Train Linf Norm: 8.9537, Test Linf Norm: 2.6570\n",
            "Epoch 79: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0618, Test L1 Norm: 0.0283, Train Linf Norm: 11.1667, Test Linf Norm: 2.7110\n",
            "Epoch 80: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0514, Test L1 Norm: 0.0280, Train Linf Norm: 8.5276, Test Linf Norm: 2.7115\n",
            "Epoch 81: Train Loss: 0.0005, Test Loss: 0.0023, Train L1 Norm: 0.0630, Test L1 Norm: 0.0330, Train Linf Norm: 11.5357, Test Linf Norm: 2.7411\n",
            "Epoch 82: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0532, Test L1 Norm: 0.0289, Train Linf Norm: 8.9689, Test Linf Norm: 2.9261\n",
            "Epoch 83: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0394, Test L1 Norm: 0.0296, Train Linf Norm: 5.4863, Test Linf Norm: 3.0118\n",
            "Epoch 84: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0600, Test L1 Norm: 0.0282, Train Linf Norm: 10.7234, Test Linf Norm: 2.7761\n",
            "Epoch 85: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0544, Test L1 Norm: 0.0286, Train Linf Norm: 9.3848, Test Linf Norm: 2.9018\n",
            "Epoch 86: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0591, Test L1 Norm: 0.0272, Train Linf Norm: 10.5305, Test Linf Norm: 2.5278\n",
            "Epoch 87: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0538, Test L1 Norm: 0.0282, Train Linf Norm: 9.1109, Test Linf Norm: 2.8137\n",
            "Epoch 88: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0596, Test L1 Norm: 0.0287, Train Linf Norm: 10.7028, Test Linf Norm: 2.9079\n",
            "Epoch 89: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0553, Test L1 Norm: 0.0274, Train Linf Norm: 9.7126, Test Linf Norm: 2.6610\n",
            "Epoch 90: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0550, Test L1 Norm: 0.0275, Train Linf Norm: 9.7213, Test Linf Norm: 2.7198\n",
            "Epoch 91: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0557, Test L1 Norm: 0.0276, Train Linf Norm: 9.9538, Test Linf Norm: 2.7058\n",
            "Epoch 92: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0540, Test L1 Norm: 0.0274, Train Linf Norm: 9.4725, Test Linf Norm: 2.6882\n",
            "Epoch 93: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0545, Test L1 Norm: 0.0277, Train Linf Norm: 9.6675, Test Linf Norm: 2.7327\n",
            "Epoch 94: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0558, Test L1 Norm: 0.0274, Train Linf Norm: 9.8050, Test Linf Norm: 2.6949\n",
            "Epoch 95: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0531, Test L1 Norm: 0.0273, Train Linf Norm: 9.2664, Test Linf Norm: 2.6442\n",
            "Epoch 96: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0545, Test L1 Norm: 0.0273, Train Linf Norm: 9.5184, Test Linf Norm: 2.6900\n",
            "Epoch 97: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0544, Test L1 Norm: 0.0271, Train Linf Norm: 9.6024, Test Linf Norm: 2.6484\n",
            "Epoch 98: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0563, Test L1 Norm: 0.0273, Train Linf Norm: 9.9908, Test Linf Norm: 2.6751\n",
            "Epoch 99: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0544, Test L1 Norm: 0.0273, Train Linf Norm: 9.6454, Test Linf Norm: 2.6749\n",
            "Epoch 100: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0551, Test L1 Norm: 0.0273, Train Linf Norm: 9.7507, Test Linf Norm: 2.6878\n",
            "Epoch 101: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0543, Test L1 Norm: 0.0274, Train Linf Norm: 9.5613, Test Linf Norm: 2.7097\n",
            "Epoch 102: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0542, Test L1 Norm: 0.0273, Train Linf Norm: 9.5796, Test Linf Norm: 2.6888\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 16:56:33,430]\u001b[0m Trial 12 finished with value: 0.027346139679849148 and parameters: {'n_layers': 2, 'n_units_0': 674, 'n_units_1': 86, 'hidden_activation': 'ReLU', 'output_activation': 'Linear', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.001325400412007455, 'batch_size': 256, 'n_epochs': 103, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.18007484166371648, 'patience': 5, 'threshold': 0.007470674215215709}. Best is trial 10 with value: 0.023221840736269952.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 103: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0545, Test L1 Norm: 0.0273, Train Linf Norm: 9.6220, Test Linf Norm: 2.6965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 16:56:34,616]\u001b[0m Trial 13 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 1.1601, Test Loss: 0.7447, Train L1 Norm: 5.9875, Test L1 Norm: 1.9301, Train Linf Norm: 1189.5881, Test Linf Norm: 179.0095\n",
            "Epoch 1: Train Loss: 0.2471, Test Loss: 0.0271, Train L1 Norm: 0.3651, Test L1 Norm: 0.0654, Train Linf Norm: 13.2264, Test Linf Norm: 1.2643\n",
            "Epoch 2: Train Loss: 0.0168, Test Loss: 0.0117, Train L1 Norm: 0.1441, Test L1 Norm: 0.0553, Train Linf Norm: 5.4118, Test Linf Norm: 1.2640\n",
            "Epoch 3: Train Loss: 0.0109, Test Loss: 0.0068, Train L1 Norm: 0.1447, Test L1 Norm: 0.0429, Train Linf Norm: 5.7423, Test Linf Norm: 1.0200\n",
            "Epoch 4: Train Loss: 0.0056, Test Loss: 0.0043, Train L1 Norm: 0.1080, Test L1 Norm: 0.0358, Train Linf Norm: 4.1427, Test Linf Norm: 0.8321\n",
            "Epoch 5: Train Loss: 0.0046, Test Loss: 0.0103, Train L1 Norm: 0.1052, Test L1 Norm: 0.0478, Train Linf Norm: 4.0797, Test Linf Norm: 1.0792\n",
            "Epoch 6: Train Loss: 0.0038, Test Loss: 0.0029, Train L1 Norm: 0.0921, Test L1 Norm: 0.0339, Train Linf Norm: 3.5193, Test Linf Norm: 0.8247\n",
            "Epoch 7: Train Loss: 0.0036, Test Loss: 0.0034, Train L1 Norm: 0.0717, Test L1 Norm: 0.0329, Train Linf Norm: 2.5884, Test Linf Norm: 0.7730\n",
            "Epoch 8: Train Loss: 0.0031, Test Loss: 0.0029, Train L1 Norm: 0.0752, Test L1 Norm: 0.0351, Train Linf Norm: 2.7820, Test Linf Norm: 0.8639\n",
            "Epoch 9: Train Loss: 0.0025, Test Loss: 0.0020, Train L1 Norm: 0.0641, Test L1 Norm: 0.0307, Train Linf Norm: 2.2815, Test Linf Norm: 0.7538\n",
            "Epoch 10: Train Loss: 0.0029, Test Loss: 0.0109, Train L1 Norm: 0.0702, Test L1 Norm: 0.0441, Train Linf Norm: 2.6088, Test Linf Norm: 0.8345\n",
            "Epoch 11: Train Loss: 0.0022, Test Loss: 0.0017, Train L1 Norm: 0.0652, Test L1 Norm: 0.0289, Train Linf Norm: 2.3969, Test Linf Norm: 0.6988\n",
            "Epoch 12: Train Loss: 0.0022, Test Loss: 0.0018, Train L1 Norm: 0.0770, Test L1 Norm: 0.0309, Train Linf Norm: 2.9813, Test Linf Norm: 0.7907\n",
            "Epoch 13: Train Loss: 0.0020, Test Loss: 0.0016, Train L1 Norm: 0.0584, Test L1 Norm: 0.0280, Train Linf Norm: 2.1064, Test Linf Norm: 0.6938\n",
            "Epoch 14: Train Loss: 0.0018, Test Loss: 0.0013, Train L1 Norm: 0.0650, Test L1 Norm: 0.0308, Train Linf Norm: 2.4432, Test Linf Norm: 0.8133\n",
            "Epoch 15: Train Loss: 0.0017, Test Loss: 0.0014, Train L1 Norm: 0.0691, Test L1 Norm: 0.0262, Train Linf Norm: 2.6286, Test Linf Norm: 0.6291\n",
            "Epoch 16: Train Loss: 0.0016, Test Loss: 0.0014, Train L1 Norm: 0.0792, Test L1 Norm: 0.0285, Train Linf Norm: 3.1414, Test Linf Norm: 0.7329\n",
            "Epoch 17: Train Loss: 0.0014, Test Loss: 0.0012, Train L1 Norm: 0.0815, Test L1 Norm: 0.0252, Train Linf Norm: 3.2734, Test Linf Norm: 0.6022\n",
            "Epoch 18: Train Loss: 0.0017, Test Loss: 0.0011, Train L1 Norm: 0.0614, Test L1 Norm: 0.0250, Train Linf Norm: 2.3055, Test Linf Norm: 0.6196\n",
            "Epoch 19: Train Loss: 0.0013, Test Loss: 0.0012, Train L1 Norm: 0.0856, Test L1 Norm: 0.0252, Train Linf Norm: 3.4846, Test Linf Norm: 0.6362\n",
            "Epoch 20: Train Loss: 0.0012, Test Loss: 0.0010, Train L1 Norm: 0.0862, Test L1 Norm: 0.0251, Train Linf Norm: 3.5189, Test Linf Norm: 0.6405\n",
            "Epoch 21: Train Loss: 0.0012, Test Loss: 0.0011, Train L1 Norm: 0.0782, Test L1 Norm: 0.0251, Train Linf Norm: 3.1611, Test Linf Norm: 0.6448\n",
            "Epoch 22: Train Loss: 0.0012, Test Loss: 0.0010, Train L1 Norm: 0.1006, Test L1 Norm: 0.0266, Train Linf Norm: 4.2433, Test Linf Norm: 0.7019\n",
            "Epoch 23: Train Loss: 0.0012, Test Loss: 0.0012, Train L1 Norm: 0.0770, Test L1 Norm: 0.0261, Train Linf Norm: 3.1067, Test Linf Norm: 0.6795\n",
            "Epoch 24: Train Loss: 0.0011, Test Loss: 0.0009, Train L1 Norm: 0.0730, Test L1 Norm: 0.0245, Train Linf Norm: 2.9308, Test Linf Norm: 0.6215\n",
            "Epoch 25: Train Loss: 0.0012, Test Loss: 0.0010, Train L1 Norm: 0.0927, Test L1 Norm: 0.0247, Train Linf Norm: 3.8620, Test Linf Norm: 0.6283\n",
            "Epoch 26: Train Loss: 0.0010, Test Loss: 0.0011, Train L1 Norm: 0.0781, Test L1 Norm: 0.0242, Train Linf Norm: 3.1749, Test Linf Norm: 0.6086\n",
            "Epoch 27: Train Loss: 0.0010, Test Loss: 0.0008, Train L1 Norm: 0.0830, Test L1 Norm: 0.0234, Train Linf Norm: 3.3950, Test Linf Norm: 0.5971\n",
            "Epoch 28: Train Loss: 0.0009, Test Loss: 0.0008, Train L1 Norm: 0.0809, Test L1 Norm: 0.0244, Train Linf Norm: 3.3306, Test Linf Norm: 0.6424\n",
            "Epoch 29: Train Loss: 0.0009, Test Loss: 0.0008, Train L1 Norm: 0.0695, Test L1 Norm: 0.0232, Train Linf Norm: 2.7811, Test Linf Norm: 0.5959\n",
            "Epoch 30: Train Loss: 0.0009, Test Loss: 0.0007, Train L1 Norm: 0.0815, Test L1 Norm: 0.0230, Train Linf Norm: 3.3715, Test Linf Norm: 0.5893\n",
            "Epoch 31: Train Loss: 0.0010, Test Loss: 0.0007, Train L1 Norm: 0.0823, Test L1 Norm: 0.0234, Train Linf Norm: 3.4038, Test Linf Norm: 0.6103\n",
            "Epoch 32: Train Loss: 0.0009, Test Loss: 0.0059, Train L1 Norm: 0.0785, Test L1 Norm: 0.0288, Train Linf Norm: 3.2256, Test Linf Norm: 0.6285\n",
            "Epoch 33: Train Loss: 0.0008, Test Loss: 0.0007, Train L1 Norm: 0.0752, Test L1 Norm: 0.0228, Train Linf Norm: 3.0744, Test Linf Norm: 0.5867\n",
            "Epoch 34: Train Loss: 0.0008, Test Loss: 0.0007, Train L1 Norm: 0.0729, Test L1 Norm: 0.0221, Train Linf Norm: 2.9800, Test Linf Norm: 0.5421\n",
            "Epoch 35: Train Loss: 0.0008, Test Loss: 0.0007, Train L1 Norm: 0.0801, Test L1 Norm: 0.0219, Train Linf Norm: 3.3213, Test Linf Norm: 0.5505\n",
            "Epoch 36: Train Loss: 0.0008, Test Loss: 0.0007, Train L1 Norm: 0.0793, Test L1 Norm: 0.0216, Train Linf Norm: 3.2907, Test Linf Norm: 0.5379\n",
            "Epoch 37: Train Loss: 0.0007, Test Loss: 0.0007, Train L1 Norm: 0.0748, Test L1 Norm: 0.0211, Train Linf Norm: 3.0819, Test Linf Norm: 0.5037\n",
            "Epoch 38: Train Loss: 0.0007, Test Loss: 0.0006, Train L1 Norm: 0.0737, Test L1 Norm: 0.0213, Train Linf Norm: 3.0299, Test Linf Norm: 0.5286\n",
            "Epoch 39: Train Loss: 0.0008, Test Loss: 0.0006, Train L1 Norm: 0.0716, Test L1 Norm: 0.0211, Train Linf Norm: 2.9295, Test Linf Norm: 0.5208\n",
            "Epoch 40: Train Loss: 0.0007, Test Loss: 0.0007, Train L1 Norm: 0.0764, Test L1 Norm: 0.0216, Train Linf Norm: 3.1646, Test Linf Norm: 0.5444\n",
            "Epoch 41: Train Loss: 0.0007, Test Loss: 0.0007, Train L1 Norm: 0.0744, Test L1 Norm: 0.0213, Train Linf Norm: 3.0654, Test Linf Norm: 0.5147\n",
            "Epoch 42: Train Loss: 0.0007, Test Loss: 0.0006, Train L1 Norm: 0.0715, Test L1 Norm: 0.0205, Train Linf Norm: 2.9312, Test Linf Norm: 0.4957\n",
            "Epoch 43: Train Loss: 0.0007, Test Loss: 0.0006, Train L1 Norm: 0.0793, Test L1 Norm: 0.0210, Train Linf Norm: 3.3118, Test Linf Norm: 0.5258\n",
            "Epoch 44: Train Loss: 0.0007, Test Loss: 0.0006, Train L1 Norm: 0.0739, Test L1 Norm: 0.0213, Train Linf Norm: 3.0372, Test Linf Norm: 0.5393\n",
            "Epoch 45: Train Loss: 0.0007, Test Loss: 0.0005, Train L1 Norm: 0.0681, Test L1 Norm: 0.0222, Train Linf Norm: 2.7782, Test Linf Norm: 0.5806\n",
            "Epoch 46: Train Loss: 0.0006, Test Loss: 0.0007, Train L1 Norm: 0.0730, Test L1 Norm: 0.0224, Train Linf Norm: 3.0210, Test Linf Norm: 0.5791\n",
            "Epoch 47: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0793, Test L1 Norm: 0.0208, Train Linf Norm: 3.3209, Test Linf Norm: 0.5280\n",
            "Epoch 48: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0634, Test L1 Norm: 0.0214, Train Linf Norm: 2.5696, Test Linf Norm: 0.5559\n",
            "Epoch 49: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0723, Test L1 Norm: 0.0209, Train Linf Norm: 2.9893, Test Linf Norm: 0.5333\n",
            "Epoch 50: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0686, Test L1 Norm: 0.0201, Train Linf Norm: 2.8198, Test Linf Norm: 0.4996\n",
            "Epoch 51: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0717, Test L1 Norm: 0.0209, Train Linf Norm: 2.9779, Test Linf Norm: 0.5314\n",
            "Epoch 52: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0847, Test L1 Norm: 0.0204, Train Linf Norm: 3.5994, Test Linf Norm: 0.4874\n",
            "Epoch 53: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0862, Test L1 Norm: 0.0218, Train Linf Norm: 3.6738, Test Linf Norm: 0.5713\n",
            "Epoch 54: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0728, Test L1 Norm: 0.0208, Train Linf Norm: 3.0174, Test Linf Norm: 0.5353\n",
            "Epoch 55: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0743, Test L1 Norm: 0.0196, Train Linf Norm: 3.1047, Test Linf Norm: 0.4842\n",
            "Epoch 56: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0746, Test L1 Norm: 0.0199, Train Linf Norm: 3.1175, Test Linf Norm: 0.4837\n",
            "Epoch 57: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0726, Test L1 Norm: 0.0195, Train Linf Norm: 3.0319, Test Linf Norm: 0.4745\n",
            "Epoch 58: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.0718, Test L1 Norm: 0.0210, Train Linf Norm: 2.9888, Test Linf Norm: 0.5260\n",
            "Epoch 59: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0739, Test L1 Norm: 0.0210, Train Linf Norm: 3.0937, Test Linf Norm: 0.5475\n",
            "Epoch 60: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0615, Test L1 Norm: 0.0197, Train Linf Norm: 2.5055, Test Linf Norm: 0.4771\n",
            "Epoch 61: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0705, Test L1 Norm: 0.0209, Train Linf Norm: 2.9400, Test Linf Norm: 0.5404\n",
            "Epoch 62: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0730, Test L1 Norm: 0.0193, Train Linf Norm: 3.0584, Test Linf Norm: 0.4748\n",
            "Epoch 63: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0640, Test L1 Norm: 0.0194, Train Linf Norm: 2.6298, Test Linf Norm: 0.4883\n",
            "Epoch 64: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0639, Test L1 Norm: 0.0193, Train Linf Norm: 2.6096, Test Linf Norm: 0.4823\n",
            "Epoch 65: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0690, Test L1 Norm: 0.0195, Train Linf Norm: 2.8678, Test Linf Norm: 0.4985\n",
            "Epoch 66: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0661, Test L1 Norm: 0.0213, Train Linf Norm: 2.7240, Test Linf Norm: 0.5693\n",
            "Epoch 67: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0674, Test L1 Norm: 0.0197, Train Linf Norm: 2.7961, Test Linf Norm: 0.5013\n",
            "Epoch 68: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0608, Test L1 Norm: 0.0207, Train Linf Norm: 2.4831, Test Linf Norm: 0.5325\n",
            "Epoch 69: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0640, Test L1 Norm: 0.0188, Train Linf Norm: 2.6341, Test Linf Norm: 0.4736\n",
            "Epoch 70: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0631, Test L1 Norm: 0.0198, Train Linf Norm: 2.5906, Test Linf Norm: 0.5139\n",
            "Epoch 71: Train Loss: 0.0005, Test Loss: 0.0261, Train L1 Norm: 0.0681, Test L1 Norm: 0.0563, Train Linf Norm: 2.8431, Test Linf Norm: 0.8818\n",
            "Epoch 72: Train Loss: 0.0006, Test Loss: 0.0004, Train L1 Norm: 0.0625, Test L1 Norm: 0.0182, Train Linf Norm: 2.5680, Test Linf Norm: 0.4495\n",
            "Epoch 73: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0636, Test L1 Norm: 0.0185, Train Linf Norm: 2.6223, Test Linf Norm: 0.4644\n",
            "Epoch 74: Train Loss: 0.0005, Test Loss: 0.0012, Train L1 Norm: 0.0645, Test L1 Norm: 0.0223, Train Linf Norm: 2.6706, Test Linf Norm: 0.5490\n",
            "Epoch 75: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0626, Test L1 Norm: 0.0183, Train Linf Norm: 2.5789, Test Linf Norm: 0.4546\n",
            "Epoch 76: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0626, Test L1 Norm: 0.0185, Train Linf Norm: 2.5808, Test Linf Norm: 0.4650\n",
            "Epoch 77: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0663, Test L1 Norm: 0.0188, Train Linf Norm: 2.7624, Test Linf Norm: 0.4732\n",
            "Epoch 78: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0607, Test L1 Norm: 0.0185, Train Linf Norm: 2.4982, Test Linf Norm: 0.4655\n",
            "Epoch 79: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0601, Test L1 Norm: 0.0191, Train Linf Norm: 2.4585, Test Linf Norm: 0.4892\n",
            "Epoch 80: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0621, Test L1 Norm: 0.0179, Train Linf Norm: 2.5631, Test Linf Norm: 0.4342\n",
            "Epoch 81: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0538, Test L1 Norm: 0.0180, Train Linf Norm: 2.1628, Test Linf Norm: 0.4463\n",
            "Epoch 82: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0601, Test L1 Norm: 0.0186, Train Linf Norm: 2.4677, Test Linf Norm: 0.4685\n",
            "Epoch 83: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0623, Test L1 Norm: 0.0190, Train Linf Norm: 2.5748, Test Linf Norm: 0.4887\n",
            "Epoch 84: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0606, Test L1 Norm: 0.0177, Train Linf Norm: 2.4996, Test Linf Norm: 0.4370\n",
            "Epoch 85: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0617, Test L1 Norm: 0.0188, Train Linf Norm: 2.5457, Test Linf Norm: 0.4798\n",
            "Epoch 86: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0612, Test L1 Norm: 0.0178, Train Linf Norm: 2.5284, Test Linf Norm: 0.4419\n",
            "Epoch 87: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0616, Test L1 Norm: 0.0176, Train Linf Norm: 2.5460, Test Linf Norm: 0.4385\n",
            "Epoch 88: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0608, Test L1 Norm: 0.0174, Train Linf Norm: 2.5036, Test Linf Norm: 0.4269\n",
            "Epoch 89: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0605, Test L1 Norm: 0.0185, Train Linf Norm: 2.4949, Test Linf Norm: 0.4746\n",
            "Epoch 90: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0593, Test L1 Norm: 0.0175, Train Linf Norm: 2.4410, Test Linf Norm: 0.4323\n",
            "Epoch 91: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0626, Test L1 Norm: 0.0182, Train Linf Norm: 2.5979, Test Linf Norm: 0.4632\n",
            "Epoch 92: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0576, Test L1 Norm: 0.0183, Train Linf Norm: 2.3587, Test Linf Norm: 0.4658\n",
            "Epoch 93: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0467, Test L1 Norm: 0.0187, Train Linf Norm: 1.8387, Test Linf Norm: 0.4729\n",
            "Epoch 94: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0589, Test L1 Norm: 0.0176, Train Linf Norm: 2.4274, Test Linf Norm: 0.4400\n",
            "Epoch 95: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0595, Test L1 Norm: 0.0169, Train Linf Norm: 2.4502, Test Linf Norm: 0.4091\n",
            "Epoch 96: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0751, Test L1 Norm: 0.0173, Train Linf Norm: 3.1969, Test Linf Norm: 0.4296\n",
            "Epoch 97: Train Loss: 0.0004, Test Loss: 0.0005, Train L1 Norm: 0.0531, Test L1 Norm: 0.0175, Train Linf Norm: 2.1510, Test Linf Norm: 0.4335\n",
            "Epoch 98: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0507, Test L1 Norm: 0.0181, Train Linf Norm: 2.0318, Test Linf Norm: 0.4644\n",
            "Epoch 99: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0359, Test L1 Norm: 0.0173, Train Linf Norm: 1.3159, Test Linf Norm: 0.4245\n",
            "Epoch 100: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0564, Test L1 Norm: 0.0175, Train Linf Norm: 2.3104, Test Linf Norm: 0.4380\n",
            "Epoch 101: Train Loss: 0.0004, Test Loss: 0.0005, Train L1 Norm: 0.0571, Test L1 Norm: 0.0176, Train Linf Norm: 2.3467, Test Linf Norm: 0.4436\n",
            "Epoch 102: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0544, Test L1 Norm: 0.0173, Train Linf Norm: 2.2160, Test Linf Norm: 0.4355\n",
            "Epoch 103: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0551, Test L1 Norm: 0.0174, Train Linf Norm: 2.2575, Test Linf Norm: 0.4357\n",
            "Epoch 104: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0534, Test L1 Norm: 0.0171, Train Linf Norm: 2.1912, Test Linf Norm: 0.4243\n",
            "Epoch 105: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0537, Test L1 Norm: 0.0172, Train Linf Norm: 2.2007, Test Linf Norm: 0.4270\n",
            "Epoch 106: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0540, Test L1 Norm: 0.0170, Train Linf Norm: 2.2172, Test Linf Norm: 0.4211\n",
            "Epoch 107: Train Loss: 0.0003, Test Loss: 0.0004, Train L1 Norm: 0.0543, Test L1 Norm: 0.0171, Train Linf Norm: 2.2304, Test Linf Norm: 0.4215\n",
            "Epoch 108: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0542, Test L1 Norm: 0.0171, Train Linf Norm: 2.2159, Test Linf Norm: 0.4268\n",
            "Epoch 109: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0541, Test L1 Norm: 0.0172, Train Linf Norm: 2.2225, Test Linf Norm: 0.4291\n",
            "Epoch 110: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0539, Test L1 Norm: 0.0170, Train Linf Norm: 2.1837, Test Linf Norm: 0.4203\n",
            "Epoch 111: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0529, Test L1 Norm: 0.0170, Train Linf Norm: 2.1651, Test Linf Norm: 0.4240\n",
            "Epoch 112: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0534, Test L1 Norm: 0.0171, Train Linf Norm: 2.1891, Test Linf Norm: 0.4258\n",
            "Epoch 113: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0535, Test L1 Norm: 0.0171, Train Linf Norm: 2.1916, Test Linf Norm: 0.4271\n",
            "Epoch 114: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0534, Test L1 Norm: 0.0171, Train Linf Norm: 2.1829, Test Linf Norm: 0.4248\n",
            "Epoch 115: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0533, Test L1 Norm: 0.0170, Train Linf Norm: 2.1839, Test Linf Norm: 0.4235\n",
            "Epoch 116: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0531, Test L1 Norm: 0.0171, Train Linf Norm: 2.1790, Test Linf Norm: 0.4251\n",
            "Epoch 117: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0535, Test L1 Norm: 0.0171, Train Linf Norm: 2.1974, Test Linf Norm: 0.4253\n",
            "Epoch 118: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0535, Test L1 Norm: 0.0171, Train Linf Norm: 2.1931, Test Linf Norm: 0.4253\n",
            "Epoch 119: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0535, Test L1 Norm: 0.0171, Train Linf Norm: 2.1979, Test Linf Norm: 0.4254\n",
            "Epoch 120: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0535, Test L1 Norm: 0.0171, Train Linf Norm: 2.1888, Test Linf Norm: 0.4252\n",
            "Epoch 121: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0535, Test L1 Norm: 0.0171, Train Linf Norm: 2.1959, Test Linf Norm: 0.4253\n",
            "Epoch 122: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0536, Test L1 Norm: 0.0171, Train Linf Norm: 2.1960, Test Linf Norm: 0.4249\n",
            "Epoch 123: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0535, Test L1 Norm: 0.0171, Train Linf Norm: 2.1894, Test Linf Norm: 0.4250\n",
            "Epoch 124: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0535, Test L1 Norm: 0.0171, Train Linf Norm: 2.1960, Test Linf Norm: 0.4250\n",
            "Epoch 125: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0535, Test L1 Norm: 0.0171, Train Linf Norm: 2.1929, Test Linf Norm: 0.4250\n",
            "Epoch 126: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0535, Test L1 Norm: 0.0171, Train Linf Norm: 2.1951, Test Linf Norm: 0.4250\n",
            "Epoch 127: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0535, Test L1 Norm: 0.0171, Train Linf Norm: 2.1887, Test Linf Norm: 0.4250\n",
            "Epoch 128: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0535, Test L1 Norm: 0.0171, Train Linf Norm: 2.1938, Test Linf Norm: 0.4250\n",
            "Epoch 129: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0535, Test L1 Norm: 0.0171, Train Linf Norm: 2.1479, Test Linf Norm: 0.4250\n",
            "Epoch 130: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0535, Test L1 Norm: 0.0171, Train Linf Norm: 2.1933, Test Linf Norm: 0.4250\n",
            "Epoch 131: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0535, Test L1 Norm: 0.0171, Train Linf Norm: 2.1807, Test Linf Norm: 0.4250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 17:05:45,950]\u001b[0m Trial 14 finished with value: 0.01707801818549633 and parameters: {'n_layers': 2, 'n_units_0': 871, 'n_units_1': 317, 'hidden_activation': 'ReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adagrad', 'lr': 0.001016472444043591, 'batch_size': 48, 'n_epochs': 132, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.10586643362396991, 'patience': 6, 'threshold': 0.001997656486403879}. Best is trial 14 with value: 0.01707801818549633.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 132: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0535, Test L1 Norm: 0.0171, Train Linf Norm: 2.1912, Test Linf Norm: 0.4250\n",
            "Epoch 1: Train Loss: 0.2224, Test Loss: 0.0258, Train L1 Norm: 0.3665, Test L1 Norm: 0.0727, Train Linf Norm: 13.9351, Test Linf Norm: 1.8374\n",
            "Epoch 2: Train Loss: 0.0358, Test Loss: 0.0043, Train L1 Norm: 0.1537, Test L1 Norm: 0.0324, Train Linf Norm: 5.7567, Test Linf Norm: 0.6342\n",
            "Epoch 3: Train Loss: 0.0087, Test Loss: 0.0042, Train L1 Norm: 0.1088, Test L1 Norm: 0.0308, Train Linf Norm: 4.2627, Test Linf Norm: 0.6876\n",
            "Epoch 4: Train Loss: 0.0030, Test Loss: 0.0021, Train L1 Norm: 0.0695, Test L1 Norm: 0.0258, Train Linf Norm: 2.6078, Test Linf Norm: 0.5953\n",
            "Epoch 5: Train Loss: 0.0044, Test Loss: 0.0024, Train L1 Norm: 0.0695, Test L1 Norm: 0.0220, Train Linf Norm: 2.6586, Test Linf Norm: 0.4666\n",
            "Epoch 6: Train Loss: 0.0031, Test Loss: 0.0010, Train L1 Norm: 0.0454, Test L1 Norm: 0.0195, Train Linf Norm: 1.5623, Test Linf Norm: 0.4386\n",
            "Epoch 7: Train Loss: 0.0016, Test Loss: 0.0008, Train L1 Norm: 0.0414, Test L1 Norm: 0.0193, Train Linf Norm: 1.4353, Test Linf Norm: 0.4454\n",
            "Epoch 8: Train Loss: 0.0010, Test Loss: 0.0009, Train L1 Norm: 0.0355, Test L1 Norm: 0.0192, Train Linf Norm: 1.1959, Test Linf Norm: 0.4397\n",
            "Epoch 9: Train Loss: 0.0021, Test Loss: 0.0006, Train L1 Norm: 0.0309, Test L1 Norm: 0.0183, Train Linf Norm: 0.9662, Test Linf Norm: 0.4248\n",
            "Epoch 10: Train Loss: 0.0028, Test Loss: 0.0085, Train L1 Norm: 0.0474, Test L1 Norm: 0.0243, Train Linf Norm: 1.7755, Test Linf Norm: 0.5090\n",
            "Epoch 11: Train Loss: 0.0012, Test Loss: 0.0011, Train L1 Norm: 0.0405, Test L1 Norm: 0.0220, Train Linf Norm: 1.4621, Test Linf Norm: 0.5402\n",
            "Epoch 12: Train Loss: 0.0007, Test Loss: 0.0006, Train L1 Norm: 0.0373, Test L1 Norm: 0.0168, Train Linf Norm: 1.3433, Test Linf Norm: 0.3931\n",
            "Epoch 13: Train Loss: 0.0007, Test Loss: 0.0005, Train L1 Norm: 0.0384, Test L1 Norm: 0.0169, Train Linf Norm: 1.4097, Test Linf Norm: 0.3965\n",
            "Epoch 14: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0394, Test L1 Norm: 0.0162, Train Linf Norm: 1.4851, Test Linf Norm: 0.3902\n",
            "Epoch 15: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0356, Test L1 Norm: 0.0166, Train Linf Norm: 1.3071, Test Linf Norm: 0.4010\n",
            "Epoch 16: Train Loss: 0.0007, Test Loss: 0.0006, Train L1 Norm: 0.0390, Test L1 Norm: 0.0166, Train Linf Norm: 1.4664, Test Linf Norm: 0.4065\n",
            "Epoch 17: Train Loss: 0.0006, Test Loss: 0.0003, Train L1 Norm: 0.0288, Test L1 Norm: 0.0161, Train Linf Norm: 0.9722, Test Linf Norm: 0.3993\n",
            "Epoch 18: Train Loss: 0.0005, Test Loss: 0.0018, Train L1 Norm: 0.0348, Test L1 Norm: 0.0172, Train Linf Norm: 1.2840, Test Linf Norm: 0.3975\n",
            "Epoch 19: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.0388, Test L1 Norm: 0.0156, Train Linf Norm: 1.4756, Test Linf Norm: 0.3876\n",
            "Epoch 20: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0359, Test L1 Norm: 0.0157, Train Linf Norm: 1.3558, Test Linf Norm: 0.3930\n",
            "Epoch 21: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.0360, Test L1 Norm: 0.0156, Train Linf Norm: 1.3542, Test Linf Norm: 0.3810\n",
            "Epoch 22: Train Loss: 0.0010, Test Loss: 0.0003, Train L1 Norm: 0.0356, Test L1 Norm: 0.0159, Train Linf Norm: 1.2972, Test Linf Norm: 0.4030\n",
            "Epoch 23: Train Loss: 0.0014, Test Loss: 0.0003, Train L1 Norm: 0.0507, Test L1 Norm: 0.0184, Train Linf Norm: 1.9724, Test Linf Norm: 0.4729\n",
            "Epoch 24: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.0401, Test L1 Norm: 0.0157, Train Linf Norm: 1.5566, Test Linf Norm: 0.3913\n",
            "Epoch 25: Train Loss: 0.0007, Test Loss: 0.0003, Train L1 Norm: 0.0380, Test L1 Norm: 0.0170, Train Linf Norm: 1.4613, Test Linf Norm: 0.4553\n",
            "Epoch 26: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.0386, Test L1 Norm: 0.0154, Train Linf Norm: 1.5012, Test Linf Norm: 0.3997\n",
            "Epoch 27: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0349, Test L1 Norm: 0.0151, Train Linf Norm: 1.3432, Test Linf Norm: 0.3868\n",
            "Epoch 28: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.0419, Test L1 Norm: 0.0162, Train Linf Norm: 1.6823, Test Linf Norm: 0.4247\n",
            "Epoch 29: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0383, Test L1 Norm: 0.0155, Train Linf Norm: 1.5087, Test Linf Norm: 0.4017\n",
            "Epoch 30: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0371, Test L1 Norm: 0.0157, Train Linf Norm: 1.4602, Test Linf Norm: 0.4213\n",
            "Epoch 31: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0373, Test L1 Norm: 0.0174, Train Linf Norm: 1.4678, Test Linf Norm: 0.4729\n",
            "Epoch 32: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0422, Test L1 Norm: 0.0145, Train Linf Norm: 1.7089, Test Linf Norm: 0.3766\n",
            "Epoch 33: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.0405, Test L1 Norm: 0.0156, Train Linf Norm: 1.6274, Test Linf Norm: 0.4261\n",
            "Epoch 34: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0374, Test L1 Norm: 0.0149, Train Linf Norm: 1.4737, Test Linf Norm: 0.3906\n",
            "Epoch 35: Train Loss: 0.0008, Test Loss: 0.0003, Train L1 Norm: 0.0321, Test L1 Norm: 0.0148, Train Linf Norm: 1.2067, Test Linf Norm: 0.3835\n",
            "Epoch 36: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.0347, Test L1 Norm: 0.0153, Train Linf Norm: 1.3466, Test Linf Norm: 0.4082\n",
            "Epoch 37: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0372, Test L1 Norm: 0.0137, Train Linf Norm: 1.4858, Test Linf Norm: 0.3469\n",
            "Epoch 38: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0423, Test L1 Norm: 0.0139, Train Linf Norm: 1.7277, Test Linf Norm: 0.3601\n",
            "Epoch 39: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.0402, Test L1 Norm: 0.0150, Train Linf Norm: 1.6210, Test Linf Norm: 0.3893\n",
            "Epoch 40: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0392, Test L1 Norm: 0.0138, Train Linf Norm: 1.5828, Test Linf Norm: 0.3596\n",
            "Epoch 41: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0336, Test L1 Norm: 0.0146, Train Linf Norm: 1.3171, Test Linf Norm: 0.3749\n",
            "Epoch 42: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0447, Test L1 Norm: 0.0142, Train Linf Norm: 1.8483, Test Linf Norm: 0.3804\n",
            "Epoch 43: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0420, Test L1 Norm: 0.0141, Train Linf Norm: 1.7277, Test Linf Norm: 0.3692\n",
            "Epoch 44: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0413, Test L1 Norm: 0.0140, Train Linf Norm: 1.6917, Test Linf Norm: 0.3715\n",
            "Epoch 45: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0391, Test L1 Norm: 0.0138, Train Linf Norm: 1.5887, Test Linf Norm: 0.3656\n",
            "Epoch 46: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0428, Test L1 Norm: 0.0134, Train Linf Norm: 1.7668, Test Linf Norm: 0.3484\n",
            "Epoch 47: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0375, Test L1 Norm: 0.0137, Train Linf Norm: 1.5000, Test Linf Norm: 0.3590\n",
            "Epoch 48: Train Loss: 0.0005, Test Loss: 0.0002, Train L1 Norm: 0.0421, Test L1 Norm: 0.0144, Train Linf Norm: 1.7235, Test Linf Norm: 0.3846\n",
            "Epoch 49: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0447, Test L1 Norm: 0.0136, Train Linf Norm: 1.8675, Test Linf Norm: 0.3617\n",
            "Epoch 50: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0381, Test L1 Norm: 0.0131, Train Linf Norm: 1.5506, Test Linf Norm: 0.3405\n",
            "Epoch 51: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0368, Test L1 Norm: 0.0146, Train Linf Norm: 1.4800, Test Linf Norm: 0.3886\n",
            "Epoch 52: Train Loss: 0.0003, Test Loss: 0.0037, Train L1 Norm: 0.0303, Test L1 Norm: 0.0239, Train Linf Norm: 1.1721, Test Linf Norm: 0.4839\n",
            "Epoch 53: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0377, Test L1 Norm: 0.0129, Train Linf Norm: 1.5313, Test Linf Norm: 0.3331\n",
            "Epoch 54: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0390, Test L1 Norm: 0.0144, Train Linf Norm: 1.5947, Test Linf Norm: 0.3758\n",
            "Epoch 55: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0374, Test L1 Norm: 0.0133, Train Linf Norm: 1.5070, Test Linf Norm: 0.3597\n",
            "Epoch 56: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0400, Test L1 Norm: 0.0131, Train Linf Norm: 1.6482, Test Linf Norm: 0.3463\n",
            "Epoch 57: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0395, Test L1 Norm: 0.0130, Train Linf Norm: 1.6216, Test Linf Norm: 0.3459\n",
            "Epoch 58: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0402, Test L1 Norm: 0.0127, Train Linf Norm: 1.6540, Test Linf Norm: 0.3320\n",
            "Epoch 59: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0390, Test L1 Norm: 0.0130, Train Linf Norm: 1.5836, Test Linf Norm: 0.3399\n",
            "Epoch 60: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0411, Test L1 Norm: 0.0126, Train Linf Norm: 1.7028, Test Linf Norm: 0.3298\n",
            "Epoch 61: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0412, Test L1 Norm: 0.0129, Train Linf Norm: 1.7072, Test Linf Norm: 0.3434\n",
            "Epoch 62: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0406, Test L1 Norm: 0.0127, Train Linf Norm: 1.6793, Test Linf Norm: 0.3367\n",
            "Epoch 63: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0414, Test L1 Norm: 0.0125, Train Linf Norm: 1.7204, Test Linf Norm: 0.3258\n",
            "Epoch 64: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0387, Test L1 Norm: 0.0127, Train Linf Norm: 1.5951, Test Linf Norm: 0.3428\n",
            "Epoch 65: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0363, Test L1 Norm: 0.0126, Train Linf Norm: 1.4855, Test Linf Norm: 0.3380\n",
            "Epoch 66: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0339, Test L1 Norm: 0.0124, Train Linf Norm: 1.3651, Test Linf Norm: 0.3255\n",
            "Epoch 67: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0393, Test L1 Norm: 0.0129, Train Linf Norm: 1.6278, Test Linf Norm: 0.3444\n",
            "Epoch 68: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.0439, Test L1 Norm: 0.0124, Train Linf Norm: 1.8281, Test Linf Norm: 0.3306\n",
            "Epoch 69: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0420, Test L1 Norm: 0.0134, Train Linf Norm: 1.7501, Test Linf Norm: 0.3744\n",
            "Epoch 70: Train Loss: 0.0002, Test Loss: 0.0003, Train L1 Norm: 0.0455, Test L1 Norm: 0.0126, Train Linf Norm: 1.9275, Test Linf Norm: 0.3265\n",
            "Epoch 71: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0418, Test L1 Norm: 0.0133, Train Linf Norm: 1.7579, Test Linf Norm: 0.3608\n",
            "Epoch 72: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0422, Test L1 Norm: 0.0121, Train Linf Norm: 1.7796, Test Linf Norm: 0.3214\n",
            "Epoch 73: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0121, Train Linf Norm: 1.7191, Test Linf Norm: 0.3196\n",
            "Epoch 74: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0411, Test L1 Norm: 0.0121, Train Linf Norm: 1.7321, Test Linf Norm: 0.3188\n",
            "Epoch 75: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0421, Test L1 Norm: 0.0121, Train Linf Norm: 1.7806, Test Linf Norm: 0.3195\n",
            "Epoch 76: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0410, Test L1 Norm: 0.0122, Train Linf Norm: 1.7250, Test Linf Norm: 0.3293\n",
            "Epoch 77: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0400, Test L1 Norm: 0.0121, Train Linf Norm: 1.6802, Test Linf Norm: 0.3197\n",
            "Epoch 78: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0417, Test L1 Norm: 0.0122, Train Linf Norm: 1.7611, Test Linf Norm: 0.3251\n",
            "Epoch 79: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0401, Test L1 Norm: 0.0121, Train Linf Norm: 1.6889, Test Linf Norm: 0.3232\n",
            "Epoch 80: Train Loss: 0.0001, Test Loss: 0.0003, Train L1 Norm: 0.0403, Test L1 Norm: 0.0125, Train Linf Norm: 1.6964, Test Linf Norm: 0.3174\n",
            "Epoch 81: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0406, Test L1 Norm: 0.0120, Train Linf Norm: 1.7063, Test Linf Norm: 0.3191\n",
            "Epoch 82: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0411, Test L1 Norm: 0.0121, Train Linf Norm: 1.7335, Test Linf Norm: 0.3224\n",
            "Epoch 83: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0408, Test L1 Norm: 0.0121, Train Linf Norm: 1.7145, Test Linf Norm: 0.3241\n",
            "Epoch 84: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0406, Test L1 Norm: 0.0120, Train Linf Norm: 1.7123, Test Linf Norm: 0.3198\n",
            "Epoch 85: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0411, Test L1 Norm: 0.0121, Train Linf Norm: 1.7294, Test Linf Norm: 0.3210\n",
            "Epoch 86: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0410, Test L1 Norm: 0.0120, Train Linf Norm: 1.7148, Test Linf Norm: 0.3201\n",
            "Epoch 87: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0408, Test L1 Norm: 0.0120, Train Linf Norm: 1.7187, Test Linf Norm: 0.3194\n",
            "Epoch 88: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0411, Test L1 Norm: 0.0120, Train Linf Norm: 1.7348, Test Linf Norm: 0.3192\n",
            "Epoch 89: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7227, Test Linf Norm: 0.3184\n",
            "Epoch 90: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7221, Test Linf Norm: 0.3192\n",
            "Epoch 91: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0407, Test L1 Norm: 0.0121, Train Linf Norm: 1.7163, Test Linf Norm: 0.3202\n",
            "Epoch 92: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0411, Test L1 Norm: 0.0120, Train Linf Norm: 1.7336, Test Linf Norm: 0.3186\n",
            "Epoch 93: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0407, Test L1 Norm: 0.0120, Train Linf Norm: 1.7178, Test Linf Norm: 0.3188\n",
            "Epoch 94: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0407, Test L1 Norm: 0.0120, Train Linf Norm: 1.7168, Test Linf Norm: 0.3186\n",
            "Epoch 95: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0408, Test L1 Norm: 0.0120, Train Linf Norm: 1.7129, Test Linf Norm: 0.3187\n",
            "Epoch 96: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0408, Test L1 Norm: 0.0120, Train Linf Norm: 1.7162, Test Linf Norm: 0.3188\n",
            "Epoch 97: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0407, Test L1 Norm: 0.0120, Train Linf Norm: 1.7176, Test Linf Norm: 0.3189\n",
            "Epoch 98: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0408, Test L1 Norm: 0.0120, Train Linf Norm: 1.7168, Test Linf Norm: 0.3189\n",
            "Epoch 99: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0407, Test L1 Norm: 0.0120, Train Linf Norm: 1.7040, Test Linf Norm: 0.3188\n",
            "Epoch 100: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0408, Test L1 Norm: 0.0120, Train Linf Norm: 1.7126, Test Linf Norm: 0.3188\n",
            "Epoch 101: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0408, Test L1 Norm: 0.0120, Train Linf Norm: 1.7137, Test Linf Norm: 0.3188\n",
            "Epoch 102: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0408, Test L1 Norm: 0.0120, Train Linf Norm: 1.7174, Test Linf Norm: 0.3188\n",
            "Epoch 103: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0408, Test L1 Norm: 0.0120, Train Linf Norm: 1.7227, Test Linf Norm: 0.3189\n",
            "Epoch 104: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0408, Test L1 Norm: 0.0120, Train Linf Norm: 1.7194, Test Linf Norm: 0.3189\n",
            "Epoch 105: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0408, Test L1 Norm: 0.0120, Train Linf Norm: 1.7238, Test Linf Norm: 0.3189\n",
            "Epoch 106: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0408, Test L1 Norm: 0.0120, Train Linf Norm: 1.7218, Test Linf Norm: 0.3190\n",
            "Epoch 107: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0408, Test L1 Norm: 0.0120, Train Linf Norm: 1.7185, Test Linf Norm: 0.3190\n",
            "Epoch 108: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0408, Test L1 Norm: 0.0120, Train Linf Norm: 1.7232, Test Linf Norm: 0.3190\n",
            "Epoch 109: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7224, Test Linf Norm: 0.3190\n",
            "Epoch 110: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7211, Test Linf Norm: 0.3190\n",
            "Epoch 111: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7226, Test Linf Norm: 0.3190\n",
            "Epoch 112: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7229, Test Linf Norm: 0.3190\n",
            "Epoch 113: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7250, Test Linf Norm: 0.3190\n",
            "Epoch 114: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7227, Test Linf Norm: 0.3190\n",
            "Epoch 115: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7185, Test Linf Norm: 0.3190\n",
            "Epoch 116: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7225, Test Linf Norm: 0.3190\n",
            "Epoch 117: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7187, Test Linf Norm: 0.3190\n",
            "Epoch 118: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7186, Test Linf Norm: 0.3190\n",
            "Epoch 119: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7195, Test Linf Norm: 0.3190\n",
            "Epoch 120: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7253, Test Linf Norm: 0.3190\n",
            "Epoch 121: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7186, Test Linf Norm: 0.3190\n",
            "Epoch 122: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7202, Test Linf Norm: 0.3190\n",
            "Epoch 123: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7244, Test Linf Norm: 0.3190\n",
            "Epoch 124: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7117, Test Linf Norm: 0.3190\n",
            "Epoch 125: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7111, Test Linf Norm: 0.3190\n",
            "Epoch 126: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7158, Test Linf Norm: 0.3190\n",
            "Epoch 127: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7224, Test Linf Norm: 0.3190\n",
            "Epoch 128: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7220, Test Linf Norm: 0.3190\n",
            "Epoch 129: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7250, Test Linf Norm: 0.3190\n",
            "Epoch 130: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7183, Test Linf Norm: 0.3190\n",
            "Epoch 131: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7200, Test Linf Norm: 0.3190\n",
            "Epoch 132: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7243, Test Linf Norm: 0.3190\n",
            "Epoch 133: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7226, Test Linf Norm: 0.3190\n",
            "Epoch 134: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7194, Test Linf Norm: 0.3190\n",
            "Epoch 135: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7202, Test Linf Norm: 0.3190\n",
            "Epoch 136: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7233, Test Linf Norm: 0.3190\n",
            "Epoch 137: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7227, Test Linf Norm: 0.3190\n",
            "Epoch 138: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7189, Test Linf Norm: 0.3190\n",
            "Epoch 139: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7204, Test Linf Norm: 0.3190\n",
            "Epoch 140: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7237, Test Linf Norm: 0.3190\n",
            "Epoch 141: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7241, Test Linf Norm: 0.3190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 17:17:13,084]\u001b[0m Trial 15 finished with value: 0.012029853112623095 and parameters: {'n_layers': 3, 'n_units_0': 1362, 'n_units_1': 351, 'n_units_2': 2005, 'hidden_activation': 'ReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adagrad', 'lr': 0.0008106842863430457, 'batch_size': 48, 'n_epochs': 142, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.11930344331358568, 'patience': 7, 'threshold': 0.0016046377935483712}. Best is trial 15 with value: 0.012029853112623095.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 142: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0409, Test L1 Norm: 0.0120, Train Linf Norm: 1.7229, Test Linf Norm: 0.3190\n",
            "Epoch 1: Train Loss: 0.2234, Test Loss: 0.0072, Train L1 Norm: 0.4709, Test L1 Norm: 0.0734, Train Linf Norm: 18.5457, Test Linf Norm: 1.9754\n",
            "Epoch 2: Train Loss: 0.0121, Test Loss: 0.0060, Train L1 Norm: 0.1352, Test L1 Norm: 0.0419, Train Linf Norm: 5.1867, Test Linf Norm: 0.8887\n",
            "Epoch 3: Train Loss: 0.0053, Test Loss: 0.0018, Train L1 Norm: 0.1003, Test L1 Norm: 0.0332, Train Linf Norm: 3.8278, Test Linf Norm: 0.7912\n",
            "Epoch 4: Train Loss: 0.0036, Test Loss: 0.0014, Train L1 Norm: 0.0763, Test L1 Norm: 0.0325, Train Linf Norm: 2.8100, Test Linf Norm: 0.8447\n",
            "Epoch 5: Train Loss: 0.0024, Test Loss: 0.0016, Train L1 Norm: 0.0718, Test L1 Norm: 0.0293, Train Linf Norm: 2.7091, Test Linf Norm: 0.7380\n",
            "Epoch 6: Train Loss: 0.0019, Test Loss: 0.0009, Train L1 Norm: 0.0717, Test L1 Norm: 0.0265, Train Linf Norm: 2.7707, Test Linf Norm: 0.6699\n",
            "Epoch 7: Train Loss: 0.0013, Test Loss: 0.0007, Train L1 Norm: 0.0403, Test L1 Norm: 0.0245, Train Linf Norm: 1.3011, Test Linf Norm: 0.6005\n",
            "Epoch 8: Train Loss: 0.0027, Test Loss: 0.0008, Train L1 Norm: 0.0459, Test L1 Norm: 0.0252, Train Linf Norm: 1.5290, Test Linf Norm: 0.6167\n",
            "Epoch 9: Train Loss: 0.0014, Test Loss: 0.0009, Train L1 Norm: 0.0576, Test L1 Norm: 0.0249, Train Linf Norm: 2.1822, Test Linf Norm: 0.6284\n",
            "Epoch 10: Train Loss: 0.0011, Test Loss: 0.0005, Train L1 Norm: 0.0495, Test L1 Norm: 0.0239, Train Linf Norm: 1.8038, Test Linf Norm: 0.6241\n",
            "Epoch 11: Train Loss: 0.0015, Test Loss: 0.0006, Train L1 Norm: 0.0387, Test L1 Norm: 0.0231, Train Linf Norm: 1.2816, Test Linf Norm: 0.5945\n",
            "Epoch 12: Train Loss: 0.0011, Test Loss: 0.0005, Train L1 Norm: 0.0368, Test L1 Norm: 0.0224, Train Linf Norm: 1.2215, Test Linf Norm: 0.5581\n",
            "Epoch 13: Train Loss: 0.0025, Test Loss: 0.0004, Train L1 Norm: 0.0513, Test L1 Norm: 0.0221, Train Linf Norm: 1.8865, Test Linf Norm: 0.5698\n",
            "Epoch 14: Train Loss: 0.0009, Test Loss: 0.0004, Train L1 Norm: 0.0347, Test L1 Norm: 0.0226, Train Linf Norm: 1.1620, Test Linf Norm: 0.5917\n",
            "Epoch 15: Train Loss: 0.0027, Test Loss: 0.0035, Train L1 Norm: 0.0621, Test L1 Norm: 0.0449, Train Linf Norm: 2.4178, Test Linf Norm: 1.0189\n",
            "Epoch 16: Train Loss: 0.0008, Test Loss: 0.0004, Train L1 Norm: 0.0358, Test L1 Norm: 0.0214, Train Linf Norm: 1.2208, Test Linf Norm: 0.5453\n",
            "Epoch 17: Train Loss: 0.0007, Test Loss: 0.0008, Train L1 Norm: 0.0394, Test L1 Norm: 0.0230, Train Linf Norm: 1.4145, Test Linf Norm: 0.5707\n",
            "Epoch 18: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.0474, Test L1 Norm: 0.0199, Train Linf Norm: 1.8263, Test Linf Norm: 0.5155\n",
            "Epoch 19: Train Loss: 0.0009, Test Loss: 0.0009, Train L1 Norm: 0.0436, Test L1 Norm: 0.0411, Train Linf Norm: 1.6294, Test Linf Norm: 1.1058\n",
            "Epoch 20: Train Loss: 0.0007, Test Loss: 0.0004, Train L1 Norm: 0.0423, Test L1 Norm: 0.0192, Train Linf Norm: 1.4923, Test Linf Norm: 0.4656\n",
            "Epoch 21: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.0453, Test L1 Norm: 0.0196, Train Linf Norm: 1.7370, Test Linf Norm: 0.5146\n",
            "Epoch 22: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.0386, Test L1 Norm: 0.0185, Train Linf Norm: 1.4288, Test Linf Norm: 0.4823\n",
            "Epoch 23: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.0414, Test L1 Norm: 0.0183, Train Linf Norm: 1.5712, Test Linf Norm: 0.4787\n",
            "Epoch 24: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.0404, Test L1 Norm: 0.0187, Train Linf Norm: 1.5235, Test Linf Norm: 0.4934\n",
            "Epoch 25: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.0382, Test L1 Norm: 0.0183, Train Linf Norm: 1.4077, Test Linf Norm: 0.4761\n",
            "Epoch 26: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0392, Test L1 Norm: 0.0176, Train Linf Norm: 1.4870, Test Linf Norm: 0.4564\n",
            "Epoch 27: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.0432, Test L1 Norm: 0.0175, Train Linf Norm: 1.6746, Test Linf Norm: 0.4554\n",
            "Epoch 28: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.0401, Test L1 Norm: 0.0179, Train Linf Norm: 1.5212, Test Linf Norm: 0.4782\n",
            "Epoch 29: Train Loss: 0.0005, Test Loss: 0.0002, Train L1 Norm: 0.0602, Test L1 Norm: 0.0171, Train Linf Norm: 2.4977, Test Linf Norm: 0.4520\n",
            "Epoch 30: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.0420, Test L1 Norm: 0.0179, Train Linf Norm: 1.6276, Test Linf Norm: 0.4768\n",
            "Epoch 31: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0436, Test L1 Norm: 0.0176, Train Linf Norm: 1.7071, Test Linf Norm: 0.4715\n",
            "Epoch 32: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0402, Test L1 Norm: 0.0173, Train Linf Norm: 1.5507, Test Linf Norm: 0.4470\n",
            "Epoch 33: Train Loss: 0.0006, Test Loss: 0.0003, Train L1 Norm: 0.0470, Test L1 Norm: 0.0172, Train Linf Norm: 1.8646, Test Linf Norm: 0.4527\n",
            "Epoch 34: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0436, Test L1 Norm: 0.0164, Train Linf Norm: 1.7190, Test Linf Norm: 0.4254\n",
            "Epoch 35: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.0438, Test L1 Norm: 0.0168, Train Linf Norm: 1.7305, Test Linf Norm: 0.4462\n",
            "Epoch 36: Train Loss: 0.0006, Test Loss: 0.0002, Train L1 Norm: 0.0511, Test L1 Norm: 0.0164, Train Linf Norm: 2.0690, Test Linf Norm: 0.4422\n",
            "Epoch 37: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0430, Test L1 Norm: 0.0162, Train Linf Norm: 1.6954, Test Linf Norm: 0.4357\n",
            "Epoch 38: Train Loss: 0.0003, Test Loss: 0.0005, Train L1 Norm: 0.0442, Test L1 Norm: 0.0163, Train Linf Norm: 1.7670, Test Linf Norm: 0.4186\n",
            "Epoch 39: Train Loss: 0.0002, Test Loss: 0.0004, Train L1 Norm: 0.0409, Test L1 Norm: 0.0189, Train Linf Norm: 1.6165, Test Linf Norm: 0.4608\n",
            "Epoch 40: Train Loss: 0.0011, Test Loss: 0.0002, Train L1 Norm: 0.0447, Test L1 Norm: 0.0164, Train Linf Norm: 1.7728, Test Linf Norm: 0.4498\n",
            "Epoch 41: Train Loss: 0.0008, Test Loss: 0.0002, Train L1 Norm: 0.0513, Test L1 Norm: 0.0160, Train Linf Norm: 2.0962, Test Linf Norm: 0.4336\n",
            "Epoch 42: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0451, Test L1 Norm: 0.0159, Train Linf Norm: 1.8155, Test Linf Norm: 0.4294\n",
            "Epoch 43: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0426, Test L1 Norm: 0.0156, Train Linf Norm: 1.7107, Test Linf Norm: 0.4232\n",
            "Epoch 44: Train Loss: 0.0003, Test Loss: 0.0004, Train L1 Norm: 0.0383, Test L1 Norm: 0.0193, Train Linf Norm: 1.5054, Test Linf Norm: 0.5207\n",
            "Epoch 45: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0442, Test L1 Norm: 0.0155, Train Linf Norm: 1.7735, Test Linf Norm: 0.4146\n",
            "Epoch 46: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0464, Test L1 Norm: 0.0153, Train Linf Norm: 1.8865, Test Linf Norm: 0.4098\n",
            "Epoch 47: Train Loss: 0.0030, Test Loss: 0.0011, Train L1 Norm: 0.0519, Test L1 Norm: 0.0295, Train Linf Norm: 2.0801, Test Linf Norm: 0.7368\n",
            "Epoch 48: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.0412, Test L1 Norm: 0.0160, Train Linf Norm: 1.6319, Test Linf Norm: 0.4175\n",
            "Epoch 49: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.0432, Test L1 Norm: 0.0165, Train Linf Norm: 1.7407, Test Linf Norm: 0.4402\n",
            "Epoch 50: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0446, Test L1 Norm: 0.0159, Train Linf Norm: 1.8135, Test Linf Norm: 0.4319\n",
            "Epoch 51: Train Loss: 0.0006, Test Loss: 0.0002, Train L1 Norm: 0.0408, Test L1 Norm: 0.0159, Train Linf Norm: 1.6143, Test Linf Norm: 0.4193\n",
            "Epoch 52: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0486, Test L1 Norm: 0.0152, Train Linf Norm: 2.0095, Test Linf Norm: 0.4069\n",
            "Epoch 53: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0492, Test L1 Norm: 0.0151, Train Linf Norm: 2.0405, Test Linf Norm: 0.4063\n",
            "Epoch 54: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0387, Test L1 Norm: 0.0148, Train Linf Norm: 1.5426, Test Linf Norm: 0.4046\n",
            "Epoch 55: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0422, Test L1 Norm: 0.0148, Train Linf Norm: 1.7044, Test Linf Norm: 0.4008\n",
            "Epoch 56: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0434, Test L1 Norm: 0.0145, Train Linf Norm: 1.7699, Test Linf Norm: 0.3786\n",
            "Epoch 57: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0403, Test L1 Norm: 0.0147, Train Linf Norm: 1.6281, Test Linf Norm: 0.3954\n",
            "Epoch 58: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0475, Test L1 Norm: 0.0146, Train Linf Norm: 1.8875, Test Linf Norm: 0.3986\n",
            "Epoch 59: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0440, Test L1 Norm: 0.0144, Train Linf Norm: 1.8000, Test Linf Norm: 0.3834\n",
            "Epoch 60: Train Loss: 0.0002, Test Loss: 0.0003, Train L1 Norm: 0.0427, Test L1 Norm: 0.0149, Train Linf Norm: 1.7401, Test Linf Norm: 0.4050\n",
            "Epoch 61: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0468, Test L1 Norm: 0.0144, Train Linf Norm: 1.9215, Test Linf Norm: 0.3936\n",
            "Epoch 62: Train Loss: 0.0005, Test Loss: 0.0002, Train L1 Norm: 0.0490, Test L1 Norm: 0.0153, Train Linf Norm: 2.0227, Test Linf Norm: 0.4228\n",
            "Epoch 63: Train Loss: 0.0002, Test Loss: 0.0004, Train L1 Norm: 0.0465, Test L1 Norm: 0.0158, Train Linf Norm: 1.9266, Test Linf Norm: 0.3975\n",
            "Epoch 64: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0468, Test L1 Norm: 0.0146, Train Linf Norm: 1.9416, Test Linf Norm: 0.3919\n",
            "Epoch 65: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0450, Test L1 Norm: 0.0142, Train Linf Norm: 1.8382, Test Linf Norm: 0.3898\n",
            "Epoch 66: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0734, Test L1 Norm: 0.0144, Train Linf Norm: 3.2161, Test Linf Norm: 0.3944\n",
            "Epoch 67: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0466, Test L1 Norm: 0.0152, Train Linf Norm: 1.9352, Test Linf Norm: 0.4227\n",
            "Epoch 68: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0453, Test L1 Norm: 0.0144, Train Linf Norm: 1.8816, Test Linf Norm: 0.3962\n",
            "Epoch 69: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0444, Test L1 Norm: 0.0144, Train Linf Norm: 1.8331, Test Linf Norm: 0.4019\n",
            "Epoch 70: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0499, Test L1 Norm: 0.0138, Train Linf Norm: 2.0960, Test Linf Norm: 0.3728\n",
            "Epoch 71: Train Loss: 0.0006, Test Loss: 0.0002, Train L1 Norm: 0.0506, Test L1 Norm: 0.0149, Train Linf Norm: 2.1091, Test Linf Norm: 0.4121\n",
            "Epoch 72: Train Loss: 0.0002, Test Loss: 0.0005, Train L1 Norm: 0.0446, Test L1 Norm: 0.0157, Train Linf Norm: 1.8521, Test Linf Norm: 0.4391\n",
            "Epoch 73: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0431, Test L1 Norm: 0.0139, Train Linf Norm: 1.7726, Test Linf Norm: 0.3821\n",
            "Epoch 74: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0456, Test L1 Norm: 0.0139, Train Linf Norm: 1.8942, Test Linf Norm: 0.3878\n",
            "Epoch 75: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0445, Test L1 Norm: 0.0139, Train Linf Norm: 1.8532, Test Linf Norm: 0.3890\n",
            "Epoch 76: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0440, Test L1 Norm: 0.0139, Train Linf Norm: 1.8411, Test Linf Norm: 0.3851\n",
            "Epoch 77: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0427, Test L1 Norm: 0.0138, Train Linf Norm: 1.7375, Test Linf Norm: 0.3824\n",
            "Epoch 78: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0431, Test L1 Norm: 0.0139, Train Linf Norm: 1.7957, Test Linf Norm: 0.3875\n",
            "Epoch 79: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0430, Test L1 Norm: 0.0139, Train Linf Norm: 1.7889, Test Linf Norm: 0.3876\n",
            "Epoch 80: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0446, Test L1 Norm: 0.0140, Train Linf Norm: 1.8721, Test Linf Norm: 0.3904\n",
            "Epoch 81: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0446, Test L1 Norm: 0.0138, Train Linf Norm: 1.8713, Test Linf Norm: 0.3841\n",
            "Epoch 82: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0430, Test L1 Norm: 0.0139, Train Linf Norm: 1.7968, Test Linf Norm: 0.3882\n",
            "Epoch 83: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0431, Test L1 Norm: 0.0139, Train Linf Norm: 1.7931, Test Linf Norm: 0.3894\n",
            "Epoch 84: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0440, Test L1 Norm: 0.0139, Train Linf Norm: 1.8359, Test Linf Norm: 0.3884\n",
            "Epoch 85: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0427, Test L1 Norm: 0.0139, Train Linf Norm: 1.7779, Test Linf Norm: 0.3867\n",
            "Epoch 86: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0440, Test L1 Norm: 0.0139, Train Linf Norm: 1.8385, Test Linf Norm: 0.3882\n",
            "Epoch 87: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0444, Test L1 Norm: 0.0139, Train Linf Norm: 1.8603, Test Linf Norm: 0.3874\n",
            "Epoch 88: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0429, Test L1 Norm: 0.0139, Train Linf Norm: 1.7836, Test Linf Norm: 0.3868\n",
            "Epoch 89: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0440, Test L1 Norm: 0.0139, Train Linf Norm: 1.8365, Test Linf Norm: 0.3875\n",
            "Epoch 90: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0430, Test L1 Norm: 0.0139, Train Linf Norm: 1.7931, Test Linf Norm: 0.3886\n",
            "Epoch 91: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0432, Test L1 Norm: 0.0139, Train Linf Norm: 1.8084, Test Linf Norm: 0.3875\n",
            "Epoch 92: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0434, Test L1 Norm: 0.0138, Train Linf Norm: 1.8138, Test Linf Norm: 0.3853\n",
            "Epoch 93: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0434, Test L1 Norm: 0.0138, Train Linf Norm: 1.8103, Test Linf Norm: 0.3873\n",
            "Epoch 94: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8316, Test Linf Norm: 0.3880\n",
            "Epoch 95: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0430, Test L1 Norm: 0.0138, Train Linf Norm: 1.7934, Test Linf Norm: 0.3851\n",
            "Epoch 96: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0438, Test L1 Norm: 0.0138, Train Linf Norm: 1.8210, Test Linf Norm: 0.3840\n",
            "Epoch 97: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0432, Test L1 Norm: 0.0138, Train Linf Norm: 1.8013, Test Linf Norm: 0.3870\n",
            "Epoch 98: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8370, Test Linf Norm: 0.3874\n",
            "Epoch 99: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0437, Test L1 Norm: 0.0138, Train Linf Norm: 1.8279, Test Linf Norm: 0.3849\n",
            "Epoch 100: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0431, Test L1 Norm: 0.0138, Train Linf Norm: 1.8025, Test Linf Norm: 0.3864\n",
            "Epoch 101: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0434, Test L1 Norm: 0.0138, Train Linf Norm: 1.8097, Test Linf Norm: 0.3849\n",
            "Epoch 102: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0431, Test L1 Norm: 0.0138, Train Linf Norm: 1.7978, Test Linf Norm: 0.3862\n",
            "Epoch 103: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0438, Test L1 Norm: 0.0138, Train Linf Norm: 1.8338, Test Linf Norm: 0.3868\n",
            "Epoch 104: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0436, Test L1 Norm: 0.0138, Train Linf Norm: 1.8124, Test Linf Norm: 0.3873\n",
            "Epoch 105: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0435, Test L1 Norm: 0.0138, Train Linf Norm: 1.8153, Test Linf Norm: 0.3842\n",
            "Epoch 106: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0445, Test L1 Norm: 0.0137, Train Linf Norm: 1.8648, Test Linf Norm: 0.3819\n",
            "Epoch 107: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0445, Test L1 Norm: 0.0138, Train Linf Norm: 1.8656, Test Linf Norm: 0.3841\n",
            "Epoch 108: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0440, Test L1 Norm: 0.0138, Train Linf Norm: 1.8329, Test Linf Norm: 0.3843\n",
            "Epoch 109: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0442, Test L1 Norm: 0.0138, Train Linf Norm: 1.8454, Test Linf Norm: 0.3847\n",
            "Epoch 110: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0442, Test L1 Norm: 0.0137, Train Linf Norm: 1.8573, Test Linf Norm: 0.3836\n",
            "Epoch 111: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8324, Test Linf Norm: 0.3845\n",
            "Epoch 112: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0438, Test L1 Norm: 0.0137, Train Linf Norm: 1.8378, Test Linf Norm: 0.3832\n",
            "Epoch 113: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0437, Test L1 Norm: 0.0138, Train Linf Norm: 1.8303, Test Linf Norm: 0.3849\n",
            "Epoch 114: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0438, Test L1 Norm: 0.0138, Train Linf Norm: 1.8296, Test Linf Norm: 0.3855\n",
            "Epoch 115: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8345, Test Linf Norm: 0.3849\n",
            "Epoch 116: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8337, Test Linf Norm: 0.3851\n",
            "Epoch 117: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8399, Test Linf Norm: 0.3851\n",
            "Epoch 118: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8375, Test Linf Norm: 0.3852\n",
            "Epoch 119: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8427, Test Linf Norm: 0.3853\n",
            "Epoch 120: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8345, Test Linf Norm: 0.3852\n",
            "Epoch 121: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8386, Test Linf Norm: 0.3852\n",
            "Epoch 122: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8321, Test Linf Norm: 0.3853\n",
            "Epoch 123: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8398, Test Linf Norm: 0.3853\n",
            "Epoch 124: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8377, Test Linf Norm: 0.3853\n",
            "Epoch 125: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8353, Test Linf Norm: 0.3853\n",
            "Epoch 126: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8239, Test Linf Norm: 0.3853\n",
            "Epoch 127: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8405, Test Linf Norm: 0.3853\n",
            "Epoch 128: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8307, Test Linf Norm: 0.3853\n",
            "Epoch 129: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8322, Test Linf Norm: 0.3853\n",
            "Epoch 130: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8337, Test Linf Norm: 0.3853\n",
            "Epoch 131: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8349, Test Linf Norm: 0.3853\n",
            "Epoch 132: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8378, Test Linf Norm: 0.3853\n",
            "Epoch 133: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8408, Test Linf Norm: 0.3853\n",
            "Epoch 134: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8385, Test Linf Norm: 0.3853\n",
            "Epoch 135: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8435, Test Linf Norm: 0.3853\n",
            "Epoch 136: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.7968, Test Linf Norm: 0.3853\n",
            "Epoch 137: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8412, Test Linf Norm: 0.3853\n",
            "Epoch 138: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8387, Test Linf Norm: 0.3853\n",
            "Epoch 139: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8370, Test Linf Norm: 0.3853\n",
            "Epoch 140: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8404, Test Linf Norm: 0.3853\n",
            "Epoch 141: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8378, Test Linf Norm: 0.3853\n",
            "Epoch 142: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8393, Test Linf Norm: 0.3853\n",
            "Epoch 143: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8349, Test Linf Norm: 0.3853\n",
            "Epoch 144: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8403, Test Linf Norm: 0.3853\n",
            "Epoch 145: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8414, Test Linf Norm: 0.3853\n",
            "Epoch 146: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8409, Test Linf Norm: 0.3853\n",
            "Epoch 147: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8311, Test Linf Norm: 0.3853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 17:29:07,948]\u001b[0m Trial 16 finished with value: 0.013775071762502193 and parameters: {'n_layers': 3, 'n_units_0': 1473, 'n_units_1': 411, 'n_units_2': 2033, 'hidden_activation': 'LeakyReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adagrad', 'lr': 0.0007818966551590938, 'batch_size': 48, 'n_epochs': 148, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.1030434455196717, 'patience': 7, 'threshold': 0.001349970552397691}. Best is trial 15 with value: 0.012029853112623095.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 148: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0439, Test L1 Norm: 0.0138, Train Linf Norm: 1.8370, Test Linf Norm: 0.3853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-e8f6ba3279cc>:146: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  t_max_fraction = trial.suggest_uniform('t_max_fraction', 0.1, 0.3)\n",
            "\u001b[32m[I 2023-05-21 17:29:13,977]\u001b[0m Trial 17 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.3537, Test Loss: 0.8034, Train L1 Norm: 0.3117, Test L1 Norm: 0.2869, Train Linf Norm: 10.5708, Test Linf Norm: 6.9467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 17:29:19,258]\u001b[0m Trial 18 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 7.3308, Test Loss: 0.0532, Train L1 Norm: 0.8228, Test L1 Norm: 0.2273, Train Linf Norm: 29.1991, Test Linf Norm: 6.0983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 17:29:21,580]\u001b[0m Trial 19 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 528.9861, Test Loss: 1.7041, Train L1 Norm: 4.5700, Test L1 Norm: 0.6001, Train Linf Norm: 436.6889, Test Linf Norm: 26.1516\n",
            "Epoch 1: Train Loss: 0.3351, Test Loss: 0.0114, Train L1 Norm: 0.3794, Test L1 Norm: 0.0623, Train Linf Norm: 13.1267, Test Linf Norm: 1.4344\n",
            "Epoch 2: Train Loss: 0.0092, Test Loss: 0.0042, Train L1 Norm: 0.0920, Test L1 Norm: 0.0360, Train Linf Norm: 3.1977, Test Linf Norm: 0.8066\n",
            "Epoch 3: Train Loss: 0.0053, Test Loss: 0.0011, Train L1 Norm: 0.0895, Test L1 Norm: 0.0291, Train Linf Norm: 3.3616, Test Linf Norm: 0.7325\n",
            "Epoch 4: Train Loss: 0.0045, Test Loss: 0.0011, Train L1 Norm: 0.0686, Test L1 Norm: 0.0244, Train Linf Norm: 2.5297, Test Linf Norm: 0.5871\n",
            "Epoch 5: Train Loss: 0.0025, Test Loss: 0.0021, Train L1 Norm: 0.0500, Test L1 Norm: 0.0247, Train Linf Norm: 1.7391, Test Linf Norm: 0.5651\n",
            "Epoch 6: Train Loss: 0.0022, Test Loss: 0.0005, Train L1 Norm: 0.0506, Test L1 Norm: 0.0262, Train Linf Norm: 1.7971, Test Linf Norm: 0.7307\n",
            "Epoch 7: Train Loss: 0.0018, Test Loss: 0.0007, Train L1 Norm: 0.0474, Test L1 Norm: 0.0207, Train Linf Norm: 1.7263, Test Linf Norm: 0.5248\n",
            "Epoch 8: Train Loss: 0.0019, Test Loss: 0.0007, Train L1 Norm: 0.0499, Test L1 Norm: 0.0207, Train Linf Norm: 1.8667, Test Linf Norm: 0.5252\n",
            "Epoch 9: Train Loss: 0.0009, Test Loss: 0.0004, Train L1 Norm: 0.0377, Test L1 Norm: 0.0213, Train Linf Norm: 1.3269, Test Linf Norm: 0.5951\n",
            "Epoch 10: Train Loss: 0.0016, Test Loss: 0.0004, Train L1 Norm: 0.0289, Test L1 Norm: 0.0237, Train Linf Norm: 0.9179, Test Linf Norm: 0.6979\n",
            "Epoch 11: Train Loss: 0.0006, Test Loss: 0.0003, Train L1 Norm: 0.0263, Test L1 Norm: 0.0172, Train Linf Norm: 0.8235, Test Linf Norm: 0.4234\n",
            "Epoch 12: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0273, Test L1 Norm: 0.0173, Train Linf Norm: 0.8995, Test Linf Norm: 0.4269\n",
            "Epoch 13: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.0308, Test L1 Norm: 0.0168, Train Linf Norm: 1.0618, Test Linf Norm: 0.4286\n",
            "Epoch 14: Train Loss: 0.0003, Test Loss: 0.0005, Train L1 Norm: 0.0281, Test L1 Norm: 0.0183, Train Linf Norm: 0.9744, Test Linf Norm: 0.4835\n",
            "Epoch 15: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0298, Test L1 Norm: 0.0164, Train Linf Norm: 1.0576, Test Linf Norm: 0.4246\n",
            "Epoch 16: Train Loss: 0.0003, Test Loss: 0.0053, Train L1 Norm: 0.0286, Test L1 Norm: 0.0254, Train Linf Norm: 1.0017, Test Linf Norm: 0.4817\n",
            "Epoch 17: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0225, Test L1 Norm: 0.0160, Train Linf Norm: 0.7242, Test Linf Norm: 0.4046\n",
            "Epoch 18: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0242, Test L1 Norm: 0.0159, Train Linf Norm: 0.8070, Test Linf Norm: 0.4062\n",
            "Epoch 19: Train Loss: 0.0002, Test Loss: 0.0009, Train L1 Norm: 0.0240, Test L1 Norm: 0.0167, Train Linf Norm: 0.7895, Test Linf Norm: 0.4002\n",
            "Epoch 20: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0239, Test L1 Norm: 0.0158, Train Linf Norm: 0.8020, Test Linf Norm: 0.4042\n",
            "Epoch 21: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0252, Test L1 Norm: 0.0158, Train Linf Norm: 0.8626, Test Linf Norm: 0.4044\n",
            "Epoch 22: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0236, Test L1 Norm: 0.0157, Train Linf Norm: 0.7868, Test Linf Norm: 0.4030\n",
            "Epoch 23: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0240, Test L1 Norm: 0.0157, Train Linf Norm: 0.8097, Test Linf Norm: 0.3977\n",
            "Epoch 24: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0245, Test L1 Norm: 0.0157, Train Linf Norm: 0.8354, Test Linf Norm: 0.4006\n",
            "Epoch 25: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0244, Test L1 Norm: 0.0157, Train Linf Norm: 0.8288, Test Linf Norm: 0.4031\n",
            "Epoch 26: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0244, Test L1 Norm: 0.0157, Train Linf Norm: 0.8346, Test Linf Norm: 0.4031\n",
            "Epoch 27: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0240, Test L1 Norm: 0.0157, Train Linf Norm: 0.8120, Test Linf Norm: 0.4012\n",
            "Epoch 28: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0244, Test L1 Norm: 0.0158, Train Linf Norm: 0.8310, Test Linf Norm: 0.4060\n",
            "Epoch 29: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0239, Test L1 Norm: 0.0158, Train Linf Norm: 0.7998, Test Linf Norm: 0.4059\n",
            "Epoch 30: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0232, Test L1 Norm: 0.0156, Train Linf Norm: 0.7652, Test Linf Norm: 0.4002\n",
            "Epoch 31: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0265, Test L1 Norm: 0.0157, Train Linf Norm: 0.9313, Test Linf Norm: 0.3987\n",
            "Epoch 32: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0243, Test L1 Norm: 0.0156, Train Linf Norm: 0.8253, Test Linf Norm: 0.3936\n",
            "Epoch 33: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0242, Test L1 Norm: 0.0154, Train Linf Norm: 0.8234, Test Linf Norm: 0.3925\n",
            "Epoch 34: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0265, Test L1 Norm: 0.0155, Train Linf Norm: 0.9307, Test Linf Norm: 0.3998\n",
            "Epoch 35: Train Loss: 0.0002, Test Loss: 0.0005, Train L1 Norm: 0.0216, Test L1 Norm: 0.0160, Train Linf Norm: 0.6969, Test Linf Norm: 0.4025\n",
            "Epoch 36: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0230, Test L1 Norm: 0.0154, Train Linf Norm: 0.7633, Test Linf Norm: 0.3977\n",
            "Epoch 37: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0268, Test L1 Norm: 0.0151, Train Linf Norm: 0.9516, Test Linf Norm: 0.3852\n",
            "Epoch 38: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0244, Test L1 Norm: 0.0150, Train Linf Norm: 0.8264, Test Linf Norm: 0.3843\n",
            "Epoch 39: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0263, Test L1 Norm: 0.0149, Train Linf Norm: 0.9242, Test Linf Norm: 0.3802\n",
            "Epoch 40: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0248, Test L1 Norm: 0.0149, Train Linf Norm: 0.8538, Test Linf Norm: 0.3848\n",
            "Epoch 41: Train Loss: 0.0011, Test Loss: 0.0003, Train L1 Norm: 0.0222, Test L1 Norm: 0.0166, Train Linf Norm: 0.6999, Test Linf Norm: 0.4178\n",
            "Epoch 42: Train Loss: 0.0005, Test Loss: 0.0002, Train L1 Norm: 0.0254, Test L1 Norm: 0.0147, Train Linf Norm: 0.8704, Test Linf Norm: 0.3761\n",
            "Epoch 43: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0237, Test L1 Norm: 0.0151, Train Linf Norm: 0.8108, Test Linf Norm: 0.3809\n",
            "Epoch 44: Train Loss: 0.0007, Test Loss: 0.0002, Train L1 Norm: 0.0281, Test L1 Norm: 0.0163, Train Linf Norm: 0.9959, Test Linf Norm: 0.4392\n",
            "Epoch 45: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0279, Test L1 Norm: 0.0157, Train Linf Norm: 1.0101, Test Linf Norm: 0.4332\n",
            "Epoch 46: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.0263, Test L1 Norm: 0.0144, Train Linf Norm: 0.9299, Test Linf Norm: 0.3744\n",
            "Epoch 47: Train Loss: 0.0012, Test Loss: 0.0003, Train L1 Norm: 0.0283, Test L1 Norm: 0.0155, Train Linf Norm: 0.9874, Test Linf Norm: 0.4000\n",
            "Epoch 48: Train Loss: 0.0007, Test Loss: 0.0002, Train L1 Norm: 0.0248, Test L1 Norm: 0.0149, Train Linf Norm: 0.8410, Test Linf Norm: 0.4102\n",
            "Epoch 49: Train Loss: 0.0006, Test Loss: 0.0008, Train L1 Norm: 0.0291, Test L1 Norm: 0.0144, Train Linf Norm: 1.0617, Test Linf Norm: 0.3712\n",
            "Epoch 50: Train Loss: 0.0005, Test Loss: 0.0002, Train L1 Norm: 0.0293, Test L1 Norm: 0.0148, Train Linf Norm: 1.0818, Test Linf Norm: 0.3882\n",
            "Epoch 51: Train Loss: 0.0006, Test Loss: 0.0085, Train L1 Norm: 0.0286, Test L1 Norm: 0.0426, Train Linf Norm: 1.0437, Test Linf Norm: 1.1544\n",
            "Epoch 52: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.0223, Test L1 Norm: 0.0152, Train Linf Norm: 0.7555, Test Linf Norm: 0.4098\n",
            "Epoch 53: Train Loss: 0.0010, Test Loss: 0.0002, Train L1 Norm: 0.0289, Test L1 Norm: 0.0144, Train Linf Norm: 1.0524, Test Linf Norm: 0.3743\n",
            "Epoch 54: Train Loss: 0.0013, Test Loss: 0.0002, Train L1 Norm: 0.0279, Test L1 Norm: 0.0158, Train Linf Norm: 0.9838, Test Linf Norm: 0.4379\n",
            "Epoch 55: Train Loss: 0.0006, Test Loss: 0.0002, Train L1 Norm: 0.0175, Test L1 Norm: 0.0137, Train Linf Norm: 0.5198, Test Linf Norm: 0.3622\n",
            "Epoch 56: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.0590, Test L1 Norm: 0.0131, Train Linf Norm: 2.5329, Test Linf Norm: 0.3513\n",
            "Epoch 57: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0206, Test L1 Norm: 0.0137, Train Linf Norm: 0.7009, Test Linf Norm: 0.3728\n",
            "Epoch 58: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0678, Test L1 Norm: 0.0135, Train Linf Norm: 2.9665, Test Linf Norm: 0.3604\n",
            "Epoch 59: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0179, Test L1 Norm: 0.0131, Train Linf Norm: 0.5857, Test Linf Norm: 0.3522\n",
            "Epoch 60: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0205, Test L1 Norm: 0.0145, Train Linf Norm: 0.7022, Test Linf Norm: 0.3818\n",
            "Epoch 61: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0188, Test L1 Norm: 0.0133, Train Linf Norm: 0.6350, Test Linf Norm: 0.3674\n",
            "Epoch 62: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0205, Test L1 Norm: 0.0133, Train Linf Norm: 0.7211, Test Linf Norm: 0.3542\n",
            "Epoch 63: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0151, Test L1 Norm: 0.0127, Train Linf Norm: 0.4622, Test Linf Norm: 0.3445\n",
            "Epoch 64: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0153, Test L1 Norm: 0.0127, Train Linf Norm: 0.4803, Test Linf Norm: 0.3437\n",
            "Epoch 65: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0154, Test L1 Norm: 0.0128, Train Linf Norm: 0.4748, Test Linf Norm: 0.3463\n",
            "Epoch 66: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0177, Test L1 Norm: 0.0126, Train Linf Norm: 0.5917, Test Linf Norm: 0.3430\n",
            "Epoch 67: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0165, Test L1 Norm: 0.0127, Train Linf Norm: 0.5401, Test Linf Norm: 0.3443\n",
            "Epoch 68: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0162, Test L1 Norm: 0.0127, Train Linf Norm: 0.5292, Test Linf Norm: 0.3450\n",
            "Epoch 69: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0181, Test L1 Norm: 0.0125, Train Linf Norm: 0.6142, Test Linf Norm: 0.3411\n",
            "Epoch 70: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0170, Test L1 Norm: 0.0125, Train Linf Norm: 0.5707, Test Linf Norm: 0.3399\n",
            "Epoch 71: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0156, Test L1 Norm: 0.0125, Train Linf Norm: 0.5044, Test Linf Norm: 0.3393\n",
            "Epoch 72: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0158, Test L1 Norm: 0.0125, Train Linf Norm: 0.5159, Test Linf Norm: 0.3393\n",
            "Epoch 73: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0153, Test L1 Norm: 0.0124, Train Linf Norm: 0.4869, Test Linf Norm: 0.3385\n",
            "Epoch 74: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0149, Test L1 Norm: 0.0124, Train Linf Norm: 0.4716, Test Linf Norm: 0.3391\n",
            "Epoch 75: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0152, Test L1 Norm: 0.0124, Train Linf Norm: 0.4860, Test Linf Norm: 0.3388\n",
            "Epoch 76: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0151, Test L1 Norm: 0.0124, Train Linf Norm: 0.4827, Test Linf Norm: 0.3388\n",
            "Epoch 77: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0152, Test L1 Norm: 0.0124, Train Linf Norm: 0.4863, Test Linf Norm: 0.3389\n",
            "Epoch 78: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0156, Test L1 Norm: 0.0124, Train Linf Norm: 0.5038, Test Linf Norm: 0.3386\n",
            "Epoch 79: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0153, Test L1 Norm: 0.0124, Train Linf Norm: 0.4848, Test Linf Norm: 0.3389\n",
            "Epoch 80: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0155, Test L1 Norm: 0.0125, Train Linf Norm: 0.4939, Test Linf Norm: 0.3397\n",
            "Epoch 81: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0162, Test L1 Norm: 0.0125, Train Linf Norm: 0.5318, Test Linf Norm: 0.3390\n",
            "Epoch 82: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0155, Test L1 Norm: 0.0124, Train Linf Norm: 0.4896, Test Linf Norm: 0.3389\n",
            "Epoch 83: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0163, Test L1 Norm: 0.0125, Train Linf Norm: 0.5298, Test Linf Norm: 0.3401\n",
            "Epoch 84: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0156, Test L1 Norm: 0.0123, Train Linf Norm: 0.4984, Test Linf Norm: 0.3347\n",
            "Epoch 85: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0157, Test L1 Norm: 0.0125, Train Linf Norm: 0.5055, Test Linf Norm: 0.3400\n",
            "Epoch 86: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0173, Test L1 Norm: 0.0124, Train Linf Norm: 0.5826, Test Linf Norm: 0.3364\n",
            "Epoch 87: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0190, Test L1 Norm: 0.0126, Train Linf Norm: 0.6616, Test Linf Norm: 0.3356\n",
            "Epoch 88: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0167, Test L1 Norm: 0.0124, Train Linf Norm: 0.5483, Test Linf Norm: 0.3404\n",
            "Epoch 89: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0190, Test L1 Norm: 0.0126, Train Linf Norm: 0.6608, Test Linf Norm: 0.3398\n",
            "Epoch 90: Train Loss: 0.0002, Test Loss: 0.0003, Train L1 Norm: 0.0166, Test L1 Norm: 0.0124, Train Linf Norm: 0.5502, Test Linf Norm: 0.3339\n",
            "Epoch 91: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0167, Test L1 Norm: 0.0123, Train Linf Norm: 0.5282, Test Linf Norm: 0.3384\n",
            "Epoch 92: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0160, Test L1 Norm: 0.0125, Train Linf Norm: 0.5189, Test Linf Norm: 0.3390\n",
            "Epoch 93: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0142, Test L1 Norm: 0.0120, Train Linf Norm: 0.4272, Test Linf Norm: 0.3261\n",
            "Epoch 94: Train Loss: 0.0009, Test Loss: 0.0002, Train L1 Norm: 0.0168, Test L1 Norm: 0.0122, Train Linf Norm: 0.5117, Test Linf Norm: 0.3320\n",
            "Epoch 95: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0172, Test L1 Norm: 0.0122, Train Linf Norm: 0.5562, Test Linf Norm: 0.3328\n",
            "Epoch 96: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0207, Test L1 Norm: 0.0122, Train Linf Norm: 0.7429, Test Linf Norm: 0.3334\n",
            "Epoch 97: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0236, Test L1 Norm: 0.0127, Train Linf Norm: 0.8700, Test Linf Norm: 0.3277\n",
            "Epoch 98: Train Loss: 0.0005, Test Loss: 0.0001, Train L1 Norm: 0.0201, Test L1 Norm: 0.0121, Train Linf Norm: 0.6909, Test Linf Norm: 0.3296\n",
            "Epoch 99: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0174, Test L1 Norm: 0.0119, Train Linf Norm: 0.5637, Test Linf Norm: 0.3239\n",
            "Epoch 100: Train Loss: 0.0074, Test Loss: 0.0006, Train L1 Norm: 0.2396, Test L1 Norm: 0.0171, Train Linf Norm: 10.9277, Test Linf Norm: 0.4820\n",
            "Epoch 101: Train Loss: 0.0009, Test Loss: 0.0003, Train L1 Norm: 0.0293, Test L1 Norm: 0.0151, Train Linf Norm: 1.0292, Test Linf Norm: 0.3965\n",
            "Epoch 102: Train Loss: 0.0007, Test Loss: 0.0002, Train L1 Norm: 0.0267, Test L1 Norm: 0.0132, Train Linf Norm: 0.9441, Test Linf Norm: 0.3448\n",
            "Epoch 103: Train Loss: 0.0005, Test Loss: 0.0002, Train L1 Norm: 0.0252, Test L1 Norm: 0.0129, Train Linf Norm: 0.8975, Test Linf Norm: 0.3371\n",
            "Epoch 104: Train Loss: 0.0012, Test Loss: 0.0003, Train L1 Norm: 0.0252, Test L1 Norm: 0.0126, Train Linf Norm: 0.8772, Test Linf Norm: 0.3242\n",
            "Epoch 105: Train Loss: 0.0006, Test Loss: 0.0007, Train L1 Norm: 0.0216, Test L1 Norm: 0.0173, Train Linf Norm: 0.7386, Test Linf Norm: 0.4561\n",
            "Epoch 106: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0199, Test L1 Norm: 0.0120, Train Linf Norm: 0.6787, Test Linf Norm: 0.3128\n",
            "Epoch 107: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0245, Test L1 Norm: 0.0125, Train Linf Norm: 0.9098, Test Linf Norm: 0.3137\n",
            "Epoch 108: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0204, Test L1 Norm: 0.0115, Train Linf Norm: 0.7153, Test Linf Norm: 0.3031\n",
            "Epoch 109: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.0257, Test L1 Norm: 0.0117, Train Linf Norm: 0.9694, Test Linf Norm: 0.3057\n",
            "Epoch 110: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0156, Test L1 Norm: 0.0112, Train Linf Norm: 0.4975, Test Linf Norm: 0.2974\n",
            "Epoch 111: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0198, Test L1 Norm: 0.0114, Train Linf Norm: 0.7037, Test Linf Norm: 0.3015\n",
            "Epoch 112: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0207, Test L1 Norm: 0.0113, Train Linf Norm: 0.7462, Test Linf Norm: 0.3006\n",
            "Epoch 113: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0229, Test L1 Norm: 0.0113, Train Linf Norm: 0.8665, Test Linf Norm: 0.2964\n",
            "Epoch 114: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0183, Test L1 Norm: 0.0113, Train Linf Norm: 0.6425, Test Linf Norm: 0.2951\n",
            "Epoch 115: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0201, Test L1 Norm: 0.0110, Train Linf Norm: 0.7335, Test Linf Norm: 0.2906\n",
            "Epoch 116: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0181, Test L1 Norm: 0.0111, Train Linf Norm: 0.6412, Test Linf Norm: 0.2940\n",
            "Epoch 117: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0154, Test L1 Norm: 0.0109, Train Linf Norm: 0.5095, Test Linf Norm: 0.2881\n",
            "Epoch 118: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0195, Test L1 Norm: 0.0107, Train Linf Norm: 0.7032, Test Linf Norm: 0.2851\n",
            "Epoch 119: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0181, Test L1 Norm: 0.0107, Train Linf Norm: 0.6394, Test Linf Norm: 0.2841\n",
            "Epoch 120: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0162, Test L1 Norm: 0.0108, Train Linf Norm: 0.5561, Test Linf Norm: 0.2844\n",
            "Epoch 121: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0182, Test L1 Norm: 0.0107, Train Linf Norm: 0.6500, Test Linf Norm: 0.2829\n",
            "Epoch 122: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0171, Test L1 Norm: 0.0107, Train Linf Norm: 0.5937, Test Linf Norm: 0.2842\n",
            "Epoch 123: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0175, Test L1 Norm: 0.0107, Train Linf Norm: 0.6191, Test Linf Norm: 0.2844\n",
            "Epoch 124: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0174, Test L1 Norm: 0.0107, Train Linf Norm: 0.6140, Test Linf Norm: 0.2848\n",
            "Epoch 125: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0178, Test L1 Norm: 0.0107, Train Linf Norm: 0.6277, Test Linf Norm: 0.2843\n",
            "Epoch 126: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0179, Test L1 Norm: 0.0107, Train Linf Norm: 0.6308, Test Linf Norm: 0.2843\n",
            "Epoch 127: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0178, Test L1 Norm: 0.0107, Train Linf Norm: 0.6222, Test Linf Norm: 0.2840\n",
            "Epoch 128: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0178, Test L1 Norm: 0.0107, Train Linf Norm: 0.6262, Test Linf Norm: 0.2839\n",
            "Epoch 129: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0176, Test L1 Norm: 0.0107, Train Linf Norm: 0.6190, Test Linf Norm: 0.2841\n",
            "Epoch 130: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0174, Test L1 Norm: 0.0107, Train Linf Norm: 0.6131, Test Linf Norm: 0.2827\n",
            "Epoch 131: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0168, Test L1 Norm: 0.0107, Train Linf Norm: 0.5811, Test Linf Norm: 0.2825\n",
            "Epoch 132: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0176, Test L1 Norm: 0.0107, Train Linf Norm: 0.6110, Test Linf Norm: 0.2821\n",
            "Epoch 133: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0166, Test L1 Norm: 0.0107, Train Linf Norm: 0.5735, Test Linf Norm: 0.2828\n",
            "Epoch 134: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0185, Test L1 Norm: 0.0108, Train Linf Norm: 0.6611, Test Linf Norm: 0.2856\n",
            "Epoch 135: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0171, Test L1 Norm: 0.0106, Train Linf Norm: 0.5955, Test Linf Norm: 0.2810\n",
            "Epoch 136: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0150, Test L1 Norm: 0.0106, Train Linf Norm: 0.4943, Test Linf Norm: 0.2805\n",
            "Epoch 137: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0151, Test L1 Norm: 0.0107, Train Linf Norm: 0.5009, Test Linf Norm: 0.2845\n",
            "Epoch 138: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0164, Test L1 Norm: 0.0111, Train Linf Norm: 0.5560, Test Linf Norm: 0.2858\n",
            "Epoch 139: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0165, Test L1 Norm: 0.0104, Train Linf Norm: 0.5596, Test Linf Norm: 0.2734\n",
            "Epoch 140: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0141, Test L1 Norm: 0.0105, Train Linf Norm: 0.4434, Test Linf Norm: 0.2739\n",
            "Epoch 141: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0171, Test L1 Norm: 0.0105, Train Linf Norm: 0.5866, Test Linf Norm: 0.2760\n",
            "Epoch 142: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0156, Test L1 Norm: 0.0104, Train Linf Norm: 0.5088, Test Linf Norm: 0.2758\n",
            "Epoch 143: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0141, Test L1 Norm: 0.0104, Train Linf Norm: 0.4538, Test Linf Norm: 0.2734\n",
            "Epoch 144: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0154, Test L1 Norm: 0.0105, Train Linf Norm: 0.5003, Test Linf Norm: 0.2766\n",
            "Epoch 145: Train Loss: 0.0002, Test Loss: 0.0019, Train L1 Norm: 0.0165, Test L1 Norm: 0.0553, Train Linf Norm: 0.5560, Test Linf Norm: 1.4028\n",
            "Epoch 146: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0151, Test L1 Norm: 0.0102, Train Linf Norm: 0.4888, Test Linf Norm: 0.2645\n",
            "Epoch 147: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0142, Test L1 Norm: 0.0105, Train Linf Norm: 0.4477, Test Linf Norm: 0.2713\n",
            "Epoch 148: Train Loss: 0.0017, Test Loss: 0.0003, Train L1 Norm: 0.0231, Test L1 Norm: 0.0112, Train Linf Norm: 0.8193, Test Linf Norm: 0.2909\n",
            "Epoch 149: Train Loss: 0.0009, Test Loss: 0.0001, Train L1 Norm: 0.0221, Test L1 Norm: 0.0104, Train Linf Norm: 0.7822, Test Linf Norm: 0.2723\n",
            "Epoch 150: Train Loss: 0.0007, Test Loss: 0.0006, Train L1 Norm: 0.0244, Test L1 Norm: 0.0117, Train Linf Norm: 0.9034, Test Linf Norm: 0.2722\n",
            "Epoch 151: Train Loss: 0.0007, Test Loss: 0.0002, Train L1 Norm: 0.0203, Test L1 Norm: 0.0105, Train Linf Norm: 0.7115, Test Linf Norm: 0.2695\n",
            "Epoch 152: Train Loss: 0.0007, Test Loss: 0.0002, Train L1 Norm: 0.0298, Test L1 Norm: 0.0109, Train Linf Norm: 1.1564, Test Linf Norm: 0.2777\n",
            "Epoch 153: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0163, Test L1 Norm: 0.0104, Train Linf Norm: 0.5414, Test Linf Norm: 0.2676\n",
            "Epoch 154: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.0147, Test L1 Norm: 0.0103, Train Linf Norm: 0.4587, Test Linf Norm: 0.2640\n",
            "Epoch 155: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0181, Test L1 Norm: 0.0101, Train Linf Norm: 0.6295, Test Linf Norm: 0.2616\n",
            "Epoch 156: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0213, Test L1 Norm: 0.0102, Train Linf Norm: 0.7791, Test Linf Norm: 0.2663\n",
            "Epoch 157: Train Loss: 0.0002, Test Loss: 0.0010, Train L1 Norm: 0.0178, Test L1 Norm: 0.0159, Train Linf Norm: 0.6288, Test Linf Norm: 0.3750\n",
            "Epoch 158: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0160, Test L1 Norm: 0.0103, Train Linf Norm: 0.5487, Test Linf Norm: 0.2698\n",
            "Epoch 159: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0157, Test L1 Norm: 0.0099, Train Linf Norm: 0.5393, Test Linf Norm: 0.2538\n",
            "Epoch 160: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0202, Test L1 Norm: 0.0099, Train Linf Norm: 0.7543, Test Linf Norm: 0.2569\n",
            "Epoch 161: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0141, Test L1 Norm: 0.0100, Train Linf Norm: 0.4669, Test Linf Norm: 0.2598\n",
            "Epoch 162: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0164, Test L1 Norm: 0.0098, Train Linf Norm: 0.5737, Test Linf Norm: 0.2541\n",
            "Epoch 163: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0187, Test L1 Norm: 0.0094, Train Linf Norm: 0.6905, Test Linf Norm: 0.2440\n",
            "Epoch 164: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0176, Test L1 Norm: 0.0095, Train Linf Norm: 0.6428, Test Linf Norm: 0.2463\n",
            "Epoch 165: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0156, Test L1 Norm: 0.0098, Train Linf Norm: 0.5431, Test Linf Norm: 0.2549\n",
            "Epoch 166: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0166, Test L1 Norm: 0.0096, Train Linf Norm: 0.5943, Test Linf Norm: 0.2480\n",
            "Epoch 167: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0162, Test L1 Norm: 0.0095, Train Linf Norm: 0.5750, Test Linf Norm: 0.2456\n",
            "Epoch 168: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0145, Test L1 Norm: 0.0094, Train Linf Norm: 0.4904, Test Linf Norm: 0.2452\n",
            "Epoch 169: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0148, Test L1 Norm: 0.0094, Train Linf Norm: 0.5123, Test Linf Norm: 0.2434\n",
            "Epoch 170: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0150, Test L1 Norm: 0.0094, Train Linf Norm: 0.5201, Test Linf Norm: 0.2444\n",
            "Epoch 171: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0152, Test L1 Norm: 0.0094, Train Linf Norm: 0.5324, Test Linf Norm: 0.2446\n",
            "Epoch 172: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0150, Test L1 Norm: 0.0094, Train Linf Norm: 0.5242, Test Linf Norm: 0.2446\n",
            "Epoch 173: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0147, Test L1 Norm: 0.0094, Train Linf Norm: 0.5100, Test Linf Norm: 0.2440\n",
            "Epoch 174: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0151, Test L1 Norm: 0.0094, Train Linf Norm: 0.5275, Test Linf Norm: 0.2438\n",
            "Epoch 175: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0149, Test L1 Norm: 0.0094, Train Linf Norm: 0.5200, Test Linf Norm: 0.2437\n",
            "Epoch 176: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0147, Test L1 Norm: 0.0094, Train Linf Norm: 0.5046, Test Linf Norm: 0.2437\n",
            "Epoch 177: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0148, Test L1 Norm: 0.0094, Train Linf Norm: 0.5152, Test Linf Norm: 0.2437\n",
            "Epoch 178: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0150, Test L1 Norm: 0.0094, Train Linf Norm: 0.5217, Test Linf Norm: 0.2436\n",
            "Epoch 179: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0147, Test L1 Norm: 0.0094, Train Linf Norm: 0.5095, Test Linf Norm: 0.2434\n",
            "Epoch 180: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0148, Test L1 Norm: 0.0094, Train Linf Norm: 0.5120, Test Linf Norm: 0.2435\n",
            "Epoch 181: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0148, Test L1 Norm: 0.0094, Train Linf Norm: 0.5141, Test Linf Norm: 0.2440\n",
            "Epoch 182: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0153, Test L1 Norm: 0.0094, Train Linf Norm: 0.5327, Test Linf Norm: 0.2437\n",
            "Epoch 183: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0163, Test L1 Norm: 0.0094, Train Linf Norm: 0.5807, Test Linf Norm: 0.2436\n",
            "Epoch 184: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0147, Test L1 Norm: 0.0095, Train Linf Norm: 0.5083, Test Linf Norm: 0.2465\n",
            "Epoch 185: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0132, Test L1 Norm: 0.0094, Train Linf Norm: 0.4356, Test Linf Norm: 0.2437\n",
            "Epoch 186: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0128, Test L1 Norm: 0.0095, Train Linf Norm: 0.4125, Test Linf Norm: 0.2453\n",
            "Epoch 187: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0169, Test L1 Norm: 0.0094, Train Linf Norm: 0.6087, Test Linf Norm: 0.2433\n",
            "Epoch 188: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0151, Test L1 Norm: 0.0093, Train Linf Norm: 0.5226, Test Linf Norm: 0.2418\n",
            "Epoch 189: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0128, Test L1 Norm: 0.0094, Train Linf Norm: 0.4161, Test Linf Norm: 0.2430\n",
            "Epoch 190: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0142, Test L1 Norm: 0.0094, Train Linf Norm: 0.4828, Test Linf Norm: 0.2434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 17:55:19,516]\u001b[0m Trial 20 finished with value: 0.009307051534205675 and parameters: {'n_layers': 5, 'n_units_0': 1790, 'n_units_1': 1093, 'n_units_2': 1795, 'n_units_3': 1456, 'n_units_4': 118, 'hidden_activation': 'LeakyReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adagrad', 'lr': 0.0009234706910236727, 'batch_size': 48, 'n_epochs': 191, 'scheduler': 'CosineAnnealingLR', 't_max_fraction': 0.13437525790218866, 'eta_min': 1.0238006774822311e-07}. Best is trial 20 with value: 0.009307051534205675.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 191: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0132, Test L1 Norm: 0.0093, Train Linf Norm: 0.4286, Test Linf Norm: 0.2401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 17:55:28,568]\u001b[0m Trial 21 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.3412, Test Loss: 0.2021, Train L1 Norm: 0.9893, Test L1 Norm: 0.2825, Train Linf Norm: 40.9074, Test Linf Norm: 8.8176\n",
            "Epoch 1: Train Loss: 0.3507, Test Loss: 0.0054, Train L1 Norm: 0.5275, Test L1 Norm: 0.1193, Train Linf Norm: 20.2205, Test Linf Norm: 3.7571\n",
            "Epoch 2: Train Loss: 0.0098, Test Loss: 0.0020, Train L1 Norm: 0.3025, Test L1 Norm: 0.0828, Train Linf Norm: 12.8775, Test Linf Norm: 2.8244\n",
            "Epoch 3: Train Loss: 0.0065, Test Loss: 0.0011, Train L1 Norm: 0.1728, Test L1 Norm: 0.0517, Train Linf Norm: 7.2091, Test Linf Norm: 1.6385\n",
            "Epoch 4: Train Loss: 0.0037, Test Loss: 0.0008, Train L1 Norm: 0.1246, Test L1 Norm: 0.0427, Train Linf Norm: 5.1061, Test Linf Norm: 1.3305\n",
            "Epoch 5: Train Loss: 0.0012, Test Loss: 0.0005, Train L1 Norm: 0.0921, Test L1 Norm: 0.0345, Train Linf Norm: 3.7112, Test Linf Norm: 1.0464\n",
            "Epoch 6: Train Loss: 0.0066, Test Loss: 0.0009, Train L1 Norm: 0.0920, Test L1 Norm: 0.0367, Train Linf Norm: 3.6901, Test Linf Norm: 1.1531\n",
            "Epoch 7: Train Loss: 0.0011, Test Loss: 0.0004, Train L1 Norm: 0.0970, Test L1 Norm: 0.0286, Train Linf Norm: 4.0472, Test Linf Norm: 0.8356\n",
            "Epoch 8: Train Loss: 0.0013, Test Loss: 0.0003, Train L1 Norm: 0.0851, Test L1 Norm: 0.0290, Train Linf Norm: 3.5159, Test Linf Norm: 0.8814\n",
            "Epoch 9: Train Loss: 0.0008, Test Loss: 0.0004, Train L1 Norm: 0.0756, Test L1 Norm: 0.0248, Train Linf Norm: 3.1023, Test Linf Norm: 0.7071\n",
            "Epoch 10: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.0746, Test L1 Norm: 0.0266, Train Linf Norm: 3.1003, Test Linf Norm: 0.8029\n",
            "Epoch 11: Train Loss: 0.0003, Test Loss: 0.0004, Train L1 Norm: 0.0706, Test L1 Norm: 0.0227, Train Linf Norm: 2.9265, Test Linf Norm: 0.6384\n",
            "Epoch 12: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0600, Test L1 Norm: 0.0234, Train Linf Norm: 2.4282, Test Linf Norm: 0.6775\n",
            "Epoch 13: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0638, Test L1 Norm: 0.0207, Train Linf Norm: 2.6275, Test Linf Norm: 0.5759\n",
            "Epoch 14: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0602, Test L1 Norm: 0.0232, Train Linf Norm: 2.4650, Test Linf Norm: 0.6878\n",
            "Epoch 15: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0625, Test L1 Norm: 0.0200, Train Linf Norm: 2.5815, Test Linf Norm: 0.5574\n",
            "Epoch 16: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0570, Test L1 Norm: 0.0208, Train Linf Norm: 2.3359, Test Linf Norm: 0.5993\n",
            "Epoch 17: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0593, Test L1 Norm: 0.0206, Train Linf Norm: 2.4467, Test Linf Norm: 0.5874\n",
            "Epoch 18: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0574, Test L1 Norm: 0.0194, Train Linf Norm: 2.3651, Test Linf Norm: 0.5370\n",
            "Epoch 19: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0549, Test L1 Norm: 0.0203, Train Linf Norm: 2.2381, Test Linf Norm: 0.5861\n",
            "Epoch 20: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0567, Test L1 Norm: 0.0193, Train Linf Norm: 2.3366, Test Linf Norm: 0.5446\n",
            "Epoch 21: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0557, Test L1 Norm: 0.0192, Train Linf Norm: 2.2981, Test Linf Norm: 0.5430\n",
            "Epoch 22: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0543, Test L1 Norm: 0.0190, Train Linf Norm: 2.2304, Test Linf Norm: 0.5348\n",
            "Epoch 23: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0540, Test L1 Norm: 0.0191, Train Linf Norm: 2.2103, Test Linf Norm: 0.5437\n",
            "Epoch 24: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0551, Test L1 Norm: 0.0188, Train Linf Norm: 2.2656, Test Linf Norm: 0.5271\n",
            "Epoch 25: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0543, Test L1 Norm: 0.0191, Train Linf Norm: 2.2295, Test Linf Norm: 0.5415\n",
            "Epoch 26: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0546, Test L1 Norm: 0.0192, Train Linf Norm: 2.2493, Test Linf Norm: 0.5441\n",
            "Epoch 27: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0540, Test L1 Norm: 0.0189, Train Linf Norm: 2.2223, Test Linf Norm: 0.5368\n",
            "Epoch 28: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0541, Test L1 Norm: 0.0188, Train Linf Norm: 2.2309, Test Linf Norm: 0.5289\n",
            "Epoch 29: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0538, Test L1 Norm: 0.0190, Train Linf Norm: 2.2122, Test Linf Norm: 0.5379\n",
            "Epoch 30: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0541, Test L1 Norm: 0.0189, Train Linf Norm: 2.2263, Test Linf Norm: 0.5355\n",
            "Epoch 31: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0539, Test L1 Norm: 0.0189, Train Linf Norm: 2.2057, Test Linf Norm: 0.5352\n",
            "Epoch 32: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0538, Test L1 Norm: 0.0189, Train Linf Norm: 2.2118, Test Linf Norm: 0.5338\n",
            "Epoch 33: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0538, Test L1 Norm: 0.0189, Train Linf Norm: 2.2098, Test Linf Norm: 0.5351\n",
            "Epoch 34: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0538, Test L1 Norm: 0.0190, Train Linf Norm: 2.2108, Test Linf Norm: 0.5410\n",
            "Epoch 35: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0540, Test L1 Norm: 0.0189, Train Linf Norm: 2.2234, Test Linf Norm: 0.5372\n",
            "Epoch 36: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0540, Test L1 Norm: 0.0189, Train Linf Norm: 2.2182, Test Linf Norm: 0.5343\n",
            "Epoch 37: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0531, Test L1 Norm: 0.0189, Train Linf Norm: 2.1828, Test Linf Norm: 0.5370\n",
            "Epoch 38: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0539, Test L1 Norm: 0.0189, Train Linf Norm: 2.2200, Test Linf Norm: 0.5385\n",
            "Epoch 39: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0528, Test L1 Norm: 0.0184, Train Linf Norm: 2.1677, Test Linf Norm: 0.5178\n",
            "Epoch 40: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0522, Test L1 Norm: 0.0185, Train Linf Norm: 2.1412, Test Linf Norm: 0.5201\n",
            "Epoch 41: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0534, Test L1 Norm: 0.0183, Train Linf Norm: 2.1971, Test Linf Norm: 0.5143\n",
            "Epoch 42: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0522, Test L1 Norm: 0.0182, Train Linf Norm: 2.1360, Test Linf Norm: 0.5121\n",
            "Epoch 43: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0524, Test L1 Norm: 0.0188, Train Linf Norm: 2.1474, Test Linf Norm: 0.5420\n",
            "Epoch 44: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0534, Test L1 Norm: 0.0180, Train Linf Norm: 2.2045, Test Linf Norm: 0.5062\n",
            "Epoch 45: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0518, Test L1 Norm: 0.0172, Train Linf Norm: 2.1215, Test Linf Norm: 0.4715\n",
            "Epoch 46: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0519, Test L1 Norm: 0.0181, Train Linf Norm: 2.1362, Test Linf Norm: 0.5171\n",
            "Epoch 47: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0509, Test L1 Norm: 0.0180, Train Linf Norm: 2.0926, Test Linf Norm: 0.5135\n",
            "Epoch 48: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0485, Test L1 Norm: 0.0174, Train Linf Norm: 1.9722, Test Linf Norm: 0.4896\n",
            "Epoch 49: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0482, Test L1 Norm: 0.0159, Train Linf Norm: 1.9517, Test Linf Norm: 0.4277\n",
            "Epoch 50: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0466, Test L1 Norm: 0.0178, Train Linf Norm: 1.8948, Test Linf Norm: 0.5133\n",
            "Epoch 51: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0505, Test L1 Norm: 0.0167, Train Linf Norm: 2.0775, Test Linf Norm: 0.4675\n",
            "Epoch 52: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0469, Test L1 Norm: 0.0164, Train Linf Norm: 1.9151, Test Linf Norm: 0.4571\n",
            "Epoch 53: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0469, Test L1 Norm: 0.0169, Train Linf Norm: 1.8984, Test Linf Norm: 0.4824\n",
            "Epoch 54: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0438, Test L1 Norm: 0.0150, Train Linf Norm: 1.7652, Test Linf Norm: 0.3983\n",
            "Epoch 55: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.0450, Test L1 Norm: 0.0162, Train Linf Norm: 1.8199, Test Linf Norm: 0.4526\n",
            "Epoch 56: Train Loss: 0.0011, Test Loss: 0.0008, Train L1 Norm: 0.0411, Test L1 Norm: 0.0153, Train Linf Norm: 1.6090, Test Linf Norm: 0.3973\n",
            "Epoch 57: Train Loss: 0.0013, Test Loss: 0.0002, Train L1 Norm: 0.0530, Test L1 Norm: 0.0159, Train Linf Norm: 2.1406, Test Linf Norm: 0.4444\n",
            "Epoch 58: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0448, Test L1 Norm: 0.0155, Train Linf Norm: 1.8254, Test Linf Norm: 0.4307\n",
            "Epoch 59: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0391, Test L1 Norm: 0.0149, Train Linf Norm: 1.5603, Test Linf Norm: 0.4135\n",
            "Epoch 60: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0393, Test L1 Norm: 0.0141, Train Linf Norm: 1.5773, Test Linf Norm: 0.3740\n",
            "Epoch 61: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0375, Test L1 Norm: 0.0153, Train Linf Norm: 1.4917, Test Linf Norm: 0.4339\n",
            "Epoch 62: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0429, Test L1 Norm: 0.0142, Train Linf Norm: 1.7431, Test Linf Norm: 0.3882\n",
            "Epoch 63: Train Loss: 0.0002, Test Loss: 0.0007, Train L1 Norm: 0.0304, Test L1 Norm: 0.0165, Train Linf Norm: 1.1512, Test Linf Norm: 0.4172\n",
            "Epoch 64: Train Loss: 0.0005, Test Loss: 0.0001, Train L1 Norm: 0.0363, Test L1 Norm: 0.0157, Train Linf Norm: 1.4213, Test Linf Norm: 0.4576\n",
            "Epoch 65: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0245, Test L1 Norm: 0.0146, Train Linf Norm: 0.8742, Test Linf Norm: 0.4056\n",
            "Epoch 66: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0368, Test L1 Norm: 0.0142, Train Linf Norm: 1.4670, Test Linf Norm: 0.4027\n",
            "Epoch 67: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0371, Test L1 Norm: 0.0141, Train Linf Norm: 1.4729, Test Linf Norm: 0.3810\n",
            "Epoch 68: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0328, Test L1 Norm: 0.0147, Train Linf Norm: 1.2801, Test Linf Norm: 0.4251\n",
            "Epoch 69: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0350, Test L1 Norm: 0.0145, Train Linf Norm: 1.3948, Test Linf Norm: 0.4160\n",
            "Epoch 70: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0360, Test L1 Norm: 0.0140, Train Linf Norm: 1.4467, Test Linf Norm: 0.3931\n",
            "Epoch 71: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0353, Test L1 Norm: 0.0131, Train Linf Norm: 1.4163, Test Linf Norm: 0.3565\n",
            "Epoch 72: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0349, Test L1 Norm: 0.0137, Train Linf Norm: 1.3941, Test Linf Norm: 0.3819\n",
            "Epoch 73: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0353, Test L1 Norm: 0.0134, Train Linf Norm: 1.4202, Test Linf Norm: 0.3728\n",
            "Epoch 74: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0137, Train Linf Norm: 1.3477, Test Linf Norm: 0.3864\n",
            "Epoch 75: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0316, Test L1 Norm: 0.0135, Train Linf Norm: 1.2307, Test Linf Norm: 0.3793\n",
            "Epoch 76: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0309, Test L1 Norm: 0.0133, Train Linf Norm: 1.2162, Test Linf Norm: 0.3692\n",
            "Epoch 77: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0311, Test L1 Norm: 0.0131, Train Linf Norm: 1.2262, Test Linf Norm: 0.3628\n",
            "Epoch 78: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0325, Test L1 Norm: 0.0123, Train Linf Norm: 1.2975, Test Linf Norm: 0.3268\n",
            "Epoch 79: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0326, Test L1 Norm: 0.0129, Train Linf Norm: 1.3052, Test Linf Norm: 0.3557\n",
            "Epoch 80: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0319, Test L1 Norm: 0.0129, Train Linf Norm: 1.2743, Test Linf Norm: 0.3574\n",
            "Epoch 81: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0314, Test L1 Norm: 0.0125, Train Linf Norm: 1.2475, Test Linf Norm: 0.3396\n",
            "Epoch 82: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0316, Test L1 Norm: 0.0129, Train Linf Norm: 1.2623, Test Linf Norm: 0.3564\n",
            "Epoch 83: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0318, Test L1 Norm: 0.0130, Train Linf Norm: 1.2648, Test Linf Norm: 0.3631\n",
            "Epoch 84: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0322, Test L1 Norm: 0.0128, Train Linf Norm: 1.2877, Test Linf Norm: 0.3559\n",
            "Epoch 85: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0317, Test L1 Norm: 0.0128, Train Linf Norm: 1.2626, Test Linf Norm: 0.3547\n",
            "Epoch 86: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0315, Test L1 Norm: 0.0126, Train Linf Norm: 1.2550, Test Linf Norm: 0.3454\n",
            "Epoch 87: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0305, Test L1 Norm: 0.0128, Train Linf Norm: 1.2020, Test Linf Norm: 0.3540\n",
            "Epoch 88: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0313, Test L1 Norm: 0.0127, Train Linf Norm: 1.2384, Test Linf Norm: 0.3519\n",
            "Epoch 89: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0312, Test L1 Norm: 0.0127, Train Linf Norm: 1.2455, Test Linf Norm: 0.3497\n",
            "Epoch 90: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0312, Test L1 Norm: 0.0127, Train Linf Norm: 1.2422, Test Linf Norm: 0.3511\n",
            "Epoch 91: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0313, Test L1 Norm: 0.0127, Train Linf Norm: 1.2433, Test Linf Norm: 0.3510\n",
            "Epoch 92: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0312, Test L1 Norm: 0.0127, Train Linf Norm: 1.2425, Test Linf Norm: 0.3506\n",
            "Epoch 93: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0312, Test L1 Norm: 0.0127, Train Linf Norm: 1.2377, Test Linf Norm: 0.3515\n",
            "Epoch 94: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0312, Test L1 Norm: 0.0127, Train Linf Norm: 1.2419, Test Linf Norm: 0.3504\n",
            "Epoch 95: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0308, Test L1 Norm: 0.0128, Train Linf Norm: 1.1633, Test Linf Norm: 0.3531\n",
            "Epoch 96: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0312, Test L1 Norm: 0.0128, Train Linf Norm: 1.2396, Test Linf Norm: 0.3566\n",
            "Epoch 97: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0302, Test L1 Norm: 0.0128, Train Linf Norm: 1.1951, Test Linf Norm: 0.3535\n",
            "Epoch 98: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0314, Test L1 Norm: 0.0127, Train Linf Norm: 1.2457, Test Linf Norm: 0.3486\n",
            "Epoch 99: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0322, Test L1 Norm: 0.0126, Train Linf Norm: 1.2882, Test Linf Norm: 0.3461\n",
            "Epoch 100: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0308, Test L1 Norm: 0.0130, Train Linf Norm: 1.2258, Test Linf Norm: 0.3643\n",
            "Epoch 101: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0302, Test L1 Norm: 0.0129, Train Linf Norm: 1.1933, Test Linf Norm: 0.3588\n",
            "Epoch 102: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0306, Test L1 Norm: 0.0125, Train Linf Norm: 1.2130, Test Linf Norm: 0.3417\n",
            "Epoch 103: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0323, Test L1 Norm: 0.0125, Train Linf Norm: 1.2939, Test Linf Norm: 0.3441\n",
            "Epoch 104: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0308, Test L1 Norm: 0.0128, Train Linf Norm: 1.2273, Test Linf Norm: 0.3523\n",
            "Epoch 105: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0313, Test L1 Norm: 0.0125, Train Linf Norm: 1.2469, Test Linf Norm: 0.3444\n",
            "Epoch 106: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0277, Test L1 Norm: 0.0129, Train Linf Norm: 1.0722, Test Linf Norm: 0.3652\n",
            "Epoch 107: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0303, Test L1 Norm: 0.0120, Train Linf Norm: 1.2031, Test Linf Norm: 0.3230\n",
            "Epoch 108: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0274, Test L1 Norm: 0.0125, Train Linf Norm: 1.0547, Test Linf Norm: 0.3460\n",
            "Epoch 109: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0289, Test L1 Norm: 0.0123, Train Linf Norm: 1.1334, Test Linf Norm: 0.3372\n",
            "Epoch 110: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0294, Test L1 Norm: 0.0125, Train Linf Norm: 1.1580, Test Linf Norm: 0.3461\n",
            "Epoch 111: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0282, Test L1 Norm: 0.0123, Train Linf Norm: 1.0971, Test Linf Norm: 0.3412\n",
            "Epoch 112: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0260, Test L1 Norm: 0.0120, Train Linf Norm: 1.0020, Test Linf Norm: 0.3250\n",
            "Epoch 113: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0276, Test L1 Norm: 0.0120, Train Linf Norm: 1.0721, Test Linf Norm: 0.2948\n",
            "Epoch 114: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0119, Train Linf Norm: 1.0509, Test Linf Norm: 0.3149\n",
            "Epoch 115: Train Loss: 0.0002, Test Loss: 0.0023, Train L1 Norm: 0.0283, Test L1 Norm: 0.0136, Train Linf Norm: 1.1049, Test Linf Norm: 0.3060\n",
            "Epoch 116: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0284, Test L1 Norm: 0.0140, Train Linf Norm: 1.0818, Test Linf Norm: 0.3934\n",
            "Epoch 117: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0279, Test L1 Norm: 0.0113, Train Linf Norm: 1.0888, Test Linf Norm: 0.2949\n",
            "Epoch 118: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0265, Test L1 Norm: 0.0122, Train Linf Norm: 1.0201, Test Linf Norm: 0.3286\n",
            "Epoch 119: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0261, Test L1 Norm: 0.0115, Train Linf Norm: 0.9903, Test Linf Norm: 0.3071\n",
            "Epoch 120: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0238, Test L1 Norm: 0.0114, Train Linf Norm: 0.8909, Test Linf Norm: 0.3104\n",
            "Epoch 121: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0257, Test L1 Norm: 0.0115, Train Linf Norm: 0.9838, Test Linf Norm: 0.3167\n",
            "Epoch 122: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0251, Test L1 Norm: 0.0118, Train Linf Norm: 0.9670, Test Linf Norm: 0.3219\n",
            "Epoch 123: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0226, Test L1 Norm: 0.0114, Train Linf Norm: 0.8229, Test Linf Norm: 0.2967\n",
            "Epoch 124: Train Loss: 0.0004, Test Loss: 0.0001, Train L1 Norm: 0.0228, Test L1 Norm: 0.0116, Train Linf Norm: 0.8347, Test Linf Norm: 0.3193\n",
            "Epoch 125: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0251, Test L1 Norm: 0.0110, Train Linf Norm: 0.9593, Test Linf Norm: 0.2945\n",
            "Epoch 126: Train Loss: 0.0015, Test Loss: 0.0006, Train L1 Norm: 0.0264, Test L1 Norm: 0.0127, Train Linf Norm: 0.9653, Test Linf Norm: 0.3447\n",
            "Epoch 127: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0255, Test L1 Norm: 0.0112, Train Linf Norm: 0.9705, Test Linf Norm: 0.3008\n",
            "Epoch 128: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0227, Test L1 Norm: 0.0108, Train Linf Norm: 0.8472, Test Linf Norm: 0.2836\n",
            "Epoch 129: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0233, Test L1 Norm: 0.0108, Train Linf Norm: 0.8776, Test Linf Norm: 0.2843\n",
            "Epoch 130: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0200, Test L1 Norm: 0.0112, Train Linf Norm: 0.7252, Test Linf Norm: 0.3120\n",
            "Epoch 131: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0201, Test L1 Norm: 0.0107, Train Linf Norm: 0.7342, Test Linf Norm: 0.2801\n",
            "Epoch 132: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0199, Test L1 Norm: 0.0106, Train Linf Norm: 0.7188, Test Linf Norm: 0.2810\n",
            "Epoch 133: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0212, Test L1 Norm: 0.0107, Train Linf Norm: 0.7922, Test Linf Norm: 0.2851\n",
            "Epoch 134: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0216, Test L1 Norm: 0.0105, Train Linf Norm: 0.8134, Test Linf Norm: 0.2758\n",
            "Epoch 135: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0203, Test L1 Norm: 0.0106, Train Linf Norm: 0.7520, Test Linf Norm: 0.2811\n",
            "Epoch 136: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0216, Test L1 Norm: 0.0106, Train Linf Norm: 0.8128, Test Linf Norm: 0.2819\n",
            "Epoch 137: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0205, Test L1 Norm: 0.0106, Train Linf Norm: 0.7621, Test Linf Norm: 0.2811\n",
            "Epoch 138: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0212, Test L1 Norm: 0.0105, Train Linf Norm: 0.7999, Test Linf Norm: 0.2771\n",
            "Epoch 139: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0212, Test L1 Norm: 0.0104, Train Linf Norm: 0.7928, Test Linf Norm: 0.2779\n",
            "Epoch 140: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0202, Test L1 Norm: 0.0108, Train Linf Norm: 0.7520, Test Linf Norm: 0.2962\n",
            "Epoch 141: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0190, Test L1 Norm: 0.0104, Train Linf Norm: 0.6948, Test Linf Norm: 0.2748\n",
            "Epoch 142: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0197, Test L1 Norm: 0.0104, Train Linf Norm: 0.7291, Test Linf Norm: 0.2744\n",
            "Epoch 143: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0197, Test L1 Norm: 0.0104, Train Linf Norm: 0.7328, Test Linf Norm: 0.2745\n",
            "Epoch 144: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0192, Test L1 Norm: 0.0104, Train Linf Norm: 0.7022, Test Linf Norm: 0.2788\n",
            "Epoch 145: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0192, Test L1 Norm: 0.0104, Train Linf Norm: 0.7063, Test Linf Norm: 0.2781\n",
            "Epoch 146: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0195, Test L1 Norm: 0.0104, Train Linf Norm: 0.7119, Test Linf Norm: 0.2753\n",
            "Epoch 147: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0191, Test L1 Norm: 0.0104, Train Linf Norm: 0.6944, Test Linf Norm: 0.2774\n",
            "Epoch 148: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0193, Test L1 Norm: 0.0104, Train Linf Norm: 0.7097, Test Linf Norm: 0.2763\n",
            "Epoch 149: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0194, Test L1 Norm: 0.0104, Train Linf Norm: 0.7061, Test Linf Norm: 0.2772\n",
            "Epoch 150: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0195, Test L1 Norm: 0.0104, Train Linf Norm: 0.7162, Test Linf Norm: 0.2770\n",
            "Epoch 151: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0195, Test L1 Norm: 0.0104, Train Linf Norm: 0.7227, Test Linf Norm: 0.2770\n",
            "Epoch 152: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0195, Test L1 Norm: 0.0104, Train Linf Norm: 0.7164, Test Linf Norm: 0.2769\n",
            "Epoch 153: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0196, Test L1 Norm: 0.0104, Train Linf Norm: 0.7174, Test Linf Norm: 0.2767\n",
            "Epoch 154: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0194, Test L1 Norm: 0.0104, Train Linf Norm: 0.7092, Test Linf Norm: 0.2775\n",
            "Epoch 155: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0196, Test L1 Norm: 0.0103, Train Linf Norm: 0.7173, Test Linf Norm: 0.2735\n",
            "Epoch 156: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0203, Test L1 Norm: 0.0104, Train Linf Norm: 0.7583, Test Linf Norm: 0.2759\n",
            "Epoch 157: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0193, Test L1 Norm: 0.0103, Train Linf Norm: 0.7136, Test Linf Norm: 0.2746\n",
            "Epoch 158: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0195, Test L1 Norm: 0.0103, Train Linf Norm: 0.7241, Test Linf Norm: 0.2729\n",
            "Epoch 159: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0192, Test L1 Norm: 0.0107, Train Linf Norm: 0.7004, Test Linf Norm: 0.2880\n",
            "Epoch 160: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0195, Test L1 Norm: 0.0104, Train Linf Norm: 0.7220, Test Linf Norm: 0.2761\n",
            "Epoch 161: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0196, Test L1 Norm: 0.0104, Train Linf Norm: 0.7269, Test Linf Norm: 0.2789\n",
            "Epoch 162: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0196, Test L1 Norm: 0.0103, Train Linf Norm: 0.7207, Test Linf Norm: 0.2740\n",
            "Epoch 163: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0211, Test L1 Norm: 0.0104, Train Linf Norm: 0.7969, Test Linf Norm: 0.2760\n",
            "Epoch 164: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0200, Test L1 Norm: 0.0105, Train Linf Norm: 0.7439, Test Linf Norm: 0.2836\n",
            "Epoch 165: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0201, Test L1 Norm: 0.0103, Train Linf Norm: 0.7494, Test Linf Norm: 0.2754\n",
            "Epoch 166: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0195, Test L1 Norm: 0.0103, Train Linf Norm: 0.7181, Test Linf Norm: 0.2738\n",
            "Epoch 167: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0192, Test L1 Norm: 0.0103, Train Linf Norm: 0.7069, Test Linf Norm: 0.2737\n",
            "Epoch 168: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0190, Test L1 Norm: 0.0110, Train Linf Norm: 0.6968, Test Linf Norm: 0.3095\n",
            "Epoch 169: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0195, Test L1 Norm: 0.0102, Train Linf Norm: 0.7216, Test Linf Norm: 0.2718\n",
            "Epoch 170: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0190, Test L1 Norm: 0.0102, Train Linf Norm: 0.6942, Test Linf Norm: 0.2718\n",
            "Epoch 171: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0201, Test L1 Norm: 0.0102, Train Linf Norm: 0.7489, Test Linf Norm: 0.2714\n",
            "Epoch 172: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0203, Test L1 Norm: 0.0103, Train Linf Norm: 0.7575, Test Linf Norm: 0.2640\n",
            "Epoch 173: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0194, Test L1 Norm: 0.0103, Train Linf Norm: 0.7202, Test Linf Norm: 0.2760\n",
            "Epoch 174: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0195, Test L1 Norm: 0.0101, Train Linf Norm: 0.7205, Test Linf Norm: 0.2639\n",
            "Epoch 175: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0206, Test L1 Norm: 0.0101, Train Linf Norm: 0.7738, Test Linf Norm: 0.2709\n",
            "Epoch 176: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0218, Test L1 Norm: 0.0104, Train Linf Norm: 0.8288, Test Linf Norm: 0.2789\n",
            "Epoch 177: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0201, Test L1 Norm: 0.0099, Train Linf Norm: 0.7453, Test Linf Norm: 0.2592\n",
            "Epoch 178: Train Loss: 0.0006, Test Loss: 0.0001, Train L1 Norm: 0.0190, Test L1 Norm: 0.0099, Train Linf Norm: 0.6696, Test Linf Norm: 0.2632\n",
            "Epoch 179: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0210, Test L1 Norm: 0.0099, Train Linf Norm: 0.7906, Test Linf Norm: 0.2663\n",
            "Epoch 180: Train Loss: 0.0007, Test Loss: 0.0001, Train L1 Norm: 0.0205, Test L1 Norm: 0.0105, Train Linf Norm: 0.7400, Test Linf Norm: 0.2766\n",
            "Epoch 181: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0173, Test L1 Norm: 0.0099, Train Linf Norm: 0.6155, Test Linf Norm: 0.2593\n",
            "Epoch 182: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0205, Test L1 Norm: 0.0107, Train Linf Norm: 0.7600, Test Linf Norm: 0.2725\n",
            "Epoch 183: Train Loss: 0.0005, Test Loss: 0.0001, Train L1 Norm: 0.0208, Test L1 Norm: 0.0101, Train Linf Norm: 0.7555, Test Linf Norm: 0.2681\n",
            "Epoch 184: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0205, Test L1 Norm: 0.0100, Train Linf Norm: 0.7706, Test Linf Norm: 0.2665\n",
            "Epoch 185: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0206, Test L1 Norm: 0.0098, Train Linf Norm: 0.7745, Test Linf Norm: 0.2631\n",
            "Epoch 186: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0200, Test L1 Norm: 0.0098, Train Linf Norm: 0.7476, Test Linf Norm: 0.2610\n",
            "Epoch 187: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0186, Test L1 Norm: 0.0096, Train Linf Norm: 0.6839, Test Linf Norm: 0.2560\n",
            "Epoch 188: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0178, Test L1 Norm: 0.0112, Train Linf Norm: 0.6515, Test Linf Norm: 0.3035\n",
            "Epoch 189: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0174, Test L1 Norm: 0.0097, Train Linf Norm: 0.6324, Test Linf Norm: 0.2621\n",
            "Epoch 190: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0192, Test L1 Norm: 0.0098, Train Linf Norm: 0.7139, Test Linf Norm: 0.2618\n",
            "Epoch 191: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0195, Test L1 Norm: 0.0096, Train Linf Norm: 0.7251, Test Linf Norm: 0.2572\n",
            "Epoch 192: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0177, Test L1 Norm: 0.0097, Train Linf Norm: 0.6452, Test Linf Norm: 0.2635\n",
            "Epoch 193: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0188, Test L1 Norm: 0.0096, Train Linf Norm: 0.6998, Test Linf Norm: 0.2545\n",
            "Epoch 194: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0183, Test L1 Norm: 0.0095, Train Linf Norm: 0.6741, Test Linf Norm: 0.2579\n",
            "Epoch 195: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0169, Test L1 Norm: 0.0096, Train Linf Norm: 0.6116, Test Linf Norm: 0.2540\n",
            "Epoch 196: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0184, Test L1 Norm: 0.0094, Train Linf Norm: 0.6877, Test Linf Norm: 0.2517\n",
            "Epoch 197: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0171, Test L1 Norm: 0.0096, Train Linf Norm: 0.6256, Test Linf Norm: 0.2600\n",
            "Epoch 198: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0165, Test L1 Norm: 0.0095, Train Linf Norm: 0.5900, Test Linf Norm: 0.2553\n",
            "Epoch 199: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0159, Test L1 Norm: 0.0093, Train Linf Norm: 0.5691, Test Linf Norm: 0.2496\n",
            "Epoch 200: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0173, Test L1 Norm: 0.0093, Train Linf Norm: 0.6371, Test Linf Norm: 0.2483\n",
            "Epoch 201: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0158, Test L1 Norm: 0.0093, Train Linf Norm: 0.5624, Test Linf Norm: 0.2480\n",
            "Epoch 202: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0159, Test L1 Norm: 0.0094, Train Linf Norm: 0.5657, Test Linf Norm: 0.2489\n",
            "Epoch 203: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0165, Test L1 Norm: 0.0093, Train Linf Norm: 0.5847, Test Linf Norm: 0.2500\n",
            "Epoch 204: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0164, Test L1 Norm: 0.0094, Train Linf Norm: 0.5948, Test Linf Norm: 0.2510\n",
            "Epoch 205: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0165, Test L1 Norm: 0.0093, Train Linf Norm: 0.5984, Test Linf Norm: 0.2488\n",
            "Epoch 206: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0163, Test L1 Norm: 0.0093, Train Linf Norm: 0.5902, Test Linf Norm: 0.2480\n",
            "Epoch 207: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0163, Test L1 Norm: 0.0093, Train Linf Norm: 0.5904, Test Linf Norm: 0.2492\n",
            "Epoch 208: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0164, Test L1 Norm: 0.0093, Train Linf Norm: 0.5884, Test Linf Norm: 0.2486\n",
            "Epoch 209: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0164, Test L1 Norm: 0.0093, Train Linf Norm: 0.5954, Test Linf Norm: 0.2488\n",
            "Epoch 210: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0164, Test L1 Norm: 0.0093, Train Linf Norm: 0.5954, Test Linf Norm: 0.2488\n",
            "Epoch 211: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0164, Test L1 Norm: 0.0093, Train Linf Norm: 0.5895, Test Linf Norm: 0.2488\n",
            "Epoch 212: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0164, Test L1 Norm: 0.0093, Train Linf Norm: 0.5950, Test Linf Norm: 0.2488\n",
            "Epoch 213: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0163, Test L1 Norm: 0.0093, Train Linf Norm: 0.5889, Test Linf Norm: 0.2491\n",
            "Epoch 214: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0160, Test L1 Norm: 0.0093, Train Linf Norm: 0.5776, Test Linf Norm: 0.2499\n",
            "Epoch 215: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0164, Test L1 Norm: 0.0093, Train Linf Norm: 0.5881, Test Linf Norm: 0.2490\n",
            "Epoch 216: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0164, Test L1 Norm: 0.0093, Train Linf Norm: 0.5952, Test Linf Norm: 0.2489\n",
            "Epoch 217: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0165, Test L1 Norm: 0.0093, Train Linf Norm: 0.5983, Test Linf Norm: 0.2475\n",
            "Epoch 218: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0165, Test L1 Norm: 0.0093, Train Linf Norm: 0.6003, Test Linf Norm: 0.2468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 18:34:04,380]\u001b[0m Trial 22 finished with value: 0.009334696703404187 and parameters: {'n_layers': 6, 'n_units_0': 1824, 'n_units_1': 2020, 'n_units_2': 2039, 'n_units_3': 1504, 'n_units_4': 682, 'n_units_5': 19, 'hidden_activation': 'LeakyReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adagrad', 'lr': 0.0003853845869849074, 'batch_size': 48, 'n_epochs': 219, 'scheduler': 'CosineAnnealingLR', 't_max_fraction': 0.14067303973539794, 'eta_min': 1.3612508895327606e-07}. Best is trial 20 with value: 0.009307051534205675.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 219: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0166, Test L1 Norm: 0.0093, Train Linf Norm: 0.6034, Test Linf Norm: 0.2502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 18:34:14,752]\u001b[0m Trial 23 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.2660, Test Loss: 0.0061, Train L1 Norm: 0.6477, Test L1 Norm: 0.1435, Train Linf Norm: 25.9007, Test Linf Norm: 4.6882\n",
            "Epoch 1: Train Loss: 0.2390, Test Loss: 0.0062, Train L1 Norm: 0.8786, Test L1 Norm: 0.1312, Train Linf Norm: 36.4884, Test Linf Norm: 4.2311\n",
            "Epoch 2: Train Loss: 0.0106, Test Loss: 0.0017, Train L1 Norm: 0.1897, Test L1 Norm: 0.0470, Train Linf Norm: 7.4915, Test Linf Norm: 1.2865\n",
            "Epoch 3: Train Loss: 0.0027, Test Loss: 0.0010, Train L1 Norm: 0.1003, Test L1 Norm: 0.0325, Train Linf Norm: 3.8614, Test Linf Norm: 0.8063\n",
            "Epoch 4: Train Loss: 0.0046, Test Loss: 0.0008, Train L1 Norm: 0.1026, Test L1 Norm: 0.0328, Train Linf Norm: 4.0807, Test Linf Norm: 0.9309\n",
            "Epoch 5: Train Loss: 0.0030, Test Loss: 0.0007, Train L1 Norm: 0.0870, Test L1 Norm: 0.0276, Train Linf Norm: 3.4601, Test Linf Norm: 0.7536\n",
            "Epoch 6: Train Loss: 0.0025, Test Loss: 0.0006, Train L1 Norm: 0.0819, Test L1 Norm: 0.0260, Train Linf Norm: 3.2912, Test Linf Norm: 0.7259\n",
            "Epoch 7: Train Loss: 0.0090, Test Loss: 0.0026, Train L1 Norm: 0.1458, Test L1 Norm: 0.0423, Train Linf Norm: 6.1758, Test Linf Norm: 1.3239\n",
            "Epoch 8: Train Loss: 0.0021, Test Loss: 0.0007, Train L1 Norm: 0.1196, Test L1 Norm: 0.0334, Train Linf Norm: 5.1049, Test Linf Norm: 1.0406\n",
            "Epoch 9: Train Loss: 0.0008, Test Loss: 0.0008, Train L1 Norm: 0.1017, Test L1 Norm: 0.0222, Train Linf Norm: 4.3547, Test Linf Norm: 0.5530\n",
            "Epoch 10: Train Loss: 0.0014, Test Loss: 0.0005, Train L1 Norm: 0.0722, Test L1 Norm: 0.0257, Train Linf Norm: 2.9487, Test Linf Norm: 0.7453\n",
            "Epoch 11: Train Loss: 0.0007, Test Loss: 0.0003, Train L1 Norm: 0.0753, Test L1 Norm: 0.0201, Train Linf Norm: 3.1216, Test Linf Norm: 0.5203\n",
            "Epoch 12: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.0698, Test L1 Norm: 0.0190, Train Linf Norm: 2.9093, Test Linf Norm: 0.4816\n",
            "Epoch 13: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.0639, Test L1 Norm: 0.0170, Train Linf Norm: 2.6292, Test Linf Norm: 0.3885\n",
            "Epoch 14: Train Loss: 0.0003, Test Loss: 0.0005, Train L1 Norm: 0.0585, Test L1 Norm: 0.0177, Train Linf Norm: 2.3933, Test Linf Norm: 0.3708\n",
            "Epoch 15: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0578, Test L1 Norm: 0.0166, Train Linf Norm: 2.3785, Test Linf Norm: 0.4053\n",
            "Epoch 16: Train Loss: 0.0002, Test Loss: 0.0003, Train L1 Norm: 0.0583, Test L1 Norm: 0.0175, Train Linf Norm: 2.4111, Test Linf Norm: 0.4583\n",
            "Epoch 17: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0549, Test L1 Norm: 0.0174, Train Linf Norm: 2.2405, Test Linf Norm: 0.4599\n",
            "Epoch 18: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0549, Test L1 Norm: 0.0172, Train Linf Norm: 2.2495, Test Linf Norm: 0.4565\n",
            "Epoch 19: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0567, Test L1 Norm: 0.0171, Train Linf Norm: 2.3432, Test Linf Norm: 0.4569\n",
            "Epoch 20: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0536, Test L1 Norm: 0.0159, Train Linf Norm: 2.2064, Test Linf Norm: 0.3997\n",
            "Epoch 21: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0518, Test L1 Norm: 0.0155, Train Linf Norm: 2.1179, Test Linf Norm: 0.3881\n",
            "Epoch 22: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0503, Test L1 Norm: 0.0157, Train Linf Norm: 2.0598, Test Linf Norm: 0.3993\n",
            "Epoch 23: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0495, Test L1 Norm: 0.0150, Train Linf Norm: 2.0176, Test Linf Norm: 0.3664\n",
            "Epoch 24: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0484, Test L1 Norm: 0.0152, Train Linf Norm: 1.9388, Test Linf Norm: 0.3777\n",
            "Epoch 25: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0508, Test L1 Norm: 0.0155, Train Linf Norm: 2.0885, Test Linf Norm: 0.3983\n",
            "Epoch 26: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0474, Test L1 Norm: 0.0154, Train Linf Norm: 1.9213, Test Linf Norm: 0.3953\n",
            "Epoch 27: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0499, Test L1 Norm: 0.0151, Train Linf Norm: 2.0484, Test Linf Norm: 0.3802\n",
            "Epoch 28: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0510, Test L1 Norm: 0.0153, Train Linf Norm: 2.1044, Test Linf Norm: 0.3884\n",
            "Epoch 29: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0499, Test L1 Norm: 0.0152, Train Linf Norm: 2.0273, Test Linf Norm: 0.3859\n",
            "Epoch 30: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0500, Test L1 Norm: 0.0150, Train Linf Norm: 2.0473, Test Linf Norm: 0.3761\n",
            "Epoch 31: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0495, Test L1 Norm: 0.0154, Train Linf Norm: 2.0245, Test Linf Norm: 0.3942\n",
            "Epoch 32: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0492, Test L1 Norm: 0.0152, Train Linf Norm: 2.0194, Test Linf Norm: 0.3869\n",
            "Epoch 33: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0490, Test L1 Norm: 0.0151, Train Linf Norm: 2.0097, Test Linf Norm: 0.3820\n",
            "Epoch 34: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0491, Test L1 Norm: 0.0151, Train Linf Norm: 2.0204, Test Linf Norm: 0.3822\n",
            "Epoch 35: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0492, Test L1 Norm: 0.0152, Train Linf Norm: 2.0227, Test Linf Norm: 0.3857\n",
            "Epoch 36: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0490, Test L1 Norm: 0.0152, Train Linf Norm: 2.0168, Test Linf Norm: 0.3858\n",
            "Epoch 37: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0493, Test L1 Norm: 0.0150, Train Linf Norm: 2.0294, Test Linf Norm: 0.3782\n",
            "Epoch 38: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0488, Test L1 Norm: 0.0152, Train Linf Norm: 2.0055, Test Linf Norm: 0.3884\n",
            "Epoch 39: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0492, Test L1 Norm: 0.0151, Train Linf Norm: 2.0165, Test Linf Norm: 0.3824\n",
            "Epoch 40: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0493, Test L1 Norm: 0.0151, Train Linf Norm: 2.0229, Test Linf Norm: 0.3824\n",
            "Epoch 41: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0488, Test L1 Norm: 0.0155, Train Linf Norm: 2.0031, Test Linf Norm: 0.4042\n",
            "Epoch 42: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0495, Test L1 Norm: 0.0156, Train Linf Norm: 2.0367, Test Linf Norm: 0.4072\n",
            "Epoch 43: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0506, Test L1 Norm: 0.0150, Train Linf Norm: 2.0772, Test Linf Norm: 0.3849\n",
            "Epoch 44: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0479, Test L1 Norm: 0.0144, Train Linf Norm: 1.9616, Test Linf Norm: 0.3514\n",
            "Epoch 45: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0469, Test L1 Norm: 0.0146, Train Linf Norm: 1.9128, Test Linf Norm: 0.3578\n",
            "Epoch 46: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0488, Test L1 Norm: 0.0157, Train Linf Norm: 2.0057, Test Linf Norm: 0.4195\n",
            "Epoch 47: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0478, Test L1 Norm: 0.0153, Train Linf Norm: 1.9599, Test Linf Norm: 0.4051\n",
            "Epoch 48: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0457, Test L1 Norm: 0.0151, Train Linf Norm: 1.8487, Test Linf Norm: 0.3939\n",
            "Epoch 49: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0493, Test L1 Norm: 0.0145, Train Linf Norm: 2.0374, Test Linf Norm: 0.3717\n",
            "Epoch 50: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0460, Test L1 Norm: 0.0140, Train Linf Norm: 1.8798, Test Linf Norm: 0.3476\n",
            "Epoch 51: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0480, Test L1 Norm: 0.0141, Train Linf Norm: 1.9744, Test Linf Norm: 0.3538\n",
            "Epoch 52: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0446, Test L1 Norm: 0.0147, Train Linf Norm: 1.8150, Test Linf Norm: 0.3379\n",
            "Epoch 53: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0450, Test L1 Norm: 0.0147, Train Linf Norm: 1.8374, Test Linf Norm: 0.3919\n",
            "Epoch 54: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0466, Test L1 Norm: 0.0148, Train Linf Norm: 1.9035, Test Linf Norm: 0.3956\n",
            "Epoch 55: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0460, Test L1 Norm: 0.0135, Train Linf Norm: 1.8846, Test Linf Norm: 0.3376\n",
            "Epoch 56: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0489, Test L1 Norm: 0.0139, Train Linf Norm: 2.0371, Test Linf Norm: 0.3640\n",
            "Epoch 57: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0441, Test L1 Norm: 0.0140, Train Linf Norm: 1.8094, Test Linf Norm: 0.3659\n",
            "Epoch 58: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0454, Test L1 Norm: 0.0142, Train Linf Norm: 1.8632, Test Linf Norm: 0.3814\n",
            "Epoch 59: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0438, Test L1 Norm: 0.0146, Train Linf Norm: 1.7965, Test Linf Norm: 0.3959\n",
            "Epoch 60: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.0434, Test L1 Norm: 0.0132, Train Linf Norm: 1.7630, Test Linf Norm: 0.3290\n",
            "Epoch 61: Train Loss: 0.0022, Test Loss: 0.0002, Train L1 Norm: 0.0470, Test L1 Norm: 0.0137, Train Linf Norm: 1.8594, Test Linf Norm: 0.3214\n",
            "Epoch 62: Train Loss: 0.0013, Test Loss: 0.0002, Train L1 Norm: 0.0514, Test L1 Norm: 0.0135, Train Linf Norm: 2.1028, Test Linf Norm: 0.3385\n",
            "Epoch 63: Train Loss: 0.0007, Test Loss: 0.0002, Train L1 Norm: 0.0492, Test L1 Norm: 0.0137, Train Linf Norm: 2.0260, Test Linf Norm: 0.3642\n",
            "Epoch 64: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.0375, Test L1 Norm: 0.0134, Train Linf Norm: 1.4860, Test Linf Norm: 0.3233\n",
            "Epoch 65: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0388, Test L1 Norm: 0.0140, Train Linf Norm: 1.5531, Test Linf Norm: 0.3832\n",
            "Epoch 66: Train Loss: 0.0006, Test Loss: 0.0002, Train L1 Norm: 0.0438, Test L1 Norm: 0.0125, Train Linf Norm: 1.7849, Test Linf Norm: 0.3039\n",
            "Epoch 67: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0128, Train Linf Norm: 1.3990, Test Linf Norm: 0.3371\n",
            "Epoch 68: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0402, Test L1 Norm: 0.0123, Train Linf Norm: 1.6388, Test Linf Norm: 0.3166\n",
            "Epoch 69: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0372, Test L1 Norm: 0.0125, Train Linf Norm: 1.4957, Test Linf Norm: 0.3124\n",
            "Epoch 70: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0383, Test L1 Norm: 0.0127, Train Linf Norm: 1.5394, Test Linf Norm: 0.3106\n",
            "Epoch 71: Train Loss: 0.0008, Test Loss: 0.0002, Train L1 Norm: 0.0302, Test L1 Norm: 0.0127, Train Linf Norm: 1.1408, Test Linf Norm: 0.3356\n",
            "Epoch 72: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.0352, Test L1 Norm: 0.0126, Train Linf Norm: 1.4020, Test Linf Norm: 0.3224\n",
            "Epoch 73: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0330, Test L1 Norm: 0.0125, Train Linf Norm: 1.3038, Test Linf Norm: 0.3358\n",
            "Epoch 74: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0365, Test L1 Norm: 0.0122, Train Linf Norm: 1.4866, Test Linf Norm: 0.3229\n",
            "Epoch 75: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0345, Test L1 Norm: 0.0119, Train Linf Norm: 1.3903, Test Linf Norm: 0.3073\n",
            "Epoch 76: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0342, Test L1 Norm: 0.0119, Train Linf Norm: 1.3716, Test Linf Norm: 0.3084\n",
            "Epoch 77: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0297, Test L1 Norm: 0.0146, Train Linf Norm: 1.1672, Test Linf Norm: 0.3901\n",
            "Epoch 78: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0308, Test L1 Norm: 0.0115, Train Linf Norm: 1.2217, Test Linf Norm: 0.3009\n",
            "Epoch 79: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0126, Train Linf Norm: 1.4477, Test Linf Norm: 0.3371\n",
            "Epoch 80: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0318, Test L1 Norm: 0.0116, Train Linf Norm: 1.2807, Test Linf Norm: 0.3081\n",
            "Epoch 81: Train Loss: 0.0001, Test Loss: 0.0007, Train L1 Norm: 0.0302, Test L1 Norm: 0.0128, Train Linf Norm: 1.2018, Test Linf Norm: 0.3085\n",
            "Epoch 82: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0292, Test L1 Norm: 0.0115, Train Linf Norm: 1.1529, Test Linf Norm: 0.3050\n",
            "Epoch 83: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0308, Test L1 Norm: 0.0119, Train Linf Norm: 1.2381, Test Linf Norm: 0.3329\n",
            "Epoch 84: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0290, Test L1 Norm: 0.0114, Train Linf Norm: 1.1514, Test Linf Norm: 0.3082\n",
            "Epoch 85: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0116, Train Linf Norm: 1.0595, Test Linf Norm: 0.3043\n",
            "Epoch 86: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0294, Test L1 Norm: 0.0114, Train Linf Norm: 1.1731, Test Linf Norm: 0.3101\n",
            "Epoch 87: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0292, Test L1 Norm: 0.0117, Train Linf Norm: 1.1636, Test Linf Norm: 0.3251\n",
            "Epoch 88: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0304, Test L1 Norm: 0.0112, Train Linf Norm: 1.2184, Test Linf Norm: 0.2983\n",
            "Epoch 89: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0289, Test L1 Norm: 0.0112, Train Linf Norm: 1.1496, Test Linf Norm: 0.3008\n",
            "Epoch 90: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0304, Test L1 Norm: 0.0112, Train Linf Norm: 1.2174, Test Linf Norm: 0.2974\n",
            "Epoch 91: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0275, Test L1 Norm: 0.0112, Train Linf Norm: 1.0829, Test Linf Norm: 0.2970\n",
            "Epoch 92: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0290, Test L1 Norm: 0.0114, Train Linf Norm: 1.1555, Test Linf Norm: 0.3098\n",
            "Epoch 93: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0274, Test L1 Norm: 0.0112, Train Linf Norm: 1.0853, Test Linf Norm: 0.2997\n",
            "Epoch 94: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0284, Test L1 Norm: 0.0112, Train Linf Norm: 1.1312, Test Linf Norm: 0.2993\n",
            "Epoch 95: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0278, Test L1 Norm: 0.0112, Train Linf Norm: 1.1005, Test Linf Norm: 0.2977\n",
            "Epoch 96: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0278, Test L1 Norm: 0.0112, Train Linf Norm: 1.0991, Test Linf Norm: 0.2986\n",
            "Epoch 97: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0278, Test L1 Norm: 0.0111, Train Linf Norm: 1.1025, Test Linf Norm: 0.2986\n",
            "Epoch 98: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0278, Test L1 Norm: 0.0111, Train Linf Norm: 1.1045, Test Linf Norm: 0.2978\n",
            "Epoch 99: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0276, Test L1 Norm: 0.0111, Train Linf Norm: 1.0941, Test Linf Norm: 0.2983\n",
            "Epoch 100: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0277, Test L1 Norm: 0.0111, Train Linf Norm: 1.0932, Test Linf Norm: 0.2982\n",
            "Epoch 101: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0276, Test L1 Norm: 0.0111, Train Linf Norm: 1.0910, Test Linf Norm: 0.2984\n",
            "Epoch 102: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0275, Test L1 Norm: 0.0111, Train Linf Norm: 1.0886, Test Linf Norm: 0.2981\n",
            "Epoch 103: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0276, Test L1 Norm: 0.0112, Train Linf Norm: 1.0910, Test Linf Norm: 0.2988\n",
            "Epoch 104: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0279, Test L1 Norm: 0.0111, Train Linf Norm: 1.1057, Test Linf Norm: 0.2983\n",
            "Epoch 105: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0280, Test L1 Norm: 0.0112, Train Linf Norm: 1.1151, Test Linf Norm: 0.2976\n",
            "Epoch 106: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0269, Test L1 Norm: 0.0111, Train Linf Norm: 1.0584, Test Linf Norm: 0.2973\n",
            "Epoch 107: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0268, Test L1 Norm: 0.0111, Train Linf Norm: 1.0559, Test Linf Norm: 0.2991\n",
            "Epoch 108: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0279, Test L1 Norm: 0.0111, Train Linf Norm: 1.1038, Test Linf Norm: 0.2984\n",
            "Epoch 109: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0279, Test L1 Norm: 0.0111, Train Linf Norm: 1.1079, Test Linf Norm: 0.2982\n",
            "Epoch 110: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0276, Test L1 Norm: 0.0111, Train Linf Norm: 1.0975, Test Linf Norm: 0.2986\n",
            "Epoch 111: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0273, Test L1 Norm: 0.0112, Train Linf Norm: 1.0733, Test Linf Norm: 0.2994\n",
            "Epoch 112: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0242, Test L1 Norm: 0.0111, Train Linf Norm: 0.9283, Test Linf Norm: 0.2990\n",
            "Epoch 113: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0277, Test L1 Norm: 0.0111, Train Linf Norm: 1.0969, Test Linf Norm: 0.2939\n",
            "Epoch 114: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0285, Test L1 Norm: 0.0111, Train Linf Norm: 1.1350, Test Linf Norm: 0.3010\n",
            "Epoch 115: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0262, Test L1 Norm: 0.0110, Train Linf Norm: 1.0241, Test Linf Norm: 0.2951\n",
            "Epoch 116: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0111, Train Linf Norm: 1.0603, Test Linf Norm: 0.3023\n",
            "Epoch 117: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0269, Test L1 Norm: 0.0111, Train Linf Norm: 1.0563, Test Linf Norm: 0.2964\n",
            "Epoch 118: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0279, Test L1 Norm: 0.0111, Train Linf Norm: 1.1075, Test Linf Norm: 0.2952\n",
            "Epoch 119: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0276, Test L1 Norm: 0.0112, Train Linf Norm: 1.0903, Test Linf Norm: 0.3072\n",
            "Epoch 120: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0285, Test L1 Norm: 0.0110, Train Linf Norm: 1.1356, Test Linf Norm: 0.2985\n",
            "Epoch 121: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0280, Test L1 Norm: 0.0112, Train Linf Norm: 1.1119, Test Linf Norm: 0.2997\n",
            "Epoch 122: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0266, Test L1 Norm: 0.0110, Train Linf Norm: 1.0437, Test Linf Norm: 0.2987\n",
            "Epoch 123: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0259, Test L1 Norm: 0.0109, Train Linf Norm: 1.0162, Test Linf Norm: 0.2950\n",
            "Epoch 124: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0280, Test L1 Norm: 0.0109, Train Linf Norm: 1.1070, Test Linf Norm: 0.2939\n",
            "Epoch 125: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0258, Test L1 Norm: 0.0109, Train Linf Norm: 1.0054, Test Linf Norm: 0.2951\n",
            "Epoch 126: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0253, Test L1 Norm: 0.0108, Train Linf Norm: 0.9874, Test Linf Norm: 0.2896\n",
            "Epoch 127: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0250, Test L1 Norm: 0.0109, Train Linf Norm: 0.9696, Test Linf Norm: 0.2930\n",
            "Epoch 128: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0276, Test L1 Norm: 0.0109, Train Linf Norm: 1.0830, Test Linf Norm: 0.2886\n",
            "Epoch 129: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0267, Test L1 Norm: 0.0113, Train Linf Norm: 1.0490, Test Linf Norm: 0.3188\n",
            "Epoch 130: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0254, Test L1 Norm: 0.0105, Train Linf Norm: 0.9863, Test Linf Norm: 0.2825\n",
            "Epoch 131: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0222, Test L1 Norm: 0.0106, Train Linf Norm: 0.8290, Test Linf Norm: 0.2877\n",
            "Epoch 132: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0274, Test L1 Norm: 0.0106, Train Linf Norm: 1.0836, Test Linf Norm: 0.2835\n",
            "Epoch 133: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0259, Test L1 Norm: 0.0104, Train Linf Norm: 1.0085, Test Linf Norm: 0.2815\n",
            "Epoch 134: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0303, Test L1 Norm: 0.0110, Train Linf Norm: 1.2219, Test Linf Norm: 0.2883\n",
            "Epoch 135: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0279, Test L1 Norm: 0.0105, Train Linf Norm: 1.1150, Test Linf Norm: 0.2916\n",
            "Epoch 136: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0232, Test L1 Norm: 0.0108, Train Linf Norm: 0.8941, Test Linf Norm: 0.2850\n",
            "Epoch 137: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0262, Test L1 Norm: 0.0103, Train Linf Norm: 1.0334, Test Linf Norm: 0.2819\n",
            "Epoch 138: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0228, Test L1 Norm: 0.0102, Train Linf Norm: 0.8768, Test Linf Norm: 0.2813\n",
            "Epoch 139: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0291, Test L1 Norm: 0.0105, Train Linf Norm: 1.1768, Test Linf Norm: 0.2851\n",
            "Epoch 140: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0238, Test L1 Norm: 0.0105, Train Linf Norm: 0.9269, Test Linf Norm: 0.2751\n",
            "Epoch 141: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0261, Test L1 Norm: 0.0102, Train Linf Norm: 1.0432, Test Linf Norm: 0.2795\n",
            "Epoch 142: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0224, Test L1 Norm: 0.0102, Train Linf Norm: 0.8649, Test Linf Norm: 0.2785\n",
            "Epoch 143: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0242, Test L1 Norm: 0.0105, Train Linf Norm: 0.9420, Test Linf Norm: 0.2828\n",
            "Epoch 144: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0252, Test L1 Norm: 0.0103, Train Linf Norm: 1.0019, Test Linf Norm: 0.2862\n",
            "Epoch 145: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0232, Test L1 Norm: 0.0100, Train Linf Norm: 0.9013, Test Linf Norm: 0.2747\n",
            "Epoch 146: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0243, Test L1 Norm: 0.0106, Train Linf Norm: 0.9586, Test Linf Norm: 0.2854\n",
            "Epoch 147: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0279, Test L1 Norm: 0.0101, Train Linf Norm: 1.1349, Test Linf Norm: 0.2750\n",
            "Epoch 148: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0266, Test L1 Norm: 0.0099, Train Linf Norm: 1.0721, Test Linf Norm: 0.2690\n",
            "Epoch 149: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0253, Test L1 Norm: 0.0106, Train Linf Norm: 1.0101, Test Linf Norm: 0.2980\n",
            "Epoch 150: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0246, Test L1 Norm: 0.0101, Train Linf Norm: 0.9774, Test Linf Norm: 0.2735\n",
            "Epoch 151: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0100, Train Linf Norm: 1.0979, Test Linf Norm: 0.2700\n",
            "Epoch 152: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0250, Test L1 Norm: 0.0099, Train Linf Norm: 0.9945, Test Linf Norm: 0.2696\n",
            "Epoch 153: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0249, Test L1 Norm: 0.0099, Train Linf Norm: 0.9957, Test Linf Norm: 0.2709\n",
            "Epoch 154: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0237, Test L1 Norm: 0.0098, Train Linf Norm: 0.9362, Test Linf Norm: 0.2692\n",
            "Epoch 155: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0252, Test L1 Norm: 0.0098, Train Linf Norm: 1.0054, Test Linf Norm: 0.2682\n",
            "Epoch 156: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0236, Test L1 Norm: 0.0099, Train Linf Norm: 0.9274, Test Linf Norm: 0.2712\n",
            "Epoch 157: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0229, Test L1 Norm: 0.0098, Train Linf Norm: 0.8995, Test Linf Norm: 0.2678\n",
            "Epoch 158: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0246, Test L1 Norm: 0.0098, Train Linf Norm: 0.9800, Test Linf Norm: 0.2682\n",
            "Epoch 159: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0252, Test L1 Norm: 0.0098, Train Linf Norm: 1.0089, Test Linf Norm: 0.2707\n",
            "Epoch 160: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0251, Test L1 Norm: 0.0098, Train Linf Norm: 1.0084, Test Linf Norm: 0.2686\n",
            "Epoch 161: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0243, Test L1 Norm: 0.0098, Train Linf Norm: 0.9688, Test Linf Norm: 0.2684\n",
            "Epoch 162: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0252, Test L1 Norm: 0.0098, Train Linf Norm: 1.0131, Test Linf Norm: 0.2682\n",
            "Epoch 163: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0251, Test L1 Norm: 0.0098, Train Linf Norm: 1.0092, Test Linf Norm: 0.2684\n",
            "Epoch 164: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0252, Test L1 Norm: 0.0098, Train Linf Norm: 0.9985, Test Linf Norm: 0.2685\n",
            "Epoch 165: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0249, Test L1 Norm: 0.0098, Train Linf Norm: 0.9975, Test Linf Norm: 0.2685\n",
            "Epoch 166: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0249, Test L1 Norm: 0.0098, Train Linf Norm: 0.9961, Test Linf Norm: 0.2685\n",
            "Epoch 167: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0248, Test L1 Norm: 0.0098, Train Linf Norm: 0.9898, Test Linf Norm: 0.2685\n",
            "Epoch 168: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0249, Test L1 Norm: 0.0098, Train Linf Norm: 0.9960, Test Linf Norm: 0.2684\n",
            "Epoch 169: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0244, Test L1 Norm: 0.0098, Train Linf Norm: 0.9738, Test Linf Norm: 0.2679\n",
            "Epoch 170: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0249, Test L1 Norm: 0.0098, Train Linf Norm: 0.9937, Test Linf Norm: 0.2685\n",
            "Epoch 171: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0245, Test L1 Norm: 0.0098, Train Linf Norm: 0.9782, Test Linf Norm: 0.2682\n",
            "Epoch 172: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0244, Test L1 Norm: 0.0098, Train Linf Norm: 0.9746, Test Linf Norm: 0.2686\n",
            "Epoch 173: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0254, Test L1 Norm: 0.0098, Train Linf Norm: 1.0203, Test Linf Norm: 0.2688\n",
            "Epoch 174: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0247, Test L1 Norm: 0.0098, Train Linf Norm: 0.9797, Test Linf Norm: 0.2682\n",
            "Epoch 175: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0258, Test L1 Norm: 0.0098, Train Linf Norm: 1.0387, Test Linf Norm: 0.2686\n",
            "Epoch 176: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0252, Test L1 Norm: 0.0099, Train Linf Norm: 1.0096, Test Linf Norm: 0.2729\n",
            "Epoch 177: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0252, Test L1 Norm: 0.0099, Train Linf Norm: 1.0097, Test Linf Norm: 0.2650\n",
            "Epoch 178: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0239, Test L1 Norm: 0.0097, Train Linf Norm: 0.9492, Test Linf Norm: 0.2675\n",
            "Epoch 179: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0260, Test L1 Norm: 0.0098, Train Linf Norm: 1.0475, Test Linf Norm: 0.2677\n",
            "Epoch 180: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0235, Test L1 Norm: 0.0097, Train Linf Norm: 0.9280, Test Linf Norm: 0.2665\n",
            "Epoch 181: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0236, Test L1 Norm: 0.0099, Train Linf Norm: 0.9318, Test Linf Norm: 0.2718\n",
            "Epoch 182: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0260, Test L1 Norm: 0.0098, Train Linf Norm: 1.0512, Test Linf Norm: 0.2686\n",
            "Epoch 183: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0257, Test L1 Norm: 0.0098, Train Linf Norm: 1.0295, Test Linf Norm: 0.2684\n",
            "Epoch 184: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0269, Test L1 Norm: 0.0097, Train Linf Norm: 1.0924, Test Linf Norm: 0.2613\n",
            "Epoch 185: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0259, Test L1 Norm: 0.0097, Train Linf Norm: 1.0410, Test Linf Norm: 0.2614\n",
            "Epoch 186: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0244, Test L1 Norm: 0.0096, Train Linf Norm: 0.9746, Test Linf Norm: 0.2631\n",
            "Epoch 187: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0252, Test L1 Norm: 0.0096, Train Linf Norm: 1.0112, Test Linf Norm: 0.2657\n",
            "Epoch 188: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0225, Test L1 Norm: 0.0096, Train Linf Norm: 0.8743, Test Linf Norm: 0.2618\n",
            "Epoch 189: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0252, Test L1 Norm: 0.0098, Train Linf Norm: 1.0058, Test Linf Norm: 0.2626\n",
            "Epoch 190: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0240, Test L1 Norm: 0.0097, Train Linf Norm: 0.9528, Test Linf Norm: 0.2670\n",
            "Epoch 191: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0241, Test L1 Norm: 0.0094, Train Linf Norm: 0.9562, Test Linf Norm: 0.2542\n",
            "Epoch 192: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0257, Test L1 Norm: 0.0095, Train Linf Norm: 1.0304, Test Linf Norm: 0.2566\n",
            "Epoch 193: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0267, Test L1 Norm: 0.0104, Train Linf Norm: 1.0858, Test Linf Norm: 0.2873\n",
            "Epoch 194: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0244, Test L1 Norm: 0.0098, Train Linf Norm: 0.9678, Test Linf Norm: 0.2521\n",
            "Epoch 195: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0260, Test L1 Norm: 0.0098, Train Linf Norm: 1.0422, Test Linf Norm: 0.2551\n",
            "Epoch 196: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0235, Test L1 Norm: 0.0095, Train Linf Norm: 0.9225, Test Linf Norm: 0.2526\n",
            "Epoch 197: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0213, Test L1 Norm: 0.0095, Train Linf Norm: 0.8212, Test Linf Norm: 0.2563\n",
            "Epoch 198: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0218, Test L1 Norm: 0.0126, Train Linf Norm: 0.8389, Test Linf Norm: 0.3812\n",
            "Epoch 199: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0215, Test L1 Norm: 0.0094, Train Linf Norm: 0.8274, Test Linf Norm: 0.2488\n",
            "Epoch 200: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0240, Test L1 Norm: 0.0096, Train Linf Norm: 0.9488, Test Linf Norm: 0.2573\n",
            "Epoch 201: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0226, Test L1 Norm: 0.0094, Train Linf Norm: 0.8796, Test Linf Norm: 0.2527\n",
            "Epoch 202: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0223, Test L1 Norm: 0.0094, Train Linf Norm: 0.8743, Test Linf Norm: 0.2519\n",
            "Epoch 203: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0227, Test L1 Norm: 0.0093, Train Linf Norm: 0.8946, Test Linf Norm: 0.2471\n",
            "Epoch 204: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0220, Test L1 Norm: 0.0091, Train Linf Norm: 0.8611, Test Linf Norm: 0.2404\n",
            "Epoch 205: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0274, Test L1 Norm: 0.0092, Train Linf Norm: 1.1236, Test Linf Norm: 0.2443\n",
            "Epoch 206: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0242, Test L1 Norm: 0.0091, Train Linf Norm: 0.9686, Test Linf Norm: 0.2460\n",
            "Epoch 207: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0278, Test L1 Norm: 0.0100, Train Linf Norm: 1.1352, Test Linf Norm: 0.2629\n",
            "Epoch 208: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0250, Test L1 Norm: 0.0089, Train Linf Norm: 1.0069, Test Linf Norm: 0.2375\n",
            "Epoch 209: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0265, Test L1 Norm: 0.0090, Train Linf Norm: 1.0827, Test Linf Norm: 0.2376\n",
            "Epoch 210: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0252, Test L1 Norm: 0.0089, Train Linf Norm: 0.9967, Test Linf Norm: 0.2374\n",
            "Epoch 211: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0239, Test L1 Norm: 0.0089, Train Linf Norm: 0.9604, Test Linf Norm: 0.2355\n",
            "Epoch 212: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0244, Test L1 Norm: 0.0089, Train Linf Norm: 0.9853, Test Linf Norm: 0.2331\n",
            "Epoch 213: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0244, Test L1 Norm: 0.0087, Train Linf Norm: 0.9856, Test Linf Norm: 0.2309\n",
            "Epoch 214: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0230, Test L1 Norm: 0.0090, Train Linf Norm: 0.9211, Test Linf Norm: 0.2388\n",
            "Epoch 215: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0230, Test L1 Norm: 0.0088, Train Linf Norm: 0.9218, Test Linf Norm: 0.2343\n",
            "Epoch 216: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0217, Test L1 Norm: 0.0088, Train Linf Norm: 0.8622, Test Linf Norm: 0.2318\n",
            "Epoch 217: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0247, Test L1 Norm: 0.0087, Train Linf Norm: 1.0049, Test Linf Norm: 0.2301\n",
            "Epoch 218: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0243, Test L1 Norm: 0.0087, Train Linf Norm: 0.9839, Test Linf Norm: 0.2317\n",
            "Epoch 219: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0235, Test L1 Norm: 0.0088, Train Linf Norm: 0.9471, Test Linf Norm: 0.2324\n",
            "Epoch 220: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0251, Test L1 Norm: 0.0087, Train Linf Norm: 1.0253, Test Linf Norm: 0.2305\n",
            "Epoch 221: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0233, Test L1 Norm: 0.0087, Train Linf Norm: 0.9366, Test Linf Norm: 0.2305\n",
            "Epoch 222: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0261, Test L1 Norm: 0.0086, Train Linf Norm: 1.0749, Test Linf Norm: 0.2280\n",
            "Epoch 223: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0227, Test L1 Norm: 0.0087, Train Linf Norm: 0.9116, Test Linf Norm: 0.2308\n",
            "Epoch 224: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0232, Test L1 Norm: 0.0086, Train Linf Norm: 0.9271, Test Linf Norm: 0.2290\n",
            "Epoch 225: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0231, Test L1 Norm: 0.0087, Train Linf Norm: 0.9261, Test Linf Norm: 0.2291\n",
            "Epoch 226: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0235, Test L1 Norm: 0.0086, Train Linf Norm: 0.9481, Test Linf Norm: 0.2286\n",
            "Epoch 227: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0234, Test L1 Norm: 0.0086, Train Linf Norm: 0.9437, Test Linf Norm: 0.2279\n",
            "Epoch 228: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0230, Test L1 Norm: 0.0086, Train Linf Norm: 0.9269, Test Linf Norm: 0.2277\n",
            "Epoch 229: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0231, Test L1 Norm: 0.0086, Train Linf Norm: 0.9319, Test Linf Norm: 0.2276\n",
            "Epoch 230: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0232, Test L1 Norm: 0.0086, Train Linf Norm: 0.9364, Test Linf Norm: 0.2273\n",
            "Epoch 231: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0233, Test L1 Norm: 0.0086, Train Linf Norm: 0.9338, Test Linf Norm: 0.2275\n",
            "Epoch 232: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0233, Test L1 Norm: 0.0086, Train Linf Norm: 0.9385, Test Linf Norm: 0.2275\n",
            "Epoch 233: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0233, Test L1 Norm: 0.0086, Train Linf Norm: 0.9392, Test Linf Norm: 0.2275\n",
            "Epoch 234: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0232, Test L1 Norm: 0.0086, Train Linf Norm: 0.9337, Test Linf Norm: 0.2274\n",
            "Epoch 235: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0234, Test L1 Norm: 0.0086, Train Linf Norm: 0.9446, Test Linf Norm: 0.2276\n",
            "Epoch 236: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0237, Test L1 Norm: 0.0086, Train Linf Norm: 0.9611, Test Linf Norm: 0.2277\n",
            "Epoch 237: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0238, Test L1 Norm: 0.0086, Train Linf Norm: 0.9626, Test Linf Norm: 0.2272\n",
            "Epoch 238: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0238, Test L1 Norm: 0.0087, Train Linf Norm: 0.9650, Test Linf Norm: 0.2284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 19:11:31,235]\u001b[0m Trial 24 finished with value: 0.00859525605328381 and parameters: {'n_layers': 6, 'n_units_0': 1232, 'n_units_1': 2042, 'n_units_2': 1457, 'n_units_3': 1465, 'n_units_4': 625, 'n_units_5': 47, 'hidden_activation': 'LeakyReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adagrad', 'lr': 0.0005138171230886355, 'batch_size': 48, 'n_epochs': 239, 'scheduler': 'CosineAnnealingLR', 't_max_fraction': 0.14112860634139976, 'eta_min': 1.5124586804607073e-06}. Best is trial 24 with value: 0.00859525605328381.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 239: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0234, Test L1 Norm: 0.0086, Train Linf Norm: 0.9450, Test Linf Norm: 0.2276\n",
            "Epoch 1: Train Loss: 0.5555, Test Loss: 0.0094, Train L1 Norm: 0.5645, Test L1 Norm: 0.1310, Train Linf Norm: 38.8028, Test Linf Norm: 7.1574\n",
            "Epoch 2: Train Loss: 0.0214, Test Loss: 0.0041, Train L1 Norm: 0.3151, Test L1 Norm: 0.0532, Train Linf Norm: 25.5621, Test Linf Norm: 2.4756\n",
            "Epoch 3: Train Loss: 0.0051, Test Loss: 0.0019, Train L1 Norm: 0.1157, Test L1 Norm: 0.0415, Train Linf Norm: 8.4453, Test Linf Norm: 1.9757\n",
            "Epoch 4: Train Loss: 0.0058, Test Loss: 0.0048, Train L1 Norm: 0.0879, Test L1 Norm: 0.0385, Train Linf Norm: 6.1360, Test Linf Norm: 1.5779\n",
            "Epoch 5: Train Loss: 0.0046, Test Loss: 0.0022, Train L1 Norm: 0.0807, Test L1 Norm: 0.0467, Train Linf Norm: 5.8054, Test Linf Norm: 2.2582\n",
            "Epoch 6: Train Loss: 0.0024, Test Loss: 0.0006, Train L1 Norm: 0.0551, Test L1 Norm: 0.0290, Train Linf Norm: 3.6705, Test Linf Norm: 1.3833\n",
            "Epoch 7: Train Loss: 0.0024, Test Loss: 0.0007, Train L1 Norm: 0.0567, Test L1 Norm: 0.0249, Train Linf Norm: 3.8897, Test Linf Norm: 1.0874\n",
            "Epoch 8: Train Loss: 0.0013, Test Loss: 0.0032, Train L1 Norm: 0.0783, Test L1 Norm: 0.0281, Train Linf Norm: 6.1089, Test Linf Norm: 0.9110\n",
            "Epoch 9: Train Loss: 0.0010, Test Loss: 0.0012, Train L1 Norm: 0.0666, Test L1 Norm: 0.0227, Train Linf Norm: 5.1060, Test Linf Norm: 0.9458\n",
            "Epoch 10: Train Loss: 0.0008, Test Loss: 0.0026, Train L1 Norm: 0.0490, Test L1 Norm: 0.0259, Train Linf Norm: 3.4667, Test Linf Norm: 1.0374\n",
            "Epoch 11: Train Loss: 0.0007, Test Loss: 0.0003, Train L1 Norm: 0.0555, Test L1 Norm: 0.0236, Train Linf Norm: 4.1897, Test Linf Norm: 1.1599\n",
            "Epoch 12: Train Loss: 0.0011, Test Loss: 0.0003, Train L1 Norm: 0.0775, Test L1 Norm: 0.0187, Train Linf Norm: 6.3047, Test Linf Norm: 0.7735\n",
            "Epoch 13: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0575, Test L1 Norm: 0.0198, Train Linf Norm: 4.4587, Test Linf Norm: 0.9147\n",
            "Epoch 14: Train Loss: 0.0006, Test Loss: 0.0003, Train L1 Norm: 0.0478, Test L1 Norm: 0.0182, Train Linf Norm: 3.5574, Test Linf Norm: 0.7860\n",
            "Epoch 15: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.0499, Test L1 Norm: 0.0187, Train Linf Norm: 3.8189, Test Linf Norm: 0.8529\n",
            "Epoch 16: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0518, Test L1 Norm: 0.0189, Train Linf Norm: 3.9979, Test Linf Norm: 0.8917\n",
            "Epoch 17: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0535, Test L1 Norm: 0.0174, Train Linf Norm: 4.1923, Test Linf Norm: 0.7606\n",
            "Epoch 18: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0517, Test L1 Norm: 0.0188, Train Linf Norm: 4.0403, Test Linf Norm: 0.8878\n",
            "Epoch 19: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0465, Test L1 Norm: 0.0181, Train Linf Norm: 3.5625, Test Linf Norm: 0.8490\n",
            "Epoch 20: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0458, Test L1 Norm: 0.0175, Train Linf Norm: 3.5094, Test Linf Norm: 0.8057\n",
            "Epoch 21: Train Loss: 0.0002, Test Loss: 0.0003, Train L1 Norm: 0.0466, Test L1 Norm: 0.0173, Train Linf Norm: 3.6013, Test Linf Norm: 0.7759\n",
            "Epoch 22: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0469, Test L1 Norm: 0.0165, Train Linf Norm: 3.6273, Test Linf Norm: 0.7125\n",
            "Epoch 23: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0440, Test L1 Norm: 0.0158, Train Linf Norm: 3.3252, Test Linf Norm: 0.6545\n",
            "Epoch 24: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0459, Test L1 Norm: 0.0167, Train Linf Norm: 3.5628, Test Linf Norm: 0.7549\n",
            "Epoch 25: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0451, Test L1 Norm: 0.0171, Train Linf Norm: 3.4755, Test Linf Norm: 0.7912\n",
            "Epoch 26: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0439, Test L1 Norm: 0.0167, Train Linf Norm: 3.3501, Test Linf Norm: 0.7574\n",
            "Epoch 27: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0439, Test L1 Norm: 0.0163, Train Linf Norm: 3.3870, Test Linf Norm: 0.7274\n",
            "Epoch 28: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0442, Test L1 Norm: 0.0164, Train Linf Norm: 3.4001, Test Linf Norm: 0.7310\n",
            "Epoch 29: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0438, Test L1 Norm: 0.0167, Train Linf Norm: 3.3666, Test Linf Norm: 0.7626\n",
            "Epoch 30: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0451, Test L1 Norm: 0.0166, Train Linf Norm: 3.5118, Test Linf Norm: 0.7542\n",
            "Epoch 31: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0449, Test L1 Norm: 0.0164, Train Linf Norm: 3.4849, Test Linf Norm: 0.7363\n",
            "Epoch 32: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0446, Test L1 Norm: 0.0164, Train Linf Norm: 3.4282, Test Linf Norm: 0.7379\n",
            "Epoch 33: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0446, Test L1 Norm: 0.0164, Train Linf Norm: 3.4448, Test Linf Norm: 0.7361\n",
            "Epoch 34: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0445, Test L1 Norm: 0.0164, Train Linf Norm: 3.4308, Test Linf Norm: 0.7359\n",
            "Epoch 35: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0445, Test L1 Norm: 0.0164, Train Linf Norm: 3.4394, Test Linf Norm: 0.7358\n",
            "Epoch 36: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0444, Test L1 Norm: 0.0163, Train Linf Norm: 3.4240, Test Linf Norm: 0.7322\n",
            "Epoch 37: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0435, Test L1 Norm: 0.0165, Train Linf Norm: 3.3380, Test Linf Norm: 0.7460\n",
            "Epoch 38: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0444, Test L1 Norm: 0.0163, Train Linf Norm: 3.4307, Test Linf Norm: 0.7306\n",
            "Epoch 39: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0455, Test L1 Norm: 0.0163, Train Linf Norm: 3.5364, Test Linf Norm: 0.7312\n",
            "Epoch 40: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0445, Test L1 Norm: 0.0166, Train Linf Norm: 3.4325, Test Linf Norm: 0.7579\n",
            "Epoch 41: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0439, Test L1 Norm: 0.0162, Train Linf Norm: 3.3740, Test Linf Norm: 0.7219\n",
            "Epoch 42: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0441, Test L1 Norm: 0.0162, Train Linf Norm: 3.3902, Test Linf Norm: 0.7217\n",
            "Epoch 43: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0424, Test L1 Norm: 0.0162, Train Linf Norm: 3.2505, Test Linf Norm: 0.7284\n",
            "Epoch 44: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0438, Test L1 Norm: 0.0152, Train Linf Norm: 3.3846, Test Linf Norm: 0.6309\n",
            "Epoch 45: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0434, Test L1 Norm: 0.0160, Train Linf Norm: 3.3532, Test Linf Norm: 0.7201\n",
            "Epoch 46: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0426, Test L1 Norm: 0.0162, Train Linf Norm: 3.2714, Test Linf Norm: 0.7397\n",
            "Epoch 47: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0412, Test L1 Norm: 0.0153, Train Linf Norm: 3.1514, Test Linf Norm: 0.6579\n",
            "Epoch 48: Train Loss: 0.0002, Test Loss: 0.0003, Train L1 Norm: 0.0392, Test L1 Norm: 0.0156, Train Linf Norm: 2.9460, Test Linf Norm: 0.6759\n",
            "Epoch 49: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0422, Test L1 Norm: 0.0159, Train Linf Norm: 3.2287, Test Linf Norm: 0.7181\n",
            "Epoch 50: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0415, Test L1 Norm: 0.0156, Train Linf Norm: 3.1920, Test Linf Norm: 0.7066\n",
            "Epoch 51: Train Loss: 0.0002, Test Loss: 0.0003, Train L1 Norm: 0.0380, Test L1 Norm: 0.0157, Train Linf Norm: 2.8578, Test Linf Norm: 0.6844\n",
            "Epoch 52: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0153, Train Linf Norm: 2.6006, Test Linf Norm: 0.6687\n",
            "Epoch 53: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0420, Test L1 Norm: 0.0143, Train Linf Norm: 3.2304, Test Linf Norm: 0.5889\n",
            "Epoch 54: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0369, Test L1 Norm: 0.0140, Train Linf Norm: 2.7374, Test Linf Norm: 0.5659\n",
            "Epoch 55: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0387, Test L1 Norm: 0.0155, Train Linf Norm: 2.9272, Test Linf Norm: 0.7042\n",
            "Epoch 56: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.0360, Test L1 Norm: 0.0143, Train Linf Norm: 2.6556, Test Linf Norm: 0.5627\n",
            "Epoch 57: Train Loss: 0.0007, Test Loss: 0.0002, Train L1 Norm: 0.0410, Test L1 Norm: 0.0141, Train Linf Norm: 3.1057, Test Linf Norm: 0.6012\n",
            "Epoch 58: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.0335, Test L1 Norm: 0.0146, Train Linf Norm: 2.3985, Test Linf Norm: 0.6466\n",
            "Epoch 59: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0327, Test L1 Norm: 0.0142, Train Linf Norm: 2.3707, Test Linf Norm: 0.6070\n",
            "Epoch 60: Train Loss: 0.0019, Test Loss: 0.0004, Train L1 Norm: 0.0401, Test L1 Norm: 0.0143, Train Linf Norm: 2.8916, Test Linf Norm: 0.5486\n",
            "Epoch 61: Train Loss: 0.0021, Test Loss: 0.0013, Train L1 Norm: 0.0337, Test L1 Norm: 0.0166, Train Linf Norm: 2.2235, Test Linf Norm: 0.5784\n",
            "Epoch 62: Train Loss: 0.0020, Test Loss: 0.0005, Train L1 Norm: 0.0401, Test L1 Norm: 0.0168, Train Linf Norm: 2.9139, Test Linf Norm: 0.7138\n",
            "Epoch 63: Train Loss: 0.0005, Test Loss: 0.0002, Train L1 Norm: 0.0341, Test L1 Norm: 0.0147, Train Linf Norm: 2.4149, Test Linf Norm: 0.6650\n",
            "Epoch 64: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.0315, Test L1 Norm: 0.0135, Train Linf Norm: 2.2445, Test Linf Norm: 0.5469\n",
            "Epoch 65: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.0283, Test L1 Norm: 0.0161, Train Linf Norm: 1.9292, Test Linf Norm: 0.7956\n",
            "Epoch 66: Train Loss: 0.0005, Test Loss: 0.0002, Train L1 Norm: 0.0319, Test L1 Norm: 0.0136, Train Linf Norm: 2.2784, Test Linf Norm: 0.6035\n",
            "Epoch 67: Train Loss: 0.0007, Test Loss: 0.0002, Train L1 Norm: 0.0284, Test L1 Norm: 0.0131, Train Linf Norm: 1.9285, Test Linf Norm: 0.5562\n",
            "Epoch 68: Train Loss: 0.0006, Test Loss: 0.0002, Train L1 Norm: 0.0294, Test L1 Norm: 0.0140, Train Linf Norm: 2.0221, Test Linf Norm: 0.5382\n",
            "Epoch 69: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.0280, Test L1 Norm: 0.0138, Train Linf Norm: 1.9446, Test Linf Norm: 0.5641\n",
            "Epoch 70: Train Loss: 0.0011, Test Loss: 0.0003, Train L1 Norm: 0.0229, Test L1 Norm: 0.0134, Train Linf Norm: 1.4065, Test Linf Norm: 0.5656\n",
            "Epoch 71: Train Loss: 0.0010, Test Loss: 0.0003, Train L1 Norm: 0.0285, Test L1 Norm: 0.0130, Train Linf Norm: 1.9424, Test Linf Norm: 0.5251\n",
            "Epoch 72: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0248, Test L1 Norm: 0.0141, Train Linf Norm: 1.6757, Test Linf Norm: 0.6531\n",
            "Epoch 73: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.0292, Test L1 Norm: 0.0125, Train Linf Norm: 2.1007, Test Linf Norm: 0.5141\n",
            "Epoch 74: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0277, Test L1 Norm: 0.0121, Train Linf Norm: 1.9715, Test Linf Norm: 0.4905\n",
            "Epoch 75: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0282, Test L1 Norm: 0.0122, Train Linf Norm: 2.0414, Test Linf Norm: 0.4843\n",
            "Epoch 76: Train Loss: 0.0002, Test Loss: 0.0003, Train L1 Norm: 0.0253, Test L1 Norm: 0.0131, Train Linf Norm: 1.7514, Test Linf Norm: 0.5404\n",
            "Epoch 77: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0213, Test L1 Norm: 0.0123, Train Linf Norm: 1.3993, Test Linf Norm: 0.5113\n",
            "Epoch 78: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0245, Test L1 Norm: 0.0122, Train Linf Norm: 1.7161, Test Linf Norm: 0.5250\n",
            "Epoch 79: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0267, Test L1 Norm: 0.0125, Train Linf Norm: 1.9222, Test Linf Norm: 0.5361\n",
            "Epoch 80: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0228, Test L1 Norm: 0.0122, Train Linf Norm: 1.5696, Test Linf Norm: 0.5124\n",
            "Epoch 81: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0241, Test L1 Norm: 0.0119, Train Linf Norm: 1.6995, Test Linf Norm: 0.4769\n",
            "Epoch 82: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0244, Test L1 Norm: 0.0117, Train Linf Norm: 1.7372, Test Linf Norm: 0.4822\n",
            "Epoch 83: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0219, Test L1 Norm: 0.0123, Train Linf Norm: 1.4966, Test Linf Norm: 0.5481\n",
            "Epoch 84: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0219, Test L1 Norm: 0.0123, Train Linf Norm: 1.5049, Test Linf Norm: 0.5508\n",
            "Epoch 85: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0276, Test L1 Norm: 0.0118, Train Linf Norm: 2.0495, Test Linf Norm: 0.4970\n",
            "Epoch 86: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0225, Test L1 Norm: 0.0120, Train Linf Norm: 1.5618, Test Linf Norm: 0.4959\n",
            "Epoch 87: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0203, Test L1 Norm: 0.0121, Train Linf Norm: 1.3531, Test Linf Norm: 0.5306\n",
            "Epoch 88: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0206, Test L1 Norm: 0.0119, Train Linf Norm: 1.3396, Test Linf Norm: 0.5166\n",
            "Epoch 89: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0209, Test L1 Norm: 0.0116, Train Linf Norm: 1.4244, Test Linf Norm: 0.4833\n",
            "Epoch 90: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0217, Test L1 Norm: 0.0118, Train Linf Norm: 1.5060, Test Linf Norm: 0.5108\n",
            "Epoch 91: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0207, Test L1 Norm: 0.0114, Train Linf Norm: 1.4031, Test Linf Norm: 0.4728\n",
            "Epoch 92: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0217, Test L1 Norm: 0.0118, Train Linf Norm: 1.4760, Test Linf Norm: 0.5104\n",
            "Epoch 93: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0209, Test L1 Norm: 0.0116, Train Linf Norm: 1.4201, Test Linf Norm: 0.4896\n",
            "Epoch 94: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0205, Test L1 Norm: 0.0115, Train Linf Norm: 1.3923, Test Linf Norm: 0.4851\n",
            "Epoch 95: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0202, Test L1 Norm: 0.0116, Train Linf Norm: 1.3618, Test Linf Norm: 0.4891\n",
            "Epoch 96: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0212, Test L1 Norm: 0.0114, Train Linf Norm: 1.4520, Test Linf Norm: 0.4741\n",
            "Epoch 97: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0203, Test L1 Norm: 0.0114, Train Linf Norm: 1.3694, Test Linf Norm: 0.4753\n",
            "Epoch 98: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0205, Test L1 Norm: 0.0114, Train Linf Norm: 1.3798, Test Linf Norm: 0.4723\n",
            "Epoch 99: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0203, Test L1 Norm: 0.0114, Train Linf Norm: 1.3748, Test Linf Norm: 0.4728\n",
            "Epoch 100: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0203, Test L1 Norm: 0.0114, Train Linf Norm: 1.3781, Test Linf Norm: 0.4727\n",
            "Epoch 101: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0204, Test L1 Norm: 0.0114, Train Linf Norm: 1.3678, Test Linf Norm: 0.4729\n",
            "Epoch 102: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0205, Test L1 Norm: 0.0114, Train Linf Norm: 1.3796, Test Linf Norm: 0.4762\n",
            "Epoch 103: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0203, Test L1 Norm: 0.0114, Train Linf Norm: 1.3813, Test Linf Norm: 0.4787\n",
            "Epoch 104: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0201, Test L1 Norm: 0.0114, Train Linf Norm: 1.3037, Test Linf Norm: 0.4795\n",
            "Epoch 105: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0206, Test L1 Norm: 0.0114, Train Linf Norm: 1.3846, Test Linf Norm: 0.4733\n",
            "Epoch 106: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0207, Test L1 Norm: 0.0115, Train Linf Norm: 1.4061, Test Linf Norm: 0.4799\n",
            "Epoch 107: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0214, Test L1 Norm: 0.0114, Train Linf Norm: 1.4640, Test Linf Norm: 0.4710\n",
            "Epoch 108: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0198, Test L1 Norm: 0.0113, Train Linf Norm: 1.3300, Test Linf Norm: 0.4683\n",
            "Epoch 109: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0213, Test L1 Norm: 0.0113, Train Linf Norm: 1.4613, Test Linf Norm: 0.4704\n",
            "Epoch 110: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0210, Test L1 Norm: 0.0113, Train Linf Norm: 1.4310, Test Linf Norm: 0.4706\n",
            "Epoch 111: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0223, Test L1 Norm: 0.0113, Train Linf Norm: 1.5570, Test Linf Norm: 0.4677\n",
            "Epoch 112: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0190, Test L1 Norm: 0.0113, Train Linf Norm: 1.2365, Test Linf Norm: 0.4721\n",
            "Epoch 113: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0201, Test L1 Norm: 0.0118, Train Linf Norm: 1.3533, Test Linf Norm: 0.5136\n",
            "Epoch 114: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0210, Test L1 Norm: 0.0117, Train Linf Norm: 1.4422, Test Linf Norm: 0.4981\n",
            "Epoch 115: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0194, Test L1 Norm: 0.0113, Train Linf Norm: 1.2855, Test Linf Norm: 0.4681\n",
            "Epoch 116: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0208, Test L1 Norm: 0.0114, Train Linf Norm: 1.4091, Test Linf Norm: 0.4704\n",
            "Epoch 117: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0197, Test L1 Norm: 0.0113, Train Linf Norm: 1.3067, Test Linf Norm: 0.4722\n",
            "Epoch 118: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0208, Test L1 Norm: 0.0121, Train Linf Norm: 1.4190, Test Linf Norm: 0.5420\n",
            "Epoch 119: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0251, Test L1 Norm: 0.0121, Train Linf Norm: 1.8421, Test Linf Norm: 0.5441\n",
            "Epoch 120: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0209, Test L1 Norm: 0.0119, Train Linf Norm: 1.4306, Test Linf Norm: 0.5294\n",
            "Epoch 121: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0200, Test L1 Norm: 0.0116, Train Linf Norm: 1.3403, Test Linf Norm: 0.5035\n",
            "Epoch 122: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0235, Test L1 Norm: 0.0116, Train Linf Norm: 1.6658, Test Linf Norm: 0.5030\n",
            "Epoch 123: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0231, Test L1 Norm: 0.0122, Train Linf Norm: 1.6020, Test Linf Norm: 0.4906\n",
            "Epoch 124: Train Loss: 0.0002, Test Loss: 0.0003, Train L1 Norm: 0.0202, Test L1 Norm: 0.0116, Train Linf Norm: 1.3397, Test Linf Norm: 0.4759\n",
            "Epoch 125: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0191, Test L1 Norm: 0.0127, Train Linf Norm: 1.2765, Test Linf Norm: 0.5492\n",
            "Epoch 126: Train Loss: 0.0017, Test Loss: 0.0004, Train L1 Norm: 0.0239, Test L1 Norm: 0.0125, Train Linf Norm: 1.4999, Test Linf Norm: 0.5249\n",
            "Epoch 127: Train Loss: 0.0009, Test Loss: 0.0001, Train L1 Norm: 0.0290, Test L1 Norm: 0.0119, Train Linf Norm: 2.0650, Test Linf Norm: 0.5194\n",
            "Epoch 128: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0187, Test L1 Norm: 0.0116, Train Linf Norm: 1.1621, Test Linf Norm: 0.4906\n",
            "Epoch 129: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0172, Test L1 Norm: 0.0117, Train Linf Norm: 1.0575, Test Linf Norm: 0.4946\n",
            "Epoch 130: Train Loss: 0.0010, Test Loss: 0.0001, Train L1 Norm: 0.0220, Test L1 Norm: 0.0125, Train Linf Norm: 1.3968, Test Linf Norm: 0.5508\n",
            "Epoch 131: Train Loss: 0.0005, Test Loss: 0.0002, Train L1 Norm: 0.0203, Test L1 Norm: 0.0112, Train Linf Norm: 1.2941, Test Linf Norm: 0.4801\n",
            "Epoch 132: Train Loss: 0.0009, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0119, Train Linf Norm: 1.1784, Test Linf Norm: 0.4741\n",
            "Epoch 133: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0180, Test L1 Norm: 0.0110, Train Linf Norm: 1.1132, Test Linf Norm: 0.4439\n",
            "Epoch 134: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0433, Test L1 Norm: 0.0113, Train Linf Norm: 3.5590, Test Linf Norm: 0.4840\n",
            "Epoch 135: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0200, Test L1 Norm: 0.0125, Train Linf Norm: 1.3274, Test Linf Norm: 0.5123\n",
            "Epoch 136: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0161, Test L1 Norm: 0.0116, Train Linf Norm: 0.9436, Test Linf Norm: 0.4774\n",
            "Epoch 137: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0172, Test L1 Norm: 0.0110, Train Linf Norm: 1.0766, Test Linf Norm: 0.4638\n",
            "Epoch 138: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0189, Test L1 Norm: 0.0112, Train Linf Norm: 1.2610, Test Linf Norm: 0.4594\n",
            "Epoch 139: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0184, Test L1 Norm: 0.0111, Train Linf Norm: 1.1996, Test Linf Norm: 0.4922\n",
            "Epoch 140: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0177, Test L1 Norm: 0.0108, Train Linf Norm: 1.1477, Test Linf Norm: 0.4589\n",
            "Epoch 141: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0213, Test L1 Norm: 0.0108, Train Linf Norm: 1.4880, Test Linf Norm: 0.4656\n",
            "Epoch 142: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0195, Test L1 Norm: 0.0110, Train Linf Norm: 1.3290, Test Linf Norm: 0.4521\n",
            "Epoch 143: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0179, Test L1 Norm: 0.0110, Train Linf Norm: 1.1869, Test Linf Norm: 0.4621\n",
            "Epoch 144: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0168, Test L1 Norm: 0.0114, Train Linf Norm: 1.0743, Test Linf Norm: 0.5105\n",
            "Epoch 145: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0171, Test L1 Norm: 0.0107, Train Linf Norm: 1.1241, Test Linf Norm: 0.4493\n",
            "Epoch 146: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0167, Test L1 Norm: 0.0110, Train Linf Norm: 1.0744, Test Linf Norm: 0.4710\n",
            "Epoch 147: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0170, Test L1 Norm: 0.0106, Train Linf Norm: 1.1109, Test Linf Norm: 0.4615\n",
            "Epoch 148: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0165, Test L1 Norm: 0.0111, Train Linf Norm: 1.0674, Test Linf Norm: 0.4912\n",
            "Epoch 149: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0154, Test L1 Norm: 0.0104, Train Linf Norm: 0.9731, Test Linf Norm: 0.4447\n",
            "Epoch 150: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0142, Test L1 Norm: 0.0105, Train Linf Norm: 0.8604, Test Linf Norm: 0.4476\n",
            "Epoch 151: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0170, Test L1 Norm: 0.0106, Train Linf Norm: 1.1110, Test Linf Norm: 0.4555\n",
            "Epoch 152: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0139, Test L1 Norm: 0.0104, Train Linf Norm: 0.8276, Test Linf Norm: 0.4482\n",
            "Epoch 153: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0146, Test L1 Norm: 0.0106, Train Linf Norm: 0.8794, Test Linf Norm: 0.4556\n",
            "Epoch 154: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0150, Test L1 Norm: 0.0104, Train Linf Norm: 0.9130, Test Linf Norm: 0.4464\n",
            "Epoch 155: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0152, Test L1 Norm: 0.0104, Train Linf Norm: 0.9548, Test Linf Norm: 0.4474\n",
            "Epoch 156: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0150, Test L1 Norm: 0.0103, Train Linf Norm: 0.9497, Test Linf Norm: 0.4454\n",
            "Epoch 157: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0135, Test L1 Norm: 0.0104, Train Linf Norm: 0.7975, Test Linf Norm: 0.4477\n",
            "Epoch 158: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0154, Test L1 Norm: 0.0105, Train Linf Norm: 0.9872, Test Linf Norm: 0.4513\n",
            "Epoch 159: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0145, Test L1 Norm: 0.0104, Train Linf Norm: 0.8965, Test Linf Norm: 0.4467\n",
            "Epoch 160: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0142, Test L1 Norm: 0.0104, Train Linf Norm: 0.8626, Test Linf Norm: 0.4478\n",
            "Epoch 161: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0142, Test L1 Norm: 0.0104, Train Linf Norm: 0.8682, Test Linf Norm: 0.4478\n",
            "Epoch 162: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0140, Test L1 Norm: 0.0103, Train Linf Norm: 0.8402, Test Linf Norm: 0.4463\n",
            "Epoch 163: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0139, Test L1 Norm: 0.0103, Train Linf Norm: 0.8368, Test Linf Norm: 0.4455\n",
            "Epoch 164: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0143, Test L1 Norm: 0.0104, Train Linf Norm: 0.8689, Test Linf Norm: 0.4471\n",
            "Epoch 165: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0142, Test L1 Norm: 0.0104, Train Linf Norm: 0.8648, Test Linf Norm: 0.4471\n",
            "Epoch 166: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0141, Test L1 Norm: 0.0104, Train Linf Norm: 0.8572, Test Linf Norm: 0.4471\n",
            "Epoch 167: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0141, Test L1 Norm: 0.0103, Train Linf Norm: 0.8205, Test Linf Norm: 0.4471\n",
            "Epoch 168: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0141, Test L1 Norm: 0.0103, Train Linf Norm: 0.8604, Test Linf Norm: 0.4465\n",
            "Epoch 169: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0141, Test L1 Norm: 0.0104, Train Linf Norm: 0.8392, Test Linf Norm: 0.4477\n",
            "Epoch 170: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0140, Test L1 Norm: 0.0103, Train Linf Norm: 0.8499, Test Linf Norm: 0.4459\n",
            "Epoch 171: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0138, Test L1 Norm: 0.0103, Train Linf Norm: 0.8281, Test Linf Norm: 0.4473\n",
            "Epoch 172: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0137, Test L1 Norm: 0.0103, Train Linf Norm: 0.8272, Test Linf Norm: 0.4446\n",
            "Epoch 173: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0144, Test L1 Norm: 0.0103, Train Linf Norm: 0.8690, Test Linf Norm: 0.4440\n",
            "Epoch 174: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0142, Test L1 Norm: 0.0103, Train Linf Norm: 0.8598, Test Linf Norm: 0.4456\n",
            "Epoch 175: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0139, Test L1 Norm: 0.0104, Train Linf Norm: 0.8400, Test Linf Norm: 0.4474\n",
            "Epoch 176: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0157, Test L1 Norm: 0.0104, Train Linf Norm: 0.9988, Test Linf Norm: 0.4490\n",
            "Epoch 177: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0144, Test L1 Norm: 0.0103, Train Linf Norm: 0.8786, Test Linf Norm: 0.4432\n",
            "Epoch 178: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0149, Test L1 Norm: 0.0102, Train Linf Norm: 0.9373, Test Linf Norm: 0.4406\n",
            "Epoch 179: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0130, Test L1 Norm: 0.0103, Train Linf Norm: 0.7560, Test Linf Norm: 0.4429\n",
            "Epoch 180: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0148, Test L1 Norm: 0.0104, Train Linf Norm: 0.9113, Test Linf Norm: 0.4519\n",
            "Epoch 181: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0151, Test L1 Norm: 0.0105, Train Linf Norm: 0.9484, Test Linf Norm: 0.4572\n",
            "Epoch 182: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0143, Test L1 Norm: 0.0102, Train Linf Norm: 0.8693, Test Linf Norm: 0.4412\n",
            "Epoch 183: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0141, Test L1 Norm: 0.0104, Train Linf Norm: 0.8596, Test Linf Norm: 0.4451\n",
            "Epoch 184: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0158, Test L1 Norm: 0.0104, Train Linf Norm: 1.0342, Test Linf Norm: 0.4500\n",
            "Epoch 185: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0142, Test L1 Norm: 0.0104, Train Linf Norm: 0.8693, Test Linf Norm: 0.4459\n",
            "Epoch 186: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0137, Test L1 Norm: 0.0105, Train Linf Norm: 0.8196, Test Linf Norm: 0.4739\n",
            "Epoch 187: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0149, Test L1 Norm: 0.0107, Train Linf Norm: 0.9358, Test Linf Norm: 0.4401\n",
            "Epoch 188: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0149, Test L1 Norm: 0.0102, Train Linf Norm: 0.9288, Test Linf Norm: 0.4441\n",
            "Epoch 189: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0166, Test L1 Norm: 0.0101, Train Linf Norm: 1.0925, Test Linf Norm: 0.4388\n",
            "Epoch 190: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0143, Test L1 Norm: 0.0104, Train Linf Norm: 0.8682, Test Linf Norm: 0.4551\n",
            "Epoch 191: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0163, Test L1 Norm: 0.0101, Train Linf Norm: 1.0580, Test Linf Norm: 0.4351\n",
            "Epoch 192: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0150, Test L1 Norm: 0.0106, Train Linf Norm: 0.9428, Test Linf Norm: 0.4419\n",
            "Epoch 193: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0152, Test L1 Norm: 0.0111, Train Linf Norm: 0.9490, Test Linf Norm: 0.5224\n",
            "Epoch 194: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0160, Test L1 Norm: 0.0103, Train Linf Norm: 1.0413, Test Linf Norm: 0.4467\n",
            "Epoch 195: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0178, Test L1 Norm: 0.0105, Train Linf Norm: 1.1847, Test Linf Norm: 0.4540\n",
            "Epoch 196: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0162, Test L1 Norm: 0.0110, Train Linf Norm: 1.0465, Test Linf Norm: 0.4754\n",
            "Epoch 197: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0145, Test L1 Norm: 0.0102, Train Linf Norm: 0.8562, Test Linf Norm: 0.4505\n",
            "Epoch 198: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0179, Test L1 Norm: 0.0101, Train Linf Norm: 1.1972, Test Linf Norm: 0.4446\n",
            "Epoch 199: Train Loss: 0.0004, Test Loss: 0.0060, Train L1 Norm: 0.0196, Test L1 Norm: 0.0272, Train Linf Norm: 1.3145, Test Linf Norm: 0.9581\n",
            "Epoch 200: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0166, Test L1 Norm: 0.0102, Train Linf Norm: 1.0648, Test Linf Norm: 0.4432\n",
            "Epoch 201: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0222, Test L1 Norm: 0.0100, Train Linf Norm: 1.6220, Test Linf Norm: 0.4407\n",
            "Epoch 202: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0176, Test L1 Norm: 0.0098, Train Linf Norm: 1.1931, Test Linf Norm: 0.4245\n",
            "Epoch 203: Train Loss: 0.0013, Test Loss: 0.0001, Train L1 Norm: 0.0243, Test L1 Norm: 0.0108, Train Linf Norm: 1.6701, Test Linf Norm: 0.5009\n",
            "Epoch 204: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0158, Test L1 Norm: 0.0101, Train Linf Norm: 1.0121, Test Linf Norm: 0.4501\n",
            "Epoch 205: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0171, Test L1 Norm: 0.0099, Train Linf Norm: 1.0977, Test Linf Norm: 0.4419\n",
            "Epoch 206: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0157, Test L1 Norm: 0.0104, Train Linf Norm: 1.0187, Test Linf Norm: 0.4531\n",
            "Epoch 207: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0150, Test L1 Norm: 0.0101, Train Linf Norm: 0.9516, Test Linf Norm: 0.4500\n",
            "Epoch 208: Train Loss: 0.0001, Test Loss: 0.0006, Train L1 Norm: 0.0173, Test L1 Norm: 0.0117, Train Linf Norm: 1.1859, Test Linf Norm: 0.4620\n",
            "Epoch 209: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0170, Test L1 Norm: 0.0112, Train Linf Norm: 1.1631, Test Linf Norm: 0.4973\n",
            "Epoch 210: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0173, Test L1 Norm: 0.0098, Train Linf Norm: 1.2014, Test Linf Norm: 0.4366\n",
            "Epoch 211: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0142, Test L1 Norm: 0.0100, Train Linf Norm: 0.8965, Test Linf Norm: 0.4492\n",
            "Epoch 212: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0170, Test L1 Norm: 0.0098, Train Linf Norm: 1.1687, Test Linf Norm: 0.4373\n",
            "Epoch 213: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0184, Test L1 Norm: 0.0098, Train Linf Norm: 1.3067, Test Linf Norm: 0.4368\n",
            "Epoch 214: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0172, Test L1 Norm: 0.0099, Train Linf Norm: 1.1825, Test Linf Norm: 0.4411\n",
            "Epoch 215: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0184, Test L1 Norm: 0.0098, Train Linf Norm: 1.3199, Test Linf Norm: 0.4411\n",
            "Epoch 216: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0176, Test L1 Norm: 0.0101, Train Linf Norm: 1.2301, Test Linf Norm: 0.4473\n",
            "Epoch 217: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0183, Test L1 Norm: 0.0098, Train Linf Norm: 1.2950, Test Linf Norm: 0.4371\n",
            "Epoch 218: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0163, Test L1 Norm: 0.0098, Train Linf Norm: 1.1108, Test Linf Norm: 0.4399\n",
            "Epoch 219: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0164, Test L1 Norm: 0.0098, Train Linf Norm: 1.1326, Test Linf Norm: 0.4404\n",
            "Epoch 220: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0178, Test L1 Norm: 0.0098, Train Linf Norm: 1.2690, Test Linf Norm: 0.4402\n",
            "Epoch 221: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0164, Test L1 Norm: 0.0098, Train Linf Norm: 1.1296, Test Linf Norm: 0.4402\n",
            "Epoch 222: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0171, Test L1 Norm: 0.0098, Train Linf Norm: 1.1953, Test Linf Norm: 0.4404\n",
            "Epoch 223: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0175, Test L1 Norm: 0.0097, Train Linf Norm: 1.2289, Test Linf Norm: 0.4348\n",
            "Epoch 224: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0169, Test L1 Norm: 0.0098, Train Linf Norm: 1.1741, Test Linf Norm: 0.4402\n",
            "Epoch 225: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0169, Test L1 Norm: 0.0097, Train Linf Norm: 1.1723, Test Linf Norm: 0.4362\n",
            "Epoch 226: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0174, Test L1 Norm: 0.0097, Train Linf Norm: 1.2238, Test Linf Norm: 0.4361\n",
            "Epoch 227: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0163, Test L1 Norm: 0.0097, Train Linf Norm: 1.1194, Test Linf Norm: 0.4367\n",
            "Epoch 228: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0169, Test L1 Norm: 0.0097, Train Linf Norm: 1.1797, Test Linf Norm: 0.4360\n",
            "Epoch 229: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0170, Test L1 Norm: 0.0097, Train Linf Norm: 1.1828, Test Linf Norm: 0.4368\n",
            "Epoch 230: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0169, Test L1 Norm: 0.0097, Train Linf Norm: 1.1576, Test Linf Norm: 0.4361\n",
            "Epoch 231: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0167, Test L1 Norm: 0.0097, Train Linf Norm: 1.1612, Test Linf Norm: 0.4360\n",
            "Epoch 232: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0168, Test L1 Norm: 0.0097, Train Linf Norm: 1.1751, Test Linf Norm: 0.4360\n",
            "Epoch 233: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0167, Test L1 Norm: 0.0097, Train Linf Norm: 1.1619, Test Linf Norm: 0.4363\n",
            "Epoch 234: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0168, Test L1 Norm: 0.0097, Train Linf Norm: 1.1615, Test Linf Norm: 0.4367\n",
            "Epoch 235: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0170, Test L1 Norm: 0.0097, Train Linf Norm: 1.1778, Test Linf Norm: 0.4361\n",
            "Epoch 236: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0168, Test L1 Norm: 0.0097, Train Linf Norm: 1.1514, Test Linf Norm: 0.4361\n",
            "Epoch 237: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0164, Test L1 Norm: 0.0097, Train Linf Norm: 1.1284, Test Linf Norm: 0.4348\n",
            "Epoch 238: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0169, Test L1 Norm: 0.0097, Train Linf Norm: 1.1759, Test Linf Norm: 0.4344\n",
            "Epoch 239: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0169, Test L1 Norm: 0.0097, Train Linf Norm: 1.1806, Test Linf Norm: 0.4357\n",
            "Epoch 240: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0168, Test L1 Norm: 0.0097, Train Linf Norm: 1.1619, Test Linf Norm: 0.4356\n",
            "Epoch 241: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0170, Test L1 Norm: 0.0097, Train Linf Norm: 1.1828, Test Linf Norm: 0.4346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 19:32:23,117]\u001b[0m Trial 25 finished with value: 0.009776758530735969 and parameters: {'n_layers': 6, 'n_units_0': 1832, 'n_units_1': 1942, 'n_units_2': 1349, 'n_units_3': 1498, 'n_units_4': 663, 'n_units_5': 61, 'hidden_activation': 'LeakyReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adagrad', 'lr': 0.0005297633347494139, 'batch_size': 96, 'n_epochs': 242, 'scheduler': 'CosineAnnealingLR', 't_max_fraction': 0.1402257780967777, 'eta_min': 1.1940333856975267e-07}. Best is trial 24 with value: 0.00859525605328381.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 242: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0164, Test L1 Norm: 0.0098, Train Linf Norm: 1.1161, Test Linf Norm: 0.4387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 19:32:26,655]\u001b[0m Trial 26 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.4500, Test Loss: 0.1788, Train L1 Norm: 0.9391, Test L1 Norm: 0.1316, Train Linf Norm: 96.1667, Test Linf Norm: 3.4656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 19:32:35,136]\u001b[0m Trial 27 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.8818, Test Loss: 0.1010, Train L1 Norm: 2.9475, Test L1 Norm: 0.7845, Train Linf Norm: 123.1633, Test Linf Norm: 27.0765\n",
            "Epoch 1: Train Loss: 1.2430, Test Loss: 0.1339, Train L1 Norm: 1.1503, Test L1 Norm: 0.1266, Train Linf Norm: 32.5970, Test Linf Norm: 2.7458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 19:32:52,170]\u001b[0m Trial 28 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 0.3278, Test Loss: 0.4675, Train L1 Norm: 0.4718, Test L1 Norm: 0.1834, Train Linf Norm: 17.1963, Test Linf Norm: 1.7952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/pruners/_percentile.py:21: RuntimeWarning: All-NaN slice encountered\n",
            "  return np.nanmin(values)\n",
            "\u001b[32m[I 2023-05-21 19:32:53,826]\u001b[0m Trial 29 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: nan, Test Loss: nan, Train L1 Norm: nan, Test L1 Norm: nan, Train Linf Norm: nan, Test Linf Norm: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 19:33:01,815]\u001b[0m Trial 30 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.8329, Test Loss: 0.0088, Train L1 Norm: 0.4017, Test L1 Norm: 0.1524, Train Linf Norm: 13.1532, Test Linf Norm: 4.9126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 19:33:07,594]\u001b[0m Trial 31 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.5041, Test Loss: 0.0353, Train L1 Norm: 0.4841, Test L1 Norm: 0.1358, Train Linf Norm: 32.2866, Test Linf Norm: 6.2313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 19:33:13,109]\u001b[0m Trial 32 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.4772, Test Loss: 0.0123, Train L1 Norm: 0.9247, Test L1 Norm: 0.1675, Train Linf Norm: 72.2379, Test Linf Norm: 9.4231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 19:33:18,287]\u001b[0m Trial 33 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.4485, Test Loss: 0.0132, Train L1 Norm: 0.8761, Test L1 Norm: 0.1395, Train Linf Norm: 67.3506, Test Linf Norm: 6.8679\n",
            "Epoch 1: Train Loss: 0.2697, Test Loss: 0.0756, Train L1 Norm: 0.2902, Test L1 Norm: 0.0525, Train Linf Norm: 11.5005, Test Linf Norm: 1.0944\n",
            "Epoch 2: Train Loss: 0.0960, Test Loss: 0.0841, Train L1 Norm: 0.1504, Test L1 Norm: 0.0613, Train Linf Norm: 7.0777, Test Linf Norm: 1.6177\n",
            "Epoch 3: Train Loss: 0.0707, Test Loss: 0.0465, Train L1 Norm: 0.1205, Test L1 Norm: 0.0345, Train Linf Norm: 5.8629, Test Linf Norm: 1.0848\n",
            "Epoch 4: Train Loss: 0.0573, Test Loss: 0.0434, Train L1 Norm: 0.0749, Test L1 Norm: 0.0247, Train Linf Norm: 3.2499, Test Linf Norm: 0.4322\n",
            "Epoch 5: Train Loss: 0.0493, Test Loss: 0.0550, Train L1 Norm: 0.0690, Test L1 Norm: 0.0283, Train Linf Norm: 3.0935, Test Linf Norm: 0.5392\n",
            "Epoch 6: Train Loss: 0.0428, Test Loss: 0.0646, Train L1 Norm: 0.0737, Test L1 Norm: 0.0295, Train Linf Norm: 3.5822, Test Linf Norm: 0.5853\n",
            "Epoch 7: Train Loss: 0.0382, Test Loss: 0.0263, Train L1 Norm: 0.0847, Test L1 Norm: 0.0172, Train Linf Norm: 4.4148, Test Linf Norm: 0.4665\n",
            "Epoch 8: Train Loss: 0.0337, Test Loss: 0.0462, Train L1 Norm: 0.0641, Test L1 Norm: 0.0232, Train Linf Norm: 3.2004, Test Linf Norm: 0.4784\n",
            "Epoch 9: Train Loss: 0.0301, Test Loss: 0.0377, Train L1 Norm: 0.0484, Test L1 Norm: 0.0263, Train Linf Norm: 2.2903, Test Linf Norm: 0.7175\n",
            "Epoch 10: Train Loss: 0.0271, Test Loss: 0.0484, Train L1 Norm: 0.0484, Test L1 Norm: 0.0224, Train Linf Norm: 2.3680, Test Linf Norm: 0.3577\n",
            "Epoch 11: Train Loss: 0.0245, Test Loss: 0.0144, Train L1 Norm: 0.0547, Test L1 Norm: 0.0143, Train Linf Norm: 2.8392, Test Linf Norm: 0.4499\n",
            "Epoch 12: Train Loss: 0.0217, Test Loss: 0.0267, Train L1 Norm: 0.0384, Test L1 Norm: 0.0179, Train Linf Norm: 1.8597, Test Linf Norm: 0.4969\n",
            "Epoch 13: Train Loss: 0.0194, Test Loss: 0.0178, Train L1 Norm: 0.0311, Test L1 Norm: 0.0121, Train Linf Norm: 1.4559, Test Linf Norm: 0.3090\n",
            "Epoch 14: Train Loss: 0.0174, Test Loss: 0.0243, Train L1 Norm: 0.0334, Test L1 Norm: 0.0152, Train Linf Norm: 1.6477, Test Linf Norm: 0.3107\n",
            "Epoch 15: Train Loss: 0.0152, Test Loss: 0.0169, Train L1 Norm: 0.0413, Test L1 Norm: 0.0125, Train Linf Norm: 2.2094, Test Linf Norm: 0.2893\n",
            "Epoch 16: Train Loss: 0.0130, Test Loss: 0.0157, Train L1 Norm: 0.0291, Test L1 Norm: 0.0150, Train Linf Norm: 1.4611, Test Linf Norm: 0.5147\n",
            "Epoch 17: Train Loss: 0.0117, Test Loss: 0.0134, Train L1 Norm: 0.0326, Test L1 Norm: 0.0096, Train Linf Norm: 1.7294, Test Linf Norm: 0.2574\n",
            "Epoch 18: Train Loss: 0.0098, Test Loss: 0.0077, Train L1 Norm: 0.0317, Test L1 Norm: 0.0081, Train Linf Norm: 1.7030, Test Linf Norm: 0.2513\n",
            "Epoch 19: Train Loss: 0.0088, Test Loss: 0.0073, Train L1 Norm: 0.0296, Test L1 Norm: 0.0078, Train Linf Norm: 1.6042, Test Linf Norm: 0.2409\n",
            "Epoch 20: Train Loss: 0.0079, Test Loss: 0.0100, Train L1 Norm: 0.0313, Test L1 Norm: 0.0100, Train Linf Norm: 1.7309, Test Linf Norm: 0.3182\n",
            "Epoch 21: Train Loss: 0.0073, Test Loss: 0.0072, Train L1 Norm: 0.0301, Test L1 Norm: 0.0079, Train Linf Norm: 1.6744, Test Linf Norm: 0.2508\n",
            "Epoch 22: Train Loss: 0.0069, Test Loss: 0.0067, Train L1 Norm: 0.0340, Test L1 Norm: 0.0083, Train Linf Norm: 1.9243, Test Linf Norm: 0.2746\n",
            "Epoch 23: Train Loss: 0.0066, Test Loss: 0.0066, Train L1 Norm: 0.0284, Test L1 Norm: 0.0074, Train Linf Norm: 1.5705, Test Linf Norm: 0.2350\n",
            "Epoch 24: Train Loss: 0.0063, Test Loss: 0.0064, Train L1 Norm: 0.0288, Test L1 Norm: 0.0071, Train Linf Norm: 1.6017, Test Linf Norm: 0.2196\n",
            "Epoch 25: Train Loss: 0.0062, Test Loss: 0.0063, Train L1 Norm: 0.0290, Test L1 Norm: 0.0071, Train Linf Norm: 1.6293, Test Linf Norm: 0.2201\n",
            "Epoch 26: Train Loss: 0.0061, Test Loss: 0.0062, Train L1 Norm: 0.0294, Test L1 Norm: 0.0070, Train Linf Norm: 1.6562, Test Linf Norm: 0.2204\n",
            "Epoch 27: Train Loss: 0.0060, Test Loss: 0.0062, Train L1 Norm: 0.0302, Test L1 Norm: 0.0073, Train Linf Norm: 1.7073, Test Linf Norm: 0.2292\n",
            "Epoch 28: Train Loss: 0.0059, Test Loss: 0.0062, Train L1 Norm: 0.0298, Test L1 Norm: 0.0071, Train Linf Norm: 1.6887, Test Linf Norm: 0.2231\n",
            "Epoch 29: Train Loss: 0.0059, Test Loss: 0.0061, Train L1 Norm: 0.0290, Test L1 Norm: 0.0071, Train Linf Norm: 1.6362, Test Linf Norm: 0.2224\n",
            "Epoch 30: Train Loss: 0.0059, Test Loss: 0.0061, Train L1 Norm: 0.0295, Test L1 Norm: 0.0070, Train Linf Norm: 1.6434, Test Linf Norm: 0.2209\n",
            "Epoch 31: Train Loss: 0.0058, Test Loss: 0.0061, Train L1 Norm: 0.0288, Test L1 Norm: 0.0070, Train Linf Norm: 1.6223, Test Linf Norm: 0.2196\n",
            "Epoch 32: Train Loss: 0.0058, Test Loss: 0.0061, Train L1 Norm: 0.0289, Test L1 Norm: 0.0070, Train Linf Norm: 1.6306, Test Linf Norm: 0.2195\n",
            "Epoch 33: Train Loss: 0.0058, Test Loss: 0.0061, Train L1 Norm: 0.0287, Test L1 Norm: 0.0070, Train Linf Norm: 1.6213, Test Linf Norm: 0.2194\n",
            "Epoch 34: Train Loss: 0.0059, Test Loss: 0.0061, Train L1 Norm: 0.0287, Test L1 Norm: 0.0070, Train Linf Norm: 1.6222, Test Linf Norm: 0.2192\n",
            "Epoch 35: Train Loss: 0.0059, Test Loss: 0.0062, Train L1 Norm: 0.0288, Test L1 Norm: 0.0070, Train Linf Norm: 1.6242, Test Linf Norm: 0.2198\n",
            "Epoch 36: Train Loss: 0.0059, Test Loss: 0.0062, Train L1 Norm: 0.0292, Test L1 Norm: 0.0070, Train Linf Norm: 1.6505, Test Linf Norm: 0.2200\n",
            "Epoch 37: Train Loss: 0.0059, Test Loss: 0.0061, Train L1 Norm: 0.0292, Test L1 Norm: 0.0069, Train Linf Norm: 1.6309, Test Linf Norm: 0.2138\n",
            "Epoch 38: Train Loss: 0.0059, Test Loss: 0.0063, Train L1 Norm: 0.0278, Test L1 Norm: 0.0070, Train Linf Norm: 1.5523, Test Linf Norm: 0.2159\n",
            "Epoch 39: Train Loss: 0.0059, Test Loss: 0.0062, Train L1 Norm: 0.0275, Test L1 Norm: 0.0070, Train Linf Norm: 1.5380, Test Linf Norm: 0.2194\n",
            "Epoch 40: Train Loss: 0.0060, Test Loss: 0.0063, Train L1 Norm: 0.0273, Test L1 Norm: 0.0070, Train Linf Norm: 1.5177, Test Linf Norm: 0.2211\n",
            "Epoch 41: Train Loss: 0.0061, Test Loss: 0.0062, Train L1 Norm: 0.0274, Test L1 Norm: 0.0070, Train Linf Norm: 1.5255, Test Linf Norm: 0.2175\n",
            "Epoch 42: Train Loss: 0.0062, Test Loss: 0.0060, Train L1 Norm: 0.0293, Test L1 Norm: 0.0068, Train Linf Norm: 1.6470, Test Linf Norm: 0.2117\n",
            "Epoch 43: Train Loss: 0.0066, Test Loss: 0.0071, Train L1 Norm: 0.0305, Test L1 Norm: 0.0072, Train Linf Norm: 1.7238, Test Linf Norm: 0.2159\n",
            "Epoch 44: Train Loss: 0.0072, Test Loss: 0.0076, Train L1 Norm: 0.0277, Test L1 Norm: 0.0073, Train Linf Norm: 1.5235, Test Linf Norm: 0.2173\n",
            "Epoch 45: Train Loss: 0.0082, Test Loss: 0.0099, Train L1 Norm: 0.0302, Test L1 Norm: 0.0078, Train Linf Norm: 1.6664, Test Linf Norm: 0.2063\n",
            "Epoch 46: Train Loss: 0.0094, Test Loss: 0.0064, Train L1 Norm: 0.0221, Test L1 Norm: 0.0067, Train Linf Norm: 1.1254, Test Linf Norm: 0.2032\n",
            "Epoch 47: Train Loss: 0.0104, Test Loss: 0.0096, Train L1 Norm: 0.0265, Test L1 Norm: 0.0075, Train Linf Norm: 1.3947, Test Linf Norm: 0.2045\n",
            "Epoch 48: Train Loss: 0.0114, Test Loss: 0.0103, Train L1 Norm: 0.0304, Test L1 Norm: 0.0080, Train Linf Norm: 1.6204, Test Linf Norm: 0.2151\n",
            "Epoch 49: Train Loss: 0.0125, Test Loss: 0.0088, Train L1 Norm: 0.0347, Test L1 Norm: 0.0091, Train Linf Norm: 1.8719, Test Linf Norm: 0.2607\n",
            "Epoch 50: Train Loss: 0.0134, Test Loss: 0.0148, Train L1 Norm: 0.0260, Test L1 Norm: 0.0121, Train Linf Norm: 1.2942, Test Linf Norm: 0.3362\n",
            "Epoch 51: Train Loss: 0.0143, Test Loss: 0.0101, Train L1 Norm: 0.0292, Test L1 Norm: 0.0090, Train Linf Norm: 1.4799, Test Linf Norm: 0.2597\n",
            "Epoch 52: Train Loss: 0.0150, Test Loss: 0.0095, Train L1 Norm: 0.0339, Test L1 Norm: 0.0110, Train Linf Norm: 1.7438, Test Linf Norm: 0.4003\n",
            "Epoch 53: Train Loss: 0.0156, Test Loss: 0.0214, Train L1 Norm: 0.0405, Test L1 Norm: 0.0128, Train Linf Norm: 2.1654, Test Linf Norm: 0.3055\n",
            "Epoch 54: Train Loss: 0.0163, Test Loss: 0.0196, Train L1 Norm: 0.0232, Test L1 Norm: 0.0130, Train Linf Norm: 1.0378, Test Linf Norm: 0.3559\n",
            "Epoch 55: Train Loss: 0.0167, Test Loss: 0.0158, Train L1 Norm: 0.0499, Test L1 Norm: 0.0093, Train Linf Norm: 2.7504, Test Linf Norm: 0.2101\n",
            "Epoch 56: Train Loss: 0.0172, Test Loss: 0.0226, Train L1 Norm: 0.0375, Test L1 Norm: 0.0132, Train Linf Norm: 1.9441, Test Linf Norm: 0.3209\n",
            "Epoch 57: Train Loss: 0.0176, Test Loss: 0.0201, Train L1 Norm: 0.0378, Test L1 Norm: 0.0137, Train Linf Norm: 1.9439, Test Linf Norm: 0.3072\n",
            "Epoch 58: Train Loss: 0.0177, Test Loss: 0.0212, Train L1 Norm: 0.0353, Test L1 Norm: 0.0104, Train Linf Norm: 1.7917, Test Linf Norm: 0.2100\n",
            "Epoch 59: Train Loss: 0.0176, Test Loss: 0.0164, Train L1 Norm: 0.0333, Test L1 Norm: 0.0165, Train Linf Norm: 1.6666, Test Linf Norm: 0.5218\n",
            "Epoch 60: Train Loss: 0.0176, Test Loss: 0.0141, Train L1 Norm: 0.0421, Test L1 Norm: 0.0092, Train Linf Norm: 2.2198, Test Linf Norm: 0.2275\n",
            "Epoch 61: Train Loss: 0.0176, Test Loss: 0.0184, Train L1 Norm: 0.0361, Test L1 Norm: 0.0101, Train Linf Norm: 1.8529, Test Linf Norm: 0.2365\n",
            "Epoch 62: Train Loss: 0.0175, Test Loss: 0.0132, Train L1 Norm: 0.0485, Test L1 Norm: 0.0100, Train Linf Norm: 2.6405, Test Linf Norm: 0.2759\n",
            "Epoch 63: Train Loss: 0.0173, Test Loss: 0.0111, Train L1 Norm: 0.0221, Test L1 Norm: 0.0081, Train Linf Norm: 0.9599, Test Linf Norm: 0.2030\n",
            "Epoch 64: Train Loss: 0.0168, Test Loss: 0.0169, Train L1 Norm: 0.0386, Test L1 Norm: 0.0119, Train Linf Norm: 2.0290, Test Linf Norm: 0.2943\n",
            "Epoch 65: Train Loss: 0.0166, Test Loss: 0.0108, Train L1 Norm: 0.0419, Test L1 Norm: 0.0124, Train Linf Norm: 2.2442, Test Linf Norm: 0.3985\n",
            "Epoch 66: Train Loss: 0.0159, Test Loss: 0.0114, Train L1 Norm: 0.0305, Test L1 Norm: 0.0096, Train Linf Norm: 1.5270, Test Linf Norm: 0.2420\n",
            "Epoch 67: Train Loss: 0.0154, Test Loss: 0.0207, Train L1 Norm: 0.0414, Test L1 Norm: 0.0123, Train Linf Norm: 2.2435, Test Linf Norm: 0.3409\n",
            "Epoch 68: Train Loss: 0.0149, Test Loss: 0.0175, Train L1 Norm: 0.0392, Test L1 Norm: 0.0092, Train Linf Norm: 2.1127, Test Linf Norm: 0.2087\n",
            "Epoch 69: Train Loss: 0.0142, Test Loss: 0.0101, Train L1 Norm: 0.0295, Test L1 Norm: 0.0083, Train Linf Norm: 1.5083, Test Linf Norm: 0.2020\n",
            "Epoch 70: Train Loss: 0.0133, Test Loss: 0.0169, Train L1 Norm: 0.0350, Test L1 Norm: 0.0127, Train Linf Norm: 1.8732, Test Linf Norm: 0.3553\n",
            "Epoch 71: Train Loss: 0.0126, Test Loss: 0.0075, Train L1 Norm: 0.0282, Test L1 Norm: 0.0082, Train Linf Norm: 1.4612, Test Linf Norm: 0.2665\n",
            "Epoch 72: Train Loss: 0.0117, Test Loss: 0.0096, Train L1 Norm: 0.0298, Test L1 Norm: 0.0072, Train Linf Norm: 1.5854, Test Linf Norm: 0.1853\n",
            "Epoch 73: Train Loss: 0.0110, Test Loss: 0.0148, Train L1 Norm: 0.0252, Test L1 Norm: 0.0081, Train Linf Norm: 1.3005, Test Linf Norm: 0.1684\n",
            "Epoch 74: Train Loss: 0.0101, Test Loss: 0.0104, Train L1 Norm: 0.0237, Test L1 Norm: 0.0078, Train Linf Norm: 1.2318, Test Linf Norm: 0.2020\n",
            "Epoch 75: Train Loss: 0.0089, Test Loss: 0.0090, Train L1 Norm: 0.0240, Test L1 Norm: 0.0067, Train Linf Norm: 1.2560, Test Linf Norm: 0.1715\n",
            "Epoch 76: Train Loss: 0.0083, Test Loss: 0.0084, Train L1 Norm: 0.0260, Test L1 Norm: 0.0064, Train Linf Norm: 1.4214, Test Linf Norm: 0.1754\n",
            "Epoch 77: Train Loss: 0.0076, Test Loss: 0.0056, Train L1 Norm: 0.0251, Test L1 Norm: 0.0064, Train Linf Norm: 1.3740, Test Linf Norm: 0.1977\n",
            "Epoch 78: Train Loss: 0.0067, Test Loss: 0.0069, Train L1 Norm: 0.0223, Test L1 Norm: 0.0059, Train Linf Norm: 1.2229, Test Linf Norm: 0.1529\n",
            "Epoch 79: Train Loss: 0.0061, Test Loss: 0.0080, Train L1 Norm: 0.0241, Test L1 Norm: 0.0069, Train Linf Norm: 1.3453, Test Linf Norm: 0.2030\n",
            "Epoch 80: Train Loss: 0.0056, Test Loss: 0.0087, Train L1 Norm: 0.0227, Test L1 Norm: 0.0061, Train Linf Norm: 1.2697, Test Linf Norm: 0.1543\n",
            "Epoch 81: Train Loss: 0.0053, Test Loss: 0.0050, Train L1 Norm: 0.0208, Test L1 Norm: 0.0055, Train Linf Norm: 1.1364, Test Linf Norm: 0.1660\n",
            "Epoch 82: Train Loss: 0.0051, Test Loss: 0.0063, Train L1 Norm: 0.0224, Test L1 Norm: 0.0058, Train Linf Norm: 1.2571, Test Linf Norm: 0.1694\n",
            "Epoch 83: Train Loss: 0.0049, Test Loss: 0.0051, Train L1 Norm: 0.0223, Test L1 Norm: 0.0056, Train Linf Norm: 1.2581, Test Linf Norm: 0.1707\n",
            "Epoch 84: Train Loss: 0.0048, Test Loss: 0.0050, Train L1 Norm: 0.0221, Test L1 Norm: 0.0052, Train Linf Norm: 1.2424, Test Linf Norm: 0.1483\n",
            "Epoch 85: Train Loss: 0.0047, Test Loss: 0.0050, Train L1 Norm: 0.0220, Test L1 Norm: 0.0052, Train Linf Norm: 1.2371, Test Linf Norm: 0.1538\n",
            "Epoch 86: Train Loss: 0.0046, Test Loss: 0.0049, Train L1 Norm: 0.0233, Test L1 Norm: 0.0051, Train Linf Norm: 1.3272, Test Linf Norm: 0.1487\n",
            "Epoch 87: Train Loss: 0.0046, Test Loss: 0.0048, Train L1 Norm: 0.0207, Test L1 Norm: 0.0052, Train Linf Norm: 1.1607, Test Linf Norm: 0.1527\n",
            "Epoch 88: Train Loss: 0.0045, Test Loss: 0.0048, Train L1 Norm: 0.0208, Test L1 Norm: 0.0051, Train Linf Norm: 1.1698, Test Linf Norm: 0.1498\n",
            "Epoch 89: Train Loss: 0.0045, Test Loss: 0.0048, Train L1 Norm: 0.0214, Test L1 Norm: 0.0051, Train Linf Norm: 1.2129, Test Linf Norm: 0.1458\n",
            "Epoch 90: Train Loss: 0.0045, Test Loss: 0.0048, Train L1 Norm: 0.0212, Test L1 Norm: 0.0051, Train Linf Norm: 1.1669, Test Linf Norm: 0.1508\n",
            "Epoch 91: Train Loss: 0.0045, Test Loss: 0.0048, Train L1 Norm: 0.0215, Test L1 Norm: 0.0051, Train Linf Norm: 1.2175, Test Linf Norm: 0.1477\n",
            "Epoch 92: Train Loss: 0.0044, Test Loss: 0.0048, Train L1 Norm: 0.0215, Test L1 Norm: 0.0051, Train Linf Norm: 1.2213, Test Linf Norm: 0.1484\n",
            "Epoch 93: Train Loss: 0.0044, Test Loss: 0.0048, Train L1 Norm: 0.0212, Test L1 Norm: 0.0051, Train Linf Norm: 1.2006, Test Linf Norm: 0.1493\n",
            "Epoch 94: Train Loss: 0.0044, Test Loss: 0.0048, Train L1 Norm: 0.0213, Test L1 Norm: 0.0051, Train Linf Norm: 1.2007, Test Linf Norm: 0.1496\n",
            "Epoch 95: Train Loss: 0.0044, Test Loss: 0.0048, Train L1 Norm: 0.0213, Test L1 Norm: 0.0051, Train Linf Norm: 1.2089, Test Linf Norm: 0.1482\n",
            "Epoch 96: Train Loss: 0.0044, Test Loss: 0.0048, Train L1 Norm: 0.0210, Test L1 Norm: 0.0051, Train Linf Norm: 1.1680, Test Linf Norm: 0.1495\n",
            "Epoch 97: Train Loss: 0.0045, Test Loss: 0.0048, Train L1 Norm: 0.0214, Test L1 Norm: 0.0051, Train Linf Norm: 1.2162, Test Linf Norm: 0.1495\n",
            "Epoch 98: Train Loss: 0.0045, Test Loss: 0.0048, Train L1 Norm: 0.0215, Test L1 Norm: 0.0051, Train Linf Norm: 1.2210, Test Linf Norm: 0.1471\n",
            "Epoch 99: Train Loss: 0.0045, Test Loss: 0.0048, Train L1 Norm: 0.0209, Test L1 Norm: 0.0051, Train Linf Norm: 1.1799, Test Linf Norm: 0.1513\n",
            "Epoch 100: Train Loss: 0.0045, Test Loss: 0.0048, Train L1 Norm: 0.0219, Test L1 Norm: 0.0052, Train Linf Norm: 1.2414, Test Linf Norm: 0.1510\n",
            "Epoch 101: Train Loss: 0.0045, Test Loss: 0.0048, Train L1 Norm: 0.0215, Test L1 Norm: 0.0052, Train Linf Norm: 1.2168, Test Linf Norm: 0.1540\n",
            "Epoch 102: Train Loss: 0.0045, Test Loss: 0.0050, Train L1 Norm: 0.0220, Test L1 Norm: 0.0052, Train Linf Norm: 1.2457, Test Linf Norm: 0.1534\n",
            "Epoch 103: Train Loss: 0.0046, Test Loss: 0.0048, Train L1 Norm: 0.0217, Test L1 Norm: 0.0051, Train Linf Norm: 1.2284, Test Linf Norm: 0.1459\n",
            "Epoch 104: Train Loss: 0.0046, Test Loss: 0.0053, Train L1 Norm: 0.0211, Test L1 Norm: 0.0054, Train Linf Norm: 1.1817, Test Linf Norm: 0.1616\n",
            "Epoch 105: Train Loss: 0.0047, Test Loss: 0.0054, Train L1 Norm: 0.0226, Test L1 Norm: 0.0051, Train Linf Norm: 1.2726, Test Linf Norm: 0.1440\n",
            "Epoch 106: Train Loss: 0.0049, Test Loss: 0.0062, Train L1 Norm: 0.0192, Test L1 Norm: 0.0054, Train Linf Norm: 1.0570, Test Linf Norm: 0.1469\n",
            "Epoch 107: Train Loss: 0.0051, Test Loss: 0.0052, Train L1 Norm: 0.0225, Test L1 Norm: 0.0054, Train Linf Norm: 1.2677, Test Linf Norm: 0.1642\n",
            "Epoch 108: Train Loss: 0.0054, Test Loss: 0.0058, Train L1 Norm: 0.0238, Test L1 Norm: 0.0056, Train Linf Norm: 1.3464, Test Linf Norm: 0.1628\n",
            "Epoch 109: Train Loss: 0.0060, Test Loss: 0.0061, Train L1 Norm: 0.0228, Test L1 Norm: 0.0070, Train Linf Norm: 1.2719, Test Linf Norm: 0.2241\n",
            "Epoch 110: Train Loss: 0.0067, Test Loss: 0.0061, Train L1 Norm: 0.0251, Test L1 Norm: 0.0055, Train Linf Norm: 1.4074, Test Linf Norm: 0.1565\n",
            "Epoch 111: Train Loss: 0.0074, Test Loss: 0.0079, Train L1 Norm: 0.0248, Test L1 Norm: 0.0061, Train Linf Norm: 1.3737, Test Linf Norm: 0.1595\n",
            "Epoch 112: Train Loss: 0.0081, Test Loss: 0.0066, Train L1 Norm: 0.0197, Test L1 Norm: 0.0072, Train Linf Norm: 1.0269, Test Linf Norm: 0.2241\n",
            "Epoch 113: Train Loss: 0.0089, Test Loss: 0.0106, Train L1 Norm: 0.0266, Test L1 Norm: 0.0083, Train Linf Norm: 1.4478, Test Linf Norm: 0.2315\n",
            "Epoch 114: Train Loss: 0.0095, Test Loss: 0.0058, Train L1 Norm: 0.0240, Test L1 Norm: 0.0075, Train Linf Norm: 1.2674, Test Linf Norm: 0.2484\n",
            "Epoch 115: Train Loss: 0.0102, Test Loss: 0.0132, Train L1 Norm: 0.0235, Test L1 Norm: 0.0084, Train Linf Norm: 1.2282, Test Linf Norm: 0.2020\n",
            "Epoch 116: Train Loss: 0.0106, Test Loss: 0.0063, Train L1 Norm: 0.0249, Test L1 Norm: 0.0094, Train Linf Norm: 1.3040, Test Linf Norm: 0.3206\n",
            "Epoch 117: Train Loss: 0.0111, Test Loss: 0.0117, Train L1 Norm: 0.0200, Test L1 Norm: 0.0073, Train Linf Norm: 0.9800, Test Linf Norm: 0.1615\n",
            "Epoch 118: Train Loss: 0.0115, Test Loss: 0.0201, Train L1 Norm: 0.0251, Test L1 Norm: 0.0096, Train Linf Norm: 1.2942, Test Linf Norm: 0.1786\n",
            "Epoch 119: Train Loss: 0.0119, Test Loss: 0.0220, Train L1 Norm: 0.0293, Test L1 Norm: 0.0182, Train Linf Norm: 1.5549, Test Linf Norm: 0.5492\n",
            "Epoch 120: Train Loss: 0.0120, Test Loss: 0.0142, Train L1 Norm: 0.0229, Test L1 Norm: 0.0074, Train Linf Norm: 1.1425, Test Linf Norm: 0.1590\n",
            "Epoch 121: Train Loss: 0.0125, Test Loss: 0.0138, Train L1 Norm: 0.0237, Test L1 Norm: 0.0079, Train Linf Norm: 1.1797, Test Linf Norm: 0.1910\n",
            "Epoch 122: Train Loss: 0.0124, Test Loss: 0.0090, Train L1 Norm: 0.0312, Test L1 Norm: 0.0065, Train Linf Norm: 1.6617, Test Linf Norm: 0.1639\n",
            "Epoch 123: Train Loss: 0.0124, Test Loss: 0.0059, Train L1 Norm: 0.0363, Test L1 Norm: 0.0072, Train Linf Norm: 1.9926, Test Linf Norm: 0.2470\n",
            "Epoch 124: Train Loss: 0.0125, Test Loss: 0.0189, Train L1 Norm: 0.0234, Test L1 Norm: 0.0111, Train Linf Norm: 1.1689, Test Linf Norm: 0.2634\n",
            "Epoch 125: Train Loss: 0.0125, Test Loss: 0.0130, Train L1 Norm: 0.0297, Test L1 Norm: 0.0083, Train Linf Norm: 1.5565, Test Linf Norm: 0.1892\n",
            "Epoch 126: Train Loss: 0.0123, Test Loss: 0.0126, Train L1 Norm: 0.0295, Test L1 Norm: 0.0074, Train Linf Norm: 1.5517, Test Linf Norm: 0.1628\n",
            "Epoch 127: Train Loss: 0.0120, Test Loss: 0.0136, Train L1 Norm: 0.0231, Test L1 Norm: 0.0075, Train Linf Norm: 1.1538, Test Linf Norm: 0.1463\n",
            "Epoch 128: Train Loss: 0.0118, Test Loss: 0.0159, Train L1 Norm: 0.0233, Test L1 Norm: 0.0081, Train Linf Norm: 1.1738, Test Linf Norm: 0.1648\n",
            "Epoch 129: Train Loss: 0.0114, Test Loss: 0.0070, Train L1 Norm: 0.0310, Test L1 Norm: 0.0069, Train Linf Norm: 1.6762, Test Linf Norm: 0.2014\n",
            "Epoch 130: Train Loss: 0.0110, Test Loss: 0.0076, Train L1 Norm: 0.0297, Test L1 Norm: 0.0073, Train Linf Norm: 1.6015, Test Linf Norm: 0.2033\n",
            "Epoch 131: Train Loss: 0.0106, Test Loss: 0.0174, Train L1 Norm: 0.0211, Test L1 Norm: 0.0130, Train Linf Norm: 1.0612, Test Linf Norm: 0.3507\n",
            "Epoch 132: Train Loss: 0.0100, Test Loss: 0.0078, Train L1 Norm: 0.0218, Test L1 Norm: 0.0058, Train Linf Norm: 1.1210, Test Linf Norm: 0.1434\n",
            "Epoch 133: Train Loss: 0.0095, Test Loss: 0.0064, Train L1 Norm: 0.0267, Test L1 Norm: 0.0060, Train Linf Norm: 1.4490, Test Linf Norm: 0.1695\n",
            "Epoch 134: Train Loss: 0.0090, Test Loss: 0.0086, Train L1 Norm: 0.0246, Test L1 Norm: 0.0060, Train Linf Norm: 1.3224, Test Linf Norm: 0.1410\n",
            "Epoch 135: Train Loss: 0.0082, Test Loss: 0.0085, Train L1 Norm: 0.0304, Test L1 Norm: 0.0074, Train Linf Norm: 1.7169, Test Linf Norm: 0.2283\n",
            "Epoch 136: Train Loss: 0.0076, Test Loss: 0.0075, Train L1 Norm: 0.0213, Test L1 Norm: 0.0057, Train Linf Norm: 1.1421, Test Linf Norm: 0.1477\n",
            "Epoch 137: Train Loss: 0.0070, Test Loss: 0.0085, Train L1 Norm: 0.0216, Test L1 Norm: 0.0068, Train Linf Norm: 1.1462, Test Linf Norm: 0.1915\n",
            "Epoch 138: Train Loss: 0.0063, Test Loss: 0.0092, Train L1 Norm: 0.0191, Test L1 Norm: 0.0060, Train Linf Norm: 1.0347, Test Linf Norm: 0.1489\n",
            "Epoch 139: Train Loss: 0.0058, Test Loss: 0.0056, Train L1 Norm: 0.0227, Test L1 Norm: 0.0054, Train Linf Norm: 1.2757, Test Linf Norm: 0.1606\n",
            "Epoch 140: Train Loss: 0.0053, Test Loss: 0.0060, Train L1 Norm: 0.0205, Test L1 Norm: 0.0052, Train Linf Norm: 1.1417, Test Linf Norm: 0.1450\n",
            "Epoch 141: Train Loss: 0.0049, Test Loss: 0.0045, Train L1 Norm: 0.0186, Test L1 Norm: 0.0050, Train Linf Norm: 1.0343, Test Linf Norm: 0.1546\n",
            "Epoch 142: Train Loss: 0.0046, Test Loss: 0.0055, Train L1 Norm: 0.0202, Test L1 Norm: 0.0050, Train Linf Norm: 1.1413, Test Linf Norm: 0.1443\n",
            "Epoch 143: Train Loss: 0.0044, Test Loss: 0.0059, Train L1 Norm: 0.0193, Test L1 Norm: 0.0056, Train Linf Norm: 1.0897, Test Linf Norm: 0.1690\n",
            "Epoch 144: Train Loss: 0.0043, Test Loss: 0.0049, Train L1 Norm: 0.0176, Test L1 Norm: 0.0050, Train Linf Norm: 0.9849, Test Linf Norm: 0.1509\n",
            "Epoch 145: Train Loss: 0.0042, Test Loss: 0.0045, Train L1 Norm: 0.0184, Test L1 Norm: 0.0048, Train Linf Norm: 1.0339, Test Linf Norm: 0.1419\n",
            "Epoch 146: Train Loss: 0.0041, Test Loss: 0.0046, Train L1 Norm: 0.0179, Test L1 Norm: 0.0047, Train Linf Norm: 1.0051, Test Linf Norm: 0.1369\n",
            "Epoch 147: Train Loss: 0.0040, Test Loss: 0.0044, Train L1 Norm: 0.0189, Test L1 Norm: 0.0047, Train Linf Norm: 1.0686, Test Linf Norm: 0.1393\n",
            "Epoch 148: Train Loss: 0.0040, Test Loss: 0.0043, Train L1 Norm: 0.0185, Test L1 Norm: 0.0046, Train Linf Norm: 1.0505, Test Linf Norm: 0.1339\n",
            "Epoch 149: Train Loss: 0.0039, Test Loss: 0.0043, Train L1 Norm: 0.0189, Test L1 Norm: 0.0045, Train Linf Norm: 1.0776, Test Linf Norm: 0.1305\n",
            "Epoch 150: Train Loss: 0.0039, Test Loss: 0.0043, Train L1 Norm: 0.0182, Test L1 Norm: 0.0045, Train Linf Norm: 1.0316, Test Linf Norm: 0.1296\n",
            "Epoch 151: Train Loss: 0.0039, Test Loss: 0.0043, Train L1 Norm: 0.0175, Test L1 Norm: 0.0046, Train Linf Norm: 0.9825, Test Linf Norm: 0.1352\n",
            "Epoch 152: Train Loss: 0.0039, Test Loss: 0.0042, Train L1 Norm: 0.0181, Test L1 Norm: 0.0046, Train Linf Norm: 1.0239, Test Linf Norm: 0.1334\n",
            "Epoch 153: Train Loss: 0.0039, Test Loss: 0.0042, Train L1 Norm: 0.0179, Test L1 Norm: 0.0045, Train Linf Norm: 1.0109, Test Linf Norm: 0.1300\n",
            "Epoch 154: Train Loss: 0.0038, Test Loss: 0.0042, Train L1 Norm: 0.0179, Test L1 Norm: 0.0045, Train Linf Norm: 1.0173, Test Linf Norm: 0.1307\n",
            "Epoch 155: Train Loss: 0.0038, Test Loss: 0.0042, Train L1 Norm: 0.0177, Test L1 Norm: 0.0045, Train Linf Norm: 1.0014, Test Linf Norm: 0.1309\n",
            "Epoch 156: Train Loss: 0.0038, Test Loss: 0.0042, Train L1 Norm: 0.0180, Test L1 Norm: 0.0045, Train Linf Norm: 1.0202, Test Linf Norm: 0.1308\n",
            "Epoch 157: Train Loss: 0.0038, Test Loss: 0.0042, Train L1 Norm: 0.0179, Test L1 Norm: 0.0045, Train Linf Norm: 1.0074, Test Linf Norm: 0.1314\n",
            "Epoch 158: Train Loss: 0.0038, Test Loss: 0.0042, Train L1 Norm: 0.0179, Test L1 Norm: 0.0045, Train Linf Norm: 1.0094, Test Linf Norm: 0.1309\n",
            "Epoch 159: Train Loss: 0.0038, Test Loss: 0.0042, Train L1 Norm: 0.0179, Test L1 Norm: 0.0045, Train Linf Norm: 1.0149, Test Linf Norm: 0.1312\n",
            "Epoch 160: Train Loss: 0.0039, Test Loss: 0.0043, Train L1 Norm: 0.0180, Test L1 Norm: 0.0045, Train Linf Norm: 1.0163, Test Linf Norm: 0.1316\n",
            "Epoch 161: Train Loss: 0.0039, Test Loss: 0.0043, Train L1 Norm: 0.0178, Test L1 Norm: 0.0046, Train Linf Norm: 1.0051, Test Linf Norm: 0.1368\n",
            "Epoch 162: Train Loss: 0.0039, Test Loss: 0.0043, Train L1 Norm: 0.0178, Test L1 Norm: 0.0046, Train Linf Norm: 1.0066, Test Linf Norm: 0.1350\n",
            "Epoch 163: Train Loss: 0.0039, Test Loss: 0.0042, Train L1 Norm: 0.0188, Test L1 Norm: 0.0047, Train Linf Norm: 1.0680, Test Linf Norm: 0.1393\n",
            "Epoch 164: Train Loss: 0.0039, Test Loss: 0.0043, Train L1 Norm: 0.0181, Test L1 Norm: 0.0046, Train Linf Norm: 1.0254, Test Linf Norm: 0.1342\n",
            "Epoch 165: Train Loss: 0.0040, Test Loss: 0.0043, Train L1 Norm: 0.0182, Test L1 Norm: 0.0045, Train Linf Norm: 1.0234, Test Linf Norm: 0.1279\n",
            "Epoch 166: Train Loss: 0.0040, Test Loss: 0.0044, Train L1 Norm: 0.0179, Test L1 Norm: 0.0049, Train Linf Norm: 1.0101, Test Linf Norm: 0.1457\n",
            "Epoch 167: Train Loss: 0.0041, Test Loss: 0.0043, Train L1 Norm: 0.0161, Test L1 Norm: 0.0045, Train Linf Norm: 0.8932, Test Linf Norm: 0.1313\n",
            "Epoch 168: Train Loss: 0.0041, Test Loss: 0.0045, Train L1 Norm: 0.0198, Test L1 Norm: 0.0046, Train Linf Norm: 1.1309, Test Linf Norm: 0.1322\n",
            "Epoch 169: Train Loss: 0.0043, Test Loss: 0.0044, Train L1 Norm: 0.0184, Test L1 Norm: 0.0046, Train Linf Norm: 1.0291, Test Linf Norm: 0.1315\n",
            "Epoch 170: Train Loss: 0.0045, Test Loss: 0.0051, Train L1 Norm: 0.0201, Test L1 Norm: 0.0047, Train Linf Norm: 1.1403, Test Linf Norm: 0.1297\n",
            "Epoch 171: Train Loss: 0.0048, Test Loss: 0.0064, Train L1 Norm: 0.0193, Test L1 Norm: 0.0052, Train Linf Norm: 1.0789, Test Linf Norm: 0.1427\n",
            "Epoch 172: Train Loss: 0.0052, Test Loss: 0.0057, Train L1 Norm: 0.0193, Test L1 Norm: 0.0063, Train Linf Norm: 1.0724, Test Linf Norm: 0.1911\n",
            "Epoch 173: Train Loss: 0.0058, Test Loss: 0.0062, Train L1 Norm: 0.0187, Test L1 Norm: 0.0054, Train Linf Norm: 1.0217, Test Linf Norm: 0.1468\n",
            "Epoch 174: Train Loss: 0.0064, Test Loss: 0.0084, Train L1 Norm: 0.0160, Test L1 Norm: 0.0054, Train Linf Norm: 0.8344, Test Linf Norm: 0.1231\n",
            "Epoch 175: Train Loss: 0.0069, Test Loss: 0.0096, Train L1 Norm: 0.0236, Test L1 Norm: 0.0085, Train Linf Norm: 1.3088, Test Linf Norm: 0.2519\n",
            "Epoch 176: Train Loss: 0.0076, Test Loss: 0.0075, Train L1 Norm: 0.0171, Test L1 Norm: 0.0059, Train Linf Norm: 0.8844, Test Linf Norm: 0.1570\n",
            "Epoch 177: Train Loss: 0.0081, Test Loss: 0.0093, Train L1 Norm: 0.0263, Test L1 Norm: 0.0064, Train Linf Norm: 1.4536, Test Linf Norm: 0.1632\n",
            "Epoch 178: Train Loss: 0.0086, Test Loss: 0.0085, Train L1 Norm: 0.0205, Test L1 Norm: 0.0074, Train Linf Norm: 1.0723, Test Linf Norm: 0.1826\n",
            "Epoch 179: Train Loss: 0.0089, Test Loss: 0.0108, Train L1 Norm: 0.0293, Test L1 Norm: 0.0086, Train Linf Norm: 1.6243, Test Linf Norm: 0.2601\n",
            "Epoch 180: Train Loss: 0.0094, Test Loss: 0.0059, Train L1 Norm: 0.0219, Test L1 Norm: 0.0061, Train Linf Norm: 1.1386, Test Linf Norm: 0.1921\n",
            "Epoch 181: Train Loss: 0.0096, Test Loss: 0.0104, Train L1 Norm: 0.0219, Test L1 Norm: 0.0075, Train Linf Norm: 1.1393, Test Linf Norm: 0.1846\n",
            "Epoch 182: Train Loss: 0.0100, Test Loss: 0.0101, Train L1 Norm: 0.0235, Test L1 Norm: 0.0060, Train Linf Norm: 1.2406, Test Linf Norm: 0.1411\n",
            "Epoch 183: Train Loss: 0.0101, Test Loss: 0.0078, Train L1 Norm: 0.0258, Test L1 Norm: 0.0061, Train Linf Norm: 1.3786, Test Linf Norm: 0.1577\n",
            "Epoch 184: Train Loss: 0.0103, Test Loss: 0.0099, Train L1 Norm: 0.0148, Test L1 Norm: 0.0066, Train Linf Norm: 0.6691, Test Linf Norm: 0.1513\n",
            "Epoch 185: Train Loss: 0.0102, Test Loss: 0.0182, Train L1 Norm: 0.0144, Test L1 Norm: 0.0099, Train Linf Norm: 0.6451, Test Linf Norm: 0.1930\n",
            "Epoch 186: Train Loss: 0.0104, Test Loss: 0.0152, Train L1 Norm: 0.0281, Test L1 Norm: 0.0090, Train Linf Norm: 1.5090, Test Linf Norm: 0.2212\n",
            "Epoch 187: Train Loss: 0.0102, Test Loss: 0.0122, Train L1 Norm: 0.0154, Test L1 Norm: 0.0074, Train Linf Norm: 0.7062, Test Linf Norm: 0.1621\n",
            "Epoch 188: Train Loss: 0.0102, Test Loss: 0.0117, Train L1 Norm: 0.0226, Test L1 Norm: 0.0096, Train Linf Norm: 1.1696, Test Linf Norm: 0.2768\n",
            "Epoch 189: Train Loss: 0.0099, Test Loss: 0.0118, Train L1 Norm: 0.0238, Test L1 Norm: 0.0088, Train Linf Norm: 1.2513, Test Linf Norm: 0.2459\n",
            "Epoch 190: Train Loss: 0.0098, Test Loss: 0.0133, Train L1 Norm: 0.0229, Test L1 Norm: 0.0080, Train Linf Norm: 1.1970, Test Linf Norm: 0.1787\n",
            "Epoch 191: Train Loss: 0.0097, Test Loss: 0.0117, Train L1 Norm: 0.0204, Test L1 Norm: 0.0072, Train Linf Norm: 1.0386, Test Linf Norm: 0.1679\n",
            "Epoch 192: Train Loss: 0.0092, Test Loss: 0.0148, Train L1 Norm: 0.0173, Test L1 Norm: 0.0086, Train Linf Norm: 0.8570, Test Linf Norm: 0.1780\n",
            "Epoch 193: Train Loss: 0.0089, Test Loss: 0.0059, Train L1 Norm: 0.0192, Test L1 Norm: 0.0055, Train Linf Norm: 0.9814, Test Linf Norm: 0.1595\n",
            "Epoch 194: Train Loss: 0.0083, Test Loss: 0.0055, Train L1 Norm: 0.0218, Test L1 Norm: 0.0055, Train Linf Norm: 1.1697, Test Linf Norm: 0.1794\n",
            "Epoch 195: Train Loss: 0.0079, Test Loss: 0.0150, Train L1 Norm: 0.0190, Test L1 Norm: 0.0082, Train Linf Norm: 0.9900, Test Linf Norm: 0.1723\n",
            "Epoch 196: Train Loss: 0.0074, Test Loss: 0.0093, Train L1 Norm: 0.0190, Test L1 Norm: 0.0063, Train Linf Norm: 1.0115, Test Linf Norm: 0.1630\n",
            "Epoch 197: Train Loss: 0.0071, Test Loss: 0.0049, Train L1 Norm: 0.0170, Test L1 Norm: 0.0051, Train Linf Norm: 0.8903, Test Linf Norm: 0.1484\n",
            "Epoch 198: Train Loss: 0.0065, Test Loss: 0.0044, Train L1 Norm: 0.0154, Test L1 Norm: 0.0044, Train Linf Norm: 0.7939, Test Linf Norm: 0.1236\n",
            "Epoch 199: Train Loss: 0.0059, Test Loss: 0.0073, Train L1 Norm: 0.0187, Test L1 Norm: 0.0058, Train Linf Norm: 1.0211, Test Linf Norm: 0.1537\n",
            "Epoch 200: Train Loss: 0.0054, Test Loss: 0.0098, Train L1 Norm: 0.0176, Test L1 Norm: 0.0072, Train Linf Norm: 0.9604, Test Linf Norm: 0.1995\n",
            "Epoch 201: Train Loss: 0.0049, Test Loss: 0.0044, Train L1 Norm: 0.0190, Test L1 Norm: 0.0051, Train Linf Norm: 1.0661, Test Linf Norm: 0.1492\n",
            "Epoch 202: Train Loss: 0.0046, Test Loss: 0.0043, Train L1 Norm: 0.0172, Test L1 Norm: 0.0043, Train Linf Norm: 0.9578, Test Linf Norm: 0.1159\n",
            "Epoch 203: Train Loss: 0.0043, Test Loss: 0.0042, Train L1 Norm: 0.0185, Test L1 Norm: 0.0045, Train Linf Norm: 1.0400, Test Linf Norm: 0.1328\n",
            "Epoch 204: Train Loss: 0.0041, Test Loss: 0.0058, Train L1 Norm: 0.0163, Test L1 Norm: 0.0052, Train Linf Norm: 0.9089, Test Linf Norm: 0.1443\n",
            "Epoch 205: Train Loss: 0.0039, Test Loss: 0.0042, Train L1 Norm: 0.0172, Test L1 Norm: 0.0043, Train Linf Norm: 0.9711, Test Linf Norm: 0.1250\n",
            "Epoch 206: Train Loss: 0.0038, Test Loss: 0.0040, Train L1 Norm: 0.0177, Test L1 Norm: 0.0043, Train Linf Norm: 1.0006, Test Linf Norm: 0.1208\n",
            "Epoch 207: Train Loss: 0.0037, Test Loss: 0.0040, Train L1 Norm: 0.0157, Test L1 Norm: 0.0042, Train Linf Norm: 0.8771, Test Linf Norm: 0.1231\n",
            "Epoch 208: Train Loss: 0.0037, Test Loss: 0.0040, Train L1 Norm: 0.0162, Test L1 Norm: 0.0048, Train Linf Norm: 0.9102, Test Linf Norm: 0.1546\n",
            "Epoch 209: Train Loss: 0.0036, Test Loss: 0.0041, Train L1 Norm: 0.0157, Test L1 Norm: 0.0042, Train Linf Norm: 0.8809, Test Linf Norm: 0.1202\n",
            "Epoch 210: Train Loss: 0.0036, Test Loss: 0.0041, Train L1 Norm: 0.0160, Test L1 Norm: 0.0042, Train Linf Norm: 0.9022, Test Linf Norm: 0.1176\n",
            "Epoch 211: Train Loss: 0.0036, Test Loss: 0.0040, Train L1 Norm: 0.0159, Test L1 Norm: 0.0041, Train Linf Norm: 0.8940, Test Linf Norm: 0.1128\n",
            "Epoch 212: Train Loss: 0.0035, Test Loss: 0.0040, Train L1 Norm: 0.0150, Test L1 Norm: 0.0041, Train Linf Norm: 0.8378, Test Linf Norm: 0.1181\n",
            "Epoch 213: Train Loss: 0.0035, Test Loss: 0.0040, Train L1 Norm: 0.0155, Test L1 Norm: 0.0041, Train Linf Norm: 0.8725, Test Linf Norm: 0.1190\n",
            "Epoch 214: Train Loss: 0.0035, Test Loss: 0.0039, Train L1 Norm: 0.0155, Test L1 Norm: 0.0041, Train Linf Norm: 0.8711, Test Linf Norm: 0.1186\n",
            "Epoch 215: Train Loss: 0.0035, Test Loss: 0.0039, Train L1 Norm: 0.0157, Test L1 Norm: 0.0041, Train Linf Norm: 0.8867, Test Linf Norm: 0.1169\n",
            "Epoch 216: Train Loss: 0.0035, Test Loss: 0.0039, Train L1 Norm: 0.0157, Test L1 Norm: 0.0041, Train Linf Norm: 0.8855, Test Linf Norm: 0.1168\n",
            "Epoch 217: Train Loss: 0.0035, Test Loss: 0.0039, Train L1 Norm: 0.0156, Test L1 Norm: 0.0041, Train Linf Norm: 0.8756, Test Linf Norm: 0.1171\n",
            "Epoch 218: Train Loss: 0.0035, Test Loss: 0.0039, Train L1 Norm: 0.0157, Test L1 Norm: 0.0041, Train Linf Norm: 0.8865, Test Linf Norm: 0.1172\n",
            "Epoch 219: Train Loss: 0.0035, Test Loss: 0.0039, Train L1 Norm: 0.0158, Test L1 Norm: 0.0041, Train Linf Norm: 0.8895, Test Linf Norm: 0.1169\n",
            "Epoch 220: Train Loss: 0.0035, Test Loss: 0.0039, Train L1 Norm: 0.0156, Test L1 Norm: 0.0041, Train Linf Norm: 0.8798, Test Linf Norm: 0.1174\n",
            "Epoch 221: Train Loss: 0.0035, Test Loss: 0.0039, Train L1 Norm: 0.0157, Test L1 Norm: 0.0041, Train Linf Norm: 0.8897, Test Linf Norm: 0.1168\n",
            "Epoch 222: Train Loss: 0.0035, Test Loss: 0.0039, Train L1 Norm: 0.0156, Test L1 Norm: 0.0041, Train Linf Norm: 0.8803, Test Linf Norm: 0.1157\n",
            "Epoch 223: Train Loss: 0.0035, Test Loss: 0.0040, Train L1 Norm: 0.0156, Test L1 Norm: 0.0042, Train Linf Norm: 0.8803, Test Linf Norm: 0.1208\n",
            "Epoch 224: Train Loss: 0.0035, Test Loss: 0.0039, Train L1 Norm: 0.0156, Test L1 Norm: 0.0041, Train Linf Norm: 0.8814, Test Linf Norm: 0.1187\n",
            "Epoch 225: Train Loss: 0.0035, Test Loss: 0.0039, Train L1 Norm: 0.0162, Test L1 Norm: 0.0041, Train Linf Norm: 0.9138, Test Linf Norm: 0.1189\n",
            "Epoch 226: Train Loss: 0.0036, Test Loss: 0.0040, Train L1 Norm: 0.0151, Test L1 Norm: 0.0041, Train Linf Norm: 0.8432, Test Linf Norm: 0.1175\n",
            "Epoch 227: Train Loss: 0.0036, Test Loss: 0.0039, Train L1 Norm: 0.0164, Test L1 Norm: 0.0042, Train Linf Norm: 0.9261, Test Linf Norm: 0.1203\n",
            "Epoch 228: Train Loss: 0.0036, Test Loss: 0.0039, Train L1 Norm: 0.0164, Test L1 Norm: 0.0040, Train Linf Norm: 0.9275, Test Linf Norm: 0.1126\n",
            "Epoch 229: Train Loss: 0.0037, Test Loss: 0.0040, Train L1 Norm: 0.0155, Test L1 Norm: 0.0042, Train Linf Norm: 0.8704, Test Linf Norm: 0.1204\n",
            "Epoch 230: Train Loss: 0.0038, Test Loss: 0.0041, Train L1 Norm: 0.0164, Test L1 Norm: 0.0042, Train Linf Norm: 0.9226, Test Linf Norm: 0.1191\n",
            "Epoch 231: Train Loss: 0.0039, Test Loss: 0.0049, Train L1 Norm: 0.0156, Test L1 Norm: 0.0044, Train Linf Norm: 0.8685, Test Linf Norm: 0.1199\n",
            "Epoch 232: Train Loss: 0.0040, Test Loss: 0.0043, Train L1 Norm: 0.0163, Test L1 Norm: 0.0042, Train Linf Norm: 0.9122, Test Linf Norm: 0.1178\n",
            "Epoch 233: Train Loss: 0.0042, Test Loss: 0.0041, Train L1 Norm: 0.0150, Test L1 Norm: 0.0044, Train Linf Norm: 0.8247, Test Linf Norm: 0.1336\n",
            "Epoch 234: Train Loss: 0.0045, Test Loss: 0.0042, Train L1 Norm: 0.0155, Test L1 Norm: 0.0044, Train Linf Norm: 0.8485, Test Linf Norm: 0.1280\n",
            "Epoch 235: Train Loss: 0.0049, Test Loss: 0.0059, Train L1 Norm: 0.0160, Test L1 Norm: 0.0049, Train Linf Norm: 0.8754, Test Linf Norm: 0.1345\n",
            "Epoch 236: Train Loss: 0.0054, Test Loss: 0.0071, Train L1 Norm: 0.0167, Test L1 Norm: 0.0050, Train Linf Norm: 0.9114, Test Linf Norm: 0.1170\n",
            "Epoch 237: Train Loss: 0.0059, Test Loss: 0.0055, Train L1 Norm: 0.0171, Test L1 Norm: 0.0047, Train Linf Norm: 0.9179, Test Linf Norm: 0.1253\n",
            "Epoch 238: Train Loss: 0.0064, Test Loss: 0.0102, Train L1 Norm: 0.0219, Test L1 Norm: 0.0069, Train Linf Norm: 1.2170, Test Linf Norm: 0.1633\n",
            "Epoch 239: Train Loss: 0.0069, Test Loss: 0.0056, Train L1 Norm: 0.0170, Test L1 Norm: 0.0051, Train Linf Norm: 0.8913, Test Linf Norm: 0.1541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 20:01:19,526]\u001b[0m Trial 34 finished with value: 0.004713432249426842 and parameters: {'n_layers': 6, 'n_units_0': 1962, 'n_units_1': 1668, 'n_units_2': 1256, 'n_units_3': 1363, 'n_units_4': 408, 'n_units_5': 1341, 'hidden_activation': 'LeakyReLU', 'output_activation': 'Linear', 'loss': 'MAE', 'optimizer': 'Adagrad', 'lr': 0.0004331907322067762, 'batch_size': 64, 'n_epochs': 240, 'scheduler': 'CosineAnnealingLR', 't_max_fraction': 0.13097129674868327, 'eta_min': 5.305846306230611e-07}. Best is trial 34 with value: 0.004713432249426842.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 240: Train Loss: 0.0073, Test Loss: 0.0042, Train L1 Norm: 0.0135, Test L1 Norm: 0.0047, Train Linf Norm: 0.6561, Test Linf Norm: 0.1489\n",
            "Epoch 1: Train Loss: 0.2233, Test Loss: 0.1325, Train L1 Norm: 0.3527, Test L1 Norm: 0.0990, Train Linf Norm: 16.0603, Test Linf Norm: 3.3265\n",
            "Epoch 2: Train Loss: 0.1044, Test Loss: 0.0644, Train L1 Norm: 0.1881, Test L1 Norm: 0.0616, Train Linf Norm: 9.1334, Test Linf Norm: 2.2018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 20:01:41,744]\u001b[0m Trial 35 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss: 0.0849, Test Loss: 0.0810, Train L1 Norm: 0.1529, Test L1 Norm: 0.0596, Train Linf Norm: 7.4502, Test Linf Norm: 1.7458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 20:01:47,850]\u001b[0m Trial 36 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.7075, Test Loss: 0.3800, Train L1 Norm: 2.3661, Test L1 Norm: 0.6146, Train Linf Norm: 115.9905, Test Linf Norm: 18.3641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 20:01:50,426]\u001b[0m Trial 37 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 1.0786, Test Loss: 0.5602, Train L1 Norm: 1.5044, Test L1 Norm: 0.3855, Train Linf Norm: 848.3310, Test Linf Norm: 69.2371\n",
            "Epoch 1: Train Loss: 0.1597, Test Loss: 0.0509, Train L1 Norm: 0.6079, Test L1 Norm: 0.0917, Train Linf Norm: 33.7137, Test Linf Norm: 3.5880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 20:02:04,474]\u001b[0m Trial 38 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 0.0744, Test Loss: 0.0892, Train L1 Norm: 0.1751, Test L1 Norm: 0.0937, Train Linf Norm: 8.7534, Test Linf Norm: 3.3696\n",
            "Epoch 1: Train Loss: 0.3559, Test Loss: 0.1222, Train L1 Norm: 0.3357, Test L1 Norm: 0.1185, Train Linf Norm: 5.6914, Test Linf Norm: 1.9120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 20:02:20,561]\u001b[0m Trial 39 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 0.1944, Test Loss: 0.2292, Train L1 Norm: 0.1709, Test L1 Norm: 0.1275, Train Linf Norm: 2.9047, Test Linf Norm: 1.5215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 20:02:22,440]\u001b[0m Trial 40 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 6.9432, Test Loss: 1.2732, Train L1 Norm: 8.7780, Test L1 Norm: 0.4538, Train Linf Norm: 2006.4374, Test Linf Norm: 33.6835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 20:02:28,685]\u001b[0m Trial 41 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.5386, Test Loss: 0.0081, Train L1 Norm: 0.8012, Test L1 Norm: 0.1710, Train Linf Norm: 59.4700, Test Linf Norm: 9.6416\n",
            "Epoch 1: Train Loss: 0.4782, Test Loss: 0.0190, Train L1 Norm: 0.6999, Test L1 Norm: 0.1114, Train Linf Norm: 36.0094, Test Linf Norm: 4.4302\n",
            "Epoch 2: Train Loss: 0.0210, Test Loss: 0.0064, Train L1 Norm: 0.1856, Test L1 Norm: 0.0629, Train Linf Norm: 9.5583, Test Linf Norm: 2.0866\n",
            "Epoch 3: Train Loss: 0.0165, Test Loss: 0.0021, Train L1 Norm: 0.1357, Test L1 Norm: 0.0545, Train Linf Norm: 6.9185, Test Linf Norm: 2.1829\n",
            "Epoch 4: Train Loss: 0.0191, Test Loss: 0.0015, Train L1 Norm: 0.1725, Test L1 Norm: 0.0481, Train Linf Norm: 9.2116, Test Linf Norm: 1.9054\n",
            "Epoch 5: Train Loss: 0.0028, Test Loss: 0.0008, Train L1 Norm: 0.1038, Test L1 Norm: 0.0387, Train Linf Norm: 5.4130, Test Linf Norm: 1.5008\n",
            "Epoch 6: Train Loss: 0.0071, Test Loss: 0.0010, Train L1 Norm: 0.1251, Test L1 Norm: 0.0421, Train Linf Norm: 6.7819, Test Linf Norm: 1.6910\n",
            "Epoch 7: Train Loss: 0.0024, Test Loss: 0.0006, Train L1 Norm: 0.1127, Test L1 Norm: 0.0343, Train Linf Norm: 6.2009, Test Linf Norm: 1.3437\n",
            "Epoch 8: Train Loss: 0.0012, Test Loss: 0.0005, Train L1 Norm: 0.0813, Test L1 Norm: 0.0303, Train Linf Norm: 4.3467, Test Linf Norm: 1.1555\n",
            "Epoch 9: Train Loss: 0.0007, Test Loss: 0.0004, Train L1 Norm: 0.0725, Test L1 Norm: 0.0263, Train Linf Norm: 3.8737, Test Linf Norm: 0.9513\n",
            "Epoch 10: Train Loss: 0.0007, Test Loss: 0.0004, Train L1 Norm: 0.0822, Test L1 Norm: 0.0238, Train Linf Norm: 4.5193, Test Linf Norm: 0.8295\n",
            "Epoch 11: Train Loss: 0.0008, Test Loss: 0.0003, Train L1 Norm: 0.0608, Test L1 Norm: 0.0259, Train Linf Norm: 3.1851, Test Linf Norm: 0.9684\n",
            "Epoch 12: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.0586, Test L1 Norm: 0.0247, Train Linf Norm: 3.0806, Test Linf Norm: 0.9223\n",
            "Epoch 13: Train Loss: 0.0007, Test Loss: 0.0004, Train L1 Norm: 0.0591, Test L1 Norm: 0.0249, Train Linf Norm: 3.1321, Test Linf Norm: 0.9208\n",
            "Epoch 14: Train Loss: 0.0006, Test Loss: 0.0003, Train L1 Norm: 0.0561, Test L1 Norm: 0.0230, Train Linf Norm: 2.9298, Test Linf Norm: 0.8500\n",
            "Epoch 15: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.0558, Test L1 Norm: 0.0201, Train Linf Norm: 2.9662, Test Linf Norm: 0.6904\n",
            "Epoch 16: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.0541, Test L1 Norm: 0.0215, Train Linf Norm: 2.8832, Test Linf Norm: 0.7808\n",
            "Epoch 17: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0491, Test L1 Norm: 0.0198, Train Linf Norm: 2.5775, Test Linf Norm: 0.6958\n",
            "Epoch 18: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0502, Test L1 Norm: 0.0195, Train Linf Norm: 2.6217, Test Linf Norm: 0.6899\n",
            "Epoch 19: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0458, Test L1 Norm: 0.0189, Train Linf Norm: 2.3786, Test Linf Norm: 0.6594\n",
            "Epoch 20: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0473, Test L1 Norm: 0.0187, Train Linf Norm: 2.4722, Test Linf Norm: 0.6537\n",
            "Epoch 21: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0440, Test L1 Norm: 0.0188, Train Linf Norm: 2.2863, Test Linf Norm: 0.6628\n",
            "Epoch 22: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0444, Test L1 Norm: 0.0185, Train Linf Norm: 2.3108, Test Linf Norm: 0.6530\n",
            "Epoch 23: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0400, Test L1 Norm: 0.0187, Train Linf Norm: 2.0351, Test Linf Norm: 0.6656\n",
            "Epoch 24: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0432, Test L1 Norm: 0.0179, Train Linf Norm: 2.2518, Test Linf Norm: 0.6213\n",
            "Epoch 25: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0405, Test L1 Norm: 0.0178, Train Linf Norm: 2.0764, Test Linf Norm: 0.6185\n",
            "Epoch 26: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0412, Test L1 Norm: 0.0175, Train Linf Norm: 2.1268, Test Linf Norm: 0.6032\n",
            "Epoch 27: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0396, Test L1 Norm: 0.0181, Train Linf Norm: 2.0167, Test Linf Norm: 0.6381\n",
            "Epoch 28: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0407, Test L1 Norm: 0.0174, Train Linf Norm: 2.0892, Test Linf Norm: 0.5973\n",
            "Epoch 29: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0401, Test L1 Norm: 0.0179, Train Linf Norm: 2.0591, Test Linf Norm: 0.6308\n",
            "Epoch 30: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0397, Test L1 Norm: 0.0178, Train Linf Norm: 2.0294, Test Linf Norm: 0.6245\n",
            "Epoch 31: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0411, Test L1 Norm: 0.0177, Train Linf Norm: 2.1260, Test Linf Norm: 0.6179\n",
            "Epoch 32: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0404, Test L1 Norm: 0.0177, Train Linf Norm: 2.0879, Test Linf Norm: 0.6207\n",
            "Epoch 33: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0403, Test L1 Norm: 0.0176, Train Linf Norm: 2.0775, Test Linf Norm: 0.6174\n",
            "Epoch 34: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0402, Test L1 Norm: 0.0176, Train Linf Norm: 2.0704, Test Linf Norm: 0.6174\n",
            "Epoch 35: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0402, Test L1 Norm: 0.0177, Train Linf Norm: 2.0735, Test Linf Norm: 0.6187\n",
            "Epoch 36: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0401, Test L1 Norm: 0.0176, Train Linf Norm: 2.0565, Test Linf Norm: 0.6154\n",
            "Epoch 37: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0400, Test L1 Norm: 0.0177, Train Linf Norm: 2.0604, Test Linf Norm: 0.6223\n",
            "Epoch 38: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0410, Test L1 Norm: 0.0179, Train Linf Norm: 2.1226, Test Linf Norm: 0.6282\n",
            "Epoch 39: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0397, Test L1 Norm: 0.0176, Train Linf Norm: 2.0349, Test Linf Norm: 0.6142\n",
            "Epoch 40: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0395, Test L1 Norm: 0.0176, Train Linf Norm: 2.0186, Test Linf Norm: 0.6153\n",
            "Epoch 41: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0398, Test L1 Norm: 0.0178, Train Linf Norm: 2.0141, Test Linf Norm: 0.6265\n",
            "Epoch 42: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0385, Test L1 Norm: 0.0176, Train Linf Norm: 1.9482, Test Linf Norm: 0.6132\n",
            "Epoch 43: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0398, Test L1 Norm: 0.0171, Train Linf Norm: 2.0401, Test Linf Norm: 0.5904\n",
            "Epoch 44: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0388, Test L1 Norm: 0.0178, Train Linf Norm: 1.9899, Test Linf Norm: 0.6318\n",
            "Epoch 45: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0399, Test L1 Norm: 0.0173, Train Linf Norm: 2.0538, Test Linf Norm: 0.6071\n",
            "Epoch 46: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0363, Test L1 Norm: 0.0173, Train Linf Norm: 1.8327, Test Linf Norm: 0.6034\n",
            "Epoch 47: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0377, Test L1 Norm: 0.0171, Train Linf Norm: 1.9325, Test Linf Norm: 0.6017\n",
            "Epoch 48: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0391, Test L1 Norm: 0.0156, Train Linf Norm: 2.0108, Test Linf Norm: 0.5095\n",
            "Epoch 49: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0347, Test L1 Norm: 0.0162, Train Linf Norm: 1.7412, Test Linf Norm: 0.5530\n",
            "Epoch 50: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0351, Test L1 Norm: 0.0164, Train Linf Norm: 1.7614, Test Linf Norm: 0.5628\n",
            "Epoch 51: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0339, Test L1 Norm: 0.0159, Train Linf Norm: 1.6764, Test Linf Norm: 0.5417\n",
            "Epoch 52: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0333, Test L1 Norm: 0.0159, Train Linf Norm: 1.6270, Test Linf Norm: 0.5491\n",
            "Epoch 53: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0384, Test L1 Norm: 0.0157, Train Linf Norm: 1.9885, Test Linf Norm: 0.5247\n",
            "Epoch 54: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0362, Test L1 Norm: 0.0155, Train Linf Norm: 1.8478, Test Linf Norm: 0.5343\n",
            "Epoch 55: Train Loss: 0.0002, Test Loss: 0.0003, Train L1 Norm: 0.0338, Test L1 Norm: 0.0160, Train Linf Norm: 1.6879, Test Linf Norm: 0.5408\n",
            "Epoch 56: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0312, Test L1 Norm: 0.0149, Train Linf Norm: 1.5455, Test Linf Norm: 0.5012\n",
            "Epoch 57: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.0289, Test L1 Norm: 0.0150, Train Linf Norm: 1.3691, Test Linf Norm: 0.5013\n",
            "Epoch 58: Train Loss: 0.0009, Test Loss: 0.0002, Train L1 Norm: 0.0428, Test L1 Norm: 0.0149, Train Linf Norm: 2.2237, Test Linf Norm: 0.5066\n",
            "Epoch 59: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0295, Test L1 Norm: 0.0149, Train Linf Norm: 1.4122, Test Linf Norm: 0.5228\n",
            "Epoch 60: Train Loss: 0.0007, Test Loss: 0.0003, Train L1 Norm: 0.0323, Test L1 Norm: 0.0157, Train Linf Norm: 1.5606, Test Linf Norm: 0.5443\n",
            "Epoch 61: Train Loss: 0.0011, Test Loss: 0.0002, Train L1 Norm: 0.0331, Test L1 Norm: 0.0145, Train Linf Norm: 1.5907, Test Linf Norm: 0.4800\n",
            "Epoch 62: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0292, Test L1 Norm: 0.0141, Train Linf Norm: 1.4179, Test Linf Norm: 0.4743\n",
            "Epoch 63: Train Loss: 0.0005, Test Loss: 0.0002, Train L1 Norm: 0.0284, Test L1 Norm: 0.0148, Train Linf Norm: 1.3442, Test Linf Norm: 0.5218\n",
            "Epoch 64: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0273, Test L1 Norm: 0.0145, Train Linf Norm: 1.2955, Test Linf Norm: 0.5088\n",
            "Epoch 65: Train Loss: 0.0018, Test Loss: 0.0007, Train L1 Norm: 0.0281, Test L1 Norm: 0.0205, Train Linf Norm: 1.2719, Test Linf Norm: 0.7105\n",
            "Epoch 66: Train Loss: 0.0008, Test Loss: 0.0002, Train L1 Norm: 0.0284, Test L1 Norm: 0.0143, Train Linf Norm: 1.3492, Test Linf Norm: 0.4920\n",
            "Epoch 67: Train Loss: 0.0057, Test Loss: 0.0006, Train L1 Norm: 0.0395, Test L1 Norm: 0.0176, Train Linf Norm: 1.8447, Test Linf Norm: 0.6358\n",
            "Epoch 68: Train Loss: 0.0008, Test Loss: 0.0002, Train L1 Norm: 0.0337, Test L1 Norm: 0.0152, Train Linf Norm: 1.6594, Test Linf Norm: 0.5363\n",
            "Epoch 69: Train Loss: 0.0016, Test Loss: 0.0003, Train L1 Norm: 0.0338, Test L1 Norm: 0.0149, Train Linf Norm: 1.6271, Test Linf Norm: 0.5078\n",
            "Epoch 70: Train Loss: 0.0016, Test Loss: 0.0005, Train L1 Norm: 0.0272, Test L1 Norm: 0.0156, Train Linf Norm: 1.2383, Test Linf Norm: 0.5416\n",
            "Epoch 71: Train Loss: 0.0007, Test Loss: 0.0003, Train L1 Norm: 0.0275, Test L1 Norm: 0.0145, Train Linf Norm: 1.2862, Test Linf Norm: 0.4620\n",
            "Epoch 72: Train Loss: 0.0004, Test Loss: 0.0631, Train L1 Norm: 0.0254, Test L1 Norm: 0.0712, Train Linf Norm: 1.1460, Test Linf Norm: 1.1064\n",
            "Epoch 73: Train Loss: 0.0006, Test Loss: 0.0002, Train L1 Norm: 0.0285, Test L1 Norm: 0.0140, Train Linf Norm: 1.3756, Test Linf Norm: 0.4990\n",
            "Epoch 74: Train Loss: 0.0004, Test Loss: 0.0001, Train L1 Norm: 0.0242, Test L1 Norm: 0.0134, Train Linf Norm: 1.0891, Test Linf Norm: 0.4605\n",
            "Epoch 75: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0255, Test L1 Norm: 0.0133, Train Linf Norm: 1.2214, Test Linf Norm: 0.4662\n",
            "Epoch 76: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0226, Test L1 Norm: 0.0137, Train Linf Norm: 1.0358, Test Linf Norm: 0.4855\n",
            "Epoch 77: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0234, Test L1 Norm: 0.0134, Train Linf Norm: 1.1089, Test Linf Norm: 0.4731\n",
            "Epoch 78: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0221, Test L1 Norm: 0.0129, Train Linf Norm: 1.0208, Test Linf Norm: 0.4423\n",
            "Epoch 79: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0253, Test L1 Norm: 0.0131, Train Linf Norm: 1.2139, Test Linf Norm: 0.4529\n",
            "Epoch 80: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0218, Test L1 Norm: 0.0130, Train Linf Norm: 0.9983, Test Linf Norm: 0.4352\n",
            "Epoch 81: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0197, Test L1 Norm: 0.0127, Train Linf Norm: 0.8820, Test Linf Norm: 0.4337\n",
            "Epoch 82: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0208, Test L1 Norm: 0.0132, Train Linf Norm: 0.9627, Test Linf Norm: 0.4737\n",
            "Epoch 83: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0215, Test L1 Norm: 0.0127, Train Linf Norm: 1.0119, Test Linf Norm: 0.4444\n",
            "Epoch 84: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0225, Test L1 Norm: 0.0127, Train Linf Norm: 1.0733, Test Linf Norm: 0.4416\n",
            "Epoch 85: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0177, Test L1 Norm: 0.0125, Train Linf Norm: 0.7735, Test Linf Norm: 0.4289\n",
            "Epoch 86: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0210, Test L1 Norm: 0.0129, Train Linf Norm: 0.9877, Test Linf Norm: 0.4659\n",
            "Epoch 87: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0196, Test L1 Norm: 0.0125, Train Linf Norm: 0.8970, Test Linf Norm: 0.4366\n",
            "Epoch 88: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0182, Test L1 Norm: 0.0124, Train Linf Norm: 0.8088, Test Linf Norm: 0.4288\n",
            "Epoch 89: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0181, Test L1 Norm: 0.0123, Train Linf Norm: 0.8066, Test Linf Norm: 0.4272\n",
            "Epoch 90: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0184, Test L1 Norm: 0.0124, Train Linf Norm: 0.8258, Test Linf Norm: 0.4284\n",
            "Epoch 91: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0194, Test L1 Norm: 0.0125, Train Linf Norm: 0.8880, Test Linf Norm: 0.4418\n",
            "Epoch 92: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0204, Test L1 Norm: 0.0124, Train Linf Norm: 0.9544, Test Linf Norm: 0.4368\n",
            "Epoch 93: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0186, Test L1 Norm: 0.0125, Train Linf Norm: 0.8358, Test Linf Norm: 0.4416\n",
            "Epoch 94: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0186, Test L1 Norm: 0.0124, Train Linf Norm: 0.8337, Test Linf Norm: 0.4295\n",
            "Epoch 95: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0187, Test L1 Norm: 0.0124, Train Linf Norm: 0.8454, Test Linf Norm: 0.4325\n",
            "Epoch 96: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0189, Test L1 Norm: 0.0123, Train Linf Norm: 0.8696, Test Linf Norm: 0.4298\n",
            "Epoch 97: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0186, Test L1 Norm: 0.0123, Train Linf Norm: 0.8450, Test Linf Norm: 0.4312\n",
            "Epoch 98: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0183, Test L1 Norm: 0.0123, Train Linf Norm: 0.8223, Test Linf Norm: 0.4301\n",
            "Epoch 99: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0186, Test L1 Norm: 0.0123, Train Linf Norm: 0.8368, Test Linf Norm: 0.4300\n",
            "Epoch 100: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0185, Test L1 Norm: 0.0123, Train Linf Norm: 0.8394, Test Linf Norm: 0.4300\n",
            "Epoch 101: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0185, Test L1 Norm: 0.0123, Train Linf Norm: 0.8427, Test Linf Norm: 0.4305\n",
            "Epoch 102: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0187, Test L1 Norm: 0.0123, Train Linf Norm: 0.8454, Test Linf Norm: 0.4311\n",
            "Epoch 103: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0186, Test L1 Norm: 0.0124, Train Linf Norm: 0.8479, Test Linf Norm: 0.4328\n",
            "Epoch 104: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0184, Test L1 Norm: 0.0123, Train Linf Norm: 0.8171, Test Linf Norm: 0.4264\n",
            "Epoch 105: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0185, Test L1 Norm: 0.0124, Train Linf Norm: 0.8397, Test Linf Norm: 0.4359\n",
            "Epoch 106: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0188, Test L1 Norm: 0.0125, Train Linf Norm: 0.8554, Test Linf Norm: 0.4451\n",
            "Epoch 107: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0186, Test L1 Norm: 0.0123, Train Linf Norm: 0.8375, Test Linf Norm: 0.4247\n",
            "Epoch 108: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0187, Test L1 Norm: 0.0123, Train Linf Norm: 0.8254, Test Linf Norm: 0.4309\n",
            "Epoch 109: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0182, Test L1 Norm: 0.0123, Train Linf Norm: 0.8237, Test Linf Norm: 0.4330\n",
            "Epoch 110: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0200, Test L1 Norm: 0.0122, Train Linf Norm: 0.9385, Test Linf Norm: 0.4203\n",
            "Epoch 111: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0205, Test L1 Norm: 0.0123, Train Linf Norm: 0.9552, Test Linf Norm: 0.4291\n",
            "Epoch 112: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0176, Test L1 Norm: 0.0121, Train Linf Norm: 0.7750, Test Linf Norm: 0.4198\n",
            "Epoch 113: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0182, Test L1 Norm: 0.0122, Train Linf Norm: 0.8100, Test Linf Norm: 0.4225\n",
            "Epoch 114: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0184, Test L1 Norm: 0.0121, Train Linf Norm: 0.8325, Test Linf Norm: 0.4222\n",
            "Epoch 115: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0195, Test L1 Norm: 0.0121, Train Linf Norm: 0.8955, Test Linf Norm: 0.4171\n",
            "Epoch 116: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0196, Test L1 Norm: 0.0123, Train Linf Norm: 0.9031, Test Linf Norm: 0.4323\n",
            "Epoch 117: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0176, Test L1 Norm: 0.0121, Train Linf Norm: 0.7781, Test Linf Norm: 0.4152\n",
            "Epoch 118: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0189, Test L1 Norm: 0.0120, Train Linf Norm: 0.8641, Test Linf Norm: 0.4166\n",
            "Epoch 119: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0183, Test L1 Norm: 0.0120, Train Linf Norm: 0.8285, Test Linf Norm: 0.4146\n",
            "Epoch 120: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0204, Test L1 Norm: 0.0120, Train Linf Norm: 0.9596, Test Linf Norm: 0.4121\n",
            "Epoch 121: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0159, Test L1 Norm: 0.0122, Train Linf Norm: 0.6736, Test Linf Norm: 0.4343\n",
            "Epoch 122: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0185, Test L1 Norm: 0.0121, Train Linf Norm: 0.8426, Test Linf Norm: 0.4266\n",
            "Epoch 123: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0183, Test L1 Norm: 0.0121, Train Linf Norm: 0.8311, Test Linf Norm: 0.4166\n",
            "Epoch 124: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0192, Test L1 Norm: 0.0122, Train Linf Norm: 0.8742, Test Linf Norm: 0.4337\n",
            "Epoch 125: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0179, Test L1 Norm: 0.0119, Train Linf Norm: 0.8045, Test Linf Norm: 0.4029\n",
            "Epoch 126: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0179, Test L1 Norm: 0.0116, Train Linf Norm: 0.7701, Test Linf Norm: 0.3981\n",
            "Epoch 127: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0180, Test L1 Norm: 0.0116, Train Linf Norm: 0.8020, Test Linf Norm: 0.4059\n",
            "Epoch 128: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0212, Test L1 Norm: 0.0123, Train Linf Norm: 1.0014, Test Linf Norm: 0.4473\n",
            "Epoch 129: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0225, Test L1 Norm: 0.0118, Train Linf Norm: 1.0818, Test Linf Norm: 0.4104\n",
            "Epoch 130: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0171, Test L1 Norm: 0.0114, Train Linf Norm: 0.7561, Test Linf Norm: 0.3952\n",
            "Epoch 131: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0146, Test L1 Norm: 0.0115, Train Linf Norm: 0.5978, Test Linf Norm: 0.3959\n",
            "Epoch 132: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0188, Test L1 Norm: 0.0113, Train Linf Norm: 0.8554, Test Linf Norm: 0.3941\n",
            "Epoch 133: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0195, Test L1 Norm: 0.0121, Train Linf Norm: 0.9160, Test Linf Norm: 0.4403\n",
            "Epoch 134: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0326, Test L1 Norm: 0.0120, Train Linf Norm: 1.7388, Test Linf Norm: 0.4430\n",
            "Epoch 135: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0189, Test L1 Norm: 0.0117, Train Linf Norm: 0.8577, Test Linf Norm: 0.4301\n",
            "Epoch 136: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0181, Test L1 Norm: 0.0114, Train Linf Norm: 0.8309, Test Linf Norm: 0.3957\n",
            "Epoch 137: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0176, Test L1 Norm: 0.0119, Train Linf Norm: 0.8057, Test Linf Norm: 0.4074\n",
            "Epoch 138: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0160, Test L1 Norm: 0.0115, Train Linf Norm: 0.6689, Test Linf Norm: 0.4060\n",
            "Epoch 139: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0158, Test L1 Norm: 0.0110, Train Linf Norm: 0.6848, Test Linf Norm: 0.3878\n",
            "Epoch 140: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0166, Test L1 Norm: 0.0110, Train Linf Norm: 0.7376, Test Linf Norm: 0.3849\n",
            "Epoch 141: Train Loss: 0.0001, Test Loss: 0.0003, Train L1 Norm: 0.0150, Test L1 Norm: 0.0115, Train Linf Norm: 0.6415, Test Linf Norm: 0.3834\n",
            "Epoch 142: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0152, Test L1 Norm: 0.0110, Train Linf Norm: 0.6606, Test Linf Norm: 0.3824\n",
            "Epoch 143: Train Loss: 0.0001, Test Loss: 0.0010, Train L1 Norm: 0.0167, Test L1 Norm: 0.0149, Train Linf Norm: 0.7518, Test Linf Norm: 0.4162\n",
            "Epoch 144: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0161, Test L1 Norm: 0.0111, Train Linf Norm: 0.7134, Test Linf Norm: 0.3853\n",
            "Epoch 145: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0164, Test L1 Norm: 0.0109, Train Linf Norm: 0.7291, Test Linf Norm: 0.3861\n",
            "Epoch 146: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0141, Test L1 Norm: 0.0108, Train Linf Norm: 0.5990, Test Linf Norm: 0.3816\n",
            "Epoch 147: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0138, Test L1 Norm: 0.0114, Train Linf Norm: 0.5676, Test Linf Norm: 0.3993\n",
            "Epoch 148: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0141, Test L1 Norm: 0.0109, Train Linf Norm: 0.5948, Test Linf Norm: 0.3843\n",
            "Epoch 149: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0148, Test L1 Norm: 0.0108, Train Linf Norm: 0.6521, Test Linf Norm: 0.3831\n",
            "Epoch 150: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0148, Test L1 Norm: 0.0108, Train Linf Norm: 0.6489, Test Linf Norm: 0.3826\n",
            "Epoch 151: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0144, Test L1 Norm: 0.0107, Train Linf Norm: 0.6303, Test Linf Norm: 0.3790\n",
            "Epoch 152: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0143, Test L1 Norm: 0.0108, Train Linf Norm: 0.6209, Test Linf Norm: 0.3758\n",
            "Epoch 153: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0154, Test L1 Norm: 0.0107, Train Linf Norm: 0.6897, Test Linf Norm: 0.3766\n",
            "Epoch 154: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0139, Test L1 Norm: 0.0107, Train Linf Norm: 0.6023, Test Linf Norm: 0.3743\n",
            "Epoch 155: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0128, Test L1 Norm: 0.0107, Train Linf Norm: 0.5322, Test Linf Norm: 0.3770\n",
            "Epoch 156: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0143, Test L1 Norm: 0.0107, Train Linf Norm: 0.5324, Test Linf Norm: 0.3780\n",
            "Epoch 157: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0137, Test L1 Norm: 0.0106, Train Linf Norm: 0.5845, Test Linf Norm: 0.3748\n",
            "Epoch 158: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0140, Test L1 Norm: 0.0106, Train Linf Norm: 0.6074, Test Linf Norm: 0.3742\n",
            "Epoch 159: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0142, Test L1 Norm: 0.0106, Train Linf Norm: 0.6177, Test Linf Norm: 0.3748\n",
            "Epoch 160: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0138, Test L1 Norm: 0.0106, Train Linf Norm: 0.5901, Test Linf Norm: 0.3738\n",
            "Epoch 161: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0141, Test L1 Norm: 0.0106, Train Linf Norm: 0.5985, Test Linf Norm: 0.3741\n",
            "Epoch 162: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0140, Test L1 Norm: 0.0106, Train Linf Norm: 0.6044, Test Linf Norm: 0.3743\n",
            "Epoch 163: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0144, Test L1 Norm: 0.0106, Train Linf Norm: 0.6291, Test Linf Norm: 0.3744\n",
            "Epoch 164: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0140, Test L1 Norm: 0.0106, Train Linf Norm: 0.6074, Test Linf Norm: 0.3744\n",
            "Epoch 165: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0139, Test L1 Norm: 0.0106, Train Linf Norm: 0.6036, Test Linf Norm: 0.3744\n",
            "Epoch 166: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0139, Test L1 Norm: 0.0106, Train Linf Norm: 0.5988, Test Linf Norm: 0.3743\n",
            "Epoch 167: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0140, Test L1 Norm: 0.0106, Train Linf Norm: 0.6033, Test Linf Norm: 0.3743\n",
            "Epoch 168: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0142, Test L1 Norm: 0.0106, Train Linf Norm: 0.6153, Test Linf Norm: 0.3740\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 20:23:39,419]\u001b[0m Trial 42 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 169: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0140, Test L1 Norm: 0.0106, Train Linf Norm: 0.6043, Test Linf Norm: 0.3743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 20:23:41,847]\u001b[0m Trial 43 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 2.3902, Test Loss: 0.3311, Train L1 Norm: 1.9929, Test L1 Norm: 0.2254, Train Linf Norm: 1414.7758, Test Linf Norm: 38.6747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 20:23:54,686]\u001b[0m Trial 44 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.2302, Test Loss: 0.0102, Train L1 Norm: 0.5748, Test L1 Norm: 0.1360, Train Linf Norm: 15.6039, Test Linf Norm: 3.1627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 20:23:58,809]\u001b[0m Trial 45 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.3166, Test Loss: 0.1260, Train L1 Norm: 0.4696, Test L1 Norm: 0.1853, Train Linf Norm: 37.4613, Test Linf Norm: 11.1738\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 20:24:03,830]\u001b[0m Trial 46 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.4035, Test Loss: 0.1859, Train L1 Norm: 0.7037, Test L1 Norm: 0.3148, Train Linf Norm: 46.1039, Test Linf Norm: 13.3301\n",
            "Epoch 1: Train Loss: 0.0731, Test Loss: 0.0046, Train L1 Norm: 0.4938, Test L1 Norm: 0.0631, Train Linf Norm: 19.5101, Test Linf Norm: 1.5937\n",
            "Epoch 2: Train Loss: 0.0049, Test Loss: 0.0016, Train L1 Norm: 0.1047, Test L1 Norm: 0.0329, Train Linf Norm: 3.8773, Test Linf Norm: 0.7658\n",
            "Epoch 3: Train Loss: 0.0025, Test Loss: 0.0005, Train L1 Norm: 0.0640, Test L1 Norm: 0.0258, Train Linf Norm: 2.2461, Test Linf Norm: 0.6894\n",
            "Epoch 4: Train Loss: 0.0018, Test Loss: 0.0006, Train L1 Norm: 0.0647, Test L1 Norm: 0.0229, Train Linf Norm: 2.4029, Test Linf Norm: 0.5572\n",
            "Epoch 5: Train Loss: 0.0016, Test Loss: 0.0003, Train L1 Norm: 0.0538, Test L1 Norm: 0.0222, Train Linf Norm: 1.9486, Test Linf Norm: 0.5960\n",
            "Epoch 6: Train Loss: 0.0008, Test Loss: 0.0078, Train L1 Norm: 0.0490, Test L1 Norm: 0.0319, Train Linf Norm: 1.8130, Test Linf Norm: 0.6025\n",
            "Epoch 7: Train Loss: 0.0008, Test Loss: 0.0002, Train L1 Norm: 0.0468, Test L1 Norm: 0.0199, Train Linf Norm: 1.6601, Test Linf Norm: 0.5304\n",
            "Epoch 8: Train Loss: 0.0006, Test Loss: 0.0002, Train L1 Norm: 0.0513, Test L1 Norm: 0.0219, Train Linf Norm: 1.9957, Test Linf Norm: 0.6247\n",
            "Epoch 9: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0373, Test L1 Norm: 0.0171, Train Linf Norm: 1.4176, Test Linf Norm: 0.4529\n",
            "Epoch 10: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0368, Test L1 Norm: 0.0170, Train Linf Norm: 1.4026, Test Linf Norm: 0.4575\n",
            "Epoch 11: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0358, Test L1 Norm: 0.0171, Train Linf Norm: 1.3458, Test Linf Norm: 0.4566\n",
            "Epoch 12: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0363, Test L1 Norm: 0.0167, Train Linf Norm: 1.3842, Test Linf Norm: 0.4456\n",
            "Epoch 13: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0401, Test L1 Norm: 0.0165, Train Linf Norm: 1.5694, Test Linf Norm: 0.4376\n",
            "Epoch 14: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0360, Test L1 Norm: 0.0162, Train Linf Norm: 1.3820, Test Linf Norm: 0.4335\n",
            "Epoch 15: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0332, Test L1 Norm: 0.0164, Train Linf Norm: 1.2452, Test Linf Norm: 0.4400\n",
            "Epoch 16: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0357, Test L1 Norm: 0.0162, Train Linf Norm: 1.3748, Test Linf Norm: 0.4366\n",
            "Epoch 17: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0360, Test L1 Norm: 0.0160, Train Linf Norm: 1.3870, Test Linf Norm: 0.4303\n",
            "Epoch 18: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0336, Test L1 Norm: 0.0160, Train Linf Norm: 1.2766, Test Linf Norm: 0.4301\n",
            "Epoch 19: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0357, Test L1 Norm: 0.0160, Train Linf Norm: 1.3818, Test Linf Norm: 0.4218\n",
            "Epoch 20: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0351, Test L1 Norm: 0.0160, Train Linf Norm: 1.3467, Test Linf Norm: 0.4229\n",
            "Epoch 21: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0342, Test L1 Norm: 0.0160, Train Linf Norm: 1.3092, Test Linf Norm: 0.4314\n",
            "Epoch 22: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0325, Test L1 Norm: 0.0159, Train Linf Norm: 1.2278, Test Linf Norm: 0.4270\n",
            "Epoch 23: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0334, Test L1 Norm: 0.0158, Train Linf Norm: 1.2687, Test Linf Norm: 0.4206\n",
            "Epoch 24: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0342, Test L1 Norm: 0.0158, Train Linf Norm: 1.2941, Test Linf Norm: 0.4172\n",
            "Epoch 25: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0330, Test L1 Norm: 0.0158, Train Linf Norm: 1.2528, Test Linf Norm: 0.4199\n",
            "Epoch 26: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0339, Test L1 Norm: 0.0158, Train Linf Norm: 1.2793, Test Linf Norm: 0.4155\n",
            "Epoch 27: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0333, Test L1 Norm: 0.0158, Train Linf Norm: 1.2698, Test Linf Norm: 0.4187\n",
            "Epoch 28: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0333, Test L1 Norm: 0.0158, Train Linf Norm: 1.2688, Test Linf Norm: 0.4199\n",
            "Epoch 29: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0333, Test L1 Norm: 0.0158, Train Linf Norm: 1.2713, Test Linf Norm: 0.4176\n",
            "Epoch 30: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0348, Test L1 Norm: 0.0157, Train Linf Norm: 1.3404, Test Linf Norm: 0.4175\n",
            "Epoch 31: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0333, Test L1 Norm: 0.0158, Train Linf Norm: 1.2666, Test Linf Norm: 0.4186\n",
            "Epoch 32: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0336, Test L1 Norm: 0.0158, Train Linf Norm: 1.2852, Test Linf Norm: 0.4188\n",
            "Epoch 33: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0342, Test L1 Norm: 0.0158, Train Linf Norm: 1.3142, Test Linf Norm: 0.4179\n",
            "Epoch 34: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0339, Test L1 Norm: 0.0158, Train Linf Norm: 1.2880, Test Linf Norm: 0.4181\n",
            "Epoch 35: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0340, Test L1 Norm: 0.0158, Train Linf Norm: 1.3024, Test Linf Norm: 0.4182\n",
            "Epoch 36: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0158, Train Linf Norm: 1.2934, Test Linf Norm: 0.4183\n",
            "Epoch 37: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0339, Test L1 Norm: 0.0157, Train Linf Norm: 1.2998, Test Linf Norm: 0.4183\n",
            "Epoch 38: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0337, Test L1 Norm: 0.0157, Train Linf Norm: 1.2850, Test Linf Norm: 0.4182\n",
            "Epoch 39: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0341, Test L1 Norm: 0.0157, Train Linf Norm: 1.3050, Test Linf Norm: 0.4173\n",
            "Epoch 40: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0337, Test L1 Norm: 0.0157, Train Linf Norm: 1.2872, Test Linf Norm: 0.4183\n",
            "Epoch 41: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0157, Train Linf Norm: 1.2568, Test Linf Norm: 0.4182\n",
            "Epoch 42: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0157, Train Linf Norm: 1.2897, Test Linf Norm: 0.4180\n",
            "Epoch 43: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0157, Train Linf Norm: 1.2958, Test Linf Norm: 0.4179\n",
            "Epoch 44: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0157, Train Linf Norm: 1.2965, Test Linf Norm: 0.4178\n",
            "Epoch 45: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0337, Test L1 Norm: 0.0157, Train Linf Norm: 1.2919, Test Linf Norm: 0.4178\n",
            "Epoch 46: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0157, Train Linf Norm: 1.2922, Test Linf Norm: 0.4178\n",
            "Epoch 47: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0337, Test L1 Norm: 0.0157, Train Linf Norm: 1.2905, Test Linf Norm: 0.4178\n",
            "Epoch 48: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0157, Train Linf Norm: 1.2936, Test Linf Norm: 0.4179\n",
            "Epoch 49: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0157, Train Linf Norm: 1.2942, Test Linf Norm: 0.4179\n",
            "Epoch 50: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0157, Train Linf Norm: 1.2862, Test Linf Norm: 0.4179\n",
            "Epoch 51: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0157, Train Linf Norm: 1.2954, Test Linf Norm: 0.4179\n",
            "Epoch 52: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0157, Train Linf Norm: 1.2925, Test Linf Norm: 0.4179\n",
            "Epoch 53: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0157, Train Linf Norm: 1.2926, Test Linf Norm: 0.4179\n",
            "Epoch 54: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0157, Train Linf Norm: 1.2921, Test Linf Norm: 0.4179\n",
            "Epoch 55: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0157, Train Linf Norm: 1.2900, Test Linf Norm: 0.4179\n",
            "Epoch 56: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0157, Train Linf Norm: 1.2796, Test Linf Norm: 0.4178\n",
            "Epoch 57: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0157, Train Linf Norm: 1.2855, Test Linf Norm: 0.4178\n",
            "Epoch 58: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0157, Train Linf Norm: 1.2917, Test Linf Norm: 0.4178\n",
            "Epoch 59: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0157, Train Linf Norm: 1.2941, Test Linf Norm: 0.4178\n",
            "Epoch 60: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0157, Train Linf Norm: 1.2955, Test Linf Norm: 0.4178\n",
            "Epoch 61: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0157, Train Linf Norm: 1.2934, Test Linf Norm: 0.4178\n",
            "Epoch 62: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0157, Train Linf Norm: 1.2983, Test Linf Norm: 0.4178\n",
            "Epoch 63: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0157, Train Linf Norm: 1.2937, Test Linf Norm: 0.4178\n",
            "Epoch 64: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0157, Train Linf Norm: 1.2960, Test Linf Norm: 0.4178\n",
            "Epoch 65: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0157, Train Linf Norm: 1.2919, Test Linf Norm: 0.4178\n"
          ]
        }
      ],
      "source": [
        "if OPTIMIZE:\n",
        "    # Creating a study object with Optuna with TPE sampler and median pruner \n",
        "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner())\n",
        "\n",
        "    # Running Optuna with 100 trials when we are optimizing.\n",
        "    study.optimize(objective, n_trials=N_TRIALS)\n",
        "\n",
        "    # Printing the best trial information\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "    print(\"  Value: \", trial.value)\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(f\"    {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmMfE9_dUZiS"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phyiHlWEUZiT"
      },
      "outputs": [],
      "source": [
        "# Creating the best network and optimizer using the best hyperparameters\n",
        "if OPTIMIZE:\n",
        "    net, \\\n",
        "    loss_fn, \\\n",
        "    optimizer, \\\n",
        "    batch_size, \\\n",
        "    n_epochs, \\\n",
        "    scheduler, \\\n",
        "    loss_name, \\\n",
        "    optimizer_name, \\\n",
        "    scheduler_name, \\\n",
        "    n_units, \\\n",
        "    n_layers, \\\n",
        "    hidden_activation, \\\n",
        "    output_activation, \\\n",
        "    lr = create_model(trial, optimize=True)\n",
        "# Creating the network with predefined hyperparameters\n",
        "else:\n",
        "    net, \\\n",
        "    loss_fn, \\\n",
        "    optimizer, \\\n",
        "    batch_size, \\\n",
        "    n_epochs, \\\n",
        "    scheduler, \\\n",
        "    loss_name, \\\n",
        "    optimizer_name, \\\n",
        "    scheduler_name, \\\n",
        "    n_units, \\\n",
        "    n_layers, \\\n",
        "    hidden_activation, \\\n",
        "    output_activation, \\\n",
        "    lr = create_model(trial=None, optimize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yq-oY81UZiU"
      },
      "outputs": [],
      "source": [
        "print(\"loss_fn:\", loss_fn)\n",
        "print(\"batch_size:\", batch_size)\n",
        "print(\"n_epochs:\", n_epochs)\n",
        "print(\"scheduler:\", scheduler)\n",
        "print(\"loss_name:\", loss_name)\n",
        "print(\"optimizer_name:\", optimizer_name)\n",
        "print(\"scheduler_name:\", scheduler_name)\n",
        "print(\"n_units:\", n_units)\n",
        "print(\"n_layers:\", n_layers)\n",
        "print(\"hidden_activation:\", hidden_activation)\n",
        "print(\"output_activation:\", output_activation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7aLWdZyUZiW"
      },
      "outputs": [],
      "source": [
        "# Training and evaluating the network using the train_and_eval function\n",
        "train_losses, test_losses, train_metrics, test_metrics = train_and_eval(\n",
        "    net, loss_fn, optimizer, batch_size, n_epochs, scheduler\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akNucrgMUZiW"
      },
      "source": [
        "## Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHsrs2Y-UZic"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# save the network to a .pth file\n",
        "torch.save(net.state_dict(), \"net.pth\")\n",
        "save_file(\"net.pth\")\n",
        "\n",
        "# save the optimizer to a .pth file\n",
        "torch.save(optimizer.state_dict(), \"optimizer.pth\")\n",
        "save_file(\"optimizer.pth\")\n",
        "\n",
        "# save the scheduler to a .pth file if it is not None\n",
        "if scheduler is not None:\n",
        "  torch.save(scheduler.state_dict(), \"scheduler.pth\")\n",
        "  save_file(\"scheduler.pth\")\n",
        "\n",
        "# create a dictionary to store the rest of the variables\n",
        "var_dict = {\n",
        "  \"batch_size\": batch_size,\n",
        "  \"n_epochs\": n_epochs,\n",
        "  \"loss_name\": loss_name,\n",
        "  \"optimizer_name\": optimizer_name,\n",
        "  \"scheduler_name\": scheduler_name,\n",
        "  \"n_units\": n_units,\n",
        "  \"n_layers\": n_layers,\n",
        "  \"hidden_activation_name\": hidden_activation.__class__.__name__,\n",
        "  \"output_activation_name\": output_activation.__class__.__name__,\n",
        "  \"lr\": lr,\n",
        "}\n",
        "\n",
        "# save the dictionary to a .json file\n",
        "with open(\"var_dict.json\", \"w\") as f:\n",
        "  json.dump(var_dict, f)\n",
        "save_file(\"var_dict.json\")\n",
        "\n",
        "# Saving the output of the training using pandas\n",
        "train_df = pd.DataFrame(\n",
        "    {\n",
        "        \"train_loss\": train_losses,\n",
        "        \"test_loss\": test_losses,\n",
        "        \"train_l1_norm\": [m[\"l1_norm\"] for m in train_metrics],\n",
        "        \"test_l1_norm\": [m[\"l1_norm\"] for m in test_metrics],\n",
        "        \"train_linf_norm\": [m[\"linf_norm\"] for m in train_metrics],\n",
        "        \"test_linf_norm\": [m[\"linf_norm\"] for m in test_metrics],\n",
        "    }\n",
        ")\n",
        "train_df.to_csv(\"train_output.csv\", index=False)\n",
        "save_file(\"train_output.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU23l7dIUZie"
      },
      "source": [
        "## Visualizing the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cippWZS6UZie"
      },
      "outputs": [],
      "source": [
        "# Plotting the losses and metrics for the best network \n",
        "plt.figure(figsize=(12, 8))\n",
        "#plt.subplot(2, 2, 1)\n",
        "#plt.plot(train_losses, label=\"Train Loss\")\n",
        "#plt.plot(test_losses, label=\"Test Loss\")\n",
        "#plt.xlabel(\"Epoch\")\n",
        "#plt.ylabel(\"Loss\")\n",
        "#plt.legend()\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot([m[\"l1_norm\"] for m in train_metrics], label=\"Train L1 Norm\")\n",
        "plt.plot([m[\"l1_norm\"] for m in test_metrics], label=\"Test L1 Norm\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"L1 Norm\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-3, 1e2)\n",
        "plt.legend()\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot([m[\"linf_norm\"] for m in train_metrics], label=\"Train Linf Norm\")\n",
        "plt.plot([m[\"linf_norm\"] for m in test_metrics], label=\"Test Linf Norm\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Linf Norm\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-3, 1e2)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Added plotting MSE of training data and MSE of test data in one plot \n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(train_losses,label=\"training data\")\n",
        "plt.plot(test_losses,label=\"test data\")\n",
        "#if scheduler is not None:\n",
        "#    plt.plot([scheduler.get_last_lr()[0] for _ in range(n_epochs)], label=\"Learning rate\") \n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-7, 1e0)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiEDutxIUZig"
      },
      "source": [
        "## Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7Mj990wUZih"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# load the dictionary from the .json file\n",
        "with open(\"var_dict.json\", \"r\") as f:\n",
        "  var_dict_loaded = json.load(f)\n",
        "\n",
        "# extract the variables from the dictionary\n",
        "batch_size_loaded = var_dict_loaded[\"batch_size\"]\n",
        "n_epochs_loaded = var_dict_loaded[\"n_epochs\"]\n",
        "loss_name_loaded = var_dict_loaded[\"loss_name\"]\n",
        "optimizer_name_loaded = var_dict_loaded[\"optimizer_name\"]\n",
        "scheduler_name_loaded = var_dict_loaded[\"scheduler_name\"]\n",
        "n_units_loaded = var_dict_loaded[\"n_units\"]\n",
        "n_layers_loaded = var_dict_loaded[\"n_layers\"]\n",
        "hidden_activation_name_loaded = var_dict_loaded[\"hidden_activation_name\"]\n",
        "output_activation_name_loaded = var_dict_loaded[\"output_activation_name\"]\n",
        "lr_loaded = var_dict_loaded[\"lr\"]\n",
        "\n",
        "# create the activation functions from their names\n",
        "if hidden_activation_name_loaded == \"ReLU\":\n",
        "  hidden_activation_loaded = nn.ReLU()\n",
        "elif hidden_activation_name_loaded == \"LeakyReLU\":\n",
        "  hidden_activation_loaded = nn.LeakyReLU() \n",
        "elif hidden_activation_name_loaded == \"ELU\":\n",
        "  hidden_activation_loaded = nn.ELU() \n",
        "elif hidden_activation_name_loaded == \"Tanh\":\n",
        "  hidden_activation_loaded = nn.Tanh()\n",
        "else:\n",
        "  hidden_activation_loaded = nn.Sigmoid()\n",
        "\n",
        "if output_activation_name_loaded == \"ReLU\":\n",
        "    output_activation_loaded = nn.ReLU()\n",
        "elif output_activation_name_loaded == \"Softplus\":\n",
        "    output_activation_loaded = nn.Softplus()\n",
        "else:\n",
        "    output_activation_loaded = nn.Identity()\n",
        "\n",
        "\n",
        "\n",
        "# load the network from the .pth file\n",
        "net_loaded = Net(n_layers_loaded, n_units_loaded, hidden_activation_loaded, output_activation_loaded).to(device)\n",
        "if torch.cuda.is_available():\n",
        " net_loaded.load_state_dict(torch.load(\"net.pth\"))\n",
        "else: \n",
        "  net_loaded.load_state_dict(torch.load(\"net.pth\", map_location=torch.device('cpu')))\n",
        "\n",
        "# create the loss function from its name\n",
        "if loss_name_loaded == \"MSE\":\n",
        "  loss_fn_loaded = nn.MSELoss()\n",
        "elif loss_name_loaded == \"MAE\":\n",
        "  loss_fn_loaded = nn.L1Loss()\n",
        "elif loss_name_loaded == \"Huber\":\n",
        "  loss_fn_loaded = nn.SmoothL1Loss() \n",
        "else:\n",
        "  # create the log-cosh loss function\n",
        "  def log_cosh_loss_loaded(y_pred, y_true):\n",
        "    return torch.mean(torch.log(torch.cosh(y_pred - y_true)))\n",
        "  loss_fn_loaded = log_cosh_loss_loaded\n",
        "\n",
        "# load the optimizer from the .pth file\n",
        "if torch.cuda.is_available():\n",
        "  optimizer_loaded_state_dict = torch.load(\"optimizer.pth\")\n",
        "else:\n",
        "  optimizer_loaded_state_dict = torch.load(\"optimizer.pth\", map_location=torch.device('cpu'))\n",
        "\n",
        "if optimizer_name_loaded == \"SGD\":\n",
        "  # Added getting the weight decay and momentum parameters from the state dict\n",
        "  weight_decay_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"weight_decay\"]\n",
        "  momentum_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"momentum\"]\n",
        "  optimizer_loaded = optim.SGD(net_loaded.parameters(), lr=lr_loaded, weight_decay=weight_decay_loaded, momentum=momentum_loaded)\n",
        "elif optimizer_name_loaded == \"Adam\":\n",
        "  # Added getting the weight decay and beta parameters from the state dict\n",
        "  weight_decay_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"weight_decay\"]\n",
        "  beta1_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"betas\"][0]\n",
        "  beta2_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"betas\"][1]\n",
        "  optimizer_loaded = optim.Adam(net_loaded.parameters(), lr=lr_loaded, weight_decay=weight_decay_loaded, betas=(beta1_loaded, beta2_loaded))\n",
        "elif optimizer_name_loaded == \"RMSprop\":\n",
        "  optimizer_loaded = optim.RMSprop(net_loaded.parameters(), lr=lr_loaded)\n",
        "else:\n",
        "  # Added loading the Adagrad optimizer\n",
        "  optimizer_loaded = optim.Adagrad(net_loaded.parameters(), lr=lr_loaded)\n",
        "optimizer_loaded.load_state_dict(optimizer_loaded_state_dict)\n",
        "\n",
        "# load the scheduler from the .pth file\n",
        "if torch.cuda.is_available():\n",
        "  scheduler_loaded_state_dict = torch.load(\"scheduler.pth\")\n",
        "else: \n",
        "  scheduler_loaded_state_dict = torch.load(\"scheduler.pth\", map_location=torch.device('cpu'))\n",
        "\n",
        "if scheduler_name_loaded == \"StepLR\":\n",
        "  # Added getting the step_size and gamma parameters from the state dict\n",
        "  step_size_loaded = scheduler_loaded_state_dict[\"step_size\"]\n",
        "  gamma_loaded = scheduler_loaded_state_dict[\"gamma\"]\n",
        "  scheduler_loaded = optim.lr_scheduler.StepLR(optimizer_loaded, step_size=step_size_loaded, gamma=gamma_loaded)\n",
        "elif scheduler_name_loaded == \"ExponentialLR\":\n",
        "  # Added getting the gamma parameter from the state dict\n",
        "  gamma_loaded = scheduler_loaded_state_dict[\"gamma\"]\n",
        "  scheduler_loaded = optim.lr_scheduler.ExponentialLR(optimizer_loaded, gamma=gamma_loaded)\n",
        "elif scheduler_name_loaded == \"CosineAnnealingLR\":\n",
        "  # Added getting the T_max parameter from the state dict\n",
        "  T_max_loaded = scheduler_loaded_state_dict[\"T_max\"]\n",
        "  scheduler_loaded = optim.lr_scheduler.CosineAnnealingLR(optimizer_loaded, T_max=T_max_loaded)\n",
        "elif scheduler_name_loaded == \"ReduceLROnPlateau\":\n",
        "  # Added getting the mode, factor, patience, threshold and min_lr parameters from the state dict\n",
        "  mode_loaded = scheduler_loaded_state_dict[\"mode\"]\n",
        "  factor_loaded = scheduler_loaded_state_dict[\"factor\"]\n",
        "  patience_loaded = scheduler_loaded_state_dict[\"patience\"]\n",
        "  threshold_loaded = scheduler_loaded_state_dict[\"threshold\"]\n",
        "  min_lr_loaded = scheduler_loaded_state_dict[\"min_lrs\"][0]\n",
        "  scheduler_loaded = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                    optimizer_loaded, mode=mode_loaded, factor=factor_loaded, patience=patience_loaded, threshold=threshold_loaded, min_lr=min_lr_loaded\n",
        "                )\n",
        "# elif scheduler_name_loaded == \"OneCycleLR\":\n",
        "#   max_lr_loaded = scheduler_loaded_state_dict[\"max_lr\"]\n",
        "#   epochs_loaded = scheduler_loaded_state_dict[\"epochs\"]\n",
        "#   steps_per_epoch_loaded = scheduler_loaded_state_dict[\"steps_per_epoch\"]\n",
        "#   pct_start_loaded = scheduler_loaded_state_dict[\"pct_start\"]\n",
        "#   max_lr_loaded = scheduler_loaded_state_dict[\"max_lr\"]\n",
        "#   scheduler_loaded = optim.lr_scheduler.OneCycleLR(\n",
        "#                     optimizer_loaded, max_lr=max_lr_loaded, epochs=epochs_loaded, steps_per_epoch=steps_per_epoch_loaded, pct_start=pct_start_loaded\n",
        "#                 )\n",
        "else:\n",
        "  scheduler_loaded = None\n",
        "\n",
        "if scheduler_loaded is not None:\n",
        "  # Added loading the state dict to the scheduler_loaded\n",
        "  scheduler_loaded.load_state_dict(scheduler_loaded_state_dict)\n",
        "\n",
        "# Loading the output of the training using pandas\n",
        "train_df_loaded = pd.read_csv(\"train_output.csv\")\n",
        "train_losses_loaded = train_df_loaded[\"train_loss\"].tolist()\n",
        "test_losses_loaded = train_df_loaded[\"test_loss\"].tolist()\n",
        "train_metrics_loaded = [\n",
        "    {\n",
        "        \"l1_norm\": train_df_loaded[\"train_l1_norm\"][i],\n",
        "        \"linf_norm\": train_df_loaded[\"train_linf_norm\"][i],\n",
        "    }\n",
        "    for i in range(len(train_df_loaded))\n",
        "]\n",
        "test_metrics_loaded = [\n",
        "    {\n",
        "        \"l1_norm\": train_df_loaded[\"test_l1_norm\"][i],\n",
        "        \"linf_norm\": train_df_loaded[\"test_linf_norm\"][i],\n",
        "    }\n",
        "    for i in range(len(train_df_loaded))\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ_fcj7zUZii"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "batch_size_loaded\n",
        "n_epochs_loaded\n",
        "loss_name_loaded\n",
        "optimizer_name_loaded\n",
        "scheduler_name_loaded\n",
        "n_units_loaded\n",
        "n_layers_loaded\n",
        "hidden_activation_name_loaded\n",
        "output_activation_name_loaded\n",
        "lr_loaded\n",
        "hidden_activation_loaded\n",
        "output_activation_loaded\n",
        "net_loaded\n",
        "net_loaded.__dict__ # print the subparameters of the network\n",
        "loss_fn_loaded\n",
        "optimizer_loaded\n",
        "optimizer_loaded.__dict__ # print the subparameters of the optimizer\n",
        "scheduler_loaded\n",
        "scheduler_loaded.__dict__ # print the subparameters of the scheduler\n",
        "train_losses_loaded\n",
        "test_losses_loaded\n",
        "train_metrics_loaded\n",
        "test_metrics_loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0B0SHa5SvExY"
      },
      "outputs": [],
      "source": [
        "train_losses_loaded[-1]\n",
        "test_losses_loaded[-1]\n",
        "test_metrics_loaded[-1]['l1_norm']\n",
        "test_metrics_loaded[-1]['linf_norm']\n",
        "print(f'Error is {test_metrics_loaded[-1][\"l1_norm\"] / (3.84e-4)} times bigger than in Dieselhorst et al.')\n",
        "print(f'Error is {test_metrics_loaded[-1][\"linf_norm\"] / (8.14e-3)} times bigger than in Dieselhorst et al.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj2XBdtmvExY"
      },
      "source": [
        "### Visualize loaded results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwLGR1aSUZik"
      },
      "source": [
        "Let us verify correct loading of the train and test metrics by visualizing them again but now through the loaded values. Likewise for the train and test losses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXiNgLsmUZil"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"last_expr_or_assign\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgq4WfSiUZil"
      },
      "outputs": [],
      "source": [
        "# Plotting the losses and metrics for the best network plt.figure(figsize=(12, \n",
        "#plt.subplot(2, 2, 1)\n",
        "#plt.plot(train_losses_loaded, label=\"Train Loss\")\n",
        "#plt.plot(test_losses_loaded, label=\"Test Loss\")\n",
        "#plt.xlabel(\"Epoch\")\n",
        "#plt.ylabel(\"Loss\")\n",
        "#plt.legend()\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot([m[\"l1_norm\"] for m in train_metrics_loaded], label=\"Train L1 Norm\")\n",
        "plt.plot([m[\"l1_norm\"] for m in test_metrics_loaded], label=\"Test L1 Norm\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"L1 Norm\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-3, 1e2)\n",
        "plt.legend()\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot([m[\"linf_norm\"] for m in train_metrics_loaded], label=\"Train Linf Norm\")\n",
        "plt.plot([m[\"linf_norm\"] for m in test_metrics_loaded], label=\"Test Linf Norm\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Linf Norm\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-3, 1e2)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Added plotting MSE of training data and MSE of test data in one plot \n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(train_losses_loaded,label=\"training data\")\n",
        "plt.plot(test_losses_loaded,label=\"test data\")\n",
        "#if scheduler is not None:\n",
        "#    plt.plot([scheduler.get_last_lr()[0] for _ in range(n_epochs)], label=\"Learning rate\") \n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-7, 1e0)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkgLqJ_UUZim"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EZQMUK8vExY"
      },
      "source": [
        "## Counting the number of parameters in the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWToFZmnvExZ"
      },
      "outputs": [],
      "source": [
        "net_loaded.eval()\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {count_parameters(net_loaded)} parameters.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxuzVSnlUZin"
      },
      "source": [
        "## Evaluating the network on arbirary input\n",
        "### Comparing `net` and `net_loaded`\n",
        "\n",
        "We compare `net` and `net_loaded` to confirm correct loading of the network. Note that `net` is only available if we have trained the model in this session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0PLAA0DUZin"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "print(list(net.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NZ8iVA7UZio"
      },
      "outputs": [],
      "source": [
        "print(list(net_loaded.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXLYbm8uUZio"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "# Set the network to evaluation mode\n",
        "net.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InGW0Xq6UZip"
      },
      "outputs": [],
      "source": [
        "rho_example, vx_example, vy_example, vz_example, epsilon_example = sample_primitive_variables(20)\n",
        "\n",
        "# Create arbitrary input\n",
        "inputs =  generate_input_data(rho_example, vx_example, vy_example, vz_example, epsilon_example)\n",
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVa1upmFUZip"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "# Pass the inputs to the network and get the outputs\n",
        "outputs = [net(input) for input in inputs]\n",
        "# Print the outputs\n",
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih9p2bosUZiq"
      },
      "outputs": [],
      "source": [
        "# Set the network to evaluation mode\n",
        "net_loaded.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-Xjfo7VUZir"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Pass the inputs to the network and get the outputs\n",
        "outputs = [net_loaded(input) for input in inputs]\n",
        "# Print the outputs\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjpIvdybUZis"
      },
      "source": [
        "## Porting the model to C++"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMlEd4RoUZis"
      },
      "outputs": [],
      "source": [
        "import torch.jit\n",
        "\n",
        "# Creating a dummy input tensor of shape (1, 5) to trace the model\n",
        "dummy_input = torch.randn(1, 5).to(device)\n",
        "dummy_input\n",
        "\n",
        "# Ensure that net_loaded is in evaluation mode.\n",
        "net_loaded.eval()\n",
        "\n",
        "# Tracing the model using the torch.jit.trace function\n",
        "traced_model = torch.jit.trace(net_loaded, dummy_input)\n",
        "\n",
        "# Saving the traced model to a file named \"net.pt\"\n",
        "traced_model.save(\"net.pt\")\n",
        "save_file(\"net.pt\")\n",
        "\n",
        "example_input_to_validate_correct_export_and_import = generate_input_data(*sample_primitive_variables(1))\n",
        "example_input_to_validate_correct_export_and_import\n",
        "net_loaded(example_input_to_validate_correct_export_and_import)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "bsc",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
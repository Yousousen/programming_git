{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8u2lhDS_c5aa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPvB1xoSUZhR"
      },
      "source": [
        "# Neural network to learn conservative-to-primitive conversion in relativistic hydrodynamics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SF-t12ApAc24"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eKD484DvExD"
      },
      "source": [
        "## How to use this notebook\n",
        "\n",
        "### Local installation\n",
        "\n",
        "1. Install required packages with `pip install -r requirements.txt` to your desired environment.\n",
        "2. If a script version of this notebook is desired, comment (not uncomment) the first line of `nbconvert` cell.\n",
        "\n",
        "### Colab installation\n",
        "\n",
        "1.  Comment (not uncomment) the first line of the drive mounting cell.\n",
        "2.  Comment (not uncomment) the first line of the `pip install` cell.\n",
        "\n",
        "<!-- - For colab we also want to set the runtime to GPU by clicking _Change runtime_ in the _Runtime_ menu, and -->\n",
        "<!-- - We want to wait for the google drive connection popup to appear and follow the instructions. -->\n",
        "\n",
        "### Training without optimization\n",
        "\n",
        "3. Set `OPTIMIZE = False` in section _Constants and flags to set_.\n",
        "4. Run the entire notebook.\n",
        "\n",
        "### Training with optimization\n",
        "\n",
        "3. Set `OPTIMIZE = True` in section _Constants and flags to set_.\n",
        "4. Run the entire notebook.\n",
        "\n",
        "### Loading an already trained model\n",
        "\n",
        "3. Run cells in section _Initialization_.\n",
        "4. Run cells with definitions in section _Generating the data_.\n",
        "5. Run cell with the definition of _Net_ in section _Defining the neural network_.\n",
        "6. Make sure the `net.pth`, `optimizer.pth`, `scheduler.pth`, `var_dict.json` and `train_output.csv` files are in the directory containing this notebook.\n",
        "7. Run the cells in section _Loading_ and continue from there.\n",
        "\n",
        "### Generating the C++ model\n",
        "\n",
        "8. Run section _Porting the model to C++_, this requires a model to be loaded.\n",
        "9. Set the path to the `net.pt` file in the C++ source file.\n",
        "10. `mkdir build && cd build`,\n",
        "11. `cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch/ ..`,\n",
        "10. Compile and run, e.g. `cmake --build . --config release && ./executable`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYPfIIglvExD"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsv90vHWvExE"
      },
      "source": [
        "\n",
        "Use this first cell to **convert this notebook** to a python script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqdgdNLHUZhV",
        "outputId": "3e000c00-e4c7-4ca8-8563-55fb5a082847",
        "tags": [
          "remove_cell"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "!jupyter nbconvert five_parameter_run_1.ipynb --TagRemovePreprocessor.enabled=True --TagRemovePreprocessor.remove_cell_tags='{\"remove_cell\"}' --to script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzcUr0LnUZhw"
      },
      "source": [
        "Next some cells for working on **google colab**,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Jm_7_2r1vExG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# check if the drive is mounted\n",
        "drive_mounted = os.path.exists(\"/content/drive\")\n",
        "# change this to your desired folder\n",
        "drive_folder = \"/content/drive/My Drive/bsc/con2prim_towards_GRMHD/five_parameter_run_2\"\n",
        "\n",
        "# define a function to save a file to the drive or the current directory\n",
        "def save_file(file_name):\n",
        "  if drive_mounted:\n",
        "    # save the file to the drive folder\n",
        "    file_path = os.path.join(drive_folder, file_name)\n",
        "    # copy the file from the current directory to the drive folder\n",
        "    shutil.copyfile(file_name, file_path)\n",
        "  else:\n",
        "    # do nothing as the file is already in the current directory\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecHw2_xlUZhx",
        "outputId": "78725784-237f-4d95-c0a3-adfcfb47bb03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#%%script echo skipping\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1rcStMLUZhy",
        "outputId": "f79a3677-3c77-4885-bcec-ee1017abd557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.1.1-py3-none-any.whl (365 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.12.2)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes>=0.9.1 (from optuna)\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.4.3)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.40.0)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n",
            "Installing collected packages: tensorboardX, Mako, colorlog, cmaes, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.11.1 cmaes-0.9.1 colorlog-6.7.0 optuna-3.1.1 tensorboardX-2.6\n"
          ]
        }
      ],
      "source": [
        "#%%script echo skipping\n",
        "\n",
        "!pip install optuna tensorboard tensorboardX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDsq9gG1vExH"
      },
      "source": [
        "Importing the **libraries** and setting the **device**,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tREdWQUVUZhz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import optuna\n",
        "import tensorboardX as tbx\n",
        "\n",
        "# Checking if GPU is available and setting the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38GvmerjUZhz"
      },
      "source": [
        "### Constants and flags to set\n",
        "Defining some constants and parameters for convenience.\n",
        "\n",
        "**NOTE**: Some **subparameters** still need to be adjusted in the `create_model` function itself as of (Tue May 16 07:42:45 AM CEST 2023)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ei6VZDYKUZh0"
      },
      "outputs": [],
      "source": [
        "\n",
        "N_TRIALS = 225 # Number of trials for hyperparameter optimization\n",
        "OPTIMIZE = True # Whether to optimize the hyperparameters or to use predetermined values from Dieseldorst et al..\n",
        "ZSCORE_NORMALIZATION = False # Whether to z-score normalize the input data.\n",
        "\n",
        "# I try out here the values as obtained in Optuna run 5, but I will increase the number of epochs.\n",
        "# N_LAYERS_NO_OPT = 3\n",
        "# N_UNITS_NO_OPT = [78, 193, 99]\n",
        "# HIDDEN_ACTIVATION_NAME_NO_OPT = \"ReLU\"\n",
        "# OUTPUT_ACTIVATION_NAME_NO_OPT = \"Linear\"\n",
        "# LOSS_NAME_NO_OPT = \"MSE\"\n",
        "# OPTIMIZER_NAME_NO_OPT = \"Adam\"\n",
        "# LR_NO_OPT = 0.00036516467819506355\n",
        "# BATCH_SIZE_NO_OPT = 170\n",
        "# N_EPOCHS_NO_OPT = 400\n",
        "# SCHEDULER_NAME_NO_OPT = \"ReduceLROnPlateau\"\n",
        "\n",
        "N_LAYERS_NO_OPT = 3\n",
        "N_UNITS_NO_OPT = [555, 458, 115]\n",
        "HIDDEN_ACTIVATION_NAME_NO_OPT = \"ReLU\"\n",
        "OUTPUT_ACTIVATION_NAME_NO_OPT = \"ReLU\"\n",
        "LOSS_NAME_NO_OPT = \"Huber\"\n",
        "OPTIMIZER_NAME_NO_OPT = \"RMSprop\"\n",
        "LR_NO_OPT = 0.000122770896701404\n",
        "BATCH_SIZE_NO_OPT = 49\n",
        "N_EPOCHS_NO_OPT = 400\n",
        "SCHEDULER_NAME_NO_OPT = \"ReduceLROnPlateau\"\n",
        "\n",
        "c = 1  # Speed of light (used in compute_conserved_variables and sample_primitive_variables functions)\n",
        "gamma = 5 / 3  # Adiabatic index (used in eos_analytic function)\n",
        "n_train_samples = 80000 # Number of training samples (used in generate_input_data and generate_labels functions)\n",
        "n_test_samples = 10000 # Number of test samples (used in generate_input_data and generate_labels functions)\n",
        "rho_interval = (0, 10.1) # Sampling interval for rest-mass density (used in sample_primitive_variables function)\n",
        "vx_interval = (0, .57 * c) # Sampling interval for velocity in x-direction (used in sample_primitive_variables function)\n",
        "vy_interval = (0, .57 * c) # Sampling interval for velocity in y-direction (used in sample_primitive_variables function)\n",
        "vz_interval = (0, .57 * c) # Sampling interval for velocity in z-direction (used in sample_primitive_variables function)\n",
        "epsilon_interval = (0, 2.02) # Sampling interval for specific internal energy (used in sample_primitive_variables function)\n",
        "\n",
        "np.random.seed(43) # Uncomment for pseudorandom data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlaP5UL2UZh1"
      },
      "source": [
        "## Generating the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "s_EvGFZcUZh1"
      },
      "outputs": [],
      "source": [
        "# Defining an analytic equation of state (EOS) for an ideal gas\n",
        "def eos_analytic(rho, epsilon):\n",
        "    \"\"\"Computes the pressure from rest-mass density and specific internal energy using an analytic EOS.\n",
        "\n",
        "    Args:\n",
        "        rho (torch.Tensor): The rest-mass density tensor of shape (n_samples,).\n",
        "        epsilon (torch.Tensor): The specific internal energy tensor of shape (n_samples,).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The pressure tensor of shape (n_samples,).\n",
        "    \"\"\"\n",
        "    # Adding some assertions to check that the input tensors are valid and have \n",
        "    # the expected shape and type \n",
        "    assert isinstance(rho, torch.Tensor), \"rho must be a torch.Tensor\"\n",
        "    assert isinstance(epsilon, torch.Tensor), \"epsilon must be a torch.Tensor\"\n",
        "    assert rho.shape == epsilon.shape, \"rho and epsilon must have the same shape\"\n",
        "    assert rho.ndim == 1, \"rho and epsilon must be one-dimensional tensors\"\n",
        "    assert rho.dtype == torch.float32, \"rho and epsilon must have dtype torch.float32\"\n",
        "\n",
        "    return (gamma - 1) * rho * epsilon\n",
        "\n",
        "\n",
        "\n",
        "# Defining a function that samples primitive variables from uniform distributions\n",
        "def sample_primitive_variables(n_samples):\n",
        "    \"\"\"Samples primitive variables from uniform distributions.\n",
        "\n",
        "    Args:\n",
        "        n_samples (int): The number of samples to generate.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple of (rho, vx, vy, vz, epsilon), where rho is rest-mass density,\n",
        "            vx is velocity in x-direction,\n",
        "            vy is velocity in y-direction,\n",
        "            vz is velocity in z-direction,\n",
        "            epsilon is specific internal energy,\n",
        "            each being a numpy array of shape (n_samples,).\n",
        "    \"\"\"\n",
        "    # Sampling from uniform distributions with intervals matching Dieseldorst \n",
        "    # et al.\n",
        "    rho = np.random.uniform(*rho_interval, size=n_samples)  # Rest-mass density\n",
        "    vx = np.random.uniform(*vx_interval, size=n_samples)  # Velocity in x-direction\n",
        "    vy = np.random.uniform(*vy_interval, size=n_samples)  # Velocity in y-direction\n",
        "    vz = np.random.uniform(*vz_interval, size=n_samples)  # Velocity in z-direction \n",
        "    epsilon = np.random.uniform(*epsilon_interval, size=n_samples)  # Specific internal energy\n",
        "\n",
        "    # Returning the primitive variables\n",
        "    return rho, vx, vy, vz, epsilon\n",
        "\n",
        "\n",
        "\n",
        "# Defining a function that computes conserved variables from primitive variables\n",
        "def compute_conserved_variables(rho, vx, vy, vz, epsilon):\n",
        "    \"\"\"Computes conserved variables from primitive variables.\n",
        "\n",
        "    Args:\n",
        "        rho (torch.Tensor): The rest-mass density tensor of shape (n_samples,).\n",
        "        vx (torch.Tensor): The velocity in x-direction tensor of shape (n_samples,)\n",
        "        vy (torch.Tensor): The velocity in y-direction tensor of shape (n_samples,)\n",
        "        vz (torch.Tensor): The velocity in z-direction tensor of shape (n_samples,)\n",
        "        epsilon (torch.Tensor): The specific internal energy tensor of shape (n_samples,).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple of (D, Sx, Sy, Sz, tau), where D is conserved density,\n",
        "            Sx is conserved momentum in x-direction,\n",
        "            Sy is conserved momentum in y-direction,\n",
        "            Sz is conserved momentum in z-direction,\n",
        "            tau is conserved energy density,\n",
        "            each being a torch tensor of shape (n_samples,).\n",
        "    \"\"\"\n",
        "\n",
        "  # Computing the pressure from the primitive variables using the EOS\n",
        "    p = eos_analytic(rho, epsilon)\n",
        "    # Computing the Lorentz factor from the velocity.\n",
        "    v2 = vx ** 2 + vy ** 2 + vz ** 2\n",
        "    W = 1 / torch.sqrt(1 - v2 / c ** 2)\n",
        "    # Specific enthalpy\n",
        "    h = 1 + epsilon + p / rho  \n",
        "\n",
        "    # Computing the conserved variables from the primitive variables\n",
        "    D = rho * W  # Conserved density\n",
        "    Sx = rho * h * W ** 2 * vx  # Conserved momentum in x-direction\n",
        "    Sy = rho * h * W ** 2 * vy  # Conserved momentum in y-direction\n",
        "    Sz = rho * h * W ** 2 * vz  # Conserved momentum in z-direction\n",
        "    tau = rho * h * W ** 2 - p - D  # Conserved energy density\n",
        "\n",
        "    # Returning the conserved variables\n",
        "    return D, Sx, Sy, Sz, tau\n",
        "\n",
        "# Defining a function that generates input data (conserved variables) from given samples of primitive variables\n",
        "def generate_input_data(rho, vx, vy, vz, epsilon):\n",
        "    # Converting the numpy arrays to torch tensors and moving them to the device\n",
        "    rho = torch.tensor(rho, dtype=torch.float32).to(device)\n",
        "    vx = torch.tensor(vx, dtype=torch.float32).to(device)\n",
        "    vy = torch.tensor(vy, dtype=torch.float32).to(device)\n",
        "    vz = torch.tensor(vz, dtype=torch.float32).to(device)\n",
        "    epsilon = torch.tensor(epsilon, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Computing the conserved variables using the compute_conserved_variables function\n",
        "    D, Sx, Sy, Sz, tau = compute_conserved_variables(rho, vx, vy, vz, epsilon) \n",
        "\n",
        "    # Stacking the conserved variables into a torch tensor\n",
        "    x = torch.stack([D, Sx, Sy, Sz, tau], axis=1)\n",
        "\n",
        "    # Returning the input data tensor\n",
        "    return x\n",
        "\n",
        "# Defining a function that generates output data (labels) from given samples of primitive variables\n",
        "def generate_labels(rho, epsilon):\n",
        "    # Converting the numpy arrays to torch tensors and moving them to the device\n",
        "    rho = torch.tensor(rho, dtype=torch.float32).to(device)\n",
        "    epsilon = torch.tensor(epsilon, dtype=torch.float32).to(device)\n",
        "   \n",
        "    # Computing the pressure from the primitive variables using the EOS\n",
        "    p = eos_analytic(rho, epsilon)\n",
        "\n",
        "    # Returning the output data tensor\n",
        "    return p\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dczop2rUZh3"
      },
      "source": [
        "Sampling the primitive variables using the sample_primitive_variables function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cKubR6C8UZh4"
      },
      "outputs": [],
      "source": [
        "rho_train, vx_train, vy_train, vz_train ,epsilon_train = sample_primitive_variables(n_train_samples)\n",
        "rho_test, vx_test ,vy_test ,vz_test ,epsilon_test = sample_primitive_variables(n_test_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zhbYH5HkvExJ",
        "outputId": "2aa73250-0ecd-49b5-a0ca-032c2276f2ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.28762072, 1.9590914 , 0.58522533, ..., 0.06985293, 0.82055397,\n",
              "       1.51444467])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "rho_train\n",
        "vx_train\n",
        "vy_train\n",
        "vz_train \n",
        "epsilon_train\n",
        "rho_test\n",
        "vx_test \n",
        "vy_test \n",
        "vz_test \n",
        "epsilon_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VMp6XJ6RUZh4"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"last_expr_or_assign\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "E5YFdqKjUZh5",
        "outputId": "cf31ba6c-cda6-4a4f-92a2-20d6d67afca6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x400 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plotting the histograms of rho, vx and epsilon\n",
        "plt.figure(figsize=(16, 4))\n",
        "plt.subplot(1, 5, 1)\n",
        "plt.hist(rho_train, bins=20)\n",
        "plt.xlabel(\"rho\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "#plt.yscale(\"log\")\n",
        "plt.subplot(1, 5, 2)\n",
        "plt.hist(vx_train, bins=20)\n",
        "plt.xlabel(\"vx\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "#plt.yscale(\"log\")\n",
        "plt.subplot(1, 5, 3)\n",
        "plt.hist(vy_train, bins=20)\n",
        "plt.xlabel(\"vy\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "#plt.yscale(\"log\")\n",
        "plt.subplot(1, 5, 4)\n",
        "plt.hist(vz_train, bins=20)\n",
        "plt.xlabel(\"vz\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "#plt.yscale(\"log\")\n",
        "plt.subplot(1, 5, 5)\n",
        "plt.hist(epsilon_train, bins=20)\n",
        "plt.xlabel(\"epsilon\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "#plt.yscale(\"log\")\n",
        "plt.suptitle(\"Primitive variables\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1fsekS7zvExL"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7ubtOXVgUZh6"
      },
      "outputs": [],
      "source": [
        "# Generating the input and output data for train and test sets.\n",
        "x_train = generate_input_data(rho_train, vx_train ,vy_train, vz_train, epsilon_train)\n",
        "y_train = generate_labels(rho_train, epsilon_train) \n",
        "x_test = generate_input_data(rho_test, vx_test, vy_test, vz_test, epsilon_test)\n",
        "y_test = generate_labels(rho_test, epsilon_test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lG_10Fx0vExM",
        "outputId": "b88885ba-13a3-4e56-b52a-0aa0d8079a91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.2253,  1.2439,  0.5428,  0.9811,  2.6203],\n",
              "        [ 8.6760, 26.3492,  1.3128, 22.2189, 32.8612],\n",
              "        [ 1.6644,  1.8987,  1.2941,  0.6595,  1.8787],\n",
              "        ...,\n",
              "        [ 0.8806,  0.3053,  0.6187,  1.1567,  1.7092],\n",
              "        [ 9.4032,  9.6825,  9.5324, 16.1767, 22.2331],\n",
              "        [ 4.6567,  2.9289,  0.3831,  2.7460,  8.2808]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.4350, 7.3753, 0.5281,  ..., 0.8335, 8.3240, 4.9954], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0324e+01, 7.6037e+00, 4.1150e+00, 7.3877e+00, 1.5725e+01],\n",
              "        [6.1274e+00, 8.8058e+00, 1.2547e+01, 1.0761e+01, 1.9520e+01],\n",
              "        [2.8945e+00, 7.2054e-01, 2.6138e+00, 1.5800e+00, 2.6378e+00],\n",
              "        ...,\n",
              "        [1.0343e+00, 9.6165e-03, 8.6076e-02, 6.1373e-02, 7.7322e-02],\n",
              "        [8.6855e+00, 1.3404e+00, 1.4288e+01, 9.4060e+00, 1.4448e+01],\n",
              "        [1.1708e+01, 1.1230e+01, 2.6180e+01, 2.4258e+00, 2.8773e+01]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([8.3647, 6.5048, 0.9900,  ..., 0.0480, 3.6481, 9.7164], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "x_train\n",
        "y_train\n",
        "x_test\n",
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CqQxpKTYvExM"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"last_expr_or_assign\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "T2bsgZyzvExM",
        "outputId": "ca96c02a-1e75-4766-c3e9-675ac739dfc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 950
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Note how we are only plotting train and not test here. \n",
        "# Plotting histograms of the input variables before z-score normalization\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.suptitle('Histograms of input variables before z-score normalization')\n",
        "for i in range(5):\n",
        "    plt.subplot(3, 2, i+1)\n",
        "    plt.hist(x_train[:, i].cpu(), bins=50) # Must be converted to cpu() for plotting.\n",
        "    plt.xlabel(f'Variable {i}')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QrN95284vExM",
        "outputId": "fe5ea762-a224-4c83-d7ca-77d58ca576fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary statistics of input variables before z-score normalization\n",
            "tensor([[2.0201e-04, 3.6966e+01, 6.3568e+00, 6.2493e+00, 3.8162e+00],\n",
            "        [1.7208e-05, 2.1647e+02, 6.8448e+00, 3.8932e+00, 8.7775e+00],\n",
            "        [2.1043e-05, 2.0937e+02, 6.8447e+00, 3.8528e+00, 8.7732e+00],\n",
            "        [3.4876e-05, 2.1949e+02, 6.8660e+00, 3.8964e+00, 8.7893e+00],\n",
            "        [6.3357e-05, 3.5165e+02, 1.2107e+01, 8.2963e+00, 1.3245e+01]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# print('Summary statistics of input variables before z-score normalization')\n",
        "# print(torch.stack([torch.min(x_train, dim=0).values,\n",
        "#                 torch.max(x_train, dim=0).values,\n",
        "#                 torch.mean(x_train, dim=0),\n",
        "#                 torch.median(x_train, dim=0).values,\n",
        "#                 torch.std(x_train, dim=0)], dim=1))\n",
        "\n",
        "# Computing summary statistics of the input variables before and after z-score normalization\n",
        "print('Summary statistics of input variables before z-score normalization')\n",
        "print(torch.stack([torch.min(x_train, dim=0).values, torch.max(x_train, dim=0).values, torch.nanmean(x_train, dim=0), torch.median(x_train, dim=0).values, torch.std(x_train, dim=0)], dim=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTEmkR1SUZh7"
      },
      "source": [
        "Perform z-score normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kqUmp1VVvExN"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yPOv6DxhUZh7"
      },
      "outputs": [],
      "source": [
        "if ZSCORE_NORMALIZATION:\n",
        "    \n",
        "    # # Computing the median of each input variable from the training set using torch.nanmedian function\n",
        "    # D_median = torch.nanmedian(x_train[:, 0])\n",
        "    # Sx_median = torch.nanmedian(x_train[:, 1])\n",
        "    # Sy_median = torch.nanmedian(x_train[:, 2])\n",
        "    # Sz_median = torch.nanmedian(x_train[:, 3])\n",
        "    # tau_median = torch.nanmedian(x_train[:, 4])\n",
        "\n",
        "    # # Computing the standard deviation of each input variable from the training set using torch.std function with a boolean mask to ignore nan values\n",
        "    # D_std = torch.std(x_train[~torch.isnan(x_train[:, 0]), 0])\n",
        "    # Sx_std = torch.std(x_train[~torch.isnan(x_train[:, 1]), 1])\n",
        "    # Sy_std = torch.std(x_train[~torch.isnan(x_train[:, 2]), 2])\n",
        "    # Sz_std = torch.std(x_train[~torch.isnan(x_train[:, 3]), 3])\n",
        "    # tau_std = torch.std(x_train[~torch.isnan(x_train[:, 4]), 4])\n",
        "\n",
        "\n",
        "    # # Applying z-score normalization to both train and test sets using the statistics from the training set\n",
        "    # x_train[:, 0] = torch.sub(x_train[:, 0], D_median).div(D_std)\n",
        "    # x_train[:, 1] = torch.sub(x_train[:, 1], Sx_median).div(Sx_std)\n",
        "    # x_train[:, 2] = torch.sub(x_train[:, 2], Sy_median).div(Sy_std)\n",
        "    # x_train[:, 3] = torch.sub(x_train[:, 3], Sz_median).div(Sz_std)\n",
        "    # x_train[:, 4] = torch.sub(x_train[:, 4], tau_median).div(tau_std)\n",
        "\n",
        "    # x_test[:, 0] = torch.sub(x_test[:, 0], D_median).div(D_std)\n",
        "    # x_test[:, 1] = torch.sub(x_test[:, 1], Sx_median).div(Sx_std)\n",
        "    # x_test[:, 2] = torch.sub(x_test[:, 2], Sy_median).div(Sy_std)\n",
        "    # x_test[:, 3] = torch.sub(x_test[:, 3], Sz_median).div(Sz_std)\n",
        "    # x_test[:, 4] = torch.sub(x_test[:, 4], tau_median).div(tau_std)\n",
        "\n",
        "    # Computing the mean and standard deviation of each column\n",
        "    mean = x_train.mean(dim=0)\n",
        "    std = x_train.std(dim=0)\n",
        "\n",
        "    # Applying z-score normalization\n",
        "    x_train = (x_train - mean) / std\n",
        "    # Use the same mean and std from the training data as we don't want test data leakage.\n",
        "    x_test = (x_test - mean) / std\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "boC-8KmXvExN",
        "outputId": "0b131c73-9c1b-4199-fa27-76e055a41a9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.1+cu118'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6J7rG-OevExN",
        "outputId": "1b19ac98-fc84-49a5-ec27-944198d73a24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.2253, 8.6760, 1.6644,  ..., 0.8806, 9.4032, 4.6567], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([80000, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.2253,  1.2439,  0.5428,  0.9811,  2.6203],\n",
              "        [ 8.6760, 26.3492,  1.3128, 22.2189, 32.8612],\n",
              "        [ 1.6644,  1.8987,  1.2941,  0.6595,  1.8787],\n",
              "        ...,\n",
              "        [ 0.8806,  0.3053,  0.6187,  1.1567,  1.7092],\n",
              "        [ 9.4032,  9.6825,  9.5324, 16.1767, 22.2331],\n",
              "        [ 4.6567,  2.9289,  0.3831,  2.7460,  8.2808]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0324e+01, 7.6037e+00, 4.1150e+00, 7.3877e+00, 1.5725e+01],\n",
              "        [6.1274e+00, 8.8058e+00, 1.2547e+01, 1.0761e+01, 1.9520e+01],\n",
              "        [2.8945e+00, 7.2054e-01, 2.6138e+00, 1.5800e+00, 2.6378e+00],\n",
              "        ...,\n",
              "        [1.0343e+00, 9.6165e-03, 8.6076e-02, 6.1373e-02, 7.7322e-02],\n",
              "        [8.6855e+00, 1.3404e+00, 1.4288e+01, 9.4060e+00, 1.4448e+01],\n",
              "        [1.1708e+01, 1.1230e+01, 2.6180e+01, 2.4258e+00, 2.8773e+01]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "x_train[:, 0]\n",
        "x_train.shape\n",
        "x_train\n",
        "x_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmdr204ZvExO"
      },
      "source": [
        "Plotting the histograms of the input data after normalization if z-score normalization was performed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "yE_qe_eLvExO"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"last_expr_or_assign\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bHw7bNaRvExO"
      },
      "outputs": [],
      "source": [
        "if ZSCORE_NORMALIZATION: \n",
        "    # Note how we are only plotting train and not test here.\n",
        "    # Plotting histograms of the input variables after z-score normalization\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.suptitle('Histograms of input variables after z-score normalization')\n",
        "    for i in range(5):\n",
        "        plt.subplot(3, 2, i+1)\n",
        "        plt.hist(x_train[:, i].cpu(), bins=50) # Must be convertedhere to cpu() for plotting.\n",
        "        plt.xlabel(f'Variable {i}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5_7RTKmuvExO"
      },
      "outputs": [],
      "source": [
        "if ZSCORE_NORMALIZATION:\n",
        "    # print('Summary statistics of input variables after z-score normalization')\n",
        "    # print(torch.stack([torch.min(x_train, dim=0).values,\n",
        "    #                 torch.max(x_train, dim=0).values,\n",
        "    #                 torch.mean(x_train, dim=0),\n",
        "    #                 torch.median(x_train, dim=0).values,\n",
        "    #                 torch.std(x_train, dim=0)], dim=1))\n",
        "    # Computing summary statistics of the input variables after z-score normalization\n",
        "    print('Summary statistics of input variables after z-score normalization')\n",
        "    print(torch.stack([torch.min(x_train, dim=0).values, torch.max(x_train, dim=0).values, torch.mean(x_train, dim=0), torch.median(x_train, dim=0).values, torch.std(x_train, dim=0)], dim=1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E96p_MsOUZh9",
        "outputId": "4b95bad0-8f3a-4364-eed1-008e0ce2a5e3"
      },
      "source": [
        "Checking if our output is always positive by plotting a histogram of y_train and y_test tensors "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vhyJwr4nvExO",
        "outputId": "1ca68631-b187-4717-edb9-a1cb0745d234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(y_train.cpu().numpy(), bins=20) # must be cpu here.\n",
        "plt.xlabel(\"y_train\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.yscale(\"log\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(y_test.cpu().numpy(), bins=20) # must be cpu here\n",
        "plt.xlabel(\"y_test\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.yscale(\"log\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "FEgjk--AUZh9"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2b9GecHUZh9"
      },
      "source": [
        "## Defining the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Iv8HA-ZXUZh-"
      },
      "outputs": [],
      "source": [
        "# Defining a class for the network\n",
        "class Net(nn.Module):\n",
        "    \"\"\"A class for creating a network with a\n",
        "    variable number of hidden layers and units.\n",
        "\n",
        "    Attributes:\n",
        "        n_layers (int): The number of hidden layers in the network.\n",
        "        n_units (list): A list of integers representing the number of units in each hidden layer.\n",
        "        hidden_activation (torch.nn.Module): The activation function for the hidden layers.\n",
        "        output_activation (torch.nn.Module): The activation function for the output layer.\n",
        "        layers (torch.nn.ModuleList): A list of linear layers in the network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_layers, n_units, hidden_activation, output_activation):\n",
        "        \"\"\"Initializes the network with the given hyperparameters.\n",
        "\n",
        "        Args:\n",
        "            n_layers (int): The number of hidden layers in the network.\n",
        "            n_units (list): A list of integers representing the number of units in each hidden layer.\n",
        "            hidden_activation (torch.nn.Module): The activation function for the hidden layers.\n",
        "            output_activation (torch.nn.Module): The activation function for the output layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.n_units = n_units\n",
        "        self.hidden_activation = hidden_activation\n",
        "        self.output_activation = output_activation\n",
        "\n",
        "        # Creating a list of linear layers with different numbers of units for each layer\n",
        "        self.layers = nn.ModuleList([nn.Linear(5, n_units[0])])\n",
        "        for i in range(1, n_layers):\n",
        "            self.layers.append(nn.Linear(n_units[i - 1], n_units[i]))\n",
        "        self.layers.append(nn.Linear(n_units[-1], 1))\n",
        "\n",
        "        # Adding some assertions to check that the input arguments are valid\n",
        "        assert isinstance(n_layers, int) and n_layers > 0, \"n_layers must be a positive integer\"\n",
        "        assert isinstance(n_units, list) and len(n_units) == n_layers, \"n_units must be a list of length n_layers\"\n",
        "        assert all(isinstance(n, int) and n > 0 for n in n_units), \"n_units must contain positive integers\"\n",
        "        assert isinstance(hidden_activation, nn.Module), \"hidden_activation must be a torch.nn.Module\"\n",
        "        assert isinstance(output_activation, nn.Module), \"output_activation must be a torch.nn.Module\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Performs a forward pass on the input tensor.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input tensor of shape (batch_size, 5).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The output tensor of shape (batch_size, 1).\n",
        "        \"\"\"\n",
        "        # Adding an assertion to check that the input tensor has the expected shape and type\n",
        "        assert isinstance(x, torch.Tensor), \"x must be a torch.Tensor\"\n",
        "        assert x.shape[1] == 5, \"x must have shape (batch_size, 5)\"\n",
        "\n",
        "        # Looping over the hidden layers and applying the linear transformation and the activation function\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = self.hidden_activation(layer(x))\n",
        "        # Applying the linear transformation and the activation function on the output layer\n",
        "        x = self.output_activation(self.layers[-1](x))\n",
        "\n",
        "        # Returning the output tensor\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GNvp55PUZh_"
      },
      "source": [
        "## Defining the model and search space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "9a1opluOUZh_"
      },
      "outputs": [],
      "source": [
        "# Defining a function to create a trial network and optimizer\n",
        "def create_model(trial, optimize):\n",
        "    \"\"\"Creates a trial network and optimizer based on the sampled hyperparameters.\n",
        "\n",
        "    Args:\n",
        "        trial (optuna.trial.Trial): The trial object that contains the hyperparameters.\n",
        "        optimize (boolean): Whether to optimize the hyperparameters or to use predefined values.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple of (net, loss_fn, optimizer, batch_size, n_epochs,\n",
        "            scheduler, loss_name, optimizer_name, scheduler_name,\n",
        "            n_units, n_layers, hidden_activation, output_activation),\n",
        "            where net is the trial network,\n",
        "            loss_fn is the loss function,\n",
        "            optimizer is the optimizer,\n",
        "            batch_size is the batch size,\n",
        "            n_epochs is the number of epochs,\n",
        "            scheduler is the learning rate scheduler,\n",
        "            loss_name is the name of the loss function,\n",
        "            optimizer_name is the name of the optimizer,\n",
        "            scheduler_name is the name of the scheduler,\n",
        "            n_units is a list of integers representing\n",
        "            the number of units in each hidden layer,\n",
        "            n_layers is an integer representing the number of hidden layers in the network,\n",
        "            hidden_activation is a torch.nn.Module representing the activation function for the hidden layers,\n",
        "            output_activation is a torch.nn.Module representing the activation function for the output layer,\n",
        "            lr is the (initial) learning rate.\n",
        "    \"\"\"\n",
        "    # If optimize is True, sample the hyperparameters from the search space\n",
        "    if optimize:\n",
        "        # Sampling the hyperparameters from the search space\n",
        "        n_layers = trial.suggest_int(\"n_layers\", 2, 10)\n",
        "        n_units = [trial.suggest_int(f\"n_units_{i}\", 32, 2048) for i in range(n_layers)] \n",
        "        hidden_activation_name = trial.suggest_categorical(\n",
        "            #\"hidden_activation\", [\"ReLU\", \"LeakyReLU\", \"ELU\", \"Tanh\", \"Sigmoid\"]\n",
        "            #\"hidden_activation\", [\"ReLU\", \"LeakyReLU\"]\n",
        "            \"hidden_activation\", [\"ReLU\", \"LeakyReLU\", \"ELU\"]\n",
        "        )\n",
        "        output_activation_name = trial.suggest_categorical(\n",
        "            #\"output_activation\", [\"Linear\", \"ReLU\", \"Softplus\"]\n",
        "            # Assuming pressure cannot be negative, linear output activation is not an option.\n",
        "            #\"output_activation\", [\"ReLU\", \"Softplus\", \"Linear\"]\n",
        "            \"output_activation\", [\"ReLU\", \"Linear\"]\n",
        "        ) \n",
        "        loss_name = trial.suggest_categorical(\n",
        "            #\"loss\", [\"MSE\", \"MAE\", \"Huber\", \"LogCosh\"] \n",
        "            \"loss\", [\"MSE\", \"MAE\", \"Huber\"] \n",
        "        )\n",
        "        optimizer_name = trial.suggest_categorical(\n",
        "            \"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\", \"Adagrad\"] \n",
        "        )\n",
        "        lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2) \n",
        "\n",
        "        batch_size_list = [32, 48, 64, 96, 128, 256, 512, 1048]\n",
        "        batch_size = trial.suggest_categorical(\"batch_size\", batch_size_list)\n",
        "        #batch_size = trial.suggest_int(\"batch_size\", 16, 1048)\n",
        "        n_epochs = trial.suggest_int(\"n_epochs\", 100, 300) \n",
        "        scheduler_name = trial.suggest_categorical(\n",
        "            \"scheduler\",\n",
        "            # [\"None\", \"CosineAnnealingLR\", \"ReduceLROnPlateau\", \"StepLR\", \"ExponentialLR\"],\n",
        "            [\"CosineAnnealingLR\", \"ReduceLROnPlateau\", \"StepLR\"],\n",
        "        )\n",
        "\n",
        "    # If optimize is False, use the predefined values\n",
        "    else:\n",
        "        # Setting the hyperparameters to the predefined values\n",
        "        n_layers = N_LAYERS_NO_OPT\n",
        "        n_units = N_UNITS_NO_OPT\n",
        "        hidden_activation_name = HIDDEN_ACTIVATION_NAME_NO_OPT\n",
        "        output_activation_name = OUTPUT_ACTIVATION_NAME_NO_OPT\n",
        "        loss_name = LOSS_NAME_NO_OPT\n",
        "        optimizer_name = OPTIMIZER_NAME_NO_OPT\n",
        "        lr = LR_NO_OPT\n",
        "        batch_size = BATCH_SIZE_NO_OPT\n",
        "        n_epochs = N_EPOCHS_NO_OPT\n",
        "        scheduler_name = SCHEDULER_NAME_NO_OPT\n",
        "\n",
        "\n",
        "    # Creating the activation functions from their names\n",
        "    if hidden_activation_name == \"ReLU\":\n",
        "        hidden_activation = nn.ReLU()\n",
        "    elif hidden_activation_name == \"LeakyReLU\":\n",
        "        hidden_activation = nn.LeakyReLU() \n",
        "    elif hidden_activation_name == \"ELU\":\n",
        "        hidden_activation = nn.ELU() \n",
        "    elif hidden_activation_name == \"Tanh\":\n",
        "        hidden_activation = nn.Tanh()\n",
        "    else:\n",
        "        hidden_activation = nn.Sigmoid()\n",
        "\n",
        "    if output_activation_name == \"ReLU\":\n",
        "        output_activation = nn.ReLU()\n",
        "    elif output_activation_name == \"Softplus\":\n",
        "        output_activation = nn.Softplus()\n",
        "    else:\n",
        "        output_activation = nn.Identity()\n",
        "\n",
        "    # Creating the loss function from its name\n",
        "    if loss_name == \"MSE\":\n",
        "        loss_fn = nn.MSELoss()\n",
        "    elif loss_name == \"MAE\":\n",
        "        loss_fn = nn.L1Loss()\n",
        "    elif loss_name == \"Huber\":\n",
        "        loss_fn = nn.SmoothL1Loss() \n",
        "    else:\n",
        "        # Creating the log-cosh loss function\n",
        "        def log_cosh_loss(y_pred, y_true):\n",
        "            return torch.mean(torch.log(torch.cosh(y_pred - y_true)))\n",
        "            \n",
        "        loss_fn = log_cosh_loss\n",
        "\n",
        "    # Creating the network with the sampled hyperparameters\n",
        "    net = Net(\n",
        "        n_layers, n_units, hidden_activation, output_activation\n",
        "    ).to(device)\n",
        "\n",
        "    if optimize:\n",
        "        # Creating the optimizer from its name\n",
        "        if optimizer_name == \"SGD\":\n",
        "            # Added sampling the weight decay and momentum for SGD\n",
        "            weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
        "            momentum = trial.suggest_uniform(\"momentum\", 0.0, 0.99)\n",
        "            optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
        "        elif optimizer_name == \"Adam\":\n",
        "            # Added sampling the weight decay and beta parameters for Adam\n",
        "            weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
        "            beta1 = trial.suggest_uniform(\"beta1\", 0.9, 0.999)\n",
        "            beta2 = trial.suggest_uniform(\"beta2\", 0.999, 0.9999)\n",
        "            optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay, betas=(beta1, beta2))\n",
        "        elif optimizer_name == \"RMSprop\":\n",
        "            optimizer = optim.RMSprop(net.parameters(), lr=lr)\n",
        "        else:\n",
        "            # Added creating the Adagrad optimizer\n",
        "            optimizer = optim.Adagrad(net.parameters(), lr=lr)\n",
        "\n",
        "        # Creating the learning rate scheduler from its name\n",
        "        if scheduler_name == \"StepLR\":\n",
        "            step_size = trial.suggest_int(\"step_size\", 5, 15)\n",
        "            gamma = trial.suggest_uniform(\"gamma\", 0.1, 0.5)\n",
        "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "        elif scheduler_name == \"ExponentialLR\":\n",
        "            gamma = trial.suggest_uniform(\"gamma\", 0.8, 0.99)\n",
        "            scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
        "        elif scheduler_name == \"CosineAnnealingLR\":\n",
        "            if n_epochs < 150:\n",
        "                t_max_fraction = trial.suggest_uniform('t_max_fraction', 0.1, 0.3)\n",
        "            elif n_epochs > 250:\n",
        "                t_max_fraction = trial.suggest_uniform('t_max_fraction', 0.05, 0.1)\n",
        "            else:\n",
        "                t_max_fraction = trial.suggest_uniform('t_max_fraction', 0.1, 0.2)\n",
        "\n",
        "            T_max = int(n_epochs * t_max_fraction)\n",
        "            eta_min = trial.suggest_loguniform(\"eta_min\", 1e-7, 1e-2)\n",
        "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)\n",
        "        elif scheduler_name == \"ReduceLROnPlateau\":\n",
        "            # Added sampling the factor, patience and threshold for ReduceLROnPlateau\n",
        "            factor = trial.suggest_uniform(\"factor\", 0.1, 0.5)\n",
        "            patience = trial.suggest_int(\"patience\", 5, 10)\n",
        "            threshold = trial.suggest_loguniform(\"threshold\", 1e-4, 1e-2)\n",
        "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                optimizer, mode=\"min\", factor=factor, patience=patience, threshold=threshold\n",
        "            )\n",
        "        # # Added using OneCycleLR scheduler as an option\n",
        "        # elif scheduler_name == \"OneCycleLR\":\n",
        "        #         # Added sampling the max_lr and pct_start for OneCycleLR\n",
        "        #         max_lr = trial.suggest_loguniform(\"max_lr\", lr, 10 * lr) \n",
        "        #         pct_start = trial.suggest_uniform(\"pct_start\", 0.1, 0.9)\n",
        "        #         scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "        #             optimizer,\n",
        "        #             max_lr=max_lr,\n",
        "        #             epochs=n_epochs,\n",
        "        #             steps_per_epoch=len(train_loader),\n",
        "        #             pct_start=pct_start,\n",
        "        #         )\n",
        "        else:\n",
        "            scheduler = None\n",
        "    else:\n",
        "        # Creating the optimizer from its name\n",
        "        if optimizer_name == \"SGD\":\n",
        "            optimizer = optim.SGD(net.parameters(), lr=lr)\n",
        "        elif optimizer_name == \"Adam\":\n",
        "            optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "        elif optimizer_name == \"RMSprop\":\n",
        "            optimizer = optim.RMSprop(net.parameters(), lr=lr)\n",
        "        else:\n",
        "            optimizer = optim.Adagrad(net.parameters(), lr=lr)\n",
        "\n",
        "        # Creating the learning rate scheduler from its name\n",
        "        if scheduler_name == \"StepLR\":\n",
        "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "        elif scheduler_name == \"ExponentialLR\":\n",
        "            scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "        elif scheduler_name == \"CosineAnnealingLR\":\n",
        "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer)\n",
        "        elif scheduler_name == \"ReduceLROnPlateau\":\n",
        "            # Creating the ReduceLROnPlateau scheduler with a threshold value of 0.01\n",
        "            #scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            #    optimizer, mode=\"min\", factor=0.1, patience=10, threshold=0.01\n",
        "            #)\n",
        "            # Use Dieseldorst et al. settings and add to that a minimum lr.\n",
        "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                        optimizer, mode=\"min\", factor=0.18979341786654758, patience=11, threshold=0.0017197466122611932 #, min_lr=1e-6\n",
        "                    )\n",
        "        else:\n",
        "            scheduler = None\n",
        "\n",
        "    # Returning all variables needed for saving and loading\n",
        "    return net, loss_fn, optimizer, batch_size, n_epochs, scheduler, loss_name, optimizer_name, scheduler_name, n_units, n_layers, hidden_activation, output_activation, lr\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-czA7VvUZiD"
      },
      "source": [
        " ## The training and evaluation loop\n",
        "\n",
        " We first define a couple of functions used in the training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "aD6FQNmxUZiD"
      },
      "outputs": [],
      "source": [
        "# Defining a function that computes loss and metrics for a given batch\n",
        "def compute_loss_and_metrics(y_pred, y_true, loss_fn):\n",
        "    \"\"\"Computes loss and metrics for a given batch.\n",
        "\n",
        "    Args:\n",
        "        y_pred (torch.Tensor): The predicted pressure tensor of shape (batch_size, 1).\n",
        "        y_true (torch.Tensor): The true pressure tensor of shape (batch_size,).\n",
        "        loss_fn (torch.nn.Module or function): The loss function to use.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple of (loss, l1_norm), where loss is a scalar tensor,\n",
        "            l1_norm is L1 norm for relative error of pressure,\n",
        "            each being a scalar tensor.\n",
        "            linf_norm is Linf norm for relative error of pressure.\n",
        "    \"\"\"\n",
        "    # Reshaping the target tensor to match the input tensor\n",
        "    y_true = y_true.view(-1, 1)\n",
        "\n",
        "    # Computing the loss using the loss function\n",
        "    loss = loss_fn(y_pred, y_true)\n",
        "\n",
        "    # Computing the relative error of pressure\n",
        "    rel_error = torch.abs((y_pred - y_true) / y_true)\n",
        "\n",
        "    # Computing the L1 norm for the relative error of pressure\n",
        "    l1_norm = torch.mean(rel_error) \n",
        "    # Computing the Linf norm for the relative error of pressure\n",
        "    linf_norm = torch.max(rel_error) \n",
        "\n",
        "    # Returning the loss and metrics\n",
        "    return loss, l1_norm, linf_norm\n",
        "\n",
        "\n",
        "# Defining a function that updates the learning rate scheduler with validation loss if applicable\n",
        "def update_scheduler(scheduler, test_loss):\n",
        "    \"\"\"Updates the learning rate scheduler with validation loss if applicable.\n",
        "\n",
        "    Args:\n",
        "        scheduler (torch.optim.lr_scheduler._LRScheduler or None): The learning rate scheduler to use.\n",
        "        test_loss (float): The validation loss to use.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Checking if scheduler is not None\n",
        "    if scheduler is not None:\n",
        "        # Checking if scheduler is ReduceLROnPlateau\n",
        "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
        "            # Updating the scheduler with test_loss\n",
        "            scheduler.step(test_loss)\n",
        "        else:\n",
        "            # Updating the scheduler without test_loss\n",
        "            scheduler.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1nE662UUZiE"
      },
      "source": [
        "Now for the actual training and evaluation loop,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "YAOjgKW3UZiF"
      },
      "outputs": [],
      "source": [
        "# Defining a function to train and evaluate a network\n",
        "def train_and_eval(net, loss_fn, optimizer, batch_size, n_epochs, scheduler, trial=None):\n",
        "    \"\"\"Trains and evaluates a network.\n",
        "\n",
        "    Args:\n",
        "        net (torch.nn.Module): The network to train and evaluate.\n",
        "        loss_fn (torch.nn.Module or function): The loss function.\n",
        "        optimizer (torch.optim.Optimizer): The optimizer.\n",
        "        batch_size (int): The batch size.\n",
        "        n_epochs (int): The number of epochs.\n",
        "        scheduler (torch.optim.lr_scheduler._LRScheduler or None): The learning rate scheduler.\n",
        "    Returns:\n",
        "        tuple: A tuple of (train_losses, test_losses, train_metrics, test_metrics), where\n",
        "            train_losses is a list of training losses for each epoch,\n",
        "            test_losses is a list of validation losses for each epoch,\n",
        "            train_metrics is a list of dictionaries containing training metrics for each epoch,\n",
        "            test_metrics is a list of dictionaries containing validation metrics for each epoch.\n",
        "    \"\"\"\n",
        "    # Creating data loaders for train and test sets\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.TensorDataset(x_test, y_test), batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Initializing lists to store the losses and metrics for each epoch\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    train_metrics = []\n",
        "    test_metrics = []\n",
        "\n",
        "    # Creating a SummaryWriter object to log data for tensorboard\n",
        "    writer = tbx.SummaryWriter()\n",
        "\n",
        "    # Looping over the epochs\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # Setting the network to training mode\n",
        "        net.train()\n",
        "\n",
        "        # Initializing variables to store the total loss and metrics for the train set\n",
        "        train_loss = 0.0\n",
        "        train_l1_norm = 0.0\n",
        "        train_linf_norm = 0.0\n",
        "\n",
        "        # Looping over the batches in the train set\n",
        "        for x_batch, y_batch in train_loader:\n",
        "\n",
        "            # Moving the batch tensors to the device\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            # Zeroing the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Performing a forward pass and computing the loss and metrics\n",
        "            y_pred = net(x_batch)\n",
        "            loss, l1_norm, linf_norm = compute_loss_and_metrics(\n",
        "                y_pred, y_batch, loss_fn\n",
        "            )\n",
        "\n",
        "\n",
        "            # Performing a backward pass and updating the weights\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Updating the total loss and metrics for the train set\n",
        "            train_loss += loss.item() * x_batch.size(0)\n",
        "            train_l1_norm += l1_norm.item() * x_batch.size(0)\n",
        "            train_linf_norm += linf_norm.item() * x_batch.size(0)\n",
        "\n",
        "        # Computing the average loss and metrics for the train set\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_l1_norm /= len(train_loader.dataset)\n",
        "        train_linf_norm /= len(train_loader.dataset)\n",
        "\n",
        "        # Appending the average loss and metrics for the train set to the lists\n",
        "        train_losses.append(train_loss)\n",
        "        train_metrics.append(\n",
        "            {\n",
        "                \"l1_norm\": train_l1_norm,\n",
        "                \"linf_norm\": train_linf_norm,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Logging the average loss and metrics for the train set to tensorboard\n",
        "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
        "        writer.add_scalar(\"L1 norm/train\", train_l1_norm, epoch)\n",
        "        writer.add_scalar(\"Linf norm/train\", train_linf_norm, epoch)\n",
        "\n",
        "        # Setting the network to evaluation mode\n",
        "        net.eval()\n",
        "\n",
        "        # Initializing variables to store the total loss and metrics for the test set\n",
        "        test_loss = 0.0\n",
        "        test_l1_norm = 0.0\n",
        "        test_linf_norm = 0.0\n",
        "\n",
        "        # Looping over the batches in the test set\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in test_loader:\n",
        "\n",
        "                # Moving the batch tensors to the device\n",
        "                x_batch = x_batch.to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "\n",
        "                # Performing a forward pass and computing the loss and metrics\n",
        "                y_pred = net(x_batch)\n",
        "                loss, l1_norm, linf_norm = compute_loss_and_metrics(\n",
        "                    y_pred, y_batch, loss_fn\n",
        "                )\n",
        "\n",
        "\n",
        "                # Updating the total loss and metrics for the test set\n",
        "                test_loss += loss.item() * x_batch.size(0)\n",
        "                test_l1_norm += l1_norm.item() * x_batch.size(0)\n",
        "                test_linf_norm += linf_norm.item() * x_batch.size(0)\n",
        "\n",
        "        # Computing the average loss and metrics for the test set\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        test_l1_norm /= len(test_loader.dataset)\n",
        "        test_linf_norm /= len(test_loader.dataset)\n",
        "\n",
        "        # Appending the average loss and metrics for the test set to the lists\n",
        "        test_losses.append(test_loss)\n",
        "        test_metrics.append(\n",
        "            {\n",
        "                \"l1_norm\": test_l1_norm,\n",
        "                \"linf_norm\": test_linf_norm,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Logging the average loss and metrics for the test set to tensorboard\n",
        "        writer.add_scalar(\"Loss/test\", test_loss, epoch)\n",
        "        writer.add_scalar(\"L1 norm/test\", test_l1_norm, epoch)\n",
        "        writer.add_scalar(\"Linf norm/test\", test_linf_norm, epoch)\n",
        "\n",
        "        # Printing the average loss and metrics for both sets for this epoch\n",
        "        print(\n",
        "            f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, \"\n",
        "            f\"Train L1 Norm: {train_l1_norm:.4f}, Test L1 Norm: {test_l1_norm:.4f}, \"\n",
        "            f\"Train Linf Norm: {train_linf_norm:.4f}, Test Linf Norm: {test_linf_norm:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Updating the learning rate scheduler with validation loss if applicable\n",
        "        update_scheduler(scheduler, test_loss)\n",
        "\n",
        "        # Reporting the intermediate metric value to Optuna if trial is not None\n",
        "        if trial is not None:\n",
        "            trial.report(test_metrics[-1][\"l1_norm\"], epoch)\n",
        "\n",
        "            # Checking if the trial should be pruned based on the intermediate value if trial is not None\n",
        "            if trial.should_prune():\n",
        "                raise optuna.TrialPruned()\n",
        "\n",
        "    # Closing the SummaryWriter object\n",
        "    writer.close()\n",
        "\n",
        "    # Returning the losses and metrics lists\n",
        "    return train_losses, test_losses, train_metrics, test_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg9jz0SvUZiQ"
      },
      "source": [
        "## The objective function and hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "fmRncQPuUZiR"
      },
      "outputs": [],
      "source": [
        "# Defining an objective function for Optuna to minimize\n",
        "def objective(trial):\n",
        "    \"\"\"Defines an objective function for Optuna to minimize.\n",
        "\n",
        "    Args:\n",
        "        trial (optuna.trial.Trial): The trial object that contains the hyperparameters.\n",
        "\n",
        "    Returns:\n",
        "        float: The validation L1 norm to minimize.\n",
        "    \"\"\"\n",
        "    # Creating a trial network and optimizer using the create_model function\n",
        "    net, \\\n",
        "    loss_fn, \\\n",
        "    optimizer, \\\n",
        "    batch_size, \\\n",
        "    n_epochs, \\\n",
        "    scheduler, \\\n",
        "    loss_name, \\\n",
        "    optimizer_name, \\\n",
        "    scheduler_name, \\\n",
        "    n_units, \\\n",
        "    n_layers, \\\n",
        "    hidden_activation, \\\n",
        "    output_activation, \\\n",
        "    lr = create_model(trial, optimize=True)\n",
        "\n",
        "    # Training and evaluating the network using the train_and_eval function\n",
        "    _, _, _, test_metrics = train_and_eval(\n",
        "        net, loss_fn, optimizer, batch_size, n_epochs, scheduler, trial\n",
        "    )\n",
        "\n",
        "    # Returning the last validation L1 norm as the objective value to minimize\n",
        "    return test_metrics[-1][\"l1_norm\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyES4NAyUZiS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b9f639b-3bcb-40ee-bfa9-416c9b99a235"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 15:31:55,446]\u001b[0m A new study created in memory with name: no-name-904d6e96-10a0-4e04-b259-2c21aeeba30b\u001b[0m\n",
            "<ipython-input-28-82e2252adf92>:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "<ipython-input-28-82e2252adf92>:121: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
            "<ipython-input-28-82e2252adf92>:122: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  momentum = trial.suggest_uniform(\"momentum\", 0.0, 0.99)\n",
            "<ipython-input-28-82e2252adf92>:157: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  factor = trial.suggest_uniform(\"factor\", 0.1, 0.5)\n",
            "<ipython-input-28-82e2252adf92>:159: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  threshold = trial.suggest_loguniform(\"threshold\", 1e-4, 1e-2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 1.8097, Test Loss: 1.0394, Train L1 Norm: 6.1162, Test L1 Norm: 2.5225, Train Linf Norm: 328.7189, Test Linf Norm: 88.5373\n",
            "Epoch 2: Train Loss: 0.9398, Test Loss: 0.8156, Train L1 Norm: 7.7187, Test L1 Norm: 2.6204, Train Linf Norm: 423.3271, Test Linf Norm: 94.4940\n",
            "Epoch 3: Train Loss: 0.4810, Test Loss: 0.1944, Train L1 Norm: 7.3976, Test L1 Norm: 1.9090, Train Linf Norm: 412.5047, Test Linf Norm: 77.2622\n",
            "Epoch 4: Train Loss: 0.1270, Test Loss: 0.0804, Train L1 Norm: 5.1952, Test L1 Norm: 1.4262, Train Linf Norm: 297.5910, Test Linf Norm: 60.7651\n",
            "Epoch 5: Train Loss: 0.0562, Test Loss: 0.0377, Train L1 Norm: 4.3524, Test L1 Norm: 1.2285, Train Linf Norm: 248.6472, Test Linf Norm: 53.7561\n",
            "Epoch 6: Train Loss: 0.0301, Test Loss: 0.0231, Train L1 Norm: 3.7931, Test L1 Norm: 1.1113, Train Linf Norm: 220.9514, Test Linf Norm: 49.5338\n",
            "Epoch 7: Train Loss: 0.0198, Test Loss: 0.0160, Train L1 Norm: 3.5667, Test L1 Norm: 1.0368, Train Linf Norm: 208.7740, Test Linf Norm: 46.7077\n",
            "Epoch 8: Train Loss: 0.0142, Test Loss: 0.0117, Train L1 Norm: 3.3532, Test L1 Norm: 0.9795, Train Linf Norm: 197.0677, Test Linf Norm: 44.4194\n",
            "Epoch 9: Train Loss: 0.0110, Test Loss: 0.0098, Train L1 Norm: 3.2333, Test L1 Norm: 0.9403, Train Linf Norm: 189.3541, Test Linf Norm: 42.7761\n",
            "Epoch 10: Train Loss: 0.0091, Test Loss: 0.0082, Train L1 Norm: 3.1174, Test L1 Norm: 0.9010, Train Linf Norm: 182.0510, Test Linf Norm: 41.1748\n",
            "Epoch 11: Train Loss: 0.0078, Test Loss: 0.0068, Train L1 Norm: 2.9977, Test L1 Norm: 0.8712, Train Linf Norm: 174.3678, Test Linf Norm: 39.9507\n",
            "Epoch 12: Train Loss: 0.0065, Test Loss: 0.0058, Train L1 Norm: 2.9225, Test L1 Norm: 0.8457, Train Linf Norm: 171.5741, Test Linf Norm: 38.8881\n",
            "Epoch 13: Train Loss: 0.0059, Test Loss: 0.0050, Train L1 Norm: 2.8246, Test L1 Norm: 0.8208, Train Linf Norm: 166.4256, Test Linf Norm: 37.8411\n",
            "Epoch 14: Train Loss: 0.0054, Test Loss: 0.0045, Train L1 Norm: 2.7570, Test L1 Norm: 0.8000, Train Linf Norm: 161.8895, Test Linf Norm: 36.9608\n",
            "Epoch 15: Train Loss: 0.0047, Test Loss: 0.0057, Train L1 Norm: 2.6936, Test L1 Norm: 0.7819, Train Linf Norm: 158.5937, Test Linf Norm: 36.0791\n",
            "Epoch 16: Train Loss: 0.0045, Test Loss: 0.0038, Train L1 Norm: 2.6365, Test L1 Norm: 0.7625, Train Linf Norm: 155.1729, Test Linf Norm: 35.3912\n",
            "Epoch 17: Train Loss: 0.0040, Test Loss: 0.0035, Train L1 Norm: 2.5740, Test L1 Norm: 0.7463, Train Linf Norm: 152.1993, Test Linf Norm: 34.7026\n",
            "Epoch 18: Train Loss: 0.0038, Test Loss: 0.0032, Train L1 Norm: 2.5235, Test L1 Norm: 0.7293, Train Linf Norm: 149.2490, Test Linf Norm: 33.9434\n",
            "Epoch 19: Train Loss: 0.0034, Test Loss: 0.0031, Train L1 Norm: 2.4762, Test L1 Norm: 0.7153, Train Linf Norm: 147.1712, Test Linf Norm: 33.3484\n",
            "Epoch 20: Train Loss: 0.0033, Test Loss: 0.0032, Train L1 Norm: 2.4216, Test L1 Norm: 0.7008, Train Linf Norm: 143.2225, Test Linf Norm: 32.7162\n",
            "Epoch 21: Train Loss: 0.0031, Test Loss: 0.0025, Train L1 Norm: 2.3848, Test L1 Norm: 0.6871, Train Linf Norm: 141.4687, Test Linf Norm: 32.1384\n",
            "Epoch 22: Train Loss: 0.0030, Test Loss: 0.0024, Train L1 Norm: 2.3414, Test L1 Norm: 0.6744, Train Linf Norm: 138.4609, Test Linf Norm: 31.5756\n",
            "Epoch 23: Train Loss: 0.0027, Test Loss: 0.0033, Train L1 Norm: 2.3011, Test L1 Norm: 0.6652, Train Linf Norm: 136.4418, Test Linf Norm: 31.0801\n",
            "Epoch 24: Train Loss: 0.0027, Test Loss: 0.0022, Train L1 Norm: 2.2691, Test L1 Norm: 0.6506, Train Linf Norm: 134.7181, Test Linf Norm: 30.5380\n",
            "Epoch 25: Train Loss: 0.0026, Test Loss: 0.0020, Train L1 Norm: 2.2301, Test L1 Norm: 0.6394, Train Linf Norm: 132.0151, Test Linf Norm: 30.0481\n",
            "Epoch 26: Train Loss: 0.0024, Test Loss: 0.0019, Train L1 Norm: 2.1862, Test L1 Norm: 0.6292, Train Linf Norm: 127.5227, Test Linf Norm: 29.5915\n",
            "Epoch 27: Train Loss: 0.0023, Test Loss: 0.0023, Train L1 Norm: 2.1616, Test L1 Norm: 0.6209, Train Linf Norm: 128.3289, Test Linf Norm: 29.1752\n",
            "Epoch 28: Train Loss: 0.0024, Test Loss: 0.0018, Train L1 Norm: 2.1163, Test L1 Norm: 0.6095, Train Linf Norm: 124.1832, Test Linf Norm: 28.7184\n",
            "Epoch 29: Train Loss: 0.0022, Test Loss: 0.0018, Train L1 Norm: 2.0928, Test L1 Norm: 0.6002, Train Linf Norm: 123.3249, Test Linf Norm: 28.2981\n",
            "Epoch 30: Train Loss: 0.0020, Test Loss: 0.0016, Train L1 Norm: 2.0649, Test L1 Norm: 0.5909, Train Linf Norm: 122.2661, Test Linf Norm: 27.8952\n",
            "Epoch 31: Train Loss: 0.0021, Test Loss: 0.0016, Train L1 Norm: 2.0355, Test L1 Norm: 0.5825, Train Linf Norm: 121.1779, Test Linf Norm: 27.5151\n",
            "Epoch 32: Train Loss: 0.0019, Test Loss: 0.0015, Train L1 Norm: 2.0066, Test L1 Norm: 0.5743, Train Linf Norm: 119.1067, Test Linf Norm: 27.1526\n",
            "Epoch 33: Train Loss: 0.0018, Test Loss: 0.0197, Train L1 Norm: 1.9800, Test L1 Norm: 0.5838, Train Linf Norm: 116.7205, Test Linf Norm: 26.6236\n",
            "Epoch 34: Train Loss: 0.0018, Test Loss: 0.0019, Train L1 Norm: 1.9541, Test L1 Norm: 0.5584, Train Linf Norm: 116.6153, Test Linf Norm: 26.3984\n",
            "Epoch 35: Train Loss: 0.0017, Test Loss: 0.0014, Train L1 Norm: 1.9271, Test L1 Norm: 0.5506, Train Linf Norm: 114.9473, Test Linf Norm: 26.0861\n",
            "Epoch 36: Train Loss: 0.0017, Test Loss: 0.0015, Train L1 Norm: 1.8995, Test L1 Norm: 0.5433, Train Linf Norm: 113.2140, Test Linf Norm: 25.7297\n",
            "Epoch 37: Train Loss: 0.0018, Test Loss: 0.0055, Train L1 Norm: 1.8820, Test L1 Norm: 0.5452, Train Linf Norm: 112.1847, Test Linf Norm: 25.5062\n",
            "Epoch 38: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 1.8526, Test L1 Norm: 0.5308, Train Linf Norm: 109.0780, Test Linf Norm: 25.1461\n",
            "Epoch 39: Train Loss: 0.0015, Test Loss: 0.0017, Train L1 Norm: 1.8276, Test L1 Norm: 0.5238, Train Linf Norm: 109.0848, Test Linf Norm: 24.8274\n",
            "Epoch 40: Train Loss: 0.0016, Test Loss: 0.0013, Train L1 Norm: 1.8096, Test L1 Norm: 0.5181, Train Linf Norm: 107.3866, Test Linf Norm: 24.5791\n",
            "Epoch 41: Train Loss: 0.0015, Test Loss: 0.0012, Train L1 Norm: 1.7914, Test L1 Norm: 0.5108, Train Linf Norm: 106.3380, Test Linf Norm: 24.2805\n",
            "Epoch 42: Train Loss: 0.0015, Test Loss: 0.0011, Train L1 Norm: 1.7715, Test L1 Norm: 0.5048, Train Linf Norm: 105.5379, Test Linf Norm: 24.0072\n",
            "Epoch 43: Train Loss: 0.0015, Test Loss: 0.0011, Train L1 Norm: 1.7519, Test L1 Norm: 0.4991, Train Linf Norm: 104.0744, Test Linf Norm: 23.7520\n",
            "Epoch 44: Train Loss: 0.0015, Test Loss: 0.0011, Train L1 Norm: 1.7331, Test L1 Norm: 0.4935, Train Linf Norm: 103.3822, Test Linf Norm: 23.4825\n",
            "Epoch 45: Train Loss: 0.0015, Test Loss: 0.0097, Train L1 Norm: 1.7137, Test L1 Norm: 0.5050, Train Linf Norm: 102.0472, Test Linf Norm: 23.3556\n",
            "Epoch 46: Train Loss: 0.0015, Test Loss: 0.0010, Train L1 Norm: 1.6953, Test L1 Norm: 0.4823, Train Linf Norm: 101.2876, Test Linf Norm: 22.9812\n",
            "Epoch 47: Train Loss: 0.0013, Test Loss: 0.0010, Train L1 Norm: 1.6751, Test L1 Norm: 0.4775, Train Linf Norm: 100.0596, Test Linf Norm: 22.7482\n",
            "Epoch 48: Train Loss: 0.0013, Test Loss: 0.0013, Train L1 Norm: 1.6530, Test L1 Norm: 0.4746, Train Linf Norm: 98.9568, Test Linf Norm: 22.5292\n",
            "Epoch 49: Train Loss: 0.0014, Test Loss: 0.0009, Train L1 Norm: 1.6401, Test L1 Norm: 0.4671, Train Linf Norm: 97.8102, Test Linf Norm: 22.2762\n",
            "Epoch 50: Train Loss: 0.0013, Test Loss: 0.0009, Train L1 Norm: 1.6254, Test L1 Norm: 0.4621, Train Linf Norm: 97.0437, Test Linf Norm: 22.0445\n",
            "Epoch 51: Train Loss: 0.0012, Test Loss: 0.0009, Train L1 Norm: 1.6068, Test L1 Norm: 0.4573, Train Linf Norm: 95.9645, Test Linf Norm: 21.8300\n",
            "Epoch 52: Train Loss: 0.0013, Test Loss: 0.0008, Train L1 Norm: 1.5917, Test L1 Norm: 0.4526, Train Linf Norm: 94.8781, Test Linf Norm: 21.6134\n",
            "Epoch 53: Train Loss: 0.0012, Test Loss: 0.0008, Train L1 Norm: 1.5736, Test L1 Norm: 0.4485, Train Linf Norm: 93.9149, Test Linf Norm: 21.4295\n",
            "Epoch 54: Train Loss: 0.0012, Test Loss: 0.0009, Train L1 Norm: 1.5594, Test L1 Norm: 0.4441, Train Linf Norm: 93.1561, Test Linf Norm: 21.2167\n",
            "Epoch 55: Train Loss: 0.0011, Test Loss: 0.0010, Train L1 Norm: 1.5486, Test L1 Norm: 0.4398, Train Linf Norm: 92.2795, Test Linf Norm: 20.9930\n",
            "Epoch 56: Train Loss: 0.0010, Test Loss: 0.0008, Train L1 Norm: 1.5340, Test L1 Norm: 0.4355, Train Linf Norm: 91.5047, Test Linf Norm: 20.8287\n",
            "Epoch 57: Train Loss: 0.0011, Test Loss: 0.0008, Train L1 Norm: 1.5172, Test L1 Norm: 0.4319, Train Linf Norm: 90.6202, Test Linf Norm: 20.6626\n",
            "Epoch 58: Train Loss: 0.0012, Test Loss: 0.0009, Train L1 Norm: 1.5072, Test L1 Norm: 0.4281, Train Linf Norm: 89.9909, Test Linf Norm: 20.4797\n",
            "Epoch 59: Train Loss: 0.0011, Test Loss: 0.0010, Train L1 Norm: 1.4926, Test L1 Norm: 0.4241, Train Linf Norm: 89.2187, Test Linf Norm: 20.2704\n",
            "Epoch 60: Train Loss: 0.0011, Test Loss: 0.0008, Train L1 Norm: 1.4818, Test L1 Norm: 0.4202, Train Linf Norm: 88.4820, Test Linf Norm: 20.1104\n",
            "Epoch 61: Train Loss: 0.0011, Test Loss: 0.0007, Train L1 Norm: 1.4669, Test L1 Norm: 0.4169, Train Linf Norm: 87.7856, Test Linf Norm: 19.9501\n",
            "Epoch 62: Train Loss: 0.0010, Test Loss: 0.0010, Train L1 Norm: 1.4537, Test L1 Norm: 0.4145, Train Linf Norm: 86.7800, Test Linf Norm: 19.7879\n",
            "Epoch 63: Train Loss: 0.0010, Test Loss: 0.0011, Train L1 Norm: 1.4423, Test L1 Norm: 0.4094, Train Linf Norm: 86.0057, Test Linf Norm: 19.5526\n",
            "Epoch 64: Train Loss: 0.0010, Test Loss: 0.0007, Train L1 Norm: 1.4313, Test L1 Norm: 0.4056, Train Linf Norm: 85.3921, Test Linf Norm: 19.4333\n",
            "Epoch 65: Train Loss: 0.0010, Test Loss: 0.0007, Train L1 Norm: 1.4173, Test L1 Norm: 0.4019, Train Linf Norm: 84.9894, Test Linf Norm: 19.2685\n",
            "Epoch 66: Train Loss: 0.0009, Test Loss: 0.0006, Train L1 Norm: 1.4047, Test L1 Norm: 0.3988, Train Linf Norm: 83.9942, Test Linf Norm: 19.1225\n",
            "Epoch 67: Train Loss: 0.0010, Test Loss: 0.0007, Train L1 Norm: 1.3953, Test L1 Norm: 0.3953, Train Linf Norm: 83.3104, Test Linf Norm: 18.9530\n",
            "Epoch 68: Train Loss: 0.0009, Test Loss: 0.0006, Train L1 Norm: 1.3863, Test L1 Norm: 0.3922, Train Linf Norm: 82.6248, Test Linf Norm: 18.8150\n",
            "Epoch 69: Train Loss: 0.0009, Test Loss: 0.0006, Train L1 Norm: 1.3712, Test L1 Norm: 0.3888, Train Linf Norm: 82.1009, Test Linf Norm: 18.6565\n",
            "Epoch 70: Train Loss: 0.0009, Test Loss: 0.0006, Train L1 Norm: 1.3623, Test L1 Norm: 0.3863, Train Linf Norm: 81.4096, Test Linf Norm: 18.5359\n",
            "Epoch 71: Train Loss: 0.0009, Test Loss: 0.0006, Train L1 Norm: 1.3526, Test L1 Norm: 0.3828, Train Linf Norm: 80.5929, Test Linf Norm: 18.3743\n",
            "Epoch 72: Train Loss: 0.0008, Test Loss: 0.0007, Train L1 Norm: 1.3426, Test L1 Norm: 0.3802, Train Linf Norm: 80.1487, Test Linf Norm: 18.2501\n",
            "Epoch 73: Train Loss: 0.0008, Test Loss: 0.0006, Train L1 Norm: 1.3317, Test L1 Norm: 0.3772, Train Linf Norm: 79.8037, Test Linf Norm: 18.1156\n",
            "Epoch 74: Train Loss: 0.0008, Test Loss: 0.0006, Train L1 Norm: 1.3208, Test L1 Norm: 0.3741, Train Linf Norm: 78.9748, Test Linf Norm: 17.9723\n",
            "Epoch 75: Train Loss: 0.0008, Test Loss: 0.0006, Train L1 Norm: 1.3114, Test L1 Norm: 0.3721, Train Linf Norm: 78.4084, Test Linf Norm: 17.8649\n",
            "Epoch 76: Train Loss: 0.0009, Test Loss: 0.0006, Train L1 Norm: 1.3044, Test L1 Norm: 0.3692, Train Linf Norm: 78.1325, Test Linf Norm: 17.7385\n",
            "Epoch 77: Train Loss: 0.0009, Test Loss: 0.0010, Train L1 Norm: 1.2921, Test L1 Norm: 0.3696, Train Linf Norm: 76.7390, Test Linf Norm: 17.6360\n",
            "Epoch 78: Train Loss: 0.0009, Test Loss: 0.0011, Train L1 Norm: 1.2816, Test L1 Norm: 0.3658, Train Linf Norm: 76.5013, Test Linf Norm: 17.4925\n",
            "Epoch 79: Train Loss: 0.0007, Test Loss: 0.0006, Train L1 Norm: 1.2743, Test L1 Norm: 0.3599, Train Linf Norm: 76.5233, Test Linf Norm: 17.2940\n",
            "Epoch 80: Train Loss: 0.0008, Test Loss: 0.0006, Train L1 Norm: 1.2639, Test L1 Norm: 0.3577, Train Linf Norm: 75.3019, Test Linf Norm: 17.2032\n",
            "Epoch 81: Train Loss: 0.0007, Test Loss: 0.0005, Train L1 Norm: 1.2581, Test L1 Norm: 0.3551, Train Linf Norm: 74.2815, Test Linf Norm: 17.0772\n",
            "Epoch 82: Train Loss: 0.0008, Test Loss: 0.0006, Train L1 Norm: 1.2469, Test L1 Norm: 0.3532, Train Linf Norm: 74.4546, Test Linf Norm: 16.9734\n",
            "Epoch 83: Train Loss: 0.0008, Test Loss: 0.0005, Train L1 Norm: 1.2369, Test L1 Norm: 0.3502, Train Linf Norm: 73.7823, Test Linf Norm: 16.8489\n",
            "Epoch 84: Train Loss: 0.0007, Test Loss: 0.0005, Train L1 Norm: 1.2301, Test L1 Norm: 0.3478, Train Linf Norm: 72.9948, Test Linf Norm: 16.7377\n",
            "Epoch 85: Train Loss: 0.0008, Test Loss: 0.0005, Train L1 Norm: 1.2208, Test L1 Norm: 0.3455, Train Linf Norm: 72.9045, Test Linf Norm: 16.6300\n",
            "Epoch 86: Train Loss: 0.0007, Test Loss: 0.0007, Train L1 Norm: 1.2113, Test L1 Norm: 0.3432, Train Linf Norm: 72.3760, Test Linf Norm: 16.5068\n",
            "Epoch 87: Train Loss: 0.0009, Test Loss: 0.0005, Train L1 Norm: 1.2051, Test L1 Norm: 0.3411, Train Linf Norm: 72.2300, Test Linf Norm: 16.4197\n",
            "Epoch 88: Train Loss: 0.0007, Test Loss: 0.0005, Train L1 Norm: 1.1970, Test L1 Norm: 0.3383, Train Linf Norm: 71.7781, Test Linf Norm: 16.2940\n",
            "Epoch 89: Train Loss: 0.0008, Test Loss: 0.0005, Train L1 Norm: 1.1913, Test L1 Norm: 0.3369, Train Linf Norm: 71.4534, Test Linf Norm: 16.2146\n",
            "Epoch 90: Train Loss: 0.0008, Test Loss: 0.0006, Train L1 Norm: 1.1825, Test L1 Norm: 0.3354, Train Linf Norm: 70.3328, Test Linf Norm: 16.1145\n",
            "Epoch 91: Train Loss: 0.0007, Test Loss: 0.0005, Train L1 Norm: 1.1744, Test L1 Norm: 0.3316, Train Linf Norm: 70.1244, Test Linf Norm: 15.9770\n",
            "Epoch 92: Train Loss: 0.0007, Test Loss: 0.0005, Train L1 Norm: 1.1667, Test L1 Norm: 0.3300, Train Linf Norm: 70.0332, Test Linf Norm: 15.8932\n",
            "Epoch 93: Train Loss: 0.0008, Test Loss: 0.0442, Train L1 Norm: 1.1590, Test L1 Norm: 0.3866, Train Linf Norm: 69.3793, Test Linf Norm: 16.2703\n",
            "Epoch 94: Train Loss: 0.0008, Test Loss: 0.0011, Train L1 Norm: 1.1533, Test L1 Norm: 0.3265, Train Linf Norm: 69.1295, Test Linf Norm: 15.5600\n",
            "Epoch 95: Train Loss: 0.0008, Test Loss: 0.0005, Train L1 Norm: 1.1461, Test L1 Norm: 0.3232, Train Linf Norm: 68.5697, Test Linf Norm: 15.5787\n",
            "Epoch 96: Train Loss: 0.0007, Test Loss: 0.0005, Train L1 Norm: 1.1402, Test L1 Norm: 0.3212, Train Linf Norm: 68.2501, Test Linf Norm: 15.4885\n",
            "Epoch 97: Train Loss: 0.0006, Test Loss: 0.0004, Train L1 Norm: 1.1299, Test L1 Norm: 0.3195, Train Linf Norm: 67.3878, Test Linf Norm: 15.3998\n",
            "Epoch 98: Train Loss: 0.0007, Test Loss: 0.0004, Train L1 Norm: 1.1242, Test L1 Norm: 0.3175, Train Linf Norm: 67.6685, Test Linf Norm: 15.3041\n",
            "Epoch 99: Train Loss: 0.0007, Test Loss: 0.0004, Train L1 Norm: 1.1169, Test L1 Norm: 0.3152, Train Linf Norm: 66.7989, Test Linf Norm: 15.2005\n",
            "Epoch 100: Train Loss: 0.0007, Test Loss: 0.0004, Train L1 Norm: 1.1122, Test L1 Norm: 0.3135, Train Linf Norm: 66.5296, Test Linf Norm: 15.1216\n",
            "Epoch 101: Train Loss: 0.0006, Test Loss: 0.0053, Train L1 Norm: 1.1049, Test L1 Norm: 0.3184, Train Linf Norm: 65.7437, Test Linf Norm: 14.9304\n",
            "Epoch 102: Train Loss: 0.0006, Test Loss: 0.0004, Train L1 Norm: 1.0997, Test L1 Norm: 0.3098, Train Linf Norm: 65.8203, Test Linf Norm: 14.9480\n",
            "Epoch 103: Train Loss: 0.0007, Test Loss: 0.0004, Train L1 Norm: 1.0909, Test L1 Norm: 0.3072, Train Linf Norm: 65.0635, Test Linf Norm: 14.8236\n",
            "Epoch 104: Train Loss: 0.0006, Test Loss: 0.0026, Train L1 Norm: 1.0846, Test L1 Norm: 0.3129, Train Linf Norm: 64.2814, Test Linf Norm: 14.8234\n",
            "Epoch 105: Train Loss: 0.0006, Test Loss: 0.0004, Train L1 Norm: 1.0781, Test L1 Norm: 0.3042, Train Linf Norm: 64.6555, Test Linf Norm: 14.6853\n",
            "Epoch 106: Train Loss: 0.0007, Test Loss: 0.0004, Train L1 Norm: 1.0733, Test L1 Norm: 0.3023, Train Linf Norm: 64.3595, Test Linf Norm: 14.5964\n",
            "Epoch 107: Train Loss: 0.0006, Test Loss: 0.0004, Train L1 Norm: 1.0667, Test L1 Norm: 0.3010, Train Linf Norm: 63.6415, Test Linf Norm: 14.5260\n",
            "Epoch 108: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 1.0599, Test L1 Norm: 0.2986, Train Linf Norm: 63.6271, Test Linf Norm: 14.4216\n",
            "Epoch 109: Train Loss: 0.0006, Test Loss: 0.0004, Train L1 Norm: 1.0538, Test L1 Norm: 0.2971, Train Linf Norm: 63.1974, Test Linf Norm: 14.3474\n",
            "Epoch 110: Train Loss: 0.0006, Test Loss: 0.0004, Train L1 Norm: 1.0488, Test L1 Norm: 0.2955, Train Linf Norm: 62.7448, Test Linf Norm: 14.2640\n",
            "Epoch 111: Train Loss: 0.0007, Test Loss: 0.0004, Train L1 Norm: 1.0429, Test L1 Norm: 0.2938, Train Linf Norm: 62.7085, Test Linf Norm: 14.1950\n",
            "Epoch 112: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 1.0376, Test L1 Norm: 0.2919, Train Linf Norm: 62.4608, Test Linf Norm: 14.1037\n",
            "Epoch 113: Train Loss: 0.0007, Test Loss: 0.0004, Train L1 Norm: 1.0317, Test L1 Norm: 0.2908, Train Linf Norm: 61.8932, Test Linf Norm: 14.0437\n",
            "Epoch 114: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 1.0260, Test L1 Norm: 0.2900, Train Linf Norm: 61.1964, Test Linf Norm: 13.9860\n",
            "Epoch 115: Train Loss: 0.0006, Test Loss: 0.0004, Train L1 Norm: 1.0196, Test L1 Norm: 0.2884, Train Linf Norm: 60.8782, Test Linf Norm: 13.9150\n",
            "Epoch 116: Train Loss: 0.0005, Test Loss: 0.0013, Train L1 Norm: 1.0148, Test L1 Norm: 0.2900, Train Linf Norm: 60.6152, Test Linf Norm: 13.8491\n",
            "Epoch 117: Train Loss: 0.0007, Test Loss: 0.0004, Train L1 Norm: 1.0099, Test L1 Norm: 0.2837, Train Linf Norm: 59.7024, Test Linf Norm: 13.7133\n",
            "Epoch 118: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 1.0028, Test L1 Norm: 0.2824, Train Linf Norm: 58.9591, Test Linf Norm: 13.6513\n",
            "Epoch 119: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.9977, Test L1 Norm: 0.2808, Train Linf Norm: 59.3169, Test Linf Norm: 13.5797\n",
            "Epoch 120: Train Loss: 0.0005, Test Loss: 0.0056, Train L1 Norm: 0.9933, Test L1 Norm: 0.2873, Train Linf Norm: 59.5497, Test Linf Norm: 13.3854\n",
            "Epoch 121: Train Loss: 0.0004, Test Loss: 0.0011, Train L1 Norm: 0.9866, Test L1 Norm: 0.2819, Train Linf Norm: 59.2002, Test Linf Norm: 13.4831\n",
            "Epoch 122: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.9833, Test L1 Norm: 0.2762, Train Linf Norm: 58.8499, Test Linf Norm: 13.3587\n",
            "Epoch 123: Train Loss: 0.0006, Test Loss: 0.0003, Train L1 Norm: 0.9983, Test L1 Norm: 0.2748, Train Linf Norm: 59.7514, Test Linf Norm: 13.2943\n",
            "Epoch 124: Train Loss: 0.0008, Test Loss: 0.0003, Train L1 Norm: 0.9744, Test L1 Norm: 0.2735, Train Linf Norm: 58.2185, Test Linf Norm: 13.2305\n",
            "Epoch 125: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.9676, Test L1 Norm: 0.2721, Train Linf Norm: 57.9910, Test Linf Norm: 13.1650\n",
            "Epoch 126: Train Loss: 0.0006, Test Loss: 0.0003, Train L1 Norm: 0.9630, Test L1 Norm: 0.2708, Train Linf Norm: 57.7323, Test Linf Norm: 13.1011\n",
            "Epoch 127: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.9603, Test L1 Norm: 0.2694, Train Linf Norm: 57.4002, Test Linf Norm: 13.0350\n",
            "Epoch 128: Train Loss: 0.0006, Test Loss: 0.0003, Train L1 Norm: 0.9540, Test L1 Norm: 0.2677, Train Linf Norm: 57.1945, Test Linf Norm: 12.9537\n",
            "Epoch 129: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.9497, Test L1 Norm: 0.2667, Train Linf Norm: 56.5031, Test Linf Norm: 12.9037\n",
            "Epoch 130: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.9446, Test L1 Norm: 0.2674, Train Linf Norm: 56.7012, Test Linf Norm: 12.8843\n",
            "Epoch 131: Train Loss: 0.0006, Test Loss: 0.0003, Train L1 Norm: 0.9383, Test L1 Norm: 0.2642, Train Linf Norm: 55.8246, Test Linf Norm: 12.7880\n",
            "Epoch 132: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.9349, Test L1 Norm: 0.2642, Train Linf Norm: 55.9364, Test Linf Norm: 12.7452\n",
            "Epoch 133: Train Loss: 0.0006, Test Loss: 0.0003, Train L1 Norm: 0.9304, Test L1 Norm: 0.2608, Train Linf Norm: 55.8577, Test Linf Norm: 12.6257\n",
            "Epoch 134: Train Loss: 0.0005, Test Loss: 0.0581, Train L1 Norm: 0.9259, Test L1 Norm: 0.3058, Train Linf Norm: 55.0288, Test Linf Norm: 12.1626\n",
            "Epoch 135: Train Loss: 0.0007, Test Loss: 0.0003, Train L1 Norm: 0.9224, Test L1 Norm: 0.2583, Train Linf Norm: 54.3589, Test Linf Norm: 12.5076\n",
            "Epoch 136: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.9163, Test L1 Norm: 0.2575, Train Linf Norm: 54.8038, Test Linf Norm: 12.4675\n",
            "Epoch 137: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.9119, Test L1 Norm: 0.2560, Train Linf Norm: 54.4171, Test Linf Norm: 12.3997\n",
            "Epoch 138: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.9078, Test L1 Norm: 0.2549, Train Linf Norm: 54.3667, Test Linf Norm: 12.3409\n",
            "Epoch 139: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.9031, Test L1 Norm: 0.2534, Train Linf Norm: 54.2468, Test Linf Norm: 12.2765\n",
            "Epoch 140: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.8987, Test L1 Norm: 0.2558, Train Linf Norm: 54.0932, Test Linf Norm: 12.2883\n",
            "Epoch 141: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.8943, Test L1 Norm: 0.2508, Train Linf Norm: 53.7033, Test Linf Norm: 12.1540\n",
            "Epoch 142: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.8895, Test L1 Norm: 0.2512, Train Linf Norm: 53.4962, Test Linf Norm: 12.1341\n",
            "Epoch 143: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.8860, Test L1 Norm: 0.2484, Train Linf Norm: 53.0034, Test Linf Norm: 12.0407\n",
            "Epoch 144: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.8819, Test L1 Norm: 0.2475, Train Linf Norm: 52.9562, Test Linf Norm: 11.9827\n",
            "Epoch 145: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.8778, Test L1 Norm: 0.2469, Train Linf Norm: 52.7291, Test Linf Norm: 11.9546\n",
            "Epoch 146: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.8741, Test L1 Norm: 0.2452, Train Linf Norm: 52.3849, Test Linf Norm: 11.8866\n",
            "Epoch 147: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.8694, Test L1 Norm: 0.2443, Train Linf Norm: 52.0066, Test Linf Norm: 11.8413\n",
            "Epoch 148: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.8668, Test L1 Norm: 0.2429, Train Linf Norm: 51.9480, Test Linf Norm: 11.7748\n",
            "Epoch 149: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.8617, Test L1 Norm: 0.2413, Train Linf Norm: 51.6369, Test Linf Norm: 11.6996\n",
            "Epoch 150: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.8575, Test L1 Norm: 0.2405, Train Linf Norm: 51.3703, Test Linf Norm: 11.6613\n",
            "Epoch 151: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.8535, Test L1 Norm: 0.2394, Train Linf Norm: 50.9298, Test Linf Norm: 11.5933\n",
            "Epoch 152: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.8502, Test L1 Norm: 0.2386, Train Linf Norm: 50.8801, Test Linf Norm: 11.5714\n",
            "Epoch 153: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.8466, Test L1 Norm: 0.2372, Train Linf Norm: 50.6413, Test Linf Norm: 11.5036\n",
            "Epoch 154: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.8429, Test L1 Norm: 0.2360, Train Linf Norm: 50.2234, Test Linf Norm: 11.4477\n",
            "Epoch 155: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.8396, Test L1 Norm: 0.2350, Train Linf Norm: 50.3143, Test Linf Norm: 11.3992\n",
            "Epoch 156: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.8364, Test L1 Norm: 0.2345, Train Linf Norm: 49.9550, Test Linf Norm: 11.3698\n",
            "Epoch 157: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.8328, Test L1 Norm: 0.2333, Train Linf Norm: 49.9033, Test Linf Norm: 11.3153\n",
            "Epoch 158: Train Loss: 0.0006, Test Loss: 0.0003, Train L1 Norm: 0.8293, Test L1 Norm: 0.2324, Train Linf Norm: 49.4312, Test Linf Norm: 11.2705\n",
            "Epoch 159: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.8255, Test L1 Norm: 0.2317, Train Linf Norm: 49.3694, Test Linf Norm: 11.2340\n",
            "Epoch 160: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.8194, Test L1 Norm: 0.2308, Train Linf Norm: 49.2042, Test Linf Norm: 11.1924\n",
            "Epoch 161: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.8183, Test L1 Norm: 0.2293, Train Linf Norm: 49.1393, Test Linf Norm: 11.1273\n",
            "Epoch 162: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.8148, Test L1 Norm: 0.2285, Train Linf Norm: 48.6707, Test Linf Norm: 11.0821\n",
            "Epoch 163: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.8118, Test L1 Norm: 0.2274, Train Linf Norm: 48.4501, Test Linf Norm: 11.0342\n",
            "Epoch 164: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.8076, Test L1 Norm: 0.2262, Train Linf Norm: 48.4538, Test Linf Norm: 10.9804\n",
            "Epoch 165: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.8036, Test L1 Norm: 0.2250, Train Linf Norm: 48.2793, Test Linf Norm: 10.9230\n",
            "Epoch 166: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.8015, Test L1 Norm: 0.2248, Train Linf Norm: 48.2231, Test Linf Norm: 10.8998\n",
            "Epoch 167: Train Loss: 0.0004, Test Loss: 0.0005, Train L1 Norm: 0.7978, Test L1 Norm: 0.2256, Train Linf Norm: 47.8931, Test Linf Norm: 10.8851\n",
            "Epoch 168: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.7949, Test L1 Norm: 0.2225, Train Linf Norm: 47.6872, Test Linf Norm: 10.8030\n",
            "Epoch 169: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.7933, Test L1 Norm: 0.2217, Train Linf Norm: 47.7493, Test Linf Norm: 10.7601\n",
            "Epoch 170: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.7899, Test L1 Norm: 0.2223, Train Linf Norm: 47.3656, Test Linf Norm: 10.7587\n",
            "Epoch 171: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.7854, Test L1 Norm: 0.2204, Train Linf Norm: 47.2006, Test Linf Norm: 10.6935\n",
            "Epoch 172: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.7823, Test L1 Norm: 0.2190, Train Linf Norm: 46.7740, Test Linf Norm: 10.6332\n",
            "Epoch 173: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.7805, Test L1 Norm: 0.2180, Train Linf Norm: 46.4923, Test Linf Norm: 10.5889\n",
            "Epoch 174: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.7755, Test L1 Norm: 0.2166, Train Linf Norm: 46.3496, Test Linf Norm: 10.5065\n",
            "Epoch 175: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.7732, Test L1 Norm: 0.2164, Train Linf Norm: 46.3505, Test Linf Norm: 10.5067\n",
            "Epoch 176: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.7702, Test L1 Norm: 0.2147, Train Linf Norm: 46.0773, Test Linf Norm: 10.4214\n",
            "Epoch 177: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.7672, Test L1 Norm: 0.2141, Train Linf Norm: 45.7829, Test Linf Norm: 10.4027\n",
            "Epoch 178: Train Loss: 0.0005, Test Loss: 0.0002, Train L1 Norm: 0.7654, Test L1 Norm: 0.2134, Train Linf Norm: 45.7241, Test Linf Norm: 10.3681\n",
            "Epoch 179: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.7603, Test L1 Norm: 0.2123, Train Linf Norm: 45.5887, Test Linf Norm: 10.2698\n",
            "Epoch 180: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.7559, Test L1 Norm: 0.2118, Train Linf Norm: 45.5005, Test Linf Norm: 10.2819\n",
            "Epoch 181: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.7530, Test L1 Norm: 0.2106, Train Linf Norm: 45.0949, Test Linf Norm: 10.2201\n",
            "Epoch 182: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.7526, Test L1 Norm: 0.2103, Train Linf Norm: 45.2927, Test Linf Norm: 10.2148\n",
            "Epoch 183: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.7494, Test L1 Norm: 0.2090, Train Linf Norm: 44.8783, Test Linf Norm: 10.1577\n",
            "Epoch 184: Train Loss: 0.0004, Test Loss: 0.0033, Train L1 Norm: 0.7496, Test L1 Norm: 0.2137, Train Linf Norm: 45.0022, Test Linf Norm: 10.0093\n",
            "Epoch 185: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.7410, Test L1 Norm: 0.2082, Train Linf Norm: 44.5458, Test Linf Norm: 10.1036\n",
            "Epoch 186: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.7400, Test L1 Norm: 0.2067, Train Linf Norm: 44.2678, Test Linf Norm: 10.0428\n",
            "Epoch 187: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.7266, Test L1 Norm: 0.2059, Train Linf Norm: 43.7142, Test Linf Norm: 9.9947\n",
            "Epoch 188: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.7336, Test L1 Norm: 0.2052, Train Linf Norm: 43.9433, Test Linf Norm: 9.9743\n",
            "Epoch 189: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.7305, Test L1 Norm: 0.2042, Train Linf Norm: 43.6876, Test Linf Norm: 9.9266\n",
            "Epoch 190: Train Loss: 0.0005, Test Loss: 0.0002, Train L1 Norm: 0.7286, Test L1 Norm: 0.2037, Train Linf Norm: 43.6615, Test Linf Norm: 9.9042\n",
            "Epoch 191: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.7244, Test L1 Norm: 0.2026, Train Linf Norm: 43.4435, Test Linf Norm: 9.8498\n",
            "Epoch 192: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.7229, Test L1 Norm: 0.2019, Train Linf Norm: 43.2661, Test Linf Norm: 9.8105\n",
            "Epoch 193: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.7199, Test L1 Norm: 0.2011, Train Linf Norm: 42.9234, Test Linf Norm: 9.7776\n",
            "Epoch 194: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.7171, Test L1 Norm: 0.2005, Train Linf Norm: 42.8239, Test Linf Norm: 9.7404\n",
            "Epoch 195: Train Loss: 0.0005, Test Loss: 0.0002, Train L1 Norm: 0.7152, Test L1 Norm: 0.2000, Train Linf Norm: 42.6911, Test Linf Norm: 9.7179\n",
            "Epoch 196: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.7122, Test L1 Norm: 0.1987, Train Linf Norm: 42.5590, Test Linf Norm: 9.6601\n",
            "Epoch 197: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.7097, Test L1 Norm: 0.1979, Train Linf Norm: 34.0608, Test Linf Norm: 9.6246\n",
            "Epoch 198: Train Loss: 0.0005, Test Loss: 0.0002, Train L1 Norm: 0.7063, Test L1 Norm: 0.1977, Train Linf Norm: 42.3703, Test Linf Norm: 9.6132\n",
            "Epoch 199: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.7041, Test L1 Norm: 0.1974, Train Linf Norm: 42.3272, Test Linf Norm: 9.5908\n",
            "Epoch 200: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.7014, Test L1 Norm: 0.1965, Train Linf Norm: 41.9573, Test Linf Norm: 9.5491\n",
            "Epoch 201: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.7005, Test L1 Norm: 0.1955, Train Linf Norm: 39.7108, Test Linf Norm: 9.5091\n",
            "Epoch 202: Train Loss: 0.0005, Test Loss: 0.0002, Train L1 Norm: 0.6949, Test L1 Norm: 0.1952, Train Linf Norm: 41.5457, Test Linf Norm: 9.4850\n",
            "Epoch 203: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.6937, Test L1 Norm: 0.1938, Train Linf Norm: 41.6968, Test Linf Norm: 9.4272\n",
            "Epoch 204: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.6906, Test L1 Norm: 0.1932, Train Linf Norm: 41.3587, Test Linf Norm: 9.3958\n",
            "Epoch 205: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.6898, Test L1 Norm: 0.1924, Train Linf Norm: 41.4444, Test Linf Norm: 9.3594\n",
            "Epoch 206: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.6862, Test L1 Norm: 0.1919, Train Linf Norm: 41.2290, Test Linf Norm: 9.3319\n",
            "Epoch 207: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.6839, Test L1 Norm: 0.1909, Train Linf Norm: 41.0458, Test Linf Norm: 9.2861\n",
            "Epoch 208: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.6819, Test L1 Norm: 0.1906, Train Linf Norm: 40.8461, Test Linf Norm: 9.2596\n",
            "Epoch 209: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.6787, Test L1 Norm: 0.1907, Train Linf Norm: 40.1223, Test Linf Norm: 9.2598\n",
            "Epoch 210: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.6761, Test L1 Norm: 0.1893, Train Linf Norm: 40.4404, Test Linf Norm: 9.2079\n",
            "Epoch 211: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.6725, Test L1 Norm: 0.1881, Train Linf Norm: 40.4831, Test Linf Norm: 9.1497\n",
            "Epoch 212: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.6716, Test L1 Norm: 0.1879, Train Linf Norm: 40.0745, Test Linf Norm: 9.1394\n",
            "Epoch 213: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.6692, Test L1 Norm: 0.1867, Train Linf Norm: 40.3208, Test Linf Norm: 9.0786\n",
            "Epoch 214: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.6699, Test L1 Norm: 0.1865, Train Linf Norm: 39.7950, Test Linf Norm: 9.0711\n",
            "Epoch 215: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.6667, Test L1 Norm: 0.1856, Train Linf Norm: 39.9950, Test Linf Norm: 9.0258\n",
            "Epoch 216: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.6616, Test L1 Norm: 0.1850, Train Linf Norm: 39.6286, Test Linf Norm: 8.9996\n",
            "Epoch 217: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.6599, Test L1 Norm: 0.1843, Train Linf Norm: 39.6819, Test Linf Norm: 8.9662\n",
            "Epoch 218: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.6588, Test L1 Norm: 0.1839, Train Linf Norm: 39.6305, Test Linf Norm: 8.9467\n",
            "Epoch 219: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.6553, Test L1 Norm: 0.1833, Train Linf Norm: 39.1799, Test Linf Norm: 8.9177\n",
            "Epoch 220: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.6534, Test L1 Norm: 0.1826, Train Linf Norm: 39.0803, Test Linf Norm: 8.8850\n",
            "Epoch 221: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.6529, Test L1 Norm: 0.1822, Train Linf Norm: 39.1654, Test Linf Norm: 8.8630\n",
            "Epoch 222: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.6499, Test L1 Norm: 0.1812, Train Linf Norm: 39.0496, Test Linf Norm: 8.8150\n",
            "Epoch 223: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.6472, Test L1 Norm: 0.1806, Train Linf Norm: 38.8302, Test Linf Norm: 8.7881\n",
            "Epoch 224: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.6446, Test L1 Norm: 0.1800, Train Linf Norm: 38.7523, Test Linf Norm: 8.7571\n",
            "Epoch 225: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.6417, Test L1 Norm: 0.1796, Train Linf Norm: 38.3245, Test Linf Norm: 8.7412\n",
            "Epoch 226: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.6403, Test L1 Norm: 0.1788, Train Linf Norm: 38.4106, Test Linf Norm: 8.6989\n",
            "Epoch 227: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.6383, Test L1 Norm: 0.1786, Train Linf Norm: 38.3657, Test Linf Norm: 8.6776\n",
            "Epoch 228: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.6363, Test L1 Norm: 0.1774, Train Linf Norm: 38.1782, Test Linf Norm: 8.6298\n",
            "Epoch 229: Train Loss: 0.0003, Test Loss: 0.0004, Train L1 Norm: 0.6337, Test L1 Norm: 0.1775, Train Linf Norm: 38.0540, Test Linf Norm: 8.5791\n",
            "Epoch 230: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.6308, Test L1 Norm: 0.1759, Train Linf Norm: 37.8672, Test Linf Norm: 8.5584\n",
            "Epoch 231: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.6292, Test L1 Norm: 0.1758, Train Linf Norm: 37.5560, Test Linf Norm: 8.5571\n",
            "Epoch 232: Train Loss: 0.0003, Test Loss: 0.0006, Train L1 Norm: 0.6281, Test L1 Norm: 0.1762, Train Linf Norm: 37.7639, Test Linf Norm: 8.5338\n",
            "Epoch 233: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.6288, Test L1 Norm: 0.1748, Train Linf Norm: 37.8237, Test Linf Norm: 8.5093\n",
            "Epoch 234: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.6218, Test L1 Norm: 0.1744, Train Linf Norm: 37.1541, Test Linf Norm: 8.4826\n",
            "Epoch 235: Train Loss: 0.0002, Test Loss: 0.0003, Train L1 Norm: 0.6220, Test L1 Norm: 0.1735, Train Linf Norm: 37.2774, Test Linf Norm: 8.4091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 15:58:39,810]\u001b[0m Trial 0 finished with value: 0.17337129907608032 and parameters: {'n_layers': 7, 'n_units_0': 1233, 'n_units_1': 1605, 'n_units_2': 1313, 'n_units_3': 1108, 'n_units_4': 1245, 'n_units_5': 969, 'n_units_6': 2042, 'hidden_activation': 'LeakyReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'SGD', 'lr': 0.0001789923268447879, 'batch_size': 64, 'n_epochs': 236, 'scheduler': 'ReduceLROnPlateau', 'weight_decay': 0.0003945400717870281, 'momentum': 0.252914135701581, 'factor': 0.2245235336799933, 'patience': 10, 'threshold': 0.002669230891788366}. Best is trial 0 with value: 0.17337129907608032.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 236: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.6186, Test L1 Norm: 0.1734, Train Linf Norm: 37.2713, Test Linf Norm: 8.4296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-82e2252adf92>:150: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  t_max_fraction = trial.suggest_uniform('t_max_fraction', 0.1, 0.2)\n",
            "<ipython-input-28-82e2252adf92>:153: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  eta_min = trial.suggest_loguniform(\"eta_min\", 1e-7, 1e-2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 5.6952, Test Loss: 0.6089, Train L1 Norm: 20.0678, Test L1 Norm: 2.2011, Train Linf Norm: 7536.0142, Test Linf Norm: 426.2824\n",
            "Epoch 2: Train Loss: 0.2232, Test Loss: 0.0335, Train L1 Norm: 4.6453, Test L1 Norm: 1.0153, Train Linf Norm: 1942.1603, Test Linf Norm: 207.6058\n",
            "Epoch 3: Train Loss: 0.0495, Test Loss: 0.0196, Train L1 Norm: 2.7019, Test L1 Norm: 0.6172, Train Linf Norm: 1105.9399, Test Linf Norm: 127.6338\n",
            "Epoch 4: Train Loss: 0.0509, Test Loss: 0.1038, Train L1 Norm: 2.0130, Test L1 Norm: 0.5380, Train Linf Norm: 834.8347, Test Linf Norm: 107.4017\n",
            "Epoch 5: Train Loss: 0.0450, Test Loss: 0.0698, Train L1 Norm: 1.7147, Test L1 Norm: 0.6032, Train Linf Norm: 708.2559, Test Linf Norm: 124.6155\n",
            "Epoch 6: Train Loss: 0.0374, Test Loss: 0.0085, Train L1 Norm: 1.5518, Test L1 Norm: 0.4181, Train Linf Norm: 650.3412, Test Linf Norm: 88.4222\n",
            "Epoch 7: Train Loss: 0.0094, Test Loss: 0.0066, Train L1 Norm: 1.3945, Test L1 Norm: 0.3679, Train Linf Norm: 585.1776, Test Linf Norm: 78.2135\n",
            "Epoch 8: Train Loss: 0.0072, Test Loss: 0.0382, Train L1 Norm: 1.2546, Test L1 Norm: 0.3676, Train Linf Norm: 526.7293, Test Linf Norm: 75.4406\n",
            "Epoch 9: Train Loss: 0.0117, Test Loss: 0.0137, Train L1 Norm: 1.2389, Test L1 Norm: 0.3295, Train Linf Norm: 529.5508, Test Linf Norm: 68.6156\n",
            "Epoch 10: Train Loss: 0.0050, Test Loss: 0.0034, Train L1 Norm: 1.1326, Test L1 Norm: 0.3144, Train Linf Norm: 483.0130, Test Linf Norm: 67.8233\n",
            "Epoch 11: Train Loss: 0.0040, Test Loss: 0.0027, Train L1 Norm: 1.1050, Test L1 Norm: 0.2964, Train Linf Norm: 376.5942, Test Linf Norm: 64.1402\n",
            "Epoch 12: Train Loss: 0.0041, Test Loss: 0.0043, Train L1 Norm: 1.0388, Test L1 Norm: 0.2975, Train Linf Norm: 434.6414, Test Linf Norm: 64.0804\n",
            "Epoch 13: Train Loss: 0.0027, Test Loss: 0.0023, Train L1 Norm: 1.0205, Test L1 Norm: 0.2860, Train Linf Norm: 423.2958, Test Linf Norm: 61.6451\n",
            "Epoch 14: Train Loss: 0.0022, Test Loss: 0.0019, Train L1 Norm: 1.0045, Test L1 Norm: 0.2690, Train Linf Norm: 427.6059, Test Linf Norm: 58.3065\n",
            "Epoch 15: Train Loss: 0.0022, Test Loss: 0.0019, Train L1 Norm: 0.9646, Test L1 Norm: 0.2734, Train Linf Norm: 324.9163, Test Linf Norm: 59.2945\n",
            "Epoch 16: Train Loss: 0.0020, Test Loss: 0.0020, Train L1 Norm: 0.9609, Test L1 Norm: 0.2652, Train Linf Norm: 408.7559, Test Linf Norm: 57.1947\n",
            "Epoch 17: Train Loss: 0.0018, Test Loss: 0.0018, Train L1 Norm: 0.9465, Test L1 Norm: 0.2663, Train Linf Norm: 404.2871, Test Linf Norm: 57.6264\n",
            "Epoch 18: Train Loss: 0.0017, Test Loss: 0.0018, Train L1 Norm: 0.9414, Test L1 Norm: 0.2636, Train Linf Norm: 398.5456, Test Linf Norm: 57.0658\n",
            "Epoch 19: Train Loss: 0.0017, Test Loss: 0.0017, Train L1 Norm: 0.9367, Test L1 Norm: 0.2624, Train Linf Norm: 398.2591, Test Linf Norm: 56.9302\n",
            "Epoch 20: Train Loss: 0.0016, Test Loss: 0.0017, Train L1 Norm: 0.9334, Test L1 Norm: 0.2615, Train Linf Norm: 400.2006, Test Linf Norm: 56.7471\n",
            "Epoch 21: Train Loss: 0.0016, Test Loss: 0.0016, Train L1 Norm: 0.9301, Test L1 Norm: 0.2605, Train Linf Norm: 401.9848, Test Linf Norm: 56.4950\n",
            "Epoch 22: Train Loss: 0.0016, Test Loss: 0.0017, Train L1 Norm: 0.9280, Test L1 Norm: 0.2599, Train Linf Norm: 398.4435, Test Linf Norm: 56.4298\n",
            "Epoch 23: Train Loss: 0.0016, Test Loss: 0.0016, Train L1 Norm: 0.9285, Test L1 Norm: 0.2596, Train Linf Norm: 394.6466, Test Linf Norm: 56.3277\n",
            "Epoch 24: Train Loss: 0.0016, Test Loss: 0.0017, Train L1 Norm: 0.9267, Test L1 Norm: 0.2595, Train Linf Norm: 392.9943, Test Linf Norm: 56.2192\n",
            "Epoch 25: Train Loss: 0.0016, Test Loss: 0.0017, Train L1 Norm: 0.9235, Test L1 Norm: 0.2596, Train Linf Norm: 395.6078, Test Linf Norm: 56.2866\n",
            "Epoch 26: Train Loss: 0.0016, Test Loss: 0.0016, Train L1 Norm: 0.9160, Test L1 Norm: 0.2557, Train Linf Norm: 385.8185, Test Linf Norm: 55.5047\n",
            "Epoch 27: Train Loss: 0.0017, Test Loss: 0.0017, Train L1 Norm: 0.9061, Test L1 Norm: 0.2571, Train Linf Norm: 380.6840, Test Linf Norm: 55.8153\n",
            "Epoch 28: Train Loss: 0.0017, Test Loss: 0.0018, Train L1 Norm: 0.8995, Test L1 Norm: 0.2500, Train Linf Norm: 380.6598, Test Linf Norm: 54.3832\n",
            "Epoch 29: Train Loss: 0.0017, Test Loss: 0.0016, Train L1 Norm: 0.8896, Test L1 Norm: 0.2498, Train Linf Norm: 377.8904, Test Linf Norm: 54.3860\n",
            "Epoch 30: Train Loss: 0.0016, Test Loss: 0.0015, Train L1 Norm: 0.8827, Test L1 Norm: 0.2406, Train Linf Norm: 375.2575, Test Linf Norm: 52.3649\n",
            "Epoch 31: Train Loss: 0.0028, Test Loss: 0.0017, Train L1 Norm: 0.8528, Test L1 Norm: 0.2375, Train Linf Norm: 360.0748, Test Linf Norm: 51.8198\n",
            "Epoch 32: Train Loss: 0.0018, Test Loss: 0.0018, Train L1 Norm: 0.8563, Test L1 Norm: 0.2363, Train Linf Norm: 333.8572, Test Linf Norm: 51.2979\n",
            "Epoch 33: Train Loss: 0.0022, Test Loss: 0.0044, Train L1 Norm: 0.8295, Test L1 Norm: 0.2332, Train Linf Norm: 348.3988, Test Linf Norm: 49.8673\n",
            "Epoch 34: Train Loss: 0.0056, Test Loss: 0.0028, Train L1 Norm: 0.8377, Test L1 Norm: 0.2332, Train Linf Norm: 357.4441, Test Linf Norm: 51.1178\n",
            "Epoch 35: Train Loss: 0.0040, Test Loss: 0.0019, Train L1 Norm: 0.8294, Test L1 Norm: 0.2386, Train Linf Norm: 353.4823, Test Linf Norm: 51.9322\n",
            "Epoch 36: Train Loss: 0.0026, Test Loss: 0.0026, Train L1 Norm: 0.8114, Test L1 Norm: 0.2159, Train Linf Norm: 346.6151, Test Linf Norm: 47.2499\n",
            "Epoch 37: Train Loss: 0.0018, Test Loss: 0.0219, Train L1 Norm: 0.7817, Test L1 Norm: 0.2560, Train Linf Norm: 312.0670, Test Linf Norm: 50.8171\n",
            "Epoch 38: Train Loss: 0.0295, Test Loss: 0.0487, Train L1 Norm: 0.8738, Test L1 Norm: 0.2604, Train Linf Norm: 368.7164, Test Linf Norm: 49.2827\n",
            "Epoch 39: Train Loss: 0.0116, Test Loss: 0.0258, Train L1 Norm: 0.8334, Test L1 Norm: 0.2513, Train Linf Norm: 350.6443, Test Linf Norm: 54.2146\n",
            "Epoch 40: Train Loss: 0.0053, Test Loss: 0.0043, Train L1 Norm: 0.7694, Test L1 Norm: 0.2225, Train Linf Norm: 319.0068, Test Linf Norm: 48.1033\n",
            "Epoch 41: Train Loss: 0.0277, Test Loss: 0.0087, Train L1 Norm: 0.9003, Test L1 Norm: 0.2395, Train Linf Norm: 379.0261, Test Linf Norm: 51.6325\n",
            "Epoch 42: Train Loss: 0.0099, Test Loss: 0.0018, Train L1 Norm: 0.8173, Test L1 Norm: 0.2236, Train Linf Norm: 321.3341, Test Linf Norm: 49.7082\n",
            "Epoch 43: Train Loss: 0.0068, Test Loss: 0.0040, Train L1 Norm: 0.7772, Test L1 Norm: 0.2217, Train Linf Norm: 311.6472, Test Linf Norm: 48.1077\n",
            "Epoch 44: Train Loss: 0.0084, Test Loss: 0.0204, Train L1 Norm: 0.7783, Test L1 Norm: 0.2238, Train Linf Norm: 327.8191, Test Linf Norm: 46.5175\n",
            "Epoch 45: Train Loss: 0.0090, Test Loss: 0.0104, Train L1 Norm: 0.7701, Test L1 Norm: 0.2057, Train Linf Norm: 325.3302, Test Linf Norm: 44.7054\n",
            "Epoch 46: Train Loss: 0.0037, Test Loss: 0.0017, Train L1 Norm: 0.7283, Test L1 Norm: 0.2100, Train Linf Norm: 310.0903, Test Linf Norm: 47.5646\n",
            "Epoch 47: Train Loss: 0.0040, Test Loss: 0.0016, Train L1 Norm: 0.7154, Test L1 Norm: 0.1948, Train Linf Norm: 301.3386, Test Linf Norm: 44.1516\n",
            "Epoch 48: Train Loss: 0.0035, Test Loss: 0.0024, Train L1 Norm: 0.6765, Test L1 Norm: 0.1854, Train Linf Norm: 288.4386, Test Linf Norm: 41.8410\n",
            "Epoch 49: Train Loss: 0.0022, Test Loss: 0.0067, Train L1 Norm: 0.6627, Test L1 Norm: 0.1814, Train Linf Norm: 280.8951, Test Linf Norm: 39.7082\n",
            "Epoch 50: Train Loss: 0.0016, Test Loss: 0.0021, Train L1 Norm: 0.6502, Test L1 Norm: 0.1803, Train Linf Norm: 277.1092, Test Linf Norm: 40.3929\n",
            "Epoch 51: Train Loss: 0.0029, Test Loss: 0.0015, Train L1 Norm: 0.6308, Test L1 Norm: 0.1708, Train Linf Norm: 262.2767, Test Linf Norm: 38.7886\n",
            "Epoch 52: Train Loss: 0.0012, Test Loss: 0.0016, Train L1 Norm: 0.6135, Test L1 Norm: 0.1700, Train Linf Norm: 264.5752, Test Linf Norm: 38.6399\n",
            "Epoch 53: Train Loss: 0.0019, Test Loss: 0.0007, Train L1 Norm: 0.6041, Test L1 Norm: 0.1724, Train Linf Norm: 256.8291, Test Linf Norm: 39.2679\n",
            "Epoch 54: Train Loss: 0.0009, Test Loss: 0.0052, Train L1 Norm: 0.5966, Test L1 Norm: 0.1781, Train Linf Norm: 253.0276, Test Linf Norm: 38.8348\n",
            "Epoch 55: Train Loss: 0.0032, Test Loss: 0.0011, Train L1 Norm: 0.5893, Test L1 Norm: 0.1683, Train Linf Norm: 251.9280, Test Linf Norm: 38.4712\n",
            "Epoch 56: Train Loss: 0.0009, Test Loss: 0.0007, Train L1 Norm: 0.5826, Test L1 Norm: 0.1671, Train Linf Norm: 246.9692, Test Linf Norm: 38.1723\n",
            "Epoch 57: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.5819, Test L1 Norm: 0.1645, Train Linf Norm: 247.1191, Test Linf Norm: 37.6578\n",
            "Epoch 58: Train Loss: 0.0007, Test Loss: 0.0006, Train L1 Norm: 0.5738, Test L1 Norm: 0.1633, Train Linf Norm: 243.5852, Test Linf Norm: 37.4353\n",
            "Epoch 59: Train Loss: 0.0007, Test Loss: 0.0007, Train L1 Norm: 0.5720, Test L1 Norm: 0.1613, Train Linf Norm: 244.5250, Test Linf Norm: 36.9160\n",
            "Epoch 60: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.5657, Test L1 Norm: 0.1623, Train Linf Norm: 242.0440, Test Linf Norm: 37.1761\n",
            "Epoch 61: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.5680, Test L1 Norm: 0.1614, Train Linf Norm: 244.0511, Test Linf Norm: 36.9667\n",
            "Epoch 62: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.5660, Test L1 Norm: 0.1605, Train Linf Norm: 240.1939, Test Linf Norm: 36.7934\n",
            "Epoch 63: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.5629, Test L1 Norm: 0.1605, Train Linf Norm: 240.0870, Test Linf Norm: 36.8208\n",
            "Epoch 64: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.5632, Test L1 Norm: 0.1606, Train Linf Norm: 239.5538, Test Linf Norm: 36.8290\n",
            "Epoch 65: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.5633, Test L1 Norm: 0.1606, Train Linf Norm: 238.1631, Test Linf Norm: 36.8210\n",
            "Epoch 66: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.5615, Test L1 Norm: 0.1603, Train Linf Norm: 244.4527, Test Linf Norm: 36.7524\n",
            "Epoch 67: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.5618, Test L1 Norm: 0.1596, Train Linf Norm: 243.2177, Test Linf Norm: 36.5932\n",
            "Epoch 68: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.5594, Test L1 Norm: 0.1592, Train Linf Norm: 225.2664, Test Linf Norm: 36.5498\n",
            "Epoch 69: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.5602, Test L1 Norm: 0.1580, Train Linf Norm: 241.5454, Test Linf Norm: 36.2491\n",
            "Epoch 70: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.5536, Test L1 Norm: 0.1575, Train Linf Norm: 238.2433, Test Linf Norm: 36.1480\n",
            "Epoch 71: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.5519, Test L1 Norm: 0.1596, Train Linf Norm: 234.5816, Test Linf Norm: 36.5128\n",
            "Epoch 72: Train Loss: 0.0008, Test Loss: 0.0010, Train L1 Norm: 0.5487, Test L1 Norm: 0.1548, Train Linf Norm: 236.9611, Test Linf Norm: 35.2760\n",
            "Epoch 73: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.5421, Test L1 Norm: 0.1540, Train Linf Norm: 229.0673, Test Linf Norm: 35.2620\n",
            "Epoch 74: Train Loss: 0.0009, Test Loss: 0.0012, Train L1 Norm: 0.5374, Test L1 Norm: 0.1596, Train Linf Norm: 232.4912, Test Linf Norm: 36.5677\n",
            "Epoch 75: Train Loss: 0.0012, Test Loss: 0.0009, Train L1 Norm: 0.5376, Test L1 Norm: 0.1607, Train Linf Norm: 232.0139, Test Linf Norm: 36.4921\n",
            "Epoch 76: Train Loss: 0.0008, Test Loss: 0.0010, Train L1 Norm: 0.5264, Test L1 Norm: 0.1506, Train Linf Norm: 225.3535, Test Linf Norm: 34.3294\n",
            "Epoch 77: Train Loss: 0.0010, Test Loss: 0.0006, Train L1 Norm: 0.5132, Test L1 Norm: 0.1502, Train Linf Norm: 218.8656, Test Linf Norm: 34.4434\n",
            "Epoch 78: Train Loss: 0.0035, Test Loss: 0.0430, Train L1 Norm: 0.5115, Test L1 Norm: 0.1839, Train Linf Norm: 204.5602, Test Linf Norm: 35.6934\n",
            "Epoch 79: Train Loss: 0.0041, Test Loss: 0.0021, Train L1 Norm: 0.5351, Test L1 Norm: 0.1592, Train Linf Norm: 228.1335, Test Linf Norm: 35.5278\n",
            "Epoch 80: Train Loss: 0.0020, Test Loss: 0.0011, Train L1 Norm: 0.5240, Test L1 Norm: 0.1466, Train Linf Norm: 224.7107, Test Linf Norm: 33.7022\n",
            "Epoch 81: Train Loss: 0.0032, Test Loss: 0.0011, Train L1 Norm: 0.5110, Test L1 Norm: 0.1334, Train Linf Norm: 218.5638, Test Linf Norm: 30.4718\n",
            "Epoch 82: Train Loss: 0.0052, Test Loss: 0.0078, Train L1 Norm: 0.4851, Test L1 Norm: 0.1505, Train Linf Norm: 202.4319, Test Linf Norm: 32.0268\n",
            "Epoch 83: Train Loss: 0.0118, Test Loss: 0.0049, Train L1 Norm: 0.5435, Test L1 Norm: 0.1665, Train Linf Norm: 231.2676, Test Linf Norm: 37.2371\n",
            "Epoch 84: Train Loss: 0.0429, Test Loss: 0.0083, Train L1 Norm: 0.6805, Test L1 Norm: 0.1930, Train Linf Norm: 282.4774, Test Linf Norm: 42.2273\n",
            "Epoch 85: Train Loss: 0.0309, Test Loss: 0.0225, Train L1 Norm: 0.7662, Test L1 Norm: 0.2275, Train Linf Norm: 321.2806, Test Linf Norm: 46.7341\n",
            "Epoch 86: Train Loss: 0.0056, Test Loss: 0.0018, Train L1 Norm: 0.7185, Test L1 Norm: 0.1874, Train Linf Norm: 310.8625, Test Linf Norm: 43.1018\n",
            "Epoch 87: Train Loss: 0.0260, Test Loss: 0.0058, Train L1 Norm: 0.7610, Test L1 Norm: 0.2247, Train Linf Norm: 321.8924, Test Linf Norm: 49.7193\n",
            "Epoch 88: Train Loss: 0.0052, Test Loss: 0.0046, Train L1 Norm: 0.6941, Test L1 Norm: 0.1778, Train Linf Norm: 300.2741, Test Linf Norm: 39.8231\n",
            "Epoch 89: Train Loss: 0.0059, Test Loss: 0.0023, Train L1 Norm: 0.6300, Test L1 Norm: 0.1734, Train Linf Norm: 268.1586, Test Linf Norm: 40.1256\n",
            "Epoch 90: Train Loss: 0.0019, Test Loss: 0.0007, Train L1 Norm: 0.6006, Test L1 Norm: 0.1683, Train Linf Norm: 259.1830, Test Linf Norm: 38.7712\n",
            "Epoch 91: Train Loss: 0.0022, Test Loss: 0.0007, Train L1 Norm: 0.5686, Test L1 Norm: 0.1535, Train Linf Norm: 241.5944, Test Linf Norm: 35.5339\n",
            "Epoch 92: Train Loss: 0.0011, Test Loss: 0.0020, Train L1 Norm: 0.5450, Test L1 Norm: 0.1548, Train Linf Norm: 234.1401, Test Linf Norm: 35.0230\n",
            "Epoch 93: Train Loss: 0.0016, Test Loss: 0.0010, Train L1 Norm: 0.5303, Test L1 Norm: 0.1397, Train Linf Norm: 228.3893, Test Linf Norm: 32.0729\n",
            "Epoch 94: Train Loss: 0.0007, Test Loss: 0.0008, Train L1 Norm: 0.5087, Test L1 Norm: 0.1507, Train Linf Norm: 217.0963, Test Linf Norm: 34.6803\n",
            "Epoch 95: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.5014, Test L1 Norm: 0.1428, Train Linf Norm: 214.1905, Test Linf Norm: 33.0631\n",
            "Epoch 96: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.4928, Test L1 Norm: 0.1360, Train Linf Norm: 197.5352, Test Linf Norm: 31.4905\n",
            "Epoch 97: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.4888, Test L1 Norm: 0.1373, Train Linf Norm: 209.4617, Test Linf Norm: 31.8693\n",
            "Epoch 98: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.4788, Test L1 Norm: 0.1375, Train Linf Norm: 205.4194, Test Linf Norm: 31.8475\n",
            "Epoch 99: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.4759, Test L1 Norm: 0.1355, Train Linf Norm: 204.5716, Test Linf Norm: 31.4745\n",
            "Epoch 100: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.4717, Test L1 Norm: 0.1368, Train Linf Norm: 202.8074, Test Linf Norm: 31.6420\n",
            "Epoch 101: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.4687, Test L1 Norm: 0.1338, Train Linf Norm: 200.6928, Test Linf Norm: 31.0652\n",
            "Epoch 102: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.4665, Test L1 Norm: 0.1332, Train Linf Norm: 199.0329, Test Linf Norm: 30.9239\n",
            "Epoch 103: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.4636, Test L1 Norm: 0.1329, Train Linf Norm: 198.6892, Test Linf Norm: 30.8618\n",
            "Epoch 104: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.4635, Test L1 Norm: 0.1322, Train Linf Norm: 200.0751, Test Linf Norm: 30.7153\n",
            "Epoch 105: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.4621, Test L1 Norm: 0.1328, Train Linf Norm: 186.9339, Test Linf Norm: 30.8202\n",
            "Epoch 106: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.4633, Test L1 Norm: 0.1323, Train Linf Norm: 201.3792, Test Linf Norm: 30.7304\n",
            "Epoch 107: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.4617, Test L1 Norm: 0.1322, Train Linf Norm: 199.4284, Test Linf Norm: 30.7097\n",
            "Epoch 108: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.4617, Test L1 Norm: 0.1325, Train Linf Norm: 198.4132, Test Linf Norm: 30.7719\n",
            "Epoch 109: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.4589, Test L1 Norm: 0.1318, Train Linf Norm: 196.7979, Test Linf Norm: 30.6192\n",
            "Epoch 110: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.4598, Test L1 Norm: 0.1315, Train Linf Norm: 199.3236, Test Linf Norm: 30.5507\n",
            "Epoch 111: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.4569, Test L1 Norm: 0.1307, Train Linf Norm: 195.8348, Test Linf Norm: 30.3562\n",
            "Epoch 112: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.4571, Test L1 Norm: 0.1300, Train Linf Norm: 195.8574, Test Linf Norm: 30.2074\n",
            "Epoch 113: Train Loss: 0.0004, Test Loss: 0.0005, Train L1 Norm: 0.4503, Test L1 Norm: 0.1281, Train Linf Norm: 194.0942, Test Linf Norm: 29.7615\n",
            "Epoch 114: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.4488, Test L1 Norm: 0.1290, Train Linf Norm: 190.8355, Test Linf Norm: 29.9339\n",
            "Epoch 115: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.4402, Test L1 Norm: 0.1285, Train Linf Norm: 181.1402, Test Linf Norm: 29.8792\n",
            "Epoch 116: Train Loss: 0.0005, Test Loss: 0.0015, Train L1 Norm: 0.4398, Test L1 Norm: 0.1266, Train Linf Norm: 190.8320, Test Linf Norm: 29.1530\n",
            "Epoch 117: Train Loss: 0.0009, Test Loss: 0.0005, Train L1 Norm: 0.4418, Test L1 Norm: 0.1192, Train Linf Norm: 189.1679, Test Linf Norm: 27.4801\n",
            "Epoch 118: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.4196, Test L1 Norm: 0.1246, Train Linf Norm: 181.1223, Test Linf Norm: 28.8988\n",
            "Epoch 119: Train Loss: 0.0007, Test Loss: 0.0005, Train L1 Norm: 0.4233, Test L1 Norm: 0.1211, Train Linf Norm: 182.6737, Test Linf Norm: 28.0307\n",
            "Epoch 120: Train Loss: 0.0007, Test Loss: 0.0005, Train L1 Norm: 0.4176, Test L1 Norm: 0.1180, Train Linf Norm: 178.2417, Test Linf Norm: 27.2535\n",
            "Epoch 121: Train Loss: 0.0007, Test Loss: 0.0006, Train L1 Norm: 0.4104, Test L1 Norm: 0.1170, Train Linf Norm: 177.1457, Test Linf Norm: 27.0256\n",
            "Epoch 122: Train Loss: 0.0005, Test Loss: 0.0008, Train L1 Norm: 0.4046, Test L1 Norm: 0.1199, Train Linf Norm: 174.2665, Test Linf Norm: 27.7316\n",
            "Epoch 123: Train Loss: 0.0011, Test Loss: 0.0014, Train L1 Norm: 0.4002, Test L1 Norm: 0.1183, Train Linf Norm: 170.9498, Test Linf Norm: 26.6463\n",
            "Epoch 124: Train Loss: 0.0096, Test Loss: 0.0052, Train L1 Norm: 0.4346, Test L1 Norm: 0.1235, Train Linf Norm: 178.6876, Test Linf Norm: 27.0013\n",
            "Epoch 125: Train Loss: 0.0044, Test Loss: 0.0030, Train L1 Norm: 0.4621, Test L1 Norm: 0.1389, Train Linf Norm: 194.0140, Test Linf Norm: 30.8084\n",
            "Epoch 126: Train Loss: 0.0182, Test Loss: 0.0052, Train L1 Norm: 0.4982, Test L1 Norm: 0.1433, Train Linf Norm: 210.1233, Test Linf Norm: 31.6414\n",
            "Epoch 127: Train Loss: 0.0063, Test Loss: 0.0060, Train L1 Norm: 0.5309, Test L1 Norm: 0.1328, Train Linf Norm: 229.5016, Test Linf Norm: 28.5492\n",
            "Epoch 128: Train Loss: 0.0020, Test Loss: 0.0006, Train L1 Norm: 0.4798, Test L1 Norm: 0.1281, Train Linf Norm: 204.4391, Test Linf Norm: 29.9391\n",
            "Epoch 129: Train Loss: 0.0036, Test Loss: 0.0505, Train L1 Norm: 0.4351, Test L1 Norm: 0.2053, Train Linf Norm: 184.9740, Test Linf Norm: 36.8109\n",
            "Epoch 130: Train Loss: 0.0087, Test Loss: 0.0114, Train L1 Norm: 0.4839, Test L1 Norm: 0.1477, Train Linf Norm: 203.1105, Test Linf Norm: 31.6647\n",
            "Epoch 131: Train Loss: 0.0061, Test Loss: 0.0016, Train L1 Norm: 0.5031, Test L1 Norm: 0.1367, Train Linf Norm: 212.9356, Test Linf Norm: 31.1627\n",
            "Epoch 132: Train Loss: 0.0016, Test Loss: 0.0024, Train L1 Norm: 0.4452, Test L1 Norm: 0.1156, Train Linf Norm: 186.9964, Test Linf Norm: 26.2154\n",
            "Epoch 133: Train Loss: 0.0014, Test Loss: 0.0026, Train L1 Norm: 0.4380, Test L1 Norm: 0.1190, Train Linf Norm: 190.8713, Test Linf Norm: 26.7248\n",
            "Epoch 134: Train Loss: 0.0014, Test Loss: 0.0005, Train L1 Norm: 0.4171, Test L1 Norm: 0.1181, Train Linf Norm: 176.7802, Test Linf Norm: 27.6043\n",
            "Epoch 135: Train Loss: 0.0010, Test Loss: 0.0006, Train L1 Norm: 0.4145, Test L1 Norm: 0.1147, Train Linf Norm: 165.8994, Test Linf Norm: 26.6056\n",
            "Epoch 136: Train Loss: 0.0011, Test Loss: 0.0013, Train L1 Norm: 0.3997, Test L1 Norm: 0.1156, Train Linf Norm: 173.1995, Test Linf Norm: 26.4912\n",
            "Epoch 137: Train Loss: 0.0008, Test Loss: 0.0005, Train L1 Norm: 0.4029, Test L1 Norm: 0.1085, Train Linf Norm: 174.1181, Test Linf Norm: 25.3015\n",
            "Epoch 138: Train Loss: 0.0004, Test Loss: 0.0005, Train L1 Norm: 0.3862, Test L1 Norm: 0.1109, Train Linf Norm: 163.3503, Test Linf Norm: 25.7886\n",
            "Epoch 139: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.3839, Test L1 Norm: 0.1099, Train Linf Norm: 136.3355, Test Linf Norm: 25.5713\n",
            "Epoch 140: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.3770, Test L1 Norm: 0.1062, Train Linf Norm: 164.1308, Test Linf Norm: 24.7575\n",
            "Epoch 141: Train Loss: 0.0004, Test Loss: 0.0005, Train L1 Norm: 0.3757, Test L1 Norm: 0.1080, Train Linf Norm: 163.0941, Test Linf Norm: 25.1253\n",
            "Epoch 142: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.3746, Test L1 Norm: 0.1066, Train Linf Norm: 161.9035, Test Linf Norm: 24.8743\n",
            "Epoch 143: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.3706, Test L1 Norm: 0.1062, Train Linf Norm: 158.2877, Test Linf Norm: 24.7634\n",
            "Epoch 144: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.3700, Test L1 Norm: 0.1057, Train Linf Norm: 158.7443, Test Linf Norm: 24.6944\n",
            "Epoch 145: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.3695, Test L1 Norm: 0.1049, Train Linf Norm: 157.1895, Test Linf Norm: 24.4907\n",
            "Epoch 146: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.3682, Test L1 Norm: 0.1055, Train Linf Norm: 158.8443, Test Linf Norm: 24.6122\n",
            "Epoch 147: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.3687, Test L1 Norm: 0.1051, Train Linf Norm: 157.5904, Test Linf Norm: 24.5326\n",
            "Epoch 148: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.3686, Test L1 Norm: 0.1052, Train Linf Norm: 155.8155, Test Linf Norm: 24.5598\n",
            "Epoch 149: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.3679, Test L1 Norm: 0.1049, Train Linf Norm: 160.6314, Test Linf Norm: 24.4841\n",
            "Epoch 150: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.3671, Test L1 Norm: 0.1048, Train Linf Norm: 157.5450, Test Linf Norm: 24.4469\n",
            "Epoch 151: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.3667, Test L1 Norm: 0.1047, Train Linf Norm: 159.0216, Test Linf Norm: 24.4318\n",
            "Epoch 152: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.3661, Test L1 Norm: 0.1041, Train Linf Norm: 157.5108, Test Linf Norm: 24.2541\n",
            "Epoch 153: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.3632, Test L1 Norm: 0.1040, Train Linf Norm: 156.8937, Test Linf Norm: 24.2361\n",
            "Epoch 154: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.3628, Test L1 Norm: 0.1029, Train Linf Norm: 158.1118, Test Linf Norm: 23.9724\n",
            "Epoch 155: Train Loss: 0.0003, Test Loss: 0.0004, Train L1 Norm: 0.3624, Test L1 Norm: 0.1038, Train Linf Norm: 151.0002, Test Linf Norm: 24.1677\n",
            "Epoch 156: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.3594, Test L1 Norm: 0.1009, Train Linf Norm: 154.3291, Test Linf Norm: 23.5401\n",
            "Epoch 157: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.3546, Test L1 Norm: 0.1022, Train Linf Norm: 152.0288, Test Linf Norm: 23.8234\n",
            "Epoch 158: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.3560, Test L1 Norm: 0.0996, Train Linf Norm: 150.0697, Test Linf Norm: 23.1488\n",
            "Epoch 159: Train Loss: 0.0004, Test Loss: 0.0008, Train L1 Norm: 0.3468, Test L1 Norm: 0.1021, Train Linf Norm: 149.5057, Test Linf Norm: 23.4213\n",
            "Epoch 160: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.3426, Test L1 Norm: 0.0982, Train Linf Norm: 148.3379, Test Linf Norm: 22.8380\n",
            "Epoch 161: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.3433, Test L1 Norm: 0.0983, Train Linf Norm: 137.4658, Test Linf Norm: 22.8367\n",
            "Epoch 162: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.3365, Test L1 Norm: 0.0975, Train Linf Norm: 145.7269, Test Linf Norm: 22.5256\n",
            "Epoch 163: Train Loss: 0.0006, Test Loss: 0.0004, Train L1 Norm: 0.3365, Test L1 Norm: 0.0942, Train Linf Norm: 141.5444, Test Linf Norm: 21.8482\n",
            "Epoch 164: Train Loss: 0.0008, Test Loss: 0.0018, Train L1 Norm: 0.3352, Test L1 Norm: 0.0992, Train Linf Norm: 141.3585, Test Linf Norm: 22.5268\n",
            "Epoch 165: Train Loss: 0.0081, Test Loss: 0.0035, Train L1 Norm: 0.3474, Test L1 Norm: 0.1043, Train Linf Norm: 142.6296, Test Linf Norm: 22.9624\n",
            "Epoch 166: Train Loss: 0.0016, Test Loss: 0.0032, Train L1 Norm: 0.3686, Test L1 Norm: 0.1017, Train Linf Norm: 158.5161, Test Linf Norm: 22.4538\n",
            "Epoch 167: Train Loss: 0.0035, Test Loss: 0.0119, Train L1 Norm: 0.3471, Test L1 Norm: 0.1301, Train Linf Norm: 147.6821, Test Linf Norm: 26.2577\n",
            "Epoch 168: Train Loss: 0.0020, Test Loss: 0.0003, Train L1 Norm: 0.3664, Test L1 Norm: 0.0989, Train Linf Norm: 155.9600, Test Linf Norm: 23.1618\n",
            "Epoch 169: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.3417, Test L1 Norm: 0.0946, Train Linf Norm: 147.3773, Test Linf Norm: 22.0161\n",
            "Epoch 170: Train Loss: 0.0026, Test Loss: 0.0009, Train L1 Norm: 0.3427, Test L1 Norm: 0.0932, Train Linf Norm: 144.9044, Test Linf Norm: 21.4309\n",
            "Epoch 171: Train Loss: 0.0006, Test Loss: 0.0004, Train L1 Norm: 0.3345, Test L1 Norm: 0.0910, Train Linf Norm: 141.8742, Test Linf Norm: 21.1670\n",
            "Epoch 172: Train Loss: 0.0011, Test Loss: 0.0007, Train L1 Norm: 0.3281, Test L1 Norm: 0.0937, Train Linf Norm: 142.1864, Test Linf Norm: 21.6824\n",
            "Epoch 173: Train Loss: 0.0016, Test Loss: 0.0007, Train L1 Norm: 0.3326, Test L1 Norm: 0.0917, Train Linf Norm: 142.2082, Test Linf Norm: 21.0918\n",
            "Epoch 174: Train Loss: 0.0106, Test Loss: 0.0481, Train L1 Norm: 0.3434, Test L1 Norm: 0.1484, Train Linf Norm: 144.1542, Test Linf Norm: 27.2141\n",
            "Epoch 175: Train Loss: 0.0046, Test Loss: 0.0006, Train L1 Norm: 0.4007, Test L1 Norm: 0.1073, Train Linf Norm: 171.1612, Test Linf Norm: 25.1480\n",
            "Epoch 176: Train Loss: 0.0009, Test Loss: 0.0019, Train L1 Norm: 0.3706, Test L1 Norm: 0.1076, Train Linf Norm: 159.5717, Test Linf Norm: 24.0831\n",
            "Epoch 177: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.3519, Test L1 Norm: 0.0945, Train Linf Norm: 148.7673, Test Linf Norm: 22.2679\n",
            "Epoch 178: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.3409, Test L1 Norm: 0.0919, Train Linf Norm: 147.5986, Test Linf Norm: 21.6449\n",
            "Epoch 179: Train Loss: 0.0008, Test Loss: 0.0003, Train L1 Norm: 0.3347, Test L1 Norm: 0.0934, Train Linf Norm: 141.0763, Test Linf Norm: 22.0256\n",
            "Epoch 180: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.3259, Test L1 Norm: 0.0915, Train Linf Norm: 140.5021, Test Linf Norm: 21.5134\n",
            "Epoch 181: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.3195, Test L1 Norm: 0.0914, Train Linf Norm: 137.6157, Test Linf Norm: 21.4056\n",
            "Epoch 182: Train Loss: 0.0003, Test Loss: 0.0007, Train L1 Norm: 0.3162, Test L1 Norm: 0.0900, Train Linf Norm: 136.8702, Test Linf Norm: 20.8557\n",
            "Epoch 183: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.3178, Test L1 Norm: 0.0879, Train Linf Norm: 129.3651, Test Linf Norm: 20.5738\n",
            "Epoch 184: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.3121, Test L1 Norm: 0.0879, Train Linf Norm: 133.0095, Test Linf Norm: 20.6235\n",
            "Epoch 185: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.3115, Test L1 Norm: 0.0885, Train Linf Norm: 109.4993, Test Linf Norm: 20.7966\n",
            "Epoch 186: Train Loss: 0.0002, Test Loss: 0.0003, Train L1 Norm: 0.3101, Test L1 Norm: 0.0877, Train Linf Norm: 135.0925, Test Linf Norm: 20.5649\n",
            "Epoch 187: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.3092, Test L1 Norm: 0.0879, Train Linf Norm: 131.6293, Test Linf Norm: 20.6465\n",
            "Epoch 188: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.3097, Test L1 Norm: 0.0879, Train Linf Norm: 132.1179, Test Linf Norm: 20.6333\n",
            "Epoch 189: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.3094, Test L1 Norm: 0.0878, Train Linf Norm: 130.1245, Test Linf Norm: 20.6271\n",
            "Epoch 190: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.3083, Test L1 Norm: 0.0878, Train Linf Norm: 132.4219, Test Linf Norm: 20.6164\n",
            "Epoch 191: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.3084, Test L1 Norm: 0.0874, Train Linf Norm: 132.4830, Test Linf Norm: 20.5253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 16:03:13,652]\u001b[0m Trial 1 finished with value: 0.0873126779794693 and parameters: {'n_layers': 9, 'n_units_0': 868, 'n_units_1': 183, 'n_units_2': 1116, 'n_units_3': 440, 'n_units_4': 2019, 'n_units_5': 259, 'n_units_6': 44, 'n_units_7': 870, 'n_units_8': 893, 'hidden_activation': 'LeakyReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'SGD', 'lr': 0.0015545778537003898, 'batch_size': 512, 'n_epochs': 192, 'scheduler': 'CosineAnnealingLR', 'weight_decay': 0.00010023154601705513, 'momentum': 0.9388325263524259, 't_max_fraction': 0.11323864214050543, 'eta_min': 4.061700627143592e-05}. Best is trial 1 with value: 0.0873126779794693.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 192: Train Loss: 0.0002, Test Loss: 0.0003, Train L1 Norm: 0.3084, Test L1 Norm: 0.0873, Train Linf Norm: 133.8619, Test Linf Norm: 20.5010\n",
            "Epoch 1: Train Loss: 199279.7471, Test Loss: 0.2452, Train L1 Norm: 12.9333, Test L1 Norm: 0.2875, Train Linf Norm: 759.5097, Test Linf Norm: 25.1433\n",
            "Epoch 2: Train Loss: 0.7493, Test Loss: 0.1080, Train L1 Norm: 0.6123, Test L1 Norm: 0.1036, Train Linf Norm: 91.4369, Test Linf Norm: 3.6470\n",
            "Epoch 3: Train Loss: 0.5299, Test Loss: 0.2777, Train L1 Norm: 0.5900, Test L1 Norm: 0.2400, Train Linf Norm: 95.0286, Test Linf Norm: 13.9383\n",
            "Epoch 4: Train Loss: 0.4122, Test Loss: 0.1490, Train L1 Norm: 0.5782, Test L1 Norm: 0.2185, Train Linf Norm: 101.6856, Test Linf Norm: 15.5384\n",
            "Epoch 5: Train Loss: 0.4474, Test Loss: 0.4498, Train L1 Norm: 0.7500, Test L1 Norm: 0.1652, Train Linf Norm: 143.3419, Test Linf Norm: 5.3255\n",
            "Epoch 6: Train Loss: 0.5648, Test Loss: 0.1700, Train L1 Norm: 0.4753, Test L1 Norm: 0.3597, Train Linf Norm: 71.6664, Test Linf Norm: 31.3986\n",
            "Epoch 7: Train Loss: 0.6390, Test Loss: 0.4355, Train L1 Norm: 0.5413, Test L1 Norm: 0.3384, Train Linf Norm: 83.7726, Test Linf Norm: 22.3815\n",
            "Epoch 8: Train Loss: 0.6397, Test Loss: 0.4721, Train L1 Norm: 0.6449, Test L1 Norm: 0.1894, Train Linf Norm: 111.8428, Test Linf Norm: 6.9135\n",
            "Epoch 9: Train Loss: 4.0840, Test Loss: 0.3480, Train L1 Norm: 0.7574, Test L1 Norm: 0.5818, Train Linf Norm: 100.5269, Test Linf Norm: 61.0480\n",
            "Epoch 10: Train Loss: 0.0635, Test Loss: 0.0281, Train L1 Norm: 1.2971, Test L1 Norm: 0.2039, Train Linf Norm: 279.2387, Test Linf Norm: 22.0989\n",
            "Epoch 11: Train Loss: 0.0227, Test Loss: 0.0111, Train L1 Norm: 0.3612, Test L1 Norm: 0.0789, Train Linf Norm: 71.0221, Test Linf Norm: 6.1075\n",
            "Epoch 12: Train Loss: 0.0389, Test Loss: 0.0088, Train L1 Norm: 0.2244, Test L1 Norm: 0.0602, Train Linf Norm: 40.6108, Test Linf Norm: 4.1998\n",
            "Epoch 13: Train Loss: 0.0314, Test Loss: 0.0071, Train L1 Norm: 0.1812, Test L1 Norm: 0.0527, Train Linf Norm: 31.7088, Test Linf Norm: 4.3119\n",
            "Epoch 14: Train Loss: 0.0308, Test Loss: 0.0244, Train L1 Norm: 0.1708, Test L1 Norm: 0.1148, Train Linf Norm: 30.5199, Test Linf Norm: 9.9199\n",
            "Epoch 15: Train Loss: 0.0339, Test Loss: 0.0127, Train L1 Norm: 0.1744, Test L1 Norm: 0.0509, Train Linf Norm: 31.8973, Test Linf Norm: 3.7672\n",
            "Epoch 16: Train Loss: 0.0255, Test Loss: 0.3072, Train L1 Norm: 0.1872, Test L1 Norm: 0.1717, Train Linf Norm: 35.8325, Test Linf Norm: 1.0079\n",
            "Epoch 17: Train Loss: 0.0267, Test Loss: 0.0109, Train L1 Norm: 0.1553, Test L1 Norm: 0.0424, Train Linf Norm: 27.8129, Test Linf Norm: 2.7987\n",
            "Epoch 18: Train Loss: 0.0240, Test Loss: 0.0538, Train L1 Norm: 0.1412, Test L1 Norm: 0.2458, Train Linf Norm: 25.1837, Test Linf Norm: 19.9340\n",
            "Epoch 19: Train Loss: 0.0239, Test Loss: 0.0032, Train L1 Norm: 0.1139, Test L1 Norm: 0.0343, Train Linf Norm: 18.8185, Test Linf Norm: 2.6440\n",
            "Epoch 20: Train Loss: 0.0218, Test Loss: 0.0121, Train L1 Norm: 0.1341, Test L1 Norm: 0.0635, Train Linf Norm: 24.3207, Test Linf Norm: 1.2243\n",
            "Epoch 21: Train Loss: 0.0228, Test Loss: 0.0033, Train L1 Norm: 0.1507, Test L1 Norm: 0.0329, Train Linf Norm: 27.9327, Test Linf Norm: 2.6333\n",
            "Epoch 22: Train Loss: 0.0195, Test Loss: 0.0057, Train L1 Norm: 0.1202, Test L1 Norm: 0.0319, Train Linf Norm: 21.8867, Test Linf Norm: 1.5117\n",
            "Epoch 23: Train Loss: 0.0198, Test Loss: 0.0270, Train L1 Norm: 0.1283, Test L1 Norm: 0.0476, Train Linf Norm: 23.3728, Test Linf Norm: 1.9609\n",
            "Epoch 24: Train Loss: 0.0187, Test Loss: 0.0024, Train L1 Norm: 0.1393, Test L1 Norm: 0.0383, Train Linf Norm: 25.9533, Test Linf Norm: 3.5366\n",
            "Epoch 25: Train Loss: 0.0196, Test Loss: 0.0023, Train L1 Norm: 0.1215, Test L1 Norm: 0.0285, Train Linf Norm: 14.2567, Test Linf Norm: 2.6491\n",
            "Epoch 26: Train Loss: 0.0159, Test Loss: 0.0146, Train L1 Norm: 0.1218, Test L1 Norm: 0.0705, Train Linf Norm: 22.1858, Test Linf Norm: 5.8673\n",
            "Epoch 27: Train Loss: 0.0161, Test Loss: 0.0159, Train L1 Norm: 0.1205, Test L1 Norm: 0.0379, Train Linf Norm: 22.2300, Test Linf Norm: 1.9370\n",
            "Epoch 28: Train Loss: 0.0156, Test Loss: 0.0364, Train L1 Norm: 0.1213, Test L1 Norm: 0.0506, Train Linf Norm: 22.5801, Test Linf Norm: 1.3765\n",
            "Epoch 29: Train Loss: 0.0141, Test Loss: 0.0011, Train L1 Norm: 0.1176, Test L1 Norm: 0.0227, Train Linf Norm: 21.4587, Test Linf Norm: 1.9129\n",
            "Epoch 30: Train Loss: 0.0146, Test Loss: 0.0010, Train L1 Norm: 0.1217, Test L1 Norm: 0.0320, Train Linf Norm: 22.7872, Test Linf Norm: 3.2756\n",
            "Epoch 31: Train Loss: 0.0143, Test Loss: 0.0075, Train L1 Norm: 0.0997, Test L1 Norm: 0.0346, Train Linf Norm: 17.9791, Test Linf Norm: 1.2765\n",
            "Epoch 32: Train Loss: 0.0141, Test Loss: 0.0138, Train L1 Norm: 0.0874, Test L1 Norm: 0.0534, Train Linf Norm: 13.9211, Test Linf Norm: 3.8786\n",
            "Epoch 33: Train Loss: 0.0137, Test Loss: 0.0397, Train L1 Norm: 0.0607, Test L1 Norm: 0.0544, Train Linf Norm: 7.8573, Test Linf Norm: 1.2958\n",
            "Epoch 34: Train Loss: 0.0136, Test Loss: 0.0038, Train L1 Norm: 0.1068, Test L1 Norm: 0.0265, Train Linf Norm: 19.7382, Test Linf Norm: 1.8482\n",
            "Epoch 35: Train Loss: 0.0120, Test Loss: 0.0009, Train L1 Norm: 0.1208, Test L1 Norm: 0.0206, Train Linf Norm: 22.9106, Test Linf Norm: 1.2688\n",
            "Epoch 36: Train Loss: 0.0127, Test Loss: 0.0014, Train L1 Norm: 0.0834, Test L1 Norm: 0.0421, Train Linf Norm: 13.8300, Test Linf Norm: 4.2596\n",
            "Epoch 37: Train Loss: 0.0130, Test Loss: 0.0010, Train L1 Norm: 0.0838, Test L1 Norm: 0.0274, Train Linf Norm: 13.8343, Test Linf Norm: 2.6968\n",
            "Epoch 38: Train Loss: 0.0116, Test Loss: 0.0012, Train L1 Norm: 0.0859, Test L1 Norm: 0.0293, Train Linf Norm: 14.9521, Test Linf Norm: 2.9275\n",
            "Epoch 39: Train Loss: 0.0118, Test Loss: 0.0012, Train L1 Norm: 0.0699, Test L1 Norm: 0.0246, Train Linf Norm: 10.6425, Test Linf Norm: 2.2686\n",
            "Epoch 40: Train Loss: 0.0117, Test Loss: 0.0037, Train L1 Norm: 0.0849, Test L1 Norm: 0.0272, Train Linf Norm: 14.7488, Test Linf Norm: 2.3638\n",
            "Epoch 41: Train Loss: 0.0119, Test Loss: 0.0194, Train L1 Norm: 0.0909, Test L1 Norm: 0.0463, Train Linf Norm: 15.4986, Test Linf Norm: 2.3522\n",
            "Epoch 42: Train Loss: 0.0108, Test Loss: 0.0009, Train L1 Norm: 0.0837, Test L1 Norm: 0.0350, Train Linf Norm: 14.2029, Test Linf Norm: 1.0122\n",
            "Epoch 43: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0701, Test L1 Norm: 0.0185, Train Linf Norm: 14.8861, Test Linf Norm: 1.7998\n",
            "Epoch 44: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0543, Test L1 Norm: 0.0159, Train Linf Norm: 11.0485, Test Linf Norm: 1.3532\n",
            "Epoch 45: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0539, Test L1 Norm: 0.0156, Train Linf Norm: 10.7976, Test Linf Norm: 1.2106\n",
            "Epoch 46: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0477, Test L1 Norm: 0.0159, Train Linf Norm: 9.3512, Test Linf Norm: 1.3777\n",
            "Epoch 47: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0397, Test L1 Norm: 0.0178, Train Linf Norm: 7.4770, Test Linf Norm: 1.7922\n",
            "Epoch 48: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.0487, Test L1 Norm: 0.0165, Train Linf Norm: 9.8413, Test Linf Norm: 1.2328\n",
            "Epoch 49: Train Loss: 0.0005, Test Loss: 0.0007, Train L1 Norm: 0.0449, Test L1 Norm: 0.0179, Train Linf Norm: 8.8844, Test Linf Norm: 1.6955\n",
            "Epoch 50: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0397, Test L1 Norm: 0.0163, Train Linf Norm: 7.5295, Test Linf Norm: 1.5512\n",
            "Epoch 51: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0396, Test L1 Norm: 0.0153, Train Linf Norm: 7.6028, Test Linf Norm: 1.4419\n",
            "Epoch 52: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0467, Test L1 Norm: 0.0148, Train Linf Norm: 9.3925, Test Linf Norm: 1.3575\n",
            "Epoch 53: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0424, Test L1 Norm: 0.0149, Train Linf Norm: 8.3647, Test Linf Norm: 1.3039\n",
            "Epoch 54: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0424, Test L1 Norm: 0.0149, Train Linf Norm: 8.3150, Test Linf Norm: 0.9294\n",
            "Epoch 55: Train Loss: 0.0004, Test Loss: 0.0006, Train L1 Norm: 0.0482, Test L1 Norm: 0.0162, Train Linf Norm: 9.8601, Test Linf Norm: 1.5288\n",
            "Epoch 56: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0430, Test L1 Norm: 0.0141, Train Linf Norm: 8.5573, Test Linf Norm: 1.2049\n",
            "Epoch 57: Train Loss: 0.0004, Test Loss: 0.0011, Train L1 Norm: 0.0469, Test L1 Norm: 0.0167, Train Linf Norm: 9.5851, Test Linf Norm: 1.4126\n",
            "Epoch 58: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.0414, Test L1 Norm: 0.0135, Train Linf Norm: 8.1062, Test Linf Norm: 1.0985\n",
            "Epoch 59: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.0387, Test L1 Norm: 0.0139, Train Linf Norm: 7.5352, Test Linf Norm: 1.2661\n",
            "Epoch 60: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.0442, Test L1 Norm: 0.0132, Train Linf Norm: 8.9100, Test Linf Norm: 1.0086\n",
            "Epoch 61: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.0353, Test L1 Norm: 0.0149, Train Linf Norm: 6.6997, Test Linf Norm: 1.5047\n",
            "Epoch 62: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.0383, Test L1 Norm: 0.0138, Train Linf Norm: 7.3824, Test Linf Norm: 1.2702\n",
            "Epoch 63: Train Loss: 0.0004, Test Loss: 0.0007, Train L1 Norm: 0.0372, Test L1 Norm: 0.0164, Train Linf Norm: 7.2235, Test Linf Norm: 1.4955\n",
            "Epoch 64: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0358, Test L1 Norm: 0.0137, Train Linf Norm: 6.8098, Test Linf Norm: 1.2243\n",
            "Epoch 65: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.0334, Test L1 Norm: 0.0146, Train Linf Norm: 6.2379, Test Linf Norm: 1.4426\n",
            "Epoch 66: Train Loss: 0.0004, Test Loss: 0.0005, Train L1 Norm: 0.0353, Test L1 Norm: 0.0178, Train Linf Norm: 6.7654, Test Linf Norm: 1.6043\n",
            "Epoch 67: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.0383, Test L1 Norm: 0.0138, Train Linf Norm: 7.5287, Test Linf Norm: 1.2537\n",
            "Epoch 68: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.0382, Test L1 Norm: 0.0131, Train Linf Norm: 7.5168, Test Linf Norm: 1.1999\n",
            "Epoch 69: Train Loss: 0.0004, Test Loss: 0.0011, Train L1 Norm: 0.0408, Test L1 Norm: 0.0158, Train Linf Norm: 8.1692, Test Linf Norm: 1.1874\n",
            "Epoch 70: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0442, Test L1 Norm: 0.0125, Train Linf Norm: 9.0712, Test Linf Norm: 1.0291\n",
            "Epoch 71: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.0403, Test L1 Norm: 0.0135, Train Linf Norm: 8.0851, Test Linf Norm: 1.2901\n",
            "Epoch 72: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0348, Test L1 Norm: 0.0130, Train Linf Norm: 6.6563, Test Linf Norm: 1.1474\n",
            "Epoch 73: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0398, Test L1 Norm: 0.0138, Train Linf Norm: 7.9763, Test Linf Norm: 1.2588\n",
            "Epoch 74: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0442, Test L1 Norm: 0.0127, Train Linf Norm: 9.0884, Test Linf Norm: 1.0999\n",
            "Epoch 75: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0424, Test L1 Norm: 0.0149, Train Linf Norm: 8.6587, Test Linf Norm: 1.4924\n",
            "Epoch 76: Train Loss: 0.0003, Test Loss: 0.0006, Train L1 Norm: 0.0399, Test L1 Norm: 0.0137, Train Linf Norm: 8.0013, Test Linf Norm: 1.1283\n",
            "Epoch 77: Train Loss: 0.0003, Test Loss: 0.0005, Train L1 Norm: 0.0411, Test L1 Norm: 0.0142, Train Linf Norm: 8.3224, Test Linf Norm: 1.2855\n",
            "Epoch 78: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0405, Test L1 Norm: 0.0126, Train Linf Norm: 8.2022, Test Linf Norm: 0.9424\n",
            "Epoch 79: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0385, Test L1 Norm: 0.0140, Train Linf Norm: 7.7153, Test Linf Norm: 1.3670\n",
            "Epoch 80: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0399, Test L1 Norm: 0.0127, Train Linf Norm: 8.0792, Test Linf Norm: 0.8027\n",
            "Epoch 81: Train Loss: 0.0003, Test Loss: 0.0009, Train L1 Norm: 0.0360, Test L1 Norm: 0.0173, Train Linf Norm: 7.0866, Test Linf Norm: 1.5356\n",
            "Epoch 82: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0415, Test L1 Norm: 0.0118, Train Linf Norm: 8.5026, Test Linf Norm: 0.8290\n",
            "Epoch 83: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0373, Test L1 Norm: 0.0123, Train Linf Norm: 7.4596, Test Linf Norm: 1.1063\n",
            "Epoch 84: Train Loss: 0.0003, Test Loss: 0.0006, Train L1 Norm: 0.0349, Test L1 Norm: 0.0134, Train Linf Norm: 6.8452, Test Linf Norm: 1.0419\n",
            "Epoch 85: Train Loss: 0.0003, Test Loss: 0.0005, Train L1 Norm: 0.0374, Test L1 Norm: 0.0180, Train Linf Norm: 7.4864, Test Linf Norm: 1.0043\n",
            "Epoch 86: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0385, Test L1 Norm: 0.0133, Train Linf Norm: 7.7574, Test Linf Norm: 1.2668\n",
            "Epoch 87: Train Loss: 0.0003, Test Loss: 0.0008, Train L1 Norm: 0.0389, Test L1 Norm: 0.0141, Train Linf Norm: 7.8717, Test Linf Norm: 0.8490\n",
            "Epoch 88: Train Loss: 0.0003, Test Loss: 0.0005, Train L1 Norm: 0.0338, Test L1 Norm: 0.0131, Train Linf Norm: 6.5547, Test Linf Norm: 1.1286\n",
            "Epoch 89: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0310, Test L1 Norm: 0.0122, Train Linf Norm: 5.9137, Test Linf Norm: 0.9999\n",
            "Epoch 90: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0377, Test L1 Norm: 0.0120, Train Linf Norm: 7.8029, Test Linf Norm: 1.0878\n",
            "Epoch 91: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0359, Test L1 Norm: 0.0115, Train Linf Norm: 7.3404, Test Linf Norm: 0.9746\n",
            "Epoch 92: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0360, Test L1 Norm: 0.0118, Train Linf Norm: 7.3484, Test Linf Norm: 1.0540\n",
            "Epoch 93: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0357, Test L1 Norm: 0.0115, Train Linf Norm: 7.2843, Test Linf Norm: 0.9794\n",
            "Epoch 94: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0365, Test L1 Norm: 0.0113, Train Linf Norm: 7.5043, Test Linf Norm: 0.9601\n",
            "Epoch 95: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0116, Train Linf Norm: 7.2286, Test Linf Norm: 1.0365\n",
            "Epoch 96: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0350, Test L1 Norm: 0.0116, Train Linf Norm: 7.1113, Test Linf Norm: 0.9853\n",
            "Epoch 97: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0349, Test L1 Norm: 0.0119, Train Linf Norm: 7.0583, Test Linf Norm: 1.0924\n",
            "Epoch 98: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0347, Test L1 Norm: 0.0116, Train Linf Norm: 7.0572, Test Linf Norm: 1.0280\n",
            "Epoch 99: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0348, Test L1 Norm: 0.0117, Train Linf Norm: 7.0275, Test Linf Norm: 1.0478\n",
            "Epoch 100: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0348, Test L1 Norm: 0.0113, Train Linf Norm: 7.0287, Test Linf Norm: 0.9494\n",
            "Epoch 101: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0348, Test L1 Norm: 0.0118, Train Linf Norm: 7.0416, Test Linf Norm: 1.0691\n",
            "Epoch 102: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0116, Train Linf Norm: 7.1733, Test Linf Norm: 1.0337\n",
            "Epoch 103: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0347, Test L1 Norm: 0.0117, Train Linf Norm: 7.0454, Test Linf Norm: 1.0599\n",
            "Epoch 104: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0341, Test L1 Norm: 0.0118, Train Linf Norm: 6.8811, Test Linf Norm: 1.0724\n",
            "Epoch 105: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0352, Test L1 Norm: 0.0115, Train Linf Norm: 7.0695, Test Linf Norm: 1.0132\n",
            "Epoch 106: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0360, Test L1 Norm: 0.0113, Train Linf Norm: 7.3877, Test Linf Norm: 0.9418\n",
            "Epoch 107: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0113, Train Linf Norm: 7.2614, Test Linf Norm: 0.9364\n",
            "Epoch 108: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0357, Test L1 Norm: 0.0118, Train Linf Norm: 7.3210, Test Linf Norm: 1.0649\n",
            "Epoch 109: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0349, Test L1 Norm: 0.0115, Train Linf Norm: 7.1006, Test Linf Norm: 1.0069\n",
            "Epoch 110: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0352, Test L1 Norm: 0.0113, Train Linf Norm: 7.2016, Test Linf Norm: 0.9802\n",
            "Epoch 111: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0349, Test L1 Norm: 0.0116, Train Linf Norm: 7.0719, Test Linf Norm: 1.0302\n",
            "Epoch 112: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0115, Train Linf Norm: 7.2276, Test Linf Norm: 1.0172\n",
            "Epoch 113: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0115, Train Linf Norm: 7.2093, Test Linf Norm: 1.0224\n",
            "Epoch 114: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0358, Test L1 Norm: 0.0115, Train Linf Norm: 7.3428, Test Linf Norm: 1.0262\n",
            "Epoch 115: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0356, Test L1 Norm: 0.0115, Train Linf Norm: 7.3029, Test Linf Norm: 1.0088\n",
            "Epoch 116: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0355, Test L1 Norm: 0.0115, Train Linf Norm: 7.2477, Test Linf Norm: 1.0055\n",
            "Epoch 117: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0350, Test L1 Norm: 0.0115, Train Linf Norm: 7.1565, Test Linf Norm: 1.0067\n",
            "Epoch 118: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0115, Train Linf Norm: 7.2339, Test Linf Norm: 1.0069\n",
            "Epoch 119: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0115, Train Linf Norm: 7.2274, Test Linf Norm: 1.0091\n",
            "Epoch 120: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0115, Train Linf Norm: 7.2315, Test Linf Norm: 1.0081\n",
            "Epoch 121: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0115, Train Linf Norm: 7.2510, Test Linf Norm: 1.0079\n",
            "Epoch 122: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0115, Train Linf Norm: 7.2368, Test Linf Norm: 1.0073\n",
            "Epoch 123: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0115, Train Linf Norm: 7.2147, Test Linf Norm: 1.0069\n",
            "Epoch 124: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0115, Train Linf Norm: 7.2469, Test Linf Norm: 1.0074\n",
            "Epoch 125: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0115, Train Linf Norm: 7.2255, Test Linf Norm: 1.0070\n",
            "Epoch 126: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0115, Train Linf Norm: 7.2034, Test Linf Norm: 1.0067\n",
            "Epoch 127: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0115, Train Linf Norm: 7.2314, Test Linf Norm: 1.0077\n",
            "Epoch 128: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0115, Train Linf Norm: 7.2424, Test Linf Norm: 1.0070\n",
            "Epoch 129: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0114, Train Linf Norm: 7.2589, Test Linf Norm: 1.0057\n",
            "Epoch 130: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0114, Train Linf Norm: 7.1897, Test Linf Norm: 1.0046\n",
            "Epoch 131: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0115, Train Linf Norm: 7.2151, Test Linf Norm: 1.0064\n",
            "Epoch 132: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0115, Train Linf Norm: 7.2043, Test Linf Norm: 1.0067\n",
            "Epoch 133: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0115, Train Linf Norm: 7.1069, Test Linf Norm: 1.0065\n",
            "Epoch 134: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0114, Train Linf Norm: 7.1724, Test Linf Norm: 1.0062\n",
            "Epoch 135: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0114, Train Linf Norm: 7.2277, Test Linf Norm: 1.0057\n",
            "Epoch 136: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0114, Train Linf Norm: 7.2335, Test Linf Norm: 1.0061\n",
            "Epoch 137: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0115, Train Linf Norm: 7.1721, Test Linf Norm: 1.0067\n",
            "Epoch 138: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0115, Train Linf Norm: 7.2114, Test Linf Norm: 1.0068\n",
            "Epoch 139: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0114, Train Linf Norm: 7.2445, Test Linf Norm: 1.0059\n",
            "Epoch 140: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0115, Train Linf Norm: 7.2046, Test Linf Norm: 1.0072\n",
            "Epoch 141: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0114, Train Linf Norm: 7.2459, Test Linf Norm: 1.0055\n",
            "Epoch 142: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0114, Train Linf Norm: 7.1970, Test Linf Norm: 1.0056\n",
            "Epoch 143: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0114, Train Linf Norm: 7.2190, Test Linf Norm: 1.0064\n",
            "Epoch 144: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0114, Train Linf Norm: 7.2032, Test Linf Norm: 1.0057\n",
            "Epoch 145: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0114, Train Linf Norm: 7.1947, Test Linf Norm: 1.0061\n",
            "Epoch 146: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0115, Train Linf Norm: 7.2560, Test Linf Norm: 1.0069\n",
            "Epoch 147: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0114, Train Linf Norm: 7.2383, Test Linf Norm: 1.0053\n",
            "Epoch 148: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0114, Train Linf Norm: 7.2170, Test Linf Norm: 1.0061\n",
            "Epoch 149: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0115, Train Linf Norm: 7.2433, Test Linf Norm: 1.0068\n",
            "Epoch 150: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0114, Train Linf Norm: 7.2586, Test Linf Norm: 1.0059\n",
            "Epoch 151: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0114, Train Linf Norm: 7.2543, Test Linf Norm: 1.0058\n",
            "Epoch 152: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0114, Train Linf Norm: 7.2567, Test Linf Norm: 1.0046\n",
            "Epoch 153: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0114, Train Linf Norm: 7.2076, Test Linf Norm: 1.0063\n",
            "Epoch 154: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0114, Train Linf Norm: 7.2548, Test Linf Norm: 1.0061\n",
            "Epoch 155: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0115, Train Linf Norm: 7.2416, Test Linf Norm: 1.0068\n",
            "Epoch 156: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0115, Train Linf Norm: 7.2412, Test Linf Norm: 1.0066\n",
            "Epoch 157: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0354, Test L1 Norm: 0.0114, Train Linf Norm: 7.2272, Test Linf Norm: 1.0060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 16:12:25,453]\u001b[0m Trial 2 finished with value: 0.011449855588376521 and parameters: {'n_layers': 9, 'n_units_0': 1562, 'n_units_1': 1078, 'n_units_2': 1031, 'n_units_3': 1570, 'n_units_4': 1085, 'n_units_5': 1753, 'n_units_6': 1226, 'n_units_7': 1364, 'n_units_8': 1272, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'MSE', 'optimizer': 'RMSprop', 'lr': 0.0003968119604873588, 'batch_size': 256, 'n_epochs': 158, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.10654227739487464, 'patience': 6, 'threshold': 0.00984371249334998}. Best is trial 2 with value: 0.011449855588376521.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 158: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0114, Train Linf Norm: 7.2131, Test Linf Norm: 1.0064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-82e2252adf92>:139: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  gamma = trial.suggest_uniform(\"gamma\", 0.1, 0.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 9918803.9897, Test Loss: 254.8603, Train L1 Norm: 9707141.3691, Test L1 Norm: 307.7779, Train Linf Norm: 149266918.1181, Test Linf Norm: 10183.8477\n",
            "Epoch 2: Train Loss: 147.2676, Test Loss: 3.8821, Train L1 Norm: 265.9789, Test L1 Norm: 2.4554, Train Linf Norm: 8815.3074, Test Linf Norm: 61.4699\n",
            "Epoch 3: Train Loss: 24352.1822, Test Loss: 1.3177, Train L1 Norm: 73593.6536, Test L1 Norm: 5.1823, Train Linf Norm: 3080396.2534, Test Linf Norm: 213.7514\n",
            "Epoch 4: Train Loss: 5543948.3002, Test Loss: 30.8092, Train L1 Norm: 2688346.6233, Test L1 Norm: 115.7582, Train Linf Norm: 62082676.5806, Test Linf Norm: 5451.5659\n",
            "Epoch 5: Train Loss: 264403.0649, Test Loss: 417.8629, Train L1 Norm: 154546.6984, Test L1 Norm: 468.6315, Train Linf Norm: 2132289.7804, Test Linf Norm: 13422.2910\n",
            "Epoch 6: Train Loss: 64826.4810, Test Loss: 96.0094, Train L1 Norm: 87289.8817, Test L1 Norm: 355.3755, Train Linf Norm: 1751675.4690, Test Linf Norm: 19051.5352\n",
            "Epoch 7: Train Loss: 123261.8576, Test Loss: 2151.2351, Train L1 Norm: 38004.1892, Test L1 Norm: 1926.0259, Train Linf Norm: 434626.6652, Test Linf Norm: 51155.0316\n",
            "Epoch 8: Train Loss: 14440.3555, Test Loss: 82.8497, Train L1 Norm: 10860.1182, Test L1 Norm: 61.2931, Train Linf Norm: 268542.5892, Test Linf Norm: 1629.6612\n",
            "Epoch 9: Train Loss: 153883.9649, Test Loss: 85.8136, Train L1 Norm: 94889.3805, Test L1 Norm: 86.9037, Train Linf Norm: 978713.7038, Test Linf Norm: 2736.0969\n",
            "Epoch 10: Train Loss: 1836002.9832, Test Loss: 3019.3088, Train L1 Norm: 18244091.1679, Test L1 Norm: 2292.7567, Train Linf Norm: 1153302298.5447, Test Linf Norm: 53966.2597\n",
            "Epoch 11: Train Loss: 2284180.3063, Test Loss: 28.1021, Train L1 Norm: 1646693.9306, Test L1 Norm: 88.6660, Train Linf Norm: 22761011.0782, Test Linf Norm: 4389.9922\n",
            "Epoch 12: Train Loss: 99568271.0173, Test Loss: 1773.3117, Train L1 Norm: 37588294.7258, Test L1 Norm: 1782.5975, Train Linf Norm: 422321985.2703, Test Linf Norm: 67858.0247\n",
            "Epoch 13: Train Loss: 210664.9096, Test Loss: 1658.2765, Train L1 Norm: 21321271.0902, Test L1 Norm: 3351.1555, Train Linf Norm: 1363565567.3005, Test Linf Norm: 155867.9339\n",
            "Epoch 14: Train Loss: 241469.7731, Test Loss: 1000.0209, Train L1 Norm: 112348.1702, Test L1 Norm: 933.7697, Train Linf Norm: 2636178.0295, Test Linf Norm: 25196.6332\n",
            "Epoch 15: Train Loss: 266.4870, Test Loss: 97.4854, Train L1 Norm: 1041.7725, Test L1 Norm: 177.4622, Train Linf Norm: 51937.9507, Test Linf Norm: 6184.6820\n",
            "Epoch 16: Train Loss: 492.7187, Test Loss: 61.0921, Train L1 Norm: 1326.3440, Test L1 Norm: 79.4372, Train Linf Norm: 74151.4422, Test Linf Norm: 2480.6622\n",
            "Epoch 17: Train Loss: 80.5344, Test Loss: 18.3444, Train L1 Norm: 214.5306, Test L1 Norm: 17.4211, Train Linf Norm: 10254.7620, Test Linf Norm: 416.0429\n",
            "Epoch 18: Train Loss: 104.3694, Test Loss: 61.0464, Train L1 Norm: 109.8491, Test L1 Norm: 59.2607, Train Linf Norm: 4391.1203, Test Linf Norm: 1524.1391\n",
            "Epoch 19: Train Loss: 1973.5714, Test Loss: 99.0635, Train L1 Norm: 2971.1020, Test L1 Norm: 116.4852, Train Linf Norm: 42646.2327, Test Linf Norm: 4110.9943\n",
            "Epoch 20: Train Loss: 71.4822, Test Loss: 39.3170, Train L1 Norm: 141.7847, Test L1 Norm: 36.5724, Train Linf Norm: 6582.7343, Test Linf Norm: 1038.6516\n",
            "Epoch 21: Train Loss: 311.3173, Test Loss: 35.8470, Train L1 Norm: 181.9764, Test L1 Norm: 36.2429, Train Linf Norm: 8956.4061, Test Linf Norm: 1097.0142\n",
            "Epoch 22: Train Loss: 389.3136, Test Loss: 11.5994, Train L1 Norm: 318.3652, Test L1 Norm: 9.1799, Train Linf Norm: 9165.8140, Test Linf Norm: 238.5876\n",
            "Epoch 23: Train Loss: 761448.2857, Test Loss: 43.6339, Train L1 Norm: 365879.7953, Test L1 Norm: 65.0074, Train Linf Norm: 3763809.5768, Test Linf Norm: 2519.5020\n",
            "Epoch 24: Train Loss: 30.9023, Test Loss: 11.3570, Train L1 Norm: 39.5662, Test L1 Norm: 13.3633, Train Linf Norm: 1445.6005, Test Linf Norm: 360.4496\n",
            "Epoch 25: Train Loss: 40.9169, Test Loss: 7.0618, Train L1 Norm: 96.5847, Test L1 Norm: 9.0804, Train Linf Norm: 4685.9369, Test Linf Norm: 251.4724\n",
            "Epoch 26: Train Loss: 254466.9642, Test Loss: 4.2420, Train L1 Norm: 118336.1190, Test L1 Norm: 9.6481, Train Linf Norm: 2139480.4312, Test Linf Norm: 415.9115\n",
            "Epoch 27: Train Loss: 283.9549, Test Loss: 20.3815, Train L1 Norm: 204.0851, Test L1 Norm: 14.8659, Train Linf Norm: 5182.1644, Test Linf Norm: 367.7159\n",
            "Epoch 28: Train Loss: 57.2268, Test Loss: 24.9539, Train L1 Norm: 68.2052, Test L1 Norm: 24.3515, Train Linf Norm: 2818.6025, Test Linf Norm: 674.8552\n",
            "Epoch 29: Train Loss: 2.4647, Test Loss: 2.2263, Train L1 Norm: 8.5705, Test L1 Norm: 4.0386, Train Linf Norm: 440.6020, Test Linf Norm: 129.5511\n",
            "Epoch 30: Train Loss: 1.1031, Test Loss: 0.7025, Train L1 Norm: 4.3162, Test L1 Norm: 0.6810, Train Linf Norm: 219.2792, Test Linf Norm: 17.8266\n",
            "Epoch 31: Train Loss: 0.6330, Test Loss: 0.4619, Train L1 Norm: 3.1057, Test L1 Norm: 0.6527, Train Linf Norm: 161.2961, Test Linf Norm: 20.0761\n",
            "Epoch 32: Train Loss: 0.4686, Test Loss: 0.4008, Train L1 Norm: 4.3337, Test L1 Norm: 1.1946, Train Linf Norm: 245.9209, Test Linf Norm: 38.8334\n",
            "Epoch 33: Train Loss: 0.3624, Test Loss: 0.2362, Train L1 Norm: 3.0250, Test L1 Norm: 0.8205, Train Linf Norm: 167.2924, Test Linf Norm: 23.6146\n",
            "Epoch 34: Train Loss: 0.2923, Test Loss: 0.5373, Train L1 Norm: 1.7415, Test L1 Norm: 0.9729, Train Linf Norm: 88.9934, Test Linf Norm: 27.3263\n",
            "Epoch 35: Train Loss: 0.2724, Test Loss: 0.2972, Train L1 Norm: 3.2112, Test L1 Norm: 0.6172, Train Linf Norm: 184.8114, Test Linf Norm: 19.6505\n",
            "Epoch 36: Train Loss: 0.2076, Test Loss: 0.0664, Train L1 Norm: 2.1797, Test L1 Norm: 0.3710, Train Linf Norm: 121.5438, Test Linf Norm: 13.0504\n",
            "Epoch 37: Train Loss: 0.1886, Test Loss: 0.3025, Train L1 Norm: 1.8776, Test L1 Norm: 0.6577, Train Linf Norm: 101.0694, Test Linf Norm: 20.4710\n",
            "Epoch 38: Train Loss: 0.2010, Test Loss: 0.0838, Train L1 Norm: 1.9109, Test L1 Norm: 0.6757, Train Linf Norm: 105.6970, Test Linf Norm: 24.9892\n",
            "Epoch 39: Train Loss: 0.1399, Test Loss: 0.0684, Train L1 Norm: 1.6185, Test L1 Norm: 0.3115, Train Linf Norm: 88.2909, Test Linf Norm: 8.6196\n",
            "Epoch 40: Train Loss: 0.2389, Test Loss: 0.0777, Train L1 Norm: 2.0130, Test L1 Norm: 0.3268, Train Linf Norm: 106.6448, Test Linf Norm: 9.9210\n",
            "Epoch 41: Train Loss: 0.2157, Test Loss: 0.0254, Train L1 Norm: 2.2444, Test L1 Norm: 0.2770, Train Linf Norm: 116.3954, Test Linf Norm: 9.9928\n",
            "Epoch 42: Train Loss: 0.1310, Test Loss: 0.0820, Train L1 Norm: 1.0952, Test L1 Norm: 0.3516, Train Linf Norm: 56.8476, Test Linf Norm: 11.9023\n",
            "Epoch 43: Train Loss: 0.0097, Test Loss: 0.0077, Train L1 Norm: 0.6570, Test L1 Norm: 0.1583, Train Linf Norm: 36.5068, Test Linf Norm: 5.5532\n",
            "Epoch 44: Train Loss: 0.0074, Test Loss: 0.0060, Train L1 Norm: 0.4933, Test L1 Norm: 0.1138, Train Linf Norm: 27.3005, Test Linf Norm: 3.6736\n",
            "Epoch 45: Train Loss: 0.0065, Test Loss: 0.0045, Train L1 Norm: 0.3841, Test L1 Norm: 0.1085, Train Linf Norm: 20.8042, Test Linf Norm: 3.7976\n",
            "Epoch 46: Train Loss: 0.0060, Test Loss: 0.0040, Train L1 Norm: 0.3249, Test L1 Norm: 0.1137, Train Linf Norm: 17.2400, Test Linf Norm: 3.9723\n",
            "Epoch 47: Train Loss: 0.0055, Test Loss: 0.0034, Train L1 Norm: 0.4185, Test L1 Norm: 0.1000, Train Linf Norm: 23.4579, Test Linf Norm: 3.5330\n",
            "Epoch 48: Train Loss: 0.0050, Test Loss: 0.0033, Train L1 Norm: 0.5137, Test L1 Norm: 0.0857, Train Linf Norm: 29.5963, Test Linf Norm: 2.6824\n",
            "Epoch 49: Train Loss: 0.0049, Test Loss: 0.0031, Train L1 Norm: 0.2941, Test L1 Norm: 0.0854, Train Linf Norm: 15.8076, Test Linf Norm: 2.8307\n",
            "Epoch 50: Train Loss: 0.0045, Test Loss: 0.0029, Train L1 Norm: 0.1934, Test L1 Norm: 0.1062, Train Linf Norm: 9.3472, Test Linf Norm: 4.0629\n",
            "Epoch 51: Train Loss: 0.0045, Test Loss: 0.0030, Train L1 Norm: 0.3033, Test L1 Norm: 0.0774, Train Linf Norm: 16.3934, Test Linf Norm: 2.4258\n",
            "Epoch 52: Train Loss: 0.0045, Test Loss: 0.0032, Train L1 Norm: 0.2924, Test L1 Norm: 0.0965, Train Linf Norm: 15.8108, Test Linf Norm: 3.3469\n",
            "Epoch 53: Train Loss: 0.0042, Test Loss: 0.0022, Train L1 Norm: 0.2662, Test L1 Norm: 0.0852, Train Linf Norm: 14.1380, Test Linf Norm: 3.0591\n",
            "Epoch 54: Train Loss: 0.0041, Test Loss: 0.0022, Train L1 Norm: 0.2619, Test L1 Norm: 0.0808, Train Linf Norm: 13.9117, Test Linf Norm: 2.8068\n",
            "Epoch 55: Train Loss: 0.0039, Test Loss: 0.0022, Train L1 Norm: 0.2530, Test L1 Norm: 0.0816, Train Linf Norm: 13.4829, Test Linf Norm: 2.8250\n",
            "Epoch 56: Train Loss: 0.0041, Test Loss: 0.0039, Train L1 Norm: 0.2097, Test L1 Norm: 0.1031, Train Linf Norm: 10.7416, Test Linf Norm: 3.6401\n",
            "Epoch 57: Train Loss: 0.0019, Test Loss: 0.0019, Train L1 Norm: 0.2270, Test L1 Norm: 0.0780, Train Linf Norm: 12.2902, Test Linf Norm: 2.7347\n",
            "Epoch 58: Train Loss: 0.0018, Test Loss: 0.0018, Train L1 Norm: 0.2181, Test L1 Norm: 0.0789, Train Linf Norm: 11.7306, Test Linf Norm: 2.8267\n",
            "Epoch 59: Train Loss: 0.0018, Test Loss: 0.0018, Train L1 Norm: 0.1948, Test L1 Norm: 0.0786, Train Linf Norm: 10.2313, Test Linf Norm: 2.7688\n",
            "Epoch 60: Train Loss: 0.0018, Test Loss: 0.0018, Train L1 Norm: 0.1858, Test L1 Norm: 0.0778, Train Linf Norm: 9.7082, Test Linf Norm: 2.7359\n",
            "Epoch 61: Train Loss: 0.0018, Test Loss: 0.0018, Train L1 Norm: 0.1937, Test L1 Norm: 0.0781, Train Linf Norm: 10.0088, Test Linf Norm: 2.7652\n",
            "Epoch 62: Train Loss: 0.0017, Test Loss: 0.0019, Train L1 Norm: 0.1898, Test L1 Norm: 0.0773, Train Linf Norm: 9.9571, Test Linf Norm: 2.7565\n",
            "Epoch 63: Train Loss: 0.0017, Test Loss: 0.0018, Train L1 Norm: 0.1824, Test L1 Norm: 0.0763, Train Linf Norm: 9.4518, Test Linf Norm: 2.6352\n",
            "Epoch 64: Train Loss: 0.0017, Test Loss: 0.0025, Train L1 Norm: 0.2075, Test L1 Norm: 0.0846, Train Linf Norm: 11.0815, Test Linf Norm: 2.9472\n",
            "Epoch 65: Train Loss: 0.0017, Test Loss: 0.0017, Train L1 Norm: 0.1825, Test L1 Norm: 0.0765, Train Linf Norm: 9.4892, Test Linf Norm: 2.6404\n",
            "Epoch 66: Train Loss: 0.0017, Test Loss: 0.0018, Train L1 Norm: 0.1960, Test L1 Norm: 0.0750, Train Linf Norm: 10.3993, Test Linf Norm: 2.6052\n",
            "Epoch 67: Train Loss: 0.0017, Test Loss: 0.0017, Train L1 Norm: 0.1842, Test L1 Norm: 0.0725, Train Linf Norm: 9.5873, Test Linf Norm: 2.4797\n",
            "Epoch 68: Train Loss: 0.0017, Test Loss: 0.0017, Train L1 Norm: 0.1823, Test L1 Norm: 0.0739, Train Linf Norm: 9.5082, Test Linf Norm: 2.4841\n",
            "Epoch 69: Train Loss: 0.0016, Test Loss: 0.0017, Train L1 Norm: 0.1913, Test L1 Norm: 0.0715, Train Linf Norm: 10.0743, Test Linf Norm: 2.4178\n",
            "Epoch 70: Train Loss: 0.0016, Test Loss: 0.0017, Train L1 Norm: 0.1593, Test L1 Norm: 0.0723, Train Linf Norm: 7.9124, Test Linf Norm: 2.4505\n",
            "Epoch 71: Train Loss: 0.0016, Test Loss: 0.0017, Train L1 Norm: 0.1707, Test L1 Norm: 0.0722, Train Linf Norm: 8.7716, Test Linf Norm: 2.4447\n",
            "Epoch 72: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1750, Test L1 Norm: 0.0722, Train Linf Norm: 9.0797, Test Linf Norm: 2.4452\n",
            "Epoch 73: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1780, Test L1 Norm: 0.0720, Train Linf Norm: 9.2547, Test Linf Norm: 2.4425\n",
            "Epoch 74: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1726, Test L1 Norm: 0.0720, Train Linf Norm: 8.8762, Test Linf Norm: 2.4410\n",
            "Epoch 75: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1710, Test L1 Norm: 0.0718, Train Linf Norm: 8.8426, Test Linf Norm: 2.4314\n",
            "Epoch 76: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1750, Test L1 Norm: 0.0721, Train Linf Norm: 9.0348, Test Linf Norm: 2.4438\n",
            "Epoch 77: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1734, Test L1 Norm: 0.0719, Train Linf Norm: 8.9412, Test Linf Norm: 2.4382\n",
            "Epoch 78: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1738, Test L1 Norm: 0.0718, Train Linf Norm: 9.0313, Test Linf Norm: 2.4342\n",
            "Epoch 79: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1707, Test L1 Norm: 0.0721, Train Linf Norm: 8.7245, Test Linf Norm: 2.4392\n",
            "Epoch 80: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1702, Test L1 Norm: 0.0722, Train Linf Norm: 8.7914, Test Linf Norm: 2.4476\n",
            "Epoch 81: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1727, Test L1 Norm: 0.0718, Train Linf Norm: 8.9539, Test Linf Norm: 2.4400\n",
            "Epoch 82: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1698, Test L1 Norm: 0.0720, Train Linf Norm: 8.7486, Test Linf Norm: 2.4456\n",
            "Epoch 83: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1721, Test L1 Norm: 0.0720, Train Linf Norm: 8.8146, Test Linf Norm: 2.4402\n",
            "Epoch 84: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1714, Test L1 Norm: 0.0721, Train Linf Norm: 8.8238, Test Linf Norm: 2.4378\n",
            "Epoch 85: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1682, Test L1 Norm: 0.0718, Train Linf Norm: 8.5974, Test Linf Norm: 2.4317\n",
            "Epoch 86: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1682, Test L1 Norm: 0.0718, Train Linf Norm: 8.5804, Test Linf Norm: 2.4328\n",
            "Epoch 87: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1690, Test L1 Norm: 0.0717, Train Linf Norm: 8.6859, Test Linf Norm: 2.4294\n",
            "Epoch 88: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1690, Test L1 Norm: 0.0717, Train Linf Norm: 8.6721, Test Linf Norm: 2.4305\n",
            "Epoch 89: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1694, Test L1 Norm: 0.0717, Train Linf Norm: 8.6563, Test Linf Norm: 2.4315\n",
            "Epoch 90: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1690, Test L1 Norm: 0.0717, Train Linf Norm: 8.7292, Test Linf Norm: 2.4305\n",
            "Epoch 91: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1692, Test L1 Norm: 0.0717, Train Linf Norm: 8.7007, Test Linf Norm: 2.4318\n",
            "Epoch 92: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1693, Test L1 Norm: 0.0717, Train Linf Norm: 8.6942, Test Linf Norm: 2.4314\n",
            "Epoch 93: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1697, Test L1 Norm: 0.0718, Train Linf Norm: 8.7586, Test Linf Norm: 2.4344\n",
            "Epoch 94: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1694, Test L1 Norm: 0.0719, Train Linf Norm: 8.7469, Test Linf Norm: 2.4361\n",
            "Epoch 95: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1693, Test L1 Norm: 0.0718, Train Linf Norm: 8.6970, Test Linf Norm: 2.4336\n",
            "Epoch 96: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1688, Test L1 Norm: 0.0719, Train Linf Norm: 8.5820, Test Linf Norm: 2.4352\n",
            "Epoch 97: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1692, Test L1 Norm: 0.0718, Train Linf Norm: 8.7311, Test Linf Norm: 2.4342\n",
            "Epoch 98: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1696, Test L1 Norm: 0.0719, Train Linf Norm: 8.7481, Test Linf Norm: 2.4348\n",
            "Epoch 99: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1688, Test L1 Norm: 0.0718, Train Linf Norm: 8.6388, Test Linf Norm: 2.4345\n",
            "Epoch 100: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1688, Test L1 Norm: 0.0718, Train Linf Norm: 8.6109, Test Linf Norm: 2.4345\n",
            "Epoch 101: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1688, Test L1 Norm: 0.0718, Train Linf Norm: 8.6986, Test Linf Norm: 2.4342\n",
            "Epoch 102: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1688, Test L1 Norm: 0.0718, Train Linf Norm: 8.7043, Test Linf Norm: 2.4341\n",
            "Epoch 103: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.1077, Test Linf Norm: 2.4339\n",
            "Epoch 104: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6926, Test Linf Norm: 2.4338\n",
            "Epoch 105: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1688, Test L1 Norm: 0.0718, Train Linf Norm: 8.6512, Test Linf Norm: 2.4338\n",
            "Epoch 106: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1688, Test L1 Norm: 0.0718, Train Linf Norm: 8.6371, Test Linf Norm: 2.4336\n",
            "Epoch 107: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1688, Test L1 Norm: 0.0718, Train Linf Norm: 8.6472, Test Linf Norm: 2.4336\n",
            "Epoch 108: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6907, Test Linf Norm: 2.4334\n",
            "Epoch 109: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1687, Test L1 Norm: 0.0718, Train Linf Norm: 8.7333, Test Linf Norm: 2.4335\n",
            "Epoch 110: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1688, Test L1 Norm: 0.0718, Train Linf Norm: 8.6790, Test Linf Norm: 2.4333\n",
            "Epoch 111: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1690, Test L1 Norm: 0.0718, Train Linf Norm: 8.7006, Test Linf Norm: 2.4333\n",
            "Epoch 112: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6712, Test Linf Norm: 2.4332\n",
            "Epoch 113: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6698, Test Linf Norm: 2.4332\n",
            "Epoch 114: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6887, Test Linf Norm: 2.4333\n",
            "Epoch 115: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6783, Test Linf Norm: 2.4332\n",
            "Epoch 116: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6653, Test Linf Norm: 2.4332\n",
            "Epoch 117: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7189, Test Linf Norm: 2.4333\n",
            "Epoch 118: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6692, Test Linf Norm: 2.4332\n",
            "Epoch 119: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6916, Test Linf Norm: 2.4333\n",
            "Epoch 120: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6604, Test Linf Norm: 2.4332\n",
            "Epoch 121: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6764, Test Linf Norm: 2.4333\n",
            "Epoch 122: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6696, Test Linf Norm: 2.4332\n",
            "Epoch 123: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6798, Test Linf Norm: 2.4332\n",
            "Epoch 124: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6901, Test Linf Norm: 2.4333\n",
            "Epoch 125: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6353, Test Linf Norm: 2.4333\n",
            "Epoch 126: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6920, Test Linf Norm: 2.4333\n",
            "Epoch 127: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7246, Test Linf Norm: 2.4333\n",
            "Epoch 128: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6814, Test Linf Norm: 2.4332\n",
            "Epoch 129: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6941, Test Linf Norm: 2.4332\n",
            "Epoch 130: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6610, Test Linf Norm: 2.4332\n",
            "Epoch 131: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7141, Test Linf Norm: 2.4333\n",
            "Epoch 132: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6232, Test Linf Norm: 2.4333\n",
            "Epoch 133: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7143, Test Linf Norm: 2.4333\n",
            "Epoch 134: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7045, Test Linf Norm: 2.4333\n",
            "Epoch 135: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.5717, Test Linf Norm: 2.4333\n",
            "Epoch 136: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6872, Test Linf Norm: 2.4333\n",
            "Epoch 137: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6349, Test Linf Norm: 2.4333\n",
            "Epoch 138: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6808, Test Linf Norm: 2.4332\n",
            "Epoch 139: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7194, Test Linf Norm: 2.4333\n",
            "Epoch 140: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6846, Test Linf Norm: 2.4332\n",
            "Epoch 141: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6602, Test Linf Norm: 2.4332\n",
            "Epoch 142: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6651, Test Linf Norm: 2.4332\n",
            "Epoch 143: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6927, Test Linf Norm: 2.4332\n",
            "Epoch 144: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6858, Test Linf Norm: 2.4332\n",
            "Epoch 145: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6769, Test Linf Norm: 2.4332\n",
            "Epoch 146: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6650, Test Linf Norm: 2.4332\n",
            "Epoch 147: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7032, Test Linf Norm: 2.4332\n",
            "Epoch 148: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6078, Test Linf Norm: 2.4332\n",
            "Epoch 149: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6844, Test Linf Norm: 2.4332\n",
            "Epoch 150: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7033, Test Linf Norm: 2.4332\n",
            "Epoch 151: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6781, Test Linf Norm: 2.4332\n",
            "Epoch 152: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6764, Test Linf Norm: 2.4332\n",
            "Epoch 153: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6927, Test Linf Norm: 2.4332\n",
            "Epoch 154: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6781, Test Linf Norm: 2.4332\n",
            "Epoch 155: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6425, Test Linf Norm: 2.4332\n",
            "Epoch 156: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6759, Test Linf Norm: 2.4332\n",
            "Epoch 157: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7150, Test Linf Norm: 2.4332\n",
            "Epoch 158: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7268, Test Linf Norm: 2.4332\n",
            "Epoch 159: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6889, Test Linf Norm: 2.4332\n",
            "Epoch 160: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6443, Test Linf Norm: 2.4332\n",
            "Epoch 161: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7182, Test Linf Norm: 2.4332\n",
            "Epoch 162: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6777, Test Linf Norm: 2.4332\n",
            "Epoch 163: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7241, Test Linf Norm: 2.4332\n",
            "Epoch 164: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6431, Test Linf Norm: 2.4332\n",
            "Epoch 165: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6871, Test Linf Norm: 2.4332\n",
            "Epoch 166: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6428, Test Linf Norm: 2.4332\n",
            "Epoch 167: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6341, Test Linf Norm: 2.4332\n",
            "Epoch 168: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.3222, Test Linf Norm: 2.4332\n",
            "Epoch 169: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7060, Test Linf Norm: 2.4332\n",
            "Epoch 170: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6292, Test Linf Norm: 2.4332\n",
            "Epoch 171: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7054, Test Linf Norm: 2.4332\n",
            "Epoch 172: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6878, Test Linf Norm: 2.4332\n",
            "Epoch 173: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7014, Test Linf Norm: 2.4332\n",
            "Epoch 174: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6755, Test Linf Norm: 2.4332\n",
            "Epoch 175: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7202, Test Linf Norm: 2.4332\n",
            "Epoch 176: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6314, Test Linf Norm: 2.4332\n",
            "Epoch 177: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7025, Test Linf Norm: 2.4332\n",
            "Epoch 178: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7231, Test Linf Norm: 2.4332\n",
            "Epoch 179: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6814, Test Linf Norm: 2.4332\n",
            "Epoch 180: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6822, Test Linf Norm: 2.4332\n",
            "Epoch 181: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7060, Test Linf Norm: 2.4332\n",
            "Epoch 182: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7206, Test Linf Norm: 2.4332\n",
            "Epoch 183: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.5550, Test Linf Norm: 2.4332\n",
            "Epoch 184: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6735, Test Linf Norm: 2.4332\n",
            "Epoch 185: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.5900, Test Linf Norm: 2.4332\n",
            "Epoch 186: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6923, Test Linf Norm: 2.4332\n",
            "Epoch 187: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.5804, Test Linf Norm: 2.4332\n",
            "Epoch 188: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.4286, Test Linf Norm: 2.4332\n",
            "Epoch 189: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6168, Test Linf Norm: 2.4332\n",
            "Epoch 190: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6665, Test Linf Norm: 2.4332\n",
            "Epoch 191: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6359, Test Linf Norm: 2.4332\n",
            "Epoch 192: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6781, Test Linf Norm: 2.4332\n",
            "Epoch 193: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7450, Test Linf Norm: 2.4332\n",
            "Epoch 194: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6683, Test Linf Norm: 2.4332\n",
            "Epoch 195: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6994, Test Linf Norm: 2.4332\n",
            "Epoch 196: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6917, Test Linf Norm: 2.4332\n",
            "Epoch 197: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6981, Test Linf Norm: 2.4332\n",
            "Epoch 198: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7100, Test Linf Norm: 2.4332\n",
            "Epoch 199: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6924, Test Linf Norm: 2.4332\n",
            "Epoch 200: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6466, Test Linf Norm: 2.4332\n",
            "Epoch 201: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6846, Test Linf Norm: 2.4332\n",
            "Epoch 202: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6819, Test Linf Norm: 2.4332\n",
            "Epoch 203: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7225, Test Linf Norm: 2.4332\n",
            "Epoch 204: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6885, Test Linf Norm: 2.4332\n",
            "Epoch 205: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6707, Test Linf Norm: 2.4332\n",
            "Epoch 206: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7201, Test Linf Norm: 2.4332\n",
            "Epoch 207: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6839, Test Linf Norm: 2.4332\n",
            "Epoch 208: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6859, Test Linf Norm: 2.4332\n",
            "Epoch 209: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.5966, Test Linf Norm: 2.4332\n",
            "Epoch 210: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6804, Test Linf Norm: 2.4332\n",
            "Epoch 211: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6884, Test Linf Norm: 2.4332\n",
            "Epoch 212: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7297, Test Linf Norm: 2.4332\n",
            "Epoch 213: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7268, Test Linf Norm: 2.4332\n",
            "Epoch 214: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6899, Test Linf Norm: 2.4332\n",
            "Epoch 215: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.4389, Test Linf Norm: 2.4332\n",
            "Epoch 216: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7167, Test Linf Norm: 2.4332\n",
            "Epoch 217: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6392, Test Linf Norm: 2.4332\n",
            "Epoch 218: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6160, Test Linf Norm: 2.4332\n",
            "Epoch 219: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6364, Test Linf Norm: 2.4332\n",
            "Epoch 220: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6690, Test Linf Norm: 2.4332\n",
            "Epoch 221: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7175, Test Linf Norm: 2.4332\n",
            "Epoch 222: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7087, Test Linf Norm: 2.4332\n",
            "Epoch 223: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6866, Test Linf Norm: 2.4332\n",
            "Epoch 224: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7073, Test Linf Norm: 2.4332\n",
            "Epoch 225: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7169, Test Linf Norm: 2.4332\n",
            "Epoch 226: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6866, Test Linf Norm: 2.4332\n",
            "Epoch 227: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6174, Test Linf Norm: 2.4332\n",
            "Epoch 228: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6970, Test Linf Norm: 2.4332\n",
            "Epoch 229: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6653, Test Linf Norm: 2.4332\n",
            "Epoch 230: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7015, Test Linf Norm: 2.4332\n",
            "Epoch 231: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7144, Test Linf Norm: 2.4332\n",
            "Epoch 232: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6803, Test Linf Norm: 2.4332\n",
            "Epoch 233: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7166, Test Linf Norm: 2.4332\n",
            "Epoch 234: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6356, Test Linf Norm: 2.4332\n",
            "Epoch 235: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7409, Test Linf Norm: 2.4332\n",
            "Epoch 236: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6903, Test Linf Norm: 2.4332\n",
            "Epoch 237: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6552, Test Linf Norm: 2.4332\n",
            "Epoch 238: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6957, Test Linf Norm: 2.4332\n",
            "Epoch 239: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6804, Test Linf Norm: 2.4332\n",
            "Epoch 240: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.6858, Test Linf Norm: 2.4332\n",
            "Epoch 241: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7440, Test Linf Norm: 2.4332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 16:35:39,969]\u001b[0m Trial 3 finished with value: 0.0717981551527977 and parameters: {'n_layers': 8, 'n_units_0': 508, 'n_units_1': 1556, 'n_units_2': 1567, 'n_units_3': 369, 'n_units_4': 981, 'n_units_5': 361, 'n_units_6': 369, 'n_units_7': 854, 'hidden_activation': 'LeakyReLU', 'output_activation': 'Linear', 'loss': 'Huber', 'optimizer': 'RMSprop', 'lr': 0.004991987285454393, 'batch_size': 64, 'n_epochs': 242, 'scheduler': 'StepLR', 'step_size': 14, 'gamma': 0.10367254341655477}. Best is trial 2 with value: 0.011449855588376521.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 242: Train Loss: 0.0015, Test Loss: 0.0016, Train L1 Norm: 0.1689, Test L1 Norm: 0.0718, Train Linf Norm: 8.7177, Test Linf Norm: 2.4332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-82e2252adf92>:126: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
            "<ipython-input-28-82e2252adf92>:127: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  beta1 = trial.suggest_uniform(\"beta1\", 0.9, 0.999)\n",
            "<ipython-input-28-82e2252adf92>:128: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  beta2 = trial.suggest_uniform(\"beta2\", 0.999, 0.9999)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 375.0184, Test Loss: 21.1527, Train L1 Norm: 1.2287, Test L1 Norm: 1.0000, Train Linf Norm: 2.0857, Test Linf Norm: 1.0000\n",
            "Epoch 2: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 3: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 4: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 5: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 6: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 7: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 8: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 9: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 10: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 11: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 12: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 13: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 14: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 15: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 16: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 17: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 18: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 19: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 20: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 21: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 22: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 23: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 24: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 25: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 26: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 27: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 28: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 29: Train Loss: 21.0195, Test Loss: 21.1527, Train L1 Norm: 1.0190, Test L1 Norm: 1.0000, Train Linf Norm: 1.9601, Test Linf Norm: 1.0000\n",
            "Epoch 30: Train Loss: 20.6257, Test Loss: 21.1527, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 31: Train Loss: 19.7252, Test Loss: 5.7049, Train L1 Norm: 1.2471, Test L1 Norm: 3.1891, Train Linf Norm: 23.9268, Test Linf Norm: 149.5103\n",
            "Epoch 32: Train Loss: 0.8603, Test Loss: 0.0890, Train L1 Norm: 3.7274, Test L1 Norm: 0.7749, Train Linf Norm: 298.8247, Test Linf Norm: 45.2259\n",
            "Epoch 33: Train Loss: 0.0664, Test Loss: 0.0312, Train L1 Norm: 2.0323, Test L1 Norm: 0.5408, Train Linf Norm: 171.3211, Test Linf Norm: 32.4600\n",
            "Epoch 34: Train Loss: 0.0536, Test Loss: 0.0232, Train L1 Norm: 1.6889, Test L1 Norm: 0.4572, Train Linf Norm: 144.7461, Test Linf Norm: 27.6078\n",
            "Epoch 35: Train Loss: 0.0430, Test Loss: 0.0209, Train L1 Norm: 1.4551, Test L1 Norm: 0.4028, Train Linf Norm: 124.1843, Test Linf Norm: 24.5499\n",
            "Epoch 36: Train Loss: 0.0340, Test Loss: 0.0590, Train L1 Norm: 1.2773, Test L1 Norm: 0.3788, Train Linf Norm: 109.2190, Test Linf Norm: 22.3624\n",
            "Epoch 37: Train Loss: 0.0353, Test Loss: 0.0639, Train L1 Norm: 1.1528, Test L1 Norm: 0.3570, Train Linf Norm: 98.9966, Test Linf Norm: 19.0398\n",
            "Epoch 38: Train Loss: 0.0582, Test Loss: 0.1300, Train L1 Norm: 1.0565, Test L1 Norm: 0.3717, Train Linf Norm: 89.7614, Test Linf Norm: 19.8296\n",
            "Epoch 39: Train Loss: 0.0346, Test Loss: 0.0294, Train L1 Norm: 1.0506, Test L1 Norm: 0.2893, Train Linf Norm: 89.9962, Test Linf Norm: 17.6666\n",
            "Epoch 40: Train Loss: 0.0413, Test Loss: 0.0108, Train L1 Norm: 0.9851, Test L1 Norm: 0.2112, Train Linf Norm: 84.0682, Test Linf Norm: 12.8904\n",
            "Epoch 41: Train Loss: 0.0369, Test Loss: 0.0214, Train L1 Norm: 0.7842, Test L1 Norm: 0.2207, Train Linf Norm: 66.5817, Test Linf Norm: 12.9492\n",
            "Epoch 42: Train Loss: 0.0202, Test Loss: 0.0126, Train L1 Norm: 0.6205, Test L1 Norm: 0.1774, Train Linf Norm: 52.6726, Test Linf Norm: 10.5196\n",
            "Epoch 43: Train Loss: 0.0528, Test Loss: 0.0226, Train L1 Norm: 0.5420, Test L1 Norm: 0.3271, Train Linf Norm: 44.2699, Test Linf Norm: 19.5465\n",
            "Epoch 44: Train Loss: 0.0147, Test Loss: 0.0156, Train L1 Norm: 0.6649, Test L1 Norm: 0.1801, Train Linf Norm: 57.1122, Test Linf Norm: 10.5216\n",
            "Epoch 45: Train Loss: 0.0170, Test Loss: 0.0111, Train L1 Norm: 0.5733, Test L1 Norm: 0.1685, Train Linf Norm: 48.5133, Test Linf Norm: 10.3593\n",
            "Epoch 46: Train Loss: 0.0098, Test Loss: 0.0098, Train L1 Norm: 0.5281, Test L1 Norm: 0.1552, Train Linf Norm: 45.5453, Test Linf Norm: 9.3154\n",
            "Epoch 47: Train Loss: 0.0088, Test Loss: 0.0086, Train L1 Norm: 0.5011, Test L1 Norm: 0.1466, Train Linf Norm: 42.6691, Test Linf Norm: 8.7837\n",
            "Epoch 48: Train Loss: 0.0088, Test Loss: 0.0072, Train L1 Norm: 0.4772, Test L1 Norm: 0.1416, Train Linf Norm: 40.7426, Test Linf Norm: 8.4731\n",
            "Epoch 49: Train Loss: 0.0092, Test Loss: 0.0095, Train L1 Norm: 0.4344, Test L1 Norm: 0.1183, Train Linf Norm: 36.9796, Test Linf Norm: 6.2765\n",
            "Epoch 50: Train Loss: 0.0083, Test Loss: 0.0195, Train L1 Norm: 0.3915, Test L1 Norm: 0.1175, Train Linf Norm: 32.9468, Test Linf Norm: 6.2611\n",
            "Epoch 51: Train Loss: 0.0098, Test Loss: 0.0057, Train L1 Norm: 0.3678, Test L1 Norm: 0.1172, Train Linf Norm: 30.7890, Test Linf Norm: 6.8097\n",
            "Epoch 52: Train Loss: 0.0095, Test Loss: 0.0057, Train L1 Norm: 0.3445, Test L1 Norm: 0.1061, Train Linf Norm: 28.4502, Test Linf Norm: 6.0080\n",
            "Epoch 53: Train Loss: 0.0102, Test Loss: 0.0108, Train L1 Norm: 0.3787, Test L1 Norm: 0.1065, Train Linf Norm: 31.8178, Test Linf Norm: 5.4995\n",
            "Epoch 54: Train Loss: 0.0090, Test Loss: 0.0142, Train L1 Norm: 0.3172, Test L1 Norm: 0.1076, Train Linf Norm: 26.2477, Test Linf Norm: 5.5930\n",
            "Epoch 55: Train Loss: 0.0090, Test Loss: 0.0162, Train L1 Norm: 0.3294, Test L1 Norm: 0.1066, Train Linf Norm: 27.2191, Test Linf Norm: 5.8484\n",
            "Epoch 56: Train Loss: 0.0078, Test Loss: 0.0063, Train L1 Norm: 0.3118, Test L1 Norm: 0.1003, Train Linf Norm: 25.5058, Test Linf Norm: 5.5713\n",
            "Epoch 57: Train Loss: 0.0116, Test Loss: 0.0056, Train L1 Norm: 0.3418, Test L1 Norm: 0.1029, Train Linf Norm: 28.0456, Test Linf Norm: 5.8520\n",
            "Epoch 58: Train Loss: 0.0078, Test Loss: 0.0051, Train L1 Norm: 0.3209, Test L1 Norm: 0.1026, Train Linf Norm: 26.6992, Test Linf Norm: 5.8227\n",
            "Epoch 59: Train Loss: 0.0115, Test Loss: 0.0054, Train L1 Norm: 0.3635, Test L1 Norm: 0.1004, Train Linf Norm: 28.2339, Test Linf Norm: 5.6503\n",
            "Epoch 60: Train Loss: 0.0094, Test Loss: 0.0059, Train L1 Norm: 0.3076, Test L1 Norm: 0.0934, Train Linf Norm: 25.1499, Test Linf Norm: 5.0956\n",
            "Epoch 61: Train Loss: 0.0052, Test Loss: 0.0048, Train L1 Norm: 0.2974, Test L1 Norm: 0.0990, Train Linf Norm: 24.8459, Test Linf Norm: 5.6357\n",
            "Epoch 62: Train Loss: 0.0049, Test Loss: 0.0047, Train L1 Norm: 0.2868, Test L1 Norm: 0.0993, Train Linf Norm: 23.9357, Test Linf Norm: 5.6790\n",
            "Epoch 63: Train Loss: 0.0050, Test Loss: 0.0047, Train L1 Norm: 0.2740, Test L1 Norm: 0.0951, Train Linf Norm: 22.6808, Test Linf Norm: 5.3895\n",
            "Epoch 64: Train Loss: 0.0055, Test Loss: 0.0053, Train L1 Norm: 0.2679, Test L1 Norm: 0.0894, Train Linf Norm: 21.7316, Test Linf Norm: 4.8556\n",
            "Epoch 65: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 0.2634, Test L1 Norm: 0.0878, Train Linf Norm: 21.8315, Test Linf Norm: 4.8500\n",
            "Epoch 66: Train Loss: 0.0048, Test Loss: 0.0047, Train L1 Norm: 0.2580, Test L1 Norm: 0.0879, Train Linf Norm: 21.2898, Test Linf Norm: 4.8536\n",
            "Epoch 67: Train Loss: 0.0051, Test Loss: 0.0050, Train L1 Norm: 0.2511, Test L1 Norm: 0.0916, Train Linf Norm: 20.7384, Test Linf Norm: 5.0994\n",
            "Epoch 68: Train Loss: 0.0050, Test Loss: 0.0072, Train L1 Norm: 0.2463, Test L1 Norm: 0.0825, Train Linf Norm: 20.0580, Test Linf Norm: 4.1024\n",
            "Epoch 69: Train Loss: 0.0049, Test Loss: 0.0043, Train L1 Norm: 0.2614, Test L1 Norm: 0.0826, Train Linf Norm: 21.5950, Test Linf Norm: 4.4848\n",
            "Epoch 70: Train Loss: 0.0047, Test Loss: 0.0055, Train L1 Norm: 0.2423, Test L1 Norm: 0.0782, Train Linf Norm: 19.8777, Test Linf Norm: 3.9491\n",
            "Epoch 71: Train Loss: 0.0047, Test Loss: 0.0051, Train L1 Norm: 0.2356, Test L1 Norm: 0.0828, Train Linf Norm: 19.2759, Test Linf Norm: 4.4714\n",
            "Epoch 72: Train Loss: 0.0051, Test Loss: 0.0046, Train L1 Norm: 0.2325, Test L1 Norm: 0.0849, Train Linf Norm: 18.9611, Test Linf Norm: 4.6510\n",
            "Epoch 73: Train Loss: 0.0046, Test Loss: 0.0044, Train L1 Norm: 0.2277, Test L1 Norm: 0.0869, Train Linf Norm: 18.4360, Test Linf Norm: 4.7916\n",
            "Epoch 74: Train Loss: 0.0049, Test Loss: 0.0042, Train L1 Norm: 0.2206, Test L1 Norm: 0.0772, Train Linf Norm: 17.8962, Test Linf Norm: 4.0905\n",
            "Epoch 75: Train Loss: 0.0045, Test Loss: 0.0040, Train L1 Norm: 0.2302, Test L1 Norm: 0.0776, Train Linf Norm: 18.9203, Test Linf Norm: 4.1304\n",
            "Epoch 76: Train Loss: 0.0040, Test Loss: 0.0044, Train L1 Norm: 0.2157, Test L1 Norm: 0.0750, Train Linf Norm: 17.6400, Test Linf Norm: 3.8787\n",
            "Epoch 77: Train Loss: 0.0039, Test Loss: 0.0039, Train L1 Norm: 0.2161, Test L1 Norm: 0.0770, Train Linf Norm: 17.7459, Test Linf Norm: 4.1111\n",
            "Epoch 78: Train Loss: 0.0040, Test Loss: 0.0039, Train L1 Norm: 0.2098, Test L1 Norm: 0.0770, Train Linf Norm: 17.0423, Test Linf Norm: 4.1126\n",
            "Epoch 79: Train Loss: 0.0041, Test Loss: 0.0039, Train L1 Norm: 0.2122, Test L1 Norm: 0.0778, Train Linf Norm: 17.1810, Test Linf Norm: 4.1834\n",
            "Epoch 80: Train Loss: 0.0039, Test Loss: 0.0039, Train L1 Norm: 0.2015, Test L1 Norm: 0.0790, Train Linf Norm: 16.1677, Test Linf Norm: 4.2727\n",
            "Epoch 81: Train Loss: 0.0040, Test Loss: 0.0042, Train L1 Norm: 0.2147, Test L1 Norm: 0.0761, Train Linf Norm: 17.4544, Test Linf Norm: 4.0223\n",
            "Epoch 82: Train Loss: 0.0039, Test Loss: 0.0039, Train L1 Norm: 0.2064, Test L1 Norm: 0.0773, Train Linf Norm: 16.7328, Test Linf Norm: 4.1542\n",
            "Epoch 83: Train Loss: 0.0039, Test Loss: 0.0039, Train L1 Norm: 0.2060, Test L1 Norm: 0.0746, Train Linf Norm: 16.5948, Test Linf Norm: 3.9397\n",
            "Epoch 84: Train Loss: 0.0040, Test Loss: 0.0039, Train L1 Norm: 0.2063, Test L1 Norm: 0.0773, Train Linf Norm: 16.5344, Test Linf Norm: 4.1687\n",
            "Epoch 85: Train Loss: 0.0039, Test Loss: 0.0040, Train L1 Norm: 0.2068, Test L1 Norm: 0.0778, Train Linf Norm: 15.8836, Test Linf Norm: 4.1991\n",
            "Epoch 86: Train Loss: 0.0039, Test Loss: 0.0040, Train L1 Norm: 0.2021, Test L1 Norm: 0.0800, Train Linf Norm: 16.4081, Test Linf Norm: 4.3524\n",
            "Epoch 87: Train Loss: 0.0038, Test Loss: 0.0040, Train L1 Norm: 0.2038, Test L1 Norm: 0.0746, Train Linf Norm: 16.5448, Test Linf Norm: 3.9766\n",
            "Epoch 88: Train Loss: 0.0038, Test Loss: 0.0038, Train L1 Norm: 0.2025, Test L1 Norm: 0.0740, Train Linf Norm: 16.4151, Test Linf Norm: 3.9284\n",
            "Epoch 89: Train Loss: 0.0038, Test Loss: 0.0038, Train L1 Norm: 0.1887, Test L1 Norm: 0.0738, Train Linf Norm: 15.1348, Test Linf Norm: 3.9272\n",
            "Epoch 90: Train Loss: 0.0038, Test Loss: 0.0039, Train L1 Norm: 0.2024, Test L1 Norm: 0.0722, Train Linf Norm: 16.4475, Test Linf Norm: 3.7741\n",
            "Epoch 91: Train Loss: 0.0037, Test Loss: 0.0038, Train L1 Norm: 0.1949, Test L1 Norm: 0.0771, Train Linf Norm: 15.7815, Test Linf Norm: 4.1736\n",
            "Epoch 92: Train Loss: 0.0037, Test Loss: 0.0038, Train L1 Norm: 0.1959, Test L1 Norm: 0.0752, Train Linf Norm: 15.8488, Test Linf Norm: 4.0390\n",
            "Epoch 93: Train Loss: 0.0037, Test Loss: 0.0038, Train L1 Norm: 0.1973, Test L1 Norm: 0.0751, Train Linf Norm: 16.0650, Test Linf Norm: 4.0311\n",
            "Epoch 94: Train Loss: 0.0037, Test Loss: 0.0038, Train L1 Norm: 0.2006, Test L1 Norm: 0.0730, Train Linf Norm: 16.2634, Test Linf Norm: 3.8672\n",
            "Epoch 95: Train Loss: 0.0037, Test Loss: 0.0038, Train L1 Norm: 0.1963, Test L1 Norm: 0.0731, Train Linf Norm: 15.7767, Test Linf Norm: 3.8768\n",
            "Epoch 96: Train Loss: 0.0037, Test Loss: 0.0038, Train L1 Norm: 0.1988, Test L1 Norm: 0.0743, Train Linf Norm: 16.0995, Test Linf Norm: 3.9838\n",
            "Epoch 97: Train Loss: 0.0036, Test Loss: 0.0038, Train L1 Norm: 0.1954, Test L1 Norm: 0.0736, Train Linf Norm: 15.6971, Test Linf Norm: 3.9172\n",
            "Epoch 98: Train Loss: 0.0036, Test Loss: 0.0037, Train L1 Norm: 0.1985, Test L1 Norm: 0.0733, Train Linf Norm: 15.9594, Test Linf Norm: 3.9014\n",
            "Epoch 99: Train Loss: 0.0037, Test Loss: 0.0038, Train L1 Norm: 0.1941, Test L1 Norm: 0.0738, Train Linf Norm: 15.6239, Test Linf Norm: 3.9383\n",
            "Epoch 100: Train Loss: 0.0037, Test Loss: 0.0038, Train L1 Norm: 0.1928, Test L1 Norm: 0.0743, Train Linf Norm: 15.5094, Test Linf Norm: 3.9827\n",
            "Epoch 101: Train Loss: 0.0036, Test Loss: 0.0037, Train L1 Norm: 0.1960, Test L1 Norm: 0.0737, Train Linf Norm: 15.9010, Test Linf Norm: 3.9426\n",
            "Epoch 102: Train Loss: 0.0036, Test Loss: 0.0037, Train L1 Norm: 0.1955, Test L1 Norm: 0.0726, Train Linf Norm: 15.7226, Test Linf Norm: 3.8489\n",
            "Epoch 103: Train Loss: 0.0036, Test Loss: 0.0039, Train L1 Norm: 0.1913, Test L1 Norm: 0.0736, Train Linf Norm: 15.3378, Test Linf Norm: 3.9197\n",
            "Epoch 104: Train Loss: 0.0036, Test Loss: 0.0038, Train L1 Norm: 0.1995, Test L1 Norm: 0.0731, Train Linf Norm: 16.2321, Test Linf Norm: 3.8979\n",
            "Epoch 105: Train Loss: 0.0036, Test Loss: 0.0037, Train L1 Norm: 0.1931, Test L1 Norm: 0.0744, Train Linf Norm: 15.6641, Test Linf Norm: 3.9974\n",
            "Epoch 106: Train Loss: 0.0036, Test Loss: 0.0037, Train L1 Norm: 0.1907, Test L1 Norm: 0.0731, Train Linf Norm: 15.4373, Test Linf Norm: 3.9022\n",
            "Epoch 107: Train Loss: 0.0036, Test Loss: 0.0037, Train L1 Norm: 0.1915, Test L1 Norm: 0.0739, Train Linf Norm: 15.4670, Test Linf Norm: 3.9668\n",
            "Epoch 108: Train Loss: 0.0036, Test Loss: 0.0038, Train L1 Norm: 0.1926, Test L1 Norm: 0.0727, Train Linf Norm: 15.5983, Test Linf Norm: 3.8613\n",
            "Epoch 109: Train Loss: 0.0036, Test Loss: 0.0037, Train L1 Norm: 0.1931, Test L1 Norm: 0.0730, Train Linf Norm: 15.5713, Test Linf Norm: 3.8885\n",
            "Epoch 110: Train Loss: 0.0036, Test Loss: 0.0037, Train L1 Norm: 0.1922, Test L1 Norm: 0.0731, Train Linf Norm: 15.4837, Test Linf Norm: 3.9034\n",
            "Epoch 111: Train Loss: 0.0036, Test Loss: 0.0037, Train L1 Norm: 0.1920, Test L1 Norm: 0.0732, Train Linf Norm: 15.4607, Test Linf Norm: 3.9067\n",
            "Epoch 112: Train Loss: 0.0036, Test Loss: 0.0037, Train L1 Norm: 0.1906, Test L1 Norm: 0.0732, Train Linf Norm: 15.4428, Test Linf Norm: 3.9112\n",
            "Epoch 113: Train Loss: 0.0036, Test Loss: 0.0037, Train L1 Norm: 0.1913, Test L1 Norm: 0.0733, Train Linf Norm: 15.4625, Test Linf Norm: 3.9203\n",
            "Epoch 114: Train Loss: 0.0036, Test Loss: 0.0037, Train L1 Norm: 0.1919, Test L1 Norm: 0.0736, Train Linf Norm: 15.3726, Test Linf Norm: 3.9421\n",
            "Epoch 115: Train Loss: 0.0036, Test Loss: 0.0037, Train L1 Norm: 0.1930, Test L1 Norm: 0.0731, Train Linf Norm: 15.5747, Test Linf Norm: 3.9035\n",
            "Epoch 116: Train Loss: 0.0036, Test Loss: 0.0037, Train L1 Norm: 0.1929, Test L1 Norm: 0.0735, Train Linf Norm: 15.6493, Test Linf Norm: 3.9339\n",
            "Epoch 117: Train Loss: 0.0036, Test Loss: 0.0037, Train L1 Norm: 0.1922, Test L1 Norm: 0.0736, Train Linf Norm: 15.4903, Test Linf Norm: 3.9447\n",
            "Epoch 118: Train Loss: 0.0036, Test Loss: 0.0037, Train L1 Norm: 0.1938, Test L1 Norm: 0.0732, Train Linf Norm: 15.6159, Test Linf Norm: 3.9158\n",
            "Epoch 119: Train Loss: 0.0036, Test Loss: 0.0037, Train L1 Norm: 0.1930, Test L1 Norm: 0.0734, Train Linf Norm: 15.6522, Test Linf Norm: 3.9273\n",
            "Epoch 120: Train Loss: 0.0036, Test Loss: 0.0037, Train L1 Norm: 0.1925, Test L1 Norm: 0.0727, Train Linf Norm: 15.5393, Test Linf Norm: 3.8753\n",
            "Epoch 121: Train Loss: 0.0036, Test Loss: 0.0037, Train L1 Norm: 0.1902, Test L1 Norm: 0.0730, Train Linf Norm: 15.3378, Test Linf Norm: 3.8970\n",
            "Epoch 122: Train Loss: 0.0036, Test Loss: 0.0037, Train L1 Norm: 0.1919, Test L1 Norm: 0.0733, Train Linf Norm: 15.5052, Test Linf Norm: 3.9237\n",
            "Epoch 123: Train Loss: 0.0036, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0733, Train Linf Norm: 15.4968, Test Linf Norm: 3.9189\n",
            "Epoch 124: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1927, Test L1 Norm: 0.0735, Train Linf Norm: 15.5478, Test Linf Norm: 3.9399\n",
            "Epoch 125: Train Loss: 0.0036, Test Loss: 0.0037, Train L1 Norm: 0.1917, Test L1 Norm: 0.0733, Train Linf Norm: 15.3838, Test Linf Norm: 3.9188\n",
            "Epoch 126: Train Loss: 0.0036, Test Loss: 0.0037, Train L1 Norm: 0.1908, Test L1 Norm: 0.0731, Train Linf Norm: 15.3376, Test Linf Norm: 3.9080\n",
            "Epoch 127: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1921, Test L1 Norm: 0.0733, Train Linf Norm: 15.5280, Test Linf Norm: 3.9193\n",
            "Epoch 128: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1917, Test L1 Norm: 0.0734, Train Linf Norm: 15.5069, Test Linf Norm: 3.9296\n",
            "Epoch 129: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1919, Test L1 Norm: 0.0732, Train Linf Norm: 15.4092, Test Linf Norm: 3.9168\n",
            "Epoch 130: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1916, Test L1 Norm: 0.0729, Train Linf Norm: 15.4430, Test Linf Norm: 3.8927\n",
            "Epoch 131: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1923, Test L1 Norm: 0.0733, Train Linf Norm: 15.6296, Test Linf Norm: 3.9261\n",
            "Epoch 132: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1921, Test L1 Norm: 0.0732, Train Linf Norm: 15.5893, Test Linf Norm: 3.9157\n",
            "Epoch 133: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1907, Test L1 Norm: 0.0732, Train Linf Norm: 15.4073, Test Linf Norm: 3.9187\n",
            "Epoch 134: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1919, Test L1 Norm: 0.0733, Train Linf Norm: 15.5773, Test Linf Norm: 3.9256\n",
            "Epoch 135: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1916, Test L1 Norm: 0.0732, Train Linf Norm: 15.4511, Test Linf Norm: 3.9132\n",
            "Epoch 136: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1915, Test L1 Norm: 0.0731, Train Linf Norm: 15.5292, Test Linf Norm: 3.9073\n",
            "Epoch 137: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1913, Test L1 Norm: 0.0731, Train Linf Norm: 15.3913, Test Linf Norm: 3.9089\n",
            "Epoch 138: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1916, Test L1 Norm: 0.0732, Train Linf Norm: 15.3479, Test Linf Norm: 3.9141\n",
            "Epoch 139: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.5033, Test Linf Norm: 3.9169\n",
            "Epoch 140: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1916, Test L1 Norm: 0.0733, Train Linf Norm: 15.5707, Test Linf Norm: 3.9219\n",
            "Epoch 141: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1919, Test L1 Norm: 0.0732, Train Linf Norm: 15.5264, Test Linf Norm: 3.9139\n",
            "Epoch 142: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1915, Test L1 Norm: 0.0732, Train Linf Norm: 15.4211, Test Linf Norm: 3.9113\n",
            "Epoch 143: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1913, Test L1 Norm: 0.0732, Train Linf Norm: 15.4125, Test Linf Norm: 3.9116\n",
            "Epoch 144: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1916, Test L1 Norm: 0.0731, Train Linf Norm: 15.4849, Test Linf Norm: 3.9090\n",
            "Epoch 145: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1918, Test L1 Norm: 0.0731, Train Linf Norm: 15.4126, Test Linf Norm: 3.9101\n",
            "Epoch 146: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4075, Test Linf Norm: 3.9166\n",
            "Epoch 147: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1918, Test L1 Norm: 0.0732, Train Linf Norm: 14.5750, Test Linf Norm: 3.9176\n",
            "Epoch 148: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1917, Test L1 Norm: 0.0732, Train Linf Norm: 15.3373, Test Linf Norm: 3.9183\n",
            "Epoch 149: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1915, Test L1 Norm: 0.0733, Train Linf Norm: 15.3249, Test Linf Norm: 3.9208\n",
            "Epoch 150: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1918, Test L1 Norm: 0.0732, Train Linf Norm: 15.5589, Test Linf Norm: 3.9172\n",
            "Epoch 151: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1915, Test L1 Norm: 0.0732, Train Linf Norm: 15.4911, Test Linf Norm: 3.9166\n",
            "Epoch 152: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1915, Test L1 Norm: 0.0732, Train Linf Norm: 15.4737, Test Linf Norm: 3.9159\n",
            "Epoch 153: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1915, Test L1 Norm: 0.0732, Train Linf Norm: 15.4800, Test Linf Norm: 3.9157\n",
            "Epoch 154: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1915, Test L1 Norm: 0.0732, Train Linf Norm: 15.4648, Test Linf Norm: 3.9162\n",
            "Epoch 155: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1915, Test L1 Norm: 0.0732, Train Linf Norm: 15.4344, Test Linf Norm: 3.9155\n",
            "Epoch 156: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1915, Test L1 Norm: 0.0732, Train Linf Norm: 15.3037, Test Linf Norm: 3.9153\n",
            "Epoch 157: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1915, Test L1 Norm: 0.0732, Train Linf Norm: 15.3993, Test Linf Norm: 3.9157\n",
            "Epoch 158: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1915, Test L1 Norm: 0.0732, Train Linf Norm: 15.5099, Test Linf Norm: 3.9159\n",
            "Epoch 159: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4699, Test Linf Norm: 3.9157\n",
            "Epoch 160: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1915, Test L1 Norm: 0.0732, Train Linf Norm: 15.4908, Test Linf Norm: 3.9157\n",
            "Epoch 161: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4764, Test Linf Norm: 3.9162\n",
            "Epoch 162: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1915, Test L1 Norm: 0.0732, Train Linf Norm: 15.5375, Test Linf Norm: 3.9154\n",
            "Epoch 163: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1915, Test L1 Norm: 0.0732, Train Linf Norm: 15.4070, Test Linf Norm: 3.9152\n",
            "Epoch 164: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1915, Test L1 Norm: 0.0732, Train Linf Norm: 15.2995, Test Linf Norm: 3.9158\n",
            "Epoch 165: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1915, Test L1 Norm: 0.0732, Train Linf Norm: 15.4895, Test Linf Norm: 3.9158\n",
            "Epoch 166: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1915, Test L1 Norm: 0.0732, Train Linf Norm: 15.4814, Test Linf Norm: 3.9157\n",
            "Epoch 167: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1915, Test L1 Norm: 0.0732, Train Linf Norm: 15.4321, Test Linf Norm: 3.9157\n",
            "Epoch 168: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1915, Test L1 Norm: 0.0732, Train Linf Norm: 15.4654, Test Linf Norm: 3.9157\n",
            "Epoch 169: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1915, Test L1 Norm: 0.0732, Train Linf Norm: 15.3867, Test Linf Norm: 3.9156\n",
            "Epoch 170: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1915, Test L1 Norm: 0.0732, Train Linf Norm: 15.4572, Test Linf Norm: 3.9155\n",
            "Epoch 171: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.2473, Test Linf Norm: 3.9155\n",
            "Epoch 172: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4442, Test Linf Norm: 3.9155\n",
            "Epoch 173: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4768, Test Linf Norm: 3.9154\n",
            "Epoch 174: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.5186, Test Linf Norm: 3.9154\n",
            "Epoch 175: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4287, Test Linf Norm: 3.9154\n",
            "Epoch 176: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.5282, Test Linf Norm: 3.9153\n",
            "Epoch 177: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.2724, Test Linf Norm: 3.9153\n",
            "Epoch 178: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4669, Test Linf Norm: 3.9153\n",
            "Epoch 179: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4760, Test Linf Norm: 3.9152\n",
            "Epoch 180: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.5098, Test Linf Norm: 3.9151\n",
            "Epoch 181: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.3784, Test Linf Norm: 3.9151\n",
            "Epoch 182: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.3597, Test Linf Norm: 3.9151\n",
            "Epoch 183: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.3280, Test Linf Norm: 3.9151\n",
            "Epoch 184: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4450, Test Linf Norm: 3.9151\n",
            "Epoch 185: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 14.5663, Test Linf Norm: 3.9151\n",
            "Epoch 186: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.5345, Test Linf Norm: 3.9151\n",
            "Epoch 187: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4688, Test Linf Norm: 3.9151\n",
            "Epoch 188: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.1491, Test Linf Norm: 3.9151\n",
            "Epoch 189: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.3515, Test Linf Norm: 3.9151\n",
            "Epoch 190: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.3707, Test Linf Norm: 3.9151\n",
            "Epoch 191: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.5623, Test Linf Norm: 3.9151\n",
            "Epoch 192: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.3525, Test Linf Norm: 3.9151\n",
            "Epoch 193: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4521, Test Linf Norm: 3.9151\n",
            "Epoch 194: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4398, Test Linf Norm: 3.9151\n",
            "Epoch 195: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4937, Test Linf Norm: 3.9151\n",
            "Epoch 196: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.3905, Test Linf Norm: 3.9151\n",
            "Epoch 197: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4970, Test Linf Norm: 3.9151\n",
            "Epoch 198: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4806, Test Linf Norm: 3.9151\n",
            "Epoch 199: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4173, Test Linf Norm: 3.9151\n",
            "Epoch 200: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4824, Test Linf Norm: 3.9151\n",
            "Epoch 201: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.5007, Test Linf Norm: 3.9151\n",
            "Epoch 202: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4929, Test Linf Norm: 3.9151\n",
            "Epoch 203: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.5036, Test Linf Norm: 3.9151\n",
            "Epoch 204: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4594, Test Linf Norm: 3.9151\n",
            "Epoch 205: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.3476, Test Linf Norm: 3.9151\n",
            "Epoch 206: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.3456, Test Linf Norm: 3.9151\n",
            "Epoch 207: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.5036, Test Linf Norm: 3.9151\n",
            "Epoch 208: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4108, Test Linf Norm: 3.9151\n",
            "Epoch 209: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.5487, Test Linf Norm: 3.9151\n",
            "Epoch 210: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4910, Test Linf Norm: 3.9151\n",
            "Epoch 211: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4106, Test Linf Norm: 3.9151\n",
            "Epoch 212: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4983, Test Linf Norm: 3.9151\n",
            "Epoch 213: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.5449, Test Linf Norm: 3.9151\n",
            "Epoch 214: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4245, Test Linf Norm: 3.9151\n",
            "Epoch 215: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.3903, Test Linf Norm: 3.9151\n",
            "Epoch 216: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.2652, Test Linf Norm: 3.9151\n",
            "Epoch 217: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.5024, Test Linf Norm: 3.9151\n",
            "Epoch 218: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4517, Test Linf Norm: 3.9151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 16:53:09,772]\u001b[0m Trial 4 finished with value: 0.07319129514694214 and parameters: {'n_layers': 4, 'n_units_0': 1604, 'n_units_1': 1319, 'n_units_2': 1303, 'n_units_3': 1895, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 0.003928099443819886, 'batch_size': 96, 'n_epochs': 219, 'scheduler': 'StepLR', 'weight_decay': 0.0033224537573659793, 'beta1': 0.9403029976537491, 'beta2': 0.9990741796651593, 'step_size': 15, 'gamma': 0.23436306503247795}. Best is trial 2 with value: 0.011449855588376521.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 219: Train Loss: 0.0035, Test Loss: 0.0037, Train L1 Norm: 0.1914, Test L1 Norm: 0.0732, Train Linf Norm: 15.4987, Test Linf Norm: 3.9151\n",
            "Epoch 1: Train Loss: 4.1280, Test Loss: 3.4549, Train L1 Norm: 1.6103, Test L1 Norm: 1.0000, Train Linf Norm: 8.4887, Test Linf Norm: 1.0000\n",
            "Epoch 2: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 3: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 4: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 5: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 6: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 7: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 16:55:01,757]\u001b[0m Trial 5 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.3123, Test Loss: 0.2453, Train L1 Norm: 2.7720, Test L1 Norm: 0.6039, Train Linf Norm: 290.6254, Test Linf Norm: 38.5936\n",
            "Epoch 2: Train Loss: 0.0676, Test Loss: 0.0306, Train L1 Norm: 0.9416, Test L1 Norm: 0.2441, Train Linf Norm: 99.0060, Test Linf Norm: 16.9471\n",
            "Epoch 3: Train Loss: 0.0333, Test Loss: 0.0282, Train L1 Norm: 0.5732, Test L1 Norm: 0.2027, Train Linf Norm: 59.6654, Test Linf Norm: 13.7416\n",
            "Epoch 4: Train Loss: 0.0249, Test Loss: 0.0500, Train L1 Norm: 0.5007, Test L1 Norm: 0.2125, Train Linf Norm: 51.7142, Test Linf Norm: 12.0125\n",
            "Epoch 5: Train Loss: 0.0204, Test Loss: 0.1441, Train L1 Norm: 0.4983, Test L1 Norm: 0.2176, Train Linf Norm: 52.8787, Test Linf Norm: 9.1624\n",
            "Epoch 6: Train Loss: 0.0201, Test Loss: 0.0170, Train L1 Norm: 0.4438, Test L1 Norm: 0.1777, Train Linf Norm: 46.0230, Test Linf Norm: 12.0985\n",
            "Epoch 7: Train Loss: 0.0183, Test Loss: 0.0043, Train L1 Norm: 0.4991, Test L1 Norm: 0.1708, Train Linf Norm: 53.5200, Test Linf Norm: 13.2081\n",
            "Epoch 8: Train Loss: 0.0162, Test Loss: 0.0295, Train L1 Norm: 0.5242, Test L1 Norm: 0.1904, Train Linf Norm: 56.2563, Test Linf Norm: 12.6173\n",
            "Epoch 9: Train Loss: 0.0187, Test Loss: 0.0507, Train L1 Norm: 0.5492, Test L1 Norm: 0.2135, Train Linf Norm: 60.2068, Test Linf Norm: 13.5678\n",
            "Epoch 10: Train Loss: 0.0119, Test Loss: 0.0045, Train L1 Norm: 0.4301, Test L1 Norm: 0.1860, Train Linf Norm: 46.7011, Test Linf Norm: 14.0702\n",
            "Epoch 11: Train Loss: 0.0118, Test Loss: 0.0050, Train L1 Norm: 0.4230, Test L1 Norm: 0.1143, Train Linf Norm: 45.8978, Test Linf Norm: 7.7176\n",
            "Epoch 12: Train Loss: 0.0103, Test Loss: 0.0017, Train L1 Norm: 0.3213, Test L1 Norm: 0.1040, Train Linf Norm: 33.4486, Test Linf Norm: 7.6912\n",
            "Epoch 13: Train Loss: 0.0103, Test Loss: 0.0026, Train L1 Norm: 0.3312, Test L1 Norm: 0.0959, Train Linf Norm: 34.4536, Test Linf Norm: 6.8298\n",
            "Epoch 14: Train Loss: 0.0088, Test Loss: 0.0031, Train L1 Norm: 0.3186, Test L1 Norm: 0.1034, Train Linf Norm: 33.6719, Test Linf Norm: 6.9187\n",
            "Epoch 15: Train Loss: 0.0123, Test Loss: 0.0176, Train L1 Norm: 0.3783, Test L1 Norm: 0.0979, Train Linf Norm: 40.1125, Test Linf Norm: 5.3312\n",
            "Epoch 16: Train Loss: 0.0015, Test Loss: 0.0013, Train L1 Norm: 0.2570, Test L1 Norm: 0.0846, Train Linf Norm: 27.8051, Test Linf Norm: 5.8542\n",
            "Epoch 17: Train Loss: 0.0012, Test Loss: 0.0010, Train L1 Norm: 0.2571, Test L1 Norm: 0.0916, Train Linf Norm: 27.5662, Test Linf Norm: 6.3857\n",
            "Epoch 18: Train Loss: 0.0012, Test Loss: 0.0006, Train L1 Norm: 0.2490, Test L1 Norm: 0.0922, Train Linf Norm: 26.3635, Test Linf Norm: 6.5645\n",
            "Epoch 19: Train Loss: 0.0013, Test Loss: 0.0010, Train L1 Norm: 0.2330, Test L1 Norm: 0.0953, Train Linf Norm: 24.5152, Test Linf Norm: 6.6237\n",
            "Epoch 20: Train Loss: 0.0011, Test Loss: 0.0007, Train L1 Norm: 0.2430, Test L1 Norm: 0.0868, Train Linf Norm: 25.7632, Test Linf Norm: 6.0653\n",
            "Epoch 21: Train Loss: 0.0011, Test Loss: 0.0008, Train L1 Norm: 0.2112, Test L1 Norm: 0.0856, Train Linf Norm: 21.6231, Test Linf Norm: 5.8996\n",
            "Epoch 22: Train Loss: 0.0012, Test Loss: 0.0009, Train L1 Norm: 0.2279, Test L1 Norm: 0.0853, Train Linf Norm: 23.7066, Test Linf Norm: 5.9911\n",
            "Epoch 23: Train Loss: 0.0010, Test Loss: 0.0005, Train L1 Norm: 0.2233, Test L1 Norm: 0.0815, Train Linf Norm: 23.4201, Test Linf Norm: 5.7268\n",
            "Epoch 24: Train Loss: 0.0009, Test Loss: 0.0065, Train L1 Norm: 0.2157, Test L1 Norm: 0.1167, Train Linf Norm: 22.4826, Test Linf Norm: 7.1639\n",
            "Epoch 25: Train Loss: 0.0013, Test Loss: 0.0007, Train L1 Norm: 0.1990, Test L1 Norm: 0.0817, Train Linf Norm: 20.4055, Test Linf Norm: 5.5768\n",
            "Epoch 26: Train Loss: 0.0010, Test Loss: 0.0008, Train L1 Norm: 0.2038, Test L1 Norm: 0.0796, Train Linf Norm: 21.0411, Test Linf Norm: 5.4066\n",
            "Epoch 27: Train Loss: 0.0013, Test Loss: 0.0008, Train L1 Norm: 0.1873, Test L1 Norm: 0.0759, Train Linf Norm: 19.1820, Test Linf Norm: 5.1607\n",
            "Epoch 28: Train Loss: 0.0011, Test Loss: 0.0005, Train L1 Norm: 0.1918, Test L1 Norm: 0.0795, Train Linf Norm: 19.6667, Test Linf Norm: 5.5508\n",
            "Epoch 29: Train Loss: 0.0016, Test Loss: 0.0007, Train L1 Norm: 0.2041, Test L1 Norm: 0.0767, Train Linf Norm: 21.1618, Test Linf Norm: 5.3400\n",
            "Epoch 30: Train Loss: 0.0011, Test Loss: 0.0006, Train L1 Norm: 0.1779, Test L1 Norm: 0.0761, Train Linf Norm: 17.9044, Test Linf Norm: 5.2463\n",
            "Epoch 31: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.1850, Test L1 Norm: 0.0779, Train Linf Norm: 18.8315, Test Linf Norm: 5.4326\n",
            "Epoch 32: Train Loss: 0.0006, Test Loss: 0.0016, Train L1 Norm: 0.1882, Test L1 Norm: 0.0803, Train Linf Norm: 19.5027, Test Linf Norm: 5.4367\n",
            "Epoch 33: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.1839, Test L1 Norm: 0.0820, Train Linf Norm: 18.7706, Test Linf Norm: 5.6854\n",
            "Epoch 34: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.1878, Test L1 Norm: 0.0793, Train Linf Norm: 19.2445, Test Linf Norm: 5.5359\n",
            "Epoch 35: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.1832, Test L1 Norm: 0.0787, Train Linf Norm: 18.5601, Test Linf Norm: 5.5008\n",
            "Epoch 36: Train Loss: 0.0005, Test Loss: 0.0023, Train L1 Norm: 0.1853, Test L1 Norm: 0.0818, Train Linf Norm: 18.9822, Test Linf Norm: 5.4001\n",
            "Epoch 37: Train Loss: 0.0005, Test Loss: 0.0007, Train L1 Norm: 0.1835, Test L1 Norm: 0.0786, Train Linf Norm: 18.9477, Test Linf Norm: 5.4059\n",
            "Epoch 38: Train Loss: 0.0006, Test Loss: 0.0004, Train L1 Norm: 0.1795, Test L1 Norm: 0.0773, Train Linf Norm: 18.3880, Test Linf Norm: 5.3706\n",
            "Epoch 39: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.1840, Test L1 Norm: 0.0775, Train Linf Norm: 18.7510, Test Linf Norm: 5.3808\n",
            "Epoch 40: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.1805, Test L1 Norm: 0.0761, Train Linf Norm: 17.8324, Test Linf Norm: 5.2083\n",
            "Epoch 41: Train Loss: 0.0006, Test Loss: 0.0004, Train L1 Norm: 0.1814, Test L1 Norm: 0.0758, Train Linf Norm: 18.7190, Test Linf Norm: 5.2398\n",
            "Epoch 42: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.1729, Test L1 Norm: 0.0779, Train Linf Norm: 17.5812, Test Linf Norm: 5.3684\n",
            "Epoch 43: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.1688, Test L1 Norm: 0.0753, Train Linf Norm: 17.0583, Test Linf Norm: 5.1874\n",
            "Epoch 44: Train Loss: 0.0005, Test Loss: 0.0011, Train L1 Norm: 0.1684, Test L1 Norm: 0.0842, Train Linf Norm: 17.0616, Test Linf Norm: 5.5876\n",
            "Epoch 45: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.1631, Test L1 Norm: 0.0743, Train Linf Norm: 16.4469, Test Linf Norm: 5.1167\n",
            "Epoch 46: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1615, Test L1 Norm: 0.0744, Train Linf Norm: 15.9718, Test Linf Norm: 5.1434\n",
            "Epoch 47: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1660, Test L1 Norm: 0.0736, Train Linf Norm: 16.9440, Test Linf Norm: 5.0502\n",
            "Epoch 48: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1620, Test L1 Norm: 0.0758, Train Linf Norm: 15.9529, Test Linf Norm: 5.2177\n",
            "Epoch 49: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1601, Test L1 Norm: 0.0746, Train Linf Norm: 16.0421, Test Linf Norm: 5.1364\n",
            "Epoch 50: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1620, Test L1 Norm: 0.0736, Train Linf Norm: 16.3521, Test Linf Norm: 5.0462\n",
            "Epoch 51: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1613, Test L1 Norm: 0.0743, Train Linf Norm: 16.2397, Test Linf Norm: 5.1233\n",
            "Epoch 52: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1609, Test L1 Norm: 0.0738, Train Linf Norm: 16.2171, Test Linf Norm: 5.0634\n",
            "Epoch 53: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1609, Test L1 Norm: 0.0736, Train Linf Norm: 16.0616, Test Linf Norm: 5.0361\n",
            "Epoch 54: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1584, Test L1 Norm: 0.0736, Train Linf Norm: 15.9280, Test Linf Norm: 5.0567\n",
            "Epoch 55: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1583, Test L1 Norm: 0.0736, Train Linf Norm: 15.8566, Test Linf Norm: 5.0713\n",
            "Epoch 56: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1577, Test L1 Norm: 0.0739, Train Linf Norm: 15.6582, Test Linf Norm: 5.0749\n",
            "Epoch 57: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1568, Test L1 Norm: 0.0734, Train Linf Norm: 15.6431, Test Linf Norm: 5.0553\n",
            "Epoch 58: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1589, Test L1 Norm: 0.0730, Train Linf Norm: 15.8205, Test Linf Norm: 5.0002\n",
            "Epoch 59: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1555, Test L1 Norm: 0.0729, Train Linf Norm: 15.1810, Test Linf Norm: 4.9968\n",
            "Epoch 60: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1561, Test L1 Norm: 0.0727, Train Linf Norm: 15.4931, Test Linf Norm: 4.9828\n",
            "Epoch 61: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1551, Test L1 Norm: 0.0727, Train Linf Norm: 15.5137, Test Linf Norm: 4.9851\n",
            "Epoch 62: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1548, Test L1 Norm: 0.0730, Train Linf Norm: 15.5014, Test Linf Norm: 5.0127\n",
            "Epoch 63: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1536, Test L1 Norm: 0.0738, Train Linf Norm: 15.4221, Test Linf Norm: 5.0585\n",
            "Epoch 64: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1541, Test L1 Norm: 0.0724, Train Linf Norm: 15.4206, Test Linf Norm: 4.9562\n",
            "Epoch 65: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1552, Test L1 Norm: 0.0725, Train Linf Norm: 15.6094, Test Linf Norm: 4.9649\n",
            "Epoch 66: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1540, Test L1 Norm: 0.0727, Train Linf Norm: 15.3646, Test Linf Norm: 4.9826\n",
            "Epoch 67: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1537, Test L1 Norm: 0.0724, Train Linf Norm: 15.2952, Test Linf Norm: 4.9591\n",
            "Epoch 68: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1539, Test L1 Norm: 0.0736, Train Linf Norm: 15.2431, Test Linf Norm: 5.0423\n",
            "Epoch 69: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1543, Test L1 Norm: 0.0723, Train Linf Norm: 15.1530, Test Linf Norm: 4.9443\n",
            "Epoch 70: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1543, Test L1 Norm: 0.0722, Train Linf Norm: 15.4788, Test Linf Norm: 4.9349\n",
            "Epoch 71: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1524, Test L1 Norm: 0.0730, Train Linf Norm: 15.1195, Test Linf Norm: 5.0028\n",
            "Epoch 72: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1529, Test L1 Norm: 0.0723, Train Linf Norm: 15.1259, Test Linf Norm: 4.9498\n",
            "Epoch 73: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1525, Test L1 Norm: 0.0727, Train Linf Norm: 15.1303, Test Linf Norm: 4.9819\n",
            "Epoch 74: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1531, Test L1 Norm: 0.0721, Train Linf Norm: 15.0408, Test Linf Norm: 4.9336\n",
            "Epoch 75: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1523, Test L1 Norm: 0.0722, Train Linf Norm: 15.1587, Test Linf Norm: 4.9454\n",
            "Epoch 76: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1516, Test L1 Norm: 0.0723, Train Linf Norm: 15.1511, Test Linf Norm: 4.9551\n",
            "Epoch 77: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1520, Test L1 Norm: 0.0724, Train Linf Norm: 14.6952, Test Linf Norm: 4.9543\n",
            "Epoch 78: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1517, Test L1 Norm: 0.0722, Train Linf Norm: 15.1752, Test Linf Norm: 4.9425\n",
            "Epoch 79: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1519, Test L1 Norm: 0.0723, Train Linf Norm: 15.0674, Test Linf Norm: 4.9515\n",
            "Epoch 80: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1515, Test L1 Norm: 0.0722, Train Linf Norm: 15.1068, Test Linf Norm: 4.9405\n",
            "Epoch 81: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1518, Test L1 Norm: 0.0721, Train Linf Norm: 15.1221, Test Linf Norm: 4.9320\n",
            "Epoch 82: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1510, Test L1 Norm: 0.0724, Train Linf Norm: 14.9805, Test Linf Norm: 4.9623\n",
            "Epoch 83: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1512, Test L1 Norm: 0.0720, Train Linf Norm: 14.9981, Test Linf Norm: 4.9196\n",
            "Epoch 84: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1509, Test L1 Norm: 0.0722, Train Linf Norm: 15.0423, Test Linf Norm: 4.9497\n",
            "Epoch 85: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1515, Test L1 Norm: 0.0721, Train Linf Norm: 15.1319, Test Linf Norm: 4.9374\n",
            "Epoch 86: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1513, Test L1 Norm: 0.0719, Train Linf Norm: 15.0901, Test Linf Norm: 4.9157\n",
            "Epoch 87: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1510, Test L1 Norm: 0.0720, Train Linf Norm: 14.9072, Test Linf Norm: 4.9259\n",
            "Epoch 88: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1506, Test L1 Norm: 0.0722, Train Linf Norm: 14.9929, Test Linf Norm: 4.9412\n",
            "Epoch 89: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1505, Test L1 Norm: 0.0720, Train Linf Norm: 14.6961, Test Linf Norm: 4.9283\n",
            "Epoch 90: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1510, Test L1 Norm: 0.0718, Train Linf Norm: 15.0433, Test Linf Norm: 4.9089\n",
            "Epoch 91: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1504, Test L1 Norm: 0.0721, Train Linf Norm: 14.9713, Test Linf Norm: 4.9378\n",
            "Epoch 92: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1504, Test L1 Norm: 0.0720, Train Linf Norm: 15.0073, Test Linf Norm: 4.9237\n",
            "Epoch 93: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1505, Test L1 Norm: 0.0719, Train Linf Norm: 14.9846, Test Linf Norm: 4.9183\n",
            "Epoch 94: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1505, Test L1 Norm: 0.0719, Train Linf Norm: 15.0801, Test Linf Norm: 4.9206\n",
            "Epoch 95: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1506, Test L1 Norm: 0.0719, Train Linf Norm: 14.9482, Test Linf Norm: 4.9208\n",
            "Epoch 96: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1505, Test L1 Norm: 0.0720, Train Linf Norm: 14.8931, Test Linf Norm: 4.9310\n",
            "Epoch 97: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1506, Test L1 Norm: 0.0720, Train Linf Norm: 14.8723, Test Linf Norm: 4.9324\n",
            "Epoch 98: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1505, Test L1 Norm: 0.0719, Train Linf Norm: 14.8959, Test Linf Norm: 4.9245\n",
            "Epoch 99: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1502, Test L1 Norm: 0.0719, Train Linf Norm: 14.9513, Test Linf Norm: 4.9147\n",
            "Epoch 100: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1504, Test L1 Norm: 0.0720, Train Linf Norm: 14.9120, Test Linf Norm: 4.9296\n",
            "Epoch 101: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1500, Test L1 Norm: 0.0719, Train Linf Norm: 14.9527, Test Linf Norm: 4.9204\n",
            "Epoch 102: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1501, Test L1 Norm: 0.0719, Train Linf Norm: 14.7403, Test Linf Norm: 4.9189\n",
            "Epoch 103: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1502, Test L1 Norm: 0.0719, Train Linf Norm: 14.8522, Test Linf Norm: 4.9194\n",
            "Epoch 104: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1502, Test L1 Norm: 0.0719, Train Linf Norm: 15.0528, Test Linf Norm: 4.9202\n",
            "Epoch 105: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1501, Test L1 Norm: 0.0719, Train Linf Norm: 14.8863, Test Linf Norm: 4.9171\n",
            "Epoch 106: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1501, Test L1 Norm: 0.0719, Train Linf Norm: 14.7656, Test Linf Norm: 4.9214\n",
            "Epoch 107: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1500, Test L1 Norm: 0.0718, Train Linf Norm: 14.9935, Test Linf Norm: 4.9116\n",
            "Epoch 108: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1499, Test L1 Norm: 0.0719, Train Linf Norm: 14.7418, Test Linf Norm: 4.9223\n",
            "Epoch 109: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1499, Test L1 Norm: 0.0719, Train Linf Norm: 14.9076, Test Linf Norm: 4.9218\n",
            "Epoch 110: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1498, Test L1 Norm: 0.0719, Train Linf Norm: 14.8368, Test Linf Norm: 4.9179\n",
            "Epoch 111: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1501, Test L1 Norm: 0.0719, Train Linf Norm: 14.4388, Test Linf Norm: 4.9168\n",
            "Epoch 112: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1498, Test L1 Norm: 0.0720, Train Linf Norm: 14.8215, Test Linf Norm: 4.9241\n",
            "Epoch 113: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1501, Test L1 Norm: 0.0719, Train Linf Norm: 14.8867, Test Linf Norm: 4.9186\n",
            "Epoch 114: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1500, Test L1 Norm: 0.0719, Train Linf Norm: 14.7527, Test Linf Norm: 4.9181\n",
            "Epoch 115: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1500, Test L1 Norm: 0.0720, Train Linf Norm: 14.9984, Test Linf Norm: 4.9275\n",
            "Epoch 116: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1502, Test L1 Norm: 0.0720, Train Linf Norm: 14.6724, Test Linf Norm: 4.9215\n",
            "Epoch 117: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1501, Test L1 Norm: 0.0720, Train Linf Norm: 14.6870, Test Linf Norm: 4.9198\n",
            "Epoch 118: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1501, Test L1 Norm: 0.0720, Train Linf Norm: 14.7694, Test Linf Norm: 4.9216\n",
            "Epoch 119: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1502, Test L1 Norm: 0.0720, Train Linf Norm: 14.8878, Test Linf Norm: 4.9215\n",
            "Epoch 120: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1501, Test L1 Norm: 0.0720, Train Linf Norm: 14.9335, Test Linf Norm: 4.9212\n",
            "Epoch 121: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1502, Test L1 Norm: 0.0720, Train Linf Norm: 14.9284, Test Linf Norm: 4.9203\n",
            "Epoch 122: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1502, Test L1 Norm: 0.0720, Train Linf Norm: 14.5526, Test Linf Norm: 4.9200\n",
            "Epoch 123: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1501, Test L1 Norm: 0.0720, Train Linf Norm: 14.9362, Test Linf Norm: 4.9225\n",
            "Epoch 124: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1502, Test L1 Norm: 0.0720, Train Linf Norm: 14.9813, Test Linf Norm: 4.9204\n",
            "Epoch 125: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1502, Test L1 Norm: 0.0720, Train Linf Norm: 14.8091, Test Linf Norm: 4.9222\n",
            "Epoch 126: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1502, Test L1 Norm: 0.0720, Train Linf Norm: 14.6515, Test Linf Norm: 4.9227\n",
            "Epoch 127: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1502, Test L1 Norm: 0.0720, Train Linf Norm: 14.8651, Test Linf Norm: 4.9220\n",
            "Epoch 128: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1502, Test L1 Norm: 0.0720, Train Linf Norm: 14.9328, Test Linf Norm: 4.9238\n",
            "Epoch 129: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1503, Test L1 Norm: 0.0720, Train Linf Norm: 14.8462, Test Linf Norm: 4.9243\n",
            "Epoch 130: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1503, Test L1 Norm: 0.0720, Train Linf Norm: 14.8993, Test Linf Norm: 4.9251\n",
            "Epoch 131: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1503, Test L1 Norm: 0.0720, Train Linf Norm: 14.8223, Test Linf Norm: 4.9237\n",
            "Epoch 132: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1503, Test L1 Norm: 0.0720, Train Linf Norm: 14.9471, Test Linf Norm: 4.9248\n",
            "Epoch 133: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1502, Test L1 Norm: 0.0721, Train Linf Norm: 15.0372, Test Linf Norm: 4.9268\n",
            "Epoch 134: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1504, Test L1 Norm: 0.0720, Train Linf Norm: 14.8912, Test Linf Norm: 4.9220\n",
            "Epoch 135: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1503, Test L1 Norm: 0.0720, Train Linf Norm: 14.6535, Test Linf Norm: 4.9211\n",
            "Epoch 136: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1502, Test L1 Norm: 0.0720, Train Linf Norm: 14.9877, Test Linf Norm: 4.9224\n",
            "Epoch 137: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1503, Test L1 Norm: 0.0720, Train Linf Norm: 14.7841, Test Linf Norm: 4.9227\n",
            "Epoch 138: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1503, Test L1 Norm: 0.0720, Train Linf Norm: 14.9153, Test Linf Norm: 4.9232\n",
            "Epoch 139: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1503, Test L1 Norm: 0.0720, Train Linf Norm: 15.0233, Test Linf Norm: 4.9235\n",
            "Epoch 140: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1504, Test L1 Norm: 0.0720, Train Linf Norm: 15.0225, Test Linf Norm: 4.9233\n",
            "Epoch 141: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1503, Test L1 Norm: 0.0720, Train Linf Norm: 14.8960, Test Linf Norm: 4.9235\n",
            "Epoch 142: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1503, Test L1 Norm: 0.0720, Train Linf Norm: 14.9893, Test Linf Norm: 4.9235\n",
            "Epoch 143: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1503, Test L1 Norm: 0.0720, Train Linf Norm: 14.6028, Test Linf Norm: 4.9239\n",
            "Epoch 144: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1504, Test L1 Norm: 0.0720, Train Linf Norm: 14.9740, Test Linf Norm: 4.9239\n",
            "Epoch 145: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1504, Test L1 Norm: 0.0720, Train Linf Norm: 14.8872, Test Linf Norm: 4.9236\n",
            "Epoch 146: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1503, Test L1 Norm: 0.0720, Train Linf Norm: 14.8313, Test Linf Norm: 4.9239\n",
            "Epoch 147: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1503, Test L1 Norm: 0.0720, Train Linf Norm: 14.9702, Test Linf Norm: 4.9239\n",
            "Epoch 148: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1504, Test L1 Norm: 0.0720, Train Linf Norm: 14.8664, Test Linf Norm: 4.9237\n",
            "Epoch 149: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1503, Test L1 Norm: 0.0720, Train Linf Norm: 14.8854, Test Linf Norm: 4.9237\n",
            "Epoch 150: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1503, Test L1 Norm: 0.0720, Train Linf Norm: 14.9806, Test Linf Norm: 4.9239\n",
            "Epoch 151: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1503, Test L1 Norm: 0.0720, Train Linf Norm: 14.8590, Test Linf Norm: 4.9238\n",
            "Epoch 152: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1503, Test L1 Norm: 0.0720, Train Linf Norm: 15.0479, Test Linf Norm: 4.9239\n",
            "Epoch 153: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1503, Test L1 Norm: 0.0720, Train Linf Norm: 14.9486, Test Linf Norm: 4.9239\n",
            "Epoch 154: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1504, Test L1 Norm: 0.0720, Train Linf Norm: 14.9689, Test Linf Norm: 4.9240\n",
            "Epoch 155: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1503, Test L1 Norm: 0.0720, Train Linf Norm: 14.9081, Test Linf Norm: 4.9239\n",
            "Epoch 156: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1503, Test L1 Norm: 0.0720, Train Linf Norm: 14.8091, Test Linf Norm: 4.9239\n",
            "Epoch 157: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1504, Test L1 Norm: 0.0720, Train Linf Norm: 14.7025, Test Linf Norm: 4.9240\n",
            "Epoch 158: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1504, Test L1 Norm: 0.0720, Train Linf Norm: 14.9384, Test Linf Norm: 4.9240\n",
            "Epoch 159: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1504, Test L1 Norm: 0.0720, Train Linf Norm: 14.9696, Test Linf Norm: 4.9240\n",
            "Epoch 160: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1504, Test L1 Norm: 0.0720, Train Linf Norm: 14.8931, Test Linf Norm: 4.9241\n",
            "Epoch 161: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1504, Test L1 Norm: 0.0720, Train Linf Norm: 14.8473, Test Linf Norm: 4.9241\n",
            "Epoch 162: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1504, Test L1 Norm: 0.0720, Train Linf Norm: 14.8979, Test Linf Norm: 4.9241\n",
            "Epoch 163: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1504, Test L1 Norm: 0.0720, Train Linf Norm: 14.8351, Test Linf Norm: 4.9241\n",
            "Epoch 164: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1504, Test L1 Norm: 0.0720, Train Linf Norm: 14.9593, Test Linf Norm: 4.9242\n",
            "Epoch 165: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1504, Test L1 Norm: 0.0720, Train Linf Norm: 14.8573, Test Linf Norm: 4.9241\n",
            "Epoch 166: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1504, Test L1 Norm: 0.0720, Train Linf Norm: 14.9073, Test Linf Norm: 4.9241\n",
            "Epoch 167: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1504, Test L1 Norm: 0.0720, Train Linf Norm: 14.7546, Test Linf Norm: 4.9241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 17:04:28,719]\u001b[0m Trial 6 finished with value: 0.0720300185918808 and parameters: {'n_layers': 10, 'n_units_0': 1838, 'n_units_1': 198, 'n_units_2': 2008, 'n_units_3': 252, 'n_units_4': 1322, 'n_units_5': 1129, 'n_units_6': 653, 'n_units_7': 446, 'n_units_8': 762, 'n_units_9': 734, 'hidden_activation': 'ELU', 'output_activation': 'Linear', 'loss': 'Huber', 'optimizer': 'SGD', 'lr': 0.0036800678372090694, 'batch_size': 128, 'n_epochs': 168, 'scheduler': 'StepLR', 'weight_decay': 0.001806277762578809, 'momentum': 0.8134437057890395, 'step_size': 15, 'gamma': 0.35345845841993384}. Best is trial 2 with value: 0.011449855588376521.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 168: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1504, Test L1 Norm: 0.0720, Train Linf Norm: 14.8462, Test Linf Norm: 4.9241\n",
            "Epoch 1: Train Loss: 0.4480, Test Loss: 0.0391, Train L1 Norm: 1.6150, Test L1 Norm: 0.2516, Train Linf Norm: 283.1534, Test Linf Norm: 27.3769\n",
            "Epoch 2: Train Loss: 0.0795, Test Loss: 0.0221, Train L1 Norm: 0.8404, Test L1 Norm: 0.2042, Train Linf Norm: 171.3944, Test Linf Norm: 20.5254\n",
            "Epoch 3: Train Loss: 0.0315, Test Loss: 0.0054, Train L1 Norm: 0.4027, Test L1 Norm: 0.0862, Train Linf Norm: 77.6322, Test Linf Norm: 9.3801\n",
            "Epoch 4: Train Loss: 0.0163, Test Loss: 0.0099, Train L1 Norm: 0.1849, Test L1 Norm: 0.0640, Train Linf Norm: 29.7990, Test Linf Norm: 4.1135\n",
            "Epoch 5: Train Loss: 0.0094, Test Loss: 0.0031, Train L1 Norm: 0.1365, Test L1 Norm: 0.0846, Train Linf Norm: 21.3933, Test Linf Norm: 9.0816\n",
            "Epoch 6: Train Loss: 0.0064, Test Loss: 0.0087, Train L1 Norm: 0.2146, Test L1 Norm: 0.0607, Train Linf Norm: 43.2239, Test Linf Norm: 4.8778\n",
            "Epoch 7: Train Loss: 0.0051, Test Loss: 0.0041, Train L1 Norm: 0.1291, Test L1 Norm: 0.0430, Train Linf Norm: 22.7035, Test Linf Norm: 2.9937\n",
            "Epoch 8: Train Loss: 0.0049, Test Loss: 0.0030, Train L1 Norm: 0.0922, Test L1 Norm: 0.0460, Train Linf Norm: 14.3046, Test Linf Norm: 4.1717\n",
            "Epoch 9: Train Loss: 0.0009, Test Loss: 0.0008, Train L1 Norm: 0.0725, Test L1 Norm: 0.0370, Train Linf Norm: 12.4469, Test Linf Norm: 3.4491\n",
            "Epoch 10: Train Loss: 0.0007, Test Loss: 0.0007, Train L1 Norm: 0.0622, Test L1 Norm: 0.0342, Train Linf Norm: 10.1204, Test Linf Norm: 3.0462\n",
            "Epoch 11: Train Loss: 0.0006, Test Loss: 0.0007, Train L1 Norm: 0.0578, Test L1 Norm: 0.0357, Train Linf Norm: 9.1414, Test Linf Norm: 3.3317\n",
            "Epoch 12: Train Loss: 0.0006, Test Loss: 0.0007, Train L1 Norm: 0.0555, Test L1 Norm: 0.0324, Train Linf Norm: 8.7282, Test Linf Norm: 2.7635\n",
            "Epoch 13: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0569, Test L1 Norm: 0.0319, Train Linf Norm: 8.7678, Test Linf Norm: 2.7668\n",
            "Epoch 14: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.0527, Test L1 Norm: 0.0301, Train Linf Norm: 8.1814, Test Linf Norm: 2.3726\n",
            "Epoch 15: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.0526, Test L1 Norm: 0.0307, Train Linf Norm: 7.7987, Test Linf Norm: 2.5967\n",
            "Epoch 16: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0474, Test L1 Norm: 0.0307, Train Linf Norm: 6.8809, Test Linf Norm: 2.6625\n",
            "Epoch 17: Train Loss: 0.0004, Test Loss: 0.0005, Train L1 Norm: 0.0465, Test L1 Norm: 0.0302, Train Linf Norm: 6.9168, Test Linf Norm: 2.5821\n",
            "Epoch 18: Train Loss: 0.0004, Test Loss: 0.0005, Train L1 Norm: 0.0446, Test L1 Norm: 0.0301, Train Linf Norm: 6.2845, Test Linf Norm: 2.5962\n",
            "Epoch 19: Train Loss: 0.0004, Test Loss: 0.0005, Train L1 Norm: 0.0448, Test L1 Norm: 0.0296, Train Linf Norm: 6.5370, Test Linf Norm: 2.5129\n",
            "Epoch 20: Train Loss: 0.0004, Test Loss: 0.0005, Train L1 Norm: 0.0439, Test L1 Norm: 0.0301, Train Linf Norm: 6.2199, Test Linf Norm: 2.5955\n",
            "Epoch 21: Train Loss: 0.0004, Test Loss: 0.0005, Train L1 Norm: 0.0443, Test L1 Norm: 0.0299, Train Linf Norm: 6.1892, Test Linf Norm: 2.5729\n",
            "Epoch 22: Train Loss: 0.0004, Test Loss: 0.0005, Train L1 Norm: 0.0441, Test L1 Norm: 0.0300, Train Linf Norm: 6.2853, Test Linf Norm: 2.5955\n",
            "Epoch 23: Train Loss: 0.0004, Test Loss: 0.0005, Train L1 Norm: 0.0439, Test L1 Norm: 0.0298, Train Linf Norm: 6.3342, Test Linf Norm: 2.5768\n",
            "Epoch 24: Train Loss: 0.0004, Test Loss: 0.0005, Train L1 Norm: 0.0432, Test L1 Norm: 0.0299, Train Linf Norm: 6.1709, Test Linf Norm: 2.6151\n",
            "Epoch 25: Train Loss: 0.0004, Test Loss: 0.0005, Train L1 Norm: 0.0421, Test L1 Norm: 0.0297, Train Linf Norm: 5.9281, Test Linf Norm: 2.5710\n",
            "Epoch 26: Train Loss: 0.0004, Test Loss: 0.0005, Train L1 Norm: 0.0426, Test L1 Norm: 0.0295, Train Linf Norm: 5.9924, Test Linf Norm: 2.5330\n",
            "Epoch 27: Train Loss: 0.0004, Test Loss: 0.0005, Train L1 Norm: 0.0427, Test L1 Norm: 0.0294, Train Linf Norm: 5.9915, Test Linf Norm: 2.5311\n",
            "Epoch 28: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0423, Test L1 Norm: 0.0294, Train Linf Norm: 5.9087, Test Linf Norm: 2.5291\n",
            "Epoch 29: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0431, Test L1 Norm: 0.0294, Train Linf Norm: 6.1878, Test Linf Norm: 2.5255\n",
            "Epoch 30: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0420, Test L1 Norm: 0.0294, Train Linf Norm: 5.7297, Test Linf Norm: 2.5235\n",
            "Epoch 31: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0428, Test L1 Norm: 0.0294, Train Linf Norm: 6.0156, Test Linf Norm: 2.5299\n",
            "Epoch 32: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0423, Test L1 Norm: 0.0294, Train Linf Norm: 5.9614, Test Linf Norm: 2.5204\n",
            "Epoch 33: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0417, Test L1 Norm: 0.0294, Train Linf Norm: 5.8114, Test Linf Norm: 2.5293\n",
            "Epoch 34: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0417, Test L1 Norm: 0.0294, Train Linf Norm: 5.8300, Test Linf Norm: 2.5298\n",
            "Epoch 35: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0418, Test L1 Norm: 0.0293, Train Linf Norm: 5.8330, Test Linf Norm: 2.5198\n",
            "Epoch 36: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0418, Test L1 Norm: 0.0294, Train Linf Norm: 5.8180, Test Linf Norm: 2.5226\n",
            "Epoch 37: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0417, Test L1 Norm: 0.0293, Train Linf Norm: 5.8113, Test Linf Norm: 2.5213\n",
            "Epoch 38: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0414, Test L1 Norm: 0.0294, Train Linf Norm: 5.5991, Test Linf Norm: 2.5244\n",
            "Epoch 39: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0417, Test L1 Norm: 0.0294, Train Linf Norm: 5.7836, Test Linf Norm: 2.5249\n",
            "Epoch 40: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0417, Test L1 Norm: 0.0294, Train Linf Norm: 5.5507, Test Linf Norm: 2.5271\n",
            "Epoch 41: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0416, Test L1 Norm: 0.0294, Train Linf Norm: 5.6859, Test Linf Norm: 2.5251\n",
            "Epoch 42: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0416, Test L1 Norm: 0.0293, Train Linf Norm: 5.7868, Test Linf Norm: 2.5234\n",
            "Epoch 43: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7695, Test Linf Norm: 2.5236\n",
            "Epoch 44: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0416, Test L1 Norm: 0.0293, Train Linf Norm: 5.7832, Test Linf Norm: 2.5230\n",
            "Epoch 45: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0416, Test L1 Norm: 0.0293, Train Linf Norm: 5.7373, Test Linf Norm: 2.5224\n",
            "Epoch 46: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0416, Test L1 Norm: 0.0293, Train Linf Norm: 5.6974, Test Linf Norm: 2.5221\n",
            "Epoch 47: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7139, Test Linf Norm: 2.5227\n",
            "Epoch 48: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0416, Test L1 Norm: 0.0293, Train Linf Norm: 5.7445, Test Linf Norm: 2.5222\n",
            "Epoch 49: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0416, Test L1 Norm: 0.0293, Train Linf Norm: 5.8167, Test Linf Norm: 2.5222\n",
            "Epoch 50: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0416, Test L1 Norm: 0.0293, Train Linf Norm: 5.7775, Test Linf Norm: 2.5220\n",
            "Epoch 51: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0416, Test L1 Norm: 0.0293, Train Linf Norm: 5.8518, Test Linf Norm: 2.5220\n",
            "Epoch 52: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8281, Test Linf Norm: 2.5221\n",
            "Epoch 53: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0416, Test L1 Norm: 0.0293, Train Linf Norm: 5.7106, Test Linf Norm: 2.5220\n",
            "Epoch 54: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0416, Test L1 Norm: 0.0293, Train Linf Norm: 5.7647, Test Linf Norm: 2.5220\n",
            "Epoch 55: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7086, Test Linf Norm: 2.5219\n",
            "Epoch 56: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.5079, Test Linf Norm: 2.5219\n",
            "Epoch 57: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6889, Test Linf Norm: 2.5218\n",
            "Epoch 58: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7864, Test Linf Norm: 2.5218\n",
            "Epoch 59: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8148, Test Linf Norm: 2.5218\n",
            "Epoch 60: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7459, Test Linf Norm: 2.5217\n",
            "Epoch 61: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7947, Test Linf Norm: 2.5217\n",
            "Epoch 62: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8576, Test Linf Norm: 2.5217\n",
            "Epoch 63: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6279, Test Linf Norm: 2.5217\n",
            "Epoch 64: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8416, Test Linf Norm: 2.5216\n",
            "Epoch 65: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7323, Test Linf Norm: 2.5216\n",
            "Epoch 66: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7477, Test Linf Norm: 2.5216\n",
            "Epoch 67: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7305, Test Linf Norm: 2.5216\n",
            "Epoch 68: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7763, Test Linf Norm: 2.5216\n",
            "Epoch 69: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6793, Test Linf Norm: 2.5216\n",
            "Epoch 70: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7999, Test Linf Norm: 2.5216\n",
            "Epoch 71: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7024, Test Linf Norm: 2.5216\n",
            "Epoch 72: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7004, Test Linf Norm: 2.5216\n",
            "Epoch 73: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8411, Test Linf Norm: 2.5216\n",
            "Epoch 74: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7022, Test Linf Norm: 2.5216\n",
            "Epoch 75: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.5464, Test Linf Norm: 2.5216\n",
            "Epoch 76: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8395, Test Linf Norm: 2.5216\n",
            "Epoch 77: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8254, Test Linf Norm: 2.5216\n",
            "Epoch 78: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6751, Test Linf Norm: 2.5216\n",
            "Epoch 79: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8603, Test Linf Norm: 2.5216\n",
            "Epoch 80: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7235, Test Linf Norm: 2.5216\n",
            "Epoch 81: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8415, Test Linf Norm: 2.5216\n",
            "Epoch 82: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7121, Test Linf Norm: 2.5216\n",
            "Epoch 83: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7765, Test Linf Norm: 2.5216\n",
            "Epoch 84: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7498, Test Linf Norm: 2.5216\n",
            "Epoch 85: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7523, Test Linf Norm: 2.5216\n",
            "Epoch 86: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8131, Test Linf Norm: 2.5216\n",
            "Epoch 87: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7084, Test Linf Norm: 2.5216\n",
            "Epoch 88: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.5990, Test Linf Norm: 2.5216\n",
            "Epoch 89: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7883, Test Linf Norm: 2.5216\n",
            "Epoch 90: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7040, Test Linf Norm: 2.5216\n",
            "Epoch 91: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6656, Test Linf Norm: 2.5216\n",
            "Epoch 92: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6726, Test Linf Norm: 2.5216\n",
            "Epoch 93: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6466, Test Linf Norm: 2.5216\n",
            "Epoch 94: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7432, Test Linf Norm: 2.5216\n",
            "Epoch 95: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6897, Test Linf Norm: 2.5216\n",
            "Epoch 96: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6518, Test Linf Norm: 2.5216\n",
            "Epoch 97: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7066, Test Linf Norm: 2.5216\n",
            "Epoch 98: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6728, Test Linf Norm: 2.5216\n",
            "Epoch 99: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6028, Test Linf Norm: 2.5216\n",
            "Epoch 100: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7424, Test Linf Norm: 2.5216\n",
            "Epoch 101: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.5544, Test Linf Norm: 2.5216\n",
            "Epoch 102: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7426, Test Linf Norm: 2.5216\n",
            "Epoch 103: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8281, Test Linf Norm: 2.5216\n",
            "Epoch 104: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7740, Test Linf Norm: 2.5216\n",
            "Epoch 105: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8339, Test Linf Norm: 2.5216\n",
            "Epoch 106: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.4945, Test Linf Norm: 2.5216\n",
            "Epoch 107: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7687, Test Linf Norm: 2.5216\n",
            "Epoch 108: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8543, Test Linf Norm: 2.5216\n",
            "Epoch 109: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7905, Test Linf Norm: 2.5216\n",
            "Epoch 110: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8555, Test Linf Norm: 2.5216\n",
            "Epoch 111: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8040, Test Linf Norm: 2.5216\n",
            "Epoch 112: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6515, Test Linf Norm: 2.5216\n",
            "Epoch 113: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6781, Test Linf Norm: 2.5216\n",
            "Epoch 114: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8786, Test Linf Norm: 2.5216\n",
            "Epoch 115: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8170, Test Linf Norm: 2.5216\n",
            "Epoch 116: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6340, Test Linf Norm: 2.5216\n",
            "Epoch 117: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.5890, Test Linf Norm: 2.5216\n",
            "Epoch 118: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7984, Test Linf Norm: 2.5216\n",
            "Epoch 119: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7793, Test Linf Norm: 2.5216\n",
            "Epoch 120: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7053, Test Linf Norm: 2.5216\n",
            "Epoch 121: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6479, Test Linf Norm: 2.5216\n",
            "Epoch 122: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8231, Test Linf Norm: 2.5216\n",
            "Epoch 123: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7872, Test Linf Norm: 2.5216\n",
            "Epoch 124: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8104, Test Linf Norm: 2.5216\n",
            "Epoch 125: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.5055, Test Linf Norm: 2.5216\n",
            "Epoch 126: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7947, Test Linf Norm: 2.5216\n",
            "Epoch 127: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8406, Test Linf Norm: 2.5216\n",
            "Epoch 128: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8008, Test Linf Norm: 2.5216\n",
            "Epoch 129: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8144, Test Linf Norm: 2.5216\n",
            "Epoch 130: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7481, Test Linf Norm: 2.5216\n",
            "Epoch 131: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8304, Test Linf Norm: 2.5216\n",
            "Epoch 132: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7719, Test Linf Norm: 2.5216\n",
            "Epoch 133: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6346, Test Linf Norm: 2.5216\n",
            "Epoch 134: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7885, Test Linf Norm: 2.5216\n",
            "Epoch 135: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7400, Test Linf Norm: 2.5216\n",
            "Epoch 136: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7632, Test Linf Norm: 2.5216\n",
            "Epoch 137: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7549, Test Linf Norm: 2.5216\n",
            "Epoch 138: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.2241, Test Linf Norm: 2.5216\n",
            "Epoch 139: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7254, Test Linf Norm: 2.5216\n",
            "Epoch 140: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7395, Test Linf Norm: 2.5216\n",
            "Epoch 141: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8463, Test Linf Norm: 2.5216\n",
            "Epoch 142: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8067, Test Linf Norm: 2.5216\n",
            "Epoch 143: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6542, Test Linf Norm: 2.5216\n",
            "Epoch 144: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8736, Test Linf Norm: 2.5216\n",
            "Epoch 145: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7490, Test Linf Norm: 2.5216\n",
            "Epoch 146: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8114, Test Linf Norm: 2.5216\n",
            "Epoch 147: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.5485, Test Linf Norm: 2.5216\n",
            "Epoch 148: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7158, Test Linf Norm: 2.5216\n",
            "Epoch 149: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.5761, Test Linf Norm: 2.5216\n",
            "Epoch 150: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7496, Test Linf Norm: 2.5216\n",
            "Epoch 151: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6740, Test Linf Norm: 2.5216\n",
            "Epoch 152: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6465, Test Linf Norm: 2.5216\n",
            "Epoch 153: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.1430, Test Linf Norm: 2.5216\n",
            "Epoch 154: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7574, Test Linf Norm: 2.5216\n",
            "Epoch 155: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8191, Test Linf Norm: 2.5216\n",
            "Epoch 156: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6524, Test Linf Norm: 2.5216\n",
            "Epoch 157: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.5615, Test Linf Norm: 2.5216\n",
            "Epoch 158: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.5408, Test Linf Norm: 2.5216\n",
            "Epoch 159: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8282, Test Linf Norm: 2.5216\n",
            "Epoch 160: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8187, Test Linf Norm: 2.5216\n",
            "Epoch 161: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8524, Test Linf Norm: 2.5216\n",
            "Epoch 162: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8004, Test Linf Norm: 2.5216\n",
            "Epoch 163: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7603, Test Linf Norm: 2.5216\n",
            "Epoch 164: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8430, Test Linf Norm: 2.5216\n",
            "Epoch 165: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6898, Test Linf Norm: 2.5216\n",
            "Epoch 166: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.5473, Test Linf Norm: 2.5216\n",
            "Epoch 167: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8399, Test Linf Norm: 2.5216\n",
            "Epoch 168: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8706, Test Linf Norm: 2.5216\n",
            "Epoch 169: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6592, Test Linf Norm: 2.5216\n",
            "Epoch 170: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6860, Test Linf Norm: 2.5216\n",
            "Epoch 171: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8350, Test Linf Norm: 2.5216\n",
            "Epoch 172: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8563, Test Linf Norm: 2.5216\n",
            "Epoch 173: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7508, Test Linf Norm: 2.5216\n",
            "Epoch 174: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.4837, Test Linf Norm: 2.5216\n",
            "Epoch 175: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6579, Test Linf Norm: 2.5216\n",
            "Epoch 176: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7970, Test Linf Norm: 2.5216\n",
            "Epoch 177: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8237, Test Linf Norm: 2.5216\n",
            "Epoch 178: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7892, Test Linf Norm: 2.5216\n",
            "Epoch 179: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.5751, Test Linf Norm: 2.5216\n",
            "Epoch 180: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6958, Test Linf Norm: 2.5216\n",
            "Epoch 181: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8035, Test Linf Norm: 2.5216\n",
            "Epoch 182: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7857, Test Linf Norm: 2.5216\n",
            "Epoch 183: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7672, Test Linf Norm: 2.5216\n",
            "Epoch 184: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7468, Test Linf Norm: 2.5216\n",
            "Epoch 185: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6849, Test Linf Norm: 2.5216\n",
            "Epoch 186: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8117, Test Linf Norm: 2.5216\n",
            "Epoch 187: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7850, Test Linf Norm: 2.5216\n",
            "Epoch 188: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7391, Test Linf Norm: 2.5216\n",
            "Epoch 189: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.5725, Test Linf Norm: 2.5216\n",
            "Epoch 190: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8526, Test Linf Norm: 2.5216\n",
            "Epoch 191: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.4292, Test Linf Norm: 2.5216\n",
            "Epoch 192: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7718, Test Linf Norm: 2.5216\n",
            "Epoch 193: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.5908, Test Linf Norm: 2.5216\n",
            "Epoch 194: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8715, Test Linf Norm: 2.5216\n",
            "Epoch 195: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8642, Test Linf Norm: 2.5216\n",
            "Epoch 196: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.2996, Test Linf Norm: 2.5216\n",
            "Epoch 197: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7119, Test Linf Norm: 2.5216\n",
            "Epoch 198: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6986, Test Linf Norm: 2.5216\n",
            "Epoch 199: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8297, Test Linf Norm: 2.5216\n",
            "Epoch 200: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7375, Test Linf Norm: 2.5216\n",
            "Epoch 201: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7283, Test Linf Norm: 2.5216\n",
            "Epoch 202: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8064, Test Linf Norm: 2.5216\n",
            "Epoch 203: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8302, Test Linf Norm: 2.5216\n",
            "Epoch 204: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7037, Test Linf Norm: 2.5216\n",
            "Epoch 205: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7401, Test Linf Norm: 2.5216\n",
            "Epoch 206: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7302, Test Linf Norm: 2.5216\n",
            "Epoch 207: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8487, Test Linf Norm: 2.5216\n",
            "Epoch 208: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7136, Test Linf Norm: 2.5216\n",
            "Epoch 209: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.2931, Test Linf Norm: 2.5216\n",
            "Epoch 210: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8194, Test Linf Norm: 2.5216\n",
            "Epoch 211: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8354, Test Linf Norm: 2.5216\n",
            "Epoch 212: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8469, Test Linf Norm: 2.5216\n",
            "Epoch 213: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7939, Test Linf Norm: 2.5216\n",
            "Epoch 214: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6684, Test Linf Norm: 2.5216\n",
            "Epoch 215: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7734, Test Linf Norm: 2.5216\n",
            "Epoch 216: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.5884, Test Linf Norm: 2.5216\n",
            "Epoch 217: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6978, Test Linf Norm: 2.5216\n",
            "Epoch 218: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6489, Test Linf Norm: 2.5216\n",
            "Epoch 219: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7511, Test Linf Norm: 2.5216\n",
            "Epoch 220: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.5457, Test Linf Norm: 2.5216\n",
            "Epoch 221: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7911, Test Linf Norm: 2.5216\n",
            "Epoch 222: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6349, Test Linf Norm: 2.5216\n",
            "Epoch 223: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6405, Test Linf Norm: 2.5216\n",
            "Epoch 224: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8093, Test Linf Norm: 2.5216\n",
            "Epoch 225: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6784, Test Linf Norm: 2.5216\n",
            "Epoch 226: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6741, Test Linf Norm: 2.5216\n",
            "Epoch 227: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7466, Test Linf Norm: 2.5216\n",
            "Epoch 228: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8108, Test Linf Norm: 2.5216\n",
            "Epoch 229: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.5335, Test Linf Norm: 2.5216\n",
            "Epoch 230: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6585, Test Linf Norm: 2.5216\n",
            "Epoch 231: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6675, Test Linf Norm: 2.5216\n",
            "Epoch 232: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.5723, Test Linf Norm: 2.5216\n",
            "Epoch 233: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8342, Test Linf Norm: 2.5216\n",
            "Epoch 234: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7573, Test Linf Norm: 2.5216\n",
            "Epoch 235: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6966, Test Linf Norm: 2.5216\n",
            "Epoch 236: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.5411, Test Linf Norm: 2.5216\n",
            "Epoch 237: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7379, Test Linf Norm: 2.5216\n",
            "Epoch 238: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.5708, Test Linf Norm: 2.5216\n",
            "Epoch 239: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8833, Test Linf Norm: 2.5216\n",
            "Epoch 240: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.7454, Test Linf Norm: 2.5216\n",
            "Epoch 241: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6075, Test Linf Norm: 2.5216\n",
            "Epoch 242: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.8274, Test Linf Norm: 2.5216\n",
            "Epoch 243: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.4483, Test Linf Norm: 2.5216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 17:14:03,579]\u001b[0m Trial 7 finished with value: 0.029325705087184906 and parameters: {'n_layers': 9, 'n_units_0': 1478, 'n_units_1': 231, 'n_units_2': 687, 'n_units_3': 589, 'n_units_4': 467, 'n_units_5': 788, 'n_units_6': 501, 'n_units_7': 1474, 'n_units_8': 1429, 'hidden_activation': 'LeakyReLU', 'output_activation': 'Linear', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.0011199236221033926, 'batch_size': 256, 'n_epochs': 244, 'scheduler': 'StepLR', 'step_size': 8, 'gamma': 0.25561222755597046}. Best is trial 2 with value: 0.011449855588376521.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 244: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0415, Test L1 Norm: 0.0293, Train Linf Norm: 5.6693, Test Linf Norm: 2.5216\n",
            "Epoch 1: Train Loss: 0.4545, Test Loss: 0.0141, Train L1 Norm: 0.8448, Test L1 Norm: 0.1363, Train Linf Norm: 22.5602, Test Linf Norm: 3.1629\n",
            "Epoch 2: Train Loss: 0.0243, Test Loss: 0.0046, Train L1 Norm: 0.5279, Test L1 Norm: 0.0957, Train Linf Norm: 15.5302, Test Linf Norm: 2.2725\n",
            "Epoch 3: Train Loss: 0.0150, Test Loss: 0.0020, Train L1 Norm: 0.4278, Test L1 Norm: 0.0795, Train Linf Norm: 12.7204, Test Linf Norm: 1.9220\n",
            "Epoch 4: Train Loss: 0.0087, Test Loss: 0.0035, Train L1 Norm: 0.2564, Test L1 Norm: 0.0898, Train Linf Norm: 7.4116, Test Linf Norm: 2.2153\n",
            "Epoch 5: Train Loss: 0.0110, Test Loss: 0.0016, Train L1 Norm: 0.2964, Test L1 Norm: 0.0413, Train Linf Norm: 8.4675, Test Linf Norm: 0.8637\n",
            "Epoch 6: Train Loss: 0.0071, Test Loss: 0.0019, Train L1 Norm: 0.2435, Test L1 Norm: 0.0766, Train Linf Norm: 7.1817, Test Linf Norm: 1.7759\n",
            "Epoch 7: Train Loss: 0.0033, Test Loss: 0.0006, Train L1 Norm: 0.1713, Test L1 Norm: 0.0474, Train Linf Norm: 4.9563, Test Linf Norm: 1.1258\n",
            "Epoch 8: Train Loss: 0.0021, Test Loss: 0.0006, Train L1 Norm: 0.1707, Test L1 Norm: 0.0466, Train Linf Norm: 5.0122, Test Linf Norm: 1.1185\n",
            "Epoch 9: Train Loss: 0.0080, Test Loss: 0.0013, Train L1 Norm: 0.1963, Test L1 Norm: 0.0471, Train Linf Norm: 5.6852, Test Linf Norm: 0.9405\n",
            "Epoch 10: Train Loss: 0.0023, Test Loss: 0.0022, Train L1 Norm: 0.1163, Test L1 Norm: 0.0447, Train Linf Norm: 3.2978, Test Linf Norm: 1.0599\n",
            "Epoch 11: Train Loss: 0.0055, Test Loss: 0.0007, Train L1 Norm: 0.1464, Test L1 Norm: 0.0402, Train Linf Norm: 4.1767, Test Linf Norm: 0.9110\n",
            "Epoch 12: Train Loss: 0.0013, Test Loss: 0.0007, Train L1 Norm: 0.1316, Test L1 Norm: 0.0384, Train Linf Norm: 3.8343, Test Linf Norm: 0.8734\n",
            "Epoch 13: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.1194, Test L1 Norm: 0.0350, Train Linf Norm: 3.5102, Test Linf Norm: 0.8181\n",
            "Epoch 14: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.1117, Test L1 Norm: 0.0345, Train Linf Norm: 3.2877, Test Linf Norm: 0.8133\n",
            "Epoch 15: Train Loss: 0.0006, Test Loss: 0.0003, Train L1 Norm: 0.0967, Test L1 Norm: 0.0356, Train Linf Norm: 2.7909, Test Linf Norm: 0.8504\n",
            "Epoch 16: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.1002, Test L1 Norm: 0.0347, Train Linf Norm: 2.9278, Test Linf Norm: 0.8252\n",
            "Epoch 17: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0996, Test L1 Norm: 0.0323, Train Linf Norm: 2.9023, Test Linf Norm: 0.7638\n",
            "Epoch 18: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.1100, Test L1 Norm: 0.0312, Train Linf Norm: 3.2432, Test Linf Norm: 0.7064\n",
            "Epoch 19: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0863, Test L1 Norm: 0.0327, Train Linf Norm: 2.4979, Test Linf Norm: 0.7778\n",
            "Epoch 20: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.1066, Test L1 Norm: 0.0294, Train Linf Norm: 3.1528, Test Linf Norm: 0.6758\n",
            "Epoch 21: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0936, Test L1 Norm: 0.0302, Train Linf Norm: 2.7342, Test Linf Norm: 0.7062\n",
            "Epoch 22: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0985, Test L1 Norm: 0.0299, Train Linf Norm: 2.8902, Test Linf Norm: 0.6862\n",
            "Epoch 23: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0909, Test L1 Norm: 0.0316, Train Linf Norm: 2.6564, Test Linf Norm: 0.7469\n",
            "Epoch 24: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.1033, Test L1 Norm: 0.0302, Train Linf Norm: 3.0621, Test Linf Norm: 0.7109\n",
            "Epoch 25: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0926, Test L1 Norm: 0.0281, Train Linf Norm: 2.7269, Test Linf Norm: 0.6522\n",
            "Epoch 26: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0846, Test L1 Norm: 0.0311, Train Linf Norm: 2.4724, Test Linf Norm: 0.7471\n",
            "Epoch 27: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0970, Test L1 Norm: 0.0295, Train Linf Norm: 2.8742, Test Linf Norm: 0.6993\n",
            "Epoch 28: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0962, Test L1 Norm: 0.0279, Train Linf Norm: 2.8444, Test Linf Norm: 0.6419\n",
            "Epoch 29: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0981, Test L1 Norm: 0.0288, Train Linf Norm: 2.9077, Test Linf Norm: 0.6784\n",
            "Epoch 30: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0906, Test L1 Norm: 0.0298, Train Linf Norm: 2.6562, Test Linf Norm: 0.7103\n",
            "Epoch 31: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0882, Test L1 Norm: 0.0300, Train Linf Norm: 2.5702, Test Linf Norm: 0.7169\n",
            "Epoch 32: Train Loss: 0.0002, Test Loss: 0.0004, Train L1 Norm: 0.0963, Test L1 Norm: 0.0302, Train Linf Norm: 2.8340, Test Linf Norm: 0.7024\n",
            "Epoch 33: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0948, Test L1 Norm: 0.0291, Train Linf Norm: 2.8027, Test Linf Norm: 0.6812\n",
            "Epoch 34: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0917, Test L1 Norm: 0.0298, Train Linf Norm: 2.7105, Test Linf Norm: 0.7092\n",
            "Epoch 35: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.1004, Test L1 Norm: 0.0287, Train Linf Norm: 2.9682, Test Linf Norm: 0.6798\n",
            "Epoch 36: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0927, Test L1 Norm: 0.0290, Train Linf Norm: 2.7375, Test Linf Norm: 0.6909\n",
            "Epoch 37: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0919, Test L1 Norm: 0.0290, Train Linf Norm: 2.6999, Test Linf Norm: 0.6900\n",
            "Epoch 38: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0910, Test L1 Norm: 0.0283, Train Linf Norm: 2.6895, Test Linf Norm: 0.6690\n",
            "Epoch 39: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0911, Test L1 Norm: 0.0284, Train Linf Norm: 2.6857, Test Linf Norm: 0.6726\n",
            "Epoch 40: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0917, Test L1 Norm: 0.0297, Train Linf Norm: 2.7079, Test Linf Norm: 0.7169\n",
            "Epoch 41: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0903, Test L1 Norm: 0.0280, Train Linf Norm: 2.6630, Test Linf Norm: 0.6576\n",
            "Epoch 42: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0916, Test L1 Norm: 0.0282, Train Linf Norm: 2.7128, Test Linf Norm: 0.6681\n",
            "Epoch 43: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0927, Test L1 Norm: 0.0287, Train Linf Norm: 2.7387, Test Linf Norm: 0.6841\n",
            "Epoch 44: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0938, Test L1 Norm: 0.0279, Train Linf Norm: 2.7858, Test Linf Norm: 0.6603\n",
            "Epoch 45: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0903, Test L1 Norm: 0.0272, Train Linf Norm: 2.6665, Test Linf Norm: 0.6371\n",
            "Epoch 46: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0883, Test L1 Norm: 0.0287, Train Linf Norm: 2.6001, Test Linf Norm: 0.6860\n",
            "Epoch 47: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0918, Test L1 Norm: 0.0288, Train Linf Norm: 2.7166, Test Linf Norm: 0.6896\n",
            "Epoch 48: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0922, Test L1 Norm: 0.0284, Train Linf Norm: 2.7298, Test Linf Norm: 0.6800\n",
            "Epoch 49: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0925, Test L1 Norm: 0.0285, Train Linf Norm: 2.7377, Test Linf Norm: 0.6811\n",
            "Epoch 50: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0919, Test L1 Norm: 0.0286, Train Linf Norm: 2.7275, Test Linf Norm: 0.6845\n",
            "Epoch 51: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0909, Test L1 Norm: 0.0281, Train Linf Norm: 2.6931, Test Linf Norm: 0.6671\n",
            "Epoch 52: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0921, Test L1 Norm: 0.0289, Train Linf Norm: 2.7094, Test Linf Norm: 0.6935\n",
            "Epoch 53: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0901, Test L1 Norm: 0.0292, Train Linf Norm: 2.6586, Test Linf Norm: 0.7018\n",
            "Epoch 54: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0904, Test L1 Norm: 0.0284, Train Linf Norm: 2.6775, Test Linf Norm: 0.6798\n",
            "Epoch 55: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0912, Test L1 Norm: 0.0285, Train Linf Norm: 2.6970, Test Linf Norm: 0.6810\n",
            "Epoch 56: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0927, Test L1 Norm: 0.0287, Train Linf Norm: 2.7512, Test Linf Norm: 0.6888\n",
            "Epoch 57: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0904, Test L1 Norm: 0.0282, Train Linf Norm: 2.6681, Test Linf Norm: 0.6704\n",
            "Epoch 58: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0923, Test L1 Norm: 0.0291, Train Linf Norm: 2.7272, Test Linf Norm: 0.7009\n",
            "Epoch 59: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0921, Test L1 Norm: 0.0279, Train Linf Norm: 2.7265, Test Linf Norm: 0.6640\n",
            "Epoch 60: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0937, Test L1 Norm: 0.0281, Train Linf Norm: 2.7786, Test Linf Norm: 0.6690\n",
            "Epoch 61: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0907, Test L1 Norm: 0.0282, Train Linf Norm: 2.6771, Test Linf Norm: 0.6723\n",
            "Epoch 62: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0905, Test L1 Norm: 0.0279, Train Linf Norm: 2.6793, Test Linf Norm: 0.6645\n",
            "Epoch 63: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0919, Test L1 Norm: 0.0283, Train Linf Norm: 2.7221, Test Linf Norm: 0.6755\n",
            "Epoch 64: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0922, Test L1 Norm: 0.0280, Train Linf Norm: 2.7342, Test Linf Norm: 0.6671\n",
            "Epoch 65: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0903, Test L1 Norm: 0.0282, Train Linf Norm: 2.6790, Test Linf Norm: 0.6741\n",
            "Epoch 66: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0924, Test L1 Norm: 0.0281, Train Linf Norm: 2.7416, Test Linf Norm: 0.6708\n",
            "Epoch 67: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0913, Test L1 Norm: 0.0280, Train Linf Norm: 2.7089, Test Linf Norm: 0.6677\n",
            "Epoch 68: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0900, Test L1 Norm: 0.0283, Train Linf Norm: 2.6653, Test Linf Norm: 0.6751\n",
            "Epoch 69: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0917, Test L1 Norm: 0.0285, Train Linf Norm: 2.7170, Test Linf Norm: 0.6831\n",
            "Epoch 70: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0937, Test L1 Norm: 0.0280, Train Linf Norm: 2.7778, Test Linf Norm: 0.6669\n",
            "Epoch 71: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0920, Test L1 Norm: 0.0280, Train Linf Norm: 2.7288, Test Linf Norm: 0.6674\n",
            "Epoch 72: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0923, Test L1 Norm: 0.0279, Train Linf Norm: 2.7307, Test Linf Norm: 0.6647\n",
            "Epoch 73: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0927, Test L1 Norm: 0.0281, Train Linf Norm: 2.7515, Test Linf Norm: 0.6700\n",
            "Epoch 74: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0912, Test L1 Norm: 0.0280, Train Linf Norm: 2.7044, Test Linf Norm: 0.6660\n",
            "Epoch 75: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0899, Test L1 Norm: 0.0282, Train Linf Norm: 2.6635, Test Linf Norm: 0.6728\n",
            "Epoch 76: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0916, Test L1 Norm: 0.0282, Train Linf Norm: 2.7175, Test Linf Norm: 0.6736\n",
            "Epoch 77: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0923, Test L1 Norm: 0.0280, Train Linf Norm: 2.7380, Test Linf Norm: 0.6680\n",
            "Epoch 78: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0904, Test L1 Norm: 0.0281, Train Linf Norm: 2.6819, Test Linf Norm: 0.6689\n",
            "Epoch 79: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0915, Test L1 Norm: 0.0281, Train Linf Norm: 2.6988, Test Linf Norm: 0.6700\n",
            "Epoch 80: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6904, Test Linf Norm: 0.6695\n",
            "Epoch 81: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0916, Test L1 Norm: 0.0281, Train Linf Norm: 2.7061, Test Linf Norm: 0.6695\n",
            "Epoch 82: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0913, Test L1 Norm: 0.0281, Train Linf Norm: 2.7081, Test Linf Norm: 0.6711\n",
            "Epoch 83: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0907, Test L1 Norm: 0.0280, Train Linf Norm: 2.6921, Test Linf Norm: 0.6685\n",
            "Epoch 84: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0916, Test L1 Norm: 0.0282, Train Linf Norm: 2.7237, Test Linf Norm: 0.6743\n",
            "Epoch 85: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0919, Test L1 Norm: 0.0282, Train Linf Norm: 2.6934, Test Linf Norm: 0.6745\n",
            "Epoch 86: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0916, Test L1 Norm: 0.0281, Train Linf Norm: 2.7197, Test Linf Norm: 0.6708\n",
            "Epoch 87: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0913, Test L1 Norm: 0.0282, Train Linf Norm: 2.7041, Test Linf Norm: 0.6726\n",
            "Epoch 88: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0917, Test L1 Norm: 0.0280, Train Linf Norm: 2.7243, Test Linf Norm: 0.6685\n",
            "Epoch 89: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0912, Test L1 Norm: 0.0282, Train Linf Norm: 2.7073, Test Linf Norm: 0.6725\n",
            "Epoch 90: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0914, Test L1 Norm: 0.0282, Train Linf Norm: 2.7089, Test Linf Norm: 0.6737\n",
            "Epoch 91: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0920, Test L1 Norm: 0.0281, Train Linf Norm: 2.7259, Test Linf Norm: 0.6714\n",
            "Epoch 92: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0914, Test L1 Norm: 0.0280, Train Linf Norm: 2.7084, Test Linf Norm: 0.6683\n",
            "Epoch 93: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0918, Test L1 Norm: 0.0282, Train Linf Norm: 2.7192, Test Linf Norm: 0.6728\n",
            "Epoch 94: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0910, Test L1 Norm: 0.0282, Train Linf Norm: 2.7008, Test Linf Norm: 0.6723\n",
            "Epoch 95: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0916, Test L1 Norm: 0.0281, Train Linf Norm: 2.7174, Test Linf Norm: 0.6722\n",
            "Epoch 96: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0913, Test L1 Norm: 0.0282, Train Linf Norm: 2.7085, Test Linf Norm: 0.6725\n",
            "Epoch 97: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0913, Test L1 Norm: 0.0281, Train Linf Norm: 2.7117, Test Linf Norm: 0.6715\n",
            "Epoch 98: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0911, Test L1 Norm: 0.0281, Train Linf Norm: 2.7031, Test Linf Norm: 0.6718\n",
            "Epoch 99: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0913, Test L1 Norm: 0.0281, Train Linf Norm: 2.7087, Test Linf Norm: 0.6700\n",
            "Epoch 100: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0914, Test L1 Norm: 0.0281, Train Linf Norm: 2.7120, Test Linf Norm: 0.6701\n",
            "Epoch 101: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0911, Test L1 Norm: 0.0281, Train Linf Norm: 2.7022, Test Linf Norm: 0.6703\n",
            "Epoch 102: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0911, Test L1 Norm: 0.0282, Train Linf Norm: 2.7011, Test Linf Norm: 0.6727\n",
            "Epoch 103: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0916, Test L1 Norm: 0.0281, Train Linf Norm: 2.7250, Test Linf Norm: 0.6708\n",
            "Epoch 104: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0912, Test L1 Norm: 0.0281, Train Linf Norm: 2.6979, Test Linf Norm: 0.6715\n",
            "Epoch 105: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0915, Test L1 Norm: 0.0281, Train Linf Norm: 2.7175, Test Linf Norm: 0.6709\n",
            "Epoch 106: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0912, Test L1 Norm: 0.0281, Train Linf Norm: 2.6875, Test Linf Norm: 0.6716\n",
            "Epoch 107: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0912, Test L1 Norm: 0.0281, Train Linf Norm: 2.6937, Test Linf Norm: 0.6719\n",
            "Epoch 108: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0914, Test L1 Norm: 0.0281, Train Linf Norm: 2.7116, Test Linf Norm: 0.6705\n",
            "Epoch 109: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0911, Test L1 Norm: 0.0281, Train Linf Norm: 2.7055, Test Linf Norm: 0.6703\n",
            "Epoch 110: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0910, Test L1 Norm: 0.0281, Train Linf Norm: 2.6846, Test Linf Norm: 0.6703\n",
            "Epoch 111: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0910, Test L1 Norm: 0.0281, Train Linf Norm: 2.6972, Test Linf Norm: 0.6703\n",
            "Epoch 112: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0912, Test L1 Norm: 0.0281, Train Linf Norm: 2.7076, Test Linf Norm: 0.6701\n",
            "Epoch 113: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0910, Test L1 Norm: 0.0281, Train Linf Norm: 2.6866, Test Linf Norm: 0.6702\n",
            "Epoch 114: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0911, Test L1 Norm: 0.0281, Train Linf Norm: 2.6962, Test Linf Norm: 0.6703\n",
            "Epoch 115: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0910, Test L1 Norm: 0.0281, Train Linf Norm: 2.7016, Test Linf Norm: 0.6704\n",
            "Epoch 116: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0911, Test L1 Norm: 0.0281, Train Linf Norm: 2.6991, Test Linf Norm: 0.6704\n",
            "Epoch 117: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0910, Test L1 Norm: 0.0281, Train Linf Norm: 2.6991, Test Linf Norm: 0.6702\n",
            "Epoch 118: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0911, Test L1 Norm: 0.0281, Train Linf Norm: 2.7021, Test Linf Norm: 0.6702\n",
            "Epoch 119: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0910, Test L1 Norm: 0.0281, Train Linf Norm: 2.7019, Test Linf Norm: 0.6702\n",
            "Epoch 120: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0911, Test L1 Norm: 0.0281, Train Linf Norm: 2.7009, Test Linf Norm: 0.6702\n",
            "Epoch 121: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0910, Test L1 Norm: 0.0281, Train Linf Norm: 2.6954, Test Linf Norm: 0.6700\n",
            "Epoch 122: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0910, Test L1 Norm: 0.0281, Train Linf Norm: 2.7009, Test Linf Norm: 0.6699\n",
            "Epoch 123: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0910, Test L1 Norm: 0.0281, Train Linf Norm: 2.6949, Test Linf Norm: 0.6698\n",
            "Epoch 124: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0909, Test L1 Norm: 0.0281, Train Linf Norm: 2.6954, Test Linf Norm: 0.6696\n",
            "Epoch 125: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0909, Test L1 Norm: 0.0281, Train Linf Norm: 2.6938, Test Linf Norm: 0.6696\n",
            "Epoch 126: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0909, Test L1 Norm: 0.0281, Train Linf Norm: 2.6813, Test Linf Norm: 0.6694\n",
            "Epoch 127: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0909, Test L1 Norm: 0.0281, Train Linf Norm: 2.6886, Test Linf Norm: 0.6693\n",
            "Epoch 128: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0909, Test L1 Norm: 0.0281, Train Linf Norm: 2.6837, Test Linf Norm: 0.6691\n",
            "Epoch 129: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0909, Test L1 Norm: 0.0281, Train Linf Norm: 2.6941, Test Linf Norm: 0.6691\n",
            "Epoch 130: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0909, Test L1 Norm: 0.0281, Train Linf Norm: 2.6910, Test Linf Norm: 0.6690\n",
            "Epoch 131: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6916, Test Linf Norm: 0.6690\n",
            "Epoch 132: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6917, Test Linf Norm: 0.6690\n",
            "Epoch 133: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6804, Test Linf Norm: 0.6690\n",
            "Epoch 134: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6935, Test Linf Norm: 0.6689\n",
            "Epoch 135: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6916, Test Linf Norm: 0.6689\n",
            "Epoch 136: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6846, Test Linf Norm: 0.6689\n",
            "Epoch 137: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6924, Test Linf Norm: 0.6689\n",
            "Epoch 138: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6944, Test Linf Norm: 0.6688\n",
            "Epoch 139: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6925, Test Linf Norm: 0.6688\n",
            "Epoch 140: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6237, Test Linf Norm: 0.6688\n",
            "Epoch 141: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6928, Test Linf Norm: 0.6688\n",
            "Epoch 142: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6870, Test Linf Norm: 0.6688\n",
            "Epoch 143: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6836, Test Linf Norm: 0.6687\n",
            "Epoch 144: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6888, Test Linf Norm: 0.6687\n",
            "Epoch 145: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6959, Test Linf Norm: 0.6687\n",
            "Epoch 146: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6943, Test Linf Norm: 0.6687\n",
            "Epoch 147: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6825, Test Linf Norm: 0.6687\n",
            "Epoch 148: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6912, Test Linf Norm: 0.6687\n",
            "Epoch 149: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6959, Test Linf Norm: 0.6687\n",
            "Epoch 150: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6918, Test Linf Norm: 0.6687\n",
            "Epoch 151: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6735, Test Linf Norm: 0.6687\n",
            "Epoch 152: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6806, Test Linf Norm: 0.6687\n",
            "Epoch 153: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6771, Test Linf Norm: 0.6687\n",
            "Epoch 154: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6815, Test Linf Norm: 0.6687\n",
            "Epoch 155: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6924, Test Linf Norm: 0.6687\n",
            "Epoch 156: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6833, Test Linf Norm: 0.6687\n",
            "Epoch 157: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6933, Test Linf Norm: 0.6687\n",
            "Epoch 158: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6970, Test Linf Norm: 0.6687\n",
            "Epoch 159: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6980, Test Linf Norm: 0.6687\n",
            "Epoch 160: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6924, Test Linf Norm: 0.6687\n",
            "Epoch 161: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6888, Test Linf Norm: 0.6687\n",
            "Epoch 162: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6832, Test Linf Norm: 0.6687\n",
            "Epoch 163: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6864, Test Linf Norm: 0.6687\n",
            "Epoch 164: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6988, Test Linf Norm: 0.6687\n",
            "Epoch 165: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6935, Test Linf Norm: 0.6687\n",
            "Epoch 166: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6920, Test Linf Norm: 0.6687\n",
            "Epoch 167: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6944, Test Linf Norm: 0.6687\n",
            "Epoch 168: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6856, Test Linf Norm: 0.6687\n",
            "Epoch 169: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6956, Test Linf Norm: 0.6687\n",
            "Epoch 170: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6936, Test Linf Norm: 0.6687\n",
            "Epoch 171: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6906, Test Linf Norm: 0.6687\n",
            "Epoch 172: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6704, Test Linf Norm: 0.6687\n",
            "Epoch 173: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6843, Test Linf Norm: 0.6687\n",
            "Epoch 174: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6909, Test Linf Norm: 0.6687\n",
            "Epoch 175: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6880, Test Linf Norm: 0.6687\n",
            "Epoch 176: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6840, Test Linf Norm: 0.6687\n",
            "Epoch 177: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6887, Test Linf Norm: 0.6687\n",
            "Epoch 178: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6842, Test Linf Norm: 0.6687\n",
            "Epoch 179: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6938, Test Linf Norm: 0.6687\n",
            "Epoch 180: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6926, Test Linf Norm: 0.6687\n",
            "Epoch 181: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6900, Test Linf Norm: 0.6687\n",
            "Epoch 182: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6871, Test Linf Norm: 0.6687\n",
            "Epoch 183: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.2262, Test Linf Norm: 0.6687\n",
            "Epoch 184: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6940, Test Linf Norm: 0.6687\n",
            "Epoch 185: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6854, Test Linf Norm: 0.6687\n",
            "Epoch 186: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6874, Test Linf Norm: 0.6687\n",
            "Epoch 187: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6920, Test Linf Norm: 0.6687\n",
            "Epoch 188: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6913, Test Linf Norm: 0.6687\n",
            "Epoch 189: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6911, Test Linf Norm: 0.6687\n",
            "Epoch 190: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6617, Test Linf Norm: 0.6687\n",
            "Epoch 191: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6725, Test Linf Norm: 0.6687\n",
            "Epoch 192: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6883, Test Linf Norm: 0.6687\n",
            "Epoch 193: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6897, Test Linf Norm: 0.6687\n",
            "Epoch 194: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6864, Test Linf Norm: 0.6687\n",
            "Epoch 195: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6818, Test Linf Norm: 0.6687\n",
            "Epoch 196: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6923, Test Linf Norm: 0.6687\n",
            "Epoch 197: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6964, Test Linf Norm: 0.6687\n",
            "Epoch 198: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6925, Test Linf Norm: 0.6687\n",
            "Epoch 199: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6927, Test Linf Norm: 0.6687\n",
            "Epoch 200: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6948, Test Linf Norm: 0.6687\n",
            "Epoch 201: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6942, Test Linf Norm: 0.6687\n",
            "Epoch 202: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6886, Test Linf Norm: 0.6687\n",
            "Epoch 203: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6956, Test Linf Norm: 0.6687\n",
            "Epoch 204: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6719, Test Linf Norm: 0.6687\n",
            "Epoch 205: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6943, Test Linf Norm: 0.6687\n",
            "Epoch 206: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6917, Test Linf Norm: 0.6687\n",
            "Epoch 207: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6847, Test Linf Norm: 0.6687\n",
            "Epoch 208: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6653, Test Linf Norm: 0.6687\n",
            "Epoch 209: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6918, Test Linf Norm: 0.6687\n",
            "Epoch 210: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6826, Test Linf Norm: 0.6687\n",
            "Epoch 211: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6585, Test Linf Norm: 0.6687\n",
            "Epoch 212: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6834, Test Linf Norm: 0.6687\n",
            "Epoch 213: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6800, Test Linf Norm: 0.6687\n",
            "Epoch 214: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6643, Test Linf Norm: 0.6687\n",
            "Epoch 215: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6921, Test Linf Norm: 0.6687\n",
            "Epoch 216: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6914, Test Linf Norm: 0.6687\n",
            "Epoch 217: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6930, Test Linf Norm: 0.6687\n",
            "Epoch 218: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6911, Test Linf Norm: 0.6687\n",
            "Epoch 219: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6792, Test Linf Norm: 0.6687\n",
            "Epoch 220: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6934, Test Linf Norm: 0.6687\n",
            "Epoch 221: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6948, Test Linf Norm: 0.6687\n",
            "Epoch 222: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6921, Test Linf Norm: 0.6687\n",
            "Epoch 223: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6912, Test Linf Norm: 0.6687\n",
            "Epoch 224: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6944, Test Linf Norm: 0.6687\n",
            "Epoch 225: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6845, Test Linf Norm: 0.6687\n",
            "Epoch 226: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6915, Test Linf Norm: 0.6687\n",
            "Epoch 227: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6951, Test Linf Norm: 0.6687\n",
            "Epoch 228: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6544, Test Linf Norm: 0.6687\n",
            "Epoch 229: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6942, Test Linf Norm: 0.6687\n",
            "Epoch 230: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6867, Test Linf Norm: 0.6687\n",
            "Epoch 231: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6960, Test Linf Norm: 0.6687\n",
            "Epoch 232: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6740, Test Linf Norm: 0.6687\n",
            "Epoch 233: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6839, Test Linf Norm: 0.6687\n",
            "Epoch 234: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6681, Test Linf Norm: 0.6687\n",
            "Epoch 235: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6936, Test Linf Norm: 0.6687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 18:20:40,788]\u001b[0m Trial 8 finished with value: 0.028053090285882355 and parameters: {'n_layers': 10, 'n_units_0': 1813, 'n_units_1': 1310, 'n_units_2': 207, 'n_units_3': 1025, 'n_units_4': 1189, 'n_units_5': 379, 'n_units_6': 826, 'n_units_7': 1319, 'n_units_8': 787, 'n_units_9': 1893, 'hidden_activation': 'ELU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adagrad', 'lr': 0.0010729127572736037, 'batch_size': 32, 'n_epochs': 236, 'scheduler': 'StepLR', 'step_size': 12, 'gamma': 0.45211394756067347}. Best is trial 2 with value: 0.011449855588376521.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 236: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0908, Test L1 Norm: 0.0281, Train Linf Norm: 2.6473, Test Linf Norm: 0.6687\n",
            "Epoch 1: Train Loss: 0.4033, Test Loss: 0.0897, Train L1 Norm: 2.0759, Test L1 Norm: 0.3457, Train Linf Norm: 150.2220, Test Linf Norm: 16.8871\n",
            "Epoch 2: Train Loss: 0.0348, Test Loss: 0.0148, Train L1 Norm: 0.5889, Test L1 Norm: 0.2080, Train Linf Norm: 44.8406, Test Linf Norm: 11.1715\n",
            "Epoch 3: Train Loss: 0.0223, Test Loss: 0.0099, Train L1 Norm: 0.5343, Test L1 Norm: 0.2017, Train Linf Norm: 41.9124, Test Linf Norm: 10.7887\n",
            "Epoch 4: Train Loss: 0.0178, Test Loss: 0.0233, Train L1 Norm: 0.4530, Test L1 Norm: 0.1601, Train Linf Norm: 35.3607, Test Linf Norm: 7.6762\n",
            "Epoch 5: Train Loss: 0.0131, Test Loss: 0.0071, Train L1 Norm: 0.3708, Test L1 Norm: 0.1444, Train Linf Norm: 28.3961, Test Linf Norm: 7.3654\n",
            "Epoch 6: Train Loss: 0.0116, Test Loss: 0.0302, Train L1 Norm: 0.3033, Test L1 Norm: 0.1356, Train Linf Norm: 22.6053, Test Linf Norm: 5.2169\n",
            "Epoch 7: Train Loss: 0.0120, Test Loss: 0.0086, Train L1 Norm: 0.2748, Test L1 Norm: 0.1522, Train Linf Norm: 19.9536, Test Linf Norm: 7.2394\n",
            "Epoch 8: Train Loss: 0.0087, Test Loss: 0.0026, Train L1 Norm: 0.2484, Test L1 Norm: 0.1074, Train Linf Norm: 18.0061, Test Linf Norm: 5.4506\n",
            "Epoch 9: Train Loss: 0.0076, Test Loss: 0.0022, Train L1 Norm: 0.2597, Test L1 Norm: 0.1022, Train Linf Norm: 19.1100, Test Linf Norm: 5.1040\n",
            "Epoch 10: Train Loss: 0.0063, Test Loss: 0.0024, Train L1 Norm: 0.2103, Test L1 Norm: 0.0959, Train Linf Norm: 14.9365, Test Linf Norm: 4.7551\n",
            "Epoch 11: Train Loss: 0.0070, Test Loss: 0.0024, Train L1 Norm: 0.1990, Test L1 Norm: 0.0945, Train Linf Norm: 13.6734, Test Linf Norm: 4.6409\n",
            "Epoch 12: Train Loss: 0.0070, Test Loss: 0.0023, Train L1 Norm: 0.2311, Test L1 Norm: 0.1012, Train Linf Norm: 16.8161, Test Linf Norm: 5.0920\n",
            "Epoch 13: Train Loss: 0.0060, Test Loss: 0.0284, Train L1 Norm: 0.2390, Test L1 Norm: 0.1186, Train Linf Norm: 17.9350, Test Linf Norm: 4.3951\n",
            "Epoch 14: Train Loss: 0.0045, Test Loss: 0.0014, Train L1 Norm: 0.2553, Test L1 Norm: 0.0916, Train Linf Norm: 19.6182, Test Linf Norm: 4.6327\n",
            "Epoch 15: Train Loss: 0.0056, Test Loss: 0.0352, Train L1 Norm: 0.2349, Test L1 Norm: 0.1253, Train Linf Norm: 17.6991, Test Linf Norm: 4.1850\n",
            "Epoch 16: Train Loss: 0.0049, Test Loss: 0.1275, Train L1 Norm: 0.2277, Test L1 Norm: 0.2339, Train Linf Norm: 17.0792, Test Linf Norm: 8.1827\n",
            "Epoch 17: Train Loss: 0.0050, Test Loss: 0.0014, Train L1 Norm: 0.2013, Test L1 Norm: 0.0939, Train Linf Norm: 14.7083, Test Linf Norm: 4.8262\n",
            "Epoch 18: Train Loss: 0.0050, Test Loss: 0.0017, Train L1 Norm: 0.2188, Test L1 Norm: 0.0932, Train Linf Norm: 16.3635, Test Linf Norm: 4.7584\n",
            "Epoch 19: Train Loss: 0.0050, Test Loss: 0.0266, Train L1 Norm: 0.2285, Test L1 Norm: 0.1282, Train Linf Norm: 17.4080, Test Linf Norm: 4.2344\n",
            "Epoch 20: Train Loss: 0.0037, Test Loss: 0.0014, Train L1 Norm: 0.2349, Test L1 Norm: 0.0894, Train Linf Norm: 18.1738, Test Linf Norm: 4.7234\n",
            "Epoch 21: Train Loss: 0.0035, Test Loss: 0.0011, Train L1 Norm: 0.2417, Test L1 Norm: 0.0898, Train Linf Norm: 18.8926, Test Linf Norm: 4.8051\n",
            "Epoch 22: Train Loss: 0.0045, Test Loss: 0.0009, Train L1 Norm: 0.2600, Test L1 Norm: 0.0904, Train Linf Norm: 20.3852, Test Linf Norm: 4.8867\n",
            "Epoch 23: Train Loss: 0.0040, Test Loss: 0.0009, Train L1 Norm: 0.2407, Test L1 Norm: 0.0910, Train Linf Norm: 18.6790, Test Linf Norm: 4.9458\n",
            "Epoch 24: Train Loss: 0.0036, Test Loss: 0.0017, Train L1 Norm: 0.2503, Test L1 Norm: 0.0901, Train Linf Norm: 19.7954, Test Linf Norm: 4.7772\n",
            "Epoch 25: Train Loss: 0.0030, Test Loss: 0.0008, Train L1 Norm: 0.2373, Test L1 Norm: 0.0895, Train Linf Norm: 18.7152, Test Linf Norm: 4.9123\n",
            "Epoch 26: Train Loss: 0.0044, Test Loss: 0.0018, Train L1 Norm: 0.2679, Test L1 Norm: 0.0919, Train Linf Norm: 21.3956, Test Linf Norm: 5.0306\n",
            "Epoch 27: Train Loss: 0.0029, Test Loss: 0.0007, Train L1 Norm: 0.2498, Test L1 Norm: 0.0906, Train Linf Norm: 19.9388, Test Linf Norm: 5.0522\n",
            "Epoch 28: Train Loss: 0.0022, Test Loss: 0.0006, Train L1 Norm: 0.2514, Test L1 Norm: 0.0905, Train Linf Norm: 20.0517, Test Linf Norm: 5.0635\n",
            "Epoch 29: Train Loss: 0.0032, Test Loss: 0.0065, Train L1 Norm: 0.2418, Test L1 Norm: 0.0986, Train Linf Norm: 19.1875, Test Linf Norm: 4.9860\n",
            "Epoch 30: Train Loss: 0.0025, Test Loss: 0.0008, Train L1 Norm: 0.2453, Test L1 Norm: 0.0933, Train Linf Norm: 19.5587, Test Linf Norm: 5.2263\n",
            "Epoch 31: Train Loss: 0.0028, Test Loss: 0.0201, Train L1 Norm: 0.2400, Test L1 Norm: 0.1424, Train Linf Norm: 19.0333, Test Linf Norm: 6.4730\n",
            "Epoch 32: Train Loss: 0.0044, Test Loss: 0.0006, Train L1 Norm: 0.2771, Test L1 Norm: 0.0916, Train Linf Norm: 22.2645, Test Linf Norm: 5.2319\n",
            "Epoch 33: Train Loss: 0.0016, Test Loss: 0.0006, Train L1 Norm: 0.2503, Test L1 Norm: 0.0908, Train Linf Norm: 20.2642, Test Linf Norm: 5.1809\n",
            "Epoch 34: Train Loss: 0.0025, Test Loss: 0.0050, Train L1 Norm: 0.2470, Test L1 Norm: 0.0958, Train Linf Norm: 19.8357, Test Linf Norm: 5.0731\n",
            "Epoch 35: Train Loss: 0.0021, Test Loss: 0.0025, Train L1 Norm: 0.2500, Test L1 Norm: 0.0940, Train Linf Norm: 20.0648, Test Linf Norm: 5.1446\n",
            "Epoch 36: Train Loss: 0.0032, Test Loss: 0.0007, Train L1 Norm: 0.2399, Test L1 Norm: 0.0935, Train Linf Norm: 18.9966, Test Linf Norm: 5.3719\n",
            "Epoch 37: Train Loss: 0.0019, Test Loss: 0.0046, Train L1 Norm: 0.2459, Test L1 Norm: 0.0974, Train Linf Norm: 19.9025, Test Linf Norm: 4.9926\n",
            "Epoch 38: Train Loss: 0.0021, Test Loss: 0.0007, Train L1 Norm: 0.2387, Test L1 Norm: 0.0988, Train Linf Norm: 19.1775, Test Linf Norm: 5.6305\n",
            "Epoch 39: Train Loss: 0.0018, Test Loss: 0.0014, Train L1 Norm: 0.2494, Test L1 Norm: 0.0934, Train Linf Norm: 20.3187, Test Linf Norm: 5.3740\n",
            "Epoch 40: Train Loss: 0.0025, Test Loss: 0.0012, Train L1 Norm: 0.2578, Test L1 Norm: 0.0917, Train Linf Norm: 20.9094, Test Linf Norm: 5.3099\n",
            "Epoch 41: Train Loss: 0.0006, Test Loss: 0.0004, Train L1 Norm: 0.2495, Test L1 Norm: 0.0915, Train Linf Norm: 20.4481, Test Linf Norm: 5.3554\n",
            "Epoch 42: Train Loss: 0.0006, Test Loss: 0.0004, Train L1 Norm: 0.2450, Test L1 Norm: 0.0916, Train Linf Norm: 20.2073, Test Linf Norm: 5.3612\n",
            "Epoch 43: Train Loss: 0.0006, Test Loss: 0.0004, Train L1 Norm: 0.2441, Test L1 Norm: 0.0904, Train Linf Norm: 20.0928, Test Linf Norm: 5.3064\n",
            "Epoch 44: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.2448, Test L1 Norm: 0.0915, Train Linf Norm: 20.2171, Test Linf Norm: 5.3501\n",
            "Epoch 45: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.2433, Test L1 Norm: 0.0903, Train Linf Norm: 20.0566, Test Linf Norm: 5.3076\n",
            "Epoch 46: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.2414, Test L1 Norm: 0.0899, Train Linf Norm: 19.8091, Test Linf Norm: 5.2844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 18:23:05,362]\u001b[0m Trial 9 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.2381, Test L1 Norm: 0.0901, Train Linf Norm: 19.4648, Test Linf Norm: 5.3002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 18:23:07,260]\u001b[0m Trial 10 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 18:23:20,874]\u001b[0m Trial 11 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 20.6170, Test Loss: 21.1527, Train L1 Norm: 1.0001, Test L1 Norm: 1.0000, Train Linf Norm: 1.0013, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 4.3414, Test Loss: 0.7169, Train L1 Norm: 2.5404, Test L1 Norm: 0.4910, Train Linf Norm: 1511.5877, Test Linf Norm: 102.0834\n",
            "Epoch 2: Train Loss: 0.9036, Test Loss: 1.4019, Train L1 Norm: 1.3982, Test L1 Norm: 0.3778, Train Linf Norm: 1001.1093, Test Linf Norm: 65.2098\n",
            "Epoch 3: Train Loss: 0.6564, Test Loss: 0.3970, Train L1 Norm: 1.1709, Test L1 Norm: 0.4825, Train Linf Norm: 852.0960, Test Linf Norm: 105.0889\n",
            "Epoch 4: Train Loss: 0.5436, Test Loss: 0.4591, Train L1 Norm: 0.7638, Test L1 Norm: 0.1836, Train Linf Norm: 466.1505, Test Linf Norm: 20.8066\n",
            "Epoch 5: Train Loss: 0.4866, Test Loss: 0.1545, Train L1 Norm: 0.7410, Test L1 Norm: 0.2670, Train Linf Norm: 462.4408, Test Linf Norm: 58.5516\n",
            "Epoch 6: Train Loss: 0.0372, Test Loss: 0.0755, Train L1 Norm: 0.3065, Test L1 Norm: 0.1867, Train Linf Norm: 210.7764, Test Linf Norm: 36.9450\n",
            "Epoch 7: Train Loss: 0.1020, Test Loss: 0.3656, Train L1 Norm: 0.2919, Test L1 Norm: 0.1510, Train Linf Norm: 162.8244, Test Linf Norm: 10.5890\n",
            "Epoch 8: Train Loss: 0.1562, Test Loss: 0.1054, Train L1 Norm: 0.4214, Test L1 Norm: 0.1971, Train Linf Norm: 281.2062, Test Linf Norm: 40.9928\n",
            "Epoch 9: Train Loss: 0.1079, Test Loss: 0.2883, Train L1 Norm: 0.4226, Test L1 Norm: 0.2134, Train Linf Norm: 317.2435, Test Linf Norm: 31.4859\n",
            "Epoch 10: Train Loss: 0.1143, Test Loss: 0.3105, Train L1 Norm: 0.3085, Test L1 Norm: 0.1403, Train Linf Norm: 181.5392, Test Linf Norm: 12.6469\n",
            "Epoch 11: Train Loss: 0.0120, Test Loss: 0.0071, Train L1 Norm: 0.1699, Test L1 Norm: 0.0575, Train Linf Norm: 123.2036, Test Linf Norm: 14.3021\n",
            "Epoch 12: Train Loss: 0.0264, Test Loss: 0.0414, Train L1 Norm: 0.2080, Test L1 Norm: 0.0873, Train Linf Norm: 148.3233, Test Linf Norm: 13.6992\n",
            "Epoch 13: Train Loss: 0.0385, Test Loss: 0.4592, Train L1 Norm: 0.1891, Test L1 Norm: 0.1780, Train Linf Norm: 117.0781, Test Linf Norm: 14.4618\n",
            "Epoch 14: Train Loss: 0.0353, Test Loss: 0.3486, Train L1 Norm: 0.2040, Test L1 Norm: 0.1344, Train Linf Norm: 142.2239, Test Linf Norm: 8.8301\n",
            "Epoch 15: Train Loss: 0.0324, Test Loss: 0.0193, Train L1 Norm: 0.1864, Test L1 Norm: 0.0841, Train Linf Norm: 130.4952, Test Linf Norm: 16.8788\n",
            "Epoch 16: Train Loss: 0.0033, Test Loss: 0.0027, Train L1 Norm: 0.1227, Test L1 Norm: 0.0393, Train Linf Norm: 93.1695, Test Linf Norm: 10.3618\n",
            "Epoch 17: Train Loss: 0.0060, Test Loss: 0.0044, Train L1 Norm: 0.1144, Test L1 Norm: 0.0409, Train Linf Norm: 81.3275, Test Linf Norm: 9.7633\n",
            "Epoch 18: Train Loss: 0.0099, Test Loss: 0.0228, Train L1 Norm: 0.1351, Test L1 Norm: 0.0733, Train Linf Norm: 98.5950, Test Linf Norm: 12.1284\n",
            "Epoch 19: Train Loss: 0.0113, Test Loss: 0.0036, Train L1 Norm: 0.1446, Test L1 Norm: 0.0354, Train Linf Norm: 104.2809, Test Linf Norm: 9.3507\n",
            "Epoch 20: Train Loss: 0.0084, Test Loss: 0.0375, Train L1 Norm: 0.1467, Test L1 Norm: 0.0808, Train Linf Norm: 112.2966, Test Linf Norm: 12.0274\n",
            "Epoch 21: Train Loss: 0.0021, Test Loss: 0.0015, Train L1 Norm: 0.0836, Test L1 Norm: 0.0283, Train Linf Norm: 58.7921, Test Linf Norm: 7.2664\n",
            "Epoch 22: Train Loss: 0.0027, Test Loss: 0.0015, Train L1 Norm: 0.0915, Test L1 Norm: 0.0292, Train Linf Norm: 67.8509, Test Linf Norm: 6.8885\n",
            "Epoch 23: Train Loss: 0.0032, Test Loss: 0.0133, Train L1 Norm: 0.0993, Test L1 Norm: 0.0522, Train Linf Norm: 76.5880, Test Linf Norm: 9.9959\n",
            "Epoch 24: Train Loss: 0.0022, Test Loss: 0.0278, Train L1 Norm: 0.0971, Test L1 Norm: 0.0658, Train Linf Norm: 76.8912, Test Linf Norm: 10.1072\n",
            "Epoch 25: Train Loss: 0.0038, Test Loss: 0.0014, Train L1 Norm: 0.0911, Test L1 Norm: 0.0252, Train Linf Norm: 65.9340, Test Linf Norm: 5.7416\n",
            "Epoch 26: Train Loss: 0.0010, Test Loss: 0.0011, Train L1 Norm: 0.0718, Test L1 Norm: 0.0254, Train Linf Norm: 53.6457, Test Linf Norm: 6.8189\n",
            "Epoch 27: Train Loss: 0.0010, Test Loss: 0.0011, Train L1 Norm: 0.0653, Test L1 Norm: 0.0244, Train Linf Norm: 41.1879, Test Linf Norm: 5.5343\n",
            "Epoch 28: Train Loss: 0.0014, Test Loss: 0.0038, Train L1 Norm: 0.0674, Test L1 Norm: 0.0292, Train Linf Norm: 47.8095, Test Linf Norm: 6.1891\n",
            "Epoch 29: Train Loss: 0.0013, Test Loss: 0.0009, Train L1 Norm: 0.0620, Test L1 Norm: 0.0236, Train Linf Norm: 41.4731, Test Linf Norm: 5.4753\n",
            "Epoch 30: Train Loss: 0.0012, Test Loss: 0.0016, Train L1 Norm: 0.0680, Test L1 Norm: 0.0272, Train Linf Norm: 51.0671, Test Linf Norm: 6.4458\n",
            "Epoch 31: Train Loss: 0.0008, Test Loss: 0.0008, Train L1 Norm: 0.0630, Test L1 Norm: 0.0239, Train Linf Norm: 46.2332, Test Linf Norm: 5.9084\n",
            "Epoch 32: Train Loss: 0.0008, Test Loss: 0.0008, Train L1 Norm: 0.0580, Test L1 Norm: 0.0232, Train Linf Norm: 42.1743, Test Linf Norm: 5.4457\n",
            "Epoch 33: Train Loss: 0.0009, Test Loss: 0.0008, Train L1 Norm: 0.0593, Test L1 Norm: 0.0215, Train Linf Norm: 40.8397, Test Linf Norm: 5.1168\n",
            "Epoch 34: Train Loss: 0.0008, Test Loss: 0.0009, Train L1 Norm: 0.0523, Test L1 Norm: 0.0234, Train Linf Norm: 36.5235, Test Linf Norm: 5.7340\n",
            "Epoch 35: Train Loss: 0.0008, Test Loss: 0.0009, Train L1 Norm: 0.0573, Test L1 Norm: 0.0220, Train Linf Norm: 41.4203, Test Linf Norm: 5.6175\n",
            "Epoch 36: Train Loss: 0.0007, Test Loss: 0.0007, Train L1 Norm: 0.0522, Test L1 Norm: 0.0213, Train Linf Norm: 36.6367, Test Linf Norm: 5.2707\n",
            "Epoch 37: Train Loss: 0.0006, Test Loss: 0.0007, Train L1 Norm: 0.0526, Test L1 Norm: 0.0214, Train Linf Norm: 37.3252, Test Linf Norm: 5.3076\n",
            "Epoch 38: Train Loss: 0.0006, Test Loss: 0.0007, Train L1 Norm: 0.0529, Test L1 Norm: 0.0208, Train Linf Norm: 37.4130, Test Linf Norm: 5.0116\n",
            "Epoch 39: Train Loss: 0.0006, Test Loss: 0.0007, Train L1 Norm: 0.0532, Test L1 Norm: 0.0215, Train Linf Norm: 38.1617, Test Linf Norm: 5.1091\n",
            "Epoch 40: Train Loss: 0.0006, Test Loss: 0.0007, Train L1 Norm: 0.0535, Test L1 Norm: 0.0208, Train Linf Norm: 38.0770, Test Linf Norm: 4.9353\n",
            "Epoch 41: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0513, Test L1 Norm: 0.0209, Train Linf Norm: 36.3954, Test Linf Norm: 5.1956\n",
            "Epoch 42: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0521, Test L1 Norm: 0.0209, Train Linf Norm: 37.8693, Test Linf Norm: 5.2516\n",
            "Epoch 43: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0501, Test L1 Norm: 0.0208, Train Linf Norm: 33.8446, Test Linf Norm: 5.1087\n",
            "Epoch 44: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0503, Test L1 Norm: 0.0207, Train Linf Norm: 30.4165, Test Linf Norm: 5.0767\n",
            "Epoch 45: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0496, Test L1 Norm: 0.0208, Train Linf Norm: 34.2575, Test Linf Norm: 5.2580\n",
            "Epoch 46: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.0510, Test L1 Norm: 0.0207, Train Linf Norm: 36.7388, Test Linf Norm: 5.1365\n",
            "Epoch 47: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.0496, Test L1 Norm: 0.0208, Train Linf Norm: 33.0738, Test Linf Norm: 5.0947\n",
            "Epoch 48: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.0497, Test L1 Norm: 0.0206, Train Linf Norm: 34.3331, Test Linf Norm: 5.0212\n",
            "Epoch 49: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.0499, Test L1 Norm: 0.0204, Train Linf Norm: 35.1716, Test Linf Norm: 5.0304\n",
            "Epoch 50: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.0499, Test L1 Norm: 0.0205, Train Linf Norm: 33.9432, Test Linf Norm: 5.0631\n",
            "Epoch 51: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.0498, Test L1 Norm: 0.0206, Train Linf Norm: 35.4498, Test Linf Norm: 5.0709\n",
            "Epoch 52: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.0495, Test L1 Norm: 0.0205, Train Linf Norm: 34.7243, Test Linf Norm: 5.0502\n",
            "Epoch 53: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.0505, Test L1 Norm: 0.0205, Train Linf Norm: 36.4939, Test Linf Norm: 5.0639\n",
            "Epoch 54: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.0489, Test L1 Norm: 0.0206, Train Linf Norm: 31.8405, Test Linf Norm: 5.0626\n",
            "Epoch 55: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.0494, Test L1 Norm: 0.0205, Train Linf Norm: 24.8138, Test Linf Norm: 5.0145\n",
            "Epoch 56: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.0496, Test L1 Norm: 0.0205, Train Linf Norm: 35.0356, Test Linf Norm: 5.0373\n",
            "Epoch 57: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.0494, Test L1 Norm: 0.0205, Train Linf Norm: 34.0756, Test Linf Norm: 5.0578\n",
            "Epoch 58: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.0498, Test L1 Norm: 0.0204, Train Linf Norm: 31.8871, Test Linf Norm: 5.0331\n",
            "Epoch 59: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0494, Test L1 Norm: 0.0204, Train Linf Norm: 34.9829, Test Linf Norm: 4.9955\n",
            "Epoch 60: Train Loss: 0.0005, Test Loss: 0.0006, Train L1 Norm: 0.0494, Test L1 Norm: 0.0205, Train Linf Norm: 34.5739, Test Linf Norm: 5.0368\n",
            "Epoch 61: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0493, Test L1 Norm: 0.0204, Train Linf Norm: 34.7165, Test Linf Norm: 5.0385\n",
            "Epoch 62: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0494, Test L1 Norm: 0.0204, Train Linf Norm: 34.3606, Test Linf Norm: 5.0300\n",
            "Epoch 63: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0495, Test L1 Norm: 0.0204, Train Linf Norm: 34.7856, Test Linf Norm: 5.0331\n",
            "Epoch 64: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0493, Test L1 Norm: 0.0204, Train Linf Norm: 32.0235, Test Linf Norm: 5.0274\n",
            "Epoch 65: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0494, Test L1 Norm: 0.0205, Train Linf Norm: 34.9169, Test Linf Norm: 5.0499\n",
            "Epoch 66: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0494, Test L1 Norm: 0.0204, Train Linf Norm: 34.9836, Test Linf Norm: 5.0273\n",
            "Epoch 67: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.4259, Test Linf Norm: 5.0273\n",
            "Epoch 68: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0493, Test L1 Norm: 0.0204, Train Linf Norm: 34.2908, Test Linf Norm: 5.0339\n",
            "Epoch 69: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0493, Test L1 Norm: 0.0204, Train Linf Norm: 34.2315, Test Linf Norm: 5.0294\n",
            "Epoch 70: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0493, Test L1 Norm: 0.0204, Train Linf Norm: 34.2968, Test Linf Norm: 5.0282\n",
            "Epoch 71: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0493, Test L1 Norm: 0.0204, Train Linf Norm: 33.8523, Test Linf Norm: 5.0260\n",
            "Epoch 72: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.7583, Test Linf Norm: 5.0234\n",
            "Epoch 73: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0493, Test L1 Norm: 0.0204, Train Linf Norm: 34.3126, Test Linf Norm: 5.0202\n",
            "Epoch 74: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.3769, Test Linf Norm: 5.0219\n",
            "Epoch 75: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.4269, Test Linf Norm: 5.0236\n",
            "Epoch 76: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0493, Test L1 Norm: 0.0204, Train Linf Norm: 34.7418, Test Linf Norm: 5.0235\n",
            "Epoch 77: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 31.6785, Test Linf Norm: 5.0224\n",
            "Epoch 78: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 35.7270, Test Linf Norm: 5.0218\n",
            "Epoch 79: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.5469, Test Linf Norm: 5.0231\n",
            "Epoch 80: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.7198, Test Linf Norm: 5.0238\n",
            "Epoch 81: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.4327, Test Linf Norm: 5.0236\n",
            "Epoch 82: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.1269, Test Linf Norm: 5.0239\n",
            "Epoch 83: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 35.0148, Test Linf Norm: 5.0240\n",
            "Epoch 84: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0493, Test L1 Norm: 0.0204, Train Linf Norm: 31.0004, Test Linf Norm: 5.0240\n",
            "Epoch 85: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.0920, Test Linf Norm: 5.0238\n",
            "Epoch 86: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.4301, Test Linf Norm: 5.0237\n",
            "Epoch 87: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 35.3308, Test Linf Norm: 5.0237\n",
            "Epoch 88: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.2131, Test Linf Norm: 5.0236\n",
            "Epoch 89: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.5901, Test Linf Norm: 5.0233\n",
            "Epoch 90: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.8401, Test Linf Norm: 5.0236\n",
            "Epoch 91: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.5482, Test Linf Norm: 5.0236\n",
            "Epoch 92: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 35.0859, Test Linf Norm: 5.0236\n",
            "Epoch 93: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.1037, Test Linf Norm: 5.0237\n",
            "Epoch 94: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.8441, Test Linf Norm: 5.0237\n",
            "Epoch 95: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.9908, Test Linf Norm: 5.0237\n",
            "Epoch 96: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.2435, Test Linf Norm: 5.0236\n",
            "Epoch 97: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.0487, Test Linf Norm: 5.0237\n",
            "Epoch 98: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.6972, Test Linf Norm: 5.0236\n",
            "Epoch 99: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.1427, Test Linf Norm: 5.0236\n",
            "Epoch 100: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 35.2335, Test Linf Norm: 5.0237\n",
            "Epoch 101: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.7735, Test Linf Norm: 5.0237\n",
            "Epoch 102: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.2099, Test Linf Norm: 5.0237\n",
            "Epoch 103: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 32.3384, Test Linf Norm: 5.0237\n",
            "Epoch 104: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.3297, Test Linf Norm: 5.0237\n",
            "Epoch 105: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.4233, Test Linf Norm: 5.0237\n",
            "Epoch 106: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 32.1800, Test Linf Norm: 5.0237\n",
            "Epoch 107: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.5890, Test Linf Norm: 5.0237\n",
            "Epoch 108: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.5796, Test Linf Norm: 5.0237\n",
            "Epoch 109: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.3793, Test Linf Norm: 5.0237\n",
            "Epoch 110: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.0165, Test Linf Norm: 5.0237\n",
            "Epoch 111: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.0533, Test Linf Norm: 5.0237\n",
            "Epoch 112: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.4848, Test Linf Norm: 5.0237\n",
            "Epoch 113: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 35.3917, Test Linf Norm: 5.0237\n",
            "Epoch 114: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 29.9680, Test Linf Norm: 5.0237\n",
            "Epoch 115: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.7323, Test Linf Norm: 5.0237\n",
            "Epoch 116: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.7568, Test Linf Norm: 5.0237\n",
            "Epoch 117: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.4251, Test Linf Norm: 5.0237\n",
            "Epoch 118: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.4783, Test Linf Norm: 5.0237\n",
            "Epoch 119: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.2623, Test Linf Norm: 5.0237\n",
            "Epoch 120: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.5842, Test Linf Norm: 5.0237\n",
            "Epoch 121: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.2057, Test Linf Norm: 5.0237\n",
            "Epoch 122: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.0557, Test Linf Norm: 5.0237\n",
            "Epoch 123: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.5595, Test Linf Norm: 5.0237\n",
            "Epoch 124: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 32.7069, Test Linf Norm: 5.0237\n",
            "Epoch 125: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.0871, Test Linf Norm: 5.0237\n",
            "Epoch 126: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.4790, Test Linf Norm: 5.0237\n",
            "Epoch 127: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.0312, Test Linf Norm: 5.0237\n",
            "Epoch 128: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.9876, Test Linf Norm: 5.0237\n",
            "Epoch 129: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.3355, Test Linf Norm: 5.0237\n",
            "Epoch 130: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.1022, Test Linf Norm: 5.0237\n",
            "Epoch 131: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.9003, Test Linf Norm: 5.0237\n",
            "Epoch 132: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.4424, Test Linf Norm: 5.0237\n",
            "Epoch 133: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.3903, Test Linf Norm: 5.0237\n",
            "Epoch 134: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.3255, Test Linf Norm: 5.0237\n",
            "Epoch 135: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.2307, Test Linf Norm: 5.0237\n",
            "Epoch 136: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.1039, Test Linf Norm: 5.0237\n",
            "Epoch 137: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.8825, Test Linf Norm: 5.0237\n",
            "Epoch 138: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 35.3009, Test Linf Norm: 5.0237\n",
            "Epoch 139: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.9104, Test Linf Norm: 5.0237\n",
            "Epoch 140: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.5228, Test Linf Norm: 5.0237\n",
            "Epoch 141: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.8789, Test Linf Norm: 5.0237\n",
            "Epoch 142: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.6545, Test Linf Norm: 5.0237\n",
            "Epoch 143: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.7560, Test Linf Norm: 5.0237\n",
            "Epoch 144: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.6852, Test Linf Norm: 5.0237\n",
            "Epoch 145: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.0445, Test Linf Norm: 5.0237\n",
            "Epoch 146: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.6986, Test Linf Norm: 5.0237\n",
            "Epoch 147: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.3305, Test Linf Norm: 5.0237\n",
            "Epoch 148: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.6245, Test Linf Norm: 5.0237\n",
            "Epoch 149: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.6764, Test Linf Norm: 5.0237\n",
            "Epoch 150: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.6053, Test Linf Norm: 5.0237\n",
            "Epoch 151: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.9649, Test Linf Norm: 5.0237\n",
            "Epoch 152: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.3882, Test Linf Norm: 5.0237\n",
            "Epoch 153: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.5704, Test Linf Norm: 5.0237\n",
            "Epoch 154: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 31.9204, Test Linf Norm: 5.0237\n",
            "Epoch 155: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.1759, Test Linf Norm: 5.0237\n",
            "Epoch 156: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.0436, Test Linf Norm: 5.0237\n",
            "Epoch 157: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.5074, Test Linf Norm: 5.0237\n",
            "Epoch 158: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.8991, Test Linf Norm: 5.0237\n",
            "Epoch 159: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.7475, Test Linf Norm: 5.0237\n",
            "Epoch 160: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.8317, Test Linf Norm: 5.0237\n",
            "Epoch 161: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.6098, Test Linf Norm: 5.0237\n",
            "Epoch 162: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.5497, Test Linf Norm: 5.0237\n",
            "Epoch 163: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.3849, Test Linf Norm: 5.0237\n",
            "Epoch 164: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.8181, Test Linf Norm: 5.0237\n",
            "Epoch 165: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.7094, Test Linf Norm: 5.0237\n",
            "Epoch 166: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.7103, Test Linf Norm: 5.0237\n",
            "Epoch 167: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.5401, Test Linf Norm: 5.0237\n",
            "Epoch 168: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.8966, Test Linf Norm: 5.0237\n",
            "Epoch 169: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.5349, Test Linf Norm: 5.0237\n",
            "Epoch 170: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.2412, Test Linf Norm: 5.0237\n",
            "Epoch 171: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.9396, Test Linf Norm: 5.0237\n",
            "Epoch 172: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.0149, Test Linf Norm: 5.0237\n",
            "Epoch 173: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.3036, Test Linf Norm: 5.0237\n",
            "Epoch 174: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.9211, Test Linf Norm: 5.0237\n",
            "Epoch 175: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 32.1486, Test Linf Norm: 5.0237\n",
            "Epoch 176: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.6473, Test Linf Norm: 5.0237\n",
            "Epoch 177: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.3790, Test Linf Norm: 5.0237\n",
            "Epoch 178: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 31.9393, Test Linf Norm: 5.0237\n",
            "Epoch 179: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.5261, Test Linf Norm: 5.0237\n",
            "Epoch 180: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.5127, Test Linf Norm: 5.0237\n",
            "Epoch 181: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.7408, Test Linf Norm: 5.0237\n",
            "Epoch 182: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.6754, Test Linf Norm: 5.0237\n",
            "Epoch 183: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.5771, Test Linf Norm: 5.0237\n",
            "Epoch 184: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.5323, Test Linf Norm: 5.0237\n",
            "Epoch 185: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.2166, Test Linf Norm: 5.0237\n",
            "Epoch 186: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 33.6608, Test Linf Norm: 5.0237\n",
            "Epoch 187: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 34.7143, Test Linf Norm: 5.0237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 18:29:05,596]\u001b[0m Trial 12 finished with value: 0.020384017257392407 and parameters: {'n_layers': 7, 'n_units_0': 1668, 'n_units_1': 851, 'n_units_2': 509, 'n_units_3': 1448, 'n_units_4': 880, 'n_units_5': 1491, 'n_units_6': 1346, 'hidden_activation': 'ReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'RMSprop', 'lr': 0.00010317627880130034, 'batch_size': 1048, 'n_epochs': 188, 'scheduler': 'StepLR', 'step_size': 5, 'gamma': 0.485765757224174}. Best is trial 2 with value: 0.011449855588376521.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 188: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0492, Test L1 Norm: 0.0204, Train Linf Norm: 35.4610, Test Linf Norm: 5.0237\n",
            "Epoch 1: Train Loss: 3.1927, Test Loss: 0.5339, Train L1 Norm: 1.8095, Test L1 Norm: 0.2910, Train Linf Norm: 997.5285, Test Linf Norm: 54.8085\n",
            "Epoch 2: Train Loss: 0.6929, Test Loss: 0.4034, Train L1 Norm: 0.9592, Test L1 Norm: 0.7870, Train Linf Norm: 619.1240, Test Linf Norm: 200.7833\n",
            "Epoch 3: Train Loss: 0.3833, Test Loss: 0.6361, Train L1 Norm: 0.8699, Test L1 Norm: 0.2032, Train Linf Norm: 613.1814, Test Linf Norm: 9.9175\n",
            "Epoch 4: Train Loss: 0.3967, Test Loss: 0.2639, Train L1 Norm: 0.7978, Test L1 Norm: 0.3271, Train Linf Norm: 494.4260, Test Linf Norm: 71.6950\n",
            "Epoch 5: Train Loss: 0.3120, Test Loss: 0.4018, Train L1 Norm: 0.5005, Test L1 Norm: 0.1858, Train Linf Norm: 250.8498, Test Linf Norm: 28.8892\n",
            "Epoch 6: Train Loss: 0.2973, Test Loss: 0.1733, Train L1 Norm: 0.4739, Test L1 Norm: 0.2676, Train Linf Norm: 265.7953, Test Linf Norm: 60.9309\n",
            "Epoch 7: Train Loss: 0.2311, Test Loss: 0.3766, Train L1 Norm: 0.6625, Test L1 Norm: 0.1635, Train Linf Norm: 468.7499, Test Linf Norm: 12.6109\n",
            "Epoch 8: Train Loss: 0.2445, Test Loss: 0.4092, Train L1 Norm: 0.4568, Test L1 Norm: 0.3195, Train Linf Norm: 271.7006, Test Linf Norm: 58.7075\n",
            "Epoch 9: Train Loss: 0.2361, Test Loss: 0.2323, Train L1 Norm: 0.5073, Test L1 Norm: 0.1255, Train Linf Norm: 306.8623, Test Linf Norm: 9.3163\n",
            "Epoch 10: Train Loss: 0.2361, Test Loss: 0.0920, Train L1 Norm: 0.5143, Test L1 Norm: 0.1896, Train Linf Norm: 315.5643, Test Linf Norm: 41.1726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 18:29:19,442]\u001b[0m Trial 13 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: Train Loss: 0.2025, Test Loss: 0.3236, Train L1 Norm: 0.6372, Test L1 Norm: 0.1655, Train Linf Norm: 477.4285, Test Linf Norm: 9.9347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 18:29:21,279]\u001b[0m Trial 14 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 5.1639, Test Loss: 0.7978, Train L1 Norm: 3.1817, Test L1 Norm: 0.6876, Train Linf Norm: 2241.5334, Test Linf Norm: 172.4706\n",
            "Epoch 1: Train Loss: 26.3778, Test Loss: 0.9543, Train L1 Norm: 1.4915, Test L1 Norm: 0.2953, Train Linf Norm: 223.3199, Test Linf Norm: 16.8641\n",
            "Epoch 2: Train Loss: 0.7931, Test Loss: 0.4742, Train L1 Norm: 0.6265, Test L1 Norm: 0.4032, Train Linf Norm: 92.5832, Test Linf Norm: 28.0889\n",
            "Epoch 3: Train Loss: 0.6935, Test Loss: 0.1345, Train L1 Norm: 0.8606, Test L1 Norm: 0.4666, Train Linf Norm: 159.6365, Test Linf Norm: 48.4588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 18:29:33,948]\u001b[0m Trial 15 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss: 0.4944, Test Loss: 0.9390, Train L1 Norm: 0.7087, Test L1 Norm: 0.4121, Train Linf Norm: 130.4846, Test Linf Norm: 24.7016\n",
            "Epoch 1: Train Loss: 0.2125, Test Loss: 0.0246, Train L1 Norm: 1.2415, Test L1 Norm: 0.2145, Train Linf Norm: 53.1049, Test Linf Norm: 7.2409\n",
            "Epoch 2: Train Loss: 0.0611, Test Loss: 0.0230, Train L1 Norm: 0.6325, Test L1 Norm: 0.1243, Train Linf Norm: 26.8890, Test Linf Norm: 3.8629\n",
            "Epoch 3: Train Loss: 0.0501, Test Loss: 0.0114, Train L1 Norm: 0.4600, Test L1 Norm: 0.1279, Train Linf Norm: 18.8447, Test Linf Norm: 4.2428\n",
            "Epoch 4: Train Loss: 0.0627, Test Loss: 0.0199, Train L1 Norm: 0.5239, Test L1 Norm: 0.1986, Train Linf Norm: 21.7931, Test Linf Norm: 6.9309\n",
            "Epoch 5: Train Loss: 0.0205, Test Loss: 0.0216, Train L1 Norm: 0.5092, Test L1 Norm: 0.1464, Train Linf Norm: 22.1624, Test Linf Norm: 4.9350\n",
            "Epoch 6: Train Loss: 0.0306, Test Loss: 0.0131, Train L1 Norm: 0.4526, Test L1 Norm: 0.1395, Train Linf Norm: 19.1612, Test Linf Norm: 4.8541\n",
            "Epoch 7: Train Loss: 0.0164, Test Loss: 0.0075, Train L1 Norm: 0.4292, Test L1 Norm: 0.1287, Train Linf Norm: 18.6695, Test Linf Norm: 4.5127\n",
            "Epoch 8: Train Loss: 0.0084, Test Loss: 0.0743, Train L1 Norm: 0.3762, Test L1 Norm: 0.1270, Train Linf Norm: 16.4875, Test Linf Norm: 3.1021\n",
            "Epoch 9: Train Loss: 0.0192, Test Loss: 0.0280, Train L1 Norm: 0.3339, Test L1 Norm: 0.1862, Train Linf Norm: 14.1823, Test Linf Norm: 5.3831\n",
            "Epoch 10: Train Loss: 0.0195, Test Loss: 0.0114, Train L1 Norm: 0.4463, Test L1 Norm: 0.1408, Train Linf Norm: 19.3275, Test Linf Norm: 5.0816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 18:30:55,803]\u001b[0m Trial 16 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: Train Loss: 0.0315, Test Loss: 0.0140, Train L1 Norm: 0.4598, Test L1 Norm: 0.1648, Train Linf Norm: 19.5623, Test Linf Norm: 5.9769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 18:30:57,923]\u001b[0m Trial 17 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 3.4098, Test Loss: 3.4549, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 1.4079, Test Loss: 0.0592, Train L1 Norm: 0.8271, Test L1 Norm: 0.1828, Train Linf Norm: 30.1594, Test Linf Norm: 5.3239\n",
            "Epoch 2: Train Loss: 0.1910, Test Loss: 0.0340, Train L1 Norm: 0.4228, Test L1 Norm: 0.1693, Train Linf Norm: 15.8667, Test Linf Norm: 5.2114\n",
            "Epoch 3: Train Loss: 0.1217, Test Loss: 0.0130, Train L1 Norm: 0.3567, Test L1 Norm: 0.1134, Train Linf Norm: 13.7888, Test Linf Norm: 3.7033\n",
            "Epoch 4: Train Loss: 0.0951, Test Loss: 0.0082, Train L1 Norm: 0.2613, Test L1 Norm: 0.0575, Train Linf Norm: 9.7251, Test Linf Norm: 1.4929\n",
            "Epoch 5: Train Loss: 0.0775, Test Loss: 0.0086, Train L1 Norm: 0.3525, Test L1 Norm: 0.0543, Train Linf Norm: 14.3510, Test Linf Norm: 1.5456\n",
            "Epoch 6: Train Loss: 0.0124, Test Loss: 0.0019, Train L1 Norm: 0.1325, Test L1 Norm: 0.0290, Train Linf Norm: 5.2406, Test Linf Norm: 0.7485\n",
            "Epoch 7: Train Loss: 0.0123, Test Loss: 0.0019, Train L1 Norm: 0.0967, Test L1 Norm: 0.0302, Train Linf Norm: 3.5731, Test Linf Norm: 0.5725\n",
            "Epoch 8: Train Loss: 0.0123, Test Loss: 0.2537, Train L1 Norm: 0.0911, Test L1 Norm: 0.1500, Train Linf Norm: 3.4105, Test Linf Norm: 1.3955\n",
            "Epoch 9: Train Loss: 0.0120, Test Loss: 0.0012, Train L1 Norm: 0.1043, Test L1 Norm: 0.0477, Train Linf Norm: 3.9677, Test Linf Norm: 1.5534\n",
            "Epoch 10: Train Loss: 0.0110, Test Loss: 0.0035, Train L1 Norm: 0.0792, Test L1 Norm: 0.0376, Train Linf Norm: 2.8829, Test Linf Norm: 0.5001\n",
            "Epoch 11: Train Loss: 0.0023, Test Loss: 0.0309, Train L1 Norm: 0.0368, Test L1 Norm: 0.0589, Train Linf Norm: 1.2321, Test Linf Norm: 1.0894\n",
            "Epoch 12: Train Loss: 0.0024, Test Loss: 0.0009, Train L1 Norm: 0.0553, Test L1 Norm: 0.0187, Train Linf Norm: 2.1426, Test Linf Norm: 0.4616\n",
            "Epoch 13: Train Loss: 0.0020, Test Loss: 0.0004, Train L1 Norm: 0.0515, Test L1 Norm: 0.0161, Train Linf Norm: 1.9490, Test Linf Norm: 0.4215\n",
            "Epoch 14: Train Loss: 0.0022, Test Loss: 0.0006, Train L1 Norm: 0.0512, Test L1 Norm: 0.0181, Train Linf Norm: 1.9614, Test Linf Norm: 0.4986\n",
            "Epoch 15: Train Loss: 0.0020, Test Loss: 0.0006, Train L1 Norm: 0.0584, Test L1 Norm: 0.0228, Train Linf Norm: 2.2779, Test Linf Norm: 0.6975\n",
            "Epoch 16: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0393, Test L1 Norm: 0.0147, Train Linf Norm: 1.5520, Test Linf Norm: 0.3764\n",
            "Epoch 17: Train Loss: 0.0006, Test Loss: 0.0004, Train L1 Norm: 0.0375, Test L1 Norm: 0.0180, Train Linf Norm: 1.4727, Test Linf Norm: 0.5252\n",
            "Epoch 18: Train Loss: 0.0005, Test Loss: 0.0008, Train L1 Norm: 0.0302, Test L1 Norm: 0.0168, Train Linf Norm: 1.1256, Test Linf Norm: 0.4521\n",
            "Epoch 19: Train Loss: 0.0006, Test Loss: 0.0003, Train L1 Norm: 0.0307, Test L1 Norm: 0.0143, Train Linf Norm: 1.1462, Test Linf Norm: 0.3894\n",
            "Epoch 20: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0374, Test L1 Norm: 0.0142, Train Linf Norm: 1.4728, Test Linf Norm: 0.3695\n",
            "Epoch 21: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0295, Test L1 Norm: 0.0130, Train Linf Norm: 1.1462, Test Linf Norm: 0.3512\n",
            "Epoch 22: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0291, Test L1 Norm: 0.0130, Train Linf Norm: 1.1321, Test Linf Norm: 0.3462\n",
            "Epoch 23: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0267, Test L1 Norm: 0.0133, Train Linf Norm: 1.0183, Test Linf Norm: 0.3503\n",
            "Epoch 24: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0217, Test L1 Norm: 0.0129, Train Linf Norm: 0.7773, Test Linf Norm: 0.3451\n",
            "Epoch 25: Train Loss: 0.0003, Test Loss: 0.0004, Train L1 Norm: 0.0256, Test L1 Norm: 0.0128, Train Linf Norm: 0.9714, Test Linf Norm: 0.2980\n",
            "Epoch 26: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0197, Test L1 Norm: 0.0118, Train Linf Norm: 0.7004, Test Linf Norm: 0.3091\n",
            "Epoch 27: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0210, Test L1 Norm: 0.0123, Train Linf Norm: 0.7590, Test Linf Norm: 0.3425\n",
            "Epoch 28: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0210, Test L1 Norm: 0.0120, Train Linf Norm: 0.7648, Test Linf Norm: 0.3283\n",
            "Epoch 29: Train Loss: 0.0002, Test Loss: 0.0003, Train L1 Norm: 0.0192, Test L1 Norm: 0.0122, Train Linf Norm: 0.6781, Test Linf Norm: 0.3232\n",
            "Epoch 30: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0216, Test L1 Norm: 0.0120, Train Linf Norm: 0.7969, Test Linf Norm: 0.3251\n",
            "Epoch 31: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0200, Test L1 Norm: 0.0122, Train Linf Norm: 0.7208, Test Linf Norm: 0.3359\n",
            "Epoch 32: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0203, Test L1 Norm: 0.0116, Train Linf Norm: 0.7408, Test Linf Norm: 0.3108\n",
            "Epoch 33: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0198, Test L1 Norm: 0.0118, Train Linf Norm: 0.7141, Test Linf Norm: 0.3202\n",
            "Epoch 34: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0210, Test L1 Norm: 0.0118, Train Linf Norm: 0.7735, Test Linf Norm: 0.3196\n",
            "Epoch 35: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0205, Test L1 Norm: 0.0119, Train Linf Norm: 0.7489, Test Linf Norm: 0.3205\n",
            "Epoch 36: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0195, Test L1 Norm: 0.0117, Train Linf Norm: 0.7000, Test Linf Norm: 0.3188\n",
            "Epoch 37: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0197, Test L1 Norm: 0.0120, Train Linf Norm: 0.7116, Test Linf Norm: 0.3313\n",
            "Epoch 38: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0203, Test L1 Norm: 0.0117, Train Linf Norm: 0.7391, Test Linf Norm: 0.3172\n",
            "Epoch 39: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0190, Test L1 Norm: 0.0116, Train Linf Norm: 0.6810, Test Linf Norm: 0.3140\n",
            "Epoch 40: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0204, Test L1 Norm: 0.0118, Train Linf Norm: 0.7479, Test Linf Norm: 0.3198\n",
            "Epoch 41: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0189, Test L1 Norm: 0.0118, Train Linf Norm: 0.6730, Test Linf Norm: 0.3207\n",
            "Epoch 42: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0196, Test L1 Norm: 0.0116, Train Linf Norm: 0.7073, Test Linf Norm: 0.3152\n",
            "Epoch 43: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0200, Test L1 Norm: 0.0117, Train Linf Norm: 0.7305, Test Linf Norm: 0.3175\n",
            "Epoch 44: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0194, Test L1 Norm: 0.0116, Train Linf Norm: 0.7030, Test Linf Norm: 0.3141\n",
            "Epoch 45: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0197, Test L1 Norm: 0.0117, Train Linf Norm: 0.7177, Test Linf Norm: 0.3196\n",
            "Epoch 46: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6945, Test Linf Norm: 0.3176\n",
            "Epoch 47: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6935, Test Linf Norm: 0.3188\n",
            "Epoch 48: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6955, Test Linf Norm: 0.3181\n",
            "Epoch 49: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0191, Test L1 Norm: 0.0117, Train Linf Norm: 0.6848, Test Linf Norm: 0.3168\n",
            "Epoch 50: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0191, Test L1 Norm: 0.0117, Train Linf Norm: 0.6853, Test Linf Norm: 0.3173\n",
            "Epoch 51: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0195, Test L1 Norm: 0.0117, Train Linf Norm: 0.7042, Test Linf Norm: 0.3165\n",
            "Epoch 52: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0194, Test L1 Norm: 0.0117, Train Linf Norm: 0.6986, Test Linf Norm: 0.3184\n",
            "Epoch 53: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0194, Test L1 Norm: 0.0117, Train Linf Norm: 0.6974, Test Linf Norm: 0.3171\n",
            "Epoch 54: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6946, Test Linf Norm: 0.3166\n",
            "Epoch 55: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6951, Test Linf Norm: 0.3166\n",
            "Epoch 56: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6954, Test Linf Norm: 0.3170\n",
            "Epoch 57: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0194, Test L1 Norm: 0.0117, Train Linf Norm: 0.6936, Test Linf Norm: 0.3168\n",
            "Epoch 58: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6975, Test Linf Norm: 0.3167\n",
            "Epoch 59: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6891, Test Linf Norm: 0.3172\n",
            "Epoch 60: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0194, Test L1 Norm: 0.0117, Train Linf Norm: 0.7001, Test Linf Norm: 0.3175\n",
            "Epoch 61: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0194, Test L1 Norm: 0.0117, Train Linf Norm: 0.6989, Test Linf Norm: 0.3174\n",
            "Epoch 62: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6932, Test Linf Norm: 0.3173\n",
            "Epoch 63: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6936, Test Linf Norm: 0.3173\n",
            "Epoch 64: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6958, Test Linf Norm: 0.3173\n",
            "Epoch 65: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6935, Test Linf Norm: 0.3173\n",
            "Epoch 66: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6944, Test Linf Norm: 0.3173\n",
            "Epoch 67: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6967, Test Linf Norm: 0.3172\n",
            "Epoch 68: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6966, Test Linf Norm: 0.3172\n",
            "Epoch 69: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6948, Test Linf Norm: 0.3172\n",
            "Epoch 70: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6909, Test Linf Norm: 0.3172\n",
            "Epoch 71: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6879, Test Linf Norm: 0.3172\n",
            "Epoch 72: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6941, Test Linf Norm: 0.3172\n",
            "Epoch 73: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6953, Test Linf Norm: 0.3172\n",
            "Epoch 74: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6974, Test Linf Norm: 0.3172\n",
            "Epoch 75: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6913, Test Linf Norm: 0.3171\n",
            "Epoch 76: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6890, Test Linf Norm: 0.3171\n",
            "Epoch 77: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6933, Test Linf Norm: 0.3171\n",
            "Epoch 78: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6938, Test Linf Norm: 0.3171\n",
            "Epoch 79: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6922, Test Linf Norm: 0.3171\n",
            "Epoch 80: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6961, Test Linf Norm: 0.3171\n",
            "Epoch 81: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6965, Test Linf Norm: 0.3171\n",
            "Epoch 82: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6939, Test Linf Norm: 0.3171\n",
            "Epoch 83: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6961, Test Linf Norm: 0.3171\n",
            "Epoch 84: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6934, Test Linf Norm: 0.3171\n",
            "Epoch 85: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6941, Test Linf Norm: 0.3171\n",
            "Epoch 86: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6946, Test Linf Norm: 0.3171\n",
            "Epoch 87: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6939, Test Linf Norm: 0.3171\n",
            "Epoch 88: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6937, Test Linf Norm: 0.3171\n",
            "Epoch 89: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6947, Test Linf Norm: 0.3171\n",
            "Epoch 90: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6955, Test Linf Norm: 0.3171\n",
            "Epoch 91: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6865, Test Linf Norm: 0.3171\n",
            "Epoch 92: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6917, Test Linf Norm: 0.3171\n",
            "Epoch 93: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6904, Test Linf Norm: 0.3171\n",
            "Epoch 94: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6924, Test Linf Norm: 0.3171\n",
            "Epoch 95: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6950, Test Linf Norm: 0.3171\n",
            "Epoch 96: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6935, Test Linf Norm: 0.3171\n",
            "Epoch 97: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6963, Test Linf Norm: 0.3171\n",
            "Epoch 98: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6924, Test Linf Norm: 0.3171\n",
            "Epoch 99: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6951, Test Linf Norm: 0.3171\n",
            "Epoch 100: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6941, Test Linf Norm: 0.3171\n",
            "Epoch 101: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6963, Test Linf Norm: 0.3171\n",
            "Epoch 102: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6947, Test Linf Norm: 0.3171\n",
            "Epoch 103: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6935, Test Linf Norm: 0.3171\n",
            "Epoch 104: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6945, Test Linf Norm: 0.3171\n",
            "Epoch 105: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6948, Test Linf Norm: 0.3171\n",
            "Epoch 106: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6948, Test Linf Norm: 0.3171\n",
            "Epoch 107: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6945, Test Linf Norm: 0.3171\n",
            "Epoch 108: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6950, Test Linf Norm: 0.3171\n",
            "Epoch 109: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6908, Test Linf Norm: 0.3171\n",
            "Epoch 110: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6902, Test Linf Norm: 0.3171\n",
            "Epoch 111: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6932, Test Linf Norm: 0.3171\n",
            "Epoch 112: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6966, Test Linf Norm: 0.3171\n",
            "Epoch 113: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6931, Test Linf Norm: 0.3171\n",
            "Epoch 114: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6930, Test Linf Norm: 0.3171\n",
            "Epoch 115: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6953, Test Linf Norm: 0.3171\n",
            "Epoch 116: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6955, Test Linf Norm: 0.3171\n",
            "Epoch 117: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6941, Test Linf Norm: 0.3171\n",
            "Epoch 118: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6960, Test Linf Norm: 0.3171\n",
            "Epoch 119: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6942, Test Linf Norm: 0.3171\n",
            "Epoch 120: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6953, Test Linf Norm: 0.3171\n",
            "Epoch 121: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6920, Test Linf Norm: 0.3171\n",
            "Epoch 122: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6955, Test Linf Norm: 0.3171\n",
            "Epoch 123: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6950, Test Linf Norm: 0.3171\n",
            "Epoch 124: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6928, Test Linf Norm: 0.3171\n",
            "Epoch 125: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6920, Test Linf Norm: 0.3171\n",
            "Epoch 126: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6915, Test Linf Norm: 0.3171\n",
            "Epoch 127: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6953, Test Linf Norm: 0.3171\n",
            "Epoch 128: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6937, Test Linf Norm: 0.3171\n",
            "Epoch 129: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6949, Test Linf Norm: 0.3171\n",
            "Epoch 130: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6929, Test Linf Norm: 0.3171\n",
            "Epoch 131: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6959, Test Linf Norm: 0.3171\n",
            "Epoch 132: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6921, Test Linf Norm: 0.3171\n",
            "Epoch 133: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6953, Test Linf Norm: 0.3171\n",
            "Epoch 134: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6899, Test Linf Norm: 0.3171\n",
            "Epoch 135: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6917, Test Linf Norm: 0.3171\n",
            "Epoch 136: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6937, Test Linf Norm: 0.3171\n",
            "Epoch 137: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6942, Test Linf Norm: 0.3171\n",
            "Epoch 138: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6842, Test Linf Norm: 0.3171\n",
            "Epoch 139: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6954, Test Linf Norm: 0.3171\n",
            "Epoch 140: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6939, Test Linf Norm: 0.3171\n",
            "Epoch 141: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6969, Test Linf Norm: 0.3171\n",
            "Epoch 142: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6949, Test Linf Norm: 0.3171\n",
            "Epoch 143: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6889, Test Linf Norm: 0.3171\n",
            "Epoch 144: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6945, Test Linf Norm: 0.3171\n",
            "Epoch 145: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6968, Test Linf Norm: 0.3171\n",
            "Epoch 146: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6953, Test Linf Norm: 0.3171\n",
            "Epoch 147: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6941, Test Linf Norm: 0.3171\n",
            "Epoch 148: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6950, Test Linf Norm: 0.3171\n",
            "Epoch 149: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6946, Test Linf Norm: 0.3171\n",
            "Epoch 150: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6927, Test Linf Norm: 0.3171\n",
            "Epoch 151: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6939, Test Linf Norm: 0.3171\n",
            "Epoch 152: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6944, Test Linf Norm: 0.3171\n",
            "Epoch 153: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6962, Test Linf Norm: 0.3171\n",
            "Epoch 154: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6949, Test Linf Norm: 0.3171\n",
            "Epoch 155: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6958, Test Linf Norm: 0.3171\n",
            "Epoch 156: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6959, Test Linf Norm: 0.3171\n",
            "Epoch 157: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6936, Test Linf Norm: 0.3171\n",
            "Epoch 158: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6882, Test Linf Norm: 0.3171\n",
            "Epoch 159: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6845, Test Linf Norm: 0.3171\n",
            "Epoch 160: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6922, Test Linf Norm: 0.3171\n",
            "Epoch 161: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6963, Test Linf Norm: 0.3171\n",
            "Epoch 162: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6867, Test Linf Norm: 0.3171\n",
            "Epoch 163: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6950, Test Linf Norm: 0.3171\n",
            "Epoch 164: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6931, Test Linf Norm: 0.3171\n",
            "Epoch 165: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6944, Test Linf Norm: 0.3171\n",
            "Epoch 166: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6931, Test Linf Norm: 0.3171\n",
            "Epoch 167: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6961, Test Linf Norm: 0.3171\n",
            "Epoch 168: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6937, Test Linf Norm: 0.3171\n",
            "Epoch 169: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6941, Test Linf Norm: 0.3171\n",
            "Epoch 170: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6903, Test Linf Norm: 0.3171\n",
            "Epoch 171: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6888, Test Linf Norm: 0.3171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 18:52:57,577]\u001b[0m Trial 18 finished with value: 0.011670211167633534 and parameters: {'n_layers': 7, 'n_units_0': 954, 'n_units_1': 832, 'n_units_2': 1043, 'n_units_3': 1274, 'n_units_4': 669, 'n_units_5': 1201, 'n_units_6': 1071, 'hidden_activation': 'ReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'RMSprop', 'lr': 0.00010559355324679815, 'batch_size': 48, 'n_epochs': 172, 'scheduler': 'StepLR', 'step_size': 5, 'gamma': 0.39962493667622123}. Best is trial 2 with value: 0.011449855588376521.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 172: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0193, Test L1 Norm: 0.0117, Train Linf Norm: 0.6937, Test Linf Norm: 0.3171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-82e2252adf92>:146: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  t_max_fraction = trial.suggest_uniform('t_max_fraction', 0.1, 0.3)\n",
            "\u001b[32m[I 2023-05-21 18:53:07,227]\u001b[0m Trial 19 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 1736.8865, Test Loss: 21.1527, Train L1 Norm: 1.2091, Test L1 Norm: 1.0000, Train Linf Norm: 1.9871, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.1907, Test Loss: 0.1129, Train L1 Norm: 0.3391, Test L1 Norm: 0.0911, Train Linf Norm: 12.3962, Test Linf Norm: 2.0882\n",
            "Epoch 2: Train Loss: 0.0899, Test Loss: 0.0697, Train L1 Norm: 0.1647, Test L1 Norm: 0.0459, Train Linf Norm: 6.0619, Test Linf Norm: 0.9074\n",
            "Epoch 3: Train Loss: 0.0797, Test Loss: 0.0922, Train L1 Norm: 0.1460, Test L1 Norm: 0.0879, Train Linf Norm: 5.4075, Test Linf Norm: 2.6432\n",
            "Epoch 4: Train Loss: 0.0694, Test Loss: 0.1741, Train L1 Norm: 0.1198, Test L1 Norm: 0.0911, Train Linf Norm: 4.3339, Test Linf Norm: 1.4453\n",
            "Epoch 5: Train Loss: 0.0607, Test Loss: 0.0697, Train L1 Norm: 0.1626, Test L1 Norm: 0.0469, Train Linf Norm: 6.5750, Test Linf Norm: 0.7760\n",
            "Epoch 6: Train Loss: 0.0514, Test Loss: 0.0214, Train L1 Norm: 0.1384, Test L1 Norm: 0.0272, Train Linf Norm: 5.5340, Test Linf Norm: 0.6844\n",
            "Epoch 7: Train Loss: 0.0540, Test Loss: 0.0388, Train L1 Norm: 0.1126, Test L1 Norm: 0.0745, Train Linf Norm: 4.2910, Test Linf Norm: 2.3993\n",
            "Epoch 8: Train Loss: 0.0496, Test Loss: 0.0193, Train L1 Norm: 0.1264, Test L1 Norm: 0.0249, Train Linf Norm: 5.0173, Test Linf Norm: 0.6592\n",
            "Epoch 9: Train Loss: 0.0392, Test Loss: 0.0258, Train L1 Norm: 0.0917, Test L1 Norm: 0.0352, Train Linf Norm: 3.5081, Test Linf Norm: 0.9694\n",
            "Epoch 10: Train Loss: 0.0446, Test Loss: 0.0308, Train L1 Norm: 0.1200, Test L1 Norm: 0.0377, Train Linf Norm: 4.7544, Test Linf Norm: 1.0422\n",
            "Epoch 11: Train Loss: 0.0425, Test Loss: 0.0233, Train L1 Norm: 0.0661, Test L1 Norm: 0.0313, Train Linf Norm: 2.2358, Test Linf Norm: 0.8605\n",
            "Epoch 12: Train Loss: 0.0411, Test Loss: 0.0435, Train L1 Norm: 0.0982, Test L1 Norm: 0.0359, Train Linf Norm: 3.8209, Test Linf Norm: 0.7422\n",
            "Epoch 13: Train Loss: 0.0383, Test Loss: 0.0231, Train L1 Norm: 0.0915, Test L1 Norm: 0.0384, Train Linf Norm: 3.5582, Test Linf Norm: 1.1153\n",
            "Epoch 14: Train Loss: 0.0345, Test Loss: 0.0175, Train L1 Norm: 0.1065, Test L1 Norm: 0.0248, Train Linf Norm: 4.3568, Test Linf Norm: 0.6473\n",
            "Epoch 15: Train Loss: 0.0352, Test Loss: 0.0242, Train L1 Norm: 0.0935, Test L1 Norm: 0.0247, Train Linf Norm: 3.7235, Test Linf Norm: 0.5133\n",
            "Epoch 16: Train Loss: 0.0359, Test Loss: 0.0589, Train L1 Norm: 0.0830, Test L1 Norm: 0.0405, Train Linf Norm: 3.2082, Test Linf Norm: 0.8910\n",
            "Epoch 17: Train Loss: 0.0360, Test Loss: 0.0264, Train L1 Norm: 0.0687, Test L1 Norm: 0.0273, Train Linf Norm: 2.5011, Test Linf Norm: 0.6965\n",
            "Epoch 18: Train Loss: 0.0344, Test Loss: 0.0321, Train L1 Norm: 0.0998, Test L1 Norm: 0.0225, Train Linf Norm: 4.0316, Test Linf Norm: 0.4766\n",
            "Epoch 19: Train Loss: 0.0312, Test Loss: 0.0204, Train L1 Norm: 0.0754, Test L1 Norm: 0.0199, Train Linf Norm: 2.9129, Test Linf Norm: 0.4679\n",
            "Epoch 20: Train Loss: 0.0314, Test Loss: 0.0429, Train L1 Norm: 0.1169, Test L1 Norm: 0.0324, Train Linf Norm: 4.9130, Test Linf Norm: 0.6733\n",
            "Epoch 21: Train Loss: 0.0291, Test Loss: 0.0456, Train L1 Norm: 0.0819, Test L1 Norm: 0.0285, Train Linf Norm: 3.2848, Test Linf Norm: 0.6004\n",
            "Epoch 22: Train Loss: 0.0312, Test Loss: 0.0217, Train L1 Norm: 0.0900, Test L1 Norm: 0.0212, Train Linf Norm: 3.6303, Test Linf Norm: 0.4298\n",
            "Epoch 23: Train Loss: 0.0296, Test Loss: 0.0301, Train L1 Norm: 0.0845, Test L1 Norm: 0.0254, Train Linf Norm: 3.4034, Test Linf Norm: 0.4657\n",
            "Epoch 24: Train Loss: 0.0133, Test Loss: 0.0135, Train L1 Norm: 0.0570, Test L1 Norm: 0.0146, Train Linf Norm: 2.3750, Test Linf Norm: 0.3665\n",
            "Epoch 25: Train Loss: 0.0129, Test Loss: 0.0104, Train L1 Norm: 0.0512, Test L1 Norm: 0.0136, Train Linf Norm: 2.1118, Test Linf Norm: 0.3442\n",
            "Epoch 26: Train Loss: 0.0130, Test Loss: 0.0172, Train L1 Norm: 0.0440, Test L1 Norm: 0.0156, Train Linf Norm: 1.7685, Test Linf Norm: 0.3981\n",
            "Epoch 27: Train Loss: 0.0135, Test Loss: 0.0095, Train L1 Norm: 0.0479, Test L1 Norm: 0.0150, Train Linf Norm: 1.9373, Test Linf Norm: 0.4312\n",
            "Epoch 28: Train Loss: 0.0124, Test Loss: 0.0114, Train L1 Norm: 0.0355, Test L1 Norm: 0.0129, Train Linf Norm: 1.3718, Test Linf Norm: 0.3025\n",
            "Epoch 29: Train Loss: 0.0122, Test Loss: 0.0231, Train L1 Norm: 0.0417, Test L1 Norm: 0.0156, Train Linf Norm: 1.6706, Test Linf Norm: 0.3206\n",
            "Epoch 30: Train Loss: 0.0129, Test Loss: 0.0137, Train L1 Norm: 0.0368, Test L1 Norm: 0.0174, Train Linf Norm: 1.4097, Test Linf Norm: 0.5035\n",
            "Epoch 31: Train Loss: 0.0122, Test Loss: 0.0086, Train L1 Norm: 0.0471, Test L1 Norm: 0.0153, Train Linf Norm: 1.9282, Test Linf Norm: 0.4621\n",
            "Epoch 32: Train Loss: 0.0126, Test Loss: 0.0243, Train L1 Norm: 0.0500, Test L1 Norm: 0.0174, Train Linf Norm: 2.0698, Test Linf Norm: 0.3765\n",
            "Epoch 33: Train Loss: 0.0123, Test Loss: 0.0083, Train L1 Norm: 0.0421, Test L1 Norm: 0.0121, Train Linf Norm: 1.6855, Test Linf Norm: 0.3297\n",
            "Epoch 34: Train Loss: 0.0124, Test Loss: 0.0137, Train L1 Norm: 0.0282, Test L1 Norm: 0.0135, Train Linf Norm: 1.0244, Test Linf Norm: 0.3397\n",
            "Epoch 35: Train Loss: 0.0125, Test Loss: 0.0144, Train L1 Norm: 0.0354, Test L1 Norm: 0.0166, Train Linf Norm: 1.3705, Test Linf Norm: 0.4723\n",
            "Epoch 36: Train Loss: 0.0129, Test Loss: 0.0084, Train L1 Norm: 0.0363, Test L1 Norm: 0.0162, Train Linf Norm: 1.4117, Test Linf Norm: 0.5016\n",
            "Epoch 37: Train Loss: 0.0125, Test Loss: 0.0138, Train L1 Norm: 0.0371, Test L1 Norm: 0.0164, Train Linf Norm: 1.4655, Test Linf Norm: 0.4451\n",
            "Epoch 38: Train Loss: 0.0125, Test Loss: 0.0087, Train L1 Norm: 0.0528, Test L1 Norm: 0.0168, Train Linf Norm: 2.2207, Test Linf Norm: 0.5341\n",
            "Epoch 39: Train Loss: 0.0119, Test Loss: 0.0271, Train L1 Norm: 0.0270, Test L1 Norm: 0.0171, Train Linf Norm: 0.9837, Test Linf Norm: 0.3550\n",
            "Epoch 40: Train Loss: 0.0118, Test Loss: 0.0177, Train L1 Norm: 0.0315, Test L1 Norm: 0.0152, Train Linf Norm: 1.1947, Test Linf Norm: 0.3038\n",
            "Epoch 41: Train Loss: 0.0117, Test Loss: 0.0190, Train L1 Norm: 0.0420, Test L1 Norm: 0.0156, Train Linf Norm: 1.5802, Test Linf Norm: 0.3972\n",
            "Epoch 42: Train Loss: 0.0122, Test Loss: 0.0181, Train L1 Norm: 0.0365, Test L1 Norm: 0.0154, Train Linf Norm: 1.4352, Test Linf Norm: 0.3265\n",
            "Epoch 43: Train Loss: 0.0077, Test Loss: 0.0070, Train L1 Norm: 0.0262, Test L1 Norm: 0.0110, Train Linf Norm: 1.0226, Test Linf Norm: 0.3128\n",
            "Epoch 44: Train Loss: 0.0078, Test Loss: 0.0071, Train L1 Norm: 0.0321, Test L1 Norm: 0.0130, Train Linf Norm: 1.3124, Test Linf Norm: 0.4005\n",
            "Epoch 45: Train Loss: 0.0076, Test Loss: 0.0105, Train L1 Norm: 0.0309, Test L1 Norm: 0.0125, Train Linf Norm: 1.2599, Test Linf Norm: 0.3495\n",
            "Epoch 46: Train Loss: 0.0079, Test Loss: 0.0085, Train L1 Norm: 0.0329, Test L1 Norm: 0.0141, Train Linf Norm: 1.3542, Test Linf Norm: 0.4378\n",
            "Epoch 47: Train Loss: 0.0076, Test Loss: 0.0093, Train L1 Norm: 0.0279, Test L1 Norm: 0.0116, Train Linf Norm: 1.1053, Test Linf Norm: 0.3074\n",
            "Epoch 48: Train Loss: 0.0078, Test Loss: 0.0136, Train L1 Norm: 0.0284, Test L1 Norm: 0.0117, Train Linf Norm: 1.1394, Test Linf Norm: 0.2658\n",
            "Epoch 49: Train Loss: 0.0075, Test Loss: 0.0069, Train L1 Norm: 0.0273, Test L1 Norm: 0.0113, Train Linf Norm: 1.0794, Test Linf Norm: 0.3360\n",
            "Epoch 50: Train Loss: 0.0076, Test Loss: 0.0070, Train L1 Norm: 0.0307, Test L1 Norm: 0.0112, Train Linf Norm: 1.2546, Test Linf Norm: 0.3106\n",
            "Epoch 51: Train Loss: 0.0075, Test Loss: 0.0079, Train L1 Norm: 0.0255, Test L1 Norm: 0.0120, Train Linf Norm: 1.0069, Test Linf Norm: 0.3543\n",
            "Epoch 52: Train Loss: 0.0075, Test Loss: 0.0072, Train L1 Norm: 0.0260, Test L1 Norm: 0.0121, Train Linf Norm: 1.0291, Test Linf Norm: 0.3423\n",
            "Epoch 53: Train Loss: 0.0074, Test Loss: 0.0068, Train L1 Norm: 0.0259, Test L1 Norm: 0.0108, Train Linf Norm: 1.0244, Test Linf Norm: 0.3029\n",
            "Epoch 54: Train Loss: 0.0074, Test Loss: 0.0082, Train L1 Norm: 0.0265, Test L1 Norm: 0.0109, Train Linf Norm: 1.0548, Test Linf Norm: 0.3018\n",
            "Epoch 55: Train Loss: 0.0076, Test Loss: 0.0075, Train L1 Norm: 0.0271, Test L1 Norm: 0.0120, Train Linf Norm: 1.0816, Test Linf Norm: 0.3462\n",
            "Epoch 56: Train Loss: 0.0074, Test Loss: 0.0081, Train L1 Norm: 0.0262, Test L1 Norm: 0.0135, Train Linf Norm: 1.0397, Test Linf Norm: 0.4224\n",
            "Epoch 57: Train Loss: 0.0076, Test Loss: 0.0066, Train L1 Norm: 0.0298, Test L1 Norm: 0.0104, Train Linf Norm: 1.2131, Test Linf Norm: 0.2857\n",
            "Epoch 58: Train Loss: 0.0074, Test Loss: 0.0088, Train L1 Norm: 0.0201, Test L1 Norm: 0.0104, Train Linf Norm: 0.7482, Test Linf Norm: 0.2727\n",
            "Epoch 59: Train Loss: 0.0072, Test Loss: 0.0073, Train L1 Norm: 0.0256, Test L1 Norm: 0.0101, Train Linf Norm: 1.0192, Test Linf Norm: 0.2703\n",
            "Epoch 60: Train Loss: 0.0074, Test Loss: 0.0107, Train L1 Norm: 0.0246, Test L1 Norm: 0.0144, Train Linf Norm: 0.9686, Test Linf Norm: 0.4383\n",
            "Epoch 61: Train Loss: 0.0075, Test Loss: 0.0077, Train L1 Norm: 0.0294, Test L1 Norm: 0.0121, Train Linf Norm: 1.1962, Test Linf Norm: 0.3639\n",
            "Epoch 62: Train Loss: 0.0072, Test Loss: 0.0064, Train L1 Norm: 0.0255, Test L1 Norm: 0.0097, Train Linf Norm: 1.0152, Test Linf Norm: 0.2564\n",
            "Epoch 63: Train Loss: 0.0071, Test Loss: 0.0085, Train L1 Norm: 0.0239, Test L1 Norm: 0.0105, Train Linf Norm: 0.9364, Test Linf Norm: 0.2788\n",
            "Epoch 64: Train Loss: 0.0071, Test Loss: 0.0071, Train L1 Norm: 0.0233, Test L1 Norm: 0.0102, Train Linf Norm: 0.9111, Test Linf Norm: 0.2841\n",
            "Epoch 65: Train Loss: 0.0072, Test Loss: 0.0106, Train L1 Norm: 0.0210, Test L1 Norm: 0.0111, Train Linf Norm: 0.7911, Test Linf Norm: 0.2931\n",
            "Epoch 66: Train Loss: 0.0073, Test Loss: 0.0067, Train L1 Norm: 0.0269, Test L1 Norm: 0.0102, Train Linf Norm: 1.0846, Test Linf Norm: 0.2946\n",
            "Epoch 67: Train Loss: 0.0072, Test Loss: 0.0070, Train L1 Norm: 0.0240, Test L1 Norm: 0.0111, Train Linf Norm: 0.9408, Test Linf Norm: 0.3138\n",
            "Epoch 68: Train Loss: 0.0070, Test Loss: 0.0075, Train L1 Norm: 0.0272, Test L1 Norm: 0.0131, Train Linf Norm: 1.1010, Test Linf Norm: 0.4039\n",
            "Epoch 69: Train Loss: 0.0070, Test Loss: 0.0090, Train L1 Norm: 0.0271, Test L1 Norm: 0.0099, Train Linf Norm: 1.0963, Test Linf Norm: 0.2628\n",
            "Epoch 70: Train Loss: 0.0071, Test Loss: 0.0065, Train L1 Norm: 0.0215, Test L1 Norm: 0.0121, Train Linf Norm: 0.8259, Test Linf Norm: 0.3664\n",
            "Epoch 71: Train Loss: 0.0071, Test Loss: 0.0066, Train L1 Norm: 0.0202, Test L1 Norm: 0.0122, Train Linf Norm: 0.7658, Test Linf Norm: 0.3801\n",
            "Epoch 72: Train Loss: 0.0058, Test Loss: 0.0057, Train L1 Norm: 0.0221, Test L1 Norm: 0.0090, Train Linf Norm: 0.8780, Test Linf Norm: 0.2493\n",
            "Epoch 73: Train Loss: 0.0059, Test Loss: 0.0062, Train L1 Norm: 0.0174, Test L1 Norm: 0.0091, Train Linf Norm: 0.6534, Test Linf Norm: 0.2514\n",
            "Epoch 74: Train Loss: 0.0058, Test Loss: 0.0057, Train L1 Norm: 0.0176, Test L1 Norm: 0.0109, Train Linf Norm: 0.6675, Test Linf Norm: 0.3240\n",
            "Epoch 75: Train Loss: 0.0059, Test Loss: 0.0058, Train L1 Norm: 0.0212, Test L1 Norm: 0.0088, Train Linf Norm: 0.8346, Test Linf Norm: 0.2444\n",
            "Epoch 76: Train Loss: 0.0058, Test Loss: 0.0064, Train L1 Norm: 0.0187, Test L1 Norm: 0.0095, Train Linf Norm: 0.7182, Test Linf Norm: 0.2690\n",
            "Epoch 77: Train Loss: 0.0058, Test Loss: 0.0063, Train L1 Norm: 0.0187, Test L1 Norm: 0.0094, Train Linf Norm: 0.7159, Test Linf Norm: 0.2668\n",
            "Epoch 78: Train Loss: 0.0058, Test Loss: 0.0060, Train L1 Norm: 0.0182, Test L1 Norm: 0.0093, Train Linf Norm: 0.6968, Test Linf Norm: 0.2661\n",
            "Epoch 79: Train Loss: 0.0057, Test Loss: 0.0057, Train L1 Norm: 0.0161, Test L1 Norm: 0.0088, Train Linf Norm: 0.5948, Test Linf Norm: 0.2424\n",
            "Epoch 80: Train Loss: 0.0058, Test Loss: 0.0057, Train L1 Norm: 0.0199, Test L1 Norm: 0.0089, Train Linf Norm: 0.7638, Test Linf Norm: 0.2478\n",
            "Epoch 81: Train Loss: 0.0058, Test Loss: 0.0061, Train L1 Norm: 0.0202, Test L1 Norm: 0.0099, Train Linf Norm: 0.7917, Test Linf Norm: 0.2917\n",
            "Epoch 82: Train Loss: 0.0053, Test Loss: 0.0055, Train L1 Norm: 0.0177, Test L1 Norm: 0.0094, Train Linf Norm: 0.6787, Test Linf Norm: 0.2735\n",
            "Epoch 83: Train Loss: 0.0053, Test Loss: 0.0056, Train L1 Norm: 0.0183, Test L1 Norm: 0.0094, Train Linf Norm: 0.7061, Test Linf Norm: 0.2757\n",
            "Epoch 84: Train Loss: 0.0053, Test Loss: 0.0056, Train L1 Norm: 0.0180, Test L1 Norm: 0.0102, Train Linf Norm: 0.6952, Test Linf Norm: 0.3096\n",
            "Epoch 85: Train Loss: 0.0053, Test Loss: 0.0061, Train L1 Norm: 0.0170, Test L1 Norm: 0.0097, Train Linf Norm: 0.6456, Test Linf Norm: 0.2786\n",
            "Epoch 86: Train Loss: 0.0053, Test Loss: 0.0055, Train L1 Norm: 0.0172, Test L1 Norm: 0.0098, Train Linf Norm: 0.6549, Test Linf Norm: 0.2897\n",
            "Epoch 87: Train Loss: 0.0053, Test Loss: 0.0055, Train L1 Norm: 0.0171, Test L1 Norm: 0.0097, Train Linf Norm: 0.6489, Test Linf Norm: 0.2872\n",
            "Epoch 88: Train Loss: 0.0052, Test Loss: 0.0055, Train L1 Norm: 0.0167, Test L1 Norm: 0.0091, Train Linf Norm: 0.6346, Test Linf Norm: 0.2607\n",
            "Epoch 89: Train Loss: 0.0052, Test Loss: 0.0056, Train L1 Norm: 0.0156, Test L1 Norm: 0.0086, Train Linf Norm: 0.5766, Test Linf Norm: 0.2369\n",
            "Epoch 90: Train Loss: 0.0053, Test Loss: 0.0057, Train L1 Norm: 0.0151, Test L1 Norm: 0.0088, Train Linf Norm: 0.5551, Test Linf Norm: 0.2467\n",
            "Epoch 91: Train Loss: 0.0052, Test Loss: 0.0055, Train L1 Norm: 0.0164, Test L1 Norm: 0.0089, Train Linf Norm: 0.5745, Test Linf Norm: 0.2550\n",
            "Epoch 92: Train Loss: 0.0052, Test Loss: 0.0055, Train L1 Norm: 0.0159, Test L1 Norm: 0.0091, Train Linf Norm: 0.5939, Test Linf Norm: 0.2557\n",
            "Epoch 93: Train Loss: 0.0052, Test Loss: 0.0058, Train L1 Norm: 0.0159, Test L1 Norm: 0.0095, Train Linf Norm: 0.5979, Test Linf Norm: 0.2750\n",
            "Epoch 94: Train Loss: 0.0052, Test Loss: 0.0057, Train L1 Norm: 0.0180, Test L1 Norm: 0.0087, Train Linf Norm: 0.6943, Test Linf Norm: 0.2431\n",
            "Epoch 95: Train Loss: 0.0052, Test Loss: 0.0058, Train L1 Norm: 0.0158, Test L1 Norm: 0.0093, Train Linf Norm: 0.5857, Test Linf Norm: 0.2663\n",
            "Epoch 96: Train Loss: 0.0052, Test Loss: 0.0057, Train L1 Norm: 0.0161, Test L1 Norm: 0.0092, Train Linf Norm: 0.6009, Test Linf Norm: 0.2621\n",
            "Epoch 97: Train Loss: 0.0052, Test Loss: 0.0058, Train L1 Norm: 0.0161, Test L1 Norm: 0.0105, Train Linf Norm: 0.6084, Test Linf Norm: 0.3185\n",
            "Epoch 98: Train Loss: 0.0052, Test Loss: 0.0055, Train L1 Norm: 0.0159, Test L1 Norm: 0.0094, Train Linf Norm: 0.5927, Test Linf Norm: 0.2756\n",
            "Epoch 99: Train Loss: 0.0052, Test Loss: 0.0054, Train L1 Norm: 0.0163, Test L1 Norm: 0.0091, Train Linf Norm: 0.6140, Test Linf Norm: 0.2631\n",
            "Epoch 100: Train Loss: 0.0052, Test Loss: 0.0059, Train L1 Norm: 0.0160, Test L1 Norm: 0.0092, Train Linf Norm: 0.5968, Test Linf Norm: 0.2604\n",
            "Epoch 101: Train Loss: 0.0052, Test Loss: 0.0055, Train L1 Norm: 0.0168, Test L1 Norm: 0.0094, Train Linf Norm: 0.6398, Test Linf Norm: 0.2728\n",
            "Epoch 102: Train Loss: 0.0052, Test Loss: 0.0057, Train L1 Norm: 0.0181, Test L1 Norm: 0.0091, Train Linf Norm: 0.6998, Test Linf Norm: 0.2611\n",
            "Epoch 103: Train Loss: 0.0051, Test Loss: 0.0054, Train L1 Norm: 0.0177, Test L1 Norm: 0.0089, Train Linf Norm: 0.6835, Test Linf Norm: 0.2550\n",
            "Epoch 104: Train Loss: 0.0052, Test Loss: 0.0057, Train L1 Norm: 0.0161, Test L1 Norm: 0.0090, Train Linf Norm: 0.6042, Test Linf Norm: 0.2565\n",
            "Epoch 105: Train Loss: 0.0052, Test Loss: 0.0057, Train L1 Norm: 0.0150, Test L1 Norm: 0.0093, Train Linf Norm: 0.5549, Test Linf Norm: 0.2683\n",
            "Epoch 106: Train Loss: 0.0052, Test Loss: 0.0055, Train L1 Norm: 0.0168, Test L1 Norm: 0.0094, Train Linf Norm: 0.6406, Test Linf Norm: 0.2708\n",
            "Epoch 107: Train Loss: 0.0052, Test Loss: 0.0056, Train L1 Norm: 0.0156, Test L1 Norm: 0.0087, Train Linf Norm: 0.5803, Test Linf Norm: 0.2444\n",
            "Epoch 108: Train Loss: 0.0052, Test Loss: 0.0058, Train L1 Norm: 0.0160, Test L1 Norm: 0.0087, Train Linf Norm: 0.5995, Test Linf Norm: 0.2399\n",
            "Epoch 109: Train Loss: 0.0050, Test Loss: 0.0056, Train L1 Norm: 0.0155, Test L1 Norm: 0.0095, Train Linf Norm: 0.5814, Test Linf Norm: 0.2809\n",
            "Epoch 110: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0155, Test L1 Norm: 0.0092, Train Linf Norm: 0.5818, Test Linf Norm: 0.2702\n",
            "Epoch 111: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0162, Test L1 Norm: 0.0090, Train Linf Norm: 0.6173, Test Linf Norm: 0.2584\n",
            "Epoch 112: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0157, Test L1 Norm: 0.0092, Train Linf Norm: 0.5819, Test Linf Norm: 0.2704\n",
            "Epoch 113: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0158, Test L1 Norm: 0.0089, Train Linf Norm: 0.5953, Test Linf Norm: 0.2552\n",
            "Epoch 114: Train Loss: 0.0050, Test Loss: 0.0053, Train L1 Norm: 0.0153, Test L1 Norm: 0.0092, Train Linf Norm: 0.5700, Test Linf Norm: 0.2665\n",
            "Epoch 115: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0151, Test L1 Norm: 0.0090, Train Linf Norm: 0.5470, Test Linf Norm: 0.2602\n",
            "Epoch 116: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0164, Test L1 Norm: 0.0089, Train Linf Norm: 0.6121, Test Linf Norm: 0.2559\n",
            "Epoch 117: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0152, Test L1 Norm: 0.0092, Train Linf Norm: 0.5694, Test Linf Norm: 0.2688\n",
            "Epoch 118: Train Loss: 0.0050, Test Loss: 0.0055, Train L1 Norm: 0.0155, Test L1 Norm: 0.0089, Train Linf Norm: 0.5780, Test Linf Norm: 0.2549\n",
            "Epoch 119: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0160, Test L1 Norm: 0.0086, Train Linf Norm: 0.6029, Test Linf Norm: 0.2440\n",
            "Epoch 120: Train Loss: 0.0050, Test Loss: 0.0055, Train L1 Norm: 0.0159, Test L1 Norm: 0.0090, Train Linf Norm: 0.5993, Test Linf Norm: 0.2585\n",
            "Epoch 121: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0160, Test L1 Norm: 0.0089, Train Linf Norm: 0.6059, Test Linf Norm: 0.2550\n",
            "Epoch 122: Train Loss: 0.0049, Test Loss: 0.0054, Train L1 Norm: 0.0150, Test L1 Norm: 0.0090, Train Linf Norm: 0.5568, Test Linf Norm: 0.2616\n",
            "Epoch 123: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0161, Test L1 Norm: 0.0089, Train Linf Norm: 0.6098, Test Linf Norm: 0.2555\n",
            "Epoch 124: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0153, Test L1 Norm: 0.0089, Train Linf Norm: 0.5709, Test Linf Norm: 0.2573\n",
            "Epoch 125: Train Loss: 0.0049, Test Loss: 0.0054, Train L1 Norm: 0.0156, Test L1 Norm: 0.0088, Train Linf Norm: 0.5845, Test Linf Norm: 0.2497\n",
            "Epoch 126: Train Loss: 0.0049, Test Loss: 0.0054, Train L1 Norm: 0.0152, Test L1 Norm: 0.0092, Train Linf Norm: 0.5680, Test Linf Norm: 0.2654\n",
            "Epoch 127: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0154, Test L1 Norm: 0.0090, Train Linf Norm: 0.5806, Test Linf Norm: 0.2569\n",
            "Epoch 128: Train Loss: 0.0049, Test Loss: 0.0054, Train L1 Norm: 0.0153, Test L1 Norm: 0.0088, Train Linf Norm: 0.5695, Test Linf Norm: 0.2511\n",
            "Epoch 129: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0159, Test L1 Norm: 0.0087, Train Linf Norm: 0.6014, Test Linf Norm: 0.2470\n",
            "Epoch 130: Train Loss: 0.0049, Test Loss: 0.0054, Train L1 Norm: 0.0152, Test L1 Norm: 0.0092, Train Linf Norm: 0.5632, Test Linf Norm: 0.2674\n",
            "Epoch 131: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0153, Test L1 Norm: 0.0090, Train Linf Norm: 0.5724, Test Linf Norm: 0.2604\n",
            "Epoch 132: Train Loss: 0.0049, Test Loss: 0.0054, Train L1 Norm: 0.0154, Test L1 Norm: 0.0088, Train Linf Norm: 0.5794, Test Linf Norm: 0.2499\n",
            "Epoch 133: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0152, Test L1 Norm: 0.0091, Train Linf Norm: 0.5644, Test Linf Norm: 0.2636\n",
            "Epoch 134: Train Loss: 0.0049, Test Loss: 0.0054, Train L1 Norm: 0.0150, Test L1 Norm: 0.0089, Train Linf Norm: 0.5547, Test Linf Norm: 0.2569\n",
            "Epoch 135: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0151, Test L1 Norm: 0.0091, Train Linf Norm: 0.5639, Test Linf Norm: 0.2631\n",
            "Epoch 136: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0149, Test L1 Norm: 0.0091, Train Linf Norm: 0.5557, Test Linf Norm: 0.2629\n",
            "Epoch 137: Train Loss: 0.0049, Test Loss: 0.0054, Train L1 Norm: 0.0151, Test L1 Norm: 0.0090, Train Linf Norm: 0.5622, Test Linf Norm: 0.2573\n",
            "Epoch 138: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0150, Test L1 Norm: 0.0089, Train Linf Norm: 0.5586, Test Linf Norm: 0.2556\n",
            "Epoch 139: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0150, Test L1 Norm: 0.0090, Train Linf Norm: 0.5600, Test Linf Norm: 0.2597\n",
            "Epoch 140: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0153, Test L1 Norm: 0.0089, Train Linf Norm: 0.5731, Test Linf Norm: 0.2560\n",
            "Epoch 141: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0154, Test L1 Norm: 0.0088, Train Linf Norm: 0.5760, Test Linf Norm: 0.2522\n",
            "Epoch 142: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0155, Test L1 Norm: 0.0090, Train Linf Norm: 0.5830, Test Linf Norm: 0.2591\n",
            "Epoch 143: Train Loss: 0.0048, Test Loss: 0.0053, Train L1 Norm: 0.0151, Test L1 Norm: 0.0089, Train Linf Norm: 0.5624, Test Linf Norm: 0.2572\n",
            "Epoch 144: Train Loss: 0.0048, Test Loss: 0.0053, Train L1 Norm: 0.0150, Test L1 Norm: 0.0090, Train Linf Norm: 0.5617, Test Linf Norm: 0.2576\n",
            "Epoch 145: Train Loss: 0.0048, Test Loss: 0.0054, Train L1 Norm: 0.0151, Test L1 Norm: 0.0090, Train Linf Norm: 0.5629, Test Linf Norm: 0.2584\n",
            "Epoch 146: Train Loss: 0.0048, Test Loss: 0.0053, Train L1 Norm: 0.0152, Test L1 Norm: 0.0090, Train Linf Norm: 0.5694, Test Linf Norm: 0.2582\n",
            "Epoch 147: Train Loss: 0.0048, Test Loss: 0.0053, Train L1 Norm: 0.0151, Test L1 Norm: 0.0089, Train Linf Norm: 0.5666, Test Linf Norm: 0.2571\n",
            "Epoch 148: Train Loss: 0.0048, Test Loss: 0.0053, Train L1 Norm: 0.0150, Test L1 Norm: 0.0089, Train Linf Norm: 0.5525, Test Linf Norm: 0.2566\n",
            "Epoch 149: Train Loss: 0.0048, Test Loss: 0.0053, Train L1 Norm: 0.0151, Test L1 Norm: 0.0089, Train Linf Norm: 0.5601, Test Linf Norm: 0.2558\n",
            "Epoch 150: Train Loss: 0.0048, Test Loss: 0.0053, Train L1 Norm: 0.0153, Test L1 Norm: 0.0089, Train Linf Norm: 0.5773, Test Linf Norm: 0.2563\n",
            "Epoch 151: Train Loss: 0.0048, Test Loss: 0.0053, Train L1 Norm: 0.0151, Test L1 Norm: 0.0089, Train Linf Norm: 0.5635, Test Linf Norm: 0.2572\n",
            "Epoch 152: Train Loss: 0.0048, Test Loss: 0.0053, Train L1 Norm: 0.0151, Test L1 Norm: 0.0089, Train Linf Norm: 0.5652, Test Linf Norm: 0.2571\n",
            "Epoch 153: Train Loss: 0.0048, Test Loss: 0.0053, Train L1 Norm: 0.0150, Test L1 Norm: 0.0090, Train Linf Norm: 0.5594, Test Linf Norm: 0.2580\n",
            "Epoch 154: Train Loss: 0.0048, Test Loss: 0.0053, Train L1 Norm: 0.0151, Test L1 Norm: 0.0090, Train Linf Norm: 0.5626, Test Linf Norm: 0.2576\n",
            "Epoch 155: Train Loss: 0.0048, Test Loss: 0.0053, Train L1 Norm: 0.0151, Test L1 Norm: 0.0090, Train Linf Norm: 0.5551, Test Linf Norm: 0.2586\n",
            "Epoch 156: Train Loss: 0.0048, Test Loss: 0.0053, Train L1 Norm: 0.0151, Test L1 Norm: 0.0090, Train Linf Norm: 0.5612, Test Linf Norm: 0.2580\n",
            "Epoch 157: Train Loss: 0.0048, Test Loss: 0.0053, Train L1 Norm: 0.0151, Test L1 Norm: 0.0090, Train Linf Norm: 0.5662, Test Linf Norm: 0.2586\n",
            "Epoch 158: Train Loss: 0.0048, Test Loss: 0.0053, Train L1 Norm: 0.0151, Test L1 Norm: 0.0089, Train Linf Norm: 0.5665, Test Linf Norm: 0.2575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 19:17:10,911]\u001b[0m Trial 20 finished with value: 0.008911744522675872 and parameters: {'n_layers': 5, 'n_units_0': 919, 'n_units_1': 1262, 'n_units_2': 1274, 'n_units_3': 1272, 'n_units_4': 1511, 'hidden_activation': 'ReLU', 'output_activation': 'Linear', 'loss': 'MAE', 'optimizer': 'Adam', 'lr': 0.0002794998027301611, 'batch_size': 48, 'n_epochs': 159, 'scheduler': 'ReduceLROnPlateau', 'weight_decay': 2.9660495540890495e-05, 'beta1': 0.9082115920514306, 'beta2': 0.9990008538255758, 'factor': 0.3485766822744549, 'patience': 8, 'threshold': 0.0006445460131446244}. Best is trial 20 with value: 0.008911744522675872.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 159: Train Loss: 0.0048, Test Loss: 0.0053, Train L1 Norm: 0.0151, Test L1 Norm: 0.0089, Train Linf Norm: 0.5647, Test Linf Norm: 0.2560\n",
            "Epoch 1: Train Loss: 0.1686, Test Loss: 0.2453, Train L1 Norm: 0.2407, Test L1 Norm: 0.1148, Train Linf Norm: 8.1496, Test Linf Norm: 1.7927\n",
            "Epoch 2: Train Loss: 0.0992, Test Loss: 0.0695, Train L1 Norm: 0.2367, Test L1 Norm: 0.0884, Train Linf Norm: 9.4091, Test Linf Norm: 2.6329\n",
            "Epoch 3: Train Loss: 0.0807, Test Loss: 0.0438, Train L1 Norm: 0.1631, Test L1 Norm: 0.0422, Train Linf Norm: 6.2306, Test Linf Norm: 0.9838\n",
            "Epoch 4: Train Loss: 0.0728, Test Loss: 0.0951, Train L1 Norm: 0.1354, Test L1 Norm: 0.0616, Train Linf Norm: 5.0263, Test Linf Norm: 1.3795\n",
            "Epoch 5: Train Loss: 0.0627, Test Loss: 0.0507, Train L1 Norm: 0.1275, Test L1 Norm: 0.0737, Train Linf Norm: 4.8217, Test Linf Norm: 1.7108\n",
            "Epoch 6: Train Loss: 0.0584, Test Loss: 0.0628, Train L1 Norm: 0.1721, Test L1 Norm: 0.0652, Train Linf Norm: 7.0549, Test Linf Norm: 1.8899\n",
            "Epoch 7: Train Loss: 0.0488, Test Loss: 0.1121, Train L1 Norm: 0.1018, Test L1 Norm: 0.0479, Train Linf Norm: 3.8547, Test Linf Norm: 0.7488\n",
            "Epoch 8: Train Loss: 0.0466, Test Loss: 0.0215, Train L1 Norm: 0.0815, Test L1 Norm: 0.0433, Train Linf Norm: 2.9473, Test Linf Norm: 1.2778\n",
            "Epoch 9: Train Loss: 0.0482, Test Loss: 0.0832, Train L1 Norm: 0.0959, Test L1 Norm: 0.0368, Train Linf Norm: 3.6127, Test Linf Norm: 0.4892\n",
            "Epoch 10: Train Loss: 0.0435, Test Loss: 0.0175, Train L1 Norm: 0.1027, Test L1 Norm: 0.0333, Train Linf Norm: 4.0045, Test Linf Norm: 0.9978\n",
            "Epoch 11: Train Loss: 0.0420, Test Loss: 0.0280, Train L1 Norm: 0.1066, Test L1 Norm: 0.0312, Train Linf Norm: 4.2345, Test Linf Norm: 0.8217\n",
            "Epoch 12: Train Loss: 0.0391, Test Loss: 0.0209, Train L1 Norm: 0.0949, Test L1 Norm: 0.0214, Train Linf Norm: 3.7370, Test Linf Norm: 0.4958\n",
            "Epoch 13: Train Loss: 0.0407, Test Loss: 0.0231, Train L1 Norm: 0.0849, Test L1 Norm: 0.0297, Train Linf Norm: 3.2095, Test Linf Norm: 0.8157\n",
            "Epoch 14: Train Loss: 0.0370, Test Loss: 0.0440, Train L1 Norm: 0.0811, Test L1 Norm: 0.0325, Train Linf Norm: 3.0778, Test Linf Norm: 0.8025\n",
            "Epoch 15: Train Loss: 0.0357, Test Loss: 0.0493, Train L1 Norm: 0.0876, Test L1 Norm: 0.0280, Train Linf Norm: 3.4150, Test Linf Norm: 0.5343\n",
            "Epoch 16: Train Loss: 0.0369, Test Loss: 0.0517, Train L1 Norm: 0.0776, Test L1 Norm: 0.0354, Train Linf Norm: 2.9477, Test Linf Norm: 0.8281\n",
            "Epoch 17: Train Loss: 0.0339, Test Loss: 0.0241, Train L1 Norm: 0.0756, Test L1 Norm: 0.0211, Train Linf Norm: 2.8971, Test Linf Norm: 0.4912\n",
            "Epoch 18: Train Loss: 0.0344, Test Loss: 0.0151, Train L1 Norm: 0.0685, Test L1 Norm: 0.0234, Train Linf Norm: 2.5503, Test Linf Norm: 0.6292\n",
            "Epoch 19: Train Loss: 0.0315, Test Loss: 0.0268, Train L1 Norm: 0.0931, Test L1 Norm: 0.0328, Train Linf Norm: 3.7867, Test Linf Norm: 0.9775\n",
            "Epoch 20: Train Loss: 0.0322, Test Loss: 0.0150, Train L1 Norm: 0.0648, Test L1 Norm: 0.0370, Train Linf Norm: 2.4366, Test Linf Norm: 1.2278\n",
            "Epoch 21: Train Loss: 0.0310, Test Loss: 0.0198, Train L1 Norm: 0.0798, Test L1 Norm: 0.0261, Train Linf Norm: 3.1459, Test Linf Norm: 0.7794\n",
            "Epoch 22: Train Loss: 0.0281, Test Loss: 0.0596, Train L1 Norm: 0.0774, Test L1 Norm: 0.0389, Train Linf Norm: 3.1025, Test Linf Norm: 0.6928\n",
            "Epoch 23: Train Loss: 0.0287, Test Loss: 0.0178, Train L1 Norm: 0.0849, Test L1 Norm: 0.0344, Train Linf Norm: 3.4310, Test Linf Norm: 1.0050\n",
            "Epoch 24: Train Loss: 0.0301, Test Loss: 0.0150, Train L1 Norm: 0.0660, Test L1 Norm: 0.0215, Train Linf Norm: 2.5017, Test Linf Norm: 0.6238\n",
            "Epoch 25: Train Loss: 0.0282, Test Loss: 0.0844, Train L1 Norm: 0.0900, Test L1 Norm: 0.0368, Train Linf Norm: 3.6849, Test Linf Norm: 0.4839\n",
            "Epoch 26: Train Loss: 0.0268, Test Loss: 0.0412, Train L1 Norm: 0.0691, Test L1 Norm: 0.0365, Train Linf Norm: 2.7250, Test Linf Norm: 0.8544\n",
            "Epoch 27: Train Loss: 0.0272, Test Loss: 0.0423, Train L1 Norm: 0.0633, Test L1 Norm: 0.0270, Train Linf Norm: 2.4283, Test Linf Norm: 0.4931\n",
            "Epoch 28: Train Loss: 0.0254, Test Loss: 0.0152, Train L1 Norm: 0.0811, Test L1 Norm: 0.0243, Train Linf Norm: 3.3083, Test Linf Norm: 0.7006\n",
            "Epoch 29: Train Loss: 0.0274, Test Loss: 0.0755, Train L1 Norm: 0.0818, Test L1 Norm: 0.0359, Train Linf Norm: 3.3254, Test Linf Norm: 0.5241\n",
            "Epoch 30: Train Loss: 0.0124, Test Loss: 0.0096, Train L1 Norm: 0.0624, Test L1 Norm: 0.0121, Train Linf Norm: 2.6648, Test Linf Norm: 0.2822\n",
            "Epoch 31: Train Loss: 0.0122, Test Loss: 0.0097, Train L1 Norm: 0.0459, Test L1 Norm: 0.0135, Train Linf Norm: 1.8831, Test Linf Norm: 0.3212\n",
            "Epoch 32: Train Loss: 0.0124, Test Loss: 0.0202, Train L1 Norm: 0.0493, Test L1 Norm: 0.0145, Train Linf Norm: 2.0530, Test Linf Norm: 0.2818\n",
            "Epoch 33: Train Loss: 0.0120, Test Loss: 0.0207, Train L1 Norm: 0.0494, Test L1 Norm: 0.0149, Train Linf Norm: 2.0533, Test Linf Norm: 0.3104\n",
            "Epoch 34: Train Loss: 0.0121, Test Loss: 0.0153, Train L1 Norm: 0.0514, Test L1 Norm: 0.0169, Train Linf Norm: 2.1625, Test Linf Norm: 0.4459\n",
            "Epoch 35: Train Loss: 0.0121, Test Loss: 0.0196, Train L1 Norm: 0.0403, Test L1 Norm: 0.0155, Train Linf Norm: 1.6170, Test Linf Norm: 0.3258\n",
            "Epoch 36: Train Loss: 0.0113, Test Loss: 0.0085, Train L1 Norm: 0.0454, Test L1 Norm: 0.0142, Train Linf Norm: 1.8905, Test Linf Norm: 0.4279\n",
            "Epoch 37: Train Loss: 0.0117, Test Loss: 0.0099, Train L1 Norm: 0.0429, Test L1 Norm: 0.0115, Train Linf Norm: 1.7499, Test Linf Norm: 0.2750\n",
            "Epoch 38: Train Loss: 0.0116, Test Loss: 0.0084, Train L1 Norm: 0.0412, Test L1 Norm: 0.0110, Train Linf Norm: 1.6834, Test Linf Norm: 0.2842\n",
            "Epoch 39: Train Loss: 0.0123, Test Loss: 0.0112, Train L1 Norm: 0.0364, Test L1 Norm: 0.0130, Train Linf Norm: 1.4459, Test Linf Norm: 0.3427\n",
            "Epoch 40: Train Loss: 0.0118, Test Loss: 0.0080, Train L1 Norm: 0.0383, Test L1 Norm: 0.0106, Train Linf Norm: 1.5414, Test Linf Norm: 0.2487\n",
            "Epoch 41: Train Loss: 0.0120, Test Loss: 0.0135, Train L1 Norm: 0.0364, Test L1 Norm: 0.0166, Train Linf Norm: 1.4482, Test Linf Norm: 0.4666\n",
            "Epoch 42: Train Loss: 0.0112, Test Loss: 0.0184, Train L1 Norm: 0.0418, Test L1 Norm: 0.0141, Train Linf Norm: 1.7215, Test Linf Norm: 0.3223\n",
            "Epoch 43: Train Loss: 0.0121, Test Loss: 0.0128, Train L1 Norm: 0.0394, Test L1 Norm: 0.0139, Train Linf Norm: 1.5911, Test Linf Norm: 0.3765\n",
            "Epoch 44: Train Loss: 0.0114, Test Loss: 0.0145, Train L1 Norm: 0.0374, Test L1 Norm: 0.0131, Train Linf Norm: 1.4964, Test Linf Norm: 0.2762\n",
            "Epoch 45: Train Loss: 0.0115, Test Loss: 0.0074, Train L1 Norm: 0.0415, Test L1 Norm: 0.0102, Train Linf Norm: 1.6991, Test Linf Norm: 0.2494\n",
            "Epoch 46: Train Loss: 0.0109, Test Loss: 0.0124, Train L1 Norm: 0.0366, Test L1 Norm: 0.0127, Train Linf Norm: 1.4751, Test Linf Norm: 0.2630\n",
            "Epoch 47: Train Loss: 0.0111, Test Loss: 0.0085, Train L1 Norm: 0.0452, Test L1 Norm: 0.0117, Train Linf Norm: 1.8788, Test Linf Norm: 0.3212\n",
            "Epoch 48: Train Loss: 0.0114, Test Loss: 0.0076, Train L1 Norm: 0.0499, Test L1 Norm: 0.0134, Train Linf Norm: 2.1043, Test Linf Norm: 0.4025\n",
            "Epoch 49: Train Loss: 0.0112, Test Loss: 0.0185, Train L1 Norm: 0.0383, Test L1 Norm: 0.0155, Train Linf Norm: 1.5528, Test Linf Norm: 0.3031\n",
            "Epoch 50: Train Loss: 0.0112, Test Loss: 0.0159, Train L1 Norm: 0.0370, Test L1 Norm: 0.0149, Train Linf Norm: 1.4993, Test Linf Norm: 0.3453\n",
            "Epoch 51: Train Loss: 0.0109, Test Loss: 0.0121, Train L1 Norm: 0.0547, Test L1 Norm: 0.0110, Train Linf Norm: 2.3479, Test Linf Norm: 0.2538\n",
            "Epoch 52: Train Loss: 0.0116, Test Loss: 0.0164, Train L1 Norm: 0.0367, Test L1 Norm: 0.0128, Train Linf Norm: 1.4790, Test Linf Norm: 0.2646\n",
            "Epoch 53: Train Loss: 0.0107, Test Loss: 0.0098, Train L1 Norm: 0.0391, Test L1 Norm: 0.0126, Train Linf Norm: 1.5983, Test Linf Norm: 0.3611\n",
            "Epoch 54: Train Loss: 0.0108, Test Loss: 0.0120, Train L1 Norm: 0.0292, Test L1 Norm: 0.0114, Train Linf Norm: 1.1307, Test Linf Norm: 0.2548\n",
            "Epoch 55: Train Loss: 0.0074, Test Loss: 0.0064, Train L1 Norm: 0.0326, Test L1 Norm: 0.0100, Train Linf Norm: 1.3559, Test Linf Norm: 0.2749\n",
            "Epoch 56: Train Loss: 0.0072, Test Loss: 0.0079, Train L1 Norm: 0.0262, Test L1 Norm: 0.0089, Train Linf Norm: 1.0539, Test Linf Norm: 0.2109\n",
            "Epoch 57: Train Loss: 0.0076, Test Loss: 0.0069, Train L1 Norm: 0.0279, Test L1 Norm: 0.0085, Train Linf Norm: 1.1295, Test Linf Norm: 0.2023\n",
            "Epoch 58: Train Loss: 0.0074, Test Loss: 0.0066, Train L1 Norm: 0.0300, Test L1 Norm: 0.0101, Train Linf Norm: 1.2363, Test Linf Norm: 0.2665\n",
            "Epoch 59: Train Loss: 0.0074, Test Loss: 0.0070, Train L1 Norm: 0.0210, Test L1 Norm: 0.0088, Train Linf Norm: 0.8055, Test Linf Norm: 0.2110\n",
            "Epoch 60: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0294, Test L1 Norm: 0.0103, Train Linf Norm: 1.2034, Test Linf Norm: 0.2660\n",
            "Epoch 61: Train Loss: 0.0071, Test Loss: 0.0077, Train L1 Norm: 0.0239, Test L1 Norm: 0.0088, Train Linf Norm: 0.9440, Test Linf Norm: 0.2060\n",
            "Epoch 62: Train Loss: 0.0073, Test Loss: 0.0066, Train L1 Norm: 0.0280, Test L1 Norm: 0.0086, Train Linf Norm: 1.1456, Test Linf Norm: 0.2138\n",
            "Epoch 63: Train Loss: 0.0073, Test Loss: 0.0062, Train L1 Norm: 0.0236, Test L1 Norm: 0.0081, Train Linf Norm: 0.9319, Test Linf Norm: 0.1918\n",
            "Epoch 64: Train Loss: 0.0072, Test Loss: 0.0064, Train L1 Norm: 0.0212, Test L1 Norm: 0.0082, Train Linf Norm: 0.8149, Test Linf Norm: 0.1960\n",
            "Epoch 65: Train Loss: 0.0073, Test Loss: 0.0096, Train L1 Norm: 0.0236, Test L1 Norm: 0.0092, Train Linf Norm: 0.9354, Test Linf Norm: 0.2166\n",
            "Epoch 66: Train Loss: 0.0073, Test Loss: 0.0068, Train L1 Norm: 0.0179, Test L1 Norm: 0.0089, Train Linf Norm: 0.6579, Test Linf Norm: 0.2212\n",
            "Epoch 67: Train Loss: 0.0070, Test Loss: 0.0074, Train L1 Norm: 0.0224, Test L1 Norm: 0.0094, Train Linf Norm: 0.8813, Test Linf Norm: 0.2471\n",
            "Epoch 68: Train Loss: 0.0072, Test Loss: 0.0084, Train L1 Norm: 0.0277, Test L1 Norm: 0.0099, Train Linf Norm: 1.1310, Test Linf Norm: 0.2507\n",
            "Epoch 69: Train Loss: 0.0071, Test Loss: 0.0067, Train L1 Norm: 0.0206, Test L1 Norm: 0.0086, Train Linf Norm: 0.7906, Test Linf Norm: 0.2154\n",
            "Epoch 70: Train Loss: 0.0071, Test Loss: 0.0076, Train L1 Norm: 0.0261, Test L1 Norm: 0.0089, Train Linf Norm: 1.0563, Test Linf Norm: 0.2178\n",
            "Epoch 71: Train Loss: 0.0069, Test Loss: 0.0063, Train L1 Norm: 0.0180, Test L1 Norm: 0.0083, Train Linf Norm: 0.6709, Test Linf Norm: 0.2010\n",
            "Epoch 72: Train Loss: 0.0070, Test Loss: 0.0110, Train L1 Norm: 0.0240, Test L1 Norm: 0.0101, Train Linf Norm: 0.9616, Test Linf Norm: 0.2299\n",
            "Epoch 73: Train Loss: 0.0059, Test Loss: 0.0057, Train L1 Norm: 0.0200, Test L1 Norm: 0.0085, Train Linf Norm: 0.7825, Test Linf Norm: 0.2207\n",
            "Epoch 74: Train Loss: 0.0059, Test Loss: 0.0063, Train L1 Norm: 0.0180, Test L1 Norm: 0.0082, Train Linf Norm: 0.6890, Test Linf Norm: 0.2010\n",
            "Epoch 75: Train Loss: 0.0059, Test Loss: 0.0058, Train L1 Norm: 0.0193, Test L1 Norm: 0.0077, Train Linf Norm: 0.7557, Test Linf Norm: 0.1880\n",
            "Epoch 76: Train Loss: 0.0059, Test Loss: 0.0061, Train L1 Norm: 0.0190, Test L1 Norm: 0.0087, Train Linf Norm: 0.7396, Test Linf Norm: 0.2278\n",
            "Epoch 77: Train Loss: 0.0058, Test Loss: 0.0062, Train L1 Norm: 0.0231, Test L1 Norm: 0.0082, Train Linf Norm: 0.9350, Test Linf Norm: 0.2061\n",
            "Epoch 78: Train Loss: 0.0059, Test Loss: 0.0059, Train L1 Norm: 0.0184, Test L1 Norm: 0.0082, Train Linf Norm: 0.7095, Test Linf Norm: 0.2092\n",
            "Epoch 79: Train Loss: 0.0059, Test Loss: 0.0058, Train L1 Norm: 0.0212, Test L1 Norm: 0.0084, Train Linf Norm: 0.8432, Test Linf Norm: 0.2127\n",
            "Epoch 80: Train Loss: 0.0058, Test Loss: 0.0059, Train L1 Norm: 0.0196, Test L1 Norm: 0.0076, Train Linf Norm: 0.7685, Test Linf Norm: 0.1816\n",
            "Epoch 81: Train Loss: 0.0059, Test Loss: 0.0074, Train L1 Norm: 0.0200, Test L1 Norm: 0.0081, Train Linf Norm: 0.7935, Test Linf Norm: 0.1958\n",
            "Epoch 82: Train Loss: 0.0058, Test Loss: 0.0058, Train L1 Norm: 0.0219, Test L1 Norm: 0.0079, Train Linf Norm: 0.8808, Test Linf Norm: 0.1989\n",
            "Epoch 83: Train Loss: 0.0053, Test Loss: 0.0062, Train L1 Norm: 0.0204, Test L1 Norm: 0.0083, Train Linf Norm: 0.8152, Test Linf Norm: 0.2130\n",
            "Epoch 84: Train Loss: 0.0054, Test Loss: 0.0060, Train L1 Norm: 0.0183, Test L1 Norm: 0.0080, Train Linf Norm: 0.7138, Test Linf Norm: 0.1983\n",
            "Epoch 85: Train Loss: 0.0054, Test Loss: 0.0057, Train L1 Norm: 0.0181, Test L1 Norm: 0.0082, Train Linf Norm: 0.7026, Test Linf Norm: 0.2116\n",
            "Epoch 86: Train Loss: 0.0054, Test Loss: 0.0056, Train L1 Norm: 0.0181, Test L1 Norm: 0.0084, Train Linf Norm: 0.7048, Test Linf Norm: 0.2174\n",
            "Epoch 87: Train Loss: 0.0054, Test Loss: 0.0055, Train L1 Norm: 0.0177, Test L1 Norm: 0.0079, Train Linf Norm: 0.6891, Test Linf Norm: 0.1985\n",
            "Epoch 88: Train Loss: 0.0053, Test Loss: 0.0056, Train L1 Norm: 0.0168, Test L1 Norm: 0.0078, Train Linf Norm: 0.6455, Test Linf Norm: 0.1959\n",
            "Epoch 89: Train Loss: 0.0054, Test Loss: 0.0055, Train L1 Norm: 0.0193, Test L1 Norm: 0.0079, Train Linf Norm: 0.7650, Test Linf Norm: 0.2002\n",
            "Epoch 90: Train Loss: 0.0053, Test Loss: 0.0056, Train L1 Norm: 0.0176, Test L1 Norm: 0.0081, Train Linf Norm: 0.6827, Test Linf Norm: 0.2062\n",
            "Epoch 91: Train Loss: 0.0053, Test Loss: 0.0057, Train L1 Norm: 0.0178, Test L1 Norm: 0.0079, Train Linf Norm: 0.6939, Test Linf Norm: 0.1990\n",
            "Epoch 92: Train Loss: 0.0053, Test Loss: 0.0058, Train L1 Norm: 0.0190, Test L1 Norm: 0.0083, Train Linf Norm: 0.7504, Test Linf Norm: 0.2155\n",
            "Epoch 93: Train Loss: 0.0053, Test Loss: 0.0056, Train L1 Norm: 0.0165, Test L1 Norm: 0.0078, Train Linf Norm: 0.6287, Test Linf Norm: 0.1961\n",
            "Epoch 94: Train Loss: 0.0053, Test Loss: 0.0055, Train L1 Norm: 0.0181, Test L1 Norm: 0.0077, Train Linf Norm: 0.7058, Test Linf Norm: 0.1923\n",
            "Epoch 95: Train Loss: 0.0053, Test Loss: 0.0062, Train L1 Norm: 0.0182, Test L1 Norm: 0.0078, Train Linf Norm: 0.7114, Test Linf Norm: 0.1923\n",
            "Epoch 96: Train Loss: 0.0053, Test Loss: 0.0055, Train L1 Norm: 0.0164, Test L1 Norm: 0.0077, Train Linf Norm: 0.6297, Test Linf Norm: 0.1930\n",
            "Epoch 97: Train Loss: 0.0053, Test Loss: 0.0057, Train L1 Norm: 0.0183, Test L1 Norm: 0.0077, Train Linf Norm: 0.7179, Test Linf Norm: 0.1890\n",
            "Epoch 98: Train Loss: 0.0053, Test Loss: 0.0057, Train L1 Norm: 0.0159, Test L1 Norm: 0.0077, Train Linf Norm: 0.6040, Test Linf Norm: 0.1939\n",
            "Epoch 99: Train Loss: 0.0053, Test Loss: 0.0055, Train L1 Norm: 0.0178, Test L1 Norm: 0.0085, Train Linf Norm: 0.6892, Test Linf Norm: 0.2264\n",
            "Epoch 100: Train Loss: 0.0053, Test Loss: 0.0056, Train L1 Norm: 0.0178, Test L1 Norm: 0.0078, Train Linf Norm: 0.6924, Test Linf Norm: 0.1952\n",
            "Epoch 101: Train Loss: 0.0053, Test Loss: 0.0055, Train L1 Norm: 0.0171, Test L1 Norm: 0.0084, Train Linf Norm: 0.6603, Test Linf Norm: 0.2235\n",
            "Epoch 102: Train Loss: 0.0053, Test Loss: 0.0058, Train L1 Norm: 0.0167, Test L1 Norm: 0.0078, Train Linf Norm: 0.6366, Test Linf Norm: 0.1941\n",
            "Epoch 103: Train Loss: 0.0053, Test Loss: 0.0055, Train L1 Norm: 0.0170, Test L1 Norm: 0.0080, Train Linf Norm: 0.6561, Test Linf Norm: 0.2073\n",
            "Epoch 104: Train Loss: 0.0053, Test Loss: 0.0067, Train L1 Norm: 0.0165, Test L1 Norm: 0.0079, Train Linf Norm: 0.6294, Test Linf Norm: 0.1960\n",
            "Epoch 105: Train Loss: 0.0053, Test Loss: 0.0055, Train L1 Norm: 0.0162, Test L1 Norm: 0.0078, Train Linf Norm: 0.6136, Test Linf Norm: 0.1981\n",
            "Epoch 106: Train Loss: 0.0053, Test Loss: 0.0057, Train L1 Norm: 0.0156, Test L1 Norm: 0.0079, Train Linf Norm: 0.5884, Test Linf Norm: 0.2002\n",
            "Epoch 107: Train Loss: 0.0052, Test Loss: 0.0055, Train L1 Norm: 0.0190, Test L1 Norm: 0.0077, Train Linf Norm: 0.7544, Test Linf Norm: 0.1943\n",
            "Epoch 108: Train Loss: 0.0053, Test Loss: 0.0054, Train L1 Norm: 0.0175, Test L1 Norm: 0.0080, Train Linf Norm: 0.6784, Test Linf Norm: 0.2069\n",
            "Epoch 109: Train Loss: 0.0052, Test Loss: 0.0059, Train L1 Norm: 0.0176, Test L1 Norm: 0.0077, Train Linf Norm: 0.6884, Test Linf Norm: 0.1885\n",
            "Epoch 110: Train Loss: 0.0053, Test Loss: 0.0057, Train L1 Norm: 0.0173, Test L1 Norm: 0.0078, Train Linf Norm: 0.6689, Test Linf Norm: 0.1929\n",
            "Epoch 111: Train Loss: 0.0052, Test Loss: 0.0057, Train L1 Norm: 0.0175, Test L1 Norm: 0.0083, Train Linf Norm: 0.6797, Test Linf Norm: 0.2164\n",
            "Epoch 112: Train Loss: 0.0053, Test Loss: 0.0055, Train L1 Norm: 0.0195, Test L1 Norm: 0.0077, Train Linf Norm: 0.7730, Test Linf Norm: 0.1959\n",
            "Epoch 113: Train Loss: 0.0052, Test Loss: 0.0055, Train L1 Norm: 0.0171, Test L1 Norm: 0.0076, Train Linf Norm: 0.6621, Test Linf Norm: 0.1869\n",
            "Epoch 114: Train Loss: 0.0052, Test Loss: 0.0060, Train L1 Norm: 0.0166, Test L1 Norm: 0.0080, Train Linf Norm: 0.6373, Test Linf Norm: 0.2046\n",
            "Epoch 115: Train Loss: 0.0052, Test Loss: 0.0061, Train L1 Norm: 0.0170, Test L1 Norm: 0.0081, Train Linf Norm: 0.6575, Test Linf Norm: 0.2059\n",
            "Epoch 116: Train Loss: 0.0052, Test Loss: 0.0055, Train L1 Norm: 0.0187, Test L1 Norm: 0.0083, Train Linf Norm: 0.7374, Test Linf Norm: 0.2194\n",
            "Epoch 117: Train Loss: 0.0052, Test Loss: 0.0055, Train L1 Norm: 0.0187, Test L1 Norm: 0.0077, Train Linf Norm: 0.7410, Test Linf Norm: 0.1930\n",
            "Epoch 118: Train Loss: 0.0050, Test Loss: 0.0055, Train L1 Norm: 0.0180, Test L1 Norm: 0.0079, Train Linf Norm: 0.7084, Test Linf Norm: 0.2007\n",
            "Epoch 119: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0161, Test L1 Norm: 0.0079, Train Linf Norm: 0.6154, Test Linf Norm: 0.2019\n",
            "Epoch 120: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0175, Test L1 Norm: 0.0078, Train Linf Norm: 0.6846, Test Linf Norm: 0.1979\n",
            "Epoch 121: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0167, Test L1 Norm: 0.0077, Train Linf Norm: 0.6465, Test Linf Norm: 0.1932\n",
            "Epoch 122: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0178, Test L1 Norm: 0.0079, Train Linf Norm: 0.6968, Test Linf Norm: 0.2029\n",
            "Epoch 123: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0164, Test L1 Norm: 0.0078, Train Linf Norm: 0.6314, Test Linf Norm: 0.1989\n",
            "Epoch 124: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0163, Test L1 Norm: 0.0080, Train Linf Norm: 0.6256, Test Linf Norm: 0.2094\n",
            "Epoch 125: Train Loss: 0.0051, Test Loss: 0.0054, Train L1 Norm: 0.0168, Test L1 Norm: 0.0077, Train Linf Norm: 0.6484, Test Linf Norm: 0.1952\n",
            "Epoch 126: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0181, Test L1 Norm: 0.0079, Train Linf Norm: 0.7151, Test Linf Norm: 0.2026\n",
            "Epoch 127: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0188, Test L1 Norm: 0.0077, Train Linf Norm: 0.7455, Test Linf Norm: 0.1950\n",
            "Epoch 128: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0187, Test L1 Norm: 0.0078, Train Linf Norm: 0.7389, Test Linf Norm: 0.1995\n",
            "Epoch 129: Train Loss: 0.0050, Test Loss: 0.0055, Train L1 Norm: 0.0185, Test L1 Norm: 0.0076, Train Linf Norm: 0.7329, Test Linf Norm: 0.1920\n",
            "Epoch 130: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0173, Test L1 Norm: 0.0079, Train Linf Norm: 0.6738, Test Linf Norm: 0.2007\n",
            "Epoch 131: Train Loss: 0.0050, Test Loss: 0.0057, Train L1 Norm: 0.0184, Test L1 Norm: 0.0078, Train Linf Norm: 0.7105, Test Linf Norm: 0.1966\n",
            "Epoch 132: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0167, Test L1 Norm: 0.0078, Train Linf Norm: 0.6472, Test Linf Norm: 0.2016\n",
            "Epoch 133: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0178, Test L1 Norm: 0.0078, Train Linf Norm: 0.6955, Test Linf Norm: 0.2001\n",
            "Epoch 134: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0170, Test L1 Norm: 0.0077, Train Linf Norm: 0.6617, Test Linf Norm: 0.1955\n",
            "Epoch 135: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0183, Test L1 Norm: 0.0078, Train Linf Norm: 0.7241, Test Linf Norm: 0.2015\n",
            "Epoch 136: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0183, Test L1 Norm: 0.0076, Train Linf Norm: 0.7239, Test Linf Norm: 0.1917\n",
            "Epoch 137: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0177, Test L1 Norm: 0.0078, Train Linf Norm: 0.6981, Test Linf Norm: 0.1991\n",
            "Epoch 138: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0177, Test L1 Norm: 0.0076, Train Linf Norm: 0.6955, Test Linf Norm: 0.1924\n",
            "Epoch 139: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0180, Test L1 Norm: 0.0078, Train Linf Norm: 0.7117, Test Linf Norm: 0.1986\n",
            "Epoch 140: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0174, Test L1 Norm: 0.0078, Train Linf Norm: 0.6810, Test Linf Norm: 0.1983\n",
            "Epoch 141: Train Loss: 0.0050, Test Loss: 0.0055, Train L1 Norm: 0.0178, Test L1 Norm: 0.0080, Train Linf Norm: 0.6999, Test Linf Norm: 0.2067\n",
            "Epoch 142: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0173, Test L1 Norm: 0.0077, Train Linf Norm: 0.6767, Test Linf Norm: 0.1966\n",
            "Epoch 143: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0173, Test L1 Norm: 0.0077, Train Linf Norm: 0.6800, Test Linf Norm: 0.1965\n",
            "Epoch 144: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0169, Test L1 Norm: 0.0077, Train Linf Norm: 0.6572, Test Linf Norm: 0.1950\n",
            "Epoch 145: Train Loss: 0.0049, Test Loss: 0.0054, Train L1 Norm: 0.0172, Test L1 Norm: 0.0078, Train Linf Norm: 0.6716, Test Linf Norm: 0.2017\n",
            "Epoch 146: Train Loss: 0.0049, Test Loss: 0.0054, Train L1 Norm: 0.0174, Test L1 Norm: 0.0078, Train Linf Norm: 0.6817, Test Linf Norm: 0.1986\n",
            "Epoch 147: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0171, Test L1 Norm: 0.0078, Train Linf Norm: 0.6649, Test Linf Norm: 0.1993\n",
            "Epoch 148: Train Loss: 0.0049, Test Loss: 0.0054, Train L1 Norm: 0.0177, Test L1 Norm: 0.0079, Train Linf Norm: 0.6992, Test Linf Norm: 0.2029\n",
            "Epoch 149: Train Loss: 0.0049, Test Loss: 0.0054, Train L1 Norm: 0.0175, Test L1 Norm: 0.0076, Train Linf Norm: 0.6866, Test Linf Norm: 0.1905\n",
            "Epoch 150: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0174, Test L1 Norm: 0.0078, Train Linf Norm: 0.6846, Test Linf Norm: 0.2015\n",
            "Epoch 151: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0174, Test L1 Norm: 0.0079, Train Linf Norm: 0.6778, Test Linf Norm: 0.2063\n",
            "Epoch 152: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0179, Test L1 Norm: 0.0076, Train Linf Norm: 0.7010, Test Linf Norm: 0.1919\n",
            "Epoch 153: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0179, Test L1 Norm: 0.0077, Train Linf Norm: 0.7071, Test Linf Norm: 0.1966\n",
            "Epoch 154: Train Loss: 0.0049, Test Loss: 0.0054, Train L1 Norm: 0.0176, Test L1 Norm: 0.0078, Train Linf Norm: 0.6900, Test Linf Norm: 0.1983\n",
            "Epoch 155: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0176, Test L1 Norm: 0.0078, Train Linf Norm: 0.6949, Test Linf Norm: 0.1996\n",
            "Epoch 156: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0176, Test L1 Norm: 0.0077, Train Linf Norm: 0.6955, Test Linf Norm: 0.1960\n",
            "Epoch 157: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0176, Test L1 Norm: 0.0077, Train Linf Norm: 0.6907, Test Linf Norm: 0.1974\n",
            "Epoch 158: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0179, Test L1 Norm: 0.0077, Train Linf Norm: 0.7102, Test Linf Norm: 0.1976\n",
            "Epoch 159: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0177, Test L1 Norm: 0.0078, Train Linf Norm: 0.6957, Test Linf Norm: 0.2000\n",
            "Epoch 160: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0177, Test L1 Norm: 0.0078, Train Linf Norm: 0.6963, Test Linf Norm: 0.2017\n",
            "Epoch 161: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0176, Test L1 Norm: 0.0077, Train Linf Norm: 0.6903, Test Linf Norm: 0.1958\n",
            "Epoch 162: Train Loss: 0.0049, Test Loss: 0.0054, Train L1 Norm: 0.0180, Test L1 Norm: 0.0078, Train Linf Norm: 0.7086, Test Linf Norm: 0.2018\n",
            "Epoch 163: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0175, Test L1 Norm: 0.0078, Train Linf Norm: 0.6906, Test Linf Norm: 0.1991\n",
            "Epoch 164: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0178, Test L1 Norm: 0.0078, Train Linf Norm: 0.7011, Test Linf Norm: 0.1986\n",
            "Epoch 165: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0177, Test L1 Norm: 0.0078, Train Linf Norm: 0.6965, Test Linf Norm: 0.1990\n",
            "Epoch 166: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0181, Test L1 Norm: 0.0078, Train Linf Norm: 0.7153, Test Linf Norm: 0.1990\n",
            "Epoch 167: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0174, Test L1 Norm: 0.0077, Train Linf Norm: 0.6836, Test Linf Norm: 0.1985\n",
            "Epoch 168: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0176, Test L1 Norm: 0.0078, Train Linf Norm: 0.6957, Test Linf Norm: 0.1990\n",
            "Epoch 169: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0176, Test L1 Norm: 0.0078, Train Linf Norm: 0.6957, Test Linf Norm: 0.1987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 19:42:57,580]\u001b[0m Trial 21 finished with value: 0.007759499435313046 and parameters: {'n_layers': 5, 'n_units_0': 900, 'n_units_1': 1356, 'n_units_2': 1177, 'n_units_3': 1304, 'n_units_4': 1603, 'hidden_activation': 'ReLU', 'output_activation': 'Linear', 'loss': 'MAE', 'optimizer': 'Adam', 'lr': 0.00027295593846122785, 'batch_size': 48, 'n_epochs': 170, 'scheduler': 'ReduceLROnPlateau', 'weight_decay': 1.1188461303124838e-05, 'beta1': 0.9032137590468967, 'beta2': 0.9990087689658779, 'factor': 0.3621354468902799, 'patience': 8, 'threshold': 0.0006353821596836712}. Best is trial 21 with value: 0.007759499435313046.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 170: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0176, Test L1 Norm: 0.0078, Train Linf Norm: 0.6962, Test Linf Norm: 0.1989\n",
            "Epoch 1: Train Loss: 0.1648, Test Loss: 0.0630, Train L1 Norm: 0.3135, Test L1 Norm: 0.0675, Train Linf Norm: 11.5992, Test Linf Norm: 1.8092\n",
            "Epoch 2: Train Loss: 0.0799, Test Loss: 0.1235, Train L1 Norm: 0.1260, Test L1 Norm: 0.0980, Train Linf Norm: 4.3660, Test Linf Norm: 2.2350\n",
            "Epoch 3: Train Loss: 0.0667, Test Loss: 0.1045, Train L1 Norm: 0.1632, Test L1 Norm: 0.0630, Train Linf Norm: 6.3444, Test Linf Norm: 1.2895\n",
            "Epoch 4: Train Loss: 0.0624, Test Loss: 0.0642, Train L1 Norm: 0.1627, Test L1 Norm: 0.0472, Train Linf Norm: 6.4982, Test Linf Norm: 1.1491\n",
            "Epoch 5: Train Loss: 0.0587, Test Loss: 0.0241, Train L1 Norm: 0.1229, Test L1 Norm: 0.0393, Train Linf Norm: 4.6187, Test Linf Norm: 1.1674\n",
            "Epoch 6: Train Loss: 0.0519, Test Loss: 0.0436, Train L1 Norm: 0.1250, Test L1 Norm: 0.0572, Train Linf Norm: 4.8814, Test Linf Norm: 1.5981\n",
            "Epoch 7: Train Loss: 0.0462, Test Loss: 0.0337, Train L1 Norm: 0.1207, Test L1 Norm: 0.0415, Train Linf Norm: 4.7898, Test Linf Norm: 1.2438\n",
            "Epoch 8: Train Loss: 0.0438, Test Loss: 0.0499, Train L1 Norm: 0.1618, Test L1 Norm: 0.0382, Train Linf Norm: 6.8053, Test Linf Norm: 0.8427\n",
            "Epoch 9: Train Loss: 0.0413, Test Loss: 0.0480, Train L1 Norm: 0.1068, Test L1 Norm: 0.0305, Train Linf Norm: 4.2147, Test Linf Norm: 0.5097\n",
            "Epoch 10: Train Loss: 0.0397, Test Loss: 0.1104, Train L1 Norm: 0.1409, Test L1 Norm: 0.0780, Train Linf Norm: 5.9130, Test Linf Norm: 1.9056\n",
            "Epoch 11: Train Loss: 0.0382, Test Loss: 0.0201, Train L1 Norm: 0.0787, Test L1 Norm: 0.0213, Train Linf Norm: 2.9409, Test Linf Norm: 0.5141\n",
            "Epoch 12: Train Loss: 0.0365, Test Loss: 0.0266, Train L1 Norm: 0.0869, Test L1 Norm: 0.0246, Train Linf Norm: 3.3493, Test Linf Norm: 0.5078\n",
            "Epoch 13: Train Loss: 0.0371, Test Loss: 0.0174, Train L1 Norm: 0.1171, Test L1 Norm: 0.0293, Train Linf Norm: 4.8233, Test Linf Norm: 0.9184\n",
            "Epoch 14: Train Loss: 0.0340, Test Loss: 0.0444, Train L1 Norm: 0.1008, Test L1 Norm: 0.0345, Train Linf Norm: 4.0817, Test Linf Norm: 0.8458\n",
            "Epoch 15: Train Loss: 0.0317, Test Loss: 0.0230, Train L1 Norm: 0.1047, Test L1 Norm: 0.0262, Train Linf Norm: 4.3015, Test Linf Norm: 0.6447\n",
            "Epoch 16: Train Loss: 0.0316, Test Loss: 0.0167, Train L1 Norm: 0.0912, Test L1 Norm: 0.0216, Train Linf Norm: 3.6707, Test Linf Norm: 0.4746\n",
            "Epoch 17: Train Loss: 0.0314, Test Loss: 0.0529, Train L1 Norm: 0.0662, Test L1 Norm: 0.0467, Train Linf Norm: 2.4887, Test Linf Norm: 1.1387\n",
            "Epoch 18: Train Loss: 0.0294, Test Loss: 0.0185, Train L1 Norm: 0.0888, Test L1 Norm: 0.0192, Train Linf Norm: 3.5926, Test Linf Norm: 0.4573\n",
            "Epoch 19: Train Loss: 0.0303, Test Loss: 0.0185, Train L1 Norm: 0.0702, Test L1 Norm: 0.0452, Train Linf Norm: 2.6949, Test Linf Norm: 1.3693\n",
            "Epoch 20: Train Loss: 0.0291, Test Loss: 0.0215, Train L1 Norm: 0.0751, Test L1 Norm: 0.0228, Train Linf Norm: 2.9558, Test Linf Norm: 0.5663\n",
            "Epoch 21: Train Loss: 0.0293, Test Loss: 0.0276, Train L1 Norm: 0.0704, Test L1 Norm: 0.0296, Train Linf Norm: 2.7325, Test Linf Norm: 0.8016\n",
            "Epoch 22: Train Loss: 0.0275, Test Loss: 0.0231, Train L1 Norm: 0.0601, Test L1 Norm: 0.0256, Train Linf Norm: 2.2629, Test Linf Norm: 0.6855\n",
            "Epoch 23: Train Loss: 0.0272, Test Loss: 0.0316, Train L1 Norm: 0.0893, Test L1 Norm: 0.0241, Train Linf Norm: 3.6666, Test Linf Norm: 0.5138\n",
            "Epoch 24: Train Loss: 0.0276, Test Loss: 0.0161, Train L1 Norm: 0.0606, Test L1 Norm: 0.0317, Train Linf Norm: 2.2918, Test Linf Norm: 1.0397\n",
            "Epoch 25: Train Loss: 0.0271, Test Loss: 0.0142, Train L1 Norm: 0.0995, Test L1 Norm: 0.0231, Train Linf Norm: 4.1746, Test Linf Norm: 0.6882\n",
            "Epoch 26: Train Loss: 0.0243, Test Loss: 0.0211, Train L1 Norm: 0.0553, Test L1 Norm: 0.0235, Train Linf Norm: 2.0700, Test Linf Norm: 0.5733\n",
            "Epoch 27: Train Loss: 0.0249, Test Loss: 0.0269, Train L1 Norm: 0.0604, Test L1 Norm: 0.0199, Train Linf Norm: 2.3374, Test Linf Norm: 0.4637\n",
            "Epoch 28: Train Loss: 0.0252, Test Loss: 0.0359, Train L1 Norm: 0.0739, Test L1 Norm: 0.0225, Train Linf Norm: 2.9726, Test Linf Norm: 0.4079\n",
            "Epoch 29: Train Loss: 0.0242, Test Loss: 0.0136, Train L1 Norm: 0.0763, Test L1 Norm: 0.0252, Train Linf Norm: 3.1128, Test Linf Norm: 0.8318\n",
            "Epoch 30: Train Loss: 0.0229, Test Loss: 0.0126, Train L1 Norm: 0.0692, Test L1 Norm: 0.0181, Train Linf Norm: 2.5541, Test Linf Norm: 0.4173\n",
            "Epoch 31: Train Loss: 0.0251, Test Loss: 0.0142, Train L1 Norm: 0.0845, Test L1 Norm: 0.0168, Train Linf Norm: 3.4837, Test Linf Norm: 0.4433\n",
            "Epoch 32: Train Loss: 0.0245, Test Loss: 0.0266, Train L1 Norm: 0.0570, Test L1 Norm: 0.0196, Train Linf Norm: 2.1700, Test Linf Norm: 0.4374\n",
            "Epoch 33: Train Loss: 0.0253, Test Loss: 0.0401, Train L1 Norm: 0.0638, Test L1 Norm: 0.0253, Train Linf Norm: 2.4931, Test Linf Norm: 0.4336\n",
            "Epoch 34: Train Loss: 0.0225, Test Loss: 0.0256, Train L1 Norm: 0.0449, Test L1 Norm: 0.0216, Train Linf Norm: 1.6337, Test Linf Norm: 0.3899\n",
            "Epoch 35: Train Loss: 0.0214, Test Loss: 0.0158, Train L1 Norm: 0.0605, Test L1 Norm: 0.0204, Train Linf Norm: 2.3985, Test Linf Norm: 0.5947\n",
            "Epoch 36: Train Loss: 0.0210, Test Loss: 0.0187, Train L1 Norm: 0.0561, Test L1 Norm: 0.0186, Train Linf Norm: 2.1877, Test Linf Norm: 0.5001\n",
            "Epoch 37: Train Loss: 0.0218, Test Loss: 0.0394, Train L1 Norm: 0.0557, Test L1 Norm: 0.0279, Train Linf Norm: 2.1729, Test Linf Norm: 0.6279\n",
            "Epoch 38: Train Loss: 0.0223, Test Loss: 0.0162, Train L1 Norm: 0.0468, Test L1 Norm: 0.0207, Train Linf Norm: 1.7297, Test Linf Norm: 0.6033\n",
            "Epoch 39: Train Loss: 0.0206, Test Loss: 0.0304, Train L1 Norm: 0.0757, Test L1 Norm: 0.0223, Train Linf Norm: 3.1451, Test Linf Norm: 0.3794\n",
            "Epoch 40: Train Loss: 0.0116, Test Loss: 0.0092, Train L1 Norm: 0.0442, Test L1 Norm: 0.0135, Train Linf Norm: 1.8160, Test Linf Norm: 0.3598\n",
            "Epoch 41: Train Loss: 0.0114, Test Loss: 0.0111, Train L1 Norm: 0.0432, Test L1 Norm: 0.0134, Train Linf Norm: 1.7619, Test Linf Norm: 0.3496\n",
            "Epoch 42: Train Loss: 0.0114, Test Loss: 0.0176, Train L1 Norm: 0.0482, Test L1 Norm: 0.0139, Train Linf Norm: 1.9990, Test Linf Norm: 0.3027\n",
            "Epoch 43: Train Loss: 0.0116, Test Loss: 0.0162, Train L1 Norm: 0.0340, Test L1 Norm: 0.0137, Train Linf Norm: 1.3264, Test Linf Norm: 0.3235\n",
            "Epoch 44: Train Loss: 0.0112, Test Loss: 0.0088, Train L1 Norm: 0.0520, Test L1 Norm: 0.0122, Train Linf Norm: 2.1866, Test Linf Norm: 0.3239\n",
            "Epoch 45: Train Loss: 0.0111, Test Loss: 0.0118, Train L1 Norm: 0.0385, Test L1 Norm: 0.0122, Train Linf Norm: 1.5546, Test Linf Norm: 0.2821\n",
            "Epoch 46: Train Loss: 0.0112, Test Loss: 0.0085, Train L1 Norm: 0.0312, Test L1 Norm: 0.0125, Train Linf Norm: 1.2140, Test Linf Norm: 0.3626\n",
            "Epoch 47: Train Loss: 0.0111, Test Loss: 0.0143, Train L1 Norm: 0.0335, Test L1 Norm: 0.0129, Train Linf Norm: 1.3188, Test Linf Norm: 0.3021\n",
            "Epoch 48: Train Loss: 0.0114, Test Loss: 0.0131, Train L1 Norm: 0.0332, Test L1 Norm: 0.0212, Train Linf Norm: 1.3041, Test Linf Norm: 0.6146\n",
            "Epoch 49: Train Loss: 0.0111, Test Loss: 0.0099, Train L1 Norm: 0.0445, Test L1 Norm: 0.0127, Train Linf Norm: 1.8472, Test Linf Norm: 0.3197\n",
            "Epoch 50: Train Loss: 0.0108, Test Loss: 0.0096, Train L1 Norm: 0.0361, Test L1 Norm: 0.0126, Train Linf Norm: 1.4413, Test Linf Norm: 0.3371\n",
            "Epoch 51: Train Loss: 0.0109, Test Loss: 0.0121, Train L1 Norm: 0.0423, Test L1 Norm: 0.0141, Train Linf Norm: 1.7402, Test Linf Norm: 0.3872\n",
            "Epoch 52: Train Loss: 0.0107, Test Loss: 0.0088, Train L1 Norm: 0.0404, Test L1 Norm: 0.0111, Train Linf Norm: 1.6533, Test Linf Norm: 0.2760\n",
            "Epoch 53: Train Loss: 0.0110, Test Loss: 0.0079, Train L1 Norm: 0.0386, Test L1 Norm: 0.0103, Train Linf Norm: 1.5690, Test Linf Norm: 0.2504\n",
            "Epoch 54: Train Loss: 0.0105, Test Loss: 0.0090, Train L1 Norm: 0.0360, Test L1 Norm: 0.0135, Train Linf Norm: 1.4521, Test Linf Norm: 0.3839\n",
            "Epoch 55: Train Loss: 0.0108, Test Loss: 0.0091, Train L1 Norm: 0.0346, Test L1 Norm: 0.0124, Train Linf Norm: 1.3789, Test Linf Norm: 0.3040\n",
            "Epoch 56: Train Loss: 0.0107, Test Loss: 0.0101, Train L1 Norm: 0.0410, Test L1 Norm: 0.0129, Train Linf Norm: 1.6894, Test Linf Norm: 0.3483\n",
            "Epoch 57: Train Loss: 0.0104, Test Loss: 0.0087, Train L1 Norm: 0.0367, Test L1 Norm: 0.0111, Train Linf Norm: 1.4925, Test Linf Norm: 0.2791\n",
            "Epoch 58: Train Loss: 0.0106, Test Loss: 0.0101, Train L1 Norm: 0.0311, Test L1 Norm: 0.0133, Train Linf Norm: 1.2191, Test Linf Norm: 0.3646\n",
            "Epoch 59: Train Loss: 0.0107, Test Loss: 0.0088, Train L1 Norm: 0.0378, Test L1 Norm: 0.0122, Train Linf Norm: 1.5377, Test Linf Norm: 0.3375\n",
            "Epoch 60: Train Loss: 0.0104, Test Loss: 0.0078, Train L1 Norm: 0.0402, Test L1 Norm: 0.0121, Train Linf Norm: 1.6643, Test Linf Norm: 0.3560\n",
            "Epoch 61: Train Loss: 0.0106, Test Loss: 0.0084, Train L1 Norm: 0.0402, Test L1 Norm: 0.0105, Train Linf Norm: 1.6509, Test Linf Norm: 0.2715\n",
            "Epoch 62: Train Loss: 0.0105, Test Loss: 0.0096, Train L1 Norm: 0.0353, Test L1 Norm: 0.0133, Train Linf Norm: 1.4259, Test Linf Norm: 0.3961\n",
            "Epoch 63: Train Loss: 0.0102, Test Loss: 0.0089, Train L1 Norm: 0.0309, Test L1 Norm: 0.0134, Train Linf Norm: 1.2227, Test Linf Norm: 0.3962\n",
            "Epoch 64: Train Loss: 0.0103, Test Loss: 0.0096, Train L1 Norm: 0.0350, Test L1 Norm: 0.0132, Train Linf Norm: 1.4161, Test Linf Norm: 0.2859\n",
            "Epoch 65: Train Loss: 0.0105, Test Loss: 0.0127, Train L1 Norm: 0.0331, Test L1 Norm: 0.0127, Train Linf Norm: 1.3069, Test Linf Norm: 0.3462\n",
            "Epoch 66: Train Loss: 0.0104, Test Loss: 0.0074, Train L1 Norm: 0.0389, Test L1 Norm: 0.0103, Train Linf Norm: 1.5974, Test Linf Norm: 0.2814\n",
            "Epoch 67: Train Loss: 0.0104, Test Loss: 0.0074, Train L1 Norm: 0.0365, Test L1 Norm: 0.0194, Train Linf Norm: 1.4901, Test Linf Norm: 0.6478\n",
            "Epoch 68: Train Loss: 0.0103, Test Loss: 0.0072, Train L1 Norm: 0.0272, Test L1 Norm: 0.0102, Train Linf Norm: 1.0444, Test Linf Norm: 0.2745\n",
            "Epoch 69: Train Loss: 0.0102, Test Loss: 0.0098, Train L1 Norm: 0.0382, Test L1 Norm: 0.0105, Train Linf Norm: 1.5703, Test Linf Norm: 0.2588\n",
            "Epoch 70: Train Loss: 0.0102, Test Loss: 0.0120, Train L1 Norm: 0.0383, Test L1 Norm: 0.0132, Train Linf Norm: 1.5763, Test Linf Norm: 0.3673\n",
            "Epoch 71: Train Loss: 0.0098, Test Loss: 0.0139, Train L1 Norm: 0.0434, Test L1 Norm: 0.0116, Train Linf Norm: 1.8198, Test Linf Norm: 0.2738\n",
            "Epoch 72: Train Loss: 0.0100, Test Loss: 0.0080, Train L1 Norm: 0.0443, Test L1 Norm: 0.0124, Train Linf Norm: 1.8695, Test Linf Norm: 0.3412\n",
            "Epoch 73: Train Loss: 0.0098, Test Loss: 0.0105, Train L1 Norm: 0.0324, Test L1 Norm: 0.0117, Train Linf Norm: 1.3013, Test Linf Norm: 0.3128\n",
            "Epoch 74: Train Loss: 0.0103, Test Loss: 0.0106, Train L1 Norm: 0.0291, Test L1 Norm: 0.0103, Train Linf Norm: 1.1402, Test Linf Norm: 0.2528\n",
            "Epoch 75: Train Loss: 0.0099, Test Loss: 0.0204, Train L1 Norm: 0.0270, Test L1 Norm: 0.0137, Train Linf Norm: 1.0452, Test Linf Norm: 0.2595\n",
            "Epoch 76: Train Loss: 0.0100, Test Loss: 0.0087, Train L1 Norm: 0.0370, Test L1 Norm: 0.0153, Train Linf Norm: 1.5151, Test Linf Norm: 0.4780\n",
            "Epoch 77: Train Loss: 0.0103, Test Loss: 0.0082, Train L1 Norm: 0.0409, Test L1 Norm: 0.0099, Train Linf Norm: 1.7067, Test Linf Norm: 0.2557\n",
            "Epoch 78: Train Loss: 0.0073, Test Loss: 0.0097, Train L1 Norm: 0.0285, Test L1 Norm: 0.0099, Train Linf Norm: 1.1675, Test Linf Norm: 0.2483\n",
            "Epoch 79: Train Loss: 0.0071, Test Loss: 0.0097, Train L1 Norm: 0.0252, Test L1 Norm: 0.0121, Train Linf Norm: 1.0188, Test Linf Norm: 0.3475\n",
            "Epoch 80: Train Loss: 0.0071, Test Loss: 0.0079, Train L1 Norm: 0.0183, Test L1 Norm: 0.0123, Train Linf Norm: 0.6816, Test Linf Norm: 0.3720\n",
            "Epoch 81: Train Loss: 0.0073, Test Loss: 0.0066, Train L1 Norm: 0.0237, Test L1 Norm: 0.0106, Train Linf Norm: 0.9381, Test Linf Norm: 0.3216\n",
            "Epoch 82: Train Loss: 0.0072, Test Loss: 0.0073, Train L1 Norm: 0.0209, Test L1 Norm: 0.0113, Train Linf Norm: 0.8051, Test Linf Norm: 0.3367\n",
            "Epoch 83: Train Loss: 0.0071, Test Loss: 0.0066, Train L1 Norm: 0.0238, Test L1 Norm: 0.0099, Train Linf Norm: 0.9469, Test Linf Norm: 0.2896\n",
            "Epoch 84: Train Loss: 0.0071, Test Loss: 0.0068, Train L1 Norm: 0.0256, Test L1 Norm: 0.0098, Train Linf Norm: 1.0309, Test Linf Norm: 0.2790\n",
            "Epoch 85: Train Loss: 0.0069, Test Loss: 0.0077, Train L1 Norm: 0.0167, Test L1 Norm: 0.0095, Train Linf Norm: 0.6116, Test Linf Norm: 0.2502\n",
            "Epoch 86: Train Loss: 0.0069, Test Loss: 0.0073, Train L1 Norm: 0.0241, Test L1 Norm: 0.0091, Train Linf Norm: 0.9672, Test Linf Norm: 0.2381\n",
            "Epoch 87: Train Loss: 0.0072, Test Loss: 0.0081, Train L1 Norm: 0.0235, Test L1 Norm: 0.0109, Train Linf Norm: 0.9337, Test Linf Norm: 0.3189\n",
            "Epoch 88: Train Loss: 0.0070, Test Loss: 0.0069, Train L1 Norm: 0.0237, Test L1 Norm: 0.0096, Train Linf Norm: 0.9423, Test Linf Norm: 0.2609\n",
            "Epoch 89: Train Loss: 0.0072, Test Loss: 0.0077, Train L1 Norm: 0.0268, Test L1 Norm: 0.0094, Train Linf Norm: 1.0922, Test Linf Norm: 0.2350\n",
            "Epoch 90: Train Loss: 0.0071, Test Loss: 0.0063, Train L1 Norm: 0.0300, Test L1 Norm: 0.0089, Train Linf Norm: 1.2297, Test Linf Norm: 0.2405\n",
            "Epoch 91: Train Loss: 0.0068, Test Loss: 0.0068, Train L1 Norm: 0.0182, Test L1 Norm: 0.0095, Train Linf Norm: 0.6853, Test Linf Norm: 0.2683\n",
            "Epoch 92: Train Loss: 0.0070, Test Loss: 0.0070, Train L1 Norm: 0.0202, Test L1 Norm: 0.0095, Train Linf Norm: 0.7835, Test Linf Norm: 0.2497\n",
            "Epoch 93: Train Loss: 0.0069, Test Loss: 0.0086, Train L1 Norm: 0.0152, Test L1 Norm: 0.0099, Train Linf Norm: 0.5413, Test Linf Norm: 0.2737\n",
            "Epoch 94: Train Loss: 0.0068, Test Loss: 0.0065, Train L1 Norm: 0.0148, Test L1 Norm: 0.0088, Train Linf Norm: 0.5245, Test Linf Norm: 0.2319\n",
            "Epoch 95: Train Loss: 0.0069, Test Loss: 0.0113, Train L1 Norm: 0.0200, Test L1 Norm: 0.0107, Train Linf Norm: 0.7746, Test Linf Norm: 0.2665\n",
            "Epoch 96: Train Loss: 0.0069, Test Loss: 0.0075, Train L1 Norm: 0.0220, Test L1 Norm: 0.0099, Train Linf Norm: 0.8648, Test Linf Norm: 0.2735\n",
            "Epoch 97: Train Loss: 0.0069, Test Loss: 0.0085, Train L1 Norm: 0.0208, Test L1 Norm: 0.0102, Train Linf Norm: 0.8061, Test Linf Norm: 0.2704\n",
            "Epoch 98: Train Loss: 0.0069, Test Loss: 0.0062, Train L1 Norm: 0.0180, Test L1 Norm: 0.0094, Train Linf Norm: 0.6764, Test Linf Norm: 0.2731\n",
            "Epoch 99: Train Loss: 0.0067, Test Loss: 0.0127, Train L1 Norm: 0.0166, Test L1 Norm: 0.0137, Train Linf Norm: 0.6120, Test Linf Norm: 0.3572\n",
            "Epoch 100: Train Loss: 0.0069, Test Loss: 0.0065, Train L1 Norm: 0.0252, Test L1 Norm: 0.0098, Train Linf Norm: 1.0223, Test Linf Norm: 0.2905\n",
            "Epoch 101: Train Loss: 0.0068, Test Loss: 0.0073, Train L1 Norm: 0.0226, Test L1 Norm: 0.0090, Train Linf Norm: 0.8963, Test Linf Norm: 0.2225\n",
            "Epoch 102: Train Loss: 0.0067, Test Loss: 0.0062, Train L1 Norm: 0.0192, Test L1 Norm: 0.0107, Train Linf Norm: 0.7351, Test Linf Norm: 0.3378\n",
            "Epoch 103: Train Loss: 0.0067, Test Loss: 0.0061, Train L1 Norm: 0.0238, Test L1 Norm: 0.0086, Train Linf Norm: 0.9489, Test Linf Norm: 0.2312\n",
            "Epoch 104: Train Loss: 0.0068, Test Loss: 0.0077, Train L1 Norm: 0.0221, Test L1 Norm: 0.0091, Train Linf Norm: 0.8759, Test Linf Norm: 0.2410\n",
            "Epoch 105: Train Loss: 0.0067, Test Loss: 0.0102, Train L1 Norm: 0.0220, Test L1 Norm: 0.0113, Train Linf Norm: 0.8682, Test Linf Norm: 0.3019\n",
            "Epoch 106: Train Loss: 0.0069, Test Loss: 0.0107, Train L1 Norm: 0.0196, Test L1 Norm: 0.0101, Train Linf Norm: 0.7540, Test Linf Norm: 0.2561\n",
            "Epoch 107: Train Loss: 0.0068, Test Loss: 0.0067, Train L1 Norm: 0.0190, Test L1 Norm: 0.0106, Train Linf Norm: 0.7258, Test Linf Norm: 0.3143\n",
            "Epoch 108: Train Loss: 0.0068, Test Loss: 0.0062, Train L1 Norm: 0.0173, Test L1 Norm: 0.0105, Train Linf Norm: 0.6287, Test Linf Norm: 0.3176\n",
            "Epoch 109: Train Loss: 0.0067, Test Loss: 0.0063, Train L1 Norm: 0.0194, Test L1 Norm: 0.0092, Train Linf Norm: 0.7424, Test Linf Norm: 0.2652\n",
            "Epoch 110: Train Loss: 0.0067, Test Loss: 0.0064, Train L1 Norm: 0.0195, Test L1 Norm: 0.0095, Train Linf Norm: 0.7523, Test Linf Norm: 0.2715\n",
            "Epoch 111: Train Loss: 0.0067, Test Loss: 0.0079, Train L1 Norm: 0.0256, Test L1 Norm: 0.0087, Train Linf Norm: 1.0467, Test Linf Norm: 0.2128\n",
            "Epoch 112: Train Loss: 0.0068, Test Loss: 0.0063, Train L1 Norm: 0.0225, Test L1 Norm: 0.0084, Train Linf Norm: 0.8967, Test Linf Norm: 0.2214\n",
            "Epoch 113: Train Loss: 0.0057, Test Loss: 0.0060, Train L1 Norm: 0.0156, Test L1 Norm: 0.0088, Train Linf Norm: 0.5837, Test Linf Norm: 0.2516\n",
            "Epoch 114: Train Loss: 0.0057, Test Loss: 0.0082, Train L1 Norm: 0.0170, Test L1 Norm: 0.0088, Train Linf Norm: 0.6531, Test Linf Norm: 0.2314\n",
            "Epoch 115: Train Loss: 0.0058, Test Loss: 0.0064, Train L1 Norm: 0.0178, Test L1 Norm: 0.0088, Train Linf Norm: 0.6806, Test Linf Norm: 0.2473\n",
            "Epoch 116: Train Loss: 0.0057, Test Loss: 0.0062, Train L1 Norm: 0.0144, Test L1 Norm: 0.0081, Train Linf Norm: 0.5316, Test Linf Norm: 0.2125\n",
            "Epoch 117: Train Loss: 0.0057, Test Loss: 0.0057, Train L1 Norm: 0.0156, Test L1 Norm: 0.0081, Train Linf Norm: 0.5850, Test Linf Norm: 0.2159\n",
            "Epoch 118: Train Loss: 0.0057, Test Loss: 0.0077, Train L1 Norm: 0.0158, Test L1 Norm: 0.0089, Train Linf Norm: 0.5957, Test Linf Norm: 0.2391\n",
            "Epoch 119: Train Loss: 0.0057, Test Loss: 0.0063, Train L1 Norm: 0.0161, Test L1 Norm: 0.0088, Train Linf Norm: 0.6095, Test Linf Norm: 0.2480\n",
            "Epoch 120: Train Loss: 0.0058, Test Loss: 0.0059, Train L1 Norm: 0.0136, Test L1 Norm: 0.0081, Train Linf Norm: 0.4878, Test Linf Norm: 0.2144\n",
            "Epoch 121: Train Loss: 0.0057, Test Loss: 0.0057, Train L1 Norm: 0.0161, Test L1 Norm: 0.0083, Train Linf Norm: 0.6055, Test Linf Norm: 0.2267\n",
            "Epoch 122: Train Loss: 0.0057, Test Loss: 0.0058, Train L1 Norm: 0.0156, Test L1 Norm: 0.0082, Train Linf Norm: 0.5839, Test Linf Norm: 0.2150\n",
            "Epoch 123: Train Loss: 0.0057, Test Loss: 0.0059, Train L1 Norm: 0.0165, Test L1 Norm: 0.0080, Train Linf Norm: 0.6257, Test Linf Norm: 0.2062\n",
            "Epoch 124: Train Loss: 0.0057, Test Loss: 0.0056, Train L1 Norm: 0.0135, Test L1 Norm: 0.0082, Train Linf Norm: 0.4866, Test Linf Norm: 0.2239\n",
            "Epoch 125: Train Loss: 0.0057, Test Loss: 0.0067, Train L1 Norm: 0.0143, Test L1 Norm: 0.0086, Train Linf Norm: 0.5242, Test Linf Norm: 0.2350\n",
            "Epoch 126: Train Loss: 0.0057, Test Loss: 0.0056, Train L1 Norm: 0.0121, Test L1 Norm: 0.0084, Train Linf Norm: 0.4190, Test Linf Norm: 0.2358\n",
            "Epoch 127: Train Loss: 0.0057, Test Loss: 0.0066, Train L1 Norm: 0.0151, Test L1 Norm: 0.0083, Train Linf Norm: 0.5670, Test Linf Norm: 0.2146\n",
            "Epoch 128: Train Loss: 0.0057, Test Loss: 0.0059, Train L1 Norm: 0.0149, Test L1 Norm: 0.0082, Train Linf Norm: 0.5552, Test Linf Norm: 0.2261\n",
            "Epoch 129: Train Loss: 0.0057, Test Loss: 0.0060, Train L1 Norm: 0.0157, Test L1 Norm: 0.0081, Train Linf Norm: 0.5871, Test Linf Norm: 0.2138\n",
            "Epoch 130: Train Loss: 0.0056, Test Loss: 0.0067, Train L1 Norm: 0.0150, Test L1 Norm: 0.0088, Train Linf Norm: 0.5599, Test Linf Norm: 0.2384\n",
            "Epoch 131: Train Loss: 0.0056, Test Loss: 0.0058, Train L1 Norm: 0.0151, Test L1 Norm: 0.0087, Train Linf Norm: 0.5635, Test Linf Norm: 0.2466\n",
            "Epoch 132: Train Loss: 0.0056, Test Loss: 0.0062, Train L1 Norm: 0.0181, Test L1 Norm: 0.0081, Train Linf Norm: 0.7115, Test Linf Norm: 0.2136\n",
            "Epoch 133: Train Loss: 0.0056, Test Loss: 0.0074, Train L1 Norm: 0.0158, Test L1 Norm: 0.0085, Train Linf Norm: 0.5994, Test Linf Norm: 0.2260\n",
            "Epoch 134: Train Loss: 0.0053, Test Loss: 0.0059, Train L1 Norm: 0.0153, Test L1 Norm: 0.0080, Train Linf Norm: 0.5803, Test Linf Norm: 0.2190\n",
            "Epoch 135: Train Loss: 0.0052, Test Loss: 0.0060, Train L1 Norm: 0.0142, Test L1 Norm: 0.0080, Train Linf Norm: 0.5275, Test Linf Norm: 0.2098\n",
            "Epoch 136: Train Loss: 0.0052, Test Loss: 0.0056, Train L1 Norm: 0.0142, Test L1 Norm: 0.0083, Train Linf Norm: 0.5261, Test Linf Norm: 0.2320\n",
            "Epoch 137: Train Loss: 0.0053, Test Loss: 0.0056, Train L1 Norm: 0.0146, Test L1 Norm: 0.0081, Train Linf Norm: 0.5470, Test Linf Norm: 0.2237\n",
            "Epoch 138: Train Loss: 0.0052, Test Loss: 0.0057, Train L1 Norm: 0.0153, Test L1 Norm: 0.0079, Train Linf Norm: 0.5791, Test Linf Norm: 0.2166\n",
            "Epoch 139: Train Loss: 0.0052, Test Loss: 0.0071, Train L1 Norm: 0.0125, Test L1 Norm: 0.0092, Train Linf Norm: 0.4485, Test Linf Norm: 0.2566\n",
            "Epoch 140: Train Loss: 0.0052, Test Loss: 0.0056, Train L1 Norm: 0.0146, Test L1 Norm: 0.0080, Train Linf Norm: 0.5461, Test Linf Norm: 0.2144\n",
            "Epoch 141: Train Loss: 0.0052, Test Loss: 0.0057, Train L1 Norm: 0.0129, Test L1 Norm: 0.0080, Train Linf Norm: 0.4652, Test Linf Norm: 0.2181\n",
            "Epoch 142: Train Loss: 0.0052, Test Loss: 0.0056, Train L1 Norm: 0.0139, Test L1 Norm: 0.0086, Train Linf Norm: 0.5109, Test Linf Norm: 0.2472\n",
            "Epoch 143: Train Loss: 0.0053, Test Loss: 0.0057, Train L1 Norm: 0.0144, Test L1 Norm: 0.0082, Train Linf Norm: 0.5395, Test Linf Norm: 0.2268\n",
            "Epoch 144: Train Loss: 0.0052, Test Loss: 0.0056, Train L1 Norm: 0.0131, Test L1 Norm: 0.0083, Train Linf Norm: 0.4744, Test Linf Norm: 0.2349\n",
            "Epoch 145: Train Loss: 0.0052, Test Loss: 0.0056, Train L1 Norm: 0.0143, Test L1 Norm: 0.0079, Train Linf Norm: 0.5342, Test Linf Norm: 0.2106\n",
            "Epoch 146: Train Loss: 0.0052, Test Loss: 0.0059, Train L1 Norm: 0.0141, Test L1 Norm: 0.0080, Train Linf Norm: 0.5218, Test Linf Norm: 0.2155\n",
            "Epoch 147: Train Loss: 0.0051, Test Loss: 0.0059, Train L1 Norm: 0.0141, Test L1 Norm: 0.0086, Train Linf Norm: 0.5268, Test Linf Norm: 0.2443\n",
            "Epoch 148: Train Loss: 0.0051, Test Loss: 0.0055, Train L1 Norm: 0.0139, Test L1 Norm: 0.0080, Train Linf Norm: 0.5164, Test Linf Norm: 0.2228\n",
            "Epoch 149: Train Loss: 0.0050, Test Loss: 0.0056, Train L1 Norm: 0.0134, Test L1 Norm: 0.0081, Train Linf Norm: 0.4926, Test Linf Norm: 0.2240\n",
            "Epoch 150: Train Loss: 0.0051, Test Loss: 0.0056, Train L1 Norm: 0.0135, Test L1 Norm: 0.0081, Train Linf Norm: 0.4966, Test Linf Norm: 0.2279\n",
            "Epoch 151: Train Loss: 0.0051, Test Loss: 0.0056, Train L1 Norm: 0.0136, Test L1 Norm: 0.0079, Train Linf Norm: 0.5032, Test Linf Norm: 0.2129\n",
            "Epoch 152: Train Loss: 0.0051, Test Loss: 0.0056, Train L1 Norm: 0.0138, Test L1 Norm: 0.0080, Train Linf Norm: 0.5102, Test Linf Norm: 0.2172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 20:01:53,581]\u001b[0m Trial 22 finished with value: 0.007967275777459145 and parameters: {'n_layers': 4, 'n_units_0': 658, 'n_units_1': 1299, 'n_units_2': 1294, 'n_units_3': 1234, 'hidden_activation': 'ReLU', 'output_activation': 'Linear', 'loss': 'MAE', 'optimizer': 'Adam', 'lr': 0.0002727399487909076, 'batch_size': 48, 'n_epochs': 153, 'scheduler': 'ReduceLROnPlateau', 'weight_decay': 1.2463422064249187e-05, 'beta1': 0.9015456177319838, 'beta2': 0.999001768674055, 'factor': 0.358202920212349, 'patience': 8, 'threshold': 0.0006754389120182475}. Best is trial 21 with value: 0.007759499435313046.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 153: Train Loss: 0.0050, Test Loss: 0.0057, Train L1 Norm: 0.0138, Test L1 Norm: 0.0080, Train Linf Norm: 0.5175, Test Linf Norm: 0.2179\n",
            "Epoch 1: Train Loss: 0.1379, Test Loss: 0.0605, Train L1 Norm: 0.2803, Test L1 Norm: 0.0441, Train Linf Norm: 10.4025, Test Linf Norm: 0.8450\n",
            "Epoch 2: Train Loss: 0.0721, Test Loss: 0.0982, Train L1 Norm: 0.1334, Test L1 Norm: 0.0868, Train Linf Norm: 4.8392, Test Linf Norm: 2.3199\n",
            "Epoch 3: Train Loss: 0.0663, Test Loss: 0.0493, Train L1 Norm: 0.1015, Test L1 Norm: 0.0571, Train Linf Norm: 3.4198, Test Linf Norm: 1.4866\n",
            "Epoch 4: Train Loss: 0.0530, Test Loss: 0.0266, Train L1 Norm: 0.0711, Test L1 Norm: 0.0248, Train Linf Norm: 2.2752, Test Linf Norm: 0.4874\n",
            "Epoch 5: Train Loss: 0.0476, Test Loss: 0.0294, Train L1 Norm: 0.1295, Test L1 Norm: 0.0259, Train Linf Norm: 5.1760, Test Linf Norm: 0.6083\n",
            "Epoch 6: Train Loss: 0.0422, Test Loss: 0.0415, Train L1 Norm: 0.0727, Test L1 Norm: 0.0476, Train Linf Norm: 2.5662, Test Linf Norm: 1.2606\n",
            "Epoch 7: Train Loss: 0.0413, Test Loss: 0.0580, Train L1 Norm: 0.0887, Test L1 Norm: 0.0318, Train Linf Norm: 3.3458, Test Linf Norm: 0.4233\n",
            "Epoch 8: Train Loss: 0.0391, Test Loss: 0.0275, Train L1 Norm: 0.0762, Test L1 Norm: 0.0381, Train Linf Norm: 2.7989, Test Linf Norm: 1.0954\n",
            "Epoch 9: Train Loss: 0.0380, Test Loss: 0.0322, Train L1 Norm: 0.0701, Test L1 Norm: 0.0345, Train Linf Norm: 2.5045, Test Linf Norm: 0.9306\n",
            "Epoch 10: Train Loss: 0.0339, Test Loss: 0.0337, Train L1 Norm: 0.0803, Test L1 Norm: 0.0301, Train Linf Norm: 3.1012, Test Linf Norm: 0.7871\n",
            "Epoch 11: Train Loss: 0.0331, Test Loss: 0.0236, Train L1 Norm: 0.0813, Test L1 Norm: 0.0188, Train Linf Norm: 3.1642, Test Linf Norm: 0.3493\n",
            "Epoch 12: Train Loss: 0.0306, Test Loss: 0.0210, Train L1 Norm: 0.0707, Test L1 Norm: 0.0299, Train Linf Norm: 2.6882, Test Linf Norm: 0.8428\n",
            "Epoch 13: Train Loss: 0.0312, Test Loss: 0.0635, Train L1 Norm: 0.0532, Test L1 Norm: 0.0396, Train Linf Norm: 1.8464, Test Linf Norm: 0.6337\n",
            "Epoch 14: Train Loss: 0.0306, Test Loss: 0.0704, Train L1 Norm: 0.0624, Test L1 Norm: 0.0475, Train Linf Norm: 2.2666, Test Linf Norm: 0.9914\n",
            "Epoch 15: Train Loss: 0.0277, Test Loss: 0.0271, Train L1 Norm: 0.0769, Test L1 Norm: 0.0206, Train Linf Norm: 3.0528, Test Linf Norm: 0.4317\n",
            "Epoch 16: Train Loss: 0.0309, Test Loss: 0.0382, Train L1 Norm: 0.0603, Test L1 Norm: 0.0427, Train Linf Norm: 2.2148, Test Linf Norm: 1.2655\n",
            "Epoch 17: Train Loss: 0.0282, Test Loss: 0.0210, Train L1 Norm: 0.0639, Test L1 Norm: 0.0289, Train Linf Norm: 2.4223, Test Linf Norm: 0.8402\n",
            "Epoch 18: Train Loss: 0.0270, Test Loss: 0.0198, Train L1 Norm: 0.0698, Test L1 Norm: 0.0243, Train Linf Norm: 2.7296, Test Linf Norm: 0.7056\n",
            "Epoch 19: Train Loss: 0.0263, Test Loss: 0.0258, Train L1 Norm: 0.0815, Test L1 Norm: 0.0190, Train Linf Norm: 3.3060, Test Linf Norm: 0.3254\n",
            "Epoch 20: Train Loss: 0.0269, Test Loss: 0.0237, Train L1 Norm: 0.0584, Test L1 Norm: 0.0231, Train Linf Norm: 2.1803, Test Linf Norm: 0.5791\n",
            "Epoch 21: Train Loss: 0.0240, Test Loss: 0.0203, Train L1 Norm: 0.0509, Test L1 Norm: 0.0197, Train Linf Norm: 1.8876, Test Linf Norm: 0.4825\n",
            "Epoch 22: Train Loss: 0.0248, Test Loss: 0.0195, Train L1 Norm: 0.0467, Test L1 Norm: 0.0217, Train Linf Norm: 1.6822, Test Linf Norm: 0.5581\n",
            "Epoch 23: Train Loss: 0.0228, Test Loss: 0.0179, Train L1 Norm: 0.0632, Test L1 Norm: 0.0178, Train Linf Norm: 2.4949, Test Linf Norm: 0.3830\n",
            "Epoch 24: Train Loss: 0.0234, Test Loss: 0.0228, Train L1 Norm: 0.0534, Test L1 Norm: 0.0202, Train Linf Norm: 2.0210, Test Linf Norm: 0.4872\n",
            "Epoch 25: Train Loss: 0.0223, Test Loss: 0.0157, Train L1 Norm: 0.0381, Test L1 Norm: 0.0162, Train Linf Norm: 1.2932, Test Linf Norm: 0.4081\n",
            "Epoch 26: Train Loss: 0.0232, Test Loss: 0.0119, Train L1 Norm: 0.0761, Test L1 Norm: 0.0162, Train Linf Norm: 3.1299, Test Linf Norm: 0.3827\n",
            "Epoch 27: Train Loss: 0.0223, Test Loss: 0.0254, Train L1 Norm: 0.0450, Test L1 Norm: 0.0215, Train Linf Norm: 1.6495, Test Linf Norm: 0.4873\n",
            "Epoch 28: Train Loss: 0.0222, Test Loss: 0.0146, Train L1 Norm: 0.0458, Test L1 Norm: 0.0227, Train Linf Norm: 1.6853, Test Linf Norm: 0.6358\n",
            "Epoch 29: Train Loss: 0.0220, Test Loss: 0.0174, Train L1 Norm: 0.0446, Test L1 Norm: 0.0186, Train Linf Norm: 1.6349, Test Linf Norm: 0.5020\n",
            "Epoch 30: Train Loss: 0.0209, Test Loss: 0.0499, Train L1 Norm: 0.0450, Test L1 Norm: 0.0282, Train Linf Norm: 1.6780, Test Linf Norm: 0.5138\n",
            "Epoch 31: Train Loss: 0.0219, Test Loss: 0.0148, Train L1 Norm: 0.0424, Test L1 Norm: 0.0153, Train Linf Norm: 1.5303, Test Linf Norm: 0.3437\n",
            "Epoch 32: Train Loss: 0.0199, Test Loss: 0.0165, Train L1 Norm: 0.0474, Test L1 Norm: 0.0225, Train Linf Norm: 1.8066, Test Linf Norm: 0.5695\n",
            "Epoch 33: Train Loss: 0.0215, Test Loss: 0.0218, Train L1 Norm: 0.0348, Test L1 Norm: 0.0193, Train Linf Norm: 1.1807, Test Linf Norm: 0.4074\n",
            "Epoch 34: Train Loss: 0.0198, Test Loss: 0.0300, Train L1 Norm: 0.0502, Test L1 Norm: 0.0258, Train Linf Norm: 1.9414, Test Linf Norm: 0.5252\n",
            "Epoch 35: Train Loss: 0.0211, Test Loss: 0.0308, Train L1 Norm: 0.0438, Test L1 Norm: 0.0233, Train Linf Norm: 1.6311, Test Linf Norm: 0.5164\n",
            "Epoch 36: Train Loss: 0.0115, Test Loss: 0.0107, Train L1 Norm: 0.0267, Test L1 Norm: 0.0134, Train Linf Norm: 0.9818, Test Linf Norm: 0.3489\n",
            "Epoch 37: Train Loss: 0.0117, Test Loss: 0.0116, Train L1 Norm: 0.0208, Test L1 Norm: 0.0124, Train Linf Norm: 0.6951, Test Linf Norm: 0.2823\n",
            "Epoch 38: Train Loss: 0.0115, Test Loss: 0.0106, Train L1 Norm: 0.0265, Test L1 Norm: 0.0113, Train Linf Norm: 0.9794, Test Linf Norm: 0.2714\n",
            "Epoch 39: Train Loss: 0.0115, Test Loss: 0.0136, Train L1 Norm: 0.0227, Test L1 Norm: 0.0154, Train Linf Norm: 0.7961, Test Linf Norm: 0.4042\n",
            "Epoch 40: Train Loss: 0.0115, Test Loss: 0.0087, Train L1 Norm: 0.0248, Test L1 Norm: 0.0114, Train Linf Norm: 0.8982, Test Linf Norm: 0.2867\n",
            "Epoch 41: Train Loss: 0.0113, Test Loss: 0.0095, Train L1 Norm: 0.0235, Test L1 Norm: 0.0120, Train Linf Norm: 0.8398, Test Linf Norm: 0.2938\n",
            "Epoch 42: Train Loss: 0.0110, Test Loss: 0.0150, Train L1 Norm: 0.0208, Test L1 Norm: 0.0129, Train Linf Norm: 0.7106, Test Linf Norm: 0.2809\n",
            "Epoch 43: Train Loss: 0.0109, Test Loss: 0.0093, Train L1 Norm: 0.0286, Test L1 Norm: 0.0112, Train Linf Norm: 1.0929, Test Linf Norm: 0.2787\n",
            "Epoch 44: Train Loss: 0.0113, Test Loss: 0.0146, Train L1 Norm: 0.0264, Test L1 Norm: 0.0144, Train Linf Norm: 0.9723, Test Linf Norm: 0.3549\n",
            "Epoch 45: Train Loss: 0.0111, Test Loss: 0.0128, Train L1 Norm: 0.0231, Test L1 Norm: 0.0131, Train Linf Norm: 0.8228, Test Linf Norm: 0.3197\n",
            "Epoch 46: Train Loss: 0.0107, Test Loss: 0.0105, Train L1 Norm: 0.0282, Test L1 Norm: 0.0116, Train Linf Norm: 1.0807, Test Linf Norm: 0.2804\n",
            "Epoch 47: Train Loss: 0.0116, Test Loss: 0.0099, Train L1 Norm: 0.0228, Test L1 Norm: 0.0108, Train Linf Norm: 0.8058, Test Linf Norm: 0.2584\n",
            "Epoch 48: Train Loss: 0.0111, Test Loss: 0.0123, Train L1 Norm: 0.0336, Test L1 Norm: 0.0115, Train Linf Norm: 1.3339, Test Linf Norm: 0.2649\n",
            "Epoch 49: Train Loss: 0.0108, Test Loss: 0.0109, Train L1 Norm: 0.0267, Test L1 Norm: 0.0126, Train Linf Norm: 1.0086, Test Linf Norm: 0.3145\n",
            "Epoch 50: Train Loss: 0.0081, Test Loss: 0.0077, Train L1 Norm: 0.0181, Test L1 Norm: 0.0108, Train Linf Norm: 0.6461, Test Linf Norm: 0.2871\n",
            "Epoch 51: Train Loss: 0.0081, Test Loss: 0.0084, Train L1 Norm: 0.0213, Test L1 Norm: 0.0100, Train Linf Norm: 0.7983, Test Linf Norm: 0.2444\n",
            "Epoch 52: Train Loss: 0.0082, Test Loss: 0.0081, Train L1 Norm: 0.0234, Test L1 Norm: 0.0099, Train Linf Norm: 0.9005, Test Linf Norm: 0.2484\n",
            "Epoch 53: Train Loss: 0.0082, Test Loss: 0.0102, Train L1 Norm: 0.0237, Test L1 Norm: 0.0154, Train Linf Norm: 0.9167, Test Linf Norm: 0.4224\n",
            "Epoch 54: Train Loss: 0.0081, Test Loss: 0.0106, Train L1 Norm: 0.0233, Test L1 Norm: 0.0106, Train Linf Norm: 0.8975, Test Linf Norm: 0.2523\n",
            "Epoch 55: Train Loss: 0.0082, Test Loss: 0.0089, Train L1 Norm: 0.0207, Test L1 Norm: 0.0103, Train Linf Norm: 0.7753, Test Linf Norm: 0.2513\n",
            "Epoch 56: Train Loss: 0.0082, Test Loss: 0.0079, Train L1 Norm: 0.0203, Test L1 Norm: 0.0093, Train Linf Norm: 0.7549, Test Linf Norm: 0.2282\n",
            "Epoch 57: Train Loss: 0.0081, Test Loss: 0.0082, Train L1 Norm: 0.0221, Test L1 Norm: 0.0097, Train Linf Norm: 0.8351, Test Linf Norm: 0.2436\n",
            "Epoch 58: Train Loss: 0.0080, Test Loss: 0.0072, Train L1 Norm: 0.0185, Test L1 Norm: 0.0091, Train Linf Norm: 0.6613, Test Linf Norm: 0.2253\n",
            "Epoch 59: Train Loss: 0.0079, Test Loss: 0.0077, Train L1 Norm: 0.0163, Test L1 Norm: 0.0092, Train Linf Norm: 0.5678, Test Linf Norm: 0.2233\n",
            "Epoch 60: Train Loss: 0.0079, Test Loss: 0.0079, Train L1 Norm: 0.0217, Test L1 Norm: 0.0097, Train Linf Norm: 0.7898, Test Linf Norm: 0.2544\n",
            "Epoch 61: Train Loss: 0.0078, Test Loss: 0.0072, Train L1 Norm: 0.0187, Test L1 Norm: 0.0093, Train Linf Norm: 0.6798, Test Linf Norm: 0.2332\n",
            "Epoch 62: Train Loss: 0.0079, Test Loss: 0.0079, Train L1 Norm: 0.0258, Test L1 Norm: 0.0095, Train Linf Norm: 1.0238, Test Linf Norm: 0.2315\n",
            "Epoch 63: Train Loss: 0.0076, Test Loss: 0.0071, Train L1 Norm: 0.0177, Test L1 Norm: 0.0098, Train Linf Norm: 0.6422, Test Linf Norm: 0.2529\n",
            "Epoch 64: Train Loss: 0.0078, Test Loss: 0.0077, Train L1 Norm: 0.0234, Test L1 Norm: 0.0105, Train Linf Norm: 0.9157, Test Linf Norm: 0.2748\n",
            "Epoch 65: Train Loss: 0.0078, Test Loss: 0.0070, Train L1 Norm: 0.0245, Test L1 Norm: 0.0125, Train Linf Norm: 0.9606, Test Linf Norm: 0.3579\n",
            "Epoch 66: Train Loss: 0.0076, Test Loss: 0.0097, Train L1 Norm: 0.0257, Test L1 Norm: 0.0095, Train Linf Norm: 1.0293, Test Linf Norm: 0.2224\n",
            "Epoch 67: Train Loss: 0.0076, Test Loss: 0.0076, Train L1 Norm: 0.0213, Test L1 Norm: 0.0104, Train Linf Norm: 0.8082, Test Linf Norm: 0.2760\n",
            "Epoch 68: Train Loss: 0.0076, Test Loss: 0.0094, Train L1 Norm: 0.0214, Test L1 Norm: 0.0103, Train Linf Norm: 0.8191, Test Linf Norm: 0.2540\n",
            "Epoch 69: Train Loss: 0.0078, Test Loss: 0.0071, Train L1 Norm: 0.0224, Test L1 Norm: 0.0089, Train Linf Norm: 0.8638, Test Linf Norm: 0.2238\n",
            "Epoch 70: Train Loss: 0.0076, Test Loss: 0.0091, Train L1 Norm: 0.0203, Test L1 Norm: 0.0092, Train Linf Norm: 0.7718, Test Linf Norm: 0.2209\n",
            "Epoch 71: Train Loss: 0.0075, Test Loss: 0.0085, Train L1 Norm: 0.0224, Test L1 Norm: 0.0107, Train Linf Norm: 0.8699, Test Linf Norm: 0.2729\n",
            "Epoch 72: Train Loss: 0.0075, Test Loss: 0.0119, Train L1 Norm: 0.0246, Test L1 Norm: 0.0102, Train Linf Norm: 0.9680, Test Linf Norm: 0.2341\n",
            "Epoch 73: Train Loss: 0.0077, Test Loss: 0.0070, Train L1 Norm: 0.0221, Test L1 Norm: 0.0087, Train Linf Norm: 0.8547, Test Linf Norm: 0.2185\n",
            "Epoch 74: Train Loss: 0.0076, Test Loss: 0.0068, Train L1 Norm: 0.0184, Test L1 Norm: 0.0091, Train Linf Norm: 0.6798, Test Linf Norm: 0.2350\n",
            "Epoch 75: Train Loss: 0.0074, Test Loss: 0.0069, Train L1 Norm: 0.0229, Test L1 Norm: 0.0097, Train Linf Norm: 0.8986, Test Linf Norm: 0.2522\n",
            "Epoch 76: Train Loss: 0.0076, Test Loss: 0.0078, Train L1 Norm: 0.0228, Test L1 Norm: 0.0094, Train Linf Norm: 0.8874, Test Linf Norm: 0.2372\n",
            "Epoch 77: Train Loss: 0.0075, Test Loss: 0.0082, Train L1 Norm: 0.0218, Test L1 Norm: 0.0113, Train Linf Norm: 0.8446, Test Linf Norm: 0.2952\n",
            "Epoch 78: Train Loss: 0.0076, Test Loss: 0.0072, Train L1 Norm: 0.0223, Test L1 Norm: 0.0124, Train Linf Norm: 0.8675, Test Linf Norm: 0.3566\n",
            "Epoch 79: Train Loss: 0.0073, Test Loss: 0.0069, Train L1 Norm: 0.0264, Test L1 Norm: 0.0090, Train Linf Norm: 1.0625, Test Linf Norm: 0.2333\n",
            "Epoch 80: Train Loss: 0.0076, Test Loss: 0.0091, Train L1 Norm: 0.0209, Test L1 Norm: 0.0106, Train Linf Norm: 0.7991, Test Linf Norm: 0.2741\n",
            "Epoch 81: Train Loss: 0.0073, Test Loss: 0.0087, Train L1 Norm: 0.0186, Test L1 Norm: 0.0098, Train Linf Norm: 0.6904, Test Linf Norm: 0.2473\n",
            "Epoch 82: Train Loss: 0.0075, Test Loss: 0.0086, Train L1 Norm: 0.0223, Test L1 Norm: 0.0100, Train Linf Norm: 0.8620, Test Linf Norm: 0.2504\n",
            "Epoch 83: Train Loss: 0.0074, Test Loss: 0.0086, Train L1 Norm: 0.0261, Test L1 Norm: 0.0111, Train Linf Norm: 1.0407, Test Linf Norm: 0.3109\n",
            "Epoch 84: Train Loss: 0.0063, Test Loss: 0.0073, Train L1 Norm: 0.0171, Test L1 Norm: 0.0086, Train Linf Norm: 0.6410, Test Linf Norm: 0.2150\n",
            "Epoch 85: Train Loss: 0.0064, Test Loss: 0.0065, Train L1 Norm: 0.0188, Test L1 Norm: 0.0083, Train Linf Norm: 0.7219, Test Linf Norm: 0.2085\n",
            "Epoch 86: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0202, Test L1 Norm: 0.0086, Train Linf Norm: 0.7836, Test Linf Norm: 0.2185\n",
            "Epoch 87: Train Loss: 0.0064, Test Loss: 0.0064, Train L1 Norm: 0.0162, Test L1 Norm: 0.0088, Train Linf Norm: 0.5945, Test Linf Norm: 0.2220\n",
            "Epoch 88: Train Loss: 0.0063, Test Loss: 0.0065, Train L1 Norm: 0.0184, Test L1 Norm: 0.0085, Train Linf Norm: 0.7067, Test Linf Norm: 0.2147\n",
            "Epoch 89: Train Loss: 0.0063, Test Loss: 0.0067, Train L1 Norm: 0.0177, Test L1 Norm: 0.0091, Train Linf Norm: 0.6686, Test Linf Norm: 0.2415\n",
            "Epoch 90: Train Loss: 0.0063, Test Loss: 0.0066, Train L1 Norm: 0.0189, Test L1 Norm: 0.0086, Train Linf Norm: 0.7277, Test Linf Norm: 0.2154\n",
            "Epoch 91: Train Loss: 0.0063, Test Loss: 0.0064, Train L1 Norm: 0.0185, Test L1 Norm: 0.0084, Train Linf Norm: 0.7074, Test Linf Norm: 0.2153\n",
            "Epoch 92: Train Loss: 0.0062, Test Loss: 0.0077, Train L1 Norm: 0.0196, Test L1 Norm: 0.0093, Train Linf Norm: 0.7630, Test Linf Norm: 0.2348\n",
            "Epoch 93: Train Loss: 0.0063, Test Loss: 0.0064, Train L1 Norm: 0.0172, Test L1 Norm: 0.0083, Train Linf Norm: 0.6452, Test Linf Norm: 0.2110\n",
            "Epoch 94: Train Loss: 0.0063, Test Loss: 0.0068, Train L1 Norm: 0.0176, Test L1 Norm: 0.0099, Train Linf Norm: 0.6704, Test Linf Norm: 0.2718\n",
            "Epoch 95: Train Loss: 0.0062, Test Loss: 0.0063, Train L1 Norm: 0.0181, Test L1 Norm: 0.0082, Train Linf Norm: 0.6917, Test Linf Norm: 0.2081\n",
            "Epoch 96: Train Loss: 0.0063, Test Loss: 0.0065, Train L1 Norm: 0.0204, Test L1 Norm: 0.0087, Train Linf Norm: 0.7993, Test Linf Norm: 0.2191\n",
            "Epoch 97: Train Loss: 0.0063, Test Loss: 0.0063, Train L1 Norm: 0.0197, Test L1 Norm: 0.0085, Train Linf Norm: 0.7705, Test Linf Norm: 0.2201\n",
            "Epoch 98: Train Loss: 0.0063, Test Loss: 0.0071, Train L1 Norm: 0.0177, Test L1 Norm: 0.0090, Train Linf Norm: 0.6715, Test Linf Norm: 0.2318\n",
            "Epoch 99: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0168, Test L1 Norm: 0.0082, Train Linf Norm: 0.6297, Test Linf Norm: 0.2054\n",
            "Epoch 100: Train Loss: 0.0063, Test Loss: 0.0078, Train L1 Norm: 0.0197, Test L1 Norm: 0.0089, Train Linf Norm: 0.7652, Test Linf Norm: 0.2168\n",
            "Epoch 101: Train Loss: 0.0062, Test Loss: 0.0075, Train L1 Norm: 0.0169, Test L1 Norm: 0.0089, Train Linf Norm: 0.6368, Test Linf Norm: 0.2196\n",
            "Epoch 102: Train Loss: 0.0063, Test Loss: 0.0065, Train L1 Norm: 0.0183, Test L1 Norm: 0.0093, Train Linf Norm: 0.7050, Test Linf Norm: 0.2479\n",
            "Epoch 103: Train Loss: 0.0063, Test Loss: 0.0076, Train L1 Norm: 0.0159, Test L1 Norm: 0.0090, Train Linf Norm: 0.5891, Test Linf Norm: 0.2205\n",
            "Epoch 104: Train Loss: 0.0062, Test Loss: 0.0073, Train L1 Norm: 0.0189, Test L1 Norm: 0.0087, Train Linf Norm: 0.7304, Test Linf Norm: 0.2164\n",
            "Epoch 105: Train Loss: 0.0058, Test Loss: 0.0063, Train L1 Norm: 0.0200, Test L1 Norm: 0.0083, Train Linf Norm: 0.7970, Test Linf Norm: 0.2085\n",
            "Epoch 106: Train Loss: 0.0058, Test Loss: 0.0064, Train L1 Norm: 0.0180, Test L1 Norm: 0.0081, Train Linf Norm: 0.6909, Test Linf Norm: 0.2039\n",
            "Epoch 107: Train Loss: 0.0058, Test Loss: 0.0062, Train L1 Norm: 0.0156, Test L1 Norm: 0.0079, Train Linf Norm: 0.5825, Test Linf Norm: 0.1981\n",
            "Epoch 108: Train Loss: 0.0057, Test Loss: 0.0062, Train L1 Norm: 0.0170, Test L1 Norm: 0.0082, Train Linf Norm: 0.6459, Test Linf Norm: 0.2123\n",
            "Epoch 109: Train Loss: 0.0058, Test Loss: 0.0069, Train L1 Norm: 0.0170, Test L1 Norm: 0.0085, Train Linf Norm: 0.6457, Test Linf Norm: 0.2113\n",
            "Epoch 110: Train Loss: 0.0058, Test Loss: 0.0062, Train L1 Norm: 0.0160, Test L1 Norm: 0.0082, Train Linf Norm: 0.6003, Test Linf Norm: 0.2063\n",
            "Epoch 111: Train Loss: 0.0058, Test Loss: 0.0061, Train L1 Norm: 0.0171, Test L1 Norm: 0.0082, Train Linf Norm: 0.6550, Test Linf Norm: 0.2078\n",
            "Epoch 112: Train Loss: 0.0057, Test Loss: 0.0070, Train L1 Norm: 0.0201, Test L1 Norm: 0.0084, Train Linf Norm: 0.8013, Test Linf Norm: 0.2108\n",
            "Epoch 113: Train Loss: 0.0057, Test Loss: 0.0067, Train L1 Norm: 0.0172, Test L1 Norm: 0.0082, Train Linf Norm: 0.6601, Test Linf Norm: 0.2041\n",
            "Epoch 114: Train Loss: 0.0058, Test Loss: 0.0063, Train L1 Norm: 0.0178, Test L1 Norm: 0.0080, Train Linf Norm: 0.6915, Test Linf Norm: 0.2026\n",
            "Epoch 115: Train Loss: 0.0058, Test Loss: 0.0061, Train L1 Norm: 0.0172, Test L1 Norm: 0.0080, Train Linf Norm: 0.6585, Test Linf Norm: 0.2003\n",
            "Epoch 116: Train Loss: 0.0058, Test Loss: 0.0064, Train L1 Norm: 0.0187, Test L1 Norm: 0.0082, Train Linf Norm: 0.7228, Test Linf Norm: 0.2103\n",
            "Epoch 117: Train Loss: 0.0058, Test Loss: 0.0062, Train L1 Norm: 0.0171, Test L1 Norm: 0.0081, Train Linf Norm: 0.6511, Test Linf Norm: 0.2014\n",
            "Epoch 118: Train Loss: 0.0057, Test Loss: 0.0064, Train L1 Norm: 0.0169, Test L1 Norm: 0.0084, Train Linf Norm: 0.6460, Test Linf Norm: 0.2166\n",
            "Epoch 119: Train Loss: 0.0058, Test Loss: 0.0061, Train L1 Norm: 0.0171, Test L1 Norm: 0.0079, Train Linf Norm: 0.6583, Test Linf Norm: 0.1983\n",
            "Epoch 120: Train Loss: 0.0057, Test Loss: 0.0061, Train L1 Norm: 0.0160, Test L1 Norm: 0.0081, Train Linf Norm: 0.6017, Test Linf Norm: 0.2007\n",
            "Epoch 121: Train Loss: 0.0057, Test Loss: 0.0061, Train L1 Norm: 0.0157, Test L1 Norm: 0.0081, Train Linf Norm: 0.5878, Test Linf Norm: 0.2064\n",
            "Epoch 122: Train Loss: 0.0058, Test Loss: 0.0061, Train L1 Norm: 0.0177, Test L1 Norm: 0.0081, Train Linf Norm: 0.6829, Test Linf Norm: 0.2042\n",
            "Epoch 123: Train Loss: 0.0057, Test Loss: 0.0062, Train L1 Norm: 0.0180, Test L1 Norm: 0.0085, Train Linf Norm: 0.7005, Test Linf Norm: 0.2210\n",
            "Epoch 124: Train Loss: 0.0057, Test Loss: 0.0063, Train L1 Norm: 0.0171, Test L1 Norm: 0.0088, Train Linf Norm: 0.6595, Test Linf Norm: 0.2300\n",
            "Epoch 125: Train Loss: 0.0057, Test Loss: 0.0062, Train L1 Norm: 0.0157, Test L1 Norm: 0.0083, Train Linf Norm: 0.5842, Test Linf Norm: 0.2138\n",
            "Epoch 126: Train Loss: 0.0057, Test Loss: 0.0061, Train L1 Norm: 0.0171, Test L1 Norm: 0.0080, Train Linf Norm: 0.6573, Test Linf Norm: 0.2017\n",
            "Epoch 127: Train Loss: 0.0057, Test Loss: 0.0070, Train L1 Norm: 0.0187, Test L1 Norm: 0.0082, Train Linf Norm: 0.7354, Test Linf Norm: 0.2011\n",
            "Epoch 128: Train Loss: 0.0057, Test Loss: 0.0064, Train L1 Norm: 0.0165, Test L1 Norm: 0.0083, Train Linf Norm: 0.6287, Test Linf Norm: 0.2039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 20:15:29,503]\u001b[0m Trial 23 finished with value: 0.00792771152332425 and parameters: {'n_layers': 3, 'n_units_0': 683, 'n_units_1': 1579, 'n_units_2': 1336, 'hidden_activation': 'ReLU', 'output_activation': 'Linear', 'loss': 'MAE', 'optimizer': 'Adam', 'lr': 0.00024544555703065517, 'batch_size': 48, 'n_epochs': 129, 'scheduler': 'ReduceLROnPlateau', 'weight_decay': 1.3512229370254397e-05, 'beta1': 0.9006924865586429, 'beta2': 0.9990017032673401, 'factor': 0.3786340154715322, 'patience': 8, 'threshold': 0.0006113863820725881}. Best is trial 21 with value: 0.007759499435313046.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 129: Train Loss: 0.0055, Test Loss: 0.0062, Train L1 Norm: 0.0166, Test L1 Norm: 0.0079, Train Linf Norm: 0.6390, Test Linf Norm: 0.1981\n",
            "Epoch 1: Train Loss: 0.1628, Test Loss: 0.1008, Train L1 Norm: 0.3786, Test L1 Norm: 0.0810, Train Linf Norm: 14.2262, Test Linf Norm: 1.8552\n",
            "Epoch 2: Train Loss: 0.0803, Test Loss: 0.0613, Train L1 Norm: 0.1752, Test L1 Norm: 0.0585, Train Linf Norm: 6.4887, Test Linf Norm: 1.5580\n",
            "Epoch 3: Train Loss: 0.0627, Test Loss: 0.0406, Train L1 Norm: 0.1598, Test L1 Norm: 0.0447, Train Linf Norm: 6.2305, Test Linf Norm: 1.1920\n",
            "Epoch 4: Train Loss: 0.0525, Test Loss: 0.0356, Train L1 Norm: 0.1330, Test L1 Norm: 0.0493, Train Linf Norm: 5.1691, Test Linf Norm: 1.3182\n",
            "Epoch 5: Train Loss: 0.0472, Test Loss: 0.0273, Train L1 Norm: 0.1370, Test L1 Norm: 0.0596, Train Linf Norm: 5.4707, Test Linf Norm: 1.9559\n",
            "Epoch 6: Train Loss: 0.0406, Test Loss: 0.0336, Train L1 Norm: 0.0996, Test L1 Norm: 0.0398, Train Linf Norm: 3.8027, Test Linf Norm: 1.1040\n",
            "Epoch 7: Train Loss: 0.0360, Test Loss: 0.0257, Train L1 Norm: 0.0659, Test L1 Norm: 0.0499, Train Linf Norm: 2.3040, Test Linf Norm: 1.4715\n",
            "Epoch 8: Train Loss: 0.0351, Test Loss: 0.0298, Train L1 Norm: 0.0749, Test L1 Norm: 0.0270, Train Linf Norm: 2.7483, Test Linf Norm: 0.6543\n",
            "Epoch 9: Train Loss: 0.0334, Test Loss: 0.0223, Train L1 Norm: 0.1072, Test L1 Norm: 0.0356, Train Linf Norm: 4.3572, Test Linf Norm: 1.0525\n",
            "Epoch 10: Train Loss: 0.0309, Test Loss: 0.0251, Train L1 Norm: 0.0964, Test L1 Norm: 0.0266, Train Linf Norm: 3.8897, Test Linf Norm: 0.7141\n",
            "Epoch 11: Train Loss: 0.0299, Test Loss: 0.0428, Train L1 Norm: 0.1211, Test L1 Norm: 0.0288, Train Linf Norm: 5.0684, Test Linf Norm: 0.6150\n",
            "Epoch 12: Train Loss: 0.0289, Test Loss: 0.0200, Train L1 Norm: 0.0865, Test L1 Norm: 0.0184, Train Linf Norm: 3.4591, Test Linf Norm: 0.4203\n",
            "Epoch 13: Train Loss: 0.0290, Test Loss: 0.0291, Train L1 Norm: 0.0535, Test L1 Norm: 0.0255, Train Linf Norm: 1.8730, Test Linf Norm: 0.6171\n",
            "Epoch 14: Train Loss: 0.0266, Test Loss: 0.0257, Train L1 Norm: 0.0885, Test L1 Norm: 0.0262, Train Linf Norm: 3.6121, Test Linf Norm: 0.6032\n",
            "Epoch 15: Train Loss: 0.0254, Test Loss: 0.0254, Train L1 Norm: 0.0800, Test L1 Norm: 0.0369, Train Linf Norm: 3.2001, Test Linf Norm: 1.0166\n",
            "Epoch 16: Train Loss: 0.0266, Test Loss: 0.0205, Train L1 Norm: 0.0943, Test L1 Norm: 0.0289, Train Linf Norm: 3.8778, Test Linf Norm: 0.8282\n",
            "Epoch 17: Train Loss: 0.0236, Test Loss: 0.0186, Train L1 Norm: 0.0849, Test L1 Norm: 0.0220, Train Linf Norm: 3.4861, Test Linf Norm: 0.5722\n",
            "Epoch 18: Train Loss: 0.0230, Test Loss: 0.0258, Train L1 Norm: 0.0606, Test L1 Norm: 0.0282, Train Linf Norm: 2.3317, Test Linf Norm: 0.7351\n",
            "Epoch 19: Train Loss: 0.0248, Test Loss: 0.0214, Train L1 Norm: 0.0542, Test L1 Norm: 0.0312, Train Linf Norm: 2.0070, Test Linf Norm: 0.9063\n",
            "Epoch 20: Train Loss: 0.0234, Test Loss: 0.0300, Train L1 Norm: 0.0714, Test L1 Norm: 0.0353, Train Linf Norm: 2.8549, Test Linf Norm: 0.9609\n",
            "Epoch 21: Train Loss: 0.0227, Test Loss: 0.0129, Train L1 Norm: 0.0602, Test L1 Norm: 0.0209, Train Linf Norm: 2.3417, Test Linf Norm: 0.6234\n",
            "Epoch 22: Train Loss: 0.0221, Test Loss: 0.0153, Train L1 Norm: 0.0528, Test L1 Norm: 0.0190, Train Linf Norm: 1.9882, Test Linf Norm: 0.4998\n",
            "Epoch 23: Train Loss: 0.0208, Test Loss: 0.0388, Train L1 Norm: 0.0599, Test L1 Norm: 0.0347, Train Linf Norm: 2.3456, Test Linf Norm: 0.7710\n",
            "Epoch 24: Train Loss: 0.0205, Test Loss: 0.0381, Train L1 Norm: 0.0486, Test L1 Norm: 0.0273, Train Linf Norm: 1.8295, Test Linf Norm: 0.5697\n",
            "Epoch 25: Train Loss: 0.0211, Test Loss: 0.0279, Train L1 Norm: 0.0575, Test L1 Norm: 0.0229, Train Linf Norm: 2.2545, Test Linf Norm: 0.4193\n",
            "Epoch 26: Train Loss: 0.0198, Test Loss: 0.0160, Train L1 Norm: 0.0507, Test L1 Norm: 0.0205, Train Linf Norm: 1.9439, Test Linf Norm: 0.5591\n",
            "Epoch 27: Train Loss: 0.0199, Test Loss: 0.0404, Train L1 Norm: 0.0474, Test L1 Norm: 0.0275, Train Linf Norm: 1.7737, Test Linf Norm: 0.5882\n",
            "Epoch 28: Train Loss: 0.0192, Test Loss: 0.0132, Train L1 Norm: 0.0439, Test L1 Norm: 0.0183, Train Linf Norm: 1.6249, Test Linf Norm: 0.5175\n",
            "Epoch 29: Train Loss: 0.0202, Test Loss: 0.0171, Train L1 Norm: 0.0487, Test L1 Norm: 0.0203, Train Linf Norm: 1.8385, Test Linf Norm: 0.5641\n",
            "Epoch 30: Train Loss: 0.0183, Test Loss: 0.0127, Train L1 Norm: 0.0521, Test L1 Norm: 0.0228, Train Linf Norm: 2.0296, Test Linf Norm: 0.6614\n",
            "Epoch 31: Train Loss: 0.0188, Test Loss: 0.0163, Train L1 Norm: 0.0362, Test L1 Norm: 0.0162, Train Linf Norm: 1.2452, Test Linf Norm: 0.3605\n",
            "Epoch 32: Train Loss: 0.0186, Test Loss: 0.0123, Train L1 Norm: 0.0394, Test L1 Norm: 0.0181, Train Linf Norm: 1.4195, Test Linf Norm: 0.5143\n",
            "Epoch 33: Train Loss: 0.0176, Test Loss: 0.0223, Train L1 Norm: 0.0546, Test L1 Norm: 0.0223, Train Linf Norm: 2.1706, Test Linf Norm: 0.6003\n",
            "Epoch 34: Train Loss: 0.0180, Test Loss: 0.0183, Train L1 Norm: 0.0437, Test L1 Norm: 0.0197, Train Linf Norm: 1.6517, Test Linf Norm: 0.5344\n",
            "Epoch 35: Train Loss: 0.0178, Test Loss: 0.0291, Train L1 Norm: 0.0494, Test L1 Norm: 0.0238, Train Linf Norm: 1.8979, Test Linf Norm: 0.5759\n",
            "Epoch 36: Train Loss: 0.0176, Test Loss: 0.0127, Train L1 Norm: 0.0581, Test L1 Norm: 0.0211, Train Linf Norm: 2.3308, Test Linf Norm: 0.6168\n",
            "Epoch 37: Train Loss: 0.0175, Test Loss: 0.0170, Train L1 Norm: 0.0511, Test L1 Norm: 0.0383, Train Linf Norm: 2.0233, Test Linf Norm: 1.2087\n",
            "Epoch 38: Train Loss: 0.0169, Test Loss: 0.0113, Train L1 Norm: 0.0618, Test L1 Norm: 0.0234, Train Linf Norm: 2.5414, Test Linf Norm: 0.7159\n",
            "Epoch 39: Train Loss: 0.0170, Test Loss: 0.0116, Train L1 Norm: 0.0533, Test L1 Norm: 0.0202, Train Linf Norm: 2.1357, Test Linf Norm: 0.6181\n",
            "Epoch 40: Train Loss: 0.0161, Test Loss: 0.0227, Train L1 Norm: 0.0460, Test L1 Norm: 0.0230, Train Linf Norm: 1.7891, Test Linf Norm: 0.6196\n",
            "Epoch 41: Train Loss: 0.0170, Test Loss: 0.0114, Train L1 Norm: 0.0457, Test L1 Norm: 0.0283, Train Linf Norm: 1.7747, Test Linf Norm: 0.9324\n",
            "Epoch 42: Train Loss: 0.0168, Test Loss: 0.0164, Train L1 Norm: 0.0718, Test L1 Norm: 0.0226, Train Linf Norm: 3.0222, Test Linf Norm: 0.6525\n",
            "Epoch 43: Train Loss: 0.0170, Test Loss: 0.0289, Train L1 Norm: 0.0583, Test L1 Norm: 0.0203, Train Linf Norm: 2.3783, Test Linf Norm: 0.3788\n",
            "Epoch 44: Train Loss: 0.0170, Test Loss: 0.0148, Train L1 Norm: 0.0487, Test L1 Norm: 0.0201, Train Linf Norm: 1.9150, Test Linf Norm: 0.5856\n",
            "Epoch 45: Train Loss: 0.0162, Test Loss: 0.0139, Train L1 Norm: 0.0425, Test L1 Norm: 0.0184, Train Linf Norm: 1.6116, Test Linf Norm: 0.5194\n",
            "Epoch 46: Train Loss: 0.0163, Test Loss: 0.0447, Train L1 Norm: 0.0481, Test L1 Norm: 0.0425, Train Linf Norm: 1.9064, Test Linf Norm: 1.0190\n",
            "Epoch 47: Train Loss: 0.0159, Test Loss: 0.0326, Train L1 Norm: 0.0501, Test L1 Norm: 0.0260, Train Linf Norm: 2.0067, Test Linf Norm: 0.6287\n",
            "Epoch 48: Train Loss: 0.0099, Test Loss: 0.0081, Train L1 Norm: 0.0482, Test L1 Norm: 0.0155, Train Linf Norm: 2.0350, Test Linf Norm: 0.4781\n",
            "Epoch 49: Train Loss: 0.0098, Test Loss: 0.0097, Train L1 Norm: 0.0314, Test L1 Norm: 0.0148, Train Linf Norm: 1.2457, Test Linf Norm: 0.4312\n",
            "Epoch 50: Train Loss: 0.0097, Test Loss: 0.0125, Train L1 Norm: 0.0350, Test L1 Norm: 0.0134, Train Linf Norm: 1.4124, Test Linf Norm: 0.3536\n",
            "Epoch 51: Train Loss: 0.0096, Test Loss: 0.0179, Train L1 Norm: 0.0276, Test L1 Norm: 0.0210, Train Linf Norm: 1.0656, Test Linf Norm: 0.6099\n",
            "Epoch 52: Train Loss: 0.0100, Test Loss: 0.0138, Train L1 Norm: 0.0359, Test L1 Norm: 0.0136, Train Linf Norm: 1.4550, Test Linf Norm: 0.3493\n",
            "Epoch 53: Train Loss: 0.0095, Test Loss: 0.0123, Train L1 Norm: 0.0302, Test L1 Norm: 0.0176, Train Linf Norm: 1.1816, Test Linf Norm: 0.5017\n",
            "Epoch 54: Train Loss: 0.0096, Test Loss: 0.0078, Train L1 Norm: 0.0249, Test L1 Norm: 0.0140, Train Linf Norm: 0.9314, Test Linf Norm: 0.4319\n",
            "Epoch 55: Train Loss: 0.0089, Test Loss: 0.0093, Train L1 Norm: 0.0386, Test L1 Norm: 0.0133, Train Linf Norm: 1.6059, Test Linf Norm: 0.3860\n",
            "Epoch 56: Train Loss: 0.0096, Test Loss: 0.0079, Train L1 Norm: 0.0320, Test L1 Norm: 0.0114, Train Linf Norm: 1.2662, Test Linf Norm: 0.3208\n",
            "Epoch 57: Train Loss: 0.0093, Test Loss: 0.0085, Train L1 Norm: 0.0346, Test L1 Norm: 0.0132, Train Linf Norm: 1.4058, Test Linf Norm: 0.3707\n",
            "Epoch 58: Train Loss: 0.0090, Test Loss: 0.0077, Train L1 Norm: 0.0316, Test L1 Norm: 0.0119, Train Linf Norm: 1.2682, Test Linf Norm: 0.3450\n",
            "Epoch 59: Train Loss: 0.0091, Test Loss: 0.0104, Train L1 Norm: 0.0235, Test L1 Norm: 0.0125, Train Linf Norm: 0.8760, Test Linf Norm: 0.3270\n",
            "Epoch 60: Train Loss: 0.0089, Test Loss: 0.0078, Train L1 Norm: 0.0304, Test L1 Norm: 0.0148, Train Linf Norm: 1.2048, Test Linf Norm: 0.4513\n",
            "Epoch 61: Train Loss: 0.0091, Test Loss: 0.0086, Train L1 Norm: 0.0470, Test L1 Norm: 0.0114, Train Linf Norm: 2.0070, Test Linf Norm: 0.3178\n",
            "Epoch 62: Train Loss: 0.0087, Test Loss: 0.0080, Train L1 Norm: 0.0253, Test L1 Norm: 0.0123, Train Linf Norm: 0.9658, Test Linf Norm: 0.3603\n",
            "Epoch 63: Train Loss: 0.0090, Test Loss: 0.0082, Train L1 Norm: 0.0354, Test L1 Norm: 0.0131, Train Linf Norm: 1.4405, Test Linf Norm: 0.3829\n",
            "Epoch 64: Train Loss: 0.0089, Test Loss: 0.0076, Train L1 Norm: 0.0385, Test L1 Norm: 0.0127, Train Linf Norm: 1.6040, Test Linf Norm: 0.3899\n",
            "Epoch 65: Train Loss: 0.0090, Test Loss: 0.0076, Train L1 Norm: 0.0366, Test L1 Norm: 0.0126, Train Linf Norm: 1.4988, Test Linf Norm: 0.3683\n",
            "Epoch 66: Train Loss: 0.0090, Test Loss: 0.0085, Train L1 Norm: 0.0227, Test L1 Norm: 0.0197, Train Linf Norm: 0.8465, Test Linf Norm: 0.6729\n",
            "Epoch 67: Train Loss: 0.0085, Test Loss: 0.0069, Train L1 Norm: 0.0276, Test L1 Norm: 0.0112, Train Linf Norm: 1.0929, Test Linf Norm: 0.3162\n",
            "Epoch 68: Train Loss: 0.0089, Test Loss: 0.0077, Train L1 Norm: 0.0276, Test L1 Norm: 0.0171, Train Linf Norm: 1.0838, Test Linf Norm: 0.5690\n",
            "Epoch 69: Train Loss: 0.0087, Test Loss: 0.0113, Train L1 Norm: 0.0354, Test L1 Norm: 0.0196, Train Linf Norm: 1.4584, Test Linf Norm: 0.5949\n",
            "Epoch 70: Train Loss: 0.0085, Test Loss: 0.0081, Train L1 Norm: 0.0262, Test L1 Norm: 0.0133, Train Linf Norm: 1.0204, Test Linf Norm: 0.4194\n",
            "Epoch 71: Train Loss: 0.0088, Test Loss: 0.0101, Train L1 Norm: 0.0270, Test L1 Norm: 0.0147, Train Linf Norm: 1.0591, Test Linf Norm: 0.4267\n",
            "Epoch 72: Train Loss: 0.0087, Test Loss: 0.0164, Train L1 Norm: 0.0225, Test L1 Norm: 0.0138, Train Linf Norm: 0.8300, Test Linf Norm: 0.3293\n",
            "Epoch 73: Train Loss: 0.0089, Test Loss: 0.0077, Train L1 Norm: 0.0303, Test L1 Norm: 0.0121, Train Linf Norm: 1.2104, Test Linf Norm: 0.3607\n",
            "Epoch 74: Train Loss: 0.0085, Test Loss: 0.0099, Train L1 Norm: 0.0290, Test L1 Norm: 0.0123, Train Linf Norm: 1.1582, Test Linf Norm: 0.3460\n",
            "Epoch 75: Train Loss: 0.0087, Test Loss: 0.0071, Train L1 Norm: 0.0317, Test L1 Norm: 0.0107, Train Linf Norm: 1.2865, Test Linf Norm: 0.3120\n",
            "Epoch 76: Train Loss: 0.0085, Test Loss: 0.0117, Train L1 Norm: 0.0294, Test L1 Norm: 0.0139, Train Linf Norm: 1.1731, Test Linf Norm: 0.3873\n",
            "Epoch 77: Train Loss: 0.0066, Test Loss: 0.0064, Train L1 Norm: 0.0215, Test L1 Norm: 0.0105, Train Linf Norm: 0.8401, Test Linf Norm: 0.3111\n",
            "Epoch 78: Train Loss: 0.0065, Test Loss: 0.0064, Train L1 Norm: 0.0251, Test L1 Norm: 0.0095, Train Linf Norm: 1.0128, Test Linf Norm: 0.2642\n",
            "Epoch 79: Train Loss: 0.0065, Test Loss: 0.0074, Train L1 Norm: 0.0210, Test L1 Norm: 0.0107, Train Linf Norm: 0.8078, Test Linf Norm: 0.3038\n",
            "Epoch 80: Train Loss: 0.0065, Test Loss: 0.0073, Train L1 Norm: 0.0212, Test L1 Norm: 0.0105, Train Linf Norm: 0.8297, Test Linf Norm: 0.3010\n",
            "Epoch 81: Train Loss: 0.0065, Test Loss: 0.0084, Train L1 Norm: 0.0174, Test L1 Norm: 0.0103, Train Linf Norm: 0.6422, Test Linf Norm: 0.2884\n",
            "Epoch 82: Train Loss: 0.0066, Test Loss: 0.0063, Train L1 Norm: 0.0262, Test L1 Norm: 0.0105, Train Linf Norm: 1.0677, Test Linf Norm: 0.3121\n",
            "Epoch 83: Train Loss: 0.0065, Test Loss: 0.0072, Train L1 Norm: 0.0216, Test L1 Norm: 0.0105, Train Linf Norm: 0.8496, Test Linf Norm: 0.3005\n",
            "Epoch 84: Train Loss: 0.0064, Test Loss: 0.0068, Train L1 Norm: 0.0192, Test L1 Norm: 0.0119, Train Linf Norm: 0.7345, Test Linf Norm: 0.3768\n",
            "Epoch 85: Train Loss: 0.0065, Test Loss: 0.0099, Train L1 Norm: 0.0268, Test L1 Norm: 0.0126, Train Linf Norm: 1.0917, Test Linf Norm: 0.3544\n",
            "Epoch 86: Train Loss: 0.0065, Test Loss: 0.0085, Train L1 Norm: 0.0234, Test L1 Norm: 0.0114, Train Linf Norm: 0.9270, Test Linf Norm: 0.3201\n",
            "Epoch 87: Train Loss: 0.0064, Test Loss: 0.0060, Train L1 Norm: 0.0238, Test L1 Norm: 0.0104, Train Linf Norm: 0.9578, Test Linf Norm: 0.3124\n",
            "Epoch 88: Train Loss: 0.0064, Test Loss: 0.0072, Train L1 Norm: 0.0177, Test L1 Norm: 0.0116, Train Linf Norm: 0.6614, Test Linf Norm: 0.3540\n",
            "Epoch 89: Train Loss: 0.0065, Test Loss: 0.0064, Train L1 Norm: 0.0186, Test L1 Norm: 0.0122, Train Linf Norm: 0.7059, Test Linf Norm: 0.3788\n",
            "Epoch 90: Train Loss: 0.0064, Test Loss: 0.0067, Train L1 Norm: 0.0223, Test L1 Norm: 0.0110, Train Linf Norm: 0.8822, Test Linf Norm: 0.3242\n",
            "Epoch 91: Train Loss: 0.0063, Test Loss: 0.0064, Train L1 Norm: 0.0239, Test L1 Norm: 0.0108, Train Linf Norm: 0.9644, Test Linf Norm: 0.3235\n",
            "Epoch 92: Train Loss: 0.0064, Test Loss: 0.0061, Train L1 Norm: 0.0266, Test L1 Norm: 0.0109, Train Linf Norm: 1.0939, Test Linf Norm: 0.3329\n",
            "Epoch 93: Train Loss: 0.0063, Test Loss: 0.0061, Train L1 Norm: 0.0204, Test L1 Norm: 0.0098, Train Linf Norm: 0.7903, Test Linf Norm: 0.2840\n",
            "Epoch 94: Train Loss: 0.0064, Test Loss: 0.0070, Train L1 Norm: 0.0226, Test L1 Norm: 0.0105, Train Linf Norm: 0.9024, Test Linf Norm: 0.3044\n",
            "Epoch 95: Train Loss: 0.0063, Test Loss: 0.0065, Train L1 Norm: 0.0190, Test L1 Norm: 0.0105, Train Linf Norm: 0.7217, Test Linf Norm: 0.3094\n",
            "Epoch 96: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0215, Test L1 Norm: 0.0099, Train Linf Norm: 0.8472, Test Linf Norm: 0.2876\n",
            "Epoch 97: Train Loss: 0.0056, Test Loss: 0.0062, Train L1 Norm: 0.0193, Test L1 Norm: 0.0095, Train Linf Norm: 0.7570, Test Linf Norm: 0.2703\n",
            "Epoch 98: Train Loss: 0.0056, Test Loss: 0.0062, Train L1 Norm: 0.0204, Test L1 Norm: 0.0099, Train Linf Norm: 0.8118, Test Linf Norm: 0.2855\n",
            "Epoch 99: Train Loss: 0.0056, Test Loss: 0.0070, Train L1 Norm: 0.0215, Test L1 Norm: 0.0097, Train Linf Norm: 0.8634, Test Linf Norm: 0.2747\n",
            "Epoch 100: Train Loss: 0.0055, Test Loss: 0.0058, Train L1 Norm: 0.0196, Test L1 Norm: 0.0095, Train Linf Norm: 0.7734, Test Linf Norm: 0.2788\n",
            "Epoch 101: Train Loss: 0.0056, Test Loss: 0.0061, Train L1 Norm: 0.0214, Test L1 Norm: 0.0096, Train Linf Norm: 0.8556, Test Linf Norm: 0.2802\n",
            "Epoch 102: Train Loss: 0.0055, Test Loss: 0.0060, Train L1 Norm: 0.0219, Test L1 Norm: 0.0105, Train Linf Norm: 0.8902, Test Linf Norm: 0.3169\n",
            "Epoch 103: Train Loss: 0.0055, Test Loss: 0.0057, Train L1 Norm: 0.0190, Test L1 Norm: 0.0105, Train Linf Norm: 0.7441, Test Linf Norm: 0.3252\n",
            "Epoch 104: Train Loss: 0.0056, Test Loss: 0.0061, Train L1 Norm: 0.0193, Test L1 Norm: 0.0095, Train Linf Norm: 0.7609, Test Linf Norm: 0.2760\n",
            "Epoch 105: Train Loss: 0.0055, Test Loss: 0.0067, Train L1 Norm: 0.0194, Test L1 Norm: 0.0108, Train Linf Norm: 0.7689, Test Linf Norm: 0.3296\n",
            "Epoch 106: Train Loss: 0.0056, Test Loss: 0.0065, Train L1 Norm: 0.0201, Test L1 Norm: 0.0105, Train Linf Norm: 0.7976, Test Linf Norm: 0.3159\n",
            "Epoch 107: Train Loss: 0.0055, Test Loss: 0.0059, Train L1 Norm: 0.0207, Test L1 Norm: 0.0104, Train Linf Norm: 0.8254, Test Linf Norm: 0.3152\n",
            "Epoch 108: Train Loss: 0.0055, Test Loss: 0.0061, Train L1 Norm: 0.0226, Test L1 Norm: 0.0093, Train Linf Norm: 0.9198, Test Linf Norm: 0.2676\n",
            "Epoch 109: Train Loss: 0.0055, Test Loss: 0.0057, Train L1 Norm: 0.0185, Test L1 Norm: 0.0096, Train Linf Norm: 0.7124, Test Linf Norm: 0.2811\n",
            "Epoch 110: Train Loss: 0.0055, Test Loss: 0.0057, Train L1 Norm: 0.0213, Test L1 Norm: 0.0094, Train Linf Norm: 0.8566, Test Linf Norm: 0.2733\n",
            "Epoch 111: Train Loss: 0.0055, Test Loss: 0.0080, Train L1 Norm: 0.0213, Test L1 Norm: 0.0103, Train Linf Norm: 0.8588, Test Linf Norm: 0.2813\n",
            "Epoch 112: Train Loss: 0.0055, Test Loss: 0.0058, Train L1 Norm: 0.0216, Test L1 Norm: 0.0094, Train Linf Norm: 0.8725, Test Linf Norm: 0.2701\n",
            "Epoch 113: Train Loss: 0.0052, Test Loss: 0.0056, Train L1 Norm: 0.0193, Test L1 Norm: 0.0098, Train Linf Norm: 0.7657, Test Linf Norm: 0.2961\n",
            "Epoch 114: Train Loss: 0.0052, Test Loss: 0.0060, Train L1 Norm: 0.0201, Test L1 Norm: 0.0097, Train Linf Norm: 0.8041, Test Linf Norm: 0.2862\n",
            "Epoch 115: Train Loss: 0.0052, Test Loss: 0.0057, Train L1 Norm: 0.0206, Test L1 Norm: 0.0091, Train Linf Norm: 0.8253, Test Linf Norm: 0.2650\n",
            "Epoch 116: Train Loss: 0.0052, Test Loss: 0.0057, Train L1 Norm: 0.0192, Test L1 Norm: 0.0095, Train Linf Norm: 0.7648, Test Linf Norm: 0.2826\n",
            "Epoch 117: Train Loss: 0.0052, Test Loss: 0.0057, Train L1 Norm: 0.0216, Test L1 Norm: 0.0088, Train Linf Norm: 0.8795, Test Linf Norm: 0.2512\n",
            "Epoch 118: Train Loss: 0.0051, Test Loss: 0.0058, Train L1 Norm: 0.0202, Test L1 Norm: 0.0090, Train Linf Norm: 0.8129, Test Linf Norm: 0.2578\n",
            "Epoch 119: Train Loss: 0.0052, Test Loss: 0.0056, Train L1 Norm: 0.0197, Test L1 Norm: 0.0094, Train Linf Norm: 0.7873, Test Linf Norm: 0.2731\n",
            "Epoch 120: Train Loss: 0.0052, Test Loss: 0.0056, Train L1 Norm: 0.0208, Test L1 Norm: 0.0092, Train Linf Norm: 0.8421, Test Linf Norm: 0.2635\n",
            "Epoch 121: Train Loss: 0.0052, Test Loss: 0.0056, Train L1 Norm: 0.0188, Test L1 Norm: 0.0097, Train Linf Norm: 0.7431, Test Linf Norm: 0.2874\n",
            "Epoch 122: Train Loss: 0.0052, Test Loss: 0.0056, Train L1 Norm: 0.0228, Test L1 Norm: 0.0093, Train Linf Norm: 0.9371, Test Linf Norm: 0.2700\n",
            "Epoch 123: Train Loss: 0.0051, Test Loss: 0.0056, Train L1 Norm: 0.0207, Test L1 Norm: 0.0099, Train Linf Norm: 0.8317, Test Linf Norm: 0.2992\n",
            "Epoch 124: Train Loss: 0.0052, Test Loss: 0.0058, Train L1 Norm: 0.0200, Test L1 Norm: 0.0093, Train Linf Norm: 0.8021, Test Linf Norm: 0.2682\n",
            "Epoch 125: Train Loss: 0.0051, Test Loss: 0.0063, Train L1 Norm: 0.0202, Test L1 Norm: 0.0100, Train Linf Norm: 0.8111, Test Linf Norm: 0.2949\n",
            "Epoch 126: Train Loss: 0.0051, Test Loss: 0.0056, Train L1 Norm: 0.0186, Test L1 Norm: 0.0090, Train Linf Norm: 0.7361, Test Linf Norm: 0.2565\n",
            "Epoch 127: Train Loss: 0.0051, Test Loss: 0.0057, Train L1 Norm: 0.0196, Test L1 Norm: 0.0095, Train Linf Norm: 0.7856, Test Linf Norm: 0.2805\n",
            "Epoch 128: Train Loss: 0.0052, Test Loss: 0.0056, Train L1 Norm: 0.0176, Test L1 Norm: 0.0088, Train Linf Norm: 0.6899, Test Linf Norm: 0.2496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 20:25:35,310]\u001b[0m Trial 24 finished with value: 0.00938896683640778 and parameters: {'n_layers': 2, 'n_units_0': 689, 'n_units_1': 1678, 'hidden_activation': 'ReLU', 'output_activation': 'Linear', 'loss': 'MAE', 'optimizer': 'Adam', 'lr': 0.00025944936602206387, 'batch_size': 48, 'n_epochs': 129, 'scheduler': 'ReduceLROnPlateau', 'weight_decay': 1.054743332829589e-05, 'beta1': 0.9043999282259548, 'beta2': 0.9990060344043487, 'factor': 0.3733874036369685, 'patience': 8, 'threshold': 0.000434728492290967}. Best is trial 21 with value: 0.007759499435313046.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 129: Train Loss: 0.0051, Test Loss: 0.0057, Train L1 Norm: 0.0206, Test L1 Norm: 0.0094, Train Linf Norm: 0.8304, Test Linf Norm: 0.2722\n",
            "Epoch 1: Train Loss: 0.1522, Test Loss: 0.0933, Train L1 Norm: 0.2311, Test L1 Norm: 0.0545, Train Linf Norm: 7.6953, Test Linf Norm: 0.8926\n",
            "Epoch 2: Train Loss: 0.0790, Test Loss: 0.0568, Train L1 Norm: 0.1266, Test L1 Norm: 0.0438, Train Linf Norm: 4.4374, Test Linf Norm: 0.8913\n",
            "Epoch 3: Train Loss: 0.0656, Test Loss: 0.0784, Train L1 Norm: 0.1614, Test L1 Norm: 0.0739, Train Linf Norm: 6.3617, Test Linf Norm: 1.8349\n",
            "Epoch 4: Train Loss: 0.0527, Test Loss: 0.0376, Train L1 Norm: 0.1349, Test L1 Norm: 0.0347, Train Linf Norm: 5.2718, Test Linf Norm: 0.7346\n",
            "Epoch 5: Train Loss: 0.0498, Test Loss: 0.0426, Train L1 Norm: 0.1441, Test L1 Norm: 0.0620, Train Linf Norm: 5.8329, Test Linf Norm: 1.7250\n",
            "Epoch 6: Train Loss: 0.0465, Test Loss: 0.0298, Train L1 Norm: 0.1014, Test L1 Norm: 0.0442, Train Linf Norm: 3.8224, Test Linf Norm: 1.3935\n",
            "Epoch 7: Train Loss: 0.0412, Test Loss: 0.0865, Train L1 Norm: 0.0926, Test L1 Norm: 0.0409, Train Linf Norm: 3.5228, Test Linf Norm: 0.6290\n",
            "Epoch 8: Train Loss: 0.0386, Test Loss: 0.0469, Train L1 Norm: 0.1045, Test L1 Norm: 0.0410, Train Linf Norm: 4.1305, Test Linf Norm: 0.9606\n",
            "Epoch 9: Train Loss: 0.0369, Test Loss: 0.0547, Train L1 Norm: 0.0923, Test L1 Norm: 0.0367, Train Linf Norm: 3.5513, Test Linf Norm: 0.7997\n",
            "Epoch 10: Train Loss: 0.0372, Test Loss: 0.0474, Train L1 Norm: 0.0797, Test L1 Norm: 0.0406, Train Linf Norm: 2.9439, Test Linf Norm: 1.0792\n",
            "Epoch 11: Train Loss: 0.0339, Test Loss: 0.0381, Train L1 Norm: 0.1065, Test L1 Norm: 0.0397, Train Linf Norm: 4.3418, Test Linf Norm: 1.0491\n",
            "Epoch 12: Train Loss: 0.0316, Test Loss: 0.0292, Train L1 Norm: 0.1282, Test L1 Norm: 0.0262, Train Linf Norm: 5.4115, Test Linf Norm: 0.6361\n",
            "Epoch 13: Train Loss: 0.0331, Test Loss: 0.0234, Train L1 Norm: 0.0854, Test L1 Norm: 0.0258, Train Linf Norm: 3.3363, Test Linf Norm: 0.6949\n",
            "Epoch 14: Train Loss: 0.0308, Test Loss: 0.0268, Train L1 Norm: 0.0904, Test L1 Norm: 0.0466, Train Linf Norm: 3.6285, Test Linf Norm: 1.5345\n",
            "Epoch 15: Train Loss: 0.0298, Test Loss: 0.0207, Train L1 Norm: 0.1006, Test L1 Norm: 0.0252, Train Linf Norm: 4.1209, Test Linf Norm: 0.7006\n",
            "Epoch 16: Train Loss: 0.0307, Test Loss: 0.0266, Train L1 Norm: 0.0933, Test L1 Norm: 0.0242, Train Linf Norm: 3.7756, Test Linf Norm: 0.5757\n",
            "Epoch 17: Train Loss: 0.0284, Test Loss: 0.0337, Train L1 Norm: 0.0792, Test L1 Norm: 0.0290, Train Linf Norm: 3.1171, Test Linf Norm: 0.7248\n",
            "Epoch 18: Train Loss: 0.0287, Test Loss: 0.0182, Train L1 Norm: 0.1008, Test L1 Norm: 0.0217, Train Linf Norm: 4.1754, Test Linf Norm: 0.6016\n",
            "Epoch 19: Train Loss: 0.0292, Test Loss: 0.0273, Train L1 Norm: 0.0737, Test L1 Norm: 0.0235, Train Linf Norm: 2.8601, Test Linf Norm: 0.5368\n",
            "Epoch 20: Train Loss: 0.0255, Test Loss: 0.0264, Train L1 Norm: 0.0903, Test L1 Norm: 0.0250, Train Linf Norm: 3.6968, Test Linf Norm: 0.5212\n",
            "Epoch 21: Train Loss: 0.0283, Test Loss: 0.0197, Train L1 Norm: 0.1054, Test L1 Norm: 0.0293, Train Linf Norm: 4.4079, Test Linf Norm: 0.9366\n",
            "Epoch 22: Train Loss: 0.0281, Test Loss: 0.0308, Train L1 Norm: 0.0966, Test L1 Norm: 0.0230, Train Linf Norm: 3.9745, Test Linf Norm: 0.5471\n",
            "Epoch 23: Train Loss: 0.0253, Test Loss: 0.0197, Train L1 Norm: 0.0951, Test L1 Norm: 0.0362, Train Linf Norm: 3.9467, Test Linf Norm: 1.1800\n",
            "Epoch 24: Train Loss: 0.0267, Test Loss: 0.0473, Train L1 Norm: 0.0844, Test L1 Norm: 0.0262, Train Linf Norm: 3.4309, Test Linf Norm: 0.4489\n",
            "Epoch 25: Train Loss: 0.0257, Test Loss: 0.0227, Train L1 Norm: 0.0808, Test L1 Norm: 0.0198, Train Linf Norm: 3.2718, Test Linf Norm: 0.4329\n",
            "Epoch 26: Train Loss: 0.0250, Test Loss: 0.0170, Train L1 Norm: 0.0960, Test L1 Norm: 0.0173, Train Linf Norm: 4.0101, Test Linf Norm: 0.3776\n",
            "Epoch 27: Train Loss: 0.0240, Test Loss: 0.0204, Train L1 Norm: 0.0664, Test L1 Norm: 0.0204, Train Linf Norm: 2.5997, Test Linf Norm: 0.4307\n",
            "Epoch 28: Train Loss: 0.0233, Test Loss: 0.0349, Train L1 Norm: 0.0686, Test L1 Norm: 0.0304, Train Linf Norm: 2.7297, Test Linf Norm: 0.5598\n",
            "Epoch 29: Train Loss: 0.0239, Test Loss: 0.0186, Train L1 Norm: 0.0429, Test L1 Norm: 0.0184, Train Linf Norm: 1.4942, Test Linf Norm: 0.4208\n",
            "Epoch 30: Train Loss: 0.0232, Test Loss: 0.0199, Train L1 Norm: 0.0690, Test L1 Norm: 0.0256, Train Linf Norm: 2.7313, Test Linf Norm: 0.6982\n",
            "Epoch 31: Train Loss: 0.0253, Test Loss: 0.0352, Train L1 Norm: 0.0586, Test L1 Norm: 0.0253, Train Linf Norm: 2.2221, Test Linf Norm: 0.5067\n",
            "Epoch 32: Train Loss: 0.0236, Test Loss: 0.0166, Train L1 Norm: 0.0820, Test L1 Norm: 0.0260, Train Linf Norm: 3.3518, Test Linf Norm: 0.7987\n",
            "Epoch 33: Train Loss: 0.0214, Test Loss: 0.0231, Train L1 Norm: 0.0819, Test L1 Norm: 0.0224, Train Linf Norm: 3.4194, Test Linf Norm: 0.5254\n",
            "Epoch 34: Train Loss: 0.0223, Test Loss: 0.0220, Train L1 Norm: 0.0468, Test L1 Norm: 0.0264, Train Linf Norm: 1.7129, Test Linf Norm: 0.6940\n",
            "Epoch 35: Train Loss: 0.0230, Test Loss: 0.0178, Train L1 Norm: 0.0609, Test L1 Norm: 0.0243, Train Linf Norm: 2.3690, Test Linf Norm: 0.7316\n",
            "Epoch 36: Train Loss: 0.0213, Test Loss: 0.0173, Train L1 Norm: 0.0484, Test L1 Norm: 0.0294, Train Linf Norm: 1.8081, Test Linf Norm: 0.8788\n",
            "Epoch 37: Train Loss: 0.0230, Test Loss: 0.0207, Train L1 Norm: 0.0465, Test L1 Norm: 0.0215, Train Linf Norm: 1.6884, Test Linf Norm: 0.4800\n",
            "Epoch 38: Train Loss: 0.0212, Test Loss: 0.0389, Train L1 Norm: 0.0507, Test L1 Norm: 0.0321, Train Linf Norm: 1.9222, Test Linf Norm: 0.8220\n",
            "Epoch 39: Train Loss: 0.0225, Test Loss: 0.0268, Train L1 Norm: 0.0683, Test L1 Norm: 0.0340, Train Linf Norm: 2.7496, Test Linf Norm: 0.9466\n",
            "Epoch 40: Train Loss: 0.0213, Test Loss: 0.0156, Train L1 Norm: 0.0543, Test L1 Norm: 0.0263, Train Linf Norm: 2.0992, Test Linf Norm: 0.7771\n",
            "Epoch 41: Train Loss: 0.0209, Test Loss: 0.0348, Train L1 Norm: 0.0668, Test L1 Norm: 0.0245, Train Linf Norm: 2.7013, Test Linf Norm: 0.5992\n",
            "Epoch 42: Train Loss: 0.0217, Test Loss: 0.0287, Train L1 Norm: 0.0811, Test L1 Norm: 0.0245, Train Linf Norm: 3.3158, Test Linf Norm: 0.6263\n",
            "Epoch 43: Train Loss: 0.0202, Test Loss: 0.0253, Train L1 Norm: 0.0550, Test L1 Norm: 0.0271, Train Linf Norm: 2.1500, Test Linf Norm: 0.7648\n",
            "Epoch 44: Train Loss: 0.0207, Test Loss: 0.0213, Train L1 Norm: 0.0470, Test L1 Norm: 0.0199, Train Linf Norm: 1.7489, Test Linf Norm: 0.5080\n",
            "Epoch 45: Train Loss: 0.0202, Test Loss: 0.0171, Train L1 Norm: 0.0681, Test L1 Norm: 0.0246, Train Linf Norm: 2.7728, Test Linf Norm: 0.6839\n",
            "Epoch 46: Train Loss: 0.0209, Test Loss: 0.0268, Train L1 Norm: 0.0441, Test L1 Norm: 0.0283, Train Linf Norm: 1.5997, Test Linf Norm: 0.6855\n",
            "Epoch 47: Train Loss: 0.0200, Test Loss: 0.0137, Train L1 Norm: 0.0667, Test L1 Norm: 0.0226, Train Linf Norm: 2.6926, Test Linf Norm: 0.6852\n",
            "Epoch 48: Train Loss: 0.0213, Test Loss: 0.0170, Train L1 Norm: 0.0741, Test L1 Norm: 0.0216, Train Linf Norm: 2.9985, Test Linf Norm: 0.4616\n",
            "Epoch 49: Train Loss: 0.0192, Test Loss: 0.0510, Train L1 Norm: 0.0545, Test L1 Norm: 0.0255, Train Linf Norm: 2.1580, Test Linf Norm: 0.4362\n",
            "Epoch 50: Train Loss: 0.0201, Test Loss: 0.0158, Train L1 Norm: 0.0523, Test L1 Norm: 0.0241, Train Linf Norm: 2.0195, Test Linf Norm: 0.7569\n",
            "Epoch 51: Train Loss: 0.0197, Test Loss: 0.0150, Train L1 Norm: 0.0478, Test L1 Norm: 0.0195, Train Linf Norm: 1.8175, Test Linf Norm: 0.5674\n",
            "Epoch 52: Train Loss: 0.0187, Test Loss: 0.0132, Train L1 Norm: 0.0421, Test L1 Norm: 0.0201, Train Linf Norm: 1.5330, Test Linf Norm: 0.5748\n",
            "Epoch 53: Train Loss: 0.0187, Test Loss: 0.0100, Train L1 Norm: 0.0620, Test L1 Norm: 0.0148, Train Linf Norm: 2.5185, Test Linf Norm: 0.4140\n",
            "Epoch 54: Train Loss: 0.0190, Test Loss: 0.0259, Train L1 Norm: 0.0529, Test L1 Norm: 0.0212, Train Linf Norm: 2.0776, Test Linf Norm: 0.4713\n",
            "Epoch 55: Train Loss: 0.0189, Test Loss: 0.0179, Train L1 Norm: 0.0784, Test L1 Norm: 0.0177, Train Linf Norm: 3.2996, Test Linf Norm: 0.5039\n",
            "Epoch 56: Train Loss: 0.0189, Test Loss: 0.0321, Train L1 Norm: 0.0396, Test L1 Norm: 0.0232, Train Linf Norm: 1.4457, Test Linf Norm: 0.5245\n",
            "Epoch 57: Train Loss: 0.0190, Test Loss: 0.0174, Train L1 Norm: 0.0665, Test L1 Norm: 0.0207, Train Linf Norm: 2.7307, Test Linf Norm: 0.5851\n",
            "Epoch 58: Train Loss: 0.0194, Test Loss: 0.0290, Train L1 Norm: 0.0586, Test L1 Norm: 0.0270, Train Linf Norm: 2.3542, Test Linf Norm: 0.7389\n",
            "Epoch 59: Train Loss: 0.0180, Test Loss: 0.0121, Train L1 Norm: 0.0598, Test L1 Norm: 0.0335, Train Linf Norm: 2.4366, Test Linf Norm: 1.1304\n",
            "Epoch 60: Train Loss: 0.0186, Test Loss: 0.0173, Train L1 Norm: 0.0569, Test L1 Norm: 0.0164, Train Linf Norm: 2.2719, Test Linf Norm: 0.4070\n",
            "Epoch 61: Train Loss: 0.0182, Test Loss: 0.0248, Train L1 Norm: 0.0696, Test L1 Norm: 0.0229, Train Linf Norm: 2.8905, Test Linf Norm: 0.6267\n"
          ]
        }
      ],
      "source": [
        "if OPTIMIZE:\n",
        "    # Creating a study object with Optuna with TPE sampler and median pruner \n",
        "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner())\n",
        "\n",
        "    # Running Optuna with 100 trials when we are optimizing.\n",
        "    study.optimize(objective, n_trials=N_TRIALS)\n",
        "\n",
        "    # Printing the best trial information\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "    print(\"  Value: \", trial.value)\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(f\"    {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmMfE9_dUZiS"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bsiL32xxg8IV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phyiHlWEUZiT"
      },
      "outputs": [],
      "source": [
        "# Creating the best network and optimizer using the best hyperparameters\n",
        "if OPTIMIZE:\n",
        "    net, \\\n",
        "    loss_fn, \\\n",
        "    optimizer, \\\n",
        "    batch_size, \\\n",
        "    n_epochs, \\\n",
        "    scheduler, \\\n",
        "    loss_name, \\\n",
        "    optimizer_name, \\\n",
        "    scheduler_name, \\\n",
        "    n_units, \\\n",
        "    n_layers, \\\n",
        "    hidden_activation, \\\n",
        "    output_activation, \\\n",
        "    lr = create_model(trial, optimize=True)\n",
        "# Creating the network with predefined hyperparameters\n",
        "else:\n",
        "    net, \\\n",
        "    loss_fn, \\\n",
        "    optimizer, \\\n",
        "    batch_size, \\\n",
        "    n_epochs, \\\n",
        "    scheduler, \\\n",
        "    loss_name, \\\n",
        "    optimizer_name, \\\n",
        "    scheduler_name, \\\n",
        "    n_units, \\\n",
        "    n_layers, \\\n",
        "    hidden_activation, \\\n",
        "    output_activation, \\\n",
        "    lr = create_model(trial=None, optimize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yq-oY81UZiU"
      },
      "outputs": [],
      "source": [
        "print(\"loss_fn:\", loss_fn)\n",
        "print(\"batch_size:\", batch_size)\n",
        "print(\"n_epochs:\", n_epochs)\n",
        "print(\"scheduler:\", scheduler)\n",
        "print(\"loss_name:\", loss_name)\n",
        "print(\"optimizer_name:\", optimizer_name)\n",
        "print(\"scheduler_name:\", scheduler_name)\n",
        "print(\"n_units:\", n_units)\n",
        "print(\"n_layers:\", n_layers)\n",
        "print(\"hidden_activation:\", hidden_activation)\n",
        "print(\"output_activation:\", output_activation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7aLWdZyUZiW"
      },
      "outputs": [],
      "source": [
        "# Training and evaluating the network using the train_and_eval function\n",
        "train_losses, test_losses, train_metrics, test_metrics = train_and_eval(\n",
        "    net, loss_fn, optimizer, batch_size, n_epochs, scheduler\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akNucrgMUZiW"
      },
      "source": [
        "## Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHsrs2Y-UZic"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# save the network to a .pth file\n",
        "torch.save(net.state_dict(), \"net.pth\")\n",
        "save_file(\"net.pth\")\n",
        "\n",
        "# save the optimizer to a .pth file\n",
        "torch.save(optimizer.state_dict(), \"optimizer.pth\")\n",
        "save_file(\"optimizer.pth\")\n",
        "\n",
        "# save the scheduler to a .pth file if it is not None\n",
        "if scheduler is not None:\n",
        "  torch.save(scheduler.state_dict(), \"scheduler.pth\")\n",
        "  save_file(\"scheduler.pth\")\n",
        "\n",
        "# create a dictionary to store the rest of the variables\n",
        "var_dict = {\n",
        "  \"batch_size\": batch_size,\n",
        "  \"n_epochs\": n_epochs,\n",
        "  \"loss_name\": loss_name,\n",
        "  \"optimizer_name\": optimizer_name,\n",
        "  \"scheduler_name\": scheduler_name,\n",
        "  \"n_units\": n_units,\n",
        "  \"n_layers\": n_layers,\n",
        "  \"hidden_activation_name\": hidden_activation.__class__.__name__,\n",
        "  \"output_activation_name\": output_activation.__class__.__name__,\n",
        "  \"lr\": lr,\n",
        "}\n",
        "\n",
        "# save the dictionary to a .json file\n",
        "with open(\"var_dict.json\", \"w\") as f:\n",
        "  json.dump(var_dict, f)\n",
        "save_file(\"var_dict.json\")\n",
        "\n",
        "# Saving the output of the training using pandas\n",
        "train_df = pd.DataFrame(\n",
        "    {\n",
        "        \"train_loss\": train_losses,\n",
        "        \"test_loss\": test_losses,\n",
        "        \"train_l1_norm\": [m[\"l1_norm\"] for m in train_metrics],\n",
        "        \"test_l1_norm\": [m[\"l1_norm\"] for m in test_metrics],\n",
        "        \"train_linf_norm\": [m[\"linf_norm\"] for m in train_metrics],\n",
        "        \"test_linf_norm\": [m[\"linf_norm\"] for m in test_metrics],\n",
        "    }\n",
        ")\n",
        "train_df.to_csv(\"train_output.csv\", index=False)\n",
        "save_file(\"train_output.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU23l7dIUZie"
      },
      "source": [
        "## Visualizing the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cippWZS6UZie"
      },
      "outputs": [],
      "source": [
        "# Plotting the losses and metrics for the best network \n",
        "plt.figure(figsize=(12, 8))\n",
        "#plt.subplot(2, 2, 1)\n",
        "#plt.plot(train_losses, label=\"Train Loss\")\n",
        "#plt.plot(test_losses, label=\"Test Loss\")\n",
        "#plt.xlabel(\"Epoch\")\n",
        "#plt.ylabel(\"Loss\")\n",
        "#plt.legend()\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot([m[\"l1_norm\"] for m in train_metrics], label=\"Train L1 Norm\")\n",
        "plt.plot([m[\"l1_norm\"] for m in test_metrics], label=\"Test L1 Norm\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"L1 Norm\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-3, 1e2)\n",
        "plt.legend()\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot([m[\"linf_norm\"] for m in train_metrics], label=\"Train Linf Norm\")\n",
        "plt.plot([m[\"linf_norm\"] for m in test_metrics], label=\"Test Linf Norm\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Linf Norm\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-3, 1e2)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Added plotting MSE of training data and MSE of test data in one plot \n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(train_losses,label=\"training data\")\n",
        "plt.plot(test_losses,label=\"test data\")\n",
        "#if scheduler is not None:\n",
        "#    plt.plot([scheduler.get_last_lr()[0] for _ in range(n_epochs)], label=\"Learning rate\") \n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-7, 1e0)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiEDutxIUZig"
      },
      "source": [
        "## Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7Mj990wUZih"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# load the dictionary from the .json file\n",
        "with open(\"var_dict.json\", \"r\") as f:\n",
        "  var_dict_loaded = json.load(f)\n",
        "\n",
        "# extract the variables from the dictionary\n",
        "batch_size_loaded = var_dict_loaded[\"batch_size\"]\n",
        "n_epochs_loaded = var_dict_loaded[\"n_epochs\"]\n",
        "loss_name_loaded = var_dict_loaded[\"loss_name\"]\n",
        "optimizer_name_loaded = var_dict_loaded[\"optimizer_name\"]\n",
        "scheduler_name_loaded = var_dict_loaded[\"scheduler_name\"]\n",
        "n_units_loaded = var_dict_loaded[\"n_units\"]\n",
        "n_layers_loaded = var_dict_loaded[\"n_layers\"]\n",
        "hidden_activation_name_loaded = var_dict_loaded[\"hidden_activation_name\"]\n",
        "output_activation_name_loaded = var_dict_loaded[\"output_activation_name\"]\n",
        "lr_loaded = var_dict_loaded[\"lr\"]\n",
        "\n",
        "# create the activation functions from their names\n",
        "if hidden_activation_name_loaded == \"ReLU\":\n",
        "  hidden_activation_loaded = nn.ReLU()\n",
        "elif hidden_activation_name_loaded == \"LeakyReLU\":\n",
        "  hidden_activation_loaded = nn.LeakyReLU() \n",
        "elif hidden_activation_name_loaded == \"ELU\":\n",
        "  hidden_activation_loaded = nn.ELU() \n",
        "elif hidden_activation_name_loaded == \"Tanh\":\n",
        "  hidden_activation_loaded = nn.Tanh()\n",
        "else:\n",
        "  hidden_activation_loaded = nn.Sigmoid()\n",
        "\n",
        "if output_activation_name_loaded == \"ReLU\":\n",
        "    output_activation_loaded = nn.ReLU()\n",
        "elif output_activation_name_loaded == \"Softplus\":\n",
        "    output_activation_loaded = nn.Softplus()\n",
        "else:\n",
        "    output_activation_loaded = nn.Identity()\n",
        "\n",
        "\n",
        "\n",
        "# load the network from the .pth file\n",
        "net_loaded = Net(n_layers_loaded, n_units_loaded, hidden_activation_loaded, output_activation_loaded).to(device)\n",
        "if torch.cuda.is_available():\n",
        " net_loaded.load_state_dict(torch.load(\"net.pth\"))\n",
        "else: \n",
        "  net_loaded.load_state_dict(torch.load(\"net.pth\", map_location=torch.device('cpu')))\n",
        "\n",
        "# create the loss function from its name\n",
        "if loss_name_loaded == \"MSE\":\n",
        "  loss_fn_loaded = nn.MSELoss()\n",
        "elif loss_name_loaded == \"MAE\":\n",
        "  loss_fn_loaded = nn.L1Loss()\n",
        "elif loss_name_loaded == \"Huber\":\n",
        "  loss_fn_loaded = nn.SmoothL1Loss() \n",
        "else:\n",
        "  # create the log-cosh loss function\n",
        "  def log_cosh_loss_loaded(y_pred, y_true):\n",
        "    return torch.mean(torch.log(torch.cosh(y_pred - y_true)))\n",
        "  loss_fn_loaded = log_cosh_loss_loaded\n",
        "\n",
        "# load the optimizer from the .pth file\n",
        "if torch.cuda.is_available():\n",
        "  optimizer_loaded_state_dict = torch.load(\"optimizer.pth\")\n",
        "else:\n",
        "  optimizer_loaded_state_dict = torch.load(\"optimizer.pth\", map_location=torch.device('cpu'))\n",
        "\n",
        "if optimizer_name_loaded == \"SGD\":\n",
        "  # Added getting the weight decay and momentum parameters from the state dict\n",
        "  weight_decay_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"weight_decay\"]\n",
        "  momentum_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"momentum\"]\n",
        "  optimizer_loaded = optim.SGD(net_loaded.parameters(), lr=lr_loaded, weight_decay=weight_decay_loaded, momentum=momentum_loaded)\n",
        "elif optimizer_name_loaded == \"Adam\":\n",
        "  # Added getting the weight decay and beta parameters from the state dict\n",
        "  weight_decay_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"weight_decay\"]\n",
        "  beta1_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"betas\"][0]\n",
        "  beta2_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"betas\"][1]\n",
        "  optimizer_loaded = optim.Adam(net_loaded.parameters(), lr=lr_loaded, weight_decay=weight_decay_loaded, betas=(beta1_loaded, beta2_loaded))\n",
        "elif optimizer_name_loaded == \"RMSprop\":\n",
        "  optimizer_loaded = optim.RMSprop(net_loaded.parameters(), lr=lr_loaded)\n",
        "else:\n",
        "  # Added loading the Adagrad optimizer\n",
        "  optimizer_loaded = optim.Adagrad(net_loaded.parameters(), lr=lr_loaded)\n",
        "optimizer_loaded.load_state_dict(optimizer_loaded_state_dict)\n",
        "\n",
        "# load the scheduler from the .pth file\n",
        "if torch.cuda.is_available():\n",
        "  scheduler_loaded_state_dict = torch.load(\"scheduler.pth\")\n",
        "else: \n",
        "  scheduler_loaded_state_dict = torch.load(\"scheduler.pth\", map_location=torch.device('cpu'))\n",
        "\n",
        "if scheduler_name_loaded == \"StepLR\":\n",
        "  # Added getting the step_size and gamma parameters from the state dict\n",
        "  step_size_loaded = scheduler_loaded_state_dict[\"step_size\"]\n",
        "  gamma_loaded = scheduler_loaded_state_dict[\"gamma\"]\n",
        "  scheduler_loaded = optim.lr_scheduler.StepLR(optimizer_loaded, step_size=step_size_loaded, gamma=gamma_loaded)\n",
        "elif scheduler_name_loaded == \"ExponentialLR\":\n",
        "  # Added getting the gamma parameter from the state dict\n",
        "  gamma_loaded = scheduler_loaded_state_dict[\"gamma\"]\n",
        "  scheduler_loaded = optim.lr_scheduler.ExponentialLR(optimizer_loaded, gamma=gamma_loaded)\n",
        "elif scheduler_name_loaded == \"CosineAnnealingLR\":\n",
        "  # Added getting the T_max parameter from the state dict\n",
        "  T_max_loaded = scheduler_loaded_state_dict[\"T_max\"]\n",
        "  scheduler_loaded = optim.lr_scheduler.CosineAnnealingLR(optimizer_loaded, T_max=T_max_loaded)\n",
        "elif scheduler_name_loaded == \"ReduceLROnPlateau\":\n",
        "  # Added getting the mode, factor, patience, threshold and min_lr parameters from the state dict\n",
        "  mode_loaded = scheduler_loaded_state_dict[\"mode\"]\n",
        "  factor_loaded = scheduler_loaded_state_dict[\"factor\"]\n",
        "  patience_loaded = scheduler_loaded_state_dict[\"patience\"]\n",
        "  threshold_loaded = scheduler_loaded_state_dict[\"threshold\"]\n",
        "  min_lr_loaded = scheduler_loaded_state_dict[\"min_lrs\"][0]\n",
        "  scheduler_loaded = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                    optimizer_loaded, mode=mode_loaded, factor=factor_loaded, patience=patience_loaded, threshold=threshold_loaded, min_lr=min_lr_loaded\n",
        "                )\n",
        "# elif scheduler_name_loaded == \"OneCycleLR\":\n",
        "#   max_lr_loaded = scheduler_loaded_state_dict[\"max_lr\"]\n",
        "#   epochs_loaded = scheduler_loaded_state_dict[\"epochs\"]\n",
        "#   steps_per_epoch_loaded = scheduler_loaded_state_dict[\"steps_per_epoch\"]\n",
        "#   pct_start_loaded = scheduler_loaded_state_dict[\"pct_start\"]\n",
        "#   max_lr_loaded = scheduler_loaded_state_dict[\"max_lr\"]\n",
        "#   scheduler_loaded = optim.lr_scheduler.OneCycleLR(\n",
        "#                     optimizer_loaded, max_lr=max_lr_loaded, epochs=epochs_loaded, steps_per_epoch=steps_per_epoch_loaded, pct_start=pct_start_loaded\n",
        "#                 )\n",
        "else:\n",
        "  scheduler_loaded = None\n",
        "\n",
        "if scheduler_loaded is not None:\n",
        "  # Added loading the state dict to the scheduler_loaded\n",
        "  scheduler_loaded.load_state_dict(scheduler_loaded_state_dict)\n",
        "\n",
        "# Loading the output of the training using pandas\n",
        "train_df_loaded = pd.read_csv(\"train_output.csv\")\n",
        "train_losses_loaded = train_df_loaded[\"train_loss\"].tolist()\n",
        "test_losses_loaded = train_df_loaded[\"test_loss\"].tolist()\n",
        "train_metrics_loaded = [\n",
        "    {\n",
        "        \"l1_norm\": train_df_loaded[\"train_l1_norm\"][i],\n",
        "        \"linf_norm\": train_df_loaded[\"train_linf_norm\"][i],\n",
        "    }\n",
        "    for i in range(len(train_df_loaded))\n",
        "]\n",
        "test_metrics_loaded = [\n",
        "    {\n",
        "        \"l1_norm\": train_df_loaded[\"test_l1_norm\"][i],\n",
        "        \"linf_norm\": train_df_loaded[\"test_linf_norm\"][i],\n",
        "    }\n",
        "    for i in range(len(train_df_loaded))\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ_fcj7zUZii"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "batch_size_loaded\n",
        "n_epochs_loaded\n",
        "loss_name_loaded\n",
        "optimizer_name_loaded\n",
        "scheduler_name_loaded\n",
        "n_units_loaded\n",
        "n_layers_loaded\n",
        "hidden_activation_name_loaded\n",
        "output_activation_name_loaded\n",
        "lr_loaded\n",
        "hidden_activation_loaded\n",
        "output_activation_loaded\n",
        "net_loaded\n",
        "net_loaded.__dict__ # print the subparameters of the network\n",
        "loss_fn_loaded\n",
        "optimizer_loaded\n",
        "optimizer_loaded.__dict__ # print the subparameters of the optimizer\n",
        "scheduler_loaded\n",
        "scheduler_loaded.__dict__ # print the subparameters of the scheduler\n",
        "train_losses_loaded\n",
        "test_losses_loaded\n",
        "train_metrics_loaded\n",
        "test_metrics_loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0B0SHa5SvExY"
      },
      "outputs": [],
      "source": [
        "train_losses_loaded[-1]\n",
        "test_losses_loaded[-1]\n",
        "test_metrics_loaded[-1]['l1_norm']\n",
        "test_metrics_loaded[-1]['linf_norm']\n",
        "print(f'Error is {test_metrics_loaded[-1][\"l1_norm\"] / (3.84e-4)} times bigger than in Dieselhorst et al.')\n",
        "print(f'Error is {test_metrics_loaded[-1][\"linf_norm\"] / (8.14e-3)} times bigger than in Dieselhorst et al.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj2XBdtmvExY"
      },
      "source": [
        "### Visualize loaded results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwLGR1aSUZik"
      },
      "source": [
        "Let us verify correct loading of the train and test metrics by visualizing them again but now through the loaded values. Likewise for the train and test losses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXiNgLsmUZil"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"last_expr_or_assign\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgq4WfSiUZil"
      },
      "outputs": [],
      "source": [
        "# Plotting the losses and metrics for the best network plt.figure(figsize=(12, \n",
        "#plt.subplot(2, 2, 1)\n",
        "#plt.plot(train_losses_loaded, label=\"Train Loss\")\n",
        "#plt.plot(test_losses_loaded, label=\"Test Loss\")\n",
        "#plt.xlabel(\"Epoch\")\n",
        "#plt.ylabel(\"Loss\")\n",
        "#plt.legend()\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot([m[\"l1_norm\"] for m in train_metrics_loaded], label=\"Train L1 Norm\")\n",
        "plt.plot([m[\"l1_norm\"] for m in test_metrics_loaded], label=\"Test L1 Norm\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"L1 Norm\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-3, 1e2)\n",
        "plt.legend()\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot([m[\"linf_norm\"] for m in train_metrics_loaded], label=\"Train Linf Norm\")\n",
        "plt.plot([m[\"linf_norm\"] for m in test_metrics_loaded], label=\"Test Linf Norm\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Linf Norm\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-3, 1e2)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Added plotting MSE of training data and MSE of test data in one plot \n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(train_losses_loaded,label=\"training data\")\n",
        "plt.plot(test_losses_loaded,label=\"test data\")\n",
        "#if scheduler is not None:\n",
        "#    plt.plot([scheduler.get_last_lr()[0] for _ in range(n_epochs)], label=\"Learning rate\") \n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-7, 1e0)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkgLqJ_UUZim"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EZQMUK8vExY"
      },
      "source": [
        "## Counting the number of parameters in the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWToFZmnvExZ"
      },
      "outputs": [],
      "source": [
        "net_loaded.eval()\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {count_parameters(net_loaded)} parameters.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxuzVSnlUZin"
      },
      "source": [
        "## Evaluating the network on arbirary input\n",
        "### Comparing `net` and `net_loaded`\n",
        "\n",
        "We compare `net` and `net_loaded` to confirm correct loading of the network. Note that `net` is only available if we have trained the model in this session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0PLAA0DUZin"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "print(list(net.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NZ8iVA7UZio"
      },
      "outputs": [],
      "source": [
        "print(list(net_loaded.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXLYbm8uUZio"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "# Set the network to evaluation mode\n",
        "net.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2kU7JqnLfaJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InGW0Xq6UZip"
      },
      "outputs": [],
      "source": [
        "rho_example, vx_example, vy_example, vz_example, epsilon_example = sample_primitive_variables(20)\n",
        "\n",
        "# Create arbitrary input\n",
        "inputs =  generate_input_data(rho_example, vx_example, vy_example, vz_example, epsilon_example)\n",
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jjrcp7qmfFPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVa1upmFUZip"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "# Pass the inputs to the network and get the outputs\n",
        "outputs = [net(input) for input in inputs]\n",
        "# Print the outputs\n",
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih9p2bosUZiq"
      },
      "outputs": [],
      "source": [
        "# Set the network to evaluation mode\n",
        "net_loaded.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-Xjfo7VUZir"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Pass the inputs to the network and get the outputs\n",
        "outputs = [net_loaded(input) for input in inputs]\n",
        "# Print the outputs\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjpIvdybUZis"
      },
      "source": [
        "## Porting the model to C++"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMlEd4RoUZis"
      },
      "outputs": [],
      "source": [
        "import torch.jit\n",
        "\n",
        "# Creating a dummy input tensor of shape (1, 5) to trace the model\n",
        "dummy_input = torch.randn(1, 5).to(device)\n",
        "dummy_input\n",
        "\n",
        "# Ensure that net_loaded is in evaluation mode.\n",
        "net_loaded.eval()\n",
        "\n",
        "# Tracing the model using the torch.jit.trace function\n",
        "traced_model = torch.jit.trace(net_loaded, dummy_input)\n",
        "\n",
        "# Saving the traced model to a file named \"net.pt\"\n",
        "traced_model.save(\"net.pt\")\n",
        "save_file(\"net.pt\")\n",
        "\n",
        "example_input_to_validate_correct_export_and_import = generate_input_data(*sample_primitive_variables(1))\n",
        "example_input_to_validate_correct_export_and_import\n",
        "net_loaded(example_input_to_validate_correct_export_and_import)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "bsc",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
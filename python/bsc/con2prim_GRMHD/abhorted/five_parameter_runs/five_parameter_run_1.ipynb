{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPvB1xoSUZhR"
      },
      "source": [
        "# Neural network to learn conservative-to-primitive conversion in relativistic hydrodynamics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eKD484DvExD"
      },
      "source": [
        "## How to use this notebook\n",
        "\n",
        "### Local installation\n",
        "\n",
        "1. Install required packages with `pip install -r requirements.txt` to your desired environment.\n",
        "2. If a script version of this notebook is desired, comment (not uncomment) the first line of `nbconvert` cell.\n",
        "\n",
        "### Colab installation\n",
        "\n",
        "1.  Comment (not uncomment) the first line of the drive mounting cell.\n",
        "2.  Comment (not uncomment) the first line of the `pip install` cell.\n",
        "\n",
        "<!-- - For colab we also want to set the runtime to GPU by clicking _Change runtime_ in the _Runtime_ menu, and -->\n",
        "<!-- - We want to wait for the google drive connection popup to appear and follow the instructions. -->\n",
        "\n",
        "### Training without optimization\n",
        "\n",
        "3. Set `OPTIMIZE = False` in section _Constants and flags to set_.\n",
        "4. Run the entire notebook.\n",
        "\n",
        "### Training with optimization\n",
        "\n",
        "3. Set `OPTIMIZE = True` in section _Constants and flags to set_.\n",
        "4. Run the entire notebook.\n",
        "\n",
        "### Loading an already trained model\n",
        "\n",
        "3. Run cells in section _Initialization_.\n",
        "4. Run cells with definitions in section _Generating the data_.\n",
        "5. Run cell with the definition of _Net_ in section _Defining the neural network_.\n",
        "6. Make sure the `net.pth`, `optimizer.pth`, `scheduler.pth`, `var_dict.json` and `train_output.csv` files are in the directory containing this notebook.\n",
        "7. Run the cells in section _Loading_ and continue from there.\n",
        "\n",
        "### Generating the C++ model\n",
        "\n",
        "8. Run section _Porting the model to C++_, this requires a model to be loaded.\n",
        "9. Set the path to the `net.pt` file in the C++ source file.\n",
        "10. `mkdir build && cd build`,\n",
        "11. `cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch/ ..`,\n",
        "10. Compile and run, e.g. `cmake --build . --config release && ./executable`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYPfIIglvExD"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsv90vHWvExE"
      },
      "source": [
        "\n",
        "Use this first cell to **convert this notebook** to a python script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqdgdNLHUZhV",
        "outputId": "d65f787b-3a98-4358-a33f-4e9f1923317b",
        "tags": [
          "remove_cell"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "!jupyter nbconvert five_parameter_run_1.ipynb --TagRemovePreprocessor.enabled=True --TagRemovePreprocessor.remove_cell_tags='{\"remove_cell\"}' --to script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzcUr0LnUZhw"
      },
      "source": [
        "Next some cells for working on **google colab**,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Jm_7_2r1vExG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# check if the drive is mounted\n",
        "drive_mounted = os.path.exists(\"/content/drive\")\n",
        "# change this to your desired folder\n",
        "drive_folder = \"/content/drive/My Drive/bsc/con2prim_towards_GRMHD/five_parameter_run_1\"\n",
        "\n",
        "# define a function to save a file to the drive or the current directory\n",
        "def save_file(file_name):\n",
        "  if drive_mounted:\n",
        "    # save the file to the drive folder\n",
        "    file_path = os.path.join(drive_folder, file_name)\n",
        "    # copy the file from the current directory to the drive folder\n",
        "    shutil.copyfile(file_name, file_path)\n",
        "  else:\n",
        "    # do nothing as the file is already in the current directory\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecHw2_xlUZhx",
        "outputId": "912cb92a-6613-4d75-fba6-d95efb9e1f0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#%%script echo skipping\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1rcStMLUZhy",
        "outputId": "9a49c8cd-1ad1-4aad-e9e5-5b0226c8bc06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.12.2)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.11.1)\n",
            "Requirement already satisfied: cmaes>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from optuna) (0.9.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.4.3)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.40.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "#%%script echo skipping\n",
        "\n",
        "!pip install optuna tensorboard tensorboardX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDsq9gG1vExH"
      },
      "source": [
        "Importing the **libraries** and setting the **device**,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "tREdWQUVUZhz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import optuna\n",
        "import tensorboardX as tbx\n",
        "\n",
        "# Checking if GPU is available and setting the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38GvmerjUZhz"
      },
      "source": [
        "### Constants and flags to set\n",
        "Defining some constants and parameters for convenience.\n",
        "\n",
        "**NOTE**: Some **subparameters** still need to be adjusted in the `create_model` function itself as of (Tue May 16 07:42:45 AM CEST 2023)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "ei6VZDYKUZh0"
      },
      "outputs": [],
      "source": [
        "\n",
        "N_TRIALS = 150 # Number of trials for hyperparameter optimization\n",
        "OPTIMIZE = True # Whether to optimize the hyperparameters or to use predetermined values from Dieseldorst et al..\n",
        "ZSCORE_NORMALIZATION = False # Whether to z-score normalize the input data.\n",
        "\n",
        "# I try out here the values as obtained in Optuna run 5, but I will increase the number of epochs.\n",
        "# N_LAYERS_NO_OPT = 3\n",
        "# N_UNITS_NO_OPT = [78, 193, 99]\n",
        "# HIDDEN_ACTIVATION_NAME_NO_OPT = \"ReLU\"\n",
        "# OUTPUT_ACTIVATION_NAME_NO_OPT = \"Linear\"\n",
        "# LOSS_NAME_NO_OPT = \"MSE\"\n",
        "# OPTIMIZER_NAME_NO_OPT = \"Adam\"\n",
        "# LR_NO_OPT = 0.00036516467819506355\n",
        "# BATCH_SIZE_NO_OPT = 170\n",
        "# N_EPOCHS_NO_OPT = 400\n",
        "# SCHEDULER_NAME_NO_OPT = \"ReduceLROnPlateau\"\n",
        "\n",
        "N_LAYERS_NO_OPT = 3\n",
        "N_UNITS_NO_OPT = [555, 458, 115]\n",
        "HIDDEN_ACTIVATION_NAME_NO_OPT = \"ReLU\"\n",
        "OUTPUT_ACTIVATION_NAME_NO_OPT = \"ReLU\"\n",
        "LOSS_NAME_NO_OPT = \"Huber\"\n",
        "OPTIMIZER_NAME_NO_OPT = \"RMSprop\"\n",
        "LR_NO_OPT = 0.000122770896701404\n",
        "BATCH_SIZE_NO_OPT = 49\n",
        "N_EPOCHS_NO_OPT = 400\n",
        "SCHEDULER_NAME_NO_OPT = \"ReduceLROnPlateau\"\n",
        "\n",
        "c = 1  # Speed of light (used in compute_conserved_variables and sample_primitive_variables functions)\n",
        "gamma = 5 / 3  # Adiabatic index (used in eos_analytic function)\n",
        "n_train_samples = 80000 # Number of training samples (used in generate_input_data and generate_labels functions)\n",
        "n_test_samples = 10000 # Number of test samples (used in generate_input_data and generate_labels functions)\n",
        "rho_interval = (0, 10.1) # Sampling interval for rest-mass density (used in sample_primitive_variables function)\n",
        "vx_interval = (0, .57 * c) # Sampling interval for velocity in x-direction (used in sample_primitive_variables function)\n",
        "vy_interval = (0, .57 * c) # Sampling interval for velocity in y-direction (used in sample_primitive_variables function)\n",
        "vz_interval = (0, .57 * c) # Sampling interval for velocity in z-direction (used in sample_primitive_variables function)\n",
        "epsilon_interval = (0, 2.02) # Sampling interval for specific internal energy (used in sample_primitive_variables function)\n",
        "\n",
        "np.random.seed(42) # Uncomment for pseudorandom data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlaP5UL2UZh1"
      },
      "source": [
        "## Generating the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "s_EvGFZcUZh1"
      },
      "outputs": [],
      "source": [
        "# Defining an analytic equation of state (EOS) for an ideal gas\n",
        "def eos_analytic(rho, epsilon):\n",
        "    \"\"\"Computes the pressure from rest-mass density and specific internal energy using an analytic EOS.\n",
        "\n",
        "    Args:\n",
        "        rho (torch.Tensor): The rest-mass density tensor of shape (n_samples,).\n",
        "        epsilon (torch.Tensor): The specific internal energy tensor of shape (n_samples,).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The pressure tensor of shape (n_samples,).\n",
        "    \"\"\"\n",
        "    # Adding some assertions to check that the input tensors are valid and have \n",
        "    # the expected shape and type \n",
        "    assert isinstance(rho, torch.Tensor), \"rho must be a torch.Tensor\"\n",
        "    assert isinstance(epsilon, torch.Tensor), \"epsilon must be a torch.Tensor\"\n",
        "    assert rho.shape == epsilon.shape, \"rho and epsilon must have the same shape\"\n",
        "    assert rho.ndim == 1, \"rho and epsilon must be one-dimensional tensors\"\n",
        "    assert rho.dtype == torch.float32, \"rho and epsilon must have dtype torch.float32\"\n",
        "\n",
        "    return (gamma - 1) * rho * epsilon\n",
        "\n",
        "\n",
        "\n",
        "# Defining a function that samples primitive variables from uniform distributions\n",
        "def sample_primitive_variables(n_samples):\n",
        "    \"\"\"Samples primitive variables from uniform distributions.\n",
        "\n",
        "    Args:\n",
        "        n_samples (int): The number of samples to generate.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple of (rho, vx, vy, vz, epsilon), where rho is rest-mass density,\n",
        "            vx is velocity in x-direction,\n",
        "            vy is velocity in y-direction,\n",
        "            vz is velocity in z-direction,\n",
        "            epsilon is specific internal energy,\n",
        "            each being a numpy array of shape (n_samples,).\n",
        "    \"\"\"\n",
        "    # Sampling from uniform distributions with intervals matching Dieseldorst \n",
        "    # et al.\n",
        "    rho = np.random.uniform(*rho_interval, size=n_samples)  # Rest-mass density\n",
        "    vx = np.random.uniform(*vx_interval, size=n_samples)  # Velocity in x-direction\n",
        "    vy = np.random.uniform(*vy_interval, size=n_samples)  # Velocity in y-direction\n",
        "    vz = np.random.uniform(*vz_interval, size=n_samples)  # Velocity in z-direction \n",
        "    epsilon = np.random.uniform(*epsilon_interval, size=n_samples)  # Specific internal energy\n",
        "\n",
        "    # Returning the primitive variables\n",
        "    return rho, vx, vy, vz, epsilon\n",
        "\n",
        "\n",
        "\n",
        "# Defining a function that computes conserved variables from primitive variables\n",
        "def compute_conserved_variables(rho, vx, vy, vz, epsilon):\n",
        "    \"\"\"Computes conserved variables from primitive variables.\n",
        "\n",
        "    Args:\n",
        "        rho (torch.Tensor): The rest-mass density tensor of shape (n_samples,).\n",
        "        vx (torch.Tensor): The velocity in x-direction tensor of shape (n_samples,)\n",
        "        vy (torch.Tensor): The velocity in y-direction tensor of shape (n_samples,)\n",
        "        vz (torch.Tensor): The velocity in z-direction tensor of shape (n_samples,)\n",
        "        epsilon (torch.Tensor): The specific internal energy tensor of shape (n_samples,).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple of (D, Sx, Sy, Sz, tau), where D is conserved density,\n",
        "            Sx is conserved momentum in x-direction,\n",
        "            Sy is conserved momentum in y-direction,\n",
        "            Sz is conserved momentum in z-direction,\n",
        "            tau is conserved energy density,\n",
        "            each being a torch tensor of shape (n_samples,).\n",
        "    \"\"\"\n",
        "\n",
        "  # Computing the pressure from the primitive variables using the EOS\n",
        "    p = eos_analytic(rho, epsilon)\n",
        "    # Computing the Lorentz factor from the velocity.\n",
        "    v2 = vx ** 2 + vy ** 2 + vz ** 2\n",
        "    W = 1 / torch.sqrt(1 - v2 / c ** 2)\n",
        "    # Specific enthalpy\n",
        "    h = 1 + epsilon + p / rho  \n",
        "\n",
        "    # Computing the conserved variables from the primitive variables\n",
        "    D = rho * W  # Conserved density\n",
        "    Sx = rho * h * W ** 2 * vx  # Conserved momentum in x-direction\n",
        "    Sy = rho * h * W ** 2 * vy  # Conserved momentum in y-direction\n",
        "    Sz = rho * h * W ** 2 * vz  # Conserved momentum in z-direction\n",
        "    tau = rho * h * W ** 2 - p - D  # Conserved energy density\n",
        "\n",
        "    # Returning the conserved variables\n",
        "    return D, Sx, Sy, Sz, tau\n",
        "\n",
        "# Defining a function that generates input data (conserved variables) from given samples of primitive variables\n",
        "def generate_input_data(rho, vx, vy, vz, epsilon):\n",
        "    # Converting the numpy arrays to torch tensors and moving them to the device\n",
        "    rho = torch.tensor(rho, dtype=torch.float32).to(device)\n",
        "    vx = torch.tensor(vx, dtype=torch.float32).to(device)\n",
        "    vy = torch.tensor(vy, dtype=torch.float32).to(device)\n",
        "    vz = torch.tensor(vz, dtype=torch.float32).to(device)\n",
        "    epsilon = torch.tensor(epsilon, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Computing the conserved variables using the compute_conserved_variables function\n",
        "    D, Sx, Sy, Sz, tau = compute_conserved_variables(rho, vx, vy, vz, epsilon) \n",
        "\n",
        "    # Stacking the conserved variables into a torch tensor\n",
        "    x = torch.stack([D, Sx, Sy, Sz, tau], axis=1)\n",
        "\n",
        "    # Returning the input data tensor\n",
        "    return x\n",
        "\n",
        "# Defining a function that generates output data (labels) from given samples of primitive variables\n",
        "def generate_labels(rho, epsilon):\n",
        "    # Converting the numpy arrays to torch tensors and moving them to the device\n",
        "    rho = torch.tensor(rho, dtype=torch.float32).to(device)\n",
        "    epsilon = torch.tensor(epsilon, dtype=torch.float32).to(device)\n",
        "   \n",
        "    # Computing the pressure from the primitive variables using the EOS\n",
        "    p = eos_analytic(rho, epsilon)\n",
        "\n",
        "    # Returning the output data tensor\n",
        "    return p\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dczop2rUZh3"
      },
      "source": [
        "Sampling the primitive variables using the sample_primitive_variables function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "cKubR6C8UZh4"
      },
      "outputs": [],
      "source": [
        "rho_train, vx_train, vy_train, vz_train ,epsilon_train = sample_primitive_variables(n_train_samples)\n",
        "rho_test, vx_test ,vy_test ,vz_test ,epsilon_test = sample_primitive_variables(n_test_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "zhbYH5HkvExJ",
        "outputId": "c7432122-c2fa-41c6-d69d-ea0fd97e029f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3.7828552 , 9.60221449, 7.39313881, ..., 3.85518754, 1.30928573,\n",
              "       9.5675983 ])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.46635374, 0.08280376, 0.53948436, ..., 0.16854057, 0.45173399,\n",
              "       0.26113605])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.53015873, 0.34538885, 0.19004683, ..., 0.0427437 , 0.34608123,\n",
              "       0.18076349])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.04282752, 0.19716017, 0.37988219, ..., 0.47604939, 0.2403435 ,\n",
              "       0.02051602])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.3414766 , 0.74092354, 1.3575724 , ..., 1.16268917, 1.80454638,\n",
              "       1.01489375])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([8.97842592, 3.18870559, 0.52428795, ..., 7.55288969, 8.90198997,\n",
              "       6.30910266])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.15696855, 0.0231761 , 0.5565851 , ..., 0.55641507, 0.0694273 ,\n",
              "       0.55457227])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.2806517 , 0.00769195, 0.47276771, ..., 0.16427298, 0.22592983,\n",
              "       0.47809263])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00997133, 0.10847447, 0.4806396 , ..., 0.501241  , 0.11057264,\n",
              "       0.43016179])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.21308179, 1.24785629, 1.86907234, ..., 1.65279226, 0.65476648,\n",
              "       0.60812168])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "rho_train\n",
        "vx_train\n",
        "vy_train\n",
        "vz_train \n",
        "epsilon_train\n",
        "rho_test\n",
        "vx_test \n",
        "vy_test \n",
        "vz_test \n",
        "epsilon_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "VMp6XJ6RUZh4"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"last_expr_or_assign\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "E5YFdqKjUZh5",
        "outputId": "a74dc216-dc1c-4325-b949-c5bbe82d5e4d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x400 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plotting the histograms of rho, vx and epsilon\n",
        "plt.figure(figsize=(16, 4))\n",
        "plt.subplot(1, 5, 1)\n",
        "plt.hist(rho_train, bins=20)\n",
        "plt.xlabel(\"rho\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "#plt.yscale(\"log\")\n",
        "plt.subplot(1, 5, 2)\n",
        "plt.hist(vx_train, bins=20)\n",
        "plt.xlabel(\"vx\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "#plt.yscale(\"log\")\n",
        "plt.subplot(1, 5, 3)\n",
        "plt.hist(vy_train, bins=20)\n",
        "plt.xlabel(\"vy\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "#plt.yscale(\"log\")\n",
        "plt.subplot(1, 5, 4)\n",
        "plt.hist(vz_train, bins=20)\n",
        "plt.xlabel(\"vz\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "#plt.yscale(\"log\")\n",
        "plt.subplot(1, 5, 5)\n",
        "plt.hist(epsilon_train, bins=20)\n",
        "plt.xlabel(\"epsilon\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "#plt.yscale(\"log\")\n",
        "plt.suptitle(\"Primitive variables\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "1fsekS7zvExL"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "7ubtOXVgUZh6"
      },
      "outputs": [],
      "source": [
        "# Generating the input and output data for train and test sets.\n",
        "x_train = generate_input_data(rho_train, vx_train ,vy_train, vz_train, epsilon_train)\n",
        "y_train = generate_labels(rho_train, epsilon_train) \n",
        "x_test = generate_input_data(rho_test, vx_test, vy_test, vz_test, epsilon_test)\n",
        "y_test = generate_labels(rho_test, epsilon_test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "lG_10Fx0vExM",
        "outputId": "4e0226bb-b92e-442b-e35f-bb23540d88b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5.3518,  5.5407,  6.2987,  0.5088,  5.6678],\n",
              "        [10.5083,  2.1281,  8.8768,  5.0672, 10.4496],\n",
              "        [10.1694, 24.6210,  8.6734, 17.3371, 28.7775],\n",
              "        ...,\n",
              "        [ 4.4721,  2.5686,  0.6514,  7.2552,  7.7801],\n",
              "        [ 1.6649,  3.8329,  2.9365,  2.0393,  5.2449],\n",
              "        [10.0924,  7.4824,  5.1795,  0.5879, 12.0876]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 70
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.8612, 4.7430, 6.6911,  ..., 2.9883, 1.5751, 6.4734], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 70
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 9.4826,  2.1303,  3.8089,  0.1353,  2.8137],\n",
              "        [ 3.2086,  0.2304,  0.0765,  1.0786,  4.0821],\n",
              "        [ 1.0799,  5.0950,  4.3277,  4.3998,  7.4208],\n",
              "        ...,\n",
              "        [11.7645, 38.2826, 11.3023, 34.4865, 48.7155],\n",
              "        [ 9.2215,  1.3869,  4.5133,  2.2089,  6.8695],\n",
              "        [11.9479, 25.2658, 21.7815, 19.5978, 31.0534]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 70
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.2754, 2.6527, 0.6533,  ..., 8.3222, 3.8858, 2.5578], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "x_train\n",
        "y_train\n",
        "x_test\n",
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "CqQxpKTYvExM"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"last_expr_or_assign\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "T2bsgZyzvExM",
        "outputId": "4289b1dd-bdcc-4b60-e8ed-40ca6329ab82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 950
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Note how we are only plotting train and not test here. \n",
        "# Plotting histograms of the input variables before z-score normalization\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.suptitle('Histograms of input variables before z-score normalization')\n",
        "for i in range(5):\n",
        "    plt.subplot(3, 2, i+1)\n",
        "    plt.hist(x_train[:, i].cpu(), bins=50) # Must be converted to cpu() for plotting.\n",
        "    plt.xlabel(f'Variable {i}')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "QrN95284vExM",
        "outputId": "499bc124-9e4a-42fb-9ab5-cbbebb1152dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary statistics of input variables before z-score normalization\n",
            "tensor([[6.6927e-05, 3.5973e+01, 6.3298e+00, 6.1996e+00, 3.8339e+00],\n",
            "        [4.0428e-06, 2.5052e+02, 6.8724e+00, 3.8580e+00, 8.9550e+00],\n",
            "        [1.2382e-06, 2.4635e+02, 6.8743e+00, 3.8543e+00, 8.9745e+00],\n",
            "        [8.8250e-06, 2.4121e+02, 6.8453e+00, 3.8405e+00, 8.9339e+00],\n",
            "        [1.6427e-05, 4.0075e+02, 1.2120e+01, 8.2927e+00, 1.3523e+01]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# print('Summary statistics of input variables before z-score normalization')\n",
        "# print(torch.stack([torch.min(x_train, dim=0).values,\n",
        "#                 torch.max(x_train, dim=0).values,\n",
        "#                 torch.mean(x_train, dim=0),\n",
        "#                 torch.median(x_train, dim=0).values,\n",
        "#                 torch.std(x_train, dim=0)], dim=1))\n",
        "\n",
        "# Computing summary statistics of the input variables before and after z-score normalization\n",
        "print('Summary statistics of input variables before z-score normalization')\n",
        "print(torch.stack([torch.min(x_train, dim=0).values, torch.max(x_train, dim=0).values, torch.nanmean(x_train, dim=0), torch.median(x_train, dim=0).values, torch.std(x_train, dim=0)], dim=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTEmkR1SUZh7"
      },
      "source": [
        "Perform z-score normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "kqUmp1VVvExN"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "yPOv6DxhUZh7"
      },
      "outputs": [],
      "source": [
        "if ZSCORE_NORMALIZATION:\n",
        "    \n",
        "    # # Computing the median of each input variable from the training set using torch.nanmedian function\n",
        "    # D_median = torch.nanmedian(x_train[:, 0])\n",
        "    # Sx_median = torch.nanmedian(x_train[:, 1])\n",
        "    # Sy_median = torch.nanmedian(x_train[:, 2])\n",
        "    # Sz_median = torch.nanmedian(x_train[:, 3])\n",
        "    # tau_median = torch.nanmedian(x_train[:, 4])\n",
        "\n",
        "    # # Computing the standard deviation of each input variable from the training set using torch.std function with a boolean mask to ignore nan values\n",
        "    # D_std = torch.std(x_train[~torch.isnan(x_train[:, 0]), 0])\n",
        "    # Sx_std = torch.std(x_train[~torch.isnan(x_train[:, 1]), 1])\n",
        "    # Sy_std = torch.std(x_train[~torch.isnan(x_train[:, 2]), 2])\n",
        "    # Sz_std = torch.std(x_train[~torch.isnan(x_train[:, 3]), 3])\n",
        "    # tau_std = torch.std(x_train[~torch.isnan(x_train[:, 4]), 4])\n",
        "\n",
        "\n",
        "    # # Applying z-score normalization to both train and test sets using the statistics from the training set\n",
        "    # x_train[:, 0] = torch.sub(x_train[:, 0], D_median).div(D_std)\n",
        "    # x_train[:, 1] = torch.sub(x_train[:, 1], Sx_median).div(Sx_std)\n",
        "    # x_train[:, 2] = torch.sub(x_train[:, 2], Sy_median).div(Sy_std)\n",
        "    # x_train[:, 3] = torch.sub(x_train[:, 3], Sz_median).div(Sz_std)\n",
        "    # x_train[:, 4] = torch.sub(x_train[:, 4], tau_median).div(tau_std)\n",
        "\n",
        "    # x_test[:, 0] = torch.sub(x_test[:, 0], D_median).div(D_std)\n",
        "    # x_test[:, 1] = torch.sub(x_test[:, 1], Sx_median).div(Sx_std)\n",
        "    # x_test[:, 2] = torch.sub(x_test[:, 2], Sy_median).div(Sy_std)\n",
        "    # x_test[:, 3] = torch.sub(x_test[:, 3], Sz_median).div(Sz_std)\n",
        "    # x_test[:, 4] = torch.sub(x_test[:, 4], tau_median).div(tau_std)\n",
        "\n",
        "    # Computing the mean and standard deviation of each column\n",
        "    mean = x_train.mean(dim=0)\n",
        "    std = x_train.std(dim=0)\n",
        "\n",
        "    # Applying z-score normalization\n",
        "    x_train = (x_train - mean) / std\n",
        "    # Use the same mean and std from the training data as we don't want test data leakage.\n",
        "    x_test = (x_test - mean) / std\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "boC-8KmXvExN",
        "outputId": "50cd4402-d9b3-4e98-a556-9261f9544eab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.1+cu118'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "6J7rG-OevExN",
        "outputId": "e8ef35b9-055d-43d5-bae5-5b8799ac98bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 5.3518, 10.5083, 10.1694,  ...,  4.4721,  1.6649, 10.0924],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 77
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([80000, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5.3518,  5.5407,  6.2987,  0.5088,  5.6678],\n",
              "        [10.5083,  2.1281,  8.8768,  5.0672, 10.4496],\n",
              "        [10.1694, 24.6210,  8.6734, 17.3371, 28.7775],\n",
              "        ...,\n",
              "        [ 4.4721,  2.5686,  0.6514,  7.2552,  7.7801],\n",
              "        [ 1.6649,  3.8329,  2.9365,  2.0393,  5.2449],\n",
              "        [10.0924,  7.4824,  5.1795,  0.5879, 12.0876]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 77
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 9.4826,  2.1303,  3.8089,  0.1353,  2.8137],\n",
              "        [ 3.2086,  0.2304,  0.0765,  1.0786,  4.0821],\n",
              "        [ 1.0799,  5.0950,  4.3277,  4.3998,  7.4208],\n",
              "        ...,\n",
              "        [11.7645, 38.2826, 11.3023, 34.4865, 48.7155],\n",
              "        [ 9.2215,  1.3869,  4.5133,  2.2089,  6.8695],\n",
              "        [11.9479, 25.2658, 21.7815, 19.5978, 31.0534]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "x_train[:, 0]\n",
        "x_train.shape\n",
        "x_train\n",
        "x_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmdr204ZvExO"
      },
      "source": [
        "Plotting the histograms of the input data after normalization if z-score normalization was performed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "yE_qe_eLvExO"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"last_expr_or_assign\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "bHw7bNaRvExO"
      },
      "outputs": [],
      "source": [
        "if ZSCORE_NORMALIZATION: \n",
        "    # Note how we are only plotting train and not test here.\n",
        "    # Plotting histograms of the input variables after z-score normalization\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.suptitle('Histograms of input variables after z-score normalization')\n",
        "    for i in range(5):\n",
        "        plt.subplot(3, 2, i+1)\n",
        "        plt.hist(x_train[:, i].cpu(), bins=50) # Must be convertedhere to cpu() for plotting.\n",
        "        plt.xlabel(f'Variable {i}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "5_7RTKmuvExO"
      },
      "outputs": [],
      "source": [
        "if ZSCORE_NORMALIZATION:\n",
        "    # print('Summary statistics of input variables after z-score normalization')\n",
        "    # print(torch.stack([torch.min(x_train, dim=0).values,\n",
        "    #                 torch.max(x_train, dim=0).values,\n",
        "    #                 torch.mean(x_train, dim=0),\n",
        "    #                 torch.median(x_train, dim=0).values,\n",
        "    #                 torch.std(x_train, dim=0)], dim=1))\n",
        "    # Computing summary statistics of the input variables after z-score normalization\n",
        "    print('Summary statistics of input variables after z-score normalization')\n",
        "    print(torch.stack([torch.min(x_train, dim=0).values, torch.max(x_train, dim=0).values, torch.mean(x_train, dim=0), torch.median(x_train, dim=0).values, torch.std(x_train, dim=0)], dim=1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E96p_MsOUZh9",
        "outputId": "4b95bad0-8f3a-4364-eed1-008e0ce2a5e3"
      },
      "source": [
        "Checking if our output is always positive by plotting a histogram of y_train and y_test tensors "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "vhyJwr4nvExO",
        "outputId": "e320e7f1-ffef-466a-b969-0955304a1c36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxUAAAGFCAYAAACL5N5gAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAtXElEQVR4nO3dfXRU9Z3H8c9ASHhoEgqRhDRAWEuw4SEgBBYquyCpGDhooMUoKiEi2u6kpY1YYXsksraLgnJQO0fcPUKknspDV7FnqViMUBRRkvDgQ1YEGwJIHkAkIUFCmLn7h4cpMQ9k5k4yuXPfr3PmHObOvTffHzeZ7/nM7947DsMwDAEAAACAn7oEuwAAAAAA1kaoAAAAAGAKoQIAAACAKYQKAAAAAKYQKgAAAACYQqgAAAAAYAqhAgAAAIApYcEuINg8Ho9OnTqlyMhIORyOYJcDAEFhGIbOnz+v+Ph4denC500S/QEApLb3B9uHilOnTmnAgAHBLgMAOoUTJ04oISEh2GV0CvQHAPiHa/UH24YKl8sll8uly5cvS/rmPyoqKirIVQFAcNTU1GjAgAGKjIwMdimdxpX/C/oDADtra39wGIZhdFBNnVJNTY2io6NVXV1N0wBgW7wXNsX/CQC0/b2QE2cBAAAAmEKoAAAAAGAKoQIAAACAKYQKAAAAAKYQKgAAAACYQqgAAAAAYAqhAgAAAIAphAoAAAAAphAqAAAAAJhCqAAAAABgCqECAICruFwuJScnKzU1NdilAIBlECoAALiK0+lUSUmJCgsLg10KAFhGWLALsLLEJdv83vbYEzMCWAkAoLPxt0fQHwBYETMVAAAAAEwhVAAAAAAwhVABAAAAwBRCBQAAAABTCBUAAAAATCFUAAAAADCFUAEAAADAFEIFAAAAAFMIFQAAAABMIVQAAAAAMIVQAQAAAMAUQgUAAAAAUwgVAAAAAEwhVAAAAAAwhVABAAAAwBRCBQAAAABTCBUAAAAATCFUAAAAADCFUAEAAADAFEIFAAAAAFMIFQAAAABMIVQAAAAAMCVkQsWFCxc0aNAgLV68ONilAAAAALYSMqHid7/7nf75n/852GUAAAAAthMSoeLIkSP69NNPlZ6eHuxSAAAAANsJC3YBu3fv1qpVq1RcXKzy8nK99tprysjIaLSOy+XSqlWrVFFRoZSUFD333HMaN26c9/XFixdr1apVeu+99zq4ev8lLtnm97bHnpgRwEoAAAAAc4I+U1FXV6eUlBS5XK5mX9+0aZNyc3OVl5en/fv3KyUlRdOmTVNVVZUk6fXXX1dSUpKSkpLa9PPq6+tVU1PT6AEAAADAf0GfqUhPT2/1tKXVq1dr4cKFys7OliStXbtW27Zt07p167RkyRK9//772rhxo7Zs2aLa2lo1NDQoKipKy5Yta3Z/K1as0PLly9tlLAAAmMVMNgArCvpMRWsuXbqk4uJipaWleZd16dJFaWlp2rt3r6RvQsKJEyd07NgxPfXUU1q4cGGLgUKSli5dqurqau/jxIkT7T4OAAAAIJQFfaaiNWfOnJHb7VZsbGyj5bGxsfr000/92mdERIQiIiICUR4AAAAAdfJQ4av58+cHuwQAAADAdjr16U8xMTHq2rWrKisrGy2vrKxUXFxckKoCAAAAcLVOHSrCw8M1ZswYFRQUeJd5PB4VFBRowoQJQawMAAAAwBVBP/2ptrZWR48e9T4vLS3VwYMH1adPHw0cOFC5ubnKysrS2LFjNW7cOK1Zs0Z1dXXeu0EBAAAACK6gh4qioiJNmTLF+zw3N1eSlJWVpfz8fGVmZur06dNatmyZKioqNGrUKG3fvr3Jxdu+crlccrlccrvdpvYDAAAA2F3QQ8XkyZNlGEar6+Tk5CgnJyegP9fpdMrpdKqmpkbR0dEB3TcAAABgJ536mgoAAAAAnR+hAgAAAIApQT/9Cb5LXLLNr+2OPTEjwJUAQOd17tw5paWl6fLly7p8+bIWLVqkhQsXBrssAAhJhAoAQEiKjIzU7t271bNnT9XV1Wn48OGaPXu2+vbtG+zSACDkECoAACGpa9eu6tmzpySpvr5ehmFc88YgVufvTLbEbDYAc2x7TYXL5VJycrJSU1ODXQoAoBm7d+/WzJkzFR8fL4fDoa1btzZZx+VyKTExUd27d9f48eO1b9++Rq+fO3dOKSkpSkhI0MMPP6yYmJgOqh4A7MW2ocLpdKqkpESFhYXBLgUA0Iy6ujqlpKTI5XI1+/qmTZuUm5urvLw87d+/XykpKZo2bZqqqqq86/Tu3VuHDh1SaWmp/vjHP6qysrLFn1dfX6+amppGDwBA29g2VAAAOrf09HT99re/1axZs5p9ffXq1Vq4cKGys7OVnJystWvXqmfPnlq3bl2TdWNjY5WSkqJ33nmnxZ+3YsUKRUdHex8DBgwI2FgAINQRKgAAlnPp0iUVFxcrLS3Nu6xLly5KS0vT3r17JUmVlZU6f/68JKm6ulq7d+/W0KFDW9zn0qVLVV1d7X2cOHGifQcBACGEC7UBAJZz5swZud1uxcbGNloeGxurTz/9VJJUVlamBx54wHuB9s9//nONGDGixX1GREQoIiKiXesGgFBFqAAAhKRx48bp4MGDwS4DAGyB058AAJYTExOjrl27NrnwurKyUnFxcUGqCgDsi5kKG+H+5QBCRXh4uMaMGaOCggJlZGRIkjwejwoKCpSTkxPc4gDAhmwbKlwul1wul9xud7BLAQA0o7a2VkePHvU+Ly0t1cGDB9WnTx8NHDhQubm5ysrK0tixYzVu3DitWbNGdXV1ys7ODmLVAGBPtg0VTqdTTqdTNTU1io6ODnY5AIBvKSoq0pQpU7zPc3NzJUlZWVnKz89XZmamTp8+rWXLlqmiokKjRo3S9u3bm1y87Ss+dAIA39k2VAAAOrfJkyfLMIxW18nJyQn46U586AQAvuNCbQAAAACmECoAAAAAmMLpTwAAwO87BHJ3QAASMxUAAAAATCJUAAAAADCFUAEAwFVcLpeSk5OVmpoa7FIAwDIIFQAAXMXpdKqkpESFhYXBLgUALMO2oYJPogAAAIDAsO3dn/hyI9/4e1cQiTuDAAAAhDrbzlQAAAAACAxCBQAAAABTCBUAAAAATCFUAAAAADCFUAEAAADAFNve/QkAgOa4XC65XC653e5gl2IJ3B0QgMRMBQAAjfDldwDgO2Yq0O78/RSLT7AAAACsgZkKAAAAAKbYNlS4XC4lJycrNTU12KUAAAAAlmbb05+cTqecTqdqamoUHR0d7HIAALAdLvIGQodtZyoAAAAABAahAgAAAIAphAoAAAAAphAqAAAAAJhCqAAA4CrcHRAAfGfbuz+h8+OuIACCgbsDAoDvmKkAAAAAYAqhAgAAAIAphAoAAAAAphAqAAAAAJjChdoAAMBy/L2ZBzfyANoHoQIAANgGdxYE2odtT3/iPuQAAABAYNh2poL7kIc2PokCAADoOLadqQAAAAAQGIQKAAAAAKYQKgAAuArX3AGA7wgVAABcxel0qqSkRIWFhcEuBQAsg1ABAAAAwBRCBQAAAABTCBUAAAAATLHt91QALfH3Oy74fgsAAGBXzFQAAAAAMIVQAQAAAMAUTn8CAABoA39Pj5U4RRahj5kKAAAAAKYQKgAAAACYQqgAAAAAYAqhAgAAAIApXKgNBAgX8AEAALuy7UyFy+VScnKyUlNTg10KAAAAYGm2DRVOp1MlJSUqLCwMdikAgE6ED50AwHe2DRUAADSHD50AwHeECgAAAACmECoAAAAAmEKoAAAAAGAKoQIAAACAKXxPBQAAQDvju4wQ6pipAAAAAGAKMxVAJ8AnWAAAwMqYqQAAAABgCqECAAAAgCmECgAAAACm+BUq/v73vwe6DgBAiKBHAID9+BUqvv/972vKlCl6+eWXdfHixUDXBACwMHoEANiPX6Fi//79GjlypHJzcxUXF6cHH3xQ+/btC3RtAAALokcAgP34FSpGjRqlZ555RqdOndK6detUXl6um266ScOHD9fq1at1+vTpQNcJALAIegQA2I+pC7XDwsI0e/ZsbdmyRU8++aSOHj2qxYsXa8CAAZo3b57Ky8sDVScAwGLoEQBgH6a+/K6oqEjr1q3Txo0b1atXLy1evFgLFizQyZMntXz5ct1+++1MeQOATdEjgMDw9wtS+XJUdCS/QsXq1au1fv16HT58WNOnT9eGDRs0ffp0denyzcTH4MGDlZ+fr8TExEDWCqAZNBt0NvQIALAfv0LF888/r/vuu0/z589X//79m12nX79+evHFF00VBwCwHqv3CJfLJZfLJbfbHexSAFP8/dBJ4oMn+M6vUHHkyJFrrhMeHq6srCx/dg8AsDCr9win0ymn06mamhpFR0cHuxwAsAS/LtRev369tmzZ0mT5li1b9NJLL5kuCgBgXfQIALAfv0LFihUrFBMT02R5v3799J//+Z+miwIAWBc9AgDsx69Qcfz4cQ0ePLjJ8kGDBun48eOmiwIAWBc9AgDsx69Q0a9fP3344YdNlh86dEh9+/Y1XVRHcLlcSk5OVmpqarBLAYCQEgo9AgDgG79CxV133aVf/OIX2rlzp9xut9xut95++20tWrRId955Z6BrbBdOp1MlJSUqLCwMdikAEFJCoUcAAHzj192fHn/8cR07dkxTp05VWNg3u/B4PJo3bx7nywKAzdEjAMB+/AoV4eHh2rRpkx5//HEdOnRIPXr00IgRIzRo0KBA1wegnXD/crQXegQA2I9foeKKpKQkJSUlBaoWAEAIoUcAgH34FSrcbrfy8/NVUFCgqqoqeTyeRq+//fbbASkOAGA99AgAsB+/QsWiRYuUn5+vGTNmaPjw4XI4HIGuCwBgUfQIALAfv0LFxo0btXnzZk2fPj3Q9QAALI4eAQD249ctZcPDw/X9738/0LUAAEIAPQIA7MevUPHQQw/pmWeekWEYga4HAGBx9AgAsB+/Tn969913tXPnTr3xxhsaNmyYunXr1uj1V199NSDFAQCshx4BWB+3HYev/AoVvXv31qxZswJdCwAgBNAjAMB+/AoV69evD3QdAIAQQY8AAPvx65oKSbp8+bLeeustvfDCCzp//rwk6dSpU6qtrQ1YcQAAa6JHAIC9+DVTUVZWpltvvVXHjx9XfX29fvSjHykyMlJPPvmk6uvrtXbt2kDXCQCwCHoEANiPXzMVixYt0tixY/XVV1+pR48e3uWzZs1SQUFBwIoDAFgPPQIA7MevmYp33nlH7733nsLDwxstT0xM1BdffBGQwgAA1kSPAAD78StUeDweud3uJstPnjypyMhI00UB6Ny41SBaQ48AAPvx6/SnW265RWvWrPE+dzgcqq2tVV5enqZPnx6o2gAAFkSPAAD78Wum4umnn9a0adOUnJysixcvau7cuTpy5IhiYmL0yiuvBLpGAICF0CMAe/N3NpuZbGvzK1QkJCTo0KFD2rhxoz788EPV1tZqwYIFuvvuuxtdlAcAsB96BADYj1+hQpLCwsJ0zz33BLIWAECIsHKPcLlccrlczV4XAgBonl+hYsOGDa2+Pm/ePL+KAQBYn9V7hNPplNPpVE1NjaKjo4NdDgBYgl+hYtGiRY2eNzQ06MKFCwoPD1fPnj07fcMAALQfegQA2I9fd3/66quvGj1qa2t1+PBh3XTTTVyEBwA2R48AAPvx+5qKbxsyZIieeOIJ3XPPPfr0008DtVsAIYbvuLAnegQAhDa/ZipaEhYWplOnTgVylwCAEEGPAIDQ5ddMxZ///OdGzw3DUHl5uX7/+9/rhz/8YUAKAwBYEz0CAOzHr1CRkZHR6LnD4dB1112nm2++WU8//XQg6gIAWBQ9AgDsx69Q4fF4Al0HACBE0CMAwH4Cek0FAAAAAPvxa6YiNze3zeuuXr3anx8BALAoegQA2I9foeLAgQM6cOCAGhoaNHToUEnSZ599pq5du+rGG2/0rudwOAJTJQDAMugRAGA/foWKmTNnKjIyUi+99JK++93vSvrmy46ys7M1adIkPfTQQwEtEgBgHfQIALAfv66pePrpp7VixQpvs5Ck7373u/rtb3/LnT0AwOboEQBgP37NVNTU1Oj06dNNlp8+fVrnz583XRQAwLroEQD8kbhkm9/bHntiRgArgT/8mqmYNWuWsrOz9eqrr+rkyZM6efKk/ud//kcLFizQ7NmzA10jAMBC6BEAYD9+zVSsXbtWixcv1ty5c9XQ0PDNjsLCtGDBAq1atSqgBV7LuXPnlJaWpsuXL+vy5ctatGiRFi5c2KE1AAD+oTP1CABAx3AYhmH4u3FdXZ0+//xzSdL111+vXr16BaywtnK73aqvr1fPnj1VV1en4cOHq6ioSH379m3T9jU1NYqOjlZ1dbWioqJ8+tlmpukA+I7p7fZj5r2wJZ2hR5hh9v+EHgF0HPpD+2nre6GpL78rLy9XeXm5hgwZol69eslEPvFb165d1bNnT0lSfX29DMMISh0AgMY6Q48AAHQMv0LFl19+qalTpyopKUnTp09XeXm5JGnBggU+3ypw9+7dmjlzpuLj4+VwOLR169Ym67hcLiUmJqp79+4aP3689u3b1+j1c+fOKSUlRQkJCXr44YcVExPjz7AAAAEQyB4BALAGv0LFr371K3Xr1k3Hjx/3zhJIUmZmprZv3+7Tvurq6pSSkiKXy9Xs65s2bVJubq7y8vK0f/9+paSkaNq0aaqqqvKu07t3bx06dEilpaX64x//qMrKyhZ/Xn19vWpqaho9AACBE8geAQCwBr8u1P7rX/+qN998UwkJCY2WDxkyRGVlZT7tKz09Xenp6S2+vnr1ai1cuFDZ2dmSvrkAcNu2bVq3bp2WLFnSaN3Y2FilpKTonXfe0U9+8pNm97dixQotX77cpxoBAG0XyB4BAG3B7WiDz69QUVdX1+jTpyvOnj2riIgI00VdcenSJRUXF2vp0qXeZV26dFFaWpr27t0rSaqsrFTPnj0VGRmp6upq7d69Wz/72c9a3OfSpUuVm5vrfV5TU6MBAwYErGYA7cffpkHD6Fgd1SMAAJ2HX6c/TZo0SRs2bPA+dzgc8ng8WrlypaZMmRKw4s6cOSO3263Y2NhGy2NjY1VRUSFJKisr06RJk5SSkqJJkybp5z//uUaMGNHiPiMiIhQVFdXoAQAInI7qEQCAzsOvmYqVK1dq6tSpKioq0qVLl/TrX/9an3zyic6ePas9e/YEusZWjRs3TgcPHuzQnwkAaFln6hEAgI7h10zF8OHD9dlnn+mmm27S7bffrrq6Os2ePVsHDhzQ9ddfH7DiYmJi1LVr1yYXXldWViouLi5gPwcAEDgd1SMAAJ2HzzMVDQ0NuvXWW7V27Vr95je/aY+avMLDwzVmzBgVFBQoIyNDkuTxeFRQUKCcnJx2/dkAAN91ZI8AAHQePoeKbt266cMPPwxYAbW1tTp69Kj3eWlpqQ4ePKg+ffpo4MCBys3NVVZWlsaOHatx48ZpzZo1qqur894NCgDQeQS6RwAArMGv05/uuecevfjiiwEpoKioSKNHj9bo0aMlSbm5uRo9erSWLVsm6Zv7mj/11FNatmyZRo0apYMHD2r79u1NLt72lcvlUnJyslJTU02PAQDwD4HsEQAAa/DrQu3Lly9r3bp1euuttzRmzBj16tWr0eurV69u874mT54swzBaXScnJyfgpzs5nU45nU7V1NQoOjo6oPsGADsLZI8AAFiDT6Hi73//uxITE/Xxxx/rxhtvlCR99tlnjdZxOByBqw4AYBn0CACwL59CxZAhQ1ReXq6dO3dK+ubUpGeffdb0qUgAAOujRwCwIr5YNTB8uqbi26cpvfHGG6qrqwtoQQAAa6JHAIB9+XWh9hXXuhYCAGBf9AgAsA+fQoXD4WhyPiznxwIAJHoEANiZT9dUGIah+fPnKyIiQpJ08eJF/fSnP21yZ49XX301cBW2E5fLJZfLJbfbHexSACAkhFKPAAD4xqdQkZWV1ej5PffcE9BiOhK3lAWAwAqlHgEA8I1PoWL9+vXtVQcAtBt/7+whcXcPX9AjAMC+TF2oDQAAAACECgAAAACmECoAACHpxIkTmjx5spKTkzVy5Eht2bIl2CUBQMjy6ZoKAACsIiwsTGvWrNGoUaNUUVGhMWPGaPr06U3uRgUAMI9QAQAISf3791f//v0lSXFxcYqJidHZs2cJFQDQDmx7+pPL5VJycrJSU1ODXQoAoBm7d+/WzJkzFR8fL4fDoa1btzZZx+VyKTExUd27d9f48eO1b9++ZvdVXFwst9utAQMGtHPVAGBPtg0VTqdTJSUlKiwsDHYpAIBm1NXVKSUlRS6Xq9nXN23apNzcXOXl5Wn//v1KSUnRtGnTVFVV1Wi9s2fPat68efqv//qvjigbAGyJ058AAJ1Senq60tPTW3x99erVWrhwobKzsyVJa9eu1bZt27Ru3TotWbJEklRfX6+MjAwtWbJEEydObPXn1dfXq76+3vu8pqYmAKMAAHuw7UwFAMC6Ll26pOLiYqWlpXmXdenSRWlpadq7d68kyTAMzZ8/XzfffLPuvffea+5zxYoVio6O9j44VQoA2o5QAQCwnDNnzsjtdis2NrbR8tjYWFVUVEiS9uzZo02bNmnr1q0aNWqURo0apY8++qjFfS5dulTV1dXex4kTJ9p1DAAQSjj9CQAQkm666SZ5PJ42rx8REaGIiIh2rAgAQhczFQAAy4mJiVHXrl1VWVnZaHllZaXi4uKCVBUA2BehAgBgOeHh4RozZowKCgq8yzwejwoKCjRhwoQgVgYA9sTpTwCATqm2tlZHjx71Pi8tLdXBgwfVp08fDRw4ULm5ucrKytLYsWM1btw4rVmzRnV1dd67QQEAOg6hAgDQKRUVFWnKlCne57m5uZKkrKws5efnKzMzU6dPn9ayZctUUVGhUaNGafv27U0u3vaVy+WSy+WS2+02tR8AsBPbhgqaBgB0bpMnT5ZhGK2uk5OTo5ycnID+XKfTKafTqZqaGkVHRwd03wAQqmx7TQXfqA0AAAAEhm1DBQAAAIDAIFQAAAAAMIVQAQAAAMAUQgUAAAAAUwgVAAAAAEyx7S1lAaAtEpds83vbY0/MCGAl6CjcchwAfMdMBQAAV+GW4wDgO0IFAAAAAFMIFQAAAABMsW2ocLlcSk5OVmpqarBLAQAAACzNtqGCc2YBAACAwLBtqAAAAAAQGIQKAAAAAKYQKgAAAACYQqgAAOAq3MgDAHxHqAAA4CrcyAMAfEeoAAAAAGAKoQIAAACAKYQKAAAAAKYQKgAAAACYQqgAAAAAYAqhAgAAAIApYcEuAAAAALCaxCXb/N722BMzAlhJ52DbmQq+3AgAAAAIDNuGCr7cCADQHD50AgDf2TZUAADQHD50AgDfESoAAAAAmEKoAAAAAGAKoQIAAACAKYQKAAAAAKYQKgAAAACYQqgAAAAAYAqhAgAAAIAphAoAAAAAphAqAAAAAJgSFuwCAAAAADtJXLLN722PPTEjgJUEDjMVAABcxeVyKTk5WampqcEuBQAsg1ABAMBVnE6nSkpKVFhYGOxSAMAyCBUAAAAATCFUAAAAADCFC7UBoJ34eyFeZ70IDwCAlth2poIL8QAAAIDAsG2o4EI8AAAAIDBsGyoAAAAABAahAgAAAIAphAoAAAAAphAqAAAAAJhCqAAAAABgCqECAAAAgCmECgAAAACmECoAAAAAmEKoAADgKi6XS8nJyUpNTQ12KQBgGYQKAACu4nQ6VVJSosLCwmCXAgCWQagAAAAAYAqhAgAAAIAphAoAAAAAphAqAAAAAJhCqAAAAABgCqECAAAAgCmECgAAAACmECoAAAAAmEKoAAAAAGAKoQIAAACAKYQKAAAAAKYQKgAAAACYQqgAAAAAYIptQ4XL5VJycrJSU1ODXQoAAABgabYNFU6nUyUlJSosLAx2KQAAAICl2TZUAAAAAAgMQgUAAFfh9FgA8B2hAgCAq3B6LAD4jlABAAAAwBRCBQAAAABTCBUAAAAATCFUAAAAADCFUAEAAADAFEIFAAAAAFMIFQAAAABMIVQAAAAAMIVQAQAAAMAUQgUAAAAAUwgVAAAAAEwhVAAAAAAwhVABAAAAwBRCBQAAAABTCBUAAAAATCFUAAAAADCFUAEAAADAFEIFAAAAAFMIFQAAAABMIVQAAAAAMIVQAQAAAMAUQgUAAAAAU8KCXQAAoLHEJdv83vbYEzMCWAkAoLPxt0e0d39gpgIAAACAKYQKAAAAAKYQKgAAuIrL5VJycrJSU1ODXQoAWAahAgCAqzidTpWUlKiwsDDYpQCAZRAqAAAAAJhCqAAAAABgCqECAAAAgCmECgAAAACmECoAAAAAmEKoAAAAAGAKoQIAAACAKYQKAAAAAKYQKgAAAACYEhbsAoLNMAxJUk1Njc/beuovBLocADDFn/eyq7e78p4Ic/1BokcA6Fzauz/YPlScP39ekjRgwIAgVwIA5kWvMbf9+fPnFR0dHZBarI7+ACCUtHd/cBg2/1jK4/Ho1KlTioyMlMPhaPN2NTU1GjBggE6cOKGoqKh2rLBjheq4pNAdG+Oyls46LsMwdP78ecXHx6tLF86MlfzvD1LnPc6BwNisKZTHJoX2+II9trb2B9vPVHTp0kUJCQl+bx8VFRVyv7xS6I5LCt2xMS5r6YzjYoaiMbP9QeqcxzlQGJs1hfLYpNAeXzDH1pb+wMdRAAAAAEwhVAAAAAAwhVDhp4iICOXl5SkiIiLYpQRUqI5LCt2xMS5rCdVxobFQPs6MzZpCeWxSaI/PKmOz/YXaAAAAAMxhpgIAAACAKYQKAAAAAKYQKgAAAACYQqgAAAAAYAqhohUul0uJiYnq3r27xo8fr3379rW6/pYtW3TDDTeoe/fuGjFihP7yl790UKVtt2LFCqWmpioyMlL9+vVTRkaGDh8+3Oo2+fn5cjgcjR7du3fvoIrb5rHHHmtS4w033NDqNlY4XomJiU3G5XA45HQ6m12/sx6r3bt3a+bMmYqPj5fD4dDWrVsbvW4YhpYtW6b+/furR48eSktL05EjR665X1//RgOttXE1NDTokUce0YgRI9SrVy/Fx8dr3rx5OnXqVKv79Od3GcFBj/hGZ33f+bZQ7RNXhEq/kEK3Z0ih3TcIFS3YtGmTcnNzlZeXp/379yslJUXTpk1TVVVVs+u/9957uuuuu7RgwQIdOHBAGRkZysjI0Mcff9zBlbfub3/7m5xOp95//33t2LFDDQ0NuuWWW1RXV9fqdlFRUSovL/c+ysrKOqjiths2bFijGt99990W17XK8SosLGw0ph07dkiS5syZ0+I2nfFY1dXVKSUlRS6Xq9nXV65cqWeffVZr167VBx98oF69emnatGm6ePFii/v09W+0PbQ2rgsXLmj//v169NFHtX//fr366qs6fPiwbrvttmvu15ffZQQHPaKxzvi+05xQ7BNXhEq/kEK3Z0gh3jcMNGvcuHGG0+n0Pne73UZ8fLyxYsWKZte/4447jBkzZjRaNn78eOPBBx9s1zrNqqqqMiQZf/vb31pcZ/369UZ0dHTHFeWHvLw8IyUlpc3rW/V4LVq0yLj++usNj8fT7OtWOFaSjNdee8373OPxGHFxccaqVau8y86dO2dEREQYr7zySov78fVvtL19e1zN2bdvnyHJKCsra3EdX3+XERz0iH+wwvuOYdinT1wRCv3CMEK3ZxhG6PUNZiqacenSJRUXFystLc27rEuXLkpLS9PevXub3Wbv3r2N1pekadOmtbh+Z1FdXS1J6tOnT6vr1dbWatCgQRowYIBuv/12ffLJJx1Rnk+OHDmi+Ph4/dM//ZPuvvtuHT9+vMV1rXi8Ll26pJdffln33XefHA5Hi+tZ4VhdrbS0VBUVFY2OR3R0tMaPH9/i8fDnb7QzqK6ulsPhUO/evVtdz5ffZXQ8ekRTVnnfCfU+cUWo9gvJXj1DslbfIFQ048yZM3K73YqNjW20PDY2VhUVFc1uU1FR4dP6nYHH49Evf/lL/fCHP9Tw4cNbXG/o0KFat26dXn/9db388svyeDyaOHGiTp482YHVtm78+PHKz8/X9u3b9fzzz6u0tFSTJk3S+fPnm13fisdr69atOnfunObPn9/iOlY4Vt925f/cl+Phz99osF28eFGPPPKI7rrrLkVFRbW4nq+/y+h49IjGrPK+Y4c+cUWo9gvJPj1Dsl7fCOvQn4ZOxel06uOPP77meXcTJkzQhAkTvM8nTpyoH/zgB3rhhRf0+OOPt3eZbZKenu7998iRIzV+/HgNGjRImzdv1oIFC4JYWeC8+OKLSk9PV3x8fIvrWOFY2VFDQ4PuuOMOGYah559/vtV17fC7DGsIpR4h2etvi35hfVbsG8xUNCMmJkZdu3ZVZWVlo+WVlZWKi4trdpu4uDif1g+2nJwc/e///q927typhIQEn7bt1q2bRo8eraNHj7ZTdeb17t1bSUlJLdZoteNVVlamt956S/fff79P21nhWF35P/flePjzNxosVxpDWVmZduzY0eqnTc251u8yOh49onVWeN+RQq9PXBHK/UIK/Z4hWbdvECqaER4erjFjxqigoMC7zOPxqKCgoFGqv9qECRMarS9JO3bsaHH9YDEMQzk5OXrttdf09ttva/DgwT7vw+1266OPPlL//v3bocLAqK2t1eeff95ijVY5XlesX79e/fr104wZM3zazgrHavDgwYqLi2t0PGpqavTBBx+0eDz8+RsNhiuN4ciRI3rrrbfUt29fn/dxrd9ldDx6ROus8L4jhV6fuCKU+4UU2j1DsnjfCO514p3Xxo0bjYiICCM/P98oKSkxHnjgAaN3795GRUWFYRiGce+99xpLlizxrr9nzx4jLCzMeOqpp4z/+7//M/Ly8oxu3boZH330UbCG0Kyf/exnRnR0tLFr1y6jvLzc+7hw4YJ3nW+Pbfny5cabb75pfP7550ZxcbFx5513Gt27dzc++eSTYAyhWQ899JCxa9cuo7S01NizZ4+RlpZmxMTEGFVVVYZhWPd4GcY3d6gYOHCg8cgjjzR5zSrH6vz588aBAweMAwcOGJKM1atXGwcOHPDezeKJJ54wevfubbz++uvGhx9+aNx+++3G4MGDja+//tq7j5tvvtl47rnnvM+v9Tca7HFdunTJuO2224yEhATj4MGDjf7e6uvrWxzXtX6X0TnQIzr/+863hXKfuCIU+oVhhG7PuNbYrN43CBWteO6554yBAwca4eHhxrhx44z333/f+9q//uu/GllZWY3W37x5s5GUlGSEh4cbw4YNM7Zt29bBFV+bpGYf69ev967z7bH98pe/9P4/xMbGGtOnTzf279/f8cW3IjMz0+jfv78RHh5ufO973zMyMzONo0ePel+36vEyDMN48803DUnG4cOHm7xmlWO1c+fOZn/vrtTu8XiMRx991IiNjTUiIiKMqVOnNhnvoEGDjLy8vEbLWvsb7Qitjau0tLTFv7edO3e2OK5r/S6j86BHfKOzvu98Wyj3iStCoV8YRuj2DMMI7b7hMAzDCPTsBwAAAAD74JoKAAAAAKYQKgAAAACYQqgAAAAAYAqhAgAAAIAphAoAAAAAphAqAAAAAJhCqAAAAABgCqECAAAAgCmECsCC5s+fr4yMjGCXAQAAIIlQAXSIxx57TKNGjQrY/p555hnl5+cHbH8AgOALdK+QpPz8fPXu3Tug+wSaExbsAgD8Q0NDg7p163bN9aKjozugGgAAgLZhpgJoow0bNqhv376qr69vtDwjI0P33ntvi9vl5+dr+fLlOnTokBwOhxwOh3eWweFw6Pnnn9dtt92mXr166Xe/+53cbrcWLFigwYMHq0ePHho6dKieeeaZRvv89ulPkydP1i9+8Qv9+te/Vp8+fRQXF6fHHnssUEMHALRRe/SKc+fO6f7779d1112nqKgo3XzzzTp06JB320OHDmnKlCmKjIxUVFSUxowZo6KiIu3atUvZ2dmqrq727pPegPZCqADaaM6cOXK73frzn//sXVZVVaVt27bpvvvua3G7zMxMPfTQQxo2bJjKy8tVXl6uzMxM7+uPPfaYZs2apY8++kj33XefPB6PEhIStGXLFpWUlGjZsmX693//d23evLnV+l566SX16tVLH3zwgVauXKn/+I//0I4dO8wPHADQZu3RK+bMmaOqqiq98cYbKi4u1o033qipU6fq7NmzkqS7775bCQkJKiwsVHFxsZYsWaJu3bpp4sSJWrNmjaKiorz7XLx4cfv+B8C2OP0JaKMePXpo7ty5Wr9+vebMmSNJevnllzVw4EBNnjy51e2+853vKCwsTHFxcU1enzt3rrKzsxstW758ufffgwcP1t69e7V582bdcccdLf6ckSNHKi8vT5I0ZMgQ/f73v1dBQYF+9KMf+TJMAIAJge4V7777rvbt26eqqipFRERIkp566ilt3bpVf/rTn/TAAw/o+PHjevjhh3XDDTdI+qYHXBEdHS2Hw9Fs/wECiVAB+GDhwoVKTU3VF198oe9973vKz8/X/Pnz5XA4/N7n2LFjmyxzuVxat26djh8/rq+//lqXLl265sV7I0eObPS8f//+qqqq8rsuAIB/AtkrDh06pNraWvXt27fR8q+//lqff/65JCk3N1f333+//vCHPygtLU1z5szR9ddfH5CxAG1FqAB8MHr0aKWkpGjDhg265ZZb9Mknn2jbtm2m9tmrV69Gzzdu3KjFixfr6aef1oQJExQZGalVq1bpgw8+aHU/377A2+FwyOPxmKoNAOC7QPaK2tpa9e/fX7t27Wry2pW7Oj322GOaO3eutm3bpjfeeEN5eXnauHGjZs2aZWIUgG8IFYCP7r//fq1Zs0ZffPGF0tLSNGDAgGtuEx4eLrfb3ab979mzRxMnTtS//du/eZdd+TQKAGANgeoVN954oyoqKhQWFqbExMQWt01KSlJSUpJ+9atf6a677tL69es1a9Ysn/oPYAYXagM+mjt3rk6ePKn//u//bvWiu6slJiaqtLRUBw8e1JkzZ5rcFeRqQ4YMUVFRkd5880199tlnevTRR1VYWBio8gEAHSBQvSItLU0TJkxQRkaG/vrXv+rYsWN677339Jvf/EZFRUX6+uuvlZOTo127dqmsrEx79uxRYWGhfvCDH3j3WVtbq4KCAp05c0YXLlxoz2HDxggVgI+io6P14x//WN/5znfa/K3WP/7xj3XrrbdqypQpuu666/TKK6+0uO6DDz6o2bNnKzMzU+PHj9eXX37ZaNYCAND5BapXOBwO/eUvf9G//Mu/KDs7W0lJSbrzzjtVVlam2NhYde3aVV9++aXmzZunpKQk3XHHHUpPT/fe8GPixIn66U9/qszMTF133XVauXJlO44aduYwDMMIdhGA1UydOlXDhg3Ts88+G+xSAACdFL0CdkKoAHzw1VdfadeuXfrJT36ikpISDR06NNglAQA6GXoF7IgLtQEfjB49Wl999ZWefPLJRk1i2LBhKisra3abF154QXfffXdHlQgACDJ6BeyImQogAMrKytTQ0NDsa7GxsYqMjOzgigAAnQ29AqGMUAEAAADAFO7+BAAAAMAUQgUAAAAAUwgVAAAAAEwhVAAAAAAwhVABAAAAwBRCBQAAAABTCBUAAAAATPl/8Xnekg58BvYAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(y_train.cpu().numpy(), bins=20) # must be cpu here.\n",
        "plt.xlabel(\"y_train\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.yscale(\"log\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(y_test.cpu().numpy(), bins=20) # must be cpu here\n",
        "plt.xlabel(\"y_test\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.yscale(\"log\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "FEgjk--AUZh9"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2b9GecHUZh9"
      },
      "source": [
        "## Defining the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "Iv8HA-ZXUZh-"
      },
      "outputs": [],
      "source": [
        "# Defining a class for the network\n",
        "class Net(nn.Module):\n",
        "    \"\"\"A class for creating a network with a\n",
        "    variable number of hidden layers and units.\n",
        "\n",
        "    Attributes:\n",
        "        n_layers (int): The number of hidden layers in the network.\n",
        "        n_units (list): A list of integers representing the number of units in each hidden layer.\n",
        "        hidden_activation (torch.nn.Module): The activation function for the hidden layers.\n",
        "        output_activation (torch.nn.Module): The activation function for the output layer.\n",
        "        layers (torch.nn.ModuleList): A list of linear layers in the network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_layers, n_units, hidden_activation, output_activation):\n",
        "        \"\"\"Initializes the network with the given hyperparameters.\n",
        "\n",
        "        Args:\n",
        "            n_layers (int): The number of hidden layers in the network.\n",
        "            n_units (list): A list of integers representing the number of units in each hidden layer.\n",
        "            hidden_activation (torch.nn.Module): The activation function for the hidden layers.\n",
        "            output_activation (torch.nn.Module): The activation function for the output layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.n_units = n_units\n",
        "        self.hidden_activation = hidden_activation\n",
        "        self.output_activation = output_activation\n",
        "\n",
        "        # Creating a list of linear layers with different numbers of units for each layer\n",
        "        self.layers = nn.ModuleList([nn.Linear(5, n_units[0])])\n",
        "        for i in range(1, n_layers):\n",
        "            self.layers.append(nn.Linear(n_units[i - 1], n_units[i]))\n",
        "        self.layers.append(nn.Linear(n_units[-1], 1))\n",
        "\n",
        "        # Adding some assertions to check that the input arguments are valid\n",
        "        assert isinstance(n_layers, int) and n_layers > 0, \"n_layers must be a positive integer\"\n",
        "        assert isinstance(n_units, list) and len(n_units) == n_layers, \"n_units must be a list of length n_layers\"\n",
        "        assert all(isinstance(n, int) and n > 0 for n in n_units), \"n_units must contain positive integers\"\n",
        "        assert isinstance(hidden_activation, nn.Module), \"hidden_activation must be a torch.nn.Module\"\n",
        "        assert isinstance(output_activation, nn.Module), \"output_activation must be a torch.nn.Module\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Performs a forward pass on the input tensor.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input tensor of shape (batch_size, 5).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The output tensor of shape (batch_size, 1).\n",
        "        \"\"\"\n",
        "        # Adding an assertion to check that the input tensor has the expected shape and type\n",
        "        assert isinstance(x, torch.Tensor), \"x must be a torch.Tensor\"\n",
        "        assert x.shape[1] == 5, \"x must have shape (batch_size, 5)\"\n",
        "\n",
        "        # Looping over the hidden layers and applying the linear transformation and the activation function\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = self.hidden_activation(layer(x))\n",
        "        # Applying the linear transformation and the activation function on the output layer\n",
        "        x = self.output_activation(self.layers[-1](x))\n",
        "\n",
        "        # Returning the output tensor\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GNvp55PUZh_"
      },
      "source": [
        "## Defining the model and search space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "9a1opluOUZh_"
      },
      "outputs": [],
      "source": [
        "# Defining a function to create a trial network and optimizer\n",
        "def create_model(trial, optimize):\n",
        "    \"\"\"Creates a trial network and optimizer based on the sampled hyperparameters.\n",
        "\n",
        "    Args:\n",
        "        trial (optuna.trial.Trial): The trial object that contains the hyperparameters.\n",
        "        optimize (boolean): Whether to optimize the hyperparameters or to use predefined values.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple of (net, loss_fn, optimizer, batch_size, n_epochs,\n",
        "            scheduler, loss_name, optimizer_name, scheduler_name,\n",
        "            n_units, n_layers, hidden_activation, output_activation),\n",
        "            where net is the trial network,\n",
        "            loss_fn is the loss function,\n",
        "            optimizer is the optimizer,\n",
        "            batch_size is the batch size,\n",
        "            n_epochs is the number of epochs,\n",
        "            scheduler is the learning rate scheduler,\n",
        "            loss_name is the name of the loss function,\n",
        "            optimizer_name is the name of the optimizer,\n",
        "            scheduler_name is the name of the scheduler,\n",
        "            n_units is a list of integers representing\n",
        "            the number of units in each hidden layer,\n",
        "            n_layers is an integer representing the number of hidden layers in the network,\n",
        "            hidden_activation is a torch.nn.Module representing the activation function for the hidden layers,\n",
        "            output_activation is a torch.nn.Module representing the activation function for the output layer,\n",
        "            lr is the (initial) learning rate.\n",
        "    \"\"\"\n",
        "    # If optimize is True, sample the hyperparameters from the search space\n",
        "    if optimize:\n",
        "        # Sampling the hyperparameters from the search space\n",
        "        n_layers = trial.suggest_int(\"n_layers\", 2, 6)\n",
        "        n_units = [trial.suggest_int(f\"n_units_{i}\", 16, 2048) for i in range(n_layers)] \n",
        "        hidden_activation_name = trial.suggest_categorical(\n",
        "            #\"hidden_activation\", [\"ReLU\", \"LeakyReLU\", \"ELU\", \"Tanh\", \"Sigmoid\"]\n",
        "            #\"hidden_activation\", [\"ReLU\", \"LeakyReLU\"]\n",
        "            \"hidden_activation\", [\"ReLU\", \"LeakyReLU\", \"ELU\"]\n",
        "        )\n",
        "        output_activation_name = trial.suggest_categorical(\n",
        "            #\"output_activation\", [\"Linear\", \"ReLU\", \"Softplus\"]\n",
        "            # Assuming pressure cannot be negative, linear output activation is not an option.\n",
        "            #\"output_activation\", [\"ReLU\", \"Softplus\", \"Linear\"]\n",
        "            \"output_activation\", [\"ReLU\", \"Linear\"]\n",
        "        ) \n",
        "        loss_name = trial.suggest_categorical(\n",
        "            #\"loss\", [\"MSE\", \"MAE\", \"Huber\", \"LogCosh\"] \n",
        "            \"loss\", [\"MSE\", \"MAE\", \"Huber\"] \n",
        "        )\n",
        "        optimizer_name = trial.suggest_categorical(\n",
        "            \"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\", \"Adagrad\"] \n",
        "        )\n",
        "        lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2) \n",
        "\n",
        "        batch_size_list = [32, 48, 64, 96, 128, 256, 512, 1048]\n",
        "        batch_size = trial.suggest_categorical(\"batch_size\", batch_size_list)\n",
        "        #batch_size = trial.suggest_int(\"batch_size\", 64, 1048)\n",
        "        n_epochs = trial.suggest_int(\"n_epochs\", 100, 300) \n",
        "        scheduler_name = trial.suggest_categorical(\n",
        "            \"scheduler\",\n",
        "            # [\"None\", \"CosineAnnealingLR\", \"ReduceLROnPlateau\", \"StepLR\", \"ExponentialLR\"],\n",
        "            [\"CosineAnnealingLR\", \"ReduceLROnPlateau\", \"StepLR\"],\n",
        "        )\n",
        "\n",
        "    # If optimize is False, use the predefined values\n",
        "    else:\n",
        "        # Setting the hyperparameters to the predefined values\n",
        "        n_layers = N_LAYERS_NO_OPT\n",
        "        n_units = N_UNITS_NO_OPT\n",
        "        hidden_activation_name = HIDDEN_ACTIVATION_NAME_NO_OPT\n",
        "        output_activation_name = OUTPUT_ACTIVATION_NAME_NO_OPT\n",
        "        loss_name = LOSS_NAME_NO_OPT\n",
        "        optimizer_name = OPTIMIZER_NAME_NO_OPT\n",
        "        lr = LR_NO_OPT\n",
        "        batch_size = BATCH_SIZE_NO_OPT\n",
        "        n_epochs = N_EPOCHS_NO_OPT\n",
        "        scheduler_name = SCHEDULER_NAME_NO_OPT\n",
        "\n",
        "\n",
        "    # Creating the activation functions from their names\n",
        "    if hidden_activation_name == \"ReLU\":\n",
        "        hidden_activation = nn.ReLU()\n",
        "    elif hidden_activation_name == \"LeakyReLU\":\n",
        "        hidden_activation = nn.LeakyReLU() \n",
        "    elif hidden_activation_name == \"ELU\":\n",
        "        hidden_activation = nn.ELU() \n",
        "    elif hidden_activation_name == \"Tanh\":\n",
        "        hidden_activation = nn.Tanh()\n",
        "    else:\n",
        "        hidden_activation = nn.Sigmoid()\n",
        "\n",
        "    if output_activation_name == \"ReLU\":\n",
        "        output_activation = nn.ReLU()\n",
        "    elif output_activation_name == \"Softplus\":\n",
        "        output_activation = nn.Softplus()\n",
        "    else:\n",
        "        output_activation = nn.Identity()\n",
        "\n",
        "    # Creating the loss function from its name\n",
        "    if loss_name == \"MSE\":\n",
        "        loss_fn = nn.MSELoss()\n",
        "    elif loss_name == \"MAE\":\n",
        "        loss_fn = nn.L1Loss()\n",
        "    elif loss_name == \"Huber\":\n",
        "        loss_fn = nn.SmoothL1Loss() \n",
        "    else:\n",
        "        # Creating the log-cosh loss function\n",
        "        def log_cosh_loss(y_pred, y_true):\n",
        "            return torch.mean(torch.log(torch.cosh(y_pred - y_true)))\n",
        "            \n",
        "        loss_fn = log_cosh_loss\n",
        "\n",
        "    # Creating the network with the sampled hyperparameters\n",
        "    net = Net(\n",
        "        n_layers, n_units, hidden_activation, output_activation\n",
        "    ).to(device)\n",
        "\n",
        "    if optimize:\n",
        "        # Creating the optimizer from its name\n",
        "        if optimizer_name == \"SGD\":\n",
        "            # Added sampling the weight decay and momentum for SGD\n",
        "            weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
        "            momentum = trial.suggest_uniform(\"momentum\", 0.0, 0.99)\n",
        "            optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
        "        elif optimizer_name == \"Adam\":\n",
        "            # Added sampling the weight decay and beta parameters for Adam\n",
        "            weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
        "            beta1 = trial.suggest_uniform(\"beta1\", 0.9, 0.999)\n",
        "            beta2 = trial.suggest_uniform(\"beta2\", 0.999, 0.9999)\n",
        "            optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay, betas=(beta1, beta2))\n",
        "        elif optimizer_name == \"RMSprop\":\n",
        "            optimizer = optim.RMSprop(net.parameters(), lr=lr)\n",
        "        else:\n",
        "            # Added creating the Adagrad optimizer\n",
        "            optimizer = optim.Adagrad(net.parameters(), lr=lr)\n",
        "\n",
        "        # Creating the learning rate scheduler from its name\n",
        "        if scheduler_name == \"StepLR\":\n",
        "            step_size = trial.suggest_int(\"step_size\", 5, 15)\n",
        "            gamma = trial.suggest_uniform(\"gamma\", 0.1, 0.5)\n",
        "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "        elif scheduler_name == \"ExponentialLR\":\n",
        "            gamma = trial.suggest_uniform(\"gamma\", 0.8, 0.99)\n",
        "            scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
        "        elif scheduler_name == \"CosineAnnealingLR\":\n",
        "            if n_epochs < 150:\n",
        "                t_max_fraction = trial.suggest_uniform('t_max_fraction', 0.1, 0.3)\n",
        "            elif n_epochs > 250:\n",
        "                t_max_fraction = trial.suggest_uniform('t_max_fraction', 0.05, 0.1)\n",
        "            else:\n",
        "                t_max_fraction = trial.suggest_uniform('t_max_fraction', 0.1, 0.2)\n",
        "\n",
        "            T_max = int(n_epochs * t_max_fraction)\n",
        "            eta_min = trial.suggest_loguniform(\"eta_min\", 1e-7, 1e-2)\n",
        "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)\n",
        "        elif scheduler_name == \"ReduceLROnPlateau\":\n",
        "            # Added sampling the factor, patience and threshold for ReduceLROnPlateau\n",
        "            factor = trial.suggest_uniform(\"factor\", 0.1, 0.5)\n",
        "            patience = trial.suggest_int(\"patience\", 5, 10)\n",
        "            threshold = trial.suggest_loguniform(\"threshold\", 1e-4, 1e-2)\n",
        "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                optimizer, mode=\"min\", factor=factor, patience=patience, threshold=threshold\n",
        "            )\n",
        "        # # Added using OneCycleLR scheduler as an option\n",
        "        # elif scheduler_name == \"OneCycleLR\":\n",
        "        #         # Added sampling the max_lr and pct_start for OneCycleLR\n",
        "        #         max_lr = trial.suggest_loguniform(\"max_lr\", lr, 10 * lr) \n",
        "        #         pct_start = trial.suggest_uniform(\"pct_start\", 0.1, 0.9)\n",
        "        #         scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "        #             optimizer,\n",
        "        #             max_lr=max_lr,\n",
        "        #             epochs=n_epochs,\n",
        "        #             steps_per_epoch=len(train_loader),\n",
        "        #             pct_start=pct_start,\n",
        "        #         )\n",
        "        else:\n",
        "            scheduler = None\n",
        "    else:\n",
        "        # Creating the optimizer from its name\n",
        "        if optimizer_name == \"SGD\":\n",
        "            optimizer = optim.SGD(net.parameters(), lr=lr)\n",
        "        elif optimizer_name == \"Adam\":\n",
        "            optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "        elif optimizer_name == \"RMSprop\":\n",
        "            optimizer = optim.RMSprop(net.parameters(), lr=lr)\n",
        "        else:\n",
        "            optimizer = optim.Adagrad(net.parameters(), lr=lr)\n",
        "\n",
        "        # Creating the learning rate scheduler from its name\n",
        "        if scheduler_name == \"StepLR\":\n",
        "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "        elif scheduler_name == \"ExponentialLR\":\n",
        "            scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "        elif scheduler_name == \"CosineAnnealingLR\":\n",
        "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer)\n",
        "        elif scheduler_name == \"ReduceLROnPlateau\":\n",
        "            # Creating the ReduceLROnPlateau scheduler with a threshold value of 0.01\n",
        "            #scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            #    optimizer, mode=\"min\", factor=0.1, patience=10, threshold=0.01\n",
        "            #)\n",
        "            # Use Dieseldorst et al. settings and add to that a minimum lr.\n",
        "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                        optimizer, mode=\"min\", factor=0.18979341786654758, patience=11, threshold=0.0017197466122611932 #, min_lr=1e-6\n",
        "                    )\n",
        "        else:\n",
        "            scheduler = None\n",
        "\n",
        "    # Returning all variables needed for saving and loading\n",
        "    return net, loss_fn, optimizer, batch_size, n_epochs, scheduler, loss_name, optimizer_name, scheduler_name, n_units, n_layers, hidden_activation, output_activation, lr\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-czA7VvUZiD"
      },
      "source": [
        " ## The training and evaluation loop\n",
        "\n",
        " We first define a couple of functions used in the training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "aD6FQNmxUZiD"
      },
      "outputs": [],
      "source": [
        "# Defining a function that computes loss and metrics for a given batch\n",
        "def compute_loss_and_metrics(y_pred, y_true, loss_fn):\n",
        "    \"\"\"Computes loss and metrics for a given batch.\n",
        "\n",
        "    Args:\n",
        "        y_pred (torch.Tensor): The predicted pressure tensor of shape (batch_size, 1).\n",
        "        y_true (torch.Tensor): The true pressure tensor of shape (batch_size,).\n",
        "        loss_fn (torch.nn.Module or function): The loss function to use.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple of (loss, l1_norm), where loss is a scalar tensor,\n",
        "            l1_norm is L1 norm for relative error of pressure,\n",
        "            each being a scalar tensor.\n",
        "            linf_norm is Linf norm for relative error of pressure.\n",
        "    \"\"\"\n",
        "    # Reshaping the target tensor to match the input tensor\n",
        "    y_true = y_true.view(-1, 1)\n",
        "\n",
        "    # Computing the loss using the loss function\n",
        "    loss = loss_fn(y_pred, y_true)\n",
        "\n",
        "    # Computing the relative error of pressure\n",
        "    rel_error = torch.abs((y_pred - y_true) / y_true)\n",
        "\n",
        "    # Computing the L1 norm for the relative error of pressure\n",
        "    l1_norm = torch.mean(rel_error) \n",
        "    # Computing the Linf norm for the relative error of pressure\n",
        "    linf_norm = torch.max(rel_error) \n",
        "\n",
        "    # Returning the loss and metrics\n",
        "    return loss, l1_norm, linf_norm\n",
        "\n",
        "\n",
        "# Defining a function that updates the learning rate scheduler with validation loss if applicable\n",
        "def update_scheduler(scheduler, test_loss):\n",
        "    \"\"\"Updates the learning rate scheduler with validation loss if applicable.\n",
        "\n",
        "    Args:\n",
        "        scheduler (torch.optim.lr_scheduler._LRScheduler or None): The learning rate scheduler to use.\n",
        "        test_loss (float): The validation loss to use.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Checking if scheduler is not None\n",
        "    if scheduler is not None:\n",
        "        # Checking if scheduler is ReduceLROnPlateau\n",
        "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
        "            # Updating the scheduler with test_loss\n",
        "            scheduler.step(test_loss)\n",
        "        else:\n",
        "            # Updating the scheduler without test_loss\n",
        "            scheduler.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1nE662UUZiE"
      },
      "source": [
        "Now for the actual training and evaluation loop,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "YAOjgKW3UZiF"
      },
      "outputs": [],
      "source": [
        "# Defining a function to train and evaluate a network\n",
        "def train_and_eval(net, loss_fn, optimizer, batch_size, n_epochs, scheduler, trial=None):\n",
        "    \"\"\"Trains and evaluates a network.\n",
        "\n",
        "    Args:\n",
        "        net (torch.nn.Module): The network to train and evaluate.\n",
        "        loss_fn (torch.nn.Module or function): The loss function.\n",
        "        optimizer (torch.optim.Optimizer): The optimizer.\n",
        "        batch_size (int): The batch size.\n",
        "        n_epochs (int): The number of epochs.\n",
        "        scheduler (torch.optim.lr_scheduler._LRScheduler or None): The learning rate scheduler.\n",
        "    Returns:\n",
        "        tuple: A tuple of (train_losses, test_losses, train_metrics, test_metrics), where\n",
        "            train_losses is a list of training losses for each epoch,\n",
        "            test_losses is a list of validation losses for each epoch,\n",
        "            train_metrics is a list of dictionaries containing training metrics for each epoch,\n",
        "            test_metrics is a list of dictionaries containing validation metrics for each epoch.\n",
        "    \"\"\"\n",
        "    # Creating data loaders for train and test sets\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.TensorDataset(x_test, y_test), batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Initializing lists to store the losses and metrics for each epoch\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    train_metrics = []\n",
        "    test_metrics = []\n",
        "\n",
        "    # Creating a SummaryWriter object to log data for tensorboard\n",
        "    writer = tbx.SummaryWriter()\n",
        "\n",
        "    # Looping over the epochs\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # Setting the network to training mode\n",
        "        net.train()\n",
        "\n",
        "        # Initializing variables to store the total loss and metrics for the train set\n",
        "        train_loss = 0.0\n",
        "        train_l1_norm = 0.0\n",
        "        train_linf_norm = 0.0\n",
        "\n",
        "        # Looping over the batches in the train set\n",
        "        for x_batch, y_batch in train_loader:\n",
        "\n",
        "            # Moving the batch tensors to the device\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            # Zeroing the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Performing a forward pass and computing the loss and metrics\n",
        "            y_pred = net(x_batch)\n",
        "            loss, l1_norm, linf_norm = compute_loss_and_metrics(\n",
        "                y_pred, y_batch, loss_fn\n",
        "            )\n",
        "\n",
        "\n",
        "            # Performing a backward pass and updating the weights\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Updating the total loss and metrics for the train set\n",
        "            train_loss += loss.item() * x_batch.size(0)\n",
        "            train_l1_norm += l1_norm.item() * x_batch.size(0)\n",
        "            train_linf_norm += linf_norm.item() * x_batch.size(0)\n",
        "\n",
        "        # Computing the average loss and metrics for the train set\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_l1_norm /= len(train_loader.dataset)\n",
        "        train_linf_norm /= len(train_loader.dataset)\n",
        "\n",
        "        # Appending the average loss and metrics for the train set to the lists\n",
        "        train_losses.append(train_loss)\n",
        "        train_metrics.append(\n",
        "            {\n",
        "                \"l1_norm\": train_l1_norm,\n",
        "                \"linf_norm\": train_linf_norm,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Logging the average loss and metrics for the train set to tensorboard\n",
        "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
        "        writer.add_scalar(\"L1 norm/train\", train_l1_norm, epoch)\n",
        "        writer.add_scalar(\"Linf norm/train\", train_linf_norm, epoch)\n",
        "\n",
        "        # Setting the network to evaluation mode\n",
        "        net.eval()\n",
        "\n",
        "        # Initializing variables to store the total loss and metrics for the test set\n",
        "        test_loss = 0.0\n",
        "        test_l1_norm = 0.0\n",
        "        test_linf_norm = 0.0\n",
        "\n",
        "        # Looping over the batches in the test set\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in test_loader:\n",
        "\n",
        "                # Moving the batch tensors to the device\n",
        "                x_batch = x_batch.to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "\n",
        "                # Performing a forward pass and computing the loss and metrics\n",
        "                y_pred = net(x_batch)\n",
        "                loss, l1_norm, linf_norm = compute_loss_and_metrics(\n",
        "                    y_pred, y_batch, loss_fn\n",
        "                )\n",
        "\n",
        "\n",
        "                # Updating the total loss and metrics for the test set\n",
        "                test_loss += loss.item() * x_batch.size(0)\n",
        "                test_l1_norm += l1_norm.item() * x_batch.size(0)\n",
        "                test_linf_norm += linf_norm.item() * x_batch.size(0)\n",
        "\n",
        "        # Computing the average loss and metrics for the test set\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        test_l1_norm /= len(test_loader.dataset)\n",
        "        test_linf_norm /= len(test_loader.dataset)\n",
        "\n",
        "        # Appending the average loss and metrics for the test set to the lists\n",
        "        test_losses.append(test_loss)\n",
        "        test_metrics.append(\n",
        "            {\n",
        "                \"l1_norm\": test_l1_norm,\n",
        "                \"linf_norm\": test_linf_norm,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Logging the average loss and metrics for the test set to tensorboard\n",
        "        writer.add_scalar(\"Loss/test\", test_loss, epoch)\n",
        "        writer.add_scalar(\"L1 norm/test\", test_l1_norm, epoch)\n",
        "        writer.add_scalar(\"Linf norm/test\", test_linf_norm, epoch)\n",
        "\n",
        "        # Printing the average loss and metrics for both sets for this epoch\n",
        "        print(\n",
        "            f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, \"\n",
        "            f\"Train L1 Norm: {train_l1_norm:.4f}, Test L1 Norm: {test_l1_norm:.4f}, \"\n",
        "            f\"Train Linf Norm: {train_linf_norm:.4f}, Test Linf Norm: {test_linf_norm:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Updating the learning rate scheduler with validation loss if applicable\n",
        "        update_scheduler(scheduler, test_loss)\n",
        "\n",
        "        # Reporting the intermediate metric value to Optuna if trial is not None\n",
        "        if trial is not None:\n",
        "            trial.report(test_metrics[-1][\"l1_norm\"], epoch)\n",
        "\n",
        "            # Checking if the trial should be pruned based on the intermediate value if trial is not None\n",
        "            if trial.should_prune():\n",
        "                raise optuna.TrialPruned()\n",
        "\n",
        "    # Closing the SummaryWriter object\n",
        "    writer.close()\n",
        "\n",
        "    # Returning the losses and metrics lists\n",
        "    return train_losses, test_losses, train_metrics, test_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg9jz0SvUZiQ"
      },
      "source": [
        "## The objective function and hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "fmRncQPuUZiR"
      },
      "outputs": [],
      "source": [
        "# Defining an objective function for Optuna to minimize\n",
        "def objective(trial):\n",
        "    \"\"\"Defines an objective function for Optuna to minimize.\n",
        "\n",
        "    Args:\n",
        "        trial (optuna.trial.Trial): The trial object that contains the hyperparameters.\n",
        "\n",
        "    Returns:\n",
        "        float: The validation L1 norm to minimize.\n",
        "    \"\"\"\n",
        "    # Creating a trial network and optimizer using the create_model function\n",
        "    net, \\\n",
        "    loss_fn, \\\n",
        "    optimizer, \\\n",
        "    batch_size, \\\n",
        "    n_epochs, \\\n",
        "    scheduler, \\\n",
        "    loss_name, \\\n",
        "    optimizer_name, \\\n",
        "    scheduler_name, \\\n",
        "    n_units, \\\n",
        "    n_layers, \\\n",
        "    hidden_activation, \\\n",
        "    output_activation, \\\n",
        "    lr = create_model(trial, optimize=True)\n",
        "\n",
        "    # Training and evaluating the network using the train_and_eval function\n",
        "    _, _, _, test_metrics = train_and_eval(\n",
        "        net, loss_fn, optimizer, batch_size, n_epochs, scheduler, trial\n",
        "    )\n",
        "\n",
        "    # Returning the last validation L1 norm as the objective value to minimize\n",
        "    return test_metrics[-1][\"l1_norm\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyES4NAyUZiS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47b6423d-0ff7-42f6-a938-16e3adfd1de3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 09:59:14,149]\u001b[0m A new study created in memory with name: no-name-736a4f99-2495-425b-b24d-e63883b1fada\u001b[0m\n",
            "<ipython-input-84-e8f6ba3279cc>:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "<ipython-input-84-e8f6ba3279cc>:121: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
            "<ipython-input-84-e8f6ba3279cc>:122: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  momentum = trial.suggest_uniform(\"momentum\", 0.0, 0.99)\n",
            "<ipython-input-84-e8f6ba3279cc>:157: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  factor = trial.suggest_uniform(\"factor\", 0.1, 0.5)\n",
            "<ipython-input-84-e8f6ba3279cc>:159: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  threshold = trial.suggest_loguniform(\"threshold\", 1e-4, 1e-2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.4420, Test Loss: 0.1745, Train L1 Norm: 0.3532, Test L1 Norm: 0.1043, Train Linf Norm: 9.5836, Test Linf Norm: 0.9691\n",
            "Epoch 2: Train Loss: 0.1889, Test Loss: 0.0520, Train L1 Norm: 0.1642, Test L1 Norm: 0.0438, Train Linf Norm: 4.9070, Test Linf Norm: 0.9201\n",
            "Epoch 3: Train Loss: 0.1627, Test Loss: 0.2275, Train L1 Norm: 0.1389, Test L1 Norm: 0.1331, Train Linf Norm: 4.1283, Test Linf Norm: 2.1668\n",
            "Epoch 4: Train Loss: 0.1509, Test Loss: 0.1073, Train L1 Norm: 0.1289, Test L1 Norm: 0.0532, Train Linf Norm: 3.9471, Test Linf Norm: 0.7232\n",
            "Epoch 5: Train Loss: 0.1430, Test Loss: 0.1072, Train L1 Norm: 0.1055, Test L1 Norm: 0.0701, Train Linf Norm: 2.7631, Test Linf Norm: 0.8141\n",
            "Epoch 6: Train Loss: 0.1321, Test Loss: 0.2095, Train L1 Norm: 0.1046, Test L1 Norm: 0.0960, Train Linf Norm: 3.0164, Test Linf Norm: 1.2267\n",
            "Epoch 7: Train Loss: 0.1260, Test Loss: 0.3000, Train L1 Norm: 0.0932, Test L1 Norm: 0.1262, Train Linf Norm: 2.4621, Test Linf Norm: 0.9018\n",
            "Epoch 8: Train Loss: 0.1196, Test Loss: 0.2065, Train L1 Norm: 0.0840, Test L1 Norm: 0.1009, Train Linf Norm: 2.1127, Test Linf Norm: 1.3945\n",
            "Epoch 9: Train Loss: 0.1217, Test Loss: 0.0502, Train L1 Norm: 0.0774, Test L1 Norm: 0.0411, Train Linf Norm: 1.6657, Test Linf Norm: 0.6877\n",
            "Epoch 10: Train Loss: 0.1145, Test Loss: 0.0836, Train L1 Norm: 0.0825, Test L1 Norm: 0.0427, Train Linf Norm: 2.2040, Test Linf Norm: 0.6138\n",
            "Epoch 11: Train Loss: 0.1188, Test Loss: 0.0406, Train L1 Norm: 0.0780, Test L1 Norm: 0.0283, Train Linf Norm: 1.8413, Test Linf Norm: 0.5518\n",
            "Epoch 12: Train Loss: 0.1146, Test Loss: 0.0636, Train L1 Norm: 0.0825, Test L1 Norm: 0.0385, Train Linf Norm: 2.2625, Test Linf Norm: 0.6185\n",
            "Epoch 13: Train Loss: 0.1035, Test Loss: 0.3245, Train L1 Norm: 0.0747, Test L1 Norm: 0.1324, Train Linf Norm: 2.0139, Test Linf Norm: 0.8814\n",
            "Epoch 14: Train Loss: 0.1084, Test Loss: 0.0699, Train L1 Norm: 0.0720, Test L1 Norm: 0.0384, Train Linf Norm: 1.7816, Test Linf Norm: 0.5926\n",
            "Epoch 15: Train Loss: 0.1032, Test Loss: 0.1589, Train L1 Norm: 0.0622, Test L1 Norm: 0.1025, Train Linf Norm: 1.2956, Test Linf Norm: 1.8405\n",
            "Epoch 16: Train Loss: 0.1066, Test Loss: 0.1354, Train L1 Norm: 0.0747, Test L1 Norm: 0.0762, Train Linf Norm: 2.0102, Test Linf Norm: 1.2518\n",
            "Epoch 17: Train Loss: 0.1026, Test Loss: 0.1440, Train L1 Norm: 0.0666, Test L1 Norm: 0.0655, Train Linf Norm: 1.6500, Test Linf Norm: 0.9257\n",
            "Epoch 18: Train Loss: 0.1048, Test Loss: 0.1021, Train L1 Norm: 0.0613, Test L1 Norm: 0.0541, Train Linf Norm: 1.2327, Test Linf Norm: 0.9305\n",
            "Epoch 19: Train Loss: 0.0244, Test Loss: 0.0295, Train L1 Norm: 0.0358, Test L1 Norm: 0.0214, Train Linf Norm: 1.3997, Test Linf Norm: 0.4860\n",
            "Epoch 20: Train Loss: 0.0236, Test Loss: 0.0136, Train L1 Norm: 0.0336, Test L1 Norm: 0.0171, Train Linf Norm: 1.2827, Test Linf Norm: 0.4462\n",
            "Epoch 21: Train Loss: 0.0242, Test Loss: 0.0294, Train L1 Norm: 0.0334, Test L1 Norm: 0.0204, Train Linf Norm: 1.2730, Test Linf Norm: 0.4140\n",
            "Epoch 22: Train Loss: 0.0244, Test Loss: 0.0563, Train L1 Norm: 0.0321, Test L1 Norm: 0.0299, Train Linf Norm: 1.1870, Test Linf Norm: 0.5107\n",
            "Epoch 23: Train Loss: 0.0245, Test Loss: 0.0222, Train L1 Norm: 0.0327, Test L1 Norm: 0.0199, Train Linf Norm: 1.2407, Test Linf Norm: 0.4626\n",
            "Epoch 24: Train Loss: 0.0244, Test Loss: 0.0381, Train L1 Norm: 0.0331, Test L1 Norm: 0.0214, Train Linf Norm: 1.2739, Test Linf Norm: 0.4111\n",
            "Epoch 25: Train Loss: 0.0254, Test Loss: 0.0191, Train L1 Norm: 0.0327, Test L1 Norm: 0.0177, Train Linf Norm: 1.2328, Test Linf Norm: 0.4425\n",
            "Epoch 26: Train Loss: 0.0255, Test Loss: 0.0307, Train L1 Norm: 0.0342, Test L1 Norm: 0.0210, Train Linf Norm: 1.3184, Test Linf Norm: 0.4553\n",
            "Epoch 27: Train Loss: 0.0266, Test Loss: 0.0132, Train L1 Norm: 0.0348, Test L1 Norm: 0.0159, Train Linf Norm: 1.3399, Test Linf Norm: 0.4344\n",
            "Epoch 28: Train Loss: 0.0254, Test Loss: 0.0276, Train L1 Norm: 0.0315, Test L1 Norm: 0.0206, Train Linf Norm: 1.1599, Test Linf Norm: 0.4350\n",
            "Epoch 29: Train Loss: 0.0249, Test Loss: 0.0523, Train L1 Norm: 0.0341, Test L1 Norm: 0.0303, Train Linf Norm: 1.3517, Test Linf Norm: 0.5192\n",
            "Epoch 30: Train Loss: 0.0255, Test Loss: 0.0184, Train L1 Norm: 0.0324, Test L1 Norm: 0.0168, Train Linf Norm: 1.2268, Test Linf Norm: 0.4052\n",
            "Epoch 31: Train Loss: 0.0261, Test Loss: 0.0483, Train L1 Norm: 0.0346, Test L1 Norm: 0.0254, Train Linf Norm: 1.3588, Test Linf Norm: 0.4317\n",
            "Epoch 32: Train Loss: 0.0259, Test Loss: 0.0280, Train L1 Norm: 0.0349, Test L1 Norm: 0.0198, Train Linf Norm: 1.3816, Test Linf Norm: 0.4121\n",
            "Epoch 33: Train Loss: 0.0268, Test Loss: 0.0264, Train L1 Norm: 0.0328, Test L1 Norm: 0.0196, Train Linf Norm: 1.2343, Test Linf Norm: 0.4298\n",
            "Epoch 34: Train Loss: 0.0252, Test Loss: 0.0182, Train L1 Norm: 0.0320, Test L1 Norm: 0.0170, Train Linf Norm: 1.2291, Test Linf Norm: 0.4188\n",
            "Epoch 35: Train Loss: 0.0092, Test Loss: 0.0092, Train L1 Norm: 0.0271, Test L1 Norm: 0.0141, Train Linf Norm: 1.2228, Test Linf Norm: 0.4088\n",
            "Epoch 36: Train Loss: 0.0090, Test Loss: 0.0082, Train L1 Norm: 0.0274, Test L1 Norm: 0.0138, Train Linf Norm: 1.2422, Test Linf Norm: 0.4024\n",
            "Epoch 37: Train Loss: 0.0089, Test Loss: 0.0086, Train L1 Norm: 0.0272, Test L1 Norm: 0.0138, Train Linf Norm: 1.2323, Test Linf Norm: 0.4064\n",
            "Epoch 38: Train Loss: 0.0087, Test Loss: 0.0078, Train L1 Norm: 0.0275, Test L1 Norm: 0.0138, Train Linf Norm: 1.2533, Test Linf Norm: 0.4107\n",
            "Epoch 39: Train Loss: 0.0087, Test Loss: 0.0081, Train L1 Norm: 0.0270, Test L1 Norm: 0.0137, Train Linf Norm: 1.2277, Test Linf Norm: 0.4067\n",
            "Epoch 40: Train Loss: 0.0087, Test Loss: 0.0076, Train L1 Norm: 0.0270, Test L1 Norm: 0.0136, Train Linf Norm: 1.2200, Test Linf Norm: 0.4069\n",
            "Epoch 41: Train Loss: 0.0086, Test Loss: 0.0077, Train L1 Norm: 0.0270, Test L1 Norm: 0.0136, Train Linf Norm: 1.2274, Test Linf Norm: 0.4053\n",
            "Epoch 42: Train Loss: 0.0085, Test Loss: 0.0085, Train L1 Norm: 0.0267, Test L1 Norm: 0.0142, Train Linf Norm: 1.2154, Test Linf Norm: 0.4168\n",
            "Epoch 43: Train Loss: 0.0086, Test Loss: 0.0095, Train L1 Norm: 0.0267, Test L1 Norm: 0.0139, Train Linf Norm: 1.2080, Test Linf Norm: 0.4036\n",
            "Epoch 44: Train Loss: 0.0084, Test Loss: 0.0077, Train L1 Norm: 0.0271, Test L1 Norm: 0.0136, Train Linf Norm: 1.2390, Test Linf Norm: 0.4049\n",
            "Epoch 45: Train Loss: 0.0084, Test Loss: 0.0078, Train L1 Norm: 0.0266, Test L1 Norm: 0.0136, Train Linf Norm: 1.2078, Test Linf Norm: 0.4023\n",
            "Epoch 46: Train Loss: 0.0083, Test Loss: 0.0094, Train L1 Norm: 0.0265, Test L1 Norm: 0.0138, Train Linf Norm: 1.2063, Test Linf Norm: 0.4012\n",
            "Epoch 47: Train Loss: 0.0083, Test Loss: 0.0092, Train L1 Norm: 0.0270, Test L1 Norm: 0.0138, Train Linf Norm: 1.2390, Test Linf Norm: 0.4010\n",
            "Epoch 48: Train Loss: 0.0071, Test Loss: 0.0072, Train L1 Norm: 0.0264, Test L1 Norm: 0.0134, Train Linf Norm: 1.2200, Test Linf Norm: 0.4058\n",
            "Epoch 49: Train Loss: 0.0070, Test Loss: 0.0073, Train L1 Norm: 0.0264, Test L1 Norm: 0.0134, Train Linf Norm: 1.2280, Test Linf Norm: 0.4061\n",
            "Epoch 50: Train Loss: 0.0070, Test Loss: 0.0071, Train L1 Norm: 0.0265, Test L1 Norm: 0.0134, Train Linf Norm: 1.2270, Test Linf Norm: 0.4065\n",
            "Epoch 51: Train Loss: 0.0070, Test Loss: 0.0074, Train L1 Norm: 0.0264, Test L1 Norm: 0.0135, Train Linf Norm: 1.2214, Test Linf Norm: 0.4020\n",
            "Epoch 52: Train Loss: 0.0070, Test Loss: 0.0071, Train L1 Norm: 0.0264, Test L1 Norm: 0.0134, Train Linf Norm: 1.2211, Test Linf Norm: 0.4048\n",
            "Epoch 53: Train Loss: 0.0070, Test Loss: 0.0072, Train L1 Norm: 0.0266, Test L1 Norm: 0.0134, Train Linf Norm: 1.2319, Test Linf Norm: 0.4048\n",
            "Epoch 54: Train Loss: 0.0070, Test Loss: 0.0071, Train L1 Norm: 0.0261, Test L1 Norm: 0.0134, Train Linf Norm: 1.2075, Test Linf Norm: 0.4059\n",
            "Epoch 55: Train Loss: 0.0069, Test Loss: 0.0072, Train L1 Norm: 0.0262, Test L1 Norm: 0.0134, Train Linf Norm: 1.2180, Test Linf Norm: 0.4031\n",
            "Epoch 56: Train Loss: 0.0069, Test Loss: 0.0070, Train L1 Norm: 0.0261, Test L1 Norm: 0.0133, Train Linf Norm: 1.2022, Test Linf Norm: 0.4020\n",
            "Epoch 57: Train Loss: 0.0069, Test Loss: 0.0071, Train L1 Norm: 0.0262, Test L1 Norm: 0.0134, Train Linf Norm: 1.2135, Test Linf Norm: 0.4044\n",
            "Epoch 58: Train Loss: 0.0069, Test Loss: 0.0071, Train L1 Norm: 0.0262, Test L1 Norm: 0.0134, Train Linf Norm: 1.2150, Test Linf Norm: 0.4064\n",
            "Epoch 59: Train Loss: 0.0069, Test Loss: 0.0071, Train L1 Norm: 0.0262, Test L1 Norm: 0.0134, Train Linf Norm: 1.2121, Test Linf Norm: 0.4056\n",
            "Epoch 60: Train Loss: 0.0068, Test Loss: 0.0071, Train L1 Norm: 0.0261, Test L1 Norm: 0.0133, Train Linf Norm: 1.2096, Test Linf Norm: 0.4038\n",
            "Epoch 61: Train Loss: 0.0068, Test Loss: 0.0071, Train L1 Norm: 0.0262, Test L1 Norm: 0.0134, Train Linf Norm: 1.2132, Test Linf Norm: 0.4064\n",
            "Epoch 62: Train Loss: 0.0068, Test Loss: 0.0071, Train L1 Norm: 0.0262, Test L1 Norm: 0.0133, Train Linf Norm: 1.2151, Test Linf Norm: 0.4049\n",
            "Epoch 63: Train Loss: 0.0068, Test Loss: 0.0070, Train L1 Norm: 0.0263, Test L1 Norm: 0.0133, Train Linf Norm: 1.2247, Test Linf Norm: 0.4049\n",
            "Epoch 64: Train Loss: 0.0068, Test Loss: 0.0069, Train L1 Norm: 0.0262, Test L1 Norm: 0.0133, Train Linf Norm: 1.2137, Test Linf Norm: 0.4029\n",
            "Epoch 65: Train Loss: 0.0068, Test Loss: 0.0070, Train L1 Norm: 0.0259, Test L1 Norm: 0.0133, Train Linf Norm: 1.2052, Test Linf Norm: 0.4037\n",
            "Epoch 66: Train Loss: 0.0068, Test Loss: 0.0071, Train L1 Norm: 0.0262, Test L1 Norm: 0.0133, Train Linf Norm: 1.2246, Test Linf Norm: 0.4049\n",
            "Epoch 67: Train Loss: 0.0068, Test Loss: 0.0073, Train L1 Norm: 0.0260, Test L1 Norm: 0.0134, Train Linf Norm: 1.2003, Test Linf Norm: 0.4065\n",
            "Epoch 68: Train Loss: 0.0068, Test Loss: 0.0069, Train L1 Norm: 0.0262, Test L1 Norm: 0.0132, Train Linf Norm: 1.2156, Test Linf Norm: 0.4020\n",
            "Epoch 69: Train Loss: 0.0067, Test Loss: 0.0069, Train L1 Norm: 0.0261, Test L1 Norm: 0.0133, Train Linf Norm: 1.2141, Test Linf Norm: 0.4030\n",
            "Epoch 70: Train Loss: 0.0067, Test Loss: 0.0068, Train L1 Norm: 0.0262, Test L1 Norm: 0.0133, Train Linf Norm: 1.2205, Test Linf Norm: 0.4050\n",
            "Epoch 71: Train Loss: 0.0067, Test Loss: 0.0074, Train L1 Norm: 0.0263, Test L1 Norm: 0.0134, Train Linf Norm: 1.2259, Test Linf Norm: 0.4075\n",
            "Epoch 72: Train Loss: 0.0067, Test Loss: 0.0069, Train L1 Norm: 0.0262, Test L1 Norm: 0.0132, Train Linf Norm: 1.2201, Test Linf Norm: 0.4009\n",
            "Epoch 73: Train Loss: 0.0067, Test Loss: 0.0069, Train L1 Norm: 0.0264, Test L1 Norm: 0.0132, Train Linf Norm: 1.2340, Test Linf Norm: 0.4014\n",
            "Epoch 74: Train Loss: 0.0067, Test Loss: 0.0068, Train L1 Norm: 0.0259, Test L1 Norm: 0.0132, Train Linf Norm: 1.2059, Test Linf Norm: 0.4015\n",
            "Epoch 75: Train Loss: 0.0067, Test Loss: 0.0069, Train L1 Norm: 0.0261, Test L1 Norm: 0.0133, Train Linf Norm: 1.2247, Test Linf Norm: 0.4015\n",
            "Epoch 76: Train Loss: 0.0066, Test Loss: 0.0068, Train L1 Norm: 0.0261, Test L1 Norm: 0.0131, Train Linf Norm: 1.2152, Test Linf Norm: 0.3991\n",
            "Epoch 77: Train Loss: 0.0067, Test Loss: 0.0069, Train L1 Norm: 0.0261, Test L1 Norm: 0.0132, Train Linf Norm: 1.2159, Test Linf Norm: 0.4011\n",
            "Epoch 78: Train Loss: 0.0067, Test Loss: 0.0067, Train L1 Norm: 0.0261, Test L1 Norm: 0.0131, Train Linf Norm: 1.2218, Test Linf Norm: 0.4001\n",
            "Epoch 79: Train Loss: 0.0066, Test Loss: 0.0075, Train L1 Norm: 0.0259, Test L1 Norm: 0.0134, Train Linf Norm: 1.2026, Test Linf Norm: 0.4056\n",
            "Epoch 80: Train Loss: 0.0066, Test Loss: 0.0068, Train L1 Norm: 0.0260, Test L1 Norm: 0.0132, Train Linf Norm: 1.2188, Test Linf Norm: 0.4024\n",
            "Epoch 81: Train Loss: 0.0066, Test Loss: 0.0067, Train L1 Norm: 0.0262, Test L1 Norm: 0.0131, Train Linf Norm: 1.2295, Test Linf Norm: 0.4014\n",
            "Epoch 82: Train Loss: 0.0066, Test Loss: 0.0070, Train L1 Norm: 0.0261, Test L1 Norm: 0.0132, Train Linf Norm: 1.2192, Test Linf Norm: 0.4017\n",
            "Epoch 83: Train Loss: 0.0066, Test Loss: 0.0067, Train L1 Norm: 0.0260, Test L1 Norm: 0.0131, Train Linf Norm: 1.2122, Test Linf Norm: 0.4006\n",
            "Epoch 84: Train Loss: 0.0066, Test Loss: 0.0069, Train L1 Norm: 0.0262, Test L1 Norm: 0.0131, Train Linf Norm: 1.2321, Test Linf Norm: 0.3986\n",
            "Epoch 85: Train Loss: 0.0066, Test Loss: 0.0068, Train L1 Norm: 0.0260, Test L1 Norm: 0.0131, Train Linf Norm: 1.2126, Test Linf Norm: 0.4010\n",
            "Epoch 86: Train Loss: 0.0066, Test Loss: 0.0067, Train L1 Norm: 0.0261, Test L1 Norm: 0.0131, Train Linf Norm: 1.2166, Test Linf Norm: 0.4015\n",
            "Epoch 87: Train Loss: 0.0066, Test Loss: 0.0068, Train L1 Norm: 0.0261, Test L1 Norm: 0.0131, Train Linf Norm: 1.2290, Test Linf Norm: 0.3997\n",
            "Epoch 88: Train Loss: 0.0065, Test Loss: 0.0067, Train L1 Norm: 0.0259, Test L1 Norm: 0.0131, Train Linf Norm: 1.2029, Test Linf Norm: 0.4004\n",
            "Epoch 89: Train Loss: 0.0065, Test Loss: 0.0066, Train L1 Norm: 0.0260, Test L1 Norm: 0.0131, Train Linf Norm: 1.2128, Test Linf Norm: 0.4012\n",
            "Epoch 90: Train Loss: 0.0065, Test Loss: 0.0067, Train L1 Norm: 0.0259, Test L1 Norm: 0.0131, Train Linf Norm: 1.2073, Test Linf Norm: 0.3999\n",
            "Epoch 91: Train Loss: 0.0065, Test Loss: 0.0066, Train L1 Norm: 0.0257, Test L1 Norm: 0.0130, Train Linf Norm: 1.1930, Test Linf Norm: 0.3949\n",
            "Epoch 92: Train Loss: 0.0065, Test Loss: 0.0066, Train L1 Norm: 0.0260, Test L1 Norm: 0.0130, Train Linf Norm: 1.2176, Test Linf Norm: 0.4012\n",
            "Epoch 93: Train Loss: 0.0065, Test Loss: 0.0077, Train L1 Norm: 0.0259, Test L1 Norm: 0.0134, Train Linf Norm: 1.2125, Test Linf Norm: 0.4052\n",
            "Epoch 94: Train Loss: 0.0065, Test Loss: 0.0067, Train L1 Norm: 0.0259, Test L1 Norm: 0.0130, Train Linf Norm: 1.2105, Test Linf Norm: 0.3985\n",
            "Epoch 95: Train Loss: 0.0065, Test Loss: 0.0066, Train L1 Norm: 0.0259, Test L1 Norm: 0.0130, Train Linf Norm: 1.2092, Test Linf Norm: 0.3962\n",
            "Epoch 96: Train Loss: 0.0064, Test Loss: 0.0067, Train L1 Norm: 0.0259, Test L1 Norm: 0.0130, Train Linf Norm: 1.2161, Test Linf Norm: 0.3932\n",
            "Epoch 97: Train Loss: 0.0064, Test Loss: 0.0068, Train L1 Norm: 0.0255, Test L1 Norm: 0.0131, Train Linf Norm: 1.1934, Test Linf Norm: 0.3986\n",
            "Epoch 98: Train Loss: 0.0064, Test Loss: 0.0067, Train L1 Norm: 0.0259, Test L1 Norm: 0.0130, Train Linf Norm: 1.2130, Test Linf Norm: 0.3965\n",
            "Epoch 99: Train Loss: 0.0064, Test Loss: 0.0067, Train L1 Norm: 0.0257, Test L1 Norm: 0.0130, Train Linf Norm: 1.1977, Test Linf Norm: 0.3974\n",
            "Epoch 100: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0130, Train Linf Norm: 1.2006, Test Linf Norm: 0.3979\n",
            "Epoch 101: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0256, Test L1 Norm: 0.0129, Train Linf Norm: 1.2007, Test Linf Norm: 0.3984\n",
            "Epoch 102: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0256, Test L1 Norm: 0.0130, Train Linf Norm: 1.2036, Test Linf Norm: 0.3986\n",
            "Epoch 103: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0258, Test L1 Norm: 0.0130, Train Linf Norm: 1.2131, Test Linf Norm: 0.3985\n",
            "Epoch 104: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0130, Train Linf Norm: 1.2093, Test Linf Norm: 0.3987\n",
            "Epoch 105: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0130, Train Linf Norm: 1.2044, Test Linf Norm: 0.3981\n",
            "Epoch 106: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2050, Test Linf Norm: 0.3976\n",
            "Epoch 107: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2042, Test Linf Norm: 0.3977\n",
            "Epoch 108: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2101, Test Linf Norm: 0.3979\n",
            "Epoch 109: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2066, Test Linf Norm: 0.3985\n",
            "Epoch 110: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0130, Train Linf Norm: 1.2096, Test Linf Norm: 0.3985\n",
            "Epoch 111: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2068, Test Linf Norm: 0.3982\n",
            "Epoch 112: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2134, Test Linf Norm: 0.3981\n",
            "Epoch 113: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2046, Test Linf Norm: 0.3981\n",
            "Epoch 114: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2025, Test Linf Norm: 0.3987\n",
            "Epoch 115: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2026, Test Linf Norm: 0.3986\n",
            "Epoch 116: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2056, Test Linf Norm: 0.3984\n",
            "Epoch 117: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2100, Test Linf Norm: 0.3985\n",
            "Epoch 118: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2029, Test Linf Norm: 0.3983\n",
            "Epoch 119: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2124, Test Linf Norm: 0.3982\n",
            "Epoch 120: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2119, Test Linf Norm: 0.3979\n",
            "Epoch 121: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.1986, Test Linf Norm: 0.3985\n",
            "Epoch 122: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.1967, Test Linf Norm: 0.3981\n",
            "Epoch 123: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2022, Test Linf Norm: 0.3983\n",
            "Epoch 124: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2097, Test Linf Norm: 0.3983\n",
            "Epoch 125: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2084, Test Linf Norm: 0.3982\n",
            "Epoch 126: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2035, Test Linf Norm: 0.3984\n",
            "Epoch 127: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2101, Test Linf Norm: 0.3983\n",
            "Epoch 128: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2095, Test Linf Norm: 0.3982\n",
            "Epoch 129: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2090, Test Linf Norm: 0.3981\n",
            "Epoch 130: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2083, Test Linf Norm: 0.3983\n",
            "Epoch 131: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2056, Test Linf Norm: 0.3983\n",
            "Epoch 132: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2104, Test Linf Norm: 0.3983\n",
            "Epoch 133: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2058, Test Linf Norm: 0.3983\n",
            "Epoch 134: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2104, Test Linf Norm: 0.3983\n",
            "Epoch 135: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2069, Test Linf Norm: 0.3982\n",
            "Epoch 136: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2051, Test Linf Norm: 0.3983\n",
            "Epoch 137: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2083, Test Linf Norm: 0.3983\n",
            "Epoch 138: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2035, Test Linf Norm: 0.3983\n",
            "Epoch 139: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2113, Test Linf Norm: 0.3983\n",
            "Epoch 140: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2084, Test Linf Norm: 0.3982\n",
            "Epoch 141: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2045, Test Linf Norm: 0.3983\n",
            "Epoch 142: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2058, Test Linf Norm: 0.3982\n",
            "Epoch 143: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2038, Test Linf Norm: 0.3983\n",
            "Epoch 144: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.1998, Test Linf Norm: 0.3983\n",
            "Epoch 145: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2041, Test Linf Norm: 0.3982\n",
            "Epoch 146: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2073, Test Linf Norm: 0.3983\n",
            "Epoch 147: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2032, Test Linf Norm: 0.3983\n",
            "Epoch 148: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2071, Test Linf Norm: 0.3982\n",
            "Epoch 149: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2135, Test Linf Norm: 0.3983\n",
            "Epoch 150: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.1978, Test Linf Norm: 0.3983\n",
            "Epoch 151: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2044, Test Linf Norm: 0.3982\n",
            "Epoch 152: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2091, Test Linf Norm: 0.3982\n",
            "Epoch 153: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2103, Test Linf Norm: 0.3983\n",
            "Epoch 154: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2093, Test Linf Norm: 0.3983\n",
            "Epoch 155: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2055, Test Linf Norm: 0.3983\n",
            "Epoch 156: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2085, Test Linf Norm: 0.3982\n",
            "Epoch 157: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2097, Test Linf Norm: 0.3983\n",
            "Epoch 158: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2101, Test Linf Norm: 0.3983\n",
            "Epoch 159: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2018, Test Linf Norm: 0.3983\n",
            "Epoch 160: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2073, Test Linf Norm: 0.3983\n",
            "Epoch 161: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2114, Test Linf Norm: 0.3983\n",
            "Epoch 162: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2126, Test Linf Norm: 0.3983\n",
            "Epoch 163: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2015, Test Linf Norm: 0.3983\n",
            "Epoch 164: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2020, Test Linf Norm: 0.3983\n",
            "Epoch 165: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2046, Test Linf Norm: 0.3983\n",
            "Epoch 166: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2122, Test Linf Norm: 0.3982\n",
            "Epoch 167: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2053, Test Linf Norm: 0.3983\n",
            "Epoch 168: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2132, Test Linf Norm: 0.3983\n",
            "Epoch 169: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2024, Test Linf Norm: 0.3982\n",
            "Epoch 170: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2069, Test Linf Norm: 0.3983\n",
            "Epoch 171: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2066, Test Linf Norm: 0.3983\n",
            "Epoch 172: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2095, Test Linf Norm: 0.3983\n",
            "Epoch 173: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2052, Test Linf Norm: 0.3983\n",
            "Epoch 174: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2104, Test Linf Norm: 0.3983\n",
            "Epoch 175: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2118, Test Linf Norm: 0.3982\n",
            "Epoch 176: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2142, Test Linf Norm: 0.3983\n",
            "Epoch 177: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2038, Test Linf Norm: 0.3983\n",
            "Epoch 178: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2066, Test Linf Norm: 0.3983\n",
            "Epoch 179: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2087, Test Linf Norm: 0.3983\n",
            "Epoch 180: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2088, Test Linf Norm: 0.3983\n",
            "Epoch 181: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2029, Test Linf Norm: 0.3983\n",
            "Epoch 182: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2092, Test Linf Norm: 0.3983\n",
            "Epoch 183: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2068, Test Linf Norm: 0.3983\n",
            "Epoch 184: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2094, Test Linf Norm: 0.3982\n",
            "Epoch 185: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2097, Test Linf Norm: 0.3982\n",
            "Epoch 186: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.1986, Test Linf Norm: 0.3982\n",
            "Epoch 187: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2061, Test Linf Norm: 0.3983\n",
            "Epoch 188: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2041, Test Linf Norm: 0.3983\n",
            "Epoch 189: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2144, Test Linf Norm: 0.3983\n",
            "Epoch 190: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2020, Test Linf Norm: 0.3983\n",
            "Epoch 191: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2108, Test Linf Norm: 0.3983\n",
            "Epoch 192: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2056, Test Linf Norm: 0.3983\n",
            "Epoch 193: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2176, Test Linf Norm: 0.3983\n",
            "Epoch 194: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2046, Test Linf Norm: 0.3983\n",
            "Epoch 195: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2098, Test Linf Norm: 0.3983\n",
            "Epoch 196: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2063, Test Linf Norm: 0.3982\n",
            "Epoch 197: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2041, Test Linf Norm: 0.3982\n",
            "Epoch 198: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2062, Test Linf Norm: 0.3982\n",
            "Epoch 199: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2089, Test Linf Norm: 0.3983\n",
            "Epoch 200: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2096, Test Linf Norm: 0.3982\n",
            "Epoch 201: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2034, Test Linf Norm: 0.3983\n",
            "Epoch 202: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2026, Test Linf Norm: 0.3983\n",
            "Epoch 203: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.1965, Test Linf Norm: 0.3982\n",
            "Epoch 204: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2014, Test Linf Norm: 0.3982\n",
            "Epoch 205: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2063, Test Linf Norm: 0.3983\n",
            "Epoch 206: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2062, Test Linf Norm: 0.3982\n",
            "Epoch 207: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2096, Test Linf Norm: 0.3982\n",
            "Epoch 208: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2084, Test Linf Norm: 0.3982\n",
            "Epoch 209: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2067, Test Linf Norm: 0.3982\n",
            "Epoch 210: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2022, Test Linf Norm: 0.3982\n",
            "Epoch 211: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2071, Test Linf Norm: 0.3982\n",
            "Epoch 212: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2072, Test Linf Norm: 0.3983\n",
            "Epoch 213: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2072, Test Linf Norm: 0.3983\n",
            "Epoch 214: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2041, Test Linf Norm: 0.3982\n",
            "Epoch 215: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2119, Test Linf Norm: 0.3983\n",
            "Epoch 216: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2065, Test Linf Norm: 0.3983\n",
            "Epoch 217: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2026, Test Linf Norm: 0.3982\n",
            "Epoch 218: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2050, Test Linf Norm: 0.3982\n",
            "Epoch 219: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2132, Test Linf Norm: 0.3982\n",
            "Epoch 220: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2090, Test Linf Norm: 0.3982\n",
            "Epoch 221: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2036, Test Linf Norm: 0.3982\n",
            "Epoch 222: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2061, Test Linf Norm: 0.3982\n",
            "Epoch 223: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2067, Test Linf Norm: 0.3982\n",
            "Epoch 224: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2043, Test Linf Norm: 0.3982\n",
            "Epoch 225: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2039, Test Linf Norm: 0.3982\n",
            "Epoch 226: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2006, Test Linf Norm: 0.3982\n",
            "Epoch 227: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2121, Test Linf Norm: 0.3982\n",
            "Epoch 228: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2033, Test Linf Norm: 0.3982\n",
            "Epoch 229: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2063, Test Linf Norm: 0.3982\n",
            "Epoch 230: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2103, Test Linf Norm: 0.3982\n",
            "Epoch 231: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2061, Test Linf Norm: 0.3982\n",
            "Epoch 232: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2111, Test Linf Norm: 0.3982\n",
            "Epoch 233: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2122, Test Linf Norm: 0.3982\n",
            "Epoch 234: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2068, Test Linf Norm: 0.3982\n",
            "Epoch 235: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2030, Test Linf Norm: 0.3982\n",
            "Epoch 236: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2116, Test Linf Norm: 0.3982\n",
            "Epoch 237: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2068, Test Linf Norm: 0.3982\n",
            "Epoch 238: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2066, Test Linf Norm: 0.3982\n",
            "Epoch 239: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2059, Test Linf Norm: 0.3982\n",
            "Epoch 240: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2038, Test Linf Norm: 0.3982\n",
            "Epoch 241: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2103, Test Linf Norm: 0.3982\n",
            "Epoch 242: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2067, Test Linf Norm: 0.3982\n",
            "Epoch 243: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2072, Test Linf Norm: 0.3982\n",
            "Epoch 244: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2093, Test Linf Norm: 0.3982\n",
            "Epoch 245: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2041, Test Linf Norm: 0.3982\n",
            "Epoch 246: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2096, Test Linf Norm: 0.3982\n",
            "Epoch 247: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2058, Test Linf Norm: 0.3982\n",
            "Epoch 248: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2058, Test Linf Norm: 0.3982\n",
            "Epoch 249: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2062, Test Linf Norm: 0.3982\n",
            "Epoch 250: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2140, Test Linf Norm: 0.3982\n",
            "Epoch 251: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2025, Test Linf Norm: 0.3982\n",
            "Epoch 252: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2100, Test Linf Norm: 0.3982\n",
            "Epoch 253: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2069, Test Linf Norm: 0.3982\n",
            "Epoch 254: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2045, Test Linf Norm: 0.3982\n",
            "Epoch 255: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2004, Test Linf Norm: 0.3982\n",
            "Epoch 256: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2089, Test Linf Norm: 0.3982\n",
            "Epoch 257: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2043, Test Linf Norm: 0.3982\n",
            "Epoch 258: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2028, Test Linf Norm: 0.3982\n",
            "Epoch 259: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2125, Test Linf Norm: 0.3982\n",
            "Epoch 260: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2060, Test Linf Norm: 0.3982\n",
            "Epoch 261: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2061, Test Linf Norm: 0.3982\n",
            "Epoch 262: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2060, Test Linf Norm: 0.3982\n",
            "Epoch 263: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2006, Test Linf Norm: 0.3982\n",
            "Epoch 264: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2078, Test Linf Norm: 0.3982\n",
            "Epoch 265: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2064, Test Linf Norm: 0.3982\n",
            "Epoch 266: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2157, Test Linf Norm: 0.3982\n",
            "Epoch 267: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2063, Test Linf Norm: 0.3982\n",
            "Epoch 268: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2108, Test Linf Norm: 0.3982\n",
            "Epoch 269: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2073, Test Linf Norm: 0.3982\n",
            "Epoch 270: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2034, Test Linf Norm: 0.3982\n",
            "Epoch 271: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.1976, Test Linf Norm: 0.3982\n",
            "Epoch 272: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2059, Test Linf Norm: 0.3982\n",
            "Epoch 273: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2079, Test Linf Norm: 0.3982\n",
            "Epoch 274: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2133, Test Linf Norm: 0.3982\n",
            "Epoch 275: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2103, Test Linf Norm: 0.3982\n",
            "Epoch 276: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2070, Test Linf Norm: 0.3982\n",
            "Epoch 277: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2101, Test Linf Norm: 0.3982\n",
            "Epoch 278: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2072, Test Linf Norm: 0.3982\n",
            "Epoch 279: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2060, Test Linf Norm: 0.3982\n",
            "Epoch 280: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2025, Test Linf Norm: 0.3982\n",
            "Epoch 281: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2029, Test Linf Norm: 0.3982\n",
            "Epoch 282: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2036, Test Linf Norm: 0.3982\n",
            "Epoch 283: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2036, Test Linf Norm: 0.3982\n",
            "Epoch 284: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2120, Test Linf Norm: 0.3982\n",
            "Epoch 285: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2002, Test Linf Norm: 0.3982\n",
            "Epoch 286: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2094, Test Linf Norm: 0.3982\n",
            "Epoch 287: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2123, Test Linf Norm: 0.3982\n",
            "Epoch 288: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2044, Test Linf Norm: 0.3982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 10:14:07,549]\u001b[0m Trial 0 finished with value: 0.012935786840319633 and parameters: {'n_layers': 2, 'n_units_0': 773, 'n_units_1': 1987, 'hidden_activation': 'LeakyReLU', 'output_activation': 'ReLU', 'loss': 'MAE', 'optimizer': 'SGD', 'lr': 0.00040663822157222863, 'batch_size': 64, 'n_epochs': 289, 'scheduler': 'ReduceLROnPlateau', 'weight_decay': 1.029492206702847e-05, 'momentum': 0.5516991905726143, 'factor': 0.26653830096536435, 'patience': 6, 'threshold': 0.0032203718974128173}. Best is trial 0 with value: 0.012935786840319633.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 289: Train Loss: 0.0062, Test Loss: 0.0065, Train L1 Norm: 0.0257, Test L1 Norm: 0.0129, Train Linf Norm: 1.2079, Test Linf Norm: 0.3982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-84-e8f6ba3279cc>:139: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  gamma = trial.suggest_uniform(\"gamma\", 0.1, 0.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 2: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 3: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 4: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 5: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 6: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 7: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 8: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 9: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 10: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 11: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 12: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 13: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 14: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 15: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 16: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 17: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 18: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 19: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 20: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 21: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 22: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 23: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 24: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 25: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 26: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 27: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 28: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 29: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 30: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 31: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 32: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 33: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 34: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 35: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 36: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 37: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 38: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 39: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 40: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 41: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 42: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 43: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 44: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 45: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 46: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 47: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 48: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 49: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 50: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 51: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 52: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 53: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 54: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 55: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 56: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 57: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 58: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 59: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 60: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 61: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 62: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 63: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 64: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 65: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 66: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 67: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 68: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 69: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 70: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 71: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 72: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 73: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 74: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 75: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 76: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 77: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 78: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 79: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 80: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 81: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 82: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 83: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 84: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 85: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 86: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 87: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 88: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 89: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 90: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 91: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 92: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 93: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 94: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 95: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 96: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 97: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 98: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 99: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 100: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 101: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 102: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 103: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 104: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 105: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 106: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 107: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 108: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 109: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 110: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 111: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 112: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 113: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 114: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 115: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 116: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 117: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 118: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 119: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 120: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 121: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 122: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 123: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 124: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 125: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 126: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 127: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 128: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 129: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 130: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 131: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 132: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 133: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 134: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 135: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 136: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 137: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 138: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 139: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 140: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 141: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 142: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 143: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 144: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 145: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 146: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 147: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 148: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 149: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 150: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 151: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 152: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 153: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 154: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 155: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 156: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 157: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 158: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 159: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 160: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 161: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 162: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 163: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 164: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 165: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 166: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 167: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 168: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 169: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 170: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 171: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 172: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 173: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 174: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 175: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 176: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 177: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 178: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 179: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 180: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 181: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 182: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 183: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 184: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 185: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 186: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 187: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 188: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 189: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 190: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 191: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 192: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 193: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 194: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 195: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 196: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 197: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 198: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 199: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 200: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 201: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 202: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 203: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 204: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 205: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 206: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 207: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 208: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 209: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 210: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 211: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 212: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 213: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 214: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 215: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 216: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 217: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 218: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 219: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 220: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 221: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 222: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 223: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 224: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 225: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 226: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 227: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 228: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 229: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 230: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 231: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 232: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 233: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 234: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 235: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 236: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 10:20:07,998]\u001b[0m Trial 1 finished with value: 1.0 and parameters: {'n_layers': 5, 'n_units_0': 1560, 'n_units_1': 445, 'n_units_2': 446, 'n_units_3': 856, 'n_units_4': 294, 'hidden_activation': 'ELU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'SGD', 'lr': 0.00010388994522070564, 'batch_size': 256, 'n_epochs': 237, 'scheduler': 'StepLR', 'weight_decay': 0.00010395519123365011, 'momentum': 0.10034993433961172, 'step_size': 10, 'gamma': 0.41148736160061095}. Best is trial 0 with value: 0.012935786840319633.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 237: Train Loss: 2.9463, Test Loss: 2.9669, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.8425, Test Loss: 0.0440, Train L1 Norm: 0.9844, Test L1 Norm: 0.1463, Train Linf Norm: 179.7404, Test Linf Norm: 11.9432\n",
            "Epoch 2: Train Loss: 0.0492, Test Loss: 0.0157, Train L1 Norm: 0.3748, Test L1 Norm: 0.0977, Train Linf Norm: 71.6290, Test Linf Norm: 10.1464\n",
            "Epoch 3: Train Loss: 0.0298, Test Loss: 0.3998, Train L1 Norm: 0.4403, Test L1 Norm: 0.3348, Train Linf Norm: 91.9617, Test Linf Norm: 23.2713\n",
            "Epoch 4: Train Loss: 0.0207, Test Loss: 0.1253, Train L1 Norm: 0.3794, Test L1 Norm: 0.1792, Train Linf Norm: 81.6216, Test Linf Norm: 12.5125\n",
            "Epoch 5: Train Loss: 0.0156, Test Loss: 0.0022, Train L1 Norm: 0.1969, Test L1 Norm: 0.0470, Train Linf Norm: 38.6883, Test Linf Norm: 4.6816\n",
            "Epoch 6: Train Loss: 0.0062, Test Loss: 0.0014, Train L1 Norm: 0.1158, Test L1 Norm: 0.0392, Train Linf Norm: 20.8593, Test Linf Norm: 3.7451\n",
            "Epoch 7: Train Loss: 0.0110, Test Loss: 0.0073, Train L1 Norm: 0.0889, Test L1 Norm: 0.0512, Train Linf Norm: 14.2233, Test Linf Norm: 4.2664\n",
            "Epoch 8: Train Loss: 0.0023, Test Loss: 0.0064, Train L1 Norm: 0.0731, Test L1 Norm: 0.0375, Train Linf Norm: 12.0563, Test Linf Norm: 2.7490\n",
            "Epoch 9: Train Loss: 0.0025, Test Loss: 0.0011, Train L1 Norm: 0.0703, Test L1 Norm: 0.0272, Train Linf Norm: 11.5408, Test Linf Norm: 2.2971\n",
            "Epoch 10: Train Loss: 0.0038, Test Loss: 0.0020, Train L1 Norm: 0.0699, Test L1 Norm: 0.0307, Train Linf Norm: 11.3897, Test Linf Norm: 2.3849\n",
            "Epoch 11: Train Loss: 0.0131, Test Loss: 0.0011, Train L1 Norm: 0.0692, Test L1 Norm: 0.0252, Train Linf Norm: 10.4063, Test Linf Norm: 1.8546\n",
            "Epoch 12: Train Loss: 0.0019, Test Loss: 0.0007, Train L1 Norm: 0.0562, Test L1 Norm: 0.0235, Train Linf Norm: 9.0770, Test Linf Norm: 1.8223\n",
            "Epoch 13: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0589, Test L1 Norm: 0.0233, Train Linf Norm: 10.4996, Test Linf Norm: 1.9219\n",
            "Epoch 14: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0607, Test L1 Norm: 0.0231, Train Linf Norm: 10.8332, Test Linf Norm: 1.9240\n",
            "Epoch 15: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0594, Test L1 Norm: 0.0231, Train Linf Norm: 10.6006, Test Linf Norm: 1.9397\n",
            "Epoch 16: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0635, Test L1 Norm: 0.0228, Train Linf Norm: 11.8596, Test Linf Norm: 1.9084\n",
            "Epoch 17: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0595, Test L1 Norm: 0.0231, Train Linf Norm: 10.7046, Test Linf Norm: 1.9504\n",
            "Epoch 18: Train Loss: 0.0005, Test Loss: 0.0015, Train L1 Norm: 0.0604, Test L1 Norm: 0.0232, Train Linf Norm: 11.0227, Test Linf Norm: 1.8992\n",
            "Epoch 19: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0604, Test L1 Norm: 0.0226, Train Linf Norm: 10.8670, Test Linf Norm: 1.9535\n",
            "Epoch 20: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0611, Test L1 Norm: 0.0223, Train Linf Norm: 11.2277, Test Linf Norm: 1.9035\n",
            "Epoch 21: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0587, Test L1 Norm: 0.0221, Train Linf Norm: 10.7057, Test Linf Norm: 1.8970\n",
            "Epoch 22: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0595, Test L1 Norm: 0.0219, Train Linf Norm: 10.9077, Test Linf Norm: 1.8629\n",
            "Epoch 23: Train Loss: 0.0005, Test Loss: 0.0005, Train L1 Norm: 0.0592, Test L1 Norm: 0.0217, Train Linf Norm: 10.8783, Test Linf Norm: 1.8471\n",
            "Epoch 24: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0593, Test L1 Norm: 0.0216, Train Linf Norm: 10.9130, Test Linf Norm: 1.8473\n",
            "Epoch 25: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0585, Test L1 Norm: 0.0215, Train Linf Norm: 10.6190, Test Linf Norm: 1.8379\n",
            "Epoch 26: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0583, Test L1 Norm: 0.0215, Train Linf Norm: 10.6695, Test Linf Norm: 1.8315\n",
            "Epoch 27: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0573, Test L1 Norm: 0.0215, Train Linf Norm: 10.3846, Test Linf Norm: 1.8319\n",
            "Epoch 28: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0582, Test L1 Norm: 0.0215, Train Linf Norm: 10.6532, Test Linf Norm: 1.8308\n",
            "Epoch 29: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0579, Test L1 Norm: 0.0215, Train Linf Norm: 10.4504, Test Linf Norm: 1.8327\n",
            "Epoch 30: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0582, Test L1 Norm: 0.0215, Train Linf Norm: 10.7199, Test Linf Norm: 1.8285\n",
            "Epoch 31: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0579, Test L1 Norm: 0.0214, Train Linf Norm: 10.6223, Test Linf Norm: 1.8177\n",
            "Epoch 32: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0580, Test L1 Norm: 0.0214, Train Linf Norm: 10.6493, Test Linf Norm: 1.8232\n",
            "Epoch 33: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0578, Test L1 Norm: 0.0213, Train Linf Norm: 10.5805, Test Linf Norm: 1.8028\n",
            "Epoch 34: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0576, Test L1 Norm: 0.0213, Train Linf Norm: 10.5585, Test Linf Norm: 1.8179\n",
            "Epoch 35: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0573, Test L1 Norm: 0.0213, Train Linf Norm: 10.4908, Test Linf Norm: 1.8102\n",
            "Epoch 36: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0571, Test L1 Norm: 0.0213, Train Linf Norm: 10.4410, Test Linf Norm: 1.8174\n",
            "Epoch 37: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0575, Test L1 Norm: 0.0213, Train Linf Norm: 10.6001, Test Linf Norm: 1.8134\n",
            "Epoch 38: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0575, Test L1 Norm: 0.0213, Train Linf Norm: 10.3990, Test Linf Norm: 1.8151\n",
            "Epoch 39: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0576, Test L1 Norm: 0.0213, Train Linf Norm: 10.5986, Test Linf Norm: 1.8153\n",
            "Epoch 40: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0576, Test L1 Norm: 0.0213, Train Linf Norm: 10.4810, Test Linf Norm: 1.8160\n",
            "Epoch 41: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0576, Test L1 Norm: 0.0213, Train Linf Norm: 10.6382, Test Linf Norm: 1.8157\n",
            "Epoch 42: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0576, Test L1 Norm: 0.0213, Train Linf Norm: 10.5078, Test Linf Norm: 1.8160\n",
            "Epoch 43: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0576, Test L1 Norm: 0.0213, Train Linf Norm: 10.5766, Test Linf Norm: 1.8163\n",
            "Epoch 44: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0576, Test L1 Norm: 0.0213, Train Linf Norm: 10.6003, Test Linf Norm: 1.8156\n",
            "Epoch 45: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0575, Test L1 Norm: 0.0213, Train Linf Norm: 10.1072, Test Linf Norm: 1.8147\n",
            "Epoch 46: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0575, Test L1 Norm: 0.0213, Train Linf Norm: 10.6080, Test Linf Norm: 1.8139\n",
            "Epoch 47: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0575, Test L1 Norm: 0.0213, Train Linf Norm: 10.4525, Test Linf Norm: 1.8132\n",
            "Epoch 48: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0213, Train Linf Norm: 10.5311, Test Linf Norm: 1.8136\n",
            "Epoch 49: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0575, Test L1 Norm: 0.0213, Train Linf Norm: 10.3408, Test Linf Norm: 1.8133\n",
            "Epoch 50: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0575, Test L1 Norm: 0.0213, Train Linf Norm: 10.4708, Test Linf Norm: 1.8131\n",
            "Epoch 51: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0575, Test L1 Norm: 0.0213, Train Linf Norm: 10.3944, Test Linf Norm: 1.8128\n",
            "Epoch 52: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0575, Test L1 Norm: 0.0213, Train Linf Norm: 10.5341, Test Linf Norm: 1.8126\n",
            "Epoch 53: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0213, Train Linf Norm: 10.5064, Test Linf Norm: 1.8124\n",
            "Epoch 54: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0213, Train Linf Norm: 10.4601, Test Linf Norm: 1.8121\n",
            "Epoch 55: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0213, Train Linf Norm: 10.4161, Test Linf Norm: 1.8119\n",
            "Epoch 56: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.3811, Test Linf Norm: 1.8117\n",
            "Epoch 57: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.3902, Test Linf Norm: 1.8115\n",
            "Epoch 58: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4688, Test Linf Norm: 1.8115\n",
            "Epoch 59: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.3058, Test Linf Norm: 1.8114\n",
            "Epoch 60: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.3708, Test Linf Norm: 1.8112\n",
            "Epoch 61: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4739, Test Linf Norm: 1.8112\n",
            "Epoch 62: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5093, Test Linf Norm: 1.8112\n",
            "Epoch 63: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5477, Test Linf Norm: 1.8112\n",
            "Epoch 64: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5543, Test Linf Norm: 1.8112\n",
            "Epoch 65: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4754, Test Linf Norm: 1.8111\n",
            "Epoch 66: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5195, Test Linf Norm: 1.8111\n",
            "Epoch 67: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4103, Test Linf Norm: 1.8111\n",
            "Epoch 68: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4748, Test Linf Norm: 1.8111\n",
            "Epoch 69: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5005, Test Linf Norm: 1.8110\n",
            "Epoch 70: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4538, Test Linf Norm: 1.8110\n",
            "Epoch 71: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5240, Test Linf Norm: 1.8110\n",
            "Epoch 72: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5932, Test Linf Norm: 1.8110\n",
            "Epoch 73: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4355, Test Linf Norm: 1.8110\n",
            "Epoch 74: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4294, Test Linf Norm: 1.8110\n",
            "Epoch 75: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5444, Test Linf Norm: 1.8110\n",
            "Epoch 76: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4535, Test Linf Norm: 1.8110\n",
            "Epoch 77: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4640, Test Linf Norm: 1.8110\n",
            "Epoch 78: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4980, Test Linf Norm: 1.8110\n",
            "Epoch 79: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4613, Test Linf Norm: 1.8110\n",
            "Epoch 80: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5289, Test Linf Norm: 1.8110\n",
            "Epoch 81: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5129, Test Linf Norm: 1.8110\n",
            "Epoch 82: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5826, Test Linf Norm: 1.8110\n",
            "Epoch 83: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4889, Test Linf Norm: 1.8110\n",
            "Epoch 84: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5481, Test Linf Norm: 1.8110\n",
            "Epoch 85: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4621, Test Linf Norm: 1.8110\n",
            "Epoch 86: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5202, Test Linf Norm: 1.8110\n",
            "Epoch 87: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5710, Test Linf Norm: 1.8110\n",
            "Epoch 88: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5181, Test Linf Norm: 1.8110\n",
            "Epoch 89: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.2243, Test Linf Norm: 1.8110\n",
            "Epoch 90: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.3697, Test Linf Norm: 1.8110\n",
            "Epoch 91: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4552, Test Linf Norm: 1.8110\n",
            "Epoch 92: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5018, Test Linf Norm: 1.8110\n",
            "Epoch 93: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5131, Test Linf Norm: 1.8110\n",
            "Epoch 94: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 8.6309, Test Linf Norm: 1.8110\n",
            "Epoch 95: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.1610, Test Linf Norm: 1.8110\n",
            "Epoch 96: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4477, Test Linf Norm: 1.8110\n",
            "Epoch 97: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.0810, Test Linf Norm: 1.8110\n",
            "Epoch 98: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4042, Test Linf Norm: 1.8110\n",
            "Epoch 99: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5324, Test Linf Norm: 1.8110\n",
            "Epoch 100: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5090, Test Linf Norm: 1.8110\n",
            "Epoch 101: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5382, Test Linf Norm: 1.8110\n",
            "Epoch 102: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4792, Test Linf Norm: 1.8110\n",
            "Epoch 103: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.3878, Test Linf Norm: 1.8110\n",
            "Epoch 104: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5295, Test Linf Norm: 1.8110\n",
            "Epoch 105: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.3549, Test Linf Norm: 1.8110\n",
            "Epoch 106: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4142, Test Linf Norm: 1.8110\n",
            "Epoch 107: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.2577, Test Linf Norm: 1.8110\n",
            "Epoch 108: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4660, Test Linf Norm: 1.8110\n",
            "Epoch 109: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4742, Test Linf Norm: 1.8110\n",
            "Epoch 110: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5008, Test Linf Norm: 1.8110\n",
            "Epoch 111: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4665, Test Linf Norm: 1.8110\n",
            "Epoch 112: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4416, Test Linf Norm: 1.8110\n",
            "Epoch 113: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5857, Test Linf Norm: 1.8110\n",
            "Epoch 114: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4869, Test Linf Norm: 1.8110\n",
            "Epoch 115: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4968, Test Linf Norm: 1.8110\n",
            "Epoch 116: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4012, Test Linf Norm: 1.8110\n",
            "Epoch 117: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5615, Test Linf Norm: 1.8110\n",
            "Epoch 118: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.3868, Test Linf Norm: 1.8110\n",
            "Epoch 119: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5234, Test Linf Norm: 1.8110\n",
            "Epoch 120: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5291, Test Linf Norm: 1.8110\n",
            "Epoch 121: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4906, Test Linf Norm: 1.8110\n",
            "Epoch 122: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.3700, Test Linf Norm: 1.8110\n",
            "Epoch 123: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5338, Test Linf Norm: 1.8110\n",
            "Epoch 124: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4766, Test Linf Norm: 1.8110\n",
            "Epoch 125: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4576, Test Linf Norm: 1.8110\n",
            "Epoch 126: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4550, Test Linf Norm: 1.8110\n",
            "Epoch 127: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5169, Test Linf Norm: 1.8110\n",
            "Epoch 128: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5403, Test Linf Norm: 1.8110\n",
            "Epoch 129: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5051, Test Linf Norm: 1.8110\n",
            "Epoch 130: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4791, Test Linf Norm: 1.8110\n",
            "Epoch 131: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4708, Test Linf Norm: 1.8110\n",
            "Epoch 132: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.3520, Test Linf Norm: 1.8110\n",
            "Epoch 133: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5497, Test Linf Norm: 1.8110\n",
            "Epoch 134: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.6148, Test Linf Norm: 1.8110\n",
            "Epoch 135: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4026, Test Linf Norm: 1.8110\n",
            "Epoch 136: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.3255, Test Linf Norm: 1.8110\n",
            "Epoch 137: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4017, Test Linf Norm: 1.8110\n",
            "Epoch 138: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.0792, Test Linf Norm: 1.8110\n",
            "Epoch 139: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.3488, Test Linf Norm: 1.8110\n",
            "Epoch 140: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.3845, Test Linf Norm: 1.8110\n",
            "Epoch 141: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4131, Test Linf Norm: 1.8110\n",
            "Epoch 142: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4196, Test Linf Norm: 1.8110\n",
            "Epoch 143: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4568, Test Linf Norm: 1.8110\n",
            "Epoch 144: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.3335, Test Linf Norm: 1.8110\n",
            "Epoch 145: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.3409, Test Linf Norm: 1.8110\n",
            "Epoch 146: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4830, Test Linf Norm: 1.8110\n",
            "Epoch 147: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4663, Test Linf Norm: 1.8110\n",
            "Epoch 148: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4995, Test Linf Norm: 1.8110\n",
            "Epoch 149: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5124, Test Linf Norm: 1.8110\n",
            "Epoch 150: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5432, Test Linf Norm: 1.8110\n",
            "Epoch 151: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4693, Test Linf Norm: 1.8110\n",
            "Epoch 152: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4245, Test Linf Norm: 1.8110\n",
            "Epoch 153: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.2662, Test Linf Norm: 1.8110\n",
            "Epoch 154: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5113, Test Linf Norm: 1.8110\n",
            "Epoch 155: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4866, Test Linf Norm: 1.8110\n",
            "Epoch 156: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5440, Test Linf Norm: 1.8110\n",
            "Epoch 157: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5104, Test Linf Norm: 1.8110\n",
            "Epoch 158: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5546, Test Linf Norm: 1.8110\n",
            "Epoch 159: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4319, Test Linf Norm: 1.8110\n",
            "Epoch 160: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5056, Test Linf Norm: 1.8110\n",
            "Epoch 161: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4358, Test Linf Norm: 1.8110\n",
            "Epoch 162: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5496, Test Linf Norm: 1.8110\n",
            "Epoch 163: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 9.9010, Test Linf Norm: 1.8110\n",
            "Epoch 164: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5386, Test Linf Norm: 1.8110\n",
            "Epoch 165: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5144, Test Linf Norm: 1.8110\n",
            "Epoch 166: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4872, Test Linf Norm: 1.8110\n",
            "Epoch 167: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 9.9334, Test Linf Norm: 1.8110\n",
            "Epoch 168: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4754, Test Linf Norm: 1.8110\n",
            "Epoch 169: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4346, Test Linf Norm: 1.8110\n",
            "Epoch 170: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4252, Test Linf Norm: 1.8110\n",
            "Epoch 171: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4331, Test Linf Norm: 1.8110\n",
            "Epoch 172: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4185, Test Linf Norm: 1.8110\n",
            "Epoch 173: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5324, Test Linf Norm: 1.8110\n",
            "Epoch 174: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.6307, Test Linf Norm: 1.8110\n",
            "Epoch 175: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.3852, Test Linf Norm: 1.8110\n",
            "Epoch 176: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4399, Test Linf Norm: 1.8110\n",
            "Epoch 177: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4287, Test Linf Norm: 1.8110\n",
            "Epoch 178: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.3168, Test Linf Norm: 1.8110\n",
            "Epoch 179: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4246, Test Linf Norm: 1.8110\n",
            "Epoch 180: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4411, Test Linf Norm: 1.8110\n",
            "Epoch 181: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5308, Test Linf Norm: 1.8110\n",
            "Epoch 182: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4431, Test Linf Norm: 1.8110\n",
            "Epoch 183: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.3651, Test Linf Norm: 1.8110\n",
            "Epoch 184: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4330, Test Linf Norm: 1.8110\n",
            "Epoch 185: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5049, Test Linf Norm: 1.8110\n",
            "Epoch 186: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.3760, Test Linf Norm: 1.8110\n",
            "Epoch 187: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.3911, Test Linf Norm: 1.8110\n",
            "Epoch 188: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4332, Test Linf Norm: 1.8110\n",
            "Epoch 189: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4038, Test Linf Norm: 1.8110\n",
            "Epoch 190: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.5265, Test Linf Norm: 1.8110\n",
            "Epoch 191: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4388, Test Linf Norm: 1.8110\n",
            "Epoch 192: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.3688, Test Linf Norm: 1.8110\n",
            "Epoch 193: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4953, Test Linf Norm: 1.8110\n",
            "Epoch 194: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4396, Test Linf Norm: 1.8110\n",
            "Epoch 195: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4756, Test Linf Norm: 1.8110\n",
            "Epoch 196: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4645, Test Linf Norm: 1.8110\n",
            "Epoch 197: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.4899, Test Linf Norm: 1.8110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 10:30:29,046]\u001b[0m Trial 2 finished with value: 0.02124303916990757 and parameters: {'n_layers': 5, 'n_units_0': 1386, 'n_units_1': 1792, 'n_units_2': 1561, 'n_units_3': 1842, 'n_units_4': 1676, 'hidden_activation': 'LeakyReLU', 'output_activation': 'Linear', 'loss': 'MSE', 'optimizer': 'Adagrad', 'lr': 0.00033404928260473296, 'batch_size': 256, 'n_epochs': 198, 'scheduler': 'StepLR', 'step_size': 12, 'gamma': 0.14980746595541447}. Best is trial 0 with value: 0.012935786840319633.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 198: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0574, Test L1 Norm: 0.0212, Train Linf Norm: 10.3678, Test Linf Norm: 1.8110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-84-e8f6ba3279cc>:126: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
            "<ipython-input-84-e8f6ba3279cc>:127: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  beta1 = trial.suggest_uniform(\"beta1\", 0.9, 0.999)\n",
            "<ipython-input-84-e8f6ba3279cc>:128: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  beta2 = trial.suggest_uniform(\"beta2\", 0.999, 0.9999)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.1904, Test Loss: 0.0429, Train L1 Norm: 0.5185, Test L1 Norm: 0.1065, Train Linf Norm: 35.9589, Test Linf Norm: 5.5928\n",
            "Epoch 2: Train Loss: 0.0678, Test Loss: 0.0504, Train L1 Norm: 0.1704, Test L1 Norm: 0.0849, Train Linf Norm: 11.3630, Test Linf Norm: 3.9682\n",
            "Epoch 3: Train Loss: 0.0531, Test Loss: 0.0530, Train L1 Norm: 0.2209, Test L1 Norm: 0.0966, Train Linf Norm: 17.0274, Test Linf Norm: 4.7207\n",
            "Epoch 4: Train Loss: 0.0599, Test Loss: 0.0603, Train L1 Norm: 0.2638, Test L1 Norm: 0.0666, Train Linf Norm: 20.7252, Test Linf Norm: 2.6924\n",
            "Epoch 5: Train Loss: 0.0434, Test Loss: 0.0385, Train L1 Norm: 0.1714, Test L1 Norm: 0.0605, Train Linf Norm: 12.8915, Test Linf Norm: 2.4305\n",
            "Epoch 6: Train Loss: 0.0548, Test Loss: 0.0397, Train L1 Norm: 0.1393, Test L1 Norm: 0.0412, Train Linf Norm: 9.1372, Test Linf Norm: 1.4406\n",
            "Epoch 7: Train Loss: 0.0530, Test Loss: 0.0313, Train L1 Norm: 0.2141, Test L1 Norm: 0.0537, Train Linf Norm: 16.8206, Test Linf Norm: 2.4414\n",
            "Epoch 8: Train Loss: 0.0385, Test Loss: 0.0567, Train L1 Norm: 0.1402, Test L1 Norm: 0.0589, Train Linf Norm: 10.3097, Test Linf Norm: 2.4943\n",
            "Epoch 9: Train Loss: 0.0328, Test Loss: 0.0396, Train L1 Norm: 0.1455, Test L1 Norm: 0.0540, Train Linf Norm: 10.9815, Test Linf Norm: 2.2978\n",
            "Epoch 10: Train Loss: 0.0286, Test Loss: 0.0336, Train L1 Norm: 0.1529, Test L1 Norm: 0.0377, Train Linf Norm: 11.8506, Test Linf Norm: 1.4042\n",
            "Epoch 11: Train Loss: 0.0373, Test Loss: 0.0268, Train L1 Norm: 0.1506, Test L1 Norm: 0.0518, Train Linf Norm: 11.3988, Test Linf Norm: 2.1844\n",
            "Epoch 12: Train Loss: 0.0337, Test Loss: 0.0362, Train L1 Norm: 0.1308, Test L1 Norm: 0.0476, Train Linf Norm: 9.4634, Test Linf Norm: 1.8217\n",
            "Epoch 13: Train Loss: 0.0308, Test Loss: 0.0181, Train L1 Norm: 0.1349, Test L1 Norm: 0.0423, Train Linf Norm: 10.0244, Test Linf Norm: 1.9859\n",
            "Epoch 14: Train Loss: 0.0304, Test Loss: 0.0456, Train L1 Norm: 0.1526, Test L1 Norm: 0.0477, Train Linf Norm: 11.8333, Test Linf Norm: 1.8788\n",
            "Epoch 15: Train Loss: 0.0348, Test Loss: 0.0236, Train L1 Norm: 0.1629, Test L1 Norm: 0.0490, Train Linf Norm: 12.6428, Test Linf Norm: 2.3786\n",
            "Epoch 16: Train Loss: 0.0395, Test Loss: 0.0197, Train L1 Norm: 0.1189, Test L1 Norm: 0.0503, Train Linf Norm: 8.1568, Test Linf Norm: 2.4319\n",
            "Epoch 17: Train Loss: 0.0259, Test Loss: 0.0192, Train L1 Norm: 0.1506, Test L1 Norm: 0.0436, Train Linf Norm: 11.8906, Test Linf Norm: 1.9848\n",
            "Epoch 18: Train Loss: 0.0293, Test Loss: 0.0446, Train L1 Norm: 0.1734, Test L1 Norm: 0.0425, Train Linf Norm: 13.8482, Test Linf Norm: 1.4341\n",
            "Epoch 19: Train Loss: 0.0267, Test Loss: 0.0189, Train L1 Norm: 0.0939, Test L1 Norm: 0.0410, Train Linf Norm: 6.3458, Test Linf Norm: 1.8512\n",
            "Epoch 20: Train Loss: 0.0262, Test Loss: 0.0285, Train L1 Norm: 0.1146, Test L1 Norm: 0.0525, Train Linf Norm: 8.1973, Test Linf Norm: 2.1252\n",
            "Epoch 21: Train Loss: 0.0365, Test Loss: 0.0230, Train L1 Norm: 0.1346, Test L1 Norm: 0.0474, Train Linf Norm: 9.8549, Test Linf Norm: 2.2285\n",
            "Epoch 22: Train Loss: 0.0316, Test Loss: 0.0357, Train L1 Norm: 0.1616, Test L1 Norm: 0.0586, Train Linf Norm: 12.6621, Test Linf Norm: 2.7004\n",
            "Epoch 23: Train Loss: 0.0190, Test Loss: 0.0150, Train L1 Norm: 0.1689, Test L1 Norm: 0.0429, Train Linf Norm: 13.9994, Test Linf Norm: 2.1115\n",
            "Epoch 24: Train Loss: 0.0150, Test Loss: 0.0120, Train L1 Norm: 0.1678, Test L1 Norm: 0.0378, Train Linf Norm: 14.0916, Test Linf Norm: 1.8328\n",
            "Epoch 25: Train Loss: 0.0194, Test Loss: 0.0288, Train L1 Norm: 0.1907, Test L1 Norm: 0.0385, Train Linf Norm: 16.0863, Test Linf Norm: 1.5196\n",
            "Epoch 26: Train Loss: 0.0155, Test Loss: 0.0152, Train L1 Norm: 0.1556, Test L1 Norm: 0.0392, Train Linf Norm: 12.8277, Test Linf Norm: 1.9174\n",
            "Epoch 27: Train Loss: 0.0157, Test Loss: 0.0142, Train L1 Norm: 0.1510, Test L1 Norm: 0.0373, Train Linf Norm: 12.4838, Test Linf Norm: 1.7609\n",
            "Epoch 28: Train Loss: 0.0160, Test Loss: 0.0138, Train L1 Norm: 0.1524, Test L1 Norm: 0.0385, Train Linf Norm: 12.6110, Test Linf Norm: 1.7499\n",
            "Epoch 29: Train Loss: 0.0161, Test Loss: 0.0209, Train L1 Norm: 0.1343, Test L1 Norm: 0.0389, Train Linf Norm: 10.8369, Test Linf Norm: 1.7917\n",
            "Epoch 30: Train Loss: 0.0154, Test Loss: 0.0131, Train L1 Norm: 0.1518, Test L1 Norm: 0.0353, Train Linf Norm: 12.5873, Test Linf Norm: 1.6553\n",
            "Epoch 31: Train Loss: 0.0161, Test Loss: 0.0127, Train L1 Norm: 0.1428, Test L1 Norm: 0.0380, Train Linf Norm: 11.7439, Test Linf Norm: 1.7550\n",
            "Epoch 32: Train Loss: 0.0162, Test Loss: 0.0118, Train L1 Norm: 0.1295, Test L1 Norm: 0.0427, Train Linf Norm: 10.4865, Test Linf Norm: 2.1667\n",
            "Epoch 33: Train Loss: 0.0164, Test Loss: 0.0163, Train L1 Norm: 0.1278, Test L1 Norm: 0.0348, Train Linf Norm: 10.2922, Test Linf Norm: 1.5404\n",
            "Epoch 34: Train Loss: 0.0159, Test Loss: 0.0246, Train L1 Norm: 0.1367, Test L1 Norm: 0.0353, Train Linf Norm: 11.0736, Test Linf Norm: 1.4921\n",
            "Epoch 35: Train Loss: 0.0145, Test Loss: 0.0125, Train L1 Norm: 0.1378, Test L1 Norm: 0.0357, Train Linf Norm: 11.3278, Test Linf Norm: 1.6498\n",
            "Epoch 36: Train Loss: 0.0146, Test Loss: 0.0127, Train L1 Norm: 0.1297, Test L1 Norm: 0.0333, Train Linf Norm: 10.3740, Test Linf Norm: 1.5353\n",
            "Epoch 37: Train Loss: 0.0167, Test Loss: 0.0129, Train L1 Norm: 0.1313, Test L1 Norm: 0.0367, Train Linf Norm: 10.5116, Test Linf Norm: 1.7700\n",
            "Epoch 38: Train Loss: 0.0146, Test Loss: 0.0108, Train L1 Norm: 0.1220, Test L1 Norm: 0.0339, Train Linf Norm: 9.8559, Test Linf Norm: 1.6613\n",
            "Epoch 39: Train Loss: 0.0157, Test Loss: 0.0166, Train L1 Norm: 0.1375, Test L1 Norm: 0.0352, Train Linf Norm: 11.3023, Test Linf Norm: 1.5241\n",
            "Epoch 40: Train Loss: 0.0143, Test Loss: 0.0191, Train L1 Norm: 0.1367, Test L1 Norm: 0.0378, Train Linf Norm: 11.1840, Test Linf Norm: 1.5075\n",
            "Epoch 41: Train Loss: 0.0155, Test Loss: 0.0150, Train L1 Norm: 0.1337, Test L1 Norm: 0.0325, Train Linf Norm: 10.8971, Test Linf Norm: 1.3171\n",
            "Epoch 42: Train Loss: 0.0154, Test Loss: 0.0128, Train L1 Norm: 0.1519, Test L1 Norm: 0.0293, Train Linf Norm: 12.6226, Test Linf Norm: 1.2400\n",
            "Epoch 43: Train Loss: 0.0156, Test Loss: 0.0170, Train L1 Norm: 0.1156, Test L1 Norm: 0.0348, Train Linf Norm: 9.1711, Test Linf Norm: 1.5877\n",
            "Epoch 44: Train Loss: 0.0166, Test Loss: 0.0134, Train L1 Norm: 0.1413, Test L1 Norm: 0.0315, Train Linf Norm: 11.5406, Test Linf Norm: 1.4184\n",
            "Epoch 45: Train Loss: 0.0138, Test Loss: 0.0116, Train L1 Norm: 0.1254, Test L1 Norm: 0.0342, Train Linf Norm: 10.0945, Test Linf Norm: 1.6170\n",
            "Epoch 46: Train Loss: 0.0143, Test Loss: 0.0102, Train L1 Norm: 0.1624, Test L1 Norm: 0.0347, Train Linf Norm: 13.6816, Test Linf Norm: 1.7208\n",
            "Epoch 47: Train Loss: 0.0147, Test Loss: 0.0112, Train L1 Norm: 0.1065, Test L1 Norm: 0.0307, Train Linf Norm: 8.3475, Test Linf Norm: 1.4299\n",
            "Epoch 48: Train Loss: 0.0136, Test Loss: 0.0113, Train L1 Norm: 0.1340, Test L1 Norm: 0.0305, Train Linf Norm: 11.0186, Test Linf Norm: 1.4014\n",
            "Epoch 49: Train Loss: 0.0129, Test Loss: 0.0160, Train L1 Norm: 0.1113, Test L1 Norm: 0.0382, Train Linf Norm: 8.8636, Test Linf Norm: 1.7466\n",
            "Epoch 50: Train Loss: 0.0142, Test Loss: 0.0142, Train L1 Norm: 0.1249, Test L1 Norm: 0.0370, Train Linf Norm: 10.1124, Test Linf Norm: 1.8011\n",
            "Epoch 51: Train Loss: 0.0144, Test Loss: 0.0128, Train L1 Norm: 0.1294, Test L1 Norm: 0.0340, Train Linf Norm: 10.5405, Test Linf Norm: 1.5573\n",
            "Epoch 52: Train Loss: 0.0150, Test Loss: 0.0127, Train L1 Norm: 0.1393, Test L1 Norm: 0.0305, Train Linf Norm: 11.5086, Test Linf Norm: 1.2636\n",
            "Epoch 53: Train Loss: 0.0170, Test Loss: 0.0112, Train L1 Norm: 0.1101, Test L1 Norm: 0.0334, Train Linf Norm: 8.6125, Test Linf Norm: 1.6030\n",
            "Epoch 54: Train Loss: 0.0143, Test Loss: 0.0119, Train L1 Norm: 0.1519, Test L1 Norm: 0.0368, Train Linf Norm: 12.7470, Test Linf Norm: 1.6994\n",
            "Epoch 55: Train Loss: 0.0142, Test Loss: 0.0134, Train L1 Norm: 0.1273, Test L1 Norm: 0.0330, Train Linf Norm: 10.3389, Test Linf Norm: 1.5289\n",
            "Epoch 56: Train Loss: 0.0102, Test Loss: 0.0095, Train L1 Norm: 0.1180, Test L1 Norm: 0.0313, Train Linf Norm: 9.6669, Test Linf Norm: 1.4994\n",
            "Epoch 57: Train Loss: 0.0107, Test Loss: 0.0095, Train L1 Norm: 0.1096, Test L1 Norm: 0.0317, Train Linf Norm: 8.8942, Test Linf Norm: 1.5236\n",
            "Epoch 58: Train Loss: 0.0108, Test Loss: 0.0093, Train L1 Norm: 0.1333, Test L1 Norm: 0.0301, Train Linf Norm: 11.0614, Test Linf Norm: 1.4389\n",
            "Epoch 59: Train Loss: 0.0109, Test Loss: 0.0120, Train L1 Norm: 0.1239, Test L1 Norm: 0.0345, Train Linf Norm: 10.2241, Test Linf Norm: 1.5497\n",
            "Epoch 60: Train Loss: 0.0097, Test Loss: 0.0094, Train L1 Norm: 0.1151, Test L1 Norm: 0.0304, Train Linf Norm: 9.3593, Test Linf Norm: 1.4720\n",
            "Epoch 61: Train Loss: 0.0098, Test Loss: 0.0113, Train L1 Norm: 0.1136, Test L1 Norm: 0.0325, Train Linf Norm: 9.2732, Test Linf Norm: 1.5875\n",
            "Epoch 62: Train Loss: 0.0106, Test Loss: 0.0093, Train L1 Norm: 0.1166, Test L1 Norm: 0.0322, Train Linf Norm: 9.5764, Test Linf Norm: 1.5953\n",
            "Epoch 63: Train Loss: 0.0104, Test Loss: 0.0094, Train L1 Norm: 0.1201, Test L1 Norm: 0.0328, Train Linf Norm: 9.8244, Test Linf Norm: 1.6035\n",
            "Epoch 64: Train Loss: 0.0103, Test Loss: 0.0095, Train L1 Norm: 0.1267, Test L1 Norm: 0.0293, Train Linf Norm: 10.4764, Test Linf Norm: 1.3831\n",
            "Epoch 65: Train Loss: 0.0103, Test Loss: 0.0131, Train L1 Norm: 0.1123, Test L1 Norm: 0.0336, Train Linf Norm: 9.1247, Test Linf Norm: 1.5476\n",
            "Epoch 66: Train Loss: 0.0107, Test Loss: 0.0104, Train L1 Norm: 0.1169, Test L1 Norm: 0.0309, Train Linf Norm: 9.5506, Test Linf Norm: 1.4740\n",
            "Epoch 67: Train Loss: 0.0103, Test Loss: 0.0086, Train L1 Norm: 0.1075, Test L1 Norm: 0.0290, Train Linf Norm: 8.6394, Test Linf Norm: 1.3744\n",
            "Epoch 68: Train Loss: 0.0099, Test Loss: 0.0095, Train L1 Norm: 0.1139, Test L1 Norm: 0.0283, Train Linf Norm: 9.2498, Test Linf Norm: 1.2921\n",
            "Epoch 69: Train Loss: 0.0114, Test Loss: 0.0191, Train L1 Norm: 0.1331, Test L1 Norm: 0.0343, Train Linf Norm: 11.1481, Test Linf Norm: 1.5058\n",
            "Epoch 70: Train Loss: 0.0100, Test Loss: 0.0095, Train L1 Norm: 0.1194, Test L1 Norm: 0.0303, Train Linf Norm: 9.8518, Test Linf Norm: 1.4310\n",
            "Epoch 71: Train Loss: 0.0108, Test Loss: 0.0099, Train L1 Norm: 0.1180, Test L1 Norm: 0.0308, Train Linf Norm: 9.6911, Test Linf Norm: 1.4642\n",
            "Epoch 72: Train Loss: 0.0104, Test Loss: 0.0146, Train L1 Norm: 0.1146, Test L1 Norm: 0.0322, Train Linf Norm: 9.2706, Test Linf Norm: 1.5087\n",
            "Epoch 73: Train Loss: 0.0112, Test Loss: 0.0141, Train L1 Norm: 0.1223, Test L1 Norm: 0.0294, Train Linf Norm: 10.1126, Test Linf Norm: 1.3271\n",
            "Epoch 74: Train Loss: 0.0105, Test Loss: 0.0094, Train L1 Norm: 0.1220, Test L1 Norm: 0.0296, Train Linf Norm: 10.0998, Test Linf Norm: 1.3398\n",
            "Epoch 75: Train Loss: 0.0101, Test Loss: 0.0096, Train L1 Norm: 0.1220, Test L1 Norm: 0.0308, Train Linf Norm: 10.1380, Test Linf Norm: 1.4961\n",
            "Epoch 76: Train Loss: 0.0094, Test Loss: 0.0097, Train L1 Norm: 0.1080, Test L1 Norm: 0.0303, Train Linf Norm: 8.8436, Test Linf Norm: 1.4406\n",
            "Epoch 77: Train Loss: 0.0085, Test Loss: 0.0081, Train L1 Norm: 0.1183, Test L1 Norm: 0.0306, Train Linf Norm: 9.8355, Test Linf Norm: 1.5193\n",
            "Epoch 78: Train Loss: 0.0085, Test Loss: 0.0089, Train L1 Norm: 0.1221, Test L1 Norm: 0.0297, Train Linf Norm: 10.1467, Test Linf Norm: 1.3854\n",
            "Epoch 79: Train Loss: 0.0085, Test Loss: 0.0087, Train L1 Norm: 0.1164, Test L1 Norm: 0.0290, Train Linf Norm: 9.6196, Test Linf Norm: 1.3927\n",
            "Epoch 80: Train Loss: 0.0086, Test Loss: 0.0100, Train L1 Norm: 0.1160, Test L1 Norm: 0.0324, Train Linf Norm: 9.5484, Test Linf Norm: 1.5589\n",
            "Epoch 81: Train Loss: 0.0093, Test Loss: 0.0111, Train L1 Norm: 0.1227, Test L1 Norm: 0.0305, Train Linf Norm: 10.2039, Test Linf Norm: 1.4626\n",
            "Epoch 82: Train Loss: 0.0086, Test Loss: 0.0100, Train L1 Norm: 0.1121, Test L1 Norm: 0.0293, Train Linf Norm: 9.1490, Test Linf Norm: 1.3823\n",
            "Epoch 83: Train Loss: 0.0085, Test Loss: 0.0081, Train L1 Norm: 0.1178, Test L1 Norm: 0.0297, Train Linf Norm: 9.6955, Test Linf Norm: 1.4532\n",
            "Epoch 84: Train Loss: 0.0086, Test Loss: 0.0090, Train L1 Norm: 0.1182, Test L1 Norm: 0.0313, Train Linf Norm: 9.7306, Test Linf Norm: 1.5488\n",
            "Epoch 85: Train Loss: 0.0082, Test Loss: 0.0086, Train L1 Norm: 0.1173, Test L1 Norm: 0.0304, Train Linf Norm: 9.7094, Test Linf Norm: 1.4488\n",
            "Epoch 86: Train Loss: 0.0089, Test Loss: 0.0082, Train L1 Norm: 0.1125, Test L1 Norm: 0.0297, Train Linf Norm: 9.2392, Test Linf Norm: 1.4488\n",
            "Epoch 87: Train Loss: 0.0079, Test Loss: 0.0078, Train L1 Norm: 0.1178, Test L1 Norm: 0.0297, Train Linf Norm: 9.7352, Test Linf Norm: 1.4653\n",
            "Epoch 88: Train Loss: 0.0080, Test Loss: 0.0079, Train L1 Norm: 0.1211, Test L1 Norm: 0.0296, Train Linf Norm: 10.0787, Test Linf Norm: 1.4592\n",
            "Epoch 89: Train Loss: 0.0081, Test Loss: 0.0081, Train L1 Norm: 0.1188, Test L1 Norm: 0.0295, Train Linf Norm: 9.8297, Test Linf Norm: 1.4434\n",
            "Epoch 90: Train Loss: 0.0079, Test Loss: 0.0078, Train L1 Norm: 0.1168, Test L1 Norm: 0.0287, Train Linf Norm: 9.6213, Test Linf Norm: 1.3941\n",
            "Epoch 91: Train Loss: 0.0078, Test Loss: 0.0078, Train L1 Norm: 0.1179, Test L1 Norm: 0.0289, Train Linf Norm: 9.7853, Test Linf Norm: 1.4077\n",
            "Epoch 92: Train Loss: 0.0082, Test Loss: 0.0084, Train L1 Norm: 0.1200, Test L1 Norm: 0.0291, Train Linf Norm: 9.9789, Test Linf Norm: 1.4107\n",
            "Epoch 93: Train Loss: 0.0082, Test Loss: 0.0080, Train L1 Norm: 0.1162, Test L1 Norm: 0.0300, Train Linf Norm: 9.6349, Test Linf Norm: 1.4845\n",
            "Epoch 94: Train Loss: 0.0078, Test Loss: 0.0081, Train L1 Norm: 0.1168, Test L1 Norm: 0.0302, Train Linf Norm: 9.6984, Test Linf Norm: 1.4657\n",
            "Epoch 95: Train Loss: 0.0080, Test Loss: 0.0078, Train L1 Norm: 0.1203, Test L1 Norm: 0.0298, Train Linf Norm: 10.0082, Test Linf Norm: 1.4691\n",
            "Epoch 96: Train Loss: 0.0079, Test Loss: 0.0078, Train L1 Norm: 0.1179, Test L1 Norm: 0.0295, Train Linf Norm: 9.7802, Test Linf Norm: 1.4512\n",
            "Epoch 97: Train Loss: 0.0079, Test Loss: 0.0078, Train L1 Norm: 0.1174, Test L1 Norm: 0.0305, Train Linf Norm: 9.7409, Test Linf Norm: 1.5262\n",
            "Epoch 98: Train Loss: 0.0079, Test Loss: 0.0078, Train L1 Norm: 0.1156, Test L1 Norm: 0.0294, Train Linf Norm: 9.5881, Test Linf Norm: 1.4436\n",
            "Epoch 99: Train Loss: 0.0077, Test Loss: 0.0081, Train L1 Norm: 0.1174, Test L1 Norm: 0.0300, Train Linf Norm: 9.7729, Test Linf Norm: 1.4733\n",
            "Epoch 100: Train Loss: 0.0079, Test Loss: 0.0078, Train L1 Norm: 0.1185, Test L1 Norm: 0.0287, Train Linf Norm: 9.8487, Test Linf Norm: 1.4000\n",
            "Epoch 101: Train Loss: 0.0079, Test Loss: 0.0084, Train L1 Norm: 0.1149, Test L1 Norm: 0.0293, Train Linf Norm: 9.5456, Test Linf Norm: 1.4177\n",
            "Epoch 102: Train Loss: 0.0079, Test Loss: 0.0077, Train L1 Norm: 0.1207, Test L1 Norm: 0.0298, Train Linf Norm: 10.0195, Test Linf Norm: 1.4742\n",
            "Epoch 103: Train Loss: 0.0079, Test Loss: 0.0080, Train L1 Norm: 0.1193, Test L1 Norm: 0.0288, Train Linf Norm: 9.9278, Test Linf Norm: 1.3940\n",
            "Epoch 104: Train Loss: 0.0079, Test Loss: 0.0079, Train L1 Norm: 0.1159, Test L1 Norm: 0.0290, Train Linf Norm: 9.5750, Test Linf Norm: 1.4050\n",
            "Epoch 105: Train Loss: 0.0079, Test Loss: 0.0079, Train L1 Norm: 0.1159, Test L1 Norm: 0.0292, Train Linf Norm: 9.5865, Test Linf Norm: 1.4218\n",
            "Epoch 106: Train Loss: 0.0081, Test Loss: 0.0081, Train L1 Norm: 0.1163, Test L1 Norm: 0.0303, Train Linf Norm: 9.6124, Test Linf Norm: 1.4845\n",
            "Epoch 107: Train Loss: 0.0079, Test Loss: 0.0077, Train L1 Norm: 0.1149, Test L1 Norm: 0.0294, Train Linf Norm: 9.5284, Test Linf Norm: 1.4317\n",
            "Epoch 108: Train Loss: 0.0079, Test Loss: 0.0078, Train L1 Norm: 0.1175, Test L1 Norm: 0.0296, Train Linf Norm: 9.7793, Test Linf Norm: 1.4618\n",
            "Epoch 109: Train Loss: 0.0077, Test Loss: 0.0078, Train L1 Norm: 0.1165, Test L1 Norm: 0.0294, Train Linf Norm: 9.6750, Test Linf Norm: 1.4511\n",
            "Epoch 110: Train Loss: 0.0077, Test Loss: 0.0077, Train L1 Norm: 0.1153, Test L1 Norm: 0.0302, Train Linf Norm: 9.5835, Test Linf Norm: 1.5084\n",
            "Epoch 111: Train Loss: 0.0078, Test Loss: 0.0076, Train L1 Norm: 0.1195, Test L1 Norm: 0.0290, Train Linf Norm: 9.9471, Test Linf Norm: 1.4245\n",
            "Epoch 112: Train Loss: 0.0079, Test Loss: 0.0088, Train L1 Norm: 0.1138, Test L1 Norm: 0.0312, Train Linf Norm: 9.3884, Test Linf Norm: 1.5187\n",
            "Epoch 113: Train Loss: 0.0078, Test Loss: 0.0081, Train L1 Norm: 0.1148, Test L1 Norm: 0.0288, Train Linf Norm: 9.5243, Test Linf Norm: 1.4047\n",
            "Epoch 114: Train Loss: 0.0077, Test Loss: 0.0079, Train L1 Norm: 0.1146, Test L1 Norm: 0.0292, Train Linf Norm: 9.4829, Test Linf Norm: 1.4318\n",
            "Epoch 115: Train Loss: 0.0078, Test Loss: 0.0076, Train L1 Norm: 0.1168, Test L1 Norm: 0.0291, Train Linf Norm: 9.6962, Test Linf Norm: 1.4241\n",
            "Epoch 116: Train Loss: 0.0078, Test Loss: 0.0078, Train L1 Norm: 0.1173, Test L1 Norm: 0.0289, Train Linf Norm: 9.6979, Test Linf Norm: 1.4192\n",
            "Epoch 117: Train Loss: 0.0078, Test Loss: 0.0078, Train L1 Norm: 0.1189, Test L1 Norm: 0.0294, Train Linf Norm: 9.8617, Test Linf Norm: 1.4525\n",
            "Epoch 118: Train Loss: 0.0078, Test Loss: 0.0076, Train L1 Norm: 0.1167, Test L1 Norm: 0.0296, Train Linf Norm: 9.6937, Test Linf Norm: 1.4677\n",
            "Epoch 119: Train Loss: 0.0079, Test Loss: 0.0077, Train L1 Norm: 0.1216, Test L1 Norm: 0.0290, Train Linf Norm: 10.1227, Test Linf Norm: 1.4132\n",
            "Epoch 120: Train Loss: 0.0078, Test Loss: 0.0077, Train L1 Norm: 0.1140, Test L1 Norm: 0.0295, Train Linf Norm: 9.4602, Test Linf Norm: 1.4488\n",
            "Epoch 121: Train Loss: 0.0077, Test Loss: 0.0076, Train L1 Norm: 0.1179, Test L1 Norm: 0.0302, Train Linf Norm: 9.8202, Test Linf Norm: 1.5141\n",
            "Epoch 122: Train Loss: 0.0077, Test Loss: 0.0077, Train L1 Norm: 0.1238, Test L1 Norm: 0.0299, Train Linf Norm: 10.3601, Test Linf Norm: 1.4752\n",
            "Epoch 123: Train Loss: 0.0080, Test Loss: 0.0079, Train L1 Norm: 0.1172, Test L1 Norm: 0.0291, Train Linf Norm: 9.7289, Test Linf Norm: 1.4295\n",
            "Epoch 124: Train Loss: 0.0079, Test Loss: 0.0075, Train L1 Norm: 0.1151, Test L1 Norm: 0.0290, Train Linf Norm: 9.5464, Test Linf Norm: 1.4158\n",
            "Epoch 125: Train Loss: 0.0076, Test Loss: 0.0080, Train L1 Norm: 0.1191, Test L1 Norm: 0.0298, Train Linf Norm: 9.9299, Test Linf Norm: 1.4710\n",
            "Epoch 126: Train Loss: 0.0079, Test Loss: 0.0076, Train L1 Norm: 0.1189, Test L1 Norm: 0.0295, Train Linf Norm: 9.8868, Test Linf Norm: 1.4603\n",
            "Epoch 127: Train Loss: 0.0078, Test Loss: 0.0077, Train L1 Norm: 0.1163, Test L1 Norm: 0.0293, Train Linf Norm: 9.6792, Test Linf Norm: 1.4543\n",
            "Epoch 128: Train Loss: 0.0077, Test Loss: 0.0081, Train L1 Norm: 0.1189, Test L1 Norm: 0.0297, Train Linf Norm: 9.9330, Test Linf Norm: 1.4729\n",
            "Epoch 129: Train Loss: 0.0077, Test Loss: 0.0076, Train L1 Norm: 0.1139, Test L1 Norm: 0.0300, Train Linf Norm: 9.3980, Test Linf Norm: 1.5003\n",
            "Epoch 130: Train Loss: 0.0078, Test Loss: 0.0088, Train L1 Norm: 0.1186, Test L1 Norm: 0.0292, Train Linf Norm: 9.8582, Test Linf Norm: 1.3773\n",
            "Epoch 131: Train Loss: 0.0078, Test Loss: 0.0076, Train L1 Norm: 0.1143, Test L1 Norm: 0.0292, Train Linf Norm: 9.4366, Test Linf Norm: 1.4408\n",
            "Epoch 132: Train Loss: 0.0077, Test Loss: 0.0079, Train L1 Norm: 0.1157, Test L1 Norm: 0.0300, Train Linf Norm: 9.5670, Test Linf Norm: 1.4823\n",
            "Epoch 133: Train Loss: 0.0076, Test Loss: 0.0079, Train L1 Norm: 0.1134, Test L1 Norm: 0.0294, Train Linf Norm: 9.3902, Test Linf Norm: 1.4433\n",
            "Epoch 134: Train Loss: 0.0074, Test Loss: 0.0075, Train L1 Norm: 0.1166, Test L1 Norm: 0.0292, Train Linf Norm: 9.7127, Test Linf Norm: 1.4313\n",
            "Epoch 135: Train Loss: 0.0074, Test Loss: 0.0078, Train L1 Norm: 0.1158, Test L1 Norm: 0.0294, Train Linf Norm: 9.6371, Test Linf Norm: 1.4562\n",
            "Epoch 136: Train Loss: 0.0075, Test Loss: 0.0077, Train L1 Norm: 0.1168, Test L1 Norm: 0.0290, Train Linf Norm: 9.6607, Test Linf Norm: 1.4269\n",
            "Epoch 137: Train Loss: 0.0074, Test Loss: 0.0077, Train L1 Norm: 0.1152, Test L1 Norm: 0.0294, Train Linf Norm: 9.6060, Test Linf Norm: 1.4567\n",
            "Epoch 138: Train Loss: 0.0075, Test Loss: 0.0077, Train L1 Norm: 0.1171, Test L1 Norm: 0.0297, Train Linf Norm: 9.4413, Test Linf Norm: 1.4777\n",
            "Epoch 139: Train Loss: 0.0074, Test Loss: 0.0074, Train L1 Norm: 0.1183, Test L1 Norm: 0.0291, Train Linf Norm: 9.8785, Test Linf Norm: 1.4390\n",
            "Epoch 140: Train Loss: 0.0075, Test Loss: 0.0076, Train L1 Norm: 0.1162, Test L1 Norm: 0.0287, Train Linf Norm: 9.6558, Test Linf Norm: 1.3954\n",
            "Epoch 141: Train Loss: 0.0074, Test Loss: 0.0074, Train L1 Norm: 0.1132, Test L1 Norm: 0.0294, Train Linf Norm: 9.3595, Test Linf Norm: 1.4629\n",
            "Epoch 142: Train Loss: 0.0074, Test Loss: 0.0078, Train L1 Norm: 0.1196, Test L1 Norm: 0.0292, Train Linf Norm: 9.9747, Test Linf Norm: 1.4386\n",
            "Epoch 143: Train Loss: 0.0074, Test Loss: 0.0075, Train L1 Norm: 0.1160, Test L1 Norm: 0.0292, Train Linf Norm: 9.6708, Test Linf Norm: 1.4464\n",
            "Epoch 144: Train Loss: 0.0074, Test Loss: 0.0075, Train L1 Norm: 0.1174, Test L1 Norm: 0.0289, Train Linf Norm: 9.7907, Test Linf Norm: 1.4178\n",
            "Epoch 145: Train Loss: 0.0075, Test Loss: 0.0074, Train L1 Norm: 0.1145, Test L1 Norm: 0.0287, Train Linf Norm: 9.5100, Test Linf Norm: 1.4117\n",
            "Epoch 146: Train Loss: 0.0075, Test Loss: 0.0075, Train L1 Norm: 0.1140, Test L1 Norm: 0.0289, Train Linf Norm: 9.4749, Test Linf Norm: 1.4224\n",
            "Epoch 147: Train Loss: 0.0074, Test Loss: 0.0078, Train L1 Norm: 0.1163, Test L1 Norm: 0.0291, Train Linf Norm: 9.6573, Test Linf Norm: 1.4370\n",
            "Epoch 148: Train Loss: 0.0075, Test Loss: 0.0075, Train L1 Norm: 0.1167, Test L1 Norm: 0.0291, Train Linf Norm: 9.7366, Test Linf Norm: 1.4315\n",
            "Epoch 149: Train Loss: 0.0073, Test Loss: 0.0074, Train L1 Norm: 0.1153, Test L1 Norm: 0.0292, Train Linf Norm: 9.5623, Test Linf Norm: 1.4442\n",
            "Epoch 150: Train Loss: 0.0073, Test Loss: 0.0074, Train L1 Norm: 0.1155, Test L1 Norm: 0.0292, Train Linf Norm: 9.6039, Test Linf Norm: 1.4447\n",
            "Epoch 151: Train Loss: 0.0073, Test Loss: 0.0075, Train L1 Norm: 0.1164, Test L1 Norm: 0.0291, Train Linf Norm: 9.6713, Test Linf Norm: 1.4342\n",
            "Epoch 152: Train Loss: 0.0073, Test Loss: 0.0074, Train L1 Norm: 0.1164, Test L1 Norm: 0.0291, Train Linf Norm: 9.6929, Test Linf Norm: 1.4402\n",
            "Epoch 153: Train Loss: 0.0073, Test Loss: 0.0074, Train L1 Norm: 0.1162, Test L1 Norm: 0.0292, Train Linf Norm: 9.6644, Test Linf Norm: 1.4527\n",
            "Epoch 154: Train Loss: 0.0073, Test Loss: 0.0075, Train L1 Norm: 0.1158, Test L1 Norm: 0.0293, Train Linf Norm: 9.6210, Test Linf Norm: 1.4553\n",
            "Epoch 155: Train Loss: 0.0073, Test Loss: 0.0074, Train L1 Norm: 0.1166, Test L1 Norm: 0.0291, Train Linf Norm: 9.7196, Test Linf Norm: 1.4434\n",
            "Epoch 156: Train Loss: 0.0073, Test Loss: 0.0074, Train L1 Norm: 0.1163, Test L1 Norm: 0.0291, Train Linf Norm: 9.6682, Test Linf Norm: 1.4346\n",
            "Epoch 157: Train Loss: 0.0073, Test Loss: 0.0074, Train L1 Norm: 0.1178, Test L1 Norm: 0.0292, Train Linf Norm: 9.8189, Test Linf Norm: 1.4449\n",
            "Epoch 158: Train Loss: 0.0073, Test Loss: 0.0074, Train L1 Norm: 0.1152, Test L1 Norm: 0.0291, Train Linf Norm: 9.5756, Test Linf Norm: 1.4410\n",
            "Epoch 159: Train Loss: 0.0073, Test Loss: 0.0076, Train L1 Norm: 0.1163, Test L1 Norm: 0.0291, Train Linf Norm: 9.6817, Test Linf Norm: 1.4381\n",
            "Epoch 160: Train Loss: 0.0073, Test Loss: 0.0075, Train L1 Norm: 0.1166, Test L1 Norm: 0.0292, Train Linf Norm: 9.7078, Test Linf Norm: 1.4419\n",
            "Epoch 161: Train Loss: 0.0073, Test Loss: 0.0074, Train L1 Norm: 0.1163, Test L1 Norm: 0.0292, Train Linf Norm: 9.6998, Test Linf Norm: 1.4497\n",
            "Epoch 162: Train Loss: 0.0073, Test Loss: 0.0074, Train L1 Norm: 0.1165, Test L1 Norm: 0.0290, Train Linf Norm: 9.7292, Test Linf Norm: 1.4342\n",
            "Epoch 163: Train Loss: 0.0073, Test Loss: 0.0074, Train L1 Norm: 0.1162, Test L1 Norm: 0.0291, Train Linf Norm: 9.5788, Test Linf Norm: 1.4404\n",
            "Epoch 164: Train Loss: 0.0073, Test Loss: 0.0074, Train L1 Norm: 0.1172, Test L1 Norm: 0.0290, Train Linf Norm: 9.7295, Test Linf Norm: 1.4269\n",
            "Epoch 165: Train Loss: 0.0073, Test Loss: 0.0074, Train L1 Norm: 0.1164, Test L1 Norm: 0.0291, Train Linf Norm: 9.6846, Test Linf Norm: 1.4440\n",
            "Epoch 166: Train Loss: 0.0073, Test Loss: 0.0074, Train L1 Norm: 0.1158, Test L1 Norm: 0.0293, Train Linf Norm: 9.6619, Test Linf Norm: 1.4537\n",
            "Epoch 167: Train Loss: 0.0073, Test Loss: 0.0074, Train L1 Norm: 0.1171, Test L1 Norm: 0.0292, Train Linf Norm: 9.7505, Test Linf Norm: 1.4493\n",
            "Epoch 168: Train Loss: 0.0073, Test Loss: 0.0074, Train L1 Norm: 0.1171, Test L1 Norm: 0.0291, Train Linf Norm: 9.7125, Test Linf Norm: 1.4417\n",
            "Epoch 169: Train Loss: 0.0072, Test Loss: 0.0074, Train L1 Norm: 0.1158, Test L1 Norm: 0.0291, Train Linf Norm: 9.6143, Test Linf Norm: 1.4334\n",
            "Epoch 170: Train Loss: 0.0073, Test Loss: 0.0074, Train L1 Norm: 0.1170, Test L1 Norm: 0.0292, Train Linf Norm: 9.7209, Test Linf Norm: 1.4454\n",
            "Epoch 171: Train Loss: 0.0073, Test Loss: 0.0074, Train L1 Norm: 0.1170, Test L1 Norm: 0.0290, Train Linf Norm: 9.7179, Test Linf Norm: 1.4294\n",
            "Epoch 172: Train Loss: 0.0072, Test Loss: 0.0073, Train L1 Norm: 0.1162, Test L1 Norm: 0.0290, Train Linf Norm: 9.6726, Test Linf Norm: 1.4385\n",
            "Epoch 173: Train Loss: 0.0072, Test Loss: 0.0074, Train L1 Norm: 0.1163, Test L1 Norm: 0.0290, Train Linf Norm: 9.6797, Test Linf Norm: 1.4336\n",
            "Epoch 174: Train Loss: 0.0072, Test Loss: 0.0073, Train L1 Norm: 0.1157, Test L1 Norm: 0.0291, Train Linf Norm: 9.5345, Test Linf Norm: 1.4396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 10:39:12,562]\u001b[0m Trial 3 finished with value: 0.028977795132249594 and parameters: {'n_layers': 4, 'n_units_0': 424, 'n_units_1': 897, 'n_units_2': 1291, 'n_units_3': 138, 'hidden_activation': 'ELU', 'output_activation': 'Linear', 'loss': 'MAE', 'optimizer': 'Adam', 'lr': 0.00020202474019662773, 'batch_size': 96, 'n_epochs': 175, 'scheduler': 'ReduceLROnPlateau', 'weight_decay': 0.004113904559939576, 'beta1': 0.9847833120865573, 'beta2': 0.9995619130347876, 'factor': 0.37020334279924105, 'patience': 8, 'threshold': 0.0012516298501235883}. Best is trial 0 with value: 0.012935786840319633.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 175: Train Loss: 0.0073, Test Loss: 0.0073, Train L1 Norm: 0.1158, Test L1 Norm: 0.0290, Train Linf Norm: 9.5672, Test Linf Norm: 1.4303\n",
            "Epoch 1: Train Loss: 21.1528, Test Loss: 20.6405, Train L1 Norm: 1.0212, Test L1 Norm: 1.0000, Train Linf Norm: 1.5744, Test Linf Norm: 1.0000\n",
            "Epoch 2: Train Loss: 20.5565, Test Loss: 20.6405, Train L1 Norm: 1.0524, Test L1 Norm: 1.0000, Train Linf Norm: 5.8520, Test Linf Norm: 1.0000\n",
            "Epoch 3: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 0.9997, Train Linf Norm: 1.0383, Test Linf Norm: 1.0029\n",
            "Epoch 4: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.1711, Test L1 Norm: 1.1196, Train Linf Norm: 17.5175, Test Linf Norm: 12.4808\n",
            "Epoch 5: Train Loss: 20.4333, Test Loss: 20.6405, Train L1 Norm: 4.7914, Test L1 Norm: 1.2145, Train Linf Norm: 362.9899, Test Linf Norm: 21.3932\n",
            "Epoch 6: Train Loss: 20.4333, Test Loss: 20.6405, Train L1 Norm: 3.9837, Test L1 Norm: 1.2289, Train Linf Norm: 285.5290, Test Linf Norm: 21.9385\n",
            "Epoch 7: Train Loss: 20.4333, Test Loss: 20.6405, Train L1 Norm: 2.8090, Test L1 Norm: 1.1140, Train Linf Norm: 173.0858, Test Linf Norm: 11.2949\n",
            "Epoch 8: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.4653, Test L1 Norm: 1.0418, Train Linf Norm: 45.5940, Test Linf Norm: 4.7871\n",
            "Epoch 9: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.1150, Test L1 Norm: 1.0132, Train Linf Norm: 12.0856, Test Linf Norm: 2.4980\n",
            "Epoch 10: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0451, Test L1 Norm: 1.0000, Train Linf Norm: 5.3555, Test Linf Norm: 1.0000\n",
            "Epoch 11: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0041, Test Linf Norm: 1.0000\n",
            "Epoch 12: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 13: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 14: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 15: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 16: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 17: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 18: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 19: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 20: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 21: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 22: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 23: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 24: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 25: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 26: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 27: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 28: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 29: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 30: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 31: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 32: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 33: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 34: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 35: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 36: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 37: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 38: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 39: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 40: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 41: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 42: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 43: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 44: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 45: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 46: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 47: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 48: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 49: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 50: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 51: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 52: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 53: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 54: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 55: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 56: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 57: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 58: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 59: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 60: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 61: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 62: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 63: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 64: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 65: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 66: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 67: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 68: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 69: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 70: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 71: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 72: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 73: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 74: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 75: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 76: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 77: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 78: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 79: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 80: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 81: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 82: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 83: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 84: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 85: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 86: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 87: Train Loss: 19.3581, Test Loss: 14.2659, Train L1 Norm: 1.3491, Test L1 Norm: 0.9860, Train Linf Norm: 38.6412, Test Linf Norm: 20.0559\n",
            "Epoch 88: Train Loss: 7.4391, Test Loss: 4.1306, Train L1 Norm: 3.9211, Test L1 Norm: 2.1730, Train Linf Norm: 281.7816, Test Linf Norm: 92.3741\n",
            "Epoch 89: Train Loss: 3.9537, Test Loss: 3.7719, Train L1 Norm: 5.9647, Test L1 Norm: 2.5272, Train Linf Norm: 433.2567, Test Linf Norm: 106.7331\n",
            "Epoch 90: Train Loss: 3.8443, Test Loss: 3.7113, Train L1 Norm: 6.1908, Test L1 Norm: 2.5243, Train Linf Norm: 446.3301, Test Linf Norm: 106.2283\n",
            "Epoch 91: Train Loss: 3.7842, Test Loss: 3.6496, Train L1 Norm: 6.1073, Test L1 Norm: 2.4903, Train Linf Norm: 438.6981, Test Linf Norm: 104.3076\n",
            "Epoch 92: Train Loss: 3.7202, Test Loss: 3.5840, Train L1 Norm: 5.9157, Test L1 Norm: 2.4305, Train Linf Norm: 421.8342, Test Linf Norm: 101.1995\n",
            "Epoch 93: Train Loss: 3.6514, Test Loss: 3.5129, Train L1 Norm: 5.5765, Test L1 Norm: 2.3517, Train Linf Norm: 389.6722, Test Linf Norm: 97.1359\n",
            "Epoch 94: Train Loss: 3.5756, Test Loss: 3.4348, Train L1 Norm: 5.3192, Test L1 Norm: 2.2402, Train Linf Norm: 369.7919, Test Linf Norm: 91.6187\n",
            "Epoch 95: Train Loss: 3.4910, Test Loss: 3.3456, Train L1 Norm: 4.8846, Test L1 Norm: 2.1257, Train Linf Norm: 332.2844, Test Linf Norm: 85.8175\n",
            "Epoch 96: Train Loss: 3.3957, Test Loss: 3.2451, Train L1 Norm: 4.4424, Test L1 Norm: 1.9947, Train Linf Norm: 299.7975, Test Linf Norm: 79.1916\n",
            "Epoch 97: Train Loss: 3.2877, Test Loss: 3.1318, Train L1 Norm: 3.7714, Test L1 Norm: 1.8182, Train Linf Norm: 241.3341, Test Linf Norm: 70.7344\n",
            "Epoch 98: Train Loss: 3.1633, Test Loss: 2.9996, Train L1 Norm: 3.2932, Test L1 Norm: 1.6297, Train Linf Norm: 206.3761, Test Linf Norm: 61.6724\n",
            "Epoch 99: Train Loss: 3.0183, Test Loss: 2.8465, Train L1 Norm: 2.5724, Test L1 Norm: 1.3920, Train Linf Norm: 147.0210, Test Linf Norm: 51.2839\n",
            "Epoch 100: Train Loss: 2.8478, Test Loss: 2.6643, Train L1 Norm: 1.8193, Test L1 Norm: 1.1942, Train Linf Norm: 89.3488, Test Linf Norm: 42.7953\n",
            "Epoch 101: Train Loss: 2.6455, Test Loss: 2.4495, Train L1 Norm: 1.4497, Test L1 Norm: 0.9883, Train Linf Norm: 66.6021, Test Linf Norm: 33.3832\n",
            "Epoch 102: Train Loss: 2.4015, Test Loss: 2.1840, Train L1 Norm: 1.1305, Test L1 Norm: 0.7856, Train Linf Norm: 47.7272, Test Linf Norm: 23.6338\n",
            "Epoch 103: Train Loss: 2.1045, Test Loss: 1.8653, Train L1 Norm: 0.8253, Test L1 Norm: 0.5675, Train Linf Norm: 31.0378, Test Linf Norm: 14.0500\n",
            "Epoch 104: Train Loss: 1.7524, Test Loss: 1.5015, Train L1 Norm: 0.5377, Test L1 Norm: 0.3794, Train Linf Norm: 15.8480, Test Linf Norm: 6.0463\n",
            "Epoch 105: Train Loss: 1.3676, Test Loss: 1.1295, Train L1 Norm: 0.3613, Test L1 Norm: 0.3025, Train Linf Norm: 6.1864, Test Linf Norm: 3.0962\n",
            "Epoch 106: Train Loss: 1.0159, Test Loss: 0.8381, Train L1 Norm: 0.3002, Test L1 Norm: 0.2880, Train Linf Norm: 3.2362, Test Linf Norm: 2.6467\n",
            "Epoch 107: Train Loss: 0.7837, Test Loss: 0.6877, Train L1 Norm: 0.2952, Test L1 Norm: 0.3028, Train Linf Norm: 2.5640, Test Linf Norm: 2.8096\n",
            "Epoch 108: Train Loss: 0.6797, Test Loss: 0.6327, Train L1 Norm: 0.3134, Test L1 Norm: 0.3211, Train Linf Norm: 2.8353, Test Linf Norm: 3.0877\n",
            "Epoch 109: Train Loss: 0.6407, Test Loss: 0.6105, Train L1 Norm: 0.3280, Test L1 Norm: 0.3320, Train Linf Norm: 3.1111, Test Linf Norm: 3.2821\n",
            "Epoch 110: Train Loss: 0.6226, Test Loss: 0.5975, Train L1 Norm: 0.3366, Test L1 Norm: 0.3376, Train Linf Norm: 3.3420, Test Linf Norm: 3.4743\n",
            "Epoch 111: Train Loss: 0.6111, Test Loss: 0.5891, Train L1 Norm: 0.3410, Test L1 Norm: 0.3412, Train Linf Norm: 3.4989, Test Linf Norm: 3.6567\n",
            "Epoch 112: Train Loss: 0.6028, Test Loss: 0.5821, Train L1 Norm: 0.3444, Test L1 Norm: 0.3448, Train Linf Norm: 3.7452, Test Linf Norm: 3.9909\n",
            "Epoch 113: Train Loss: 0.5961, Test Loss: 0.5772, Train L1 Norm: 0.3476, Test L1 Norm: 0.3459, Train Linf Norm: 4.0910, Test Linf Norm: 4.1324\n",
            "Epoch 114: Train Loss: 0.5912, Test Loss: 0.5725, Train L1 Norm: 0.3488, Test L1 Norm: 0.3479, Train Linf Norm: 4.3137, Test Linf Norm: 4.4102\n",
            "Epoch 115: Train Loss: 0.5867, Test Loss: 0.5700, Train L1 Norm: 0.3504, Test L1 Norm: 0.3479, Train Linf Norm: 4.6054, Test Linf Norm: 4.4610\n",
            "Epoch 116: Train Loss: 0.5834, Test Loss: 0.5664, Train L1 Norm: 0.3502, Test L1 Norm: 0.3500, Train Linf Norm: 4.6486, Test Linf Norm: 4.7567\n",
            "Epoch 117: Train Loss: 0.5807, Test Loss: 0.5649, Train L1 Norm: 0.3504, Test L1 Norm: 0.3518, Train Linf Norm: 4.7952, Test Linf Norm: 5.0135\n",
            "Epoch 118: Train Loss: 0.5782, Test Loss: 0.5626, Train L1 Norm: 0.3514, Test L1 Norm: 0.3530, Train Linf Norm: 4.9722, Test Linf Norm: 5.2723\n",
            "Epoch 119: Train Loss: 0.5758, Test Loss: 0.5608, Train L1 Norm: 0.3521, Test L1 Norm: 0.3529, Train Linf Norm: 5.1216, Test Linf Norm: 5.3980\n",
            "Epoch 120: Train Loss: 0.5740, Test Loss: 0.5591, Train L1 Norm: 0.3518, Test L1 Norm: 0.3541, Train Linf Norm: 5.1818, Test Linf Norm: 5.6173\n",
            "Epoch 121: Train Loss: 0.5723, Test Loss: 0.5580, Train L1 Norm: 0.3523, Test L1 Norm: 0.3534, Train Linf Norm: 5.5019, Test Linf Norm: 5.6315\n",
            "Epoch 122: Train Loss: 0.5702, Test Loss: 0.5557, Train L1 Norm: 0.3517, Test L1 Norm: 0.3557, Train Linf Norm: 5.4857, Test Linf Norm: 5.9933\n",
            "Epoch 123: Train Loss: 0.5686, Test Loss: 0.5544, Train L1 Norm: 0.3523, Test L1 Norm: 0.3552, Train Linf Norm: 5.7111, Test Linf Norm: 6.0312\n",
            "Epoch 124: Train Loss: 0.5671, Test Loss: 0.5528, Train L1 Norm: 0.3522, Test L1 Norm: 0.3556, Train Linf Norm: 5.7387, Test Linf Norm: 6.1724\n",
            "Epoch 125: Train Loss: 0.5655, Test Loss: 0.5530, Train L1 Norm: 0.3524, Test L1 Norm: 0.3532, Train Linf Norm: 5.9434, Test Linf Norm: 5.9805\n",
            "Epoch 126: Train Loss: 0.5638, Test Loss: 0.5496, Train L1 Norm: 0.3528, Test L1 Norm: 0.3533, Train Linf Norm: 6.0194, Test Linf Norm: 6.1782\n",
            "Epoch 127: Train Loss: 0.5621, Test Loss: 0.5485, Train L1 Norm: 0.3515, Test L1 Norm: 0.3524, Train Linf Norm: 5.9618, Test Linf Norm: 6.1454\n",
            "Epoch 128: Train Loss: 0.5604, Test Loss: 0.5466, Train L1 Norm: 0.3510, Test L1 Norm: 0.3529, Train Linf Norm: 6.0549, Test Linf Norm: 6.2881\n",
            "Epoch 129: Train Loss: 0.5589, Test Loss: 0.5456, Train L1 Norm: 0.3509, Test L1 Norm: 0.3518, Train Linf Norm: 6.1077, Test Linf Norm: 6.2466\n",
            "Epoch 130: Train Loss: 0.5570, Test Loss: 0.5434, Train L1 Norm: 0.3506, Test L1 Norm: 0.3524, Train Linf Norm: 6.1567, Test Linf Norm: 6.4109\n",
            "Epoch 131: Train Loss: 0.5556, Test Loss: 0.5451, Train L1 Norm: 0.3502, Test L1 Norm: 0.3505, Train Linf Norm: 6.2020, Test Linf Norm: 6.1705\n",
            "Epoch 132: Train Loss: 0.5535, Test Loss: 0.5401, Train L1 Norm: 0.3502, Test L1 Norm: 0.3511, Train Linf Norm: 6.1926, Test Linf Norm: 6.4670\n",
            "Epoch 133: Train Loss: 0.5518, Test Loss: 0.5428, Train L1 Norm: 0.3497, Test L1 Norm: 0.3485, Train Linf Norm: 6.2839, Test Linf Norm: 6.1762\n",
            "Epoch 134: Train Loss: 0.5504, Test Loss: 0.5371, Train L1 Norm: 0.3494, Test L1 Norm: 0.3498, Train Linf Norm: 6.3982, Test Linf Norm: 6.4500\n",
            "Epoch 135: Train Loss: 0.5485, Test Loss: 0.5354, Train L1 Norm: 0.3488, Test L1 Norm: 0.3489, Train Linf Norm: 6.2892, Test Linf Norm: 6.4149\n",
            "Epoch 136: Train Loss: 0.5465, Test Loss: 0.5337, Train L1 Norm: 0.3476, Test L1 Norm: 0.3490, Train Linf Norm: 6.3632, Test Linf Norm: 6.4622\n",
            "Epoch 137: Train Loss: 0.5446, Test Loss: 0.5314, Train L1 Norm: 0.3486, Test L1 Norm: 0.3469, Train Linf Norm: 6.4111, Test Linf Norm: 6.4087\n",
            "Epoch 138: Train Loss: 0.5430, Test Loss: 0.5308, Train L1 Norm: 0.3465, Test L1 Norm: 0.3467, Train Linf Norm: 6.3015, Test Linf Norm: 6.3455\n",
            "Epoch 139: Train Loss: 0.5408, Test Loss: 0.5275, Train L1 Norm: 0.3465, Test L1 Norm: 0.3469, Train Linf Norm: 6.4448, Test Linf Norm: 6.4760\n",
            "Epoch 140: Train Loss: 0.5388, Test Loss: 0.5255, Train L1 Norm: 0.3458, Test L1 Norm: 0.3458, Train Linf Norm: 6.3736, Test Linf Norm: 6.4439\n",
            "Epoch 141: Train Loss: 0.5367, Test Loss: 0.5233, Train L1 Norm: 0.3450, Test L1 Norm: 0.3457, Train Linf Norm: 6.2964, Test Linf Norm: 6.4842\n",
            "Epoch 142: Train Loss: 0.5345, Test Loss: 0.5236, Train L1 Norm: 0.3450, Test L1 Norm: 0.3434, Train Linf Norm: 6.3423, Test Linf Norm: 6.2709\n",
            "Epoch 143: Train Loss: 0.5324, Test Loss: 0.5190, Train L1 Norm: 0.3440, Test L1 Norm: 0.3430, Train Linf Norm: 6.3835, Test Linf Norm: 6.3288\n",
            "Epoch 144: Train Loss: 0.5297, Test Loss: 0.5166, Train L1 Norm: 0.3422, Test L1 Norm: 0.3427, Train Linf Norm: 6.1733, Test Linf Norm: 6.3572\n",
            "Epoch 145: Train Loss: 0.5274, Test Loss: 0.5134, Train L1 Norm: 0.3428, Test L1 Norm: 0.3411, Train Linf Norm: 6.3244, Test Linf Norm: 6.3300\n",
            "Epoch 146: Train Loss: 0.5247, Test Loss: 0.5153, Train L1 Norm: 0.3401, Test L1 Norm: 0.3406, Train Linf Norm: 6.1806, Test Linf Norm: 6.1884\n",
            "Epoch 147: Train Loss: 0.5216, Test Loss: 0.5074, Train L1 Norm: 0.3417, Test L1 Norm: 0.3402, Train Linf Norm: 6.2642, Test Linf Norm: 6.3827\n",
            "Epoch 148: Train Loss: 0.5192, Test Loss: 0.5055, Train L1 Norm: 0.3393, Test L1 Norm: 0.3383, Train Linf Norm: 6.2947, Test Linf Norm: 6.2102\n",
            "Epoch 149: Train Loss: 0.5165, Test Loss: 0.5031, Train L1 Norm: 0.3381, Test L1 Norm: 0.3371, Train Linf Norm: 6.1070, Test Linf Norm: 6.1392\n",
            "Epoch 150: Train Loss: 0.5132, Test Loss: 0.4983, Train L1 Norm: 0.3374, Test L1 Norm: 0.3369, Train Linf Norm: 6.2082, Test Linf Norm: 6.2216\n",
            "Epoch 151: Train Loss: 0.5100, Test Loss: 0.4955, Train L1 Norm: 0.3358, Test L1 Norm: 0.3353, Train Linf Norm: 6.0656, Test Linf Norm: 6.1097\n",
            "Epoch 152: Train Loss: 0.5068, Test Loss: 0.4920, Train L1 Norm: 0.3350, Test L1 Norm: 0.3342, Train Linf Norm: 6.1088, Test Linf Norm: 6.0997\n",
            "Epoch 153: Train Loss: 0.5034, Test Loss: 0.4881, Train L1 Norm: 0.3333, Test L1 Norm: 0.3337, Train Linf Norm: 6.0156, Test Linf Norm: 6.1171\n",
            "Epoch 154: Train Loss: 0.5000, Test Loss: 0.4843, Train L1 Norm: 0.3330, Test L1 Norm: 0.3316, Train Linf Norm: 5.9642, Test Linf Norm: 6.0045\n",
            "Epoch 155: Train Loss: 0.4962, Test Loss: 0.4807, Train L1 Norm: 0.3312, Test L1 Norm: 0.3311, Train Linf Norm: 5.9351, Test Linf Norm: 6.0582\n",
            "Epoch 156: Train Loss: 0.4929, Test Loss: 0.4768, Train L1 Norm: 0.3293, Test L1 Norm: 0.3296, Train Linf Norm: 5.8604, Test Linf Norm: 5.9613\n",
            "Epoch 157: Train Loss: 0.4894, Test Loss: 0.4732, Train L1 Norm: 0.3279, Test L1 Norm: 0.3282, Train Linf Norm: 5.7671, Test Linf Norm: 5.8810\n",
            "Epoch 158: Train Loss: 0.4857, Test Loss: 0.4704, Train L1 Norm: 0.3271, Test L1 Norm: 0.3259, Train Linf Norm: 5.6645, Test Linf Norm: 5.6508\n",
            "Epoch 159: Train Loss: 0.4819, Test Loss: 0.4657, Train L1 Norm: 0.3250, Test L1 Norm: 0.3252, Train Linf Norm: 5.6383, Test Linf Norm: 5.6904\n",
            "Epoch 160: Train Loss: 0.4783, Test Loss: 0.4653, Train L1 Norm: 0.3235, Test L1 Norm: 0.3224, Train Linf Norm: 5.5643, Test Linf Norm: 5.4013\n",
            "Epoch 161: Train Loss: 0.4749, Test Loss: 0.4584, Train L1 Norm: 0.3225, Test L1 Norm: 0.3217, Train Linf Norm: 5.3298, Test Linf Norm: 5.4712\n",
            "Epoch 162: Train Loss: 0.4711, Test Loss: 0.4555, Train L1 Norm: 0.3209, Test L1 Norm: 0.3189, Train Linf Norm: 5.3979, Test Linf Norm: 5.2648\n",
            "Epoch 163: Train Loss: 0.4679, Test Loss: 0.4524, Train L1 Norm: 0.3180, Test L1 Norm: 0.3193, Train Linf Norm: 5.2422, Test Linf Norm: 5.3105\n",
            "Epoch 164: Train Loss: 0.4644, Test Loss: 0.4477, Train L1 Norm: 0.3177, Test L1 Norm: 0.3171, Train Linf Norm: 5.2436, Test Linf Norm: 5.2864\n",
            "Epoch 165: Train Loss: 0.4616, Test Loss: 0.4446, Train L1 Norm: 0.3160, Test L1 Norm: 0.3154, Train Linf Norm: 5.1658, Test Linf Norm: 5.1725\n",
            "Epoch 166: Train Loss: 0.4588, Test Loss: 0.4434, Train L1 Norm: 0.3141, Test L1 Norm: 0.3130, Train Linf Norm: 5.0254, Test Linf Norm: 4.9631\n",
            "Epoch 167: Train Loss: 0.4561, Test Loss: 0.4413, Train L1 Norm: 0.3129, Test L1 Norm: 0.3115, Train Linf Norm: 4.9429, Test Linf Norm: 4.8625\n",
            "Epoch 168: Train Loss: 0.4538, Test Loss: 0.4386, Train L1 Norm: 0.3116, Test L1 Norm: 0.3102, Train Linf Norm: 4.8769, Test Linf Norm: 4.7820\n",
            "Epoch 169: Train Loss: 0.4513, Test Loss: 0.4370, Train L1 Norm: 0.3097, Test L1 Norm: 0.3093, Train Linf Norm: 4.8244, Test Linf Norm: 4.7118\n",
            "Epoch 170: Train Loss: 0.4494, Test Loss: 0.4328, Train L1 Norm: 0.3091, Test L1 Norm: 0.3081, Train Linf Norm: 4.7877, Test Linf Norm: 4.7235\n",
            "Epoch 171: Train Loss: 0.4474, Test Loss: 0.4301, Train L1 Norm: 0.3076, Test L1 Norm: 0.3073, Train Linf Norm: 4.6969, Test Linf Norm: 4.7168\n",
            "Epoch 172: Train Loss: 0.4456, Test Loss: 0.4291, Train L1 Norm: 0.3063, Test L1 Norm: 0.3056, Train Linf Norm: 4.6773, Test Linf Norm: 4.5574\n",
            "Epoch 173: Train Loss: 0.4441, Test Loss: 0.4274, Train L1 Norm: 0.3053, Test L1 Norm: 0.3047, Train Linf Norm: 4.5620, Test Linf Norm: 4.5249\n",
            "Epoch 174: Train Loss: 0.4427, Test Loss: 0.4262, Train L1 Norm: 0.3045, Test L1 Norm: 0.3032, Train Linf Norm: 4.5141, Test Linf Norm: 4.4624\n",
            "Epoch 175: Train Loss: 0.4416, Test Loss: 0.4243, Train L1 Norm: 0.3031, Test L1 Norm: 0.3033, Train Linf Norm: 4.4760, Test Linf Norm: 4.4961\n",
            "Epoch 176: Train Loss: 0.4402, Test Loss: 0.4227, Train L1 Norm: 0.3030, Test L1 Norm: 0.3019, Train Linf Norm: 4.4741, Test Linf Norm: 4.4233\n",
            "Epoch 177: Train Loss: 0.4394, Test Loss: 0.4220, Train L1 Norm: 0.3013, Test L1 Norm: 0.3010, Train Linf Norm: 4.4150, Test Linf Norm: 4.3863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 10:50:27,307]\u001b[0m Trial 4 finished with value: 0.30045639176368716 and parameters: {'n_layers': 3, 'n_units_0': 696, 'n_units_1': 1895, 'n_units_2': 1569, 'hidden_activation': 'ELU', 'output_activation': 'ReLU', 'loss': 'MSE', 'optimizer': 'Adam', 'lr': 0.0008409600460964214, 'batch_size': 96, 'n_epochs': 178, 'scheduler': 'ReduceLROnPlateau', 'weight_decay': 0.002240699477743407, 'beta1': 0.9025981352990071, 'beta2': 0.9995581573861478, 'factor': 0.39905339110195304, 'patience': 9, 'threshold': 0.0040278834744635035}. Best is trial 0 with value: 0.012935786840319633.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 178: Train Loss: 0.4384, Test Loss: 0.4216, Train L1 Norm: 0.3008, Test L1 Norm: 0.3005, Train Linf Norm: 4.3498, Test Linf Norm: 4.3647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-84-e8f6ba3279cc>:148: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  t_max_fraction = trial.suggest_uniform('t_max_fraction', 0.05, 0.1)\n",
            "<ipython-input-84-e8f6ba3279cc>:153: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  eta_min = trial.suggest_loguniform(\"eta_min\", 1e-7, 1e-2)\n",
            "\u001b[32m[I 2023-05-21 10:50:36,873]\u001b[0m Trial 5 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.1079, Test Loss: 0.0056, Train L1 Norm: 2.1395, Test L1 Norm: 0.3547, Train Linf Norm: 63.0990, Test Linf Norm: 9.2812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 10:50:38,492]\u001b[0m Trial 6 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 7.5595, Test Loss: 0.0269, Train L1 Norm: 2.8117, Test L1 Norm: 0.2679, Train Linf Norm: 1030.0729, Test Linf Norm: 38.7594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-84-e8f6ba3279cc>:150: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  t_max_fraction = trial.suggest_uniform('t_max_fraction', 0.1, 0.2)\n",
            "\u001b[32m[I 2023-05-21 10:50:43,652]\u001b[0m Trial 7 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 450.0821, Test Loss: 20.6405, Train L1 Norm: 1.1590, Test L1 Norm: 1.0000, Train Linf Norm: 1.4608, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-84-e8f6ba3279cc>:146: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  t_max_fraction = trial.suggest_uniform('t_max_fraction', 0.1, 0.3)\n",
            "\u001b[32m[I 2023-05-21 10:50:47,247]\u001b[0m Trial 8 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.2177, Test Loss: 0.1897, Train L1 Norm: 3.0303, Test L1 Norm: 0.6047, Train Linf Norm: 174.9908, Test Linf Norm: 23.2419\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 10:50:49,565]\u001b[0m Trial 9 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 625.8397, Test Loss: 1.2418, Train L1 Norm: 735.2398, Test L1 Norm: 2.3743, Train Linf Norm: 33342.7238, Test Linf Norm: 124.9416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 10:50:52,447]\u001b[0m Trial 10 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 1.4544, Test Loss: 1.1655, Train L1 Norm: 3.1942, Test L1 Norm: 0.9896, Train Linf Norm: 158.9673, Test Linf Norm: 27.8242\n",
            "Epoch 1: Train Loss: 0.3784, Test Loss: 0.1645, Train L1 Norm: 0.4081, Test L1 Norm: 0.1391, Train Linf Norm: 30.7157, Test Linf Norm: 6.1108\n",
            "Epoch 2: Train Loss: 0.1435, Test Loss: 0.0706, Train L1 Norm: 0.1217, Test L1 Norm: 0.0464, Train Linf Norm: 7.5051, Test Linf Norm: 1.7247\n",
            "Epoch 3: Train Loss: 0.1035, Test Loss: 0.0898, Train L1 Norm: 0.1015, Test L1 Norm: 0.0605, Train Linf Norm: 7.1415, Test Linf Norm: 2.1996\n",
            "Epoch 4: Train Loss: 0.0847, Test Loss: 0.1047, Train L1 Norm: 0.1189, Test L1 Norm: 0.0414, Train Linf Norm: 10.4224, Test Linf Norm: 0.6073\n",
            "Epoch 5: Train Loss: 0.0733, Test Loss: 0.0828, Train L1 Norm: 0.0573, Test L1 Norm: 0.0348, Train Linf Norm: 3.2126, Test Linf Norm: 0.5417\n",
            "Epoch 6: Train Loss: 0.0653, Test Loss: 0.0627, Train L1 Norm: 0.0736, Test L1 Norm: 0.0328, Train Linf Norm: 5.6921, Test Linf Norm: 0.5947\n",
            "Epoch 7: Train Loss: 0.0598, Test Loss: 0.0448, Train L1 Norm: 0.0556, Test L1 Norm: 0.0257, Train Linf Norm: 3.7867, Test Linf Norm: 0.9318\n",
            "Epoch 8: Train Loss: 0.0553, Test Loss: 0.0604, Train L1 Norm: 0.0555, Test L1 Norm: 0.0267, Train Linf Norm: 3.9715, Test Linf Norm: 0.4855\n",
            "Epoch 9: Train Loss: 0.0512, Test Loss: 0.0622, Train L1 Norm: 0.0393, Test L1 Norm: 0.0257, Train Linf Norm: 2.1539, Test Linf Norm: 0.5502\n",
            "Epoch 10: Train Loss: 0.0483, Test Loss: 0.0547, Train L1 Norm: 0.0435, Test L1 Norm: 0.0251, Train Linf Norm: 2.8550, Test Linf Norm: 0.4713\n",
            "Epoch 11: Train Loss: 0.0453, Test Loss: 0.0336, Train L1 Norm: 0.0352, Test L1 Norm: 0.0255, Train Linf Norm: 1.9685, Test Linf Norm: 0.6298\n",
            "Epoch 12: Train Loss: 0.0437, Test Loss: 0.0435, Train L1 Norm: 0.0330, Test L1 Norm: 0.0248, Train Linf Norm: 1.7614, Test Linf Norm: 0.7686\n",
            "Epoch 13: Train Loss: 0.0414, Test Loss: 0.0292, Train L1 Norm: 0.0301, Test L1 Norm: 0.0162, Train Linf Norm: 1.5187, Test Linf Norm: 0.4736\n",
            "Epoch 14: Train Loss: 0.0396, Test Loss: 0.0411, Train L1 Norm: 0.0321, Test L1 Norm: 0.0210, Train Linf Norm: 1.8794, Test Linf Norm: 0.5717\n",
            "Epoch 15: Train Loss: 0.0383, Test Loss: 0.0394, Train L1 Norm: 0.0294, Test L1 Norm: 0.0172, Train Linf Norm: 1.6396, Test Linf Norm: 0.4250\n",
            "Epoch 16: Train Loss: 0.0130, Test Loss: 0.0121, Train L1 Norm: 0.0199, Test L1 Norm: 0.0096, Train Linf Norm: 1.6393, Test Linf Norm: 0.3987\n",
            "Epoch 17: Train Loss: 0.0117, Test Loss: 0.0112, Train L1 Norm: 0.0191, Test L1 Norm: 0.0092, Train Linf Norm: 1.6021, Test Linf Norm: 0.3718\n",
            "Epoch 18: Train Loss: 0.0109, Test Loss: 0.0107, Train L1 Norm: 0.0179, Test L1 Norm: 0.0091, Train Linf Norm: 1.4772, Test Linf Norm: 0.3861\n",
            "Epoch 19: Train Loss: 0.0104, Test Loss: 0.0102, Train L1 Norm: 0.0172, Test L1 Norm: 0.0088, Train Linf Norm: 1.4195, Test Linf Norm: 0.3691\n",
            "Epoch 20: Train Loss: 0.0100, Test Loss: 0.0097, Train L1 Norm: 0.0175, Test L1 Norm: 0.0086, Train Linf Norm: 1.4585, Test Linf Norm: 0.3639\n",
            "Epoch 21: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0173, Test L1 Norm: 0.0083, Train Linf Norm: 1.4580, Test Linf Norm: 0.3383\n",
            "Epoch 22: Train Loss: 0.0093, Test Loss: 0.0091, Train L1 Norm: 0.0168, Test L1 Norm: 0.0082, Train Linf Norm: 1.4263, Test Linf Norm: 0.3317\n",
            "Epoch 23: Train Loss: 0.0090, Test Loss: 0.0089, Train L1 Norm: 0.0171, Test L1 Norm: 0.0080, Train Linf Norm: 1.4641, Test Linf Norm: 0.3282\n",
            "Epoch 24: Train Loss: 0.0088, Test Loss: 0.0092, Train L1 Norm: 0.0165, Test L1 Norm: 0.0082, Train Linf Norm: 1.3960, Test Linf Norm: 0.3396\n",
            "Epoch 25: Train Loss: 0.0086, Test Loss: 0.0088, Train L1 Norm: 0.0162, Test L1 Norm: 0.0080, Train Linf Norm: 1.3607, Test Linf Norm: 0.3225\n",
            "Epoch 26: Train Loss: 0.0084, Test Loss: 0.0084, Train L1 Norm: 0.0166, Test L1 Norm: 0.0076, Train Linf Norm: 1.4351, Test Linf Norm: 0.3130\n",
            "Epoch 27: Train Loss: 0.0082, Test Loss: 0.0083, Train L1 Norm: 0.0166, Test L1 Norm: 0.0077, Train Linf Norm: 1.4451, Test Linf Norm: 0.3217\n",
            "Epoch 28: Train Loss: 0.0081, Test Loss: 0.0082, Train L1 Norm: 0.0156, Test L1 Norm: 0.0076, Train Linf Norm: 1.3360, Test Linf Norm: 0.3167\n",
            "Epoch 29: Train Loss: 0.0079, Test Loss: 0.0079, Train L1 Norm: 0.0156, Test L1 Norm: 0.0075, Train Linf Norm: 1.3214, Test Linf Norm: 0.3062\n",
            "Epoch 30: Train Loss: 0.0078, Test Loss: 0.0079, Train L1 Norm: 0.0156, Test L1 Norm: 0.0076, Train Linf Norm: 1.3400, Test Linf Norm: 0.3191\n",
            "Epoch 31: Train Loss: 0.0076, Test Loss: 0.0078, Train L1 Norm: 0.0153, Test L1 Norm: 0.0073, Train Linf Norm: 1.3154, Test Linf Norm: 0.2979\n",
            "Epoch 32: Train Loss: 0.0076, Test Loss: 0.0078, Train L1 Norm: 0.0155, Test L1 Norm: 0.0073, Train Linf Norm: 1.3373, Test Linf Norm: 0.2996\n",
            "Epoch 33: Train Loss: 0.0075, Test Loss: 0.0077, Train L1 Norm: 0.0154, Test L1 Norm: 0.0073, Train Linf Norm: 1.3013, Test Linf Norm: 0.2999\n",
            "Epoch 34: Train Loss: 0.0075, Test Loss: 0.0078, Train L1 Norm: 0.0155, Test L1 Norm: 0.0073, Train Linf Norm: 1.3316, Test Linf Norm: 0.3001\n",
            "Epoch 35: Train Loss: 0.0075, Test Loss: 0.0078, Train L1 Norm: 0.0155, Test L1 Norm: 0.0073, Train Linf Norm: 1.3405, Test Linf Norm: 0.3012\n",
            "Epoch 36: Train Loss: 0.0075, Test Loss: 0.0077, Train L1 Norm: 0.0152, Test L1 Norm: 0.0073, Train Linf Norm: 1.2980, Test Linf Norm: 0.3009\n",
            "Epoch 37: Train Loss: 0.0075, Test Loss: 0.0077, Train L1 Norm: 0.0153, Test L1 Norm: 0.0073, Train Linf Norm: 1.3207, Test Linf Norm: 0.3008\n",
            "Epoch 38: Train Loss: 0.0075, Test Loss: 0.0077, Train L1 Norm: 0.0153, Test L1 Norm: 0.0072, Train Linf Norm: 1.3099, Test Linf Norm: 0.2976\n",
            "Epoch 39: Train Loss: 0.0075, Test Loss: 0.0077, Train L1 Norm: 0.0154, Test L1 Norm: 0.0072, Train Linf Norm: 1.3321, Test Linf Norm: 0.2958\n",
            "Epoch 40: Train Loss: 0.0075, Test Loss: 0.0077, Train L1 Norm: 0.0153, Test L1 Norm: 0.0072, Train Linf Norm: 1.3155, Test Linf Norm: 0.2951\n",
            "Epoch 41: Train Loss: 0.0074, Test Loss: 0.0077, Train L1 Norm: 0.0154, Test L1 Norm: 0.0072, Train Linf Norm: 1.3261, Test Linf Norm: 0.2977\n",
            "Epoch 42: Train Loss: 0.0074, Test Loss: 0.0077, Train L1 Norm: 0.0153, Test L1 Norm: 0.0072, Train Linf Norm: 1.3121, Test Linf Norm: 0.2961\n",
            "Epoch 43: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3153, Test Linf Norm: 0.2959\n",
            "Epoch 44: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0153, Test L1 Norm: 0.0072, Train Linf Norm: 1.3157, Test Linf Norm: 0.2982\n",
            "Epoch 45: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0153, Test L1 Norm: 0.0072, Train Linf Norm: 1.3257, Test Linf Norm: 0.2971\n",
            "Epoch 46: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0151, Test L1 Norm: 0.0072, Train Linf Norm: 1.3045, Test Linf Norm: 0.2970\n",
            "Epoch 47: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3113, Test Linf Norm: 0.2968\n",
            "Epoch 48: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3078, Test Linf Norm: 0.2961\n",
            "Epoch 49: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3201, Test Linf Norm: 0.2959\n",
            "Epoch 50: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3097, Test Linf Norm: 0.2961\n",
            "Epoch 51: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3191, Test Linf Norm: 0.2961\n",
            "Epoch 52: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3096, Test Linf Norm: 0.2962\n",
            "Epoch 53: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3142, Test Linf Norm: 0.2957\n",
            "Epoch 54: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3099, Test Linf Norm: 0.2956\n",
            "Epoch 55: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3177, Test Linf Norm: 0.2958\n",
            "Epoch 56: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3210, Test Linf Norm: 0.2955\n",
            "Epoch 57: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3237, Test Linf Norm: 0.2955\n",
            "Epoch 58: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3221, Test Linf Norm: 0.2953\n",
            "Epoch 59: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3151, Test Linf Norm: 0.2953\n",
            "Epoch 60: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3174, Test Linf Norm: 0.2953\n",
            "Epoch 61: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.2818, Test Linf Norm: 0.2953\n",
            "Epoch 62: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3200, Test Linf Norm: 0.2953\n",
            "Epoch 63: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3132, Test Linf Norm: 0.2953\n",
            "Epoch 64: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3225, Test Linf Norm: 0.2953\n",
            "Epoch 65: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3171, Test Linf Norm: 0.2953\n",
            "Epoch 66: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3174, Test Linf Norm: 0.2953\n",
            "Epoch 67: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3211, Test Linf Norm: 0.2953\n",
            "Epoch 68: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3189, Test Linf Norm: 0.2953\n",
            "Epoch 69: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3109, Test Linf Norm: 0.2953\n",
            "Epoch 70: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3105, Test Linf Norm: 0.2953\n",
            "Epoch 71: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3232, Test Linf Norm: 0.2953\n",
            "Epoch 72: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3173, Test Linf Norm: 0.2953\n",
            "Epoch 73: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3170, Test Linf Norm: 0.2953\n",
            "Epoch 74: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3176, Test Linf Norm: 0.2953\n",
            "Epoch 75: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3224, Test Linf Norm: 0.2953\n",
            "Epoch 76: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3159, Test Linf Norm: 0.2953\n",
            "Epoch 77: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3154, Test Linf Norm: 0.2953\n",
            "Epoch 78: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3158, Test Linf Norm: 0.2953\n",
            "Epoch 79: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3136, Test Linf Norm: 0.2953\n",
            "Epoch 80: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3136, Test Linf Norm: 0.2953\n",
            "Epoch 81: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3097, Test Linf Norm: 0.2953\n",
            "Epoch 82: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3170, Test Linf Norm: 0.2953\n",
            "Epoch 83: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3198, Test Linf Norm: 0.2953\n",
            "Epoch 84: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3145, Test Linf Norm: 0.2953\n",
            "Epoch 85: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3219, Test Linf Norm: 0.2953\n",
            "Epoch 86: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3236, Test Linf Norm: 0.2953\n",
            "Epoch 87: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3155, Test Linf Norm: 0.2953\n",
            "Epoch 88: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3140, Test Linf Norm: 0.2953\n",
            "Epoch 89: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3062, Test Linf Norm: 0.2953\n",
            "Epoch 90: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3069, Test Linf Norm: 0.2953\n",
            "Epoch 91: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3099, Test Linf Norm: 0.2953\n",
            "Epoch 92: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3055, Test Linf Norm: 0.2953\n",
            "Epoch 93: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3057, Test Linf Norm: 0.2953\n",
            "Epoch 94: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3144, Test Linf Norm: 0.2953\n",
            "Epoch 95: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3165, Test Linf Norm: 0.2953\n",
            "Epoch 96: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3130, Test Linf Norm: 0.2953\n",
            "Epoch 97: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3165, Test Linf Norm: 0.2953\n",
            "Epoch 98: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3215, Test Linf Norm: 0.2953\n",
            "Epoch 99: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3207, Test Linf Norm: 0.2953\n",
            "Epoch 100: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3149, Test Linf Norm: 0.2953\n",
            "Epoch 101: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3213, Test Linf Norm: 0.2953\n",
            "Epoch 102: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3074, Test Linf Norm: 0.2953\n",
            "Epoch 103: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3184, Test Linf Norm: 0.2953\n",
            "Epoch 104: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3208, Test Linf Norm: 0.2953\n",
            "Epoch 105: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3082, Test Linf Norm: 0.2953\n",
            "Epoch 106: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3110, Test Linf Norm: 0.2953\n",
            "Epoch 107: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3163, Test Linf Norm: 0.2953\n",
            "Epoch 108: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3228, Test Linf Norm: 0.2953\n",
            "Epoch 109: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3022, Test Linf Norm: 0.2953\n",
            "Epoch 110: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3201, Test Linf Norm: 0.2953\n",
            "Epoch 111: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3137, Test Linf Norm: 0.2953\n",
            "Epoch 112: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3153, Test Linf Norm: 0.2953\n",
            "Epoch 113: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3098, Test Linf Norm: 0.2953\n",
            "Epoch 114: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3165, Test Linf Norm: 0.2953\n",
            "Epoch 115: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3181, Test Linf Norm: 0.2953\n",
            "Epoch 116: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3211, Test Linf Norm: 0.2953\n",
            "Epoch 117: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3122, Test Linf Norm: 0.2953\n",
            "Epoch 118: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3111, Test Linf Norm: 0.2953\n",
            "Epoch 119: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3099, Test Linf Norm: 0.2953\n",
            "Epoch 120: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3176, Test Linf Norm: 0.2953\n",
            "Epoch 121: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3137, Test Linf Norm: 0.2953\n",
            "Epoch 122: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3183, Test Linf Norm: 0.2953\n",
            "Epoch 123: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3135, Test Linf Norm: 0.2953\n",
            "Epoch 124: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3157, Test Linf Norm: 0.2953\n",
            "Epoch 125: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3161, Test Linf Norm: 0.2953\n",
            "Epoch 126: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3181, Test Linf Norm: 0.2953\n",
            "Epoch 127: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3139, Test Linf Norm: 0.2953\n",
            "Epoch 128: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3124, Test Linf Norm: 0.2953\n",
            "Epoch 129: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3150, Test Linf Norm: 0.2953\n",
            "Epoch 130: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3210, Test Linf Norm: 0.2953\n",
            "Epoch 131: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3081, Test Linf Norm: 0.2953\n",
            "Epoch 132: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3148, Test Linf Norm: 0.2953\n",
            "Epoch 133: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3209, Test Linf Norm: 0.2953\n",
            "Epoch 134: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3063, Test Linf Norm: 0.2953\n",
            "Epoch 135: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3093, Test Linf Norm: 0.2953\n",
            "Epoch 136: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3193, Test Linf Norm: 0.2953\n",
            "Epoch 137: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3171, Test Linf Norm: 0.2953\n",
            "Epoch 138: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3184, Test Linf Norm: 0.2953\n",
            "Epoch 139: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3128, Test Linf Norm: 0.2953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 11:04:50,831]\u001b[0m Trial 11 finished with value: 0.0071948581174016 and parameters: {'n_layers': 6, 'n_units_0': 1992, 'n_units_1': 2040, 'n_units_2': 2042, 'n_units_3': 2029, 'n_units_4': 2047, 'n_units_5': 125, 'hidden_activation': 'LeakyReLU', 'output_activation': 'ReLU', 'loss': 'MAE', 'optimizer': 'Adagrad', 'lr': 0.00032125573031380997, 'batch_size': 128, 'n_epochs': 140, 'scheduler': 'StepLR', 'step_size': 15, 'gamma': 0.10437837591079809}. Best is trial 11 with value: 0.0071948581174016.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 140: Train Loss: 0.0074, Test Loss: 0.0076, Train L1 Norm: 0.0152, Test L1 Norm: 0.0072, Train Linf Norm: 1.3142, Test Linf Norm: 0.2953\n",
            "Epoch 1: Train Loss: 0.3610, Test Loss: 0.1684, Train L1 Norm: 0.2644, Test L1 Norm: 0.0923, Train Linf Norm: 13.2622, Test Linf Norm: 2.8625\n",
            "Epoch 2: Train Loss: 0.1261, Test Loss: 0.0658, Train L1 Norm: 0.1094, Test L1 Norm: 0.0399, Train Linf Norm: 6.7323, Test Linf Norm: 1.4362\n",
            "Epoch 3: Train Loss: 0.0925, Test Loss: 0.1111, Train L1 Norm: 0.0864, Test L1 Norm: 0.0659, Train Linf Norm: 5.6650, Test Linf Norm: 2.1000\n",
            "Epoch 4: Train Loss: 0.0746, Test Loss: 0.0855, Train L1 Norm: 0.0601, Test L1 Norm: 0.0589, Train Linf Norm: 3.2431, Test Linf Norm: 2.3152\n",
            "Epoch 5: Train Loss: 0.0652, Test Loss: 0.0516, Train L1 Norm: 0.0612, Test L1 Norm: 0.0269, Train Linf Norm: 4.0097, Test Linf Norm: 0.5396\n",
            "Epoch 6: Train Loss: 0.0588, Test Loss: 0.0415, Train L1 Norm: 0.0605, Test L1 Norm: 0.0293, Train Linf Norm: 4.2847, Test Linf Norm: 1.1892\n",
            "Epoch 7: Train Loss: 0.0536, Test Loss: 0.0427, Train L1 Norm: 0.0472, Test L1 Norm: 0.0285, Train Linf Norm: 2.8980, Test Linf Norm: 1.1199\n",
            "Epoch 8: Train Loss: 0.0492, Test Loss: 0.0493, Train L1 Norm: 0.0475, Test L1 Norm: 0.0246, Train Linf Norm: 3.1794, Test Linf Norm: 0.5246\n",
            "Epoch 9: Train Loss: 0.0460, Test Loss: 0.0425, Train L1 Norm: 0.0433, Test L1 Norm: 0.0276, Train Linf Norm: 2.8497, Test Linf Norm: 1.1176\n",
            "Epoch 10: Train Loss: 0.0435, Test Loss: 0.0502, Train L1 Norm: 0.0434, Test L1 Norm: 0.0238, Train Linf Norm: 3.0040, Test Linf Norm: 0.4867\n",
            "Epoch 11: Train Loss: 0.0415, Test Loss: 0.0453, Train L1 Norm: 0.0368, Test L1 Norm: 0.0295, Train Linf Norm: 2.2809, Test Linf Norm: 1.0411\n",
            "Epoch 12: Train Loss: 0.0395, Test Loss: 0.0453, Train L1 Norm: 0.0411, Test L1 Norm: 0.0244, Train Linf Norm: 2.9095, Test Linf Norm: 0.5139\n",
            "Epoch 13: Train Loss: 0.0368, Test Loss: 0.0428, Train L1 Norm: 0.0388, Test L1 Norm: 0.0205, Train Linf Norm: 2.7829, Test Linf Norm: 0.5356\n",
            "Epoch 14: Train Loss: 0.0361, Test Loss: 0.0278, Train L1 Norm: 0.0361, Test L1 Norm: 0.0212, Train Linf Norm: 2.4921, Test Linf Norm: 0.9130\n",
            "Epoch 15: Train Loss: 0.0346, Test Loss: 0.0311, Train L1 Norm: 0.0344, Test L1 Norm: 0.0182, Train Linf Norm: 2.3550, Test Linf Norm: 0.4650\n",
            "Epoch 16: Train Loss: 0.0136, Test Loss: 0.0133, Train L1 Norm: 0.0240, Test L1 Norm: 0.0111, Train Linf Norm: 2.0394, Test Linf Norm: 0.4602\n",
            "Epoch 17: Train Loss: 0.0128, Test Loss: 0.0128, Train L1 Norm: 0.0230, Test L1 Norm: 0.0104, Train Linf Norm: 1.9540, Test Linf Norm: 0.3999\n",
            "Epoch 18: Train Loss: 0.0123, Test Loss: 0.0123, Train L1 Norm: 0.0227, Test L1 Norm: 0.0102, Train Linf Norm: 1.9604, Test Linf Norm: 0.3915\n",
            "Epoch 19: Train Loss: 0.0119, Test Loss: 0.0120, Train L1 Norm: 0.0223, Test L1 Norm: 0.0101, Train Linf Norm: 1.8974, Test Linf Norm: 0.4013\n",
            "Epoch 20: Train Loss: 0.0116, Test Loss: 0.0116, Train L1 Norm: 0.0213, Test L1 Norm: 0.0097, Train Linf Norm: 1.7974, Test Linf Norm: 0.3570\n",
            "Epoch 21: Train Loss: 0.0113, Test Loss: 0.0114, Train L1 Norm: 0.0218, Test L1 Norm: 0.0096, Train Linf Norm: 1.8548, Test Linf Norm: 0.3650\n",
            "Epoch 22: Train Loss: 0.0110, Test Loss: 0.0113, Train L1 Norm: 0.0215, Test L1 Norm: 0.0095, Train Linf Norm: 1.8359, Test Linf Norm: 0.3551\n",
            "Epoch 23: Train Loss: 0.0108, Test Loss: 0.0110, Train L1 Norm: 0.0213, Test L1 Norm: 0.0095, Train Linf Norm: 1.8336, Test Linf Norm: 0.3470\n",
            "Epoch 24: Train Loss: 0.0106, Test Loss: 0.0110, Train L1 Norm: 0.0212, Test L1 Norm: 0.0092, Train Linf Norm: 1.8404, Test Linf Norm: 0.3392\n",
            "Epoch 25: Train Loss: 0.0104, Test Loss: 0.0107, Train L1 Norm: 0.0204, Test L1 Norm: 0.0091, Train Linf Norm: 1.7498, Test Linf Norm: 0.3397\n",
            "Epoch 26: Train Loss: 0.0102, Test Loss: 0.0106, Train L1 Norm: 0.0209, Test L1 Norm: 0.0092, Train Linf Norm: 1.8060, Test Linf Norm: 0.3387\n",
            "Epoch 27: Train Loss: 0.0100, Test Loss: 0.0104, Train L1 Norm: 0.0207, Test L1 Norm: 0.0090, Train Linf Norm: 1.8010, Test Linf Norm: 0.3367\n",
            "Epoch 28: Train Loss: 0.0099, Test Loss: 0.0101, Train L1 Norm: 0.0206, Test L1 Norm: 0.0090, Train Linf Norm: 1.8158, Test Linf Norm: 0.3376\n",
            "Epoch 29: Train Loss: 0.0098, Test Loss: 0.0100, Train L1 Norm: 0.0201, Test L1 Norm: 0.0089, Train Linf Norm: 1.7507, Test Linf Norm: 0.3434\n",
            "Epoch 30: Train Loss: 0.0096, Test Loss: 0.0098, Train L1 Norm: 0.0203, Test L1 Norm: 0.0087, Train Linf Norm: 1.7773, Test Linf Norm: 0.3135\n",
            "Epoch 31: Train Loss: 0.0094, Test Loss: 0.0098, Train L1 Norm: 0.0202, Test L1 Norm: 0.0088, Train Linf Norm: 1.7681, Test Linf Norm: 0.3336\n",
            "Epoch 32: Train Loss: 0.0094, Test Loss: 0.0098, Train L1 Norm: 0.0202, Test L1 Norm: 0.0088, Train Linf Norm: 1.7729, Test Linf Norm: 0.3330\n",
            "Epoch 33: Train Loss: 0.0094, Test Loss: 0.0098, Train L1 Norm: 0.0201, Test L1 Norm: 0.0088, Train Linf Norm: 1.7613, Test Linf Norm: 0.3296\n",
            "Epoch 34: Train Loss: 0.0094, Test Loss: 0.0098, Train L1 Norm: 0.0200, Test L1 Norm: 0.0088, Train Linf Norm: 1.7605, Test Linf Norm: 0.3323\n",
            "Epoch 35: Train Loss: 0.0094, Test Loss: 0.0097, Train L1 Norm: 0.0200, Test L1 Norm: 0.0087, Train Linf Norm: 1.7433, Test Linf Norm: 0.3280\n",
            "Epoch 36: Train Loss: 0.0094, Test Loss: 0.0097, Train L1 Norm: 0.0200, Test L1 Norm: 0.0088, Train Linf Norm: 1.7590, Test Linf Norm: 0.3325\n",
            "Epoch 37: Train Loss: 0.0093, Test Loss: 0.0097, Train L1 Norm: 0.0200, Test L1 Norm: 0.0088, Train Linf Norm: 1.7565, Test Linf Norm: 0.3307\n",
            "Epoch 38: Train Loss: 0.0093, Test Loss: 0.0098, Train L1 Norm: 0.0200, Test L1 Norm: 0.0088, Train Linf Norm: 1.7633, Test Linf Norm: 0.3395\n",
            "Epoch 39: Train Loss: 0.0093, Test Loss: 0.0097, Train L1 Norm: 0.0200, Test L1 Norm: 0.0087, Train Linf Norm: 1.7594, Test Linf Norm: 0.3305\n",
            "Epoch 40: Train Loss: 0.0093, Test Loss: 0.0097, Train L1 Norm: 0.0199, Test L1 Norm: 0.0087, Train Linf Norm: 1.7314, Test Linf Norm: 0.3328\n",
            "Epoch 41: Train Loss: 0.0093, Test Loss: 0.0097, Train L1 Norm: 0.0199, Test L1 Norm: 0.0088, Train Linf Norm: 1.7394, Test Linf Norm: 0.3352\n",
            "Epoch 42: Train Loss: 0.0093, Test Loss: 0.0097, Train L1 Norm: 0.0199, Test L1 Norm: 0.0088, Train Linf Norm: 1.7428, Test Linf Norm: 0.3353\n",
            "Epoch 43: Train Loss: 0.0093, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0088, Train Linf Norm: 1.7368, Test Linf Norm: 0.3368\n",
            "Epoch 44: Train Loss: 0.0093, Test Loss: 0.0096, Train L1 Norm: 0.0199, Test L1 Norm: 0.0088, Train Linf Norm: 1.7448, Test Linf Norm: 0.3368\n",
            "Epoch 45: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0197, Test L1 Norm: 0.0087, Train Linf Norm: 1.7204, Test Linf Norm: 0.3286\n",
            "Epoch 46: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7236, Test Linf Norm: 0.3283\n",
            "Epoch 47: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7362, Test Linf Norm: 0.3287\n",
            "Epoch 48: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7450, Test Linf Norm: 0.3291\n",
            "Epoch 49: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7341, Test Linf Norm: 0.3292\n",
            "Epoch 50: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0199, Test L1 Norm: 0.0087, Train Linf Norm: 1.7441, Test Linf Norm: 0.3298\n",
            "Epoch 51: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7331, Test Linf Norm: 0.3299\n",
            "Epoch 52: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7328, Test Linf Norm: 0.3301\n",
            "Epoch 53: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7390, Test Linf Norm: 0.3301\n",
            "Epoch 54: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7413, Test Linf Norm: 0.3301\n",
            "Epoch 55: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7323, Test Linf Norm: 0.3303\n",
            "Epoch 56: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7466, Test Linf Norm: 0.3303\n",
            "Epoch 57: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7388, Test Linf Norm: 0.3305\n",
            "Epoch 58: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7382, Test Linf Norm: 0.3306\n",
            "Epoch 59: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7276, Test Linf Norm: 0.3307\n",
            "Epoch 60: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7267, Test Linf Norm: 0.3311\n",
            "Epoch 61: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7307, Test Linf Norm: 0.3311\n",
            "Epoch 62: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7468, Test Linf Norm: 0.3311\n",
            "Epoch 63: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7429, Test Linf Norm: 0.3311\n",
            "Epoch 64: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7345, Test Linf Norm: 0.3311\n",
            "Epoch 65: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7483, Test Linf Norm: 0.3311\n",
            "Epoch 66: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7377, Test Linf Norm: 0.3311\n",
            "Epoch 67: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7377, Test Linf Norm: 0.3311\n",
            "Epoch 68: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7352, Test Linf Norm: 0.3311\n",
            "Epoch 69: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7468, Test Linf Norm: 0.3311\n",
            "Epoch 70: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7385, Test Linf Norm: 0.3311\n",
            "Epoch 71: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7350, Test Linf Norm: 0.3312\n",
            "Epoch 72: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7450, Test Linf Norm: 0.3312\n",
            "Epoch 73: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7372, Test Linf Norm: 0.3312\n",
            "Epoch 74: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7340, Test Linf Norm: 0.3312\n",
            "Epoch 75: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7404, Test Linf Norm: 0.3312\n",
            "Epoch 76: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7266, Test Linf Norm: 0.3312\n",
            "Epoch 77: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7340, Test Linf Norm: 0.3312\n",
            "Epoch 78: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7258, Test Linf Norm: 0.3312\n",
            "Epoch 79: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7290, Test Linf Norm: 0.3312\n",
            "Epoch 80: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7419, Test Linf Norm: 0.3312\n",
            "Epoch 81: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7334, Test Linf Norm: 0.3312\n",
            "Epoch 82: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7502, Test Linf Norm: 0.3312\n",
            "Epoch 83: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7229, Test Linf Norm: 0.3312\n",
            "Epoch 84: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7320, Test Linf Norm: 0.3312\n",
            "Epoch 85: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7381, Test Linf Norm: 0.3312\n",
            "Epoch 86: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7359, Test Linf Norm: 0.3312\n",
            "Epoch 87: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7288, Test Linf Norm: 0.3312\n",
            "Epoch 88: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7401, Test Linf Norm: 0.3312\n",
            "Epoch 89: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7376, Test Linf Norm: 0.3312\n",
            "Epoch 90: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7288, Test Linf Norm: 0.3312\n",
            "Epoch 91: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7363, Test Linf Norm: 0.3312\n",
            "Epoch 92: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7317, Test Linf Norm: 0.3312\n",
            "Epoch 93: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7278, Test Linf Norm: 0.3312\n",
            "Epoch 94: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7326, Test Linf Norm: 0.3312\n",
            "Epoch 95: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7295, Test Linf Norm: 0.3312\n",
            "Epoch 96: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7317, Test Linf Norm: 0.3312\n",
            "Epoch 97: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7322, Test Linf Norm: 0.3312\n",
            "Epoch 98: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7384, Test Linf Norm: 0.3312\n",
            "Epoch 99: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7343, Test Linf Norm: 0.3312\n",
            "Epoch 100: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7391, Test Linf Norm: 0.3312\n",
            "Epoch 101: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7288, Test Linf Norm: 0.3312\n",
            "Epoch 102: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7326, Test Linf Norm: 0.3312\n",
            "Epoch 103: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7410, Test Linf Norm: 0.3312\n",
            "Epoch 104: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7164, Test Linf Norm: 0.3312\n",
            "Epoch 105: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7366, Test Linf Norm: 0.3312\n",
            "Epoch 106: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7318, Test Linf Norm: 0.3312\n",
            "Epoch 107: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7282, Test Linf Norm: 0.3312\n",
            "Epoch 108: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7321, Test Linf Norm: 0.3312\n",
            "Epoch 109: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7303, Test Linf Norm: 0.3312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 11:11:56,444]\u001b[0m Trial 12 finished with value: 0.00870779783502221 and parameters: {'n_layers': 6, 'n_units_0': 1936, 'n_units_1': 1502, 'n_units_2': 824, 'n_units_3': 1432, 'n_units_4': 1988, 'n_units_5': 128, 'hidden_activation': 'LeakyReLU', 'output_activation': 'ReLU', 'loss': 'MAE', 'optimizer': 'Adagrad', 'lr': 0.000301133676971451, 'batch_size': 128, 'n_epochs': 110, 'scheduler': 'StepLR', 'step_size': 15, 'gamma': 0.10762289665402752}. Best is trial 11 with value: 0.0071948581174016.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 110: Train Loss: 0.0092, Test Loss: 0.0096, Train L1 Norm: 0.0198, Test L1 Norm: 0.0087, Train Linf Norm: 1.7362, Test Linf Norm: 0.3312\n",
            "Epoch 1: Train Loss: 0.3128, Test Loss: 0.1368, Train L1 Norm: 0.2607, Test L1 Norm: 0.0634, Train Linf Norm: 14.2745, Test Linf Norm: 1.0677\n",
            "Epoch 2: Train Loss: 0.0951, Test Loss: 0.0612, Train L1 Norm: 0.0878, Test L1 Norm: 0.0464, Train Linf Norm: 5.2356, Test Linf Norm: 1.8048\n",
            "Epoch 3: Train Loss: 0.0679, Test Loss: 0.0651, Train L1 Norm: 0.0658, Test L1 Norm: 0.0423, Train Linf Norm: 3.9767, Test Linf Norm: 1.4081\n",
            "Epoch 4: Train Loss: 0.0559, Test Loss: 0.0573, Train L1 Norm: 0.0803, Test L1 Norm: 0.0332, Train Linf Norm: 6.6312, Test Linf Norm: 0.7136\n",
            "Epoch 5: Train Loss: 0.0482, Test Loss: 0.0435, Train L1 Norm: 0.0845, Test L1 Norm: 0.0318, Train Linf Norm: 7.5609, Test Linf Norm: 1.1066\n",
            "Epoch 6: Train Loss: 0.0427, Test Loss: 0.0400, Train L1 Norm: 0.0702, Test L1 Norm: 0.0245, Train Linf Norm: 6.1050, Test Linf Norm: 0.6618\n",
            "Epoch 7: Train Loss: 0.0394, Test Loss: 0.0322, Train L1 Norm: 0.0447, Test L1 Norm: 0.0229, Train Linf Norm: 2.9892, Test Linf Norm: 0.7529\n",
            "Epoch 8: Train Loss: 0.0357, Test Loss: 0.0309, Train L1 Norm: 0.0427, Test L1 Norm: 0.0235, Train Linf Norm: 2.9586, Test Linf Norm: 0.8215\n",
            "Epoch 9: Train Loss: 0.0336, Test Loss: 0.0187, Train L1 Norm: 0.0440, Test L1 Norm: 0.0175, Train Linf Norm: 3.2332, Test Linf Norm: 0.5950\n",
            "Epoch 10: Train Loss: 0.0318, Test Loss: 0.0308, Train L1 Norm: 0.0431, Test L1 Norm: 0.0240, Train Linf Norm: 3.2246, Test Linf Norm: 0.8346\n",
            "Epoch 11: Train Loss: 0.0305, Test Loss: 0.0214, Train L1 Norm: 0.0423, Test L1 Norm: 0.0181, Train Linf Norm: 3.1937, Test Linf Norm: 0.6482\n",
            "Epoch 12: Train Loss: 0.0286, Test Loss: 0.0180, Train L1 Norm: 0.0397, Test L1 Norm: 0.0160, Train Linf Norm: 2.9966, Test Linf Norm: 0.5641\n",
            "Epoch 13: Train Loss: 0.0270, Test Loss: 0.0224, Train L1 Norm: 0.0379, Test L1 Norm: 0.0233, Train Linf Norm: 2.8879, Test Linf Norm: 1.0147\n",
            "Epoch 14: Train Loss: 0.0257, Test Loss: 0.0146, Train L1 Norm: 0.0383, Test L1 Norm: 0.0148, Train Linf Norm: 2.9711, Test Linf Norm: 0.5705\n",
            "Epoch 15: Train Loss: 0.0253, Test Loss: 0.0252, Train L1 Norm: 0.0350, Test L1 Norm: 0.0179, Train Linf Norm: 2.6005, Test Linf Norm: 0.5331\n",
            "Epoch 16: Train Loss: 0.0127, Test Loss: 0.0126, Train L1 Norm: 0.0310, Test L1 Norm: 0.0136, Train Linf Norm: 2.6343, Test Linf Norm: 0.5145\n",
            "Epoch 17: Train Loss: 0.0124, Test Loss: 0.0124, Train L1 Norm: 0.0308, Test L1 Norm: 0.0136, Train Linf Norm: 2.6119, Test Linf Norm: 0.5131\n",
            "Epoch 18: Train Loss: 0.0122, Test Loss: 0.0122, Train L1 Norm: 0.0313, Test L1 Norm: 0.0133, Train Linf Norm: 2.7081, Test Linf Norm: 0.4968\n",
            "Epoch 19: Train Loss: 0.0121, Test Loss: 0.0122, Train L1 Norm: 0.0308, Test L1 Norm: 0.0134, Train Linf Norm: 2.6705, Test Linf Norm: 0.5046\n",
            "Epoch 20: Train Loss: 0.0119, Test Loss: 0.0119, Train L1 Norm: 0.0309, Test L1 Norm: 0.0132, Train Linf Norm: 2.6925, Test Linf Norm: 0.4959\n",
            "Epoch 21: Train Loss: 0.0118, Test Loss: 0.0119, Train L1 Norm: 0.0308, Test L1 Norm: 0.0132, Train Linf Norm: 2.6773, Test Linf Norm: 0.4963\n",
            "Epoch 22: Train Loss: 0.0117, Test Loss: 0.0117, Train L1 Norm: 0.0309, Test L1 Norm: 0.0132, Train Linf Norm: 2.7081, Test Linf Norm: 0.5018\n",
            "Epoch 23: Train Loss: 0.0116, Test Loss: 0.0116, Train L1 Norm: 0.0308, Test L1 Norm: 0.0131, Train Linf Norm: 2.7058, Test Linf Norm: 0.5002\n",
            "Epoch 24: Train Loss: 0.0115, Test Loss: 0.0115, Train L1 Norm: 0.0303, Test L1 Norm: 0.0130, Train Linf Norm: 2.6423, Test Linf Norm: 0.4877\n",
            "Epoch 25: Train Loss: 0.0114, Test Loss: 0.0114, Train L1 Norm: 0.0304, Test L1 Norm: 0.0131, Train Linf Norm: 2.6677, Test Linf Norm: 0.4977\n",
            "Epoch 26: Train Loss: 0.0113, Test Loss: 0.0113, Train L1 Norm: 0.0301, Test L1 Norm: 0.0129, Train Linf Norm: 2.6355, Test Linf Norm: 0.4876\n",
            "Epoch 27: Train Loss: 0.0112, Test Loss: 0.0114, Train L1 Norm: 0.0301, Test L1 Norm: 0.0129, Train Linf Norm: 2.6422, Test Linf Norm: 0.4875\n",
            "Epoch 28: Train Loss: 0.0111, Test Loss: 0.0111, Train L1 Norm: 0.0298, Test L1 Norm: 0.0128, Train Linf Norm: 2.6037, Test Linf Norm: 0.4874\n",
            "Epoch 29: Train Loss: 0.0110, Test Loss: 0.0111, Train L1 Norm: 0.0298, Test L1 Norm: 0.0127, Train Linf Norm: 2.6134, Test Linf Norm: 0.4811\n",
            "Epoch 30: Train Loss: 0.0109, Test Loss: 0.0110, Train L1 Norm: 0.0301, Test L1 Norm: 0.0130, Train Linf Norm: 2.6489, Test Linf Norm: 0.5080\n",
            "Epoch 31: Train Loss: 0.0109, Test Loss: 0.0109, Train L1 Norm: 0.0308, Test L1 Norm: 0.0127, Train Linf Norm: 2.7661, Test Linf Norm: 0.4786\n",
            "Epoch 32: Train Loss: 0.0108, Test Loss: 0.0109, Train L1 Norm: 0.0300, Test L1 Norm: 0.0127, Train Linf Norm: 2.6589, Test Linf Norm: 0.4787\n",
            "Epoch 33: Train Loss: 0.0108, Test Loss: 0.0109, Train L1 Norm: 0.0298, Test L1 Norm: 0.0127, Train Linf Norm: 2.6164, Test Linf Norm: 0.4775\n",
            "Epoch 34: Train Loss: 0.0108, Test Loss: 0.0109, Train L1 Norm: 0.0298, Test L1 Norm: 0.0127, Train Linf Norm: 2.6293, Test Linf Norm: 0.4774\n",
            "Epoch 35: Train Loss: 0.0108, Test Loss: 0.0109, Train L1 Norm: 0.0297, Test L1 Norm: 0.0127, Train Linf Norm: 2.6161, Test Linf Norm: 0.4775\n",
            "Epoch 36: Train Loss: 0.0108, Test Loss: 0.0109, Train L1 Norm: 0.0297, Test L1 Norm: 0.0126, Train Linf Norm: 2.5902, Test Linf Norm: 0.4773\n",
            "Epoch 37: Train Loss: 0.0108, Test Loss: 0.0109, Train L1 Norm: 0.0297, Test L1 Norm: 0.0126, Train Linf Norm: 2.6233, Test Linf Norm: 0.4777\n",
            "Epoch 38: Train Loss: 0.0108, Test Loss: 0.0109, Train L1 Norm: 0.0297, Test L1 Norm: 0.0126, Train Linf Norm: 2.6015, Test Linf Norm: 0.4773\n",
            "Epoch 39: Train Loss: 0.0108, Test Loss: 0.0109, Train L1 Norm: 0.0296, Test L1 Norm: 0.0126, Train Linf Norm: 2.6084, Test Linf Norm: 0.4757\n",
            "Epoch 40: Train Loss: 0.0108, Test Loss: 0.0109, Train L1 Norm: 0.0297, Test L1 Norm: 0.0126, Train Linf Norm: 2.6195, Test Linf Norm: 0.4743\n",
            "Epoch 41: Train Loss: 0.0108, Test Loss: 0.0109, Train L1 Norm: 0.0297, Test L1 Norm: 0.0126, Train Linf Norm: 2.6019, Test Linf Norm: 0.4742\n",
            "Epoch 42: Train Loss: 0.0108, Test Loss: 0.0109, Train L1 Norm: 0.0296, Test L1 Norm: 0.0126, Train Linf Norm: 2.6078, Test Linf Norm: 0.4752\n",
            "Epoch 43: Train Loss: 0.0108, Test Loss: 0.0109, Train L1 Norm: 0.0296, Test L1 Norm: 0.0126, Train Linf Norm: 2.6157, Test Linf Norm: 0.4749\n",
            "Epoch 44: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0296, Test L1 Norm: 0.0126, Train Linf Norm: 2.6037, Test Linf Norm: 0.4735\n",
            "Epoch 45: Train Loss: 0.0107, Test Loss: 0.0109, Train L1 Norm: 0.0296, Test L1 Norm: 0.0126, Train Linf Norm: 2.6056, Test Linf Norm: 0.4734\n",
            "Epoch 46: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5978, Test Linf Norm: 0.4740\n",
            "Epoch 47: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0296, Test L1 Norm: 0.0126, Train Linf Norm: 2.6032, Test Linf Norm: 0.4738\n",
            "Epoch 48: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0296, Test L1 Norm: 0.0126, Train Linf Norm: 2.6020, Test Linf Norm: 0.4741\n",
            "Epoch 49: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0296, Test L1 Norm: 0.0126, Train Linf Norm: 2.6065, Test Linf Norm: 0.4739\n",
            "Epoch 50: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0296, Test L1 Norm: 0.0126, Train Linf Norm: 2.6006, Test Linf Norm: 0.4739\n",
            "Epoch 51: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0296, Test L1 Norm: 0.0126, Train Linf Norm: 2.5595, Test Linf Norm: 0.4741\n",
            "Epoch 52: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0296, Test L1 Norm: 0.0126, Train Linf Norm: 2.6074, Test Linf Norm: 0.4739\n",
            "Epoch 53: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0296, Test L1 Norm: 0.0126, Train Linf Norm: 2.6042, Test Linf Norm: 0.4738\n",
            "Epoch 54: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0296, Test L1 Norm: 0.0126, Train Linf Norm: 2.6029, Test Linf Norm: 0.4739\n",
            "Epoch 55: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0296, Test L1 Norm: 0.0126, Train Linf Norm: 2.5925, Test Linf Norm: 0.4740\n",
            "Epoch 56: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0296, Test L1 Norm: 0.0126, Train Linf Norm: 2.5952, Test Linf Norm: 0.4737\n",
            "Epoch 57: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0296, Test L1 Norm: 0.0126, Train Linf Norm: 2.5859, Test Linf Norm: 0.4738\n",
            "Epoch 58: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5968, Test Linf Norm: 0.4738\n",
            "Epoch 59: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0296, Test L1 Norm: 0.0126, Train Linf Norm: 2.6024, Test Linf Norm: 0.4736\n",
            "Epoch 60: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.6007, Test Linf Norm: 0.4737\n",
            "Epoch 61: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5718, Test Linf Norm: 0.4737\n",
            "Epoch 62: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.6084, Test Linf Norm: 0.4737\n",
            "Epoch 63: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5832, Test Linf Norm: 0.4737\n",
            "Epoch 64: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.6041, Test Linf Norm: 0.4737\n",
            "Epoch 65: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5821, Test Linf Norm: 0.4737\n",
            "Epoch 66: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.6025, Test Linf Norm: 0.4737\n",
            "Epoch 67: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5942, Test Linf Norm: 0.4737\n",
            "Epoch 68: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5948, Test Linf Norm: 0.4736\n",
            "Epoch 69: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.6031, Test Linf Norm: 0.4736\n",
            "Epoch 70: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.6036, Test Linf Norm: 0.4736\n",
            "Epoch 71: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.6118, Test Linf Norm: 0.4736\n",
            "Epoch 72: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5916, Test Linf Norm: 0.4736\n",
            "Epoch 73: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5734, Test Linf Norm: 0.4736\n",
            "Epoch 74: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.6043, Test Linf Norm: 0.4736\n",
            "Epoch 75: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5983, Test Linf Norm: 0.4736\n",
            "Epoch 76: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5929, Test Linf Norm: 0.4736\n",
            "Epoch 77: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.6038, Test Linf Norm: 0.4736\n",
            "Epoch 78: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5904, Test Linf Norm: 0.4736\n",
            "Epoch 79: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.6016, Test Linf Norm: 0.4736\n",
            "Epoch 80: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5853, Test Linf Norm: 0.4736\n",
            "Epoch 81: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.6206, Test Linf Norm: 0.4736\n",
            "Epoch 82: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5942, Test Linf Norm: 0.4736\n",
            "Epoch 83: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5891, Test Linf Norm: 0.4736\n",
            "Epoch 84: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.6018, Test Linf Norm: 0.4736\n",
            "Epoch 85: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.6056, Test Linf Norm: 0.4736\n",
            "Epoch 86: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.6023, Test Linf Norm: 0.4736\n",
            "Epoch 87: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5891, Test Linf Norm: 0.4736\n",
            "Epoch 88: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5955, Test Linf Norm: 0.4736\n",
            "Epoch 89: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5828, Test Linf Norm: 0.4736\n",
            "Epoch 90: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.6038, Test Linf Norm: 0.4736\n",
            "Epoch 91: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5747, Test Linf Norm: 0.4736\n",
            "Epoch 92: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5909, Test Linf Norm: 0.4736\n",
            "Epoch 93: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.6028, Test Linf Norm: 0.4736\n",
            "Epoch 94: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5976, Test Linf Norm: 0.4736\n",
            "Epoch 95: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5928, Test Linf Norm: 0.4736\n",
            "Epoch 96: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5994, Test Linf Norm: 0.4736\n",
            "Epoch 97: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5933, Test Linf Norm: 0.4736\n",
            "Epoch 98: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5567, Test Linf Norm: 0.4736\n",
            "Epoch 99: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5984, Test Linf Norm: 0.4736\n",
            "Epoch 100: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.6056, Test Linf Norm: 0.4736\n",
            "Epoch 101: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5806, Test Linf Norm: 0.4736\n",
            "Epoch 102: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5975, Test Linf Norm: 0.4736\n",
            "Epoch 103: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.6126, Test Linf Norm: 0.4736\n",
            "Epoch 104: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5966, Test Linf Norm: 0.4736\n",
            "Epoch 105: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5791, Test Linf Norm: 0.4736\n",
            "Epoch 106: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5918, Test Linf Norm: 0.4736\n",
            "Epoch 107: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.5685, Test Linf Norm: 0.4736\n",
            "Epoch 108: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.6051, Test Linf Norm: 0.4736\n",
            "Epoch 109: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.6124, Test Linf Norm: 0.4736\n",
            "Epoch 110: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.6017, Test Linf Norm: 0.4736\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 11:18:28,751]\u001b[0m Trial 13 finished with value: 0.012574341285228729 and parameters: {'n_layers': 6, 'n_units_0': 1944, 'n_units_1': 1471, 'n_units_2': 100, 'n_units_3': 1415, 'n_units_4': 2040, 'n_units_5': 133, 'hidden_activation': 'LeakyReLU', 'output_activation': 'ReLU', 'loss': 'MAE', 'optimizer': 'Adagrad', 'lr': 0.0001372855975549327, 'batch_size': 128, 'n_epochs': 111, 'scheduler': 'StepLR', 'step_size': 15, 'gamma': 0.11511660244492616}. Best is trial 11 with value: 0.0071948581174016.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 111: Train Loss: 0.0107, Test Loss: 0.0108, Train L1 Norm: 0.0295, Test L1 Norm: 0.0126, Train Linf Norm: 2.6068, Test Linf Norm: 0.4736\n",
            "Epoch 1: Train Loss: 0.3205, Test Loss: 0.1388, Train L1 Norm: 0.4714, Test L1 Norm: 0.0660, Train Linf Norm: 40.0678, Test Linf Norm: 1.0579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 11:18:37,250]\u001b[0m Trial 14 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 0.1136, Test Loss: 0.1063, Train L1 Norm: 0.1074, Test L1 Norm: 0.0781, Train Linf Norm: 6.7136, Test Linf Norm: 3.0385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 11:18:48,771]\u001b[0m Trial 15 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 3.3922, Test Loss: 3.4125, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.3285, Test Loss: 0.1097, Train L1 Norm: 0.4904, Test L1 Norm: 0.0589, Train Linf Norm: 41.7599, Test Linf Norm: 1.6510\n",
            "Epoch 2: Train Loss: 0.0958, Test Loss: 0.0758, Train L1 Norm: 0.1089, Test L1 Norm: 0.0400, Train Linf Norm: 7.7079, Test Linf Norm: 0.8012\n",
            "Epoch 3: Train Loss: 0.0680, Test Loss: 0.0547, Train L1 Norm: 0.1015, Test L1 Norm: 0.0449, Train Linf Norm: 8.4925, Test Linf Norm: 1.8738\n",
            "Epoch 4: Train Loss: 0.0548, Test Loss: 0.0566, Train L1 Norm: 0.0667, Test L1 Norm: 0.0326, Train Linf Norm: 4.8547, Test Linf Norm: 0.6153\n",
            "Epoch 5: Train Loss: 0.0468, Test Loss: 0.0349, Train L1 Norm: 0.0643, Test L1 Norm: 0.0227, Train Linf Norm: 5.0170, Test Linf Norm: 0.6459\n",
            "Epoch 6: Train Loss: 0.0413, Test Loss: 0.0543, Train L1 Norm: 0.0536, Test L1 Norm: 0.0277, Train Linf Norm: 3.9909, Test Linf Norm: 0.5362\n",
            "Epoch 7: Train Loss: 0.0392, Test Loss: 0.0356, Train L1 Norm: 0.0506, Test L1 Norm: 0.0210, Train Linf Norm: 3.7773, Test Linf Norm: 0.5637\n",
            "Epoch 8: Train Loss: 0.0351, Test Loss: 0.0476, Train L1 Norm: 0.0480, Test L1 Norm: 0.0251, Train Linf Norm: 3.6929, Test Linf Norm: 0.4817\n",
            "Epoch 9: Train Loss: 0.0331, Test Loss: 0.0303, Train L1 Norm: 0.0498, Test L1 Norm: 0.0221, Train Linf Norm: 4.0483, Test Linf Norm: 0.8010\n",
            "Epoch 10: Train Loss: 0.0310, Test Loss: 0.0377, Train L1 Norm: 0.0422, Test L1 Norm: 0.0254, Train Linf Norm: 3.2246, Test Linf Norm: 0.9080\n",
            "Epoch 11: Train Loss: 0.0295, Test Loss: 0.0280, Train L1 Norm: 0.0457, Test L1 Norm: 0.0204, Train Linf Norm: 3.7573, Test Linf Norm: 0.7496\n",
            "Epoch 12: Train Loss: 0.0274, Test Loss: 0.0269, Train L1 Norm: 0.0435, Test L1 Norm: 0.0181, Train Linf Norm: 3.6123, Test Linf Norm: 0.4624\n",
            "Epoch 13: Train Loss: 0.0136, Test Loss: 0.0132, Train L1 Norm: 0.0349, Test L1 Norm: 0.0133, Train Linf Norm: 3.1771, Test Linf Norm: 0.5173\n",
            "Epoch 14: Train Loss: 0.0129, Test Loss: 0.0127, Train L1 Norm: 0.0342, Test L1 Norm: 0.0130, Train Linf Norm: 3.1232, Test Linf Norm: 0.5083\n",
            "Epoch 15: Train Loss: 0.0125, Test Loss: 0.0124, Train L1 Norm: 0.0341, Test L1 Norm: 0.0131, Train Linf Norm: 3.1307, Test Linf Norm: 0.5252\n",
            "Epoch 16: Train Loss: 0.0121, Test Loss: 0.0123, Train L1 Norm: 0.0342, Test L1 Norm: 0.0126, Train Linf Norm: 3.1595, Test Linf Norm: 0.4892\n",
            "Epoch 17: Train Loss: 0.0118, Test Loss: 0.0117, Train L1 Norm: 0.0340, Test L1 Norm: 0.0126, Train Linf Norm: 3.1524, Test Linf Norm: 0.4872\n",
            "Epoch 18: Train Loss: 0.0115, Test Loss: 0.0114, Train L1 Norm: 0.0330, Test L1 Norm: 0.0125, Train Linf Norm: 3.0358, Test Linf Norm: 0.4744\n",
            "Epoch 19: Train Loss: 0.0113, Test Loss: 0.0112, Train L1 Norm: 0.0330, Test L1 Norm: 0.0123, Train Linf Norm: 3.0705, Test Linf Norm: 0.4869\n",
            "Epoch 20: Train Loss: 0.0111, Test Loss: 0.0109, Train L1 Norm: 0.0324, Test L1 Norm: 0.0124, Train Linf Norm: 2.9757, Test Linf Norm: 0.4961\n",
            "Epoch 21: Train Loss: 0.0109, Test Loss: 0.0107, Train L1 Norm: 0.0320, Test L1 Norm: 0.0120, Train Linf Norm: 2.9617, Test Linf Norm: 0.4654\n",
            "Epoch 22: Train Loss: 0.0107, Test Loss: 0.0106, Train L1 Norm: 0.0314, Test L1 Norm: 0.0117, Train Linf Norm: 2.8911, Test Linf Norm: 0.4613\n",
            "Epoch 23: Train Loss: 0.0105, Test Loss: 0.0104, Train L1 Norm: 0.0318, Test L1 Norm: 0.0118, Train Linf Norm: 2.9581, Test Linf Norm: 0.4833\n",
            "Epoch 24: Train Loss: 0.0103, Test Loss: 0.0104, Train L1 Norm: 0.0317, Test L1 Norm: 0.0116, Train Linf Norm: 2.9653, Test Linf Norm: 0.4570\n",
            "Epoch 25: Train Loss: 0.0101, Test Loss: 0.0101, Train L1 Norm: 0.0315, Test L1 Norm: 0.0116, Train Linf Norm: 2.9273, Test Linf Norm: 0.4673\n",
            "Epoch 26: Train Loss: 0.0100, Test Loss: 0.0101, Train L1 Norm: 0.0312, Test L1 Norm: 0.0116, Train Linf Norm: 2.9079, Test Linf Norm: 0.4606\n",
            "Epoch 27: Train Loss: 0.0100, Test Loss: 0.0101, Train L1 Norm: 0.0311, Test L1 Norm: 0.0115, Train Linf Norm: 2.8998, Test Linf Norm: 0.4584\n",
            "Epoch 28: Train Loss: 0.0100, Test Loss: 0.0100, Train L1 Norm: 0.0309, Test L1 Norm: 0.0115, Train Linf Norm: 2.8888, Test Linf Norm: 0.4524\n",
            "Epoch 29: Train Loss: 0.0099, Test Loss: 0.0100, Train L1 Norm: 0.0307, Test L1 Norm: 0.0115, Train Linf Norm: 2.8584, Test Linf Norm: 0.4631\n",
            "Epoch 30: Train Loss: 0.0099, Test Loss: 0.0099, Train L1 Norm: 0.0310, Test L1 Norm: 0.0114, Train Linf Norm: 2.8860, Test Linf Norm: 0.4474\n",
            "Epoch 31: Train Loss: 0.0099, Test Loss: 0.0099, Train L1 Norm: 0.0308, Test L1 Norm: 0.0114, Train Linf Norm: 2.8655, Test Linf Norm: 0.4541\n",
            "Epoch 32: Train Loss: 0.0098, Test Loss: 0.0099, Train L1 Norm: 0.0310, Test L1 Norm: 0.0114, Train Linf Norm: 2.8882, Test Linf Norm: 0.4567\n",
            "Epoch 33: Train Loss: 0.0098, Test Loss: 0.0098, Train L1 Norm: 0.0309, Test L1 Norm: 0.0114, Train Linf Norm: 2.8786, Test Linf Norm: 0.4479\n",
            "Epoch 34: Train Loss: 0.0098, Test Loss: 0.0098, Train L1 Norm: 0.0307, Test L1 Norm: 0.0114, Train Linf Norm: 2.8851, Test Linf Norm: 0.4489\n",
            "Epoch 35: Train Loss: 0.0097, Test Loss: 0.0098, Train L1 Norm: 0.0304, Test L1 Norm: 0.0113, Train Linf Norm: 2.6465, Test Linf Norm: 0.4539\n",
            "Epoch 36: Train Loss: 0.0097, Test Loss: 0.0098, Train L1 Norm: 0.0306, Test L1 Norm: 0.0113, Train Linf Norm: 2.8542, Test Linf Norm: 0.4509\n",
            "Epoch 37: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0306, Test L1 Norm: 0.0113, Train Linf Norm: 2.8339, Test Linf Norm: 0.4505\n",
            "Epoch 38: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0308, Test L1 Norm: 0.0113, Train Linf Norm: 2.8622, Test Linf Norm: 0.4494\n",
            "Epoch 39: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0305, Test L1 Norm: 0.0113, Train Linf Norm: 2.8137, Test Linf Norm: 0.4473\n",
            "Epoch 40: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0306, Test L1 Norm: 0.0113, Train Linf Norm: 2.8475, Test Linf Norm: 0.4496\n",
            "Epoch 41: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0306, Test L1 Norm: 0.0113, Train Linf Norm: 2.8652, Test Linf Norm: 0.4434\n",
            "Epoch 42: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0305, Test L1 Norm: 0.0113, Train Linf Norm: 2.8313, Test Linf Norm: 0.4446\n",
            "Epoch 43: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0305, Test L1 Norm: 0.0113, Train Linf Norm: 2.8573, Test Linf Norm: 0.4448\n",
            "Epoch 44: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0306, Test L1 Norm: 0.0112, Train Linf Norm: 2.8277, Test Linf Norm: 0.4425\n",
            "Epoch 45: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8393, Test Linf Norm: 0.4455\n",
            "Epoch 46: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0306, Test L1 Norm: 0.0112, Train Linf Norm: 2.8538, Test Linf Norm: 0.4424\n",
            "Epoch 47: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0306, Test L1 Norm: 0.0113, Train Linf Norm: 2.8432, Test Linf Norm: 0.4494\n",
            "Epoch 48: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8474, Test Linf Norm: 0.4447\n",
            "Epoch 49: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8416, Test Linf Norm: 0.4454\n",
            "Epoch 50: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8641, Test Linf Norm: 0.4454\n",
            "Epoch 51: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8368, Test Linf Norm: 0.4446\n",
            "Epoch 52: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8354, Test Linf Norm: 0.4454\n",
            "Epoch 53: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8646, Test Linf Norm: 0.4447\n",
            "Epoch 54: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8521, Test Linf Norm: 0.4433\n",
            "Epoch 55: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8496, Test Linf Norm: 0.4448\n",
            "Epoch 56: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8483, Test Linf Norm: 0.4441\n",
            "Epoch 57: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8485, Test Linf Norm: 0.4435\n",
            "Epoch 58: Train Loss: 0.0096, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8469, Test Linf Norm: 0.4450\n",
            "Epoch 59: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8403, Test Linf Norm: 0.4456\n",
            "Epoch 60: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8524, Test Linf Norm: 0.4439\n",
            "Epoch 61: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8638, Test Linf Norm: 0.4438\n",
            "Epoch 62: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8415, Test Linf Norm: 0.4438\n",
            "Epoch 63: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8450, Test Linf Norm: 0.4440\n",
            "Epoch 64: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8557, Test Linf Norm: 0.4439\n",
            "Epoch 65: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8516, Test Linf Norm: 0.4438\n",
            "Epoch 66: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8585, Test Linf Norm: 0.4440\n",
            "Epoch 67: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8327, Test Linf Norm: 0.4440\n",
            "Epoch 68: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8501, Test Linf Norm: 0.4440\n",
            "Epoch 69: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8627, Test Linf Norm: 0.4440\n",
            "Epoch 70: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8328, Test Linf Norm: 0.4439\n",
            "Epoch 71: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8567, Test Linf Norm: 0.4440\n",
            "Epoch 72: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8340, Test Linf Norm: 0.4440\n",
            "Epoch 73: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8577, Test Linf Norm: 0.4440\n",
            "Epoch 74: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8586, Test Linf Norm: 0.4440\n",
            "Epoch 75: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8472, Test Linf Norm: 0.4440\n",
            "Epoch 76: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8291, Test Linf Norm: 0.4440\n",
            "Epoch 77: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8510, Test Linf Norm: 0.4440\n",
            "Epoch 78: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8495, Test Linf Norm: 0.4440\n",
            "Epoch 79: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8496, Test Linf Norm: 0.4440\n",
            "Epoch 80: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8511, Test Linf Norm: 0.4440\n",
            "Epoch 81: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.7658, Test Linf Norm: 0.4440\n",
            "Epoch 82: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8486, Test Linf Norm: 0.4440\n",
            "Epoch 83: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8626, Test Linf Norm: 0.4440\n",
            "Epoch 84: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8410, Test Linf Norm: 0.4440\n",
            "Epoch 85: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8337, Test Linf Norm: 0.4440\n",
            "Epoch 86: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8482, Test Linf Norm: 0.4440\n",
            "Epoch 87: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8383, Test Linf Norm: 0.4440\n",
            "Epoch 88: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8349, Test Linf Norm: 0.4440\n",
            "Epoch 89: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8288, Test Linf Norm: 0.4440\n",
            "Epoch 90: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8282, Test Linf Norm: 0.4440\n",
            "Epoch 91: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8580, Test Linf Norm: 0.4440\n",
            "Epoch 92: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8554, Test Linf Norm: 0.4440\n",
            "Epoch 93: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8508, Test Linf Norm: 0.4440\n",
            "Epoch 94: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8303, Test Linf Norm: 0.4440\n",
            "Epoch 95: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8658, Test Linf Norm: 0.4440\n",
            "Epoch 96: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8651, Test Linf Norm: 0.4440\n",
            "Epoch 97: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8280, Test Linf Norm: 0.4440\n",
            "Epoch 98: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8527, Test Linf Norm: 0.4440\n",
            "Epoch 99: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8562, Test Linf Norm: 0.4440\n",
            "Epoch 100: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8503, Test Linf Norm: 0.4440\n",
            "Epoch 101: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8279, Test Linf Norm: 0.4440\n",
            "Epoch 102: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8319, Test Linf Norm: 0.4440\n",
            "Epoch 103: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8549, Test Linf Norm: 0.4440\n",
            "Epoch 104: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8437, Test Linf Norm: 0.4440\n",
            "Epoch 105: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8302, Test Linf Norm: 0.4440\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 11:25:20,107]\u001b[0m Trial 16 finished with value: 0.011224059377610684 and parameters: {'n_layers': 6, 'n_units_0': 2042, 'n_units_1': 1243, 'n_units_2': 937, 'n_units_3': 2029, 'n_units_4': 436, 'n_units_5': 533, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'MAE', 'optimizer': 'Adagrad', 'lr': 0.00010004922522277522, 'batch_size': 128, 'n_epochs': 106, 'scheduler': 'StepLR', 'step_size': 12, 'gamma': 0.23343121945561063}. Best is trial 11 with value: 0.0071948581174016.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 106: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0305, Test L1 Norm: 0.0112, Train Linf Norm: 2.8594, Test Linf Norm: 0.4440\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 11:25:22,040]\u001b[0m Trial 17 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.8018, Test Loss: 0.4206, Train L1 Norm: 0.8756, Test L1 Norm: 0.3213, Train Linf Norm: 377.2215, Test Linf Norm: 42.9503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 11:25:26,477]\u001b[0m Trial 18 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 3.3922, Test Loss: 3.4125, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 11:25:29,409]\u001b[0m Trial 19 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 3.4440, Test Loss: 3.4125, Train L1 Norm: 1.1393, Test L1 Norm: 1.0000, Train Linf Norm: 15.1420, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 11:25:30,922]\u001b[0m Trial 20 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.7232, Test Loss: 0.4564, Train L1 Norm: 0.9060, Test L1 Norm: 0.1769, Train Linf Norm: 254.2427, Test Linf Norm: 8.0636\n",
            "Epoch 1: Train Loss: 0.3375, Test Loss: 0.1395, Train L1 Norm: 0.3684, Test L1 Norm: 0.0657, Train Linf Norm: 24.7821, Test Linf Norm: 1.3027\n",
            "Epoch 2: Train Loss: 0.0969, Test Loss: 0.0713, Train L1 Norm: 0.0938, Test L1 Norm: 0.0404, Train Linf Norm: 5.7938, Test Linf Norm: 1.0166\n",
            "Epoch 3: Train Loss: 0.0673, Test Loss: 0.0598, Train L1 Norm: 0.0722, Test L1 Norm: 0.0452, Train Linf Norm: 4.7528, Test Linf Norm: 1.7307\n",
            "Epoch 4: Train Loss: 0.0555, Test Loss: 0.0473, Train L1 Norm: 0.0682, Test L1 Norm: 0.0289, Train Linf Norm: 4.9707, Test Linf Norm: 0.8101\n",
            "Epoch 5: Train Loss: 0.0467, Test Loss: 0.0545, Train L1 Norm: 0.0596, Test L1 Norm: 0.0306, Train Linf Norm: 4.2258, Test Linf Norm: 0.7268\n",
            "Epoch 6: Train Loss: 0.0425, Test Loss: 0.0456, Train L1 Norm: 0.0521, Test L1 Norm: 0.0385, Train Linf Norm: 3.6865, Test Linf Norm: 1.6381\n",
            "Epoch 7: Train Loss: 0.0379, Test Loss: 0.0371, Train L1 Norm: 0.0531, Test L1 Norm: 0.0240, Train Linf Norm: 4.0794, Test Linf Norm: 0.6965\n",
            "Epoch 8: Train Loss: 0.0358, Test Loss: 0.0452, Train L1 Norm: 0.0450, Test L1 Norm: 0.0325, Train Linf Norm: 3.2149, Test Linf Norm: 1.1925\n",
            "Epoch 9: Train Loss: 0.0331, Test Loss: 0.0430, Train L1 Norm: 0.0424, Test L1 Norm: 0.0235, Train Linf Norm: 3.0659, Test Linf Norm: 0.6687\n",
            "Epoch 10: Train Loss: 0.0308, Test Loss: 0.0390, Train L1 Norm: 0.0474, Test L1 Norm: 0.0222, Train Linf Norm: 3.8867, Test Linf Norm: 0.6137\n",
            "Epoch 11: Train Loss: 0.0288, Test Loss: 0.0210, Train L1 Norm: 0.0420, Test L1 Norm: 0.0167, Train Linf Norm: 3.3085, Test Linf Norm: 0.6119\n",
            "Epoch 12: Train Loss: 0.0278, Test Loss: 0.0423, Train L1 Norm: 0.0446, Test L1 Norm: 0.0296, Train Linf Norm: 3.6882, Test Linf Norm: 1.0796\n",
            "Epoch 13: Train Loss: 0.0261, Test Loss: 0.0177, Train L1 Norm: 0.0386, Test L1 Norm: 0.0158, Train Linf Norm: 3.0298, Test Linf Norm: 0.6141\n",
            "Epoch 14: Train Loss: 0.0131, Test Loss: 0.0129, Train L1 Norm: 0.0308, Test L1 Norm: 0.0147, Train Linf Norm: 2.6688, Test Linf Norm: 0.6737\n",
            "Epoch 15: Train Loss: 0.0126, Test Loss: 0.0124, Train L1 Norm: 0.0299, Test L1 Norm: 0.0141, Train Linf Norm: 2.6060, Test Linf Norm: 0.6261\n",
            "Epoch 16: Train Loss: 0.0122, Test Loss: 0.0122, Train L1 Norm: 0.0289, Test L1 Norm: 0.0137, Train Linf Norm: 2.4746, Test Linf Norm: 0.5911\n",
            "Epoch 17: Train Loss: 0.0119, Test Loss: 0.0117, Train L1 Norm: 0.0288, Test L1 Norm: 0.0137, Train Linf Norm: 2.4938, Test Linf Norm: 0.6203\n",
            "Epoch 18: Train Loss: 0.0116, Test Loss: 0.0115, Train L1 Norm: 0.0287, Test L1 Norm: 0.0133, Train Linf Norm: 2.4980, Test Linf Norm: 0.5863\n",
            "Epoch 19: Train Loss: 0.0114, Test Loss: 0.0114, Train L1 Norm: 0.0283, Test L1 Norm: 0.0133, Train Linf Norm: 2.4453, Test Linf Norm: 0.5949\n",
            "Epoch 20: Train Loss: 0.0112, Test Loss: 0.0111, Train L1 Norm: 0.0279, Test L1 Norm: 0.0130, Train Linf Norm: 2.4094, Test Linf Norm: 0.5704\n",
            "Epoch 21: Train Loss: 0.0110, Test Loss: 0.0111, Train L1 Norm: 0.0277, Test L1 Norm: 0.0133, Train Linf Norm: 2.3925, Test Linf Norm: 0.6140\n",
            "Epoch 22: Train Loss: 0.0108, Test Loss: 0.0107, Train L1 Norm: 0.0276, Test L1 Norm: 0.0127, Train Linf Norm: 2.3957, Test Linf Norm: 0.5536\n",
            "Epoch 23: Train Loss: 0.0106, Test Loss: 0.0106, Train L1 Norm: 0.0265, Test L1 Norm: 0.0126, Train Linf Norm: 2.2617, Test Linf Norm: 0.5563\n",
            "Epoch 24: Train Loss: 0.0105, Test Loss: 0.0104, Train L1 Norm: 0.0268, Test L1 Norm: 0.0126, Train Linf Norm: 2.3252, Test Linf Norm: 0.5808\n",
            "Epoch 25: Train Loss: 0.0103, Test Loss: 0.0102, Train L1 Norm: 0.0267, Test L1 Norm: 0.0128, Train Linf Norm: 2.3277, Test Linf Norm: 0.5781\n",
            "Epoch 26: Train Loss: 0.0102, Test Loss: 0.0101, Train L1 Norm: 0.0256, Test L1 Norm: 0.0121, Train Linf Norm: 2.1972, Test Linf Norm: 0.5417\n",
            "Epoch 27: Train Loss: 0.0099, Test Loss: 0.0100, Train L1 Norm: 0.0262, Test L1 Norm: 0.0121, Train Linf Norm: 2.2748, Test Linf Norm: 0.5337\n",
            "Epoch 28: Train Loss: 0.0099, Test Loss: 0.0100, Train L1 Norm: 0.0260, Test L1 Norm: 0.0120, Train Linf Norm: 2.2646, Test Linf Norm: 0.5320\n",
            "Epoch 29: Train Loss: 0.0099, Test Loss: 0.0100, Train L1 Norm: 0.0260, Test L1 Norm: 0.0120, Train Linf Norm: 2.2523, Test Linf Norm: 0.5260\n",
            "Epoch 30: Train Loss: 0.0098, Test Loss: 0.0099, Train L1 Norm: 0.0257, Test L1 Norm: 0.0120, Train Linf Norm: 2.2240, Test Linf Norm: 0.5281\n",
            "Epoch 31: Train Loss: 0.0098, Test Loss: 0.0099, Train L1 Norm: 0.0258, Test L1 Norm: 0.0120, Train Linf Norm: 2.2490, Test Linf Norm: 0.5300\n",
            "Epoch 32: Train Loss: 0.0098, Test Loss: 0.0100, Train L1 Norm: 0.0258, Test L1 Norm: 0.0119, Train Linf Norm: 2.2331, Test Linf Norm: 0.5217\n",
            "Epoch 33: Train Loss: 0.0098, Test Loss: 0.0099, Train L1 Norm: 0.0259, Test L1 Norm: 0.0119, Train Linf Norm: 2.2698, Test Linf Norm: 0.5261\n",
            "Epoch 34: Train Loss: 0.0097, Test Loss: 0.0099, Train L1 Norm: 0.0258, Test L1 Norm: 0.0119, Train Linf Norm: 2.2512, Test Linf Norm: 0.5230\n",
            "Epoch 35: Train Loss: 0.0097, Test Loss: 0.0098, Train L1 Norm: 0.0257, Test L1 Norm: 0.0119, Train Linf Norm: 2.2445, Test Linf Norm: 0.5301\n",
            "Epoch 36: Train Loss: 0.0097, Test Loss: 0.0098, Train L1 Norm: 0.0258, Test L1 Norm: 0.0119, Train Linf Norm: 2.2629, Test Linf Norm: 0.5238\n",
            "Epoch 37: Train Loss: 0.0097, Test Loss: 0.0097, Train L1 Norm: 0.0257, Test L1 Norm: 0.0119, Train Linf Norm: 2.2527, Test Linf Norm: 0.5223\n",
            "Epoch 38: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0254, Test L1 Norm: 0.0118, Train Linf Norm: 2.1736, Test Linf Norm: 0.5222\n",
            "Epoch 39: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0255, Test L1 Norm: 0.0118, Train Linf Norm: 2.2274, Test Linf Norm: 0.5187\n",
            "Epoch 40: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0255, Test L1 Norm: 0.0118, Train Linf Norm: 2.2219, Test Linf Norm: 0.5177\n",
            "Epoch 41: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0256, Test L1 Norm: 0.0118, Train Linf Norm: 2.2214, Test Linf Norm: 0.5154\n",
            "Epoch 42: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0256, Test L1 Norm: 0.0118, Train Linf Norm: 2.2310, Test Linf Norm: 0.5180\n",
            "Epoch 43: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0254, Test L1 Norm: 0.0118, Train Linf Norm: 2.2343, Test Linf Norm: 0.5173\n",
            "Epoch 44: Train Loss: 0.0096, Test Loss: 0.0097, Train L1 Norm: 0.0255, Test L1 Norm: 0.0118, Train Linf Norm: 2.2138, Test Linf Norm: 0.5187\n",
            "Epoch 45: Train Loss: 0.0095, Test Loss: 0.0097, Train L1 Norm: 0.0255, Test L1 Norm: 0.0118, Train Linf Norm: 2.2317, Test Linf Norm: 0.5185\n",
            "Epoch 46: Train Loss: 0.0095, Test Loss: 0.0097, Train L1 Norm: 0.0254, Test L1 Norm: 0.0118, Train Linf Norm: 2.2216, Test Linf Norm: 0.5175\n",
            "Epoch 47: Train Loss: 0.0095, Test Loss: 0.0097, Train L1 Norm: 0.0254, Test L1 Norm: 0.0118, Train Linf Norm: 2.2249, Test Linf Norm: 0.5200\n",
            "Epoch 48: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0255, Test L1 Norm: 0.0118, Train Linf Norm: 2.2348, Test Linf Norm: 0.5185\n",
            "Epoch 49: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0255, Test L1 Norm: 0.0118, Train Linf Norm: 2.2171, Test Linf Norm: 0.5195\n",
            "Epoch 50: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0118, Train Linf Norm: 2.2257, Test Linf Norm: 0.5185\n",
            "Epoch 51: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2190, Test Linf Norm: 0.5162\n",
            "Epoch 52: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0255, Test L1 Norm: 0.0117, Train Linf Norm: 2.2333, Test Linf Norm: 0.5177\n",
            "Epoch 53: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2158, Test Linf Norm: 0.5165\n",
            "Epoch 54: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2024, Test Linf Norm: 0.5157\n",
            "Epoch 55: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2212, Test Linf Norm: 0.5162\n",
            "Epoch 56: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2043, Test Linf Norm: 0.5167\n",
            "Epoch 57: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2144, Test Linf Norm: 0.5156\n",
            "Epoch 58: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2269, Test Linf Norm: 0.5160\n",
            "Epoch 59: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2160, Test Linf Norm: 0.5165\n",
            "Epoch 60: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2229, Test Linf Norm: 0.5152\n",
            "Epoch 61: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.1945, Test Linf Norm: 0.5167\n",
            "Epoch 62: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2054, Test Linf Norm: 0.5152\n",
            "Epoch 63: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.1943, Test Linf Norm: 0.5168\n",
            "Epoch 64: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2216, Test Linf Norm: 0.5162\n",
            "Epoch 65: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2045, Test Linf Norm: 0.5159\n",
            "Epoch 66: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2184, Test Linf Norm: 0.5158\n",
            "Epoch 67: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.1935, Test Linf Norm: 0.5160\n",
            "Epoch 68: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2138, Test Linf Norm: 0.5159\n",
            "Epoch 69: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2174, Test Linf Norm: 0.5160\n",
            "Epoch 70: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2288, Test Linf Norm: 0.5159\n",
            "Epoch 71: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.1910, Test Linf Norm: 0.5159\n",
            "Epoch 72: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2120, Test Linf Norm: 0.5160\n",
            "Epoch 73: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2069, Test Linf Norm: 0.5160\n",
            "Epoch 74: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2125, Test Linf Norm: 0.5160\n",
            "Epoch 75: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2142, Test Linf Norm: 0.5159\n",
            "Epoch 76: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.1672, Test Linf Norm: 0.5160\n",
            "Epoch 77: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2209, Test Linf Norm: 0.5159\n",
            "Epoch 78: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2170, Test Linf Norm: 0.5158\n",
            "Epoch 79: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2176, Test Linf Norm: 0.5158\n",
            "Epoch 80: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2196, Test Linf Norm: 0.5159\n",
            "Epoch 81: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2056, Test Linf Norm: 0.5159\n",
            "Epoch 82: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2154, Test Linf Norm: 0.5159\n",
            "Epoch 83: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2266, Test Linf Norm: 0.5159\n",
            "Epoch 84: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2254, Test Linf Norm: 0.5159\n",
            "Epoch 85: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2178, Test Linf Norm: 0.5159\n",
            "Epoch 86: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2075, Test Linf Norm: 0.5159\n",
            "Epoch 87: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2213, Test Linf Norm: 0.5159\n",
            "Epoch 88: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2091, Test Linf Norm: 0.5159\n",
            "Epoch 89: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.1978, Test Linf Norm: 0.5159\n",
            "Epoch 90: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2159, Test Linf Norm: 0.5159\n",
            "Epoch 91: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2098, Test Linf Norm: 0.5159\n",
            "Epoch 92: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.1990, Test Linf Norm: 0.5159\n",
            "Epoch 93: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2174, Test Linf Norm: 0.5159\n",
            "Epoch 94: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2154, Test Linf Norm: 0.5159\n",
            "Epoch 95: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2104, Test Linf Norm: 0.5159\n",
            "Epoch 96: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2017, Test Linf Norm: 0.5159\n",
            "Epoch 97: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2172, Test Linf Norm: 0.5159\n",
            "Epoch 98: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2090, Test Linf Norm: 0.5159\n",
            "Epoch 99: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2203, Test Linf Norm: 0.5159\n",
            "Epoch 100: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2052, Test Linf Norm: 0.5159\n",
            "Epoch 101: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2187, Test Linf Norm: 0.5159\n",
            "Epoch 102: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2144, Test Linf Norm: 0.5159\n",
            "Epoch 103: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2154, Test Linf Norm: 0.5159\n",
            "Epoch 104: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2201, Test Linf Norm: 0.5159\n",
            "Epoch 105: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2126, Test Linf Norm: 0.5159\n",
            "Epoch 106: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2049, Test Linf Norm: 0.5159\n",
            "Epoch 107: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.1951, Test Linf Norm: 0.5159\n",
            "Epoch 108: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.1904, Test Linf Norm: 0.5159\n",
            "Epoch 109: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2177, Test Linf Norm: 0.5159\n",
            "Epoch 110: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2051, Test Linf Norm: 0.5159\n",
            "Epoch 111: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.1972, Test Linf Norm: 0.5159\n",
            "Epoch 112: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2097, Test Linf Norm: 0.5159\n",
            "Epoch 113: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2191, Test Linf Norm: 0.5159\n",
            "Epoch 114: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2243, Test Linf Norm: 0.5159\n",
            "Epoch 115: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2152, Test Linf Norm: 0.5159\n",
            "Epoch 116: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2190, Test Linf Norm: 0.5159\n",
            "Epoch 117: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2167, Test Linf Norm: 0.5159\n",
            "Epoch 118: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2187, Test Linf Norm: 0.5159\n",
            "Epoch 119: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2030, Test Linf Norm: 0.5159\n",
            "Epoch 120: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2107, Test Linf Norm: 0.5159\n",
            "Epoch 121: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2143, Test Linf Norm: 0.5159\n",
            "Epoch 122: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2160, Test Linf Norm: 0.5159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 11:32:58,973]\u001b[0m Trial 21 finished with value: 0.011723011416196824 and parameters: {'n_layers': 6, 'n_units_0': 2043, 'n_units_1': 1325, 'n_units_2': 979, 'n_units_3': 2008, 'n_units_4': 239, 'n_units_5': 341, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'MAE', 'optimizer': 'Adagrad', 'lr': 0.0001069357396592891, 'batch_size': 128, 'n_epochs': 123, 'scheduler': 'StepLR', 'step_size': 13, 'gamma': 0.22839255793674637}. Best is trial 11 with value: 0.0071948581174016.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 123: Train Loss: 0.0095, Test Loss: 0.0096, Train L1 Norm: 0.0254, Test L1 Norm: 0.0117, Train Linf Norm: 2.2216, Test Linf Norm: 0.5159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 11:33:02,744]\u001b[0m Trial 22 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 3.3922, Test Loss: 3.4125, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.3484, Test Loss: 0.2017, Train L1 Norm: 0.3755, Test L1 Norm: 0.0822, Train Linf Norm: 27.8196, Test Linf Norm: 1.4597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 11:33:10,709]\u001b[0m Trial 23 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 0.1287, Test Loss: 0.0712, Train L1 Norm: 0.1323, Test L1 Norm: 0.0521, Train Linf Norm: 9.3681, Test Linf Norm: 2.1776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 11:33:12,505]\u001b[0m Trial 24 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.8732, Test Loss: 0.5081, Train L1 Norm: 1.1296, Test L1 Norm: 0.3662, Train Linf Norm: 510.6913, Test Linf Norm: 48.9539\n",
            "Epoch 1: Train Loss: 0.1957, Test Loss: 0.1164, Train L1 Norm: 0.2517, Test L1 Norm: 0.0596, Train Linf Norm: 7.8807, Test Linf Norm: 0.4682\n",
            "Epoch 2: Train Loss: 0.0620, Test Loss: 0.0385, Train L1 Norm: 0.0808, Test L1 Norm: 0.0305, Train Linf Norm: 2.5492, Test Linf Norm: 0.5880\n",
            "Epoch 3: Train Loss: 0.0445, Test Loss: 0.0176, Train L1 Norm: 0.0584, Test L1 Norm: 0.0172, Train Linf Norm: 1.8304, Test Linf Norm: 0.3261\n",
            "Epoch 4: Train Loss: 0.0373, Test Loss: 0.0276, Train L1 Norm: 0.0467, Test L1 Norm: 0.0208, Train Linf Norm: 1.4347, Test Linf Norm: 0.4120\n",
            "Epoch 5: Train Loss: 0.0326, Test Loss: 0.0186, Train L1 Norm: 0.0483, Test L1 Norm: 0.0155, Train Linf Norm: 1.5966, Test Linf Norm: 0.3129\n",
            "Epoch 6: Train Loss: 0.0288, Test Loss: 0.0410, Train L1 Norm: 0.0414, Test L1 Norm: 0.0277, Train Linf Norm: 1.3564, Test Linf Norm: 0.4922\n",
            "Epoch 7: Train Loss: 0.0263, Test Loss: 0.0251, Train L1 Norm: 0.0506, Test L1 Norm: 0.0211, Train Linf Norm: 1.8595, Test Linf Norm: 0.4362\n",
            "Epoch 8: Train Loss: 0.0245, Test Loss: 0.0166, Train L1 Norm: 0.0324, Test L1 Norm: 0.0143, Train Linf Norm: 1.0171, Test Linf Norm: 0.2534\n",
            "Epoch 9: Train Loss: 0.0224, Test Loss: 0.0349, Train L1 Norm: 0.0305, Test L1 Norm: 0.0188, Train Linf Norm: 0.9630, Test Linf Norm: 0.2540\n",
            "Epoch 10: Train Loss: 0.0215, Test Loss: 0.0102, Train L1 Norm: 0.0359, Test L1 Norm: 0.0142, Train Linf Norm: 1.2472, Test Linf Norm: 0.3440\n",
            "Epoch 11: Train Loss: 0.0202, Test Loss: 0.0133, Train L1 Norm: 0.0284, Test L1 Norm: 0.0122, Train Linf Norm: 0.9180, Test Linf Norm: 0.2284\n",
            "Epoch 12: Train Loss: 0.0190, Test Loss: 0.0173, Train L1 Norm: 0.0259, Test L1 Norm: 0.0132, Train Linf Norm: 0.8172, Test Linf Norm: 0.2707\n",
            "Epoch 13: Train Loss: 0.0183, Test Loss: 0.0167, Train L1 Norm: 0.0318, Test L1 Norm: 0.0118, Train Linf Norm: 1.1158, Test Linf Norm: 0.2335\n",
            "Epoch 14: Train Loss: 0.0175, Test Loss: 0.0118, Train L1 Norm: 0.0262, Test L1 Norm: 0.0101, Train Linf Norm: 0.8587, Test Linf Norm: 0.2048\n",
            "Epoch 15: Train Loss: 0.0079, Test Loss: 0.0077, Train L1 Norm: 0.0215, Test L1 Norm: 0.0088, Train Linf Norm: 0.8045, Test Linf Norm: 0.1998\n",
            "Epoch 16: Train Loss: 0.0076, Test Loss: 0.0077, Train L1 Norm: 0.0204, Test L1 Norm: 0.0086, Train Linf Norm: 0.7518, Test Linf Norm: 0.1969\n",
            "Epoch 17: Train Loss: 0.0075, Test Loss: 0.0084, Train L1 Norm: 0.0203, Test L1 Norm: 0.0085, Train Linf Norm: 0.7512, Test Linf Norm: 0.1826\n",
            "Epoch 18: Train Loss: 0.0074, Test Loss: 0.0075, Train L1 Norm: 0.0199, Test L1 Norm: 0.0084, Train Linf Norm: 0.7306, Test Linf Norm: 0.1879\n",
            "Epoch 19: Train Loss: 0.0073, Test Loss: 0.0074, Train L1 Norm: 0.0204, Test L1 Norm: 0.0083, Train Linf Norm: 0.7628, Test Linf Norm: 0.1809\n",
            "Epoch 20: Train Loss: 0.0072, Test Loss: 0.0076, Train L1 Norm: 0.0191, Test L1 Norm: 0.0085, Train Linf Norm: 0.7029, Test Linf Norm: 0.1897\n",
            "Epoch 21: Train Loss: 0.0072, Test Loss: 0.0077, Train L1 Norm: 0.0200, Test L1 Norm: 0.0085, Train Linf Norm: 0.7415, Test Linf Norm: 0.1932\n",
            "Epoch 22: Train Loss: 0.0071, Test Loss: 0.0072, Train L1 Norm: 0.0200, Test L1 Norm: 0.0080, Train Linf Norm: 0.7470, Test Linf Norm: 0.1774\n",
            "Epoch 23: Train Loss: 0.0070, Test Loss: 0.0070, Train L1 Norm: 0.0195, Test L1 Norm: 0.0080, Train Linf Norm: 0.7220, Test Linf Norm: 0.1746\n",
            "Epoch 24: Train Loss: 0.0070, Test Loss: 0.0069, Train L1 Norm: 0.0196, Test L1 Norm: 0.0079, Train Linf Norm: 0.7300, Test Linf Norm: 0.1764\n",
            "Epoch 25: Train Loss: 0.0069, Test Loss: 0.0068, Train L1 Norm: 0.0193, Test L1 Norm: 0.0080, Train Linf Norm: 0.7182, Test Linf Norm: 0.1762\n",
            "Epoch 26: Train Loss: 0.0068, Test Loss: 0.0069, Train L1 Norm: 0.0195, Test L1 Norm: 0.0079, Train Linf Norm: 0.7296, Test Linf Norm: 0.1755\n",
            "Epoch 27: Train Loss: 0.0068, Test Loss: 0.0069, Train L1 Norm: 0.0191, Test L1 Norm: 0.0080, Train Linf Norm: 0.7056, Test Linf Norm: 0.1823\n",
            "Epoch 28: Train Loss: 0.0068, Test Loss: 0.0067, Train L1 Norm: 0.0188, Test L1 Norm: 0.0077, Train Linf Norm: 0.6938, Test Linf Norm: 0.1707\n",
            "Epoch 29: Train Loss: 0.0066, Test Loss: 0.0068, Train L1 Norm: 0.0186, Test L1 Norm: 0.0078, Train Linf Norm: 0.6896, Test Linf Norm: 0.1737\n",
            "Epoch 30: Train Loss: 0.0066, Test Loss: 0.0067, Train L1 Norm: 0.0188, Test L1 Norm: 0.0077, Train Linf Norm: 0.6987, Test Linf Norm: 0.1698\n",
            "Epoch 31: Train Loss: 0.0066, Test Loss: 0.0067, Train L1 Norm: 0.0186, Test L1 Norm: 0.0077, Train Linf Norm: 0.6897, Test Linf Norm: 0.1712\n",
            "Epoch 32: Train Loss: 0.0066, Test Loss: 0.0067, Train L1 Norm: 0.0186, Test L1 Norm: 0.0077, Train Linf Norm: 0.6888, Test Linf Norm: 0.1705\n",
            "Epoch 33: Train Loss: 0.0065, Test Loss: 0.0067, Train L1 Norm: 0.0187, Test L1 Norm: 0.0078, Train Linf Norm: 0.6937, Test Linf Norm: 0.1721\n",
            "Epoch 34: Train Loss: 0.0065, Test Loss: 0.0067, Train L1 Norm: 0.0187, Test L1 Norm: 0.0077, Train Linf Norm: 0.6966, Test Linf Norm: 0.1702\n",
            "Epoch 35: Train Loss: 0.0065, Test Loss: 0.0067, Train L1 Norm: 0.0185, Test L1 Norm: 0.0077, Train Linf Norm: 0.6895, Test Linf Norm: 0.1704\n",
            "Epoch 36: Train Loss: 0.0065, Test Loss: 0.0067, Train L1 Norm: 0.0186, Test L1 Norm: 0.0077, Train Linf Norm: 0.6895, Test Linf Norm: 0.1711\n",
            "Epoch 37: Train Loss: 0.0065, Test Loss: 0.0066, Train L1 Norm: 0.0186, Test L1 Norm: 0.0077, Train Linf Norm: 0.6899, Test Linf Norm: 0.1706\n",
            "Epoch 38: Train Loss: 0.0065, Test Loss: 0.0067, Train L1 Norm: 0.0186, Test L1 Norm: 0.0077, Train Linf Norm: 0.6915, Test Linf Norm: 0.1700\n",
            "Epoch 39: Train Loss: 0.0065, Test Loss: 0.0067, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6831, Test Linf Norm: 0.1692\n",
            "Epoch 40: Train Loss: 0.0065, Test Loss: 0.0066, Train L1 Norm: 0.0186, Test L1 Norm: 0.0077, Train Linf Norm: 0.6902, Test Linf Norm: 0.1719\n",
            "Epoch 41: Train Loss: 0.0065, Test Loss: 0.0067, Train L1 Norm: 0.0186, Test L1 Norm: 0.0077, Train Linf Norm: 0.6913, Test Linf Norm: 0.1688\n",
            "Epoch 42: Train Loss: 0.0065, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6860, Test Linf Norm: 0.1692\n",
            "Epoch 43: Train Loss: 0.0065, Test Loss: 0.0066, Train L1 Norm: 0.0185, Test L1 Norm: 0.0077, Train Linf Norm: 0.6861, Test Linf Norm: 0.1703\n",
            "Epoch 44: Train Loss: 0.0065, Test Loss: 0.0066, Train L1 Norm: 0.0186, Test L1 Norm: 0.0077, Train Linf Norm: 0.6893, Test Linf Norm: 0.1703\n",
            "Epoch 45: Train Loss: 0.0065, Test Loss: 0.0066, Train L1 Norm: 0.0185, Test L1 Norm: 0.0077, Train Linf Norm: 0.6901, Test Linf Norm: 0.1698\n",
            "Epoch 46: Train Loss: 0.0065, Test Loss: 0.0066, Train L1 Norm: 0.0185, Test L1 Norm: 0.0077, Train Linf Norm: 0.6851, Test Linf Norm: 0.1705\n",
            "Epoch 47: Train Loss: 0.0065, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6841, Test Linf Norm: 0.1703\n",
            "Epoch 48: Train Loss: 0.0065, Test Loss: 0.0066, Train L1 Norm: 0.0185, Test L1 Norm: 0.0077, Train Linf Norm: 0.6852, Test Linf Norm: 0.1706\n",
            "Epoch 49: Train Loss: 0.0065, Test Loss: 0.0066, Train L1 Norm: 0.0185, Test L1 Norm: 0.0077, Train Linf Norm: 0.6868, Test Linf Norm: 0.1698\n",
            "Epoch 50: Train Loss: 0.0065, Test Loss: 0.0066, Train L1 Norm: 0.0185, Test L1 Norm: 0.0077, Train Linf Norm: 0.6820, Test Linf Norm: 0.1702\n",
            "Epoch 51: Train Loss: 0.0065, Test Loss: 0.0066, Train L1 Norm: 0.0185, Test L1 Norm: 0.0077, Train Linf Norm: 0.6847, Test Linf Norm: 0.1698\n",
            "Epoch 52: Train Loss: 0.0065, Test Loss: 0.0066, Train L1 Norm: 0.0185, Test L1 Norm: 0.0077, Train Linf Norm: 0.6880, Test Linf Norm: 0.1694\n",
            "Epoch 53: Train Loss: 0.0065, Test Loss: 0.0066, Train L1 Norm: 0.0185, Test L1 Norm: 0.0077, Train Linf Norm: 0.6855, Test Linf Norm: 0.1703\n",
            "Epoch 54: Train Loss: 0.0065, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6840, Test Linf Norm: 0.1698\n",
            "Epoch 55: Train Loss: 0.0065, Test Loss: 0.0066, Train L1 Norm: 0.0185, Test L1 Norm: 0.0077, Train Linf Norm: 0.6879, Test Linf Norm: 0.1696\n",
            "Epoch 56: Train Loss: 0.0065, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6857, Test Linf Norm: 0.1696\n",
            "Epoch 57: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6864, Test Linf Norm: 0.1694\n",
            "Epoch 58: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6819, Test Linf Norm: 0.1694\n",
            "Epoch 59: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6839, Test Linf Norm: 0.1694\n",
            "Epoch 60: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6840, Test Linf Norm: 0.1695\n",
            "Epoch 61: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6843, Test Linf Norm: 0.1694\n",
            "Epoch 62: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6846, Test Linf Norm: 0.1695\n",
            "Epoch 63: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6845, Test Linf Norm: 0.1695\n",
            "Epoch 64: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6839, Test Linf Norm: 0.1695\n",
            "Epoch 65: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6829, Test Linf Norm: 0.1695\n",
            "Epoch 66: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6834, Test Linf Norm: 0.1695\n",
            "Epoch 67: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6858, Test Linf Norm: 0.1695\n",
            "Epoch 68: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6841, Test Linf Norm: 0.1695\n",
            "Epoch 69: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6840, Test Linf Norm: 0.1695\n",
            "Epoch 70: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6819, Test Linf Norm: 0.1695\n",
            "Epoch 71: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6835, Test Linf Norm: 0.1695\n",
            "Epoch 72: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6823, Test Linf Norm: 0.1695\n",
            "Epoch 73: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6817, Test Linf Norm: 0.1695\n",
            "Epoch 74: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6807, Test Linf Norm: 0.1695\n",
            "Epoch 75: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6839, Test Linf Norm: 0.1695\n",
            "Epoch 76: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6812, Test Linf Norm: 0.1695\n",
            "Epoch 77: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6799, Test Linf Norm: 0.1695\n",
            "Epoch 78: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6843, Test Linf Norm: 0.1695\n",
            "Epoch 79: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6813, Test Linf Norm: 0.1695\n",
            "Epoch 80: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6821, Test Linf Norm: 0.1695\n",
            "Epoch 81: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6840, Test Linf Norm: 0.1695\n",
            "Epoch 82: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6811, Test Linf Norm: 0.1695\n",
            "Epoch 83: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6828, Test Linf Norm: 0.1695\n",
            "Epoch 84: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6827, Test Linf Norm: 0.1695\n",
            "Epoch 85: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6815, Test Linf Norm: 0.1695\n",
            "Epoch 86: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6816, Test Linf Norm: 0.1695\n",
            "Epoch 87: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6816, Test Linf Norm: 0.1695\n",
            "Epoch 88: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6832, Test Linf Norm: 0.1695\n",
            "Epoch 89: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6757, Test Linf Norm: 0.1695\n",
            "Epoch 90: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6826, Test Linf Norm: 0.1695\n",
            "Epoch 91: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6846, Test Linf Norm: 0.1695\n",
            "Epoch 92: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6800, Test Linf Norm: 0.1695\n",
            "Epoch 93: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6846, Test Linf Norm: 0.1695\n",
            "Epoch 94: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6763, Test Linf Norm: 0.1695\n",
            "Epoch 95: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6830, Test Linf Norm: 0.1695\n",
            "Epoch 96: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6846, Test Linf Norm: 0.1695\n",
            "Epoch 97: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6846, Test Linf Norm: 0.1695\n",
            "Epoch 98: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6819, Test Linf Norm: 0.1695\n",
            "Epoch 99: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6796, Test Linf Norm: 0.1695\n",
            "Epoch 100: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6803, Test Linf Norm: 0.1695\n",
            "Epoch 101: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6813, Test Linf Norm: 0.1695\n",
            "Epoch 102: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6840, Test Linf Norm: 0.1695\n",
            "Epoch 103: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6835, Test Linf Norm: 0.1695\n",
            "Epoch 104: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6858, Test Linf Norm: 0.1695\n",
            "Epoch 105: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6851, Test Linf Norm: 0.1695\n",
            "Epoch 106: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6854, Test Linf Norm: 0.1695\n",
            "Epoch 107: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6839, Test Linf Norm: 0.1695\n",
            "Epoch 108: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6801, Test Linf Norm: 0.1695\n",
            "Epoch 109: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6840, Test Linf Norm: 0.1695\n",
            "Epoch 110: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6807, Test Linf Norm: 0.1695\n",
            "Epoch 111: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6815, Test Linf Norm: 0.1695\n",
            "Epoch 112: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6826, Test Linf Norm: 0.1695\n",
            "Epoch 113: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6834, Test Linf Norm: 0.1695\n",
            "Epoch 114: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6848, Test Linf Norm: 0.1695\n",
            "Epoch 115: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6824, Test Linf Norm: 0.1695\n",
            "Epoch 116: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6854, Test Linf Norm: 0.1695\n",
            "Epoch 117: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6826, Test Linf Norm: 0.1695\n",
            "Epoch 118: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6844, Test Linf Norm: 0.1695\n",
            "Epoch 119: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6844, Test Linf Norm: 0.1695\n",
            "Epoch 120: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6860, Test Linf Norm: 0.1695\n",
            "Epoch 121: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6803, Test Linf Norm: 0.1695\n",
            "Epoch 122: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6850, Test Linf Norm: 0.1695\n",
            "Epoch 123: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6809, Test Linf Norm: 0.1695\n",
            "Epoch 124: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6851, Test Linf Norm: 0.1695\n",
            "Epoch 125: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6851, Test Linf Norm: 0.1695\n",
            "Epoch 126: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6835, Test Linf Norm: 0.1695\n",
            "Epoch 127: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6857, Test Linf Norm: 0.1695\n",
            "Epoch 128: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6853, Test Linf Norm: 0.1695\n",
            "Epoch 129: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6835, Test Linf Norm: 0.1695\n",
            "Epoch 130: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6830, Test Linf Norm: 0.1695\n",
            "Epoch 131: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6777, Test Linf Norm: 0.1695\n",
            "Epoch 132: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6836, Test Linf Norm: 0.1695\n",
            "Epoch 133: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6828, Test Linf Norm: 0.1695\n",
            "Epoch 134: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6827, Test Linf Norm: 0.1695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 11:53:27,796]\u001b[0m Trial 25 finished with value: 0.007664273254945874 and parameters: {'n_layers': 6, 'n_units_0': 1881, 'n_units_1': 1840, 'n_units_2': 826, 'n_units_3': 752, 'n_units_4': 1863, 'n_units_5': 470, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'MAE', 'optimizer': 'Adagrad', 'lr': 0.00013916951237101513, 'batch_size': 48, 'n_epochs': 135, 'scheduler': 'StepLR', 'step_size': 14, 'gamma': 0.1532838785738647}. Best is trial 11 with value: 0.0071948581174016.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 135: Train Loss: 0.0064, Test Loss: 0.0066, Train L1 Norm: 0.0184, Test L1 Norm: 0.0077, Train Linf Norm: 0.6834, Test Linf Norm: 0.1695\n",
            "Epoch 1: Train Loss: 0.1771, Test Loss: 0.0379, Train L1 Norm: 0.2144, Test L1 Norm: 0.0334, Train Linf Norm: 6.6015, Test Linf Norm: 0.6966\n",
            "Epoch 2: Train Loss: 0.0581, Test Loss: 0.0399, Train L1 Norm: 0.0888, Test L1 Norm: 0.0246, Train Linf Norm: 3.0243, Test Linf Norm: 0.4060\n",
            "Epoch 3: Train Loss: 0.0432, Test Loss: 0.0492, Train L1 Norm: 0.0838, Test L1 Norm: 0.0284, Train Linf Norm: 3.0802, Test Linf Norm: 0.3603\n",
            "Epoch 4: Train Loss: 0.0355, Test Loss: 0.0398, Train L1 Norm: 0.0515, Test L1 Norm: 0.0255, Train Linf Norm: 1.6861, Test Linf Norm: 0.4534\n",
            "Epoch 5: Train Loss: 0.0309, Test Loss: 0.0454, Train L1 Norm: 0.0424, Test L1 Norm: 0.0342, Train Linf Norm: 1.3522, Test Linf Norm: 0.6699\n",
            "Epoch 6: Train Loss: 0.0274, Test Loss: 0.0564, Train L1 Norm: 0.0430, Test L1 Norm: 0.0304, Train Linf Norm: 1.4459, Test Linf Norm: 0.4793\n",
            "Epoch 7: Train Loss: 0.0253, Test Loss: 0.0428, Train L1 Norm: 0.0460, Test L1 Norm: 0.0257, Train Linf Norm: 1.6400, Test Linf Norm: 0.4431\n",
            "Epoch 8: Train Loss: 0.0231, Test Loss: 0.0116, Train L1 Norm: 0.0364, Test L1 Norm: 0.0141, Train Linf Norm: 1.2205, Test Linf Norm: 0.3240\n",
            "Epoch 9: Train Loss: 0.0215, Test Loss: 0.0162, Train L1 Norm: 0.0337, Test L1 Norm: 0.0123, Train Linf Norm: 1.1307, Test Linf Norm: 0.2231\n",
            "Epoch 10: Train Loss: 0.0206, Test Loss: 0.0428, Train L1 Norm: 0.0349, Test L1 Norm: 0.0262, Train Linf Norm: 1.2115, Test Linf Norm: 0.4427\n",
            "Epoch 11: Train Loss: 0.0194, Test Loss: 0.0296, Train L1 Norm: 0.0332, Test L1 Norm: 0.0164, Train Linf Norm: 1.1497, Test Linf Norm: 0.2484\n",
            "Epoch 12: Train Loss: 0.0185, Test Loss: 0.0142, Train L1 Norm: 0.0333, Test L1 Norm: 0.0127, Train Linf Norm: 1.1670, Test Linf Norm: 0.2295\n",
            "Epoch 13: Train Loss: 0.0179, Test Loss: 0.0100, Train L1 Norm: 0.0284, Test L1 Norm: 0.0107, Train Linf Norm: 0.9514, Test Linf Norm: 0.2154\n",
            "Epoch 14: Train Loss: 0.0173, Test Loss: 0.0109, Train L1 Norm: 0.0272, Test L1 Norm: 0.0123, Train Linf Norm: 0.9112, Test Linf Norm: 0.2364\n",
            "Epoch 15: Train Loss: 0.0074, Test Loss: 0.0073, Train L1 Norm: 0.0248, Test L1 Norm: 0.0086, Train Linf Norm: 0.9525, Test Linf Norm: 0.1770\n",
            "Epoch 16: Train Loss: 0.0072, Test Loss: 0.0072, Train L1 Norm: 0.0240, Test L1 Norm: 0.0086, Train Linf Norm: 0.9152, Test Linf Norm: 0.1774\n",
            "Epoch 17: Train Loss: 0.0071, Test Loss: 0.0070, Train L1 Norm: 0.0235, Test L1 Norm: 0.0087, Train Linf Norm: 0.8988, Test Linf Norm: 0.1839\n",
            "Epoch 18: Train Loss: 0.0070, Test Loss: 0.0079, Train L1 Norm: 0.0237, Test L1 Norm: 0.0087, Train Linf Norm: 0.9054, Test Linf Norm: 0.1754\n",
            "Epoch 19: Train Loss: 0.0069, Test Loss: 0.0070, Train L1 Norm: 0.0241, Test L1 Norm: 0.0084, Train Linf Norm: 0.9296, Test Linf Norm: 0.1748\n",
            "Epoch 20: Train Loss: 0.0069, Test Loss: 0.0068, Train L1 Norm: 0.0236, Test L1 Norm: 0.0085, Train Linf Norm: 0.9048, Test Linf Norm: 0.1773\n",
            "Epoch 21: Train Loss: 0.0068, Test Loss: 0.0068, Train L1 Norm: 0.0241, Test L1 Norm: 0.0085, Train Linf Norm: 0.9309, Test Linf Norm: 0.1805\n",
            "Epoch 22: Train Loss: 0.0067, Test Loss: 0.0067, Train L1 Norm: 0.0240, Test L1 Norm: 0.0084, Train Linf Norm: 0.9257, Test Linf Norm: 0.1764\n",
            "Epoch 23: Train Loss: 0.0067, Test Loss: 0.0066, Train L1 Norm: 0.0238, Test L1 Norm: 0.0083, Train Linf Norm: 0.9165, Test Linf Norm: 0.1728\n",
            "Epoch 24: Train Loss: 0.0066, Test Loss: 0.0066, Train L1 Norm: 0.0234, Test L1 Norm: 0.0083, Train Linf Norm: 0.9033, Test Linf Norm: 0.1799\n",
            "Epoch 25: Train Loss: 0.0065, Test Loss: 0.0068, Train L1 Norm: 0.0227, Test L1 Norm: 0.0083, Train Linf Norm: 0.8698, Test Linf Norm: 0.1758\n",
            "Epoch 26: Train Loss: 0.0065, Test Loss: 0.0069, Train L1 Norm: 0.0235, Test L1 Norm: 0.0084, Train Linf Norm: 0.9044, Test Linf Norm: 0.1751\n",
            "Epoch 27: Train Loss: 0.0064, Test Loss: 0.0067, Train L1 Norm: 0.0233, Test L1 Norm: 0.0082, Train Linf Norm: 0.8943, Test Linf Norm: 0.1733\n",
            "Epoch 28: Train Loss: 0.0064, Test Loss: 0.0064, Train L1 Norm: 0.0231, Test L1 Norm: 0.0081, Train Linf Norm: 0.8912, Test Linf Norm: 0.1712\n",
            "Epoch 29: Train Loss: 0.0062, Test Loss: 0.0063, Train L1 Norm: 0.0228, Test L1 Norm: 0.0081, Train Linf Norm: 0.8798, Test Linf Norm: 0.1707\n",
            "Epoch 30: Train Loss: 0.0062, Test Loss: 0.0064, Train L1 Norm: 0.0229, Test L1 Norm: 0.0081, Train Linf Norm: 0.8902, Test Linf Norm: 0.1727\n",
            "Epoch 31: Train Loss: 0.0062, Test Loss: 0.0063, Train L1 Norm: 0.0229, Test L1 Norm: 0.0081, Train Linf Norm: 0.8855, Test Linf Norm: 0.1716\n",
            "Epoch 32: Train Loss: 0.0062, Test Loss: 0.0063, Train L1 Norm: 0.0228, Test L1 Norm: 0.0080, Train Linf Norm: 0.8813, Test Linf Norm: 0.1704\n",
            "Epoch 33: Train Loss: 0.0062, Test Loss: 0.0063, Train L1 Norm: 0.0229, Test L1 Norm: 0.0080, Train Linf Norm: 0.8851, Test Linf Norm: 0.1697\n",
            "Epoch 34: Train Loss: 0.0062, Test Loss: 0.0063, Train L1 Norm: 0.0228, Test L1 Norm: 0.0081, Train Linf Norm: 0.8847, Test Linf Norm: 0.1716\n",
            "Epoch 35: Train Loss: 0.0062, Test Loss: 0.0063, Train L1 Norm: 0.0229, Test L1 Norm: 0.0081, Train Linf Norm: 0.8845, Test Linf Norm: 0.1714\n",
            "Epoch 36: Train Loss: 0.0062, Test Loss: 0.0064, Train L1 Norm: 0.0228, Test L1 Norm: 0.0081, Train Linf Norm: 0.8813, Test Linf Norm: 0.1728\n",
            "Epoch 37: Train Loss: 0.0062, Test Loss: 0.0063, Train L1 Norm: 0.0228, Test L1 Norm: 0.0081, Train Linf Norm: 0.8804, Test Linf Norm: 0.1712\n",
            "Epoch 38: Train Loss: 0.0062, Test Loss: 0.0063, Train L1 Norm: 0.0227, Test L1 Norm: 0.0080, Train Linf Norm: 0.8794, Test Linf Norm: 0.1713\n",
            "Epoch 39: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0227, Test L1 Norm: 0.0080, Train Linf Norm: 0.8756, Test Linf Norm: 0.1711\n",
            "Epoch 40: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0227, Test L1 Norm: 0.0080, Train Linf Norm: 0.8767, Test Linf Norm: 0.1707\n",
            "Epoch 41: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0227, Test L1 Norm: 0.0080, Train Linf Norm: 0.8758, Test Linf Norm: 0.1703\n",
            "Epoch 42: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0227, Test L1 Norm: 0.0080, Train Linf Norm: 0.8784, Test Linf Norm: 0.1689\n",
            "Epoch 43: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0227, Test L1 Norm: 0.0080, Train Linf Norm: 0.8785, Test Linf Norm: 0.1704\n",
            "Epoch 44: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0227, Test L1 Norm: 0.0080, Train Linf Norm: 0.8777, Test Linf Norm: 0.1704\n",
            "Epoch 45: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0227, Test L1 Norm: 0.0080, Train Linf Norm: 0.8784, Test Linf Norm: 0.1704\n",
            "Epoch 46: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0227, Test L1 Norm: 0.0080, Train Linf Norm: 0.8782, Test Linf Norm: 0.1704\n",
            "Epoch 47: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0227, Test L1 Norm: 0.0080, Train Linf Norm: 0.8742, Test Linf Norm: 0.1702\n",
            "Epoch 48: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8765, Test Linf Norm: 0.1703\n",
            "Epoch 49: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0227, Test L1 Norm: 0.0080, Train Linf Norm: 0.8731, Test Linf Norm: 0.1701\n",
            "Epoch 50: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0227, Test L1 Norm: 0.0080, Train Linf Norm: 0.8778, Test Linf Norm: 0.1704\n",
            "Epoch 51: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0227, Test L1 Norm: 0.0080, Train Linf Norm: 0.8795, Test Linf Norm: 0.1705\n",
            "Epoch 52: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8758, Test Linf Norm: 0.1702\n",
            "Epoch 53: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8767, Test Linf Norm: 0.1704\n",
            "Epoch 54: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8741, Test Linf Norm: 0.1705\n",
            "Epoch 55: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8774, Test Linf Norm: 0.1705\n",
            "Epoch 56: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8796, Test Linf Norm: 0.1704\n",
            "Epoch 57: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8761, Test Linf Norm: 0.1704\n",
            "Epoch 58: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8733, Test Linf Norm: 0.1703\n",
            "Epoch 59: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8754, Test Linf Norm: 0.1703\n",
            "Epoch 60: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8756, Test Linf Norm: 0.1703\n",
            "Epoch 61: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8755, Test Linf Norm: 0.1703\n",
            "Epoch 62: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8752, Test Linf Norm: 0.1703\n",
            "Epoch 63: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8770, Test Linf Norm: 0.1703\n",
            "Epoch 64: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8752, Test Linf Norm: 0.1703\n",
            "Epoch 65: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8773, Test Linf Norm: 0.1703\n",
            "Epoch 66: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8755, Test Linf Norm: 0.1703\n",
            "Epoch 67: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8730, Test Linf Norm: 0.1703\n",
            "Epoch 68: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8771, Test Linf Norm: 0.1703\n",
            "Epoch 69: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8747, Test Linf Norm: 0.1704\n",
            "Epoch 70: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8766, Test Linf Norm: 0.1703\n",
            "Epoch 71: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8763, Test Linf Norm: 0.1703\n",
            "Epoch 72: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8783, Test Linf Norm: 0.1703\n",
            "Epoch 73: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8757, Test Linf Norm: 0.1703\n",
            "Epoch 74: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8736, Test Linf Norm: 0.1703\n",
            "Epoch 75: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8736, Test Linf Norm: 0.1703\n",
            "Epoch 76: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8743, Test Linf Norm: 0.1703\n",
            "Epoch 77: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8743, Test Linf Norm: 0.1703\n",
            "Epoch 78: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8800, Test Linf Norm: 0.1703\n",
            "Epoch 79: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8774, Test Linf Norm: 0.1703\n",
            "Epoch 80: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8764, Test Linf Norm: 0.1703\n",
            "Epoch 81: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8764, Test Linf Norm: 0.1703\n",
            "Epoch 82: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8746, Test Linf Norm: 0.1703\n",
            "Epoch 83: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8764, Test Linf Norm: 0.1703\n",
            "Epoch 84: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8745, Test Linf Norm: 0.1703\n",
            "Epoch 85: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8778, Test Linf Norm: 0.1703\n",
            "Epoch 86: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8731, Test Linf Norm: 0.1703\n",
            "Epoch 87: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8800, Test Linf Norm: 0.1703\n",
            "Epoch 88: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8780, Test Linf Norm: 0.1703\n",
            "Epoch 89: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8780, Test Linf Norm: 0.1703\n",
            "Epoch 90: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8737, Test Linf Norm: 0.1703\n",
            "Epoch 91: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8760, Test Linf Norm: 0.1703\n",
            "Epoch 92: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8739, Test Linf Norm: 0.1703\n",
            "Epoch 93: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8770, Test Linf Norm: 0.1703\n",
            "Epoch 94: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8775, Test Linf Norm: 0.1703\n",
            "Epoch 95: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8774, Test Linf Norm: 0.1703\n",
            "Epoch 96: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8758, Test Linf Norm: 0.1703\n",
            "Epoch 97: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8694, Test Linf Norm: 0.1703\n",
            "Epoch 98: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8761, Test Linf Norm: 0.1703\n",
            "Epoch 99: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8746, Test Linf Norm: 0.1703\n",
            "Epoch 100: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8688, Test Linf Norm: 0.1703\n",
            "Epoch 101: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8770, Test Linf Norm: 0.1703\n",
            "Epoch 102: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8801, Test Linf Norm: 0.1703\n",
            "Epoch 103: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8736, Test Linf Norm: 0.1703\n",
            "Epoch 104: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8783, Test Linf Norm: 0.1703\n",
            "Epoch 105: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8775, Test Linf Norm: 0.1703\n",
            "Epoch 106: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8721, Test Linf Norm: 0.1703\n",
            "Epoch 107: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8766, Test Linf Norm: 0.1703\n",
            "Epoch 108: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8776, Test Linf Norm: 0.1703\n",
            "Epoch 109: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8752, Test Linf Norm: 0.1703\n",
            "Epoch 110: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8741, Test Linf Norm: 0.1703\n",
            "Epoch 111: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8794, Test Linf Norm: 0.1703\n",
            "Epoch 112: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8723, Test Linf Norm: 0.1703\n",
            "Epoch 113: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8767, Test Linf Norm: 0.1703\n",
            "Epoch 114: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8768, Test Linf Norm: 0.1703\n",
            "Epoch 115: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8760, Test Linf Norm: 0.1703\n",
            "Epoch 116: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8781, Test Linf Norm: 0.1703\n",
            "Epoch 117: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8794, Test Linf Norm: 0.1703\n",
            "Epoch 118: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8775, Test Linf Norm: 0.1703\n",
            "Epoch 119: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8734, Test Linf Norm: 0.1703\n",
            "Epoch 120: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8750, Test Linf Norm: 0.1703\n",
            "Epoch 121: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8763, Test Linf Norm: 0.1703\n",
            "Epoch 122: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8769, Test Linf Norm: 0.1703\n",
            "Epoch 123: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8781, Test Linf Norm: 0.1703\n",
            "Epoch 124: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8773, Test Linf Norm: 0.1703\n",
            "Epoch 125: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8775, Test Linf Norm: 0.1703\n",
            "Epoch 126: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8754, Test Linf Norm: 0.1703\n",
            "Epoch 127: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8747, Test Linf Norm: 0.1703\n",
            "Epoch 128: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8740, Test Linf Norm: 0.1703\n",
            "Epoch 129: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8748, Test Linf Norm: 0.1703\n",
            "Epoch 130: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8751, Test Linf Norm: 0.1703\n",
            "Epoch 131: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8788, Test Linf Norm: 0.1703\n",
            "Epoch 132: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8772, Test Linf Norm: 0.1703\n",
            "Epoch 133: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8761, Test Linf Norm: 0.1703\n",
            "Epoch 134: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8751, Test Linf Norm: 0.1703\n",
            "Epoch 135: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8742, Test Linf Norm: 0.1703\n",
            "Epoch 136: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8749, Test Linf Norm: 0.1703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 12:11:24,018]\u001b[0m Trial 26 finished with value: 0.008006741362065077 and parameters: {'n_layers': 5, 'n_units_0': 1508, 'n_units_1': 1868, 'n_units_2': 705, 'n_units_3': 710, 'n_units_4': 1865, 'hidden_activation': 'LeakyReLU', 'output_activation': 'ReLU', 'loss': 'MAE', 'optimizer': 'Adagrad', 'lr': 0.00017442414362699935, 'batch_size': 48, 'n_epochs': 137, 'scheduler': 'StepLR', 'step_size': 14, 'gamma': 0.1432885286720946}. Best is trial 11 with value: 0.0071948581174016.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 137: Train Loss: 0.0061, Test Loss: 0.0063, Train L1 Norm: 0.0226, Test L1 Norm: 0.0080, Train Linf Norm: 0.8765, Test Linf Norm: 0.1703\n",
            "Epoch 1: Train Loss: 0.2080, Test Loss: 0.0543, Train L1 Norm: 0.3544, Test L1 Norm: 0.0807, Train Linf Norm: 12.2581, Test Linf Norm: 2.1700\n",
            "Epoch 2: Train Loss: 0.0510, Test Loss: 0.0425, Train L1 Norm: 0.2035, Test L1 Norm: 0.0396, Train Linf Norm: 8.3065, Test Linf Norm: 0.9740\n",
            "Epoch 3: Train Loss: 0.0415, Test Loss: 0.1008, Train L1 Norm: 0.3565, Test L1 Norm: 0.0813, Train Linf Norm: 15.9477, Test Linf Norm: 2.0398\n",
            "Epoch 4: Train Loss: 0.0465, Test Loss: 0.0339, Train L1 Norm: 0.2180, Test L1 Norm: 0.0320, Train Linf Norm: 9.3003, Test Linf Norm: 0.7807\n",
            "Epoch 5: Train Loss: 0.0391, Test Loss: 0.0164, Train L1 Norm: 0.1193, Test L1 Norm: 0.0356, Train Linf Norm: 4.7272, Test Linf Norm: 1.1046\n",
            "Epoch 6: Train Loss: 0.0411, Test Loss: 0.0224, Train L1 Norm: 0.2213, Test L1 Norm: 0.0346, Train Linf Norm: 9.5803, Test Linf Norm: 0.8866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 12:12:22,526]\u001b[0m Trial 27 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train Loss: 0.0343, Test Loss: 0.0376, Train L1 Norm: 0.1156, Test L1 Norm: 0.0525, Train Linf Norm: 4.7170, Test Linf Norm: 1.5848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 12:12:27,886]\u001b[0m Trial 28 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 1.3807, Test Loss: 0.0451, Train L1 Norm: 0.5068, Test L1 Norm: 0.1393, Train Linf Norm: 16.8861, Test Linf Norm: 3.5806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 12:12:35,110]\u001b[0m Trial 29 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.3227, Test Loss: 0.0759, Train L1 Norm: 1.0388, Test L1 Norm: 0.2158, Train Linf Norm: 34.3186, Test Linf Norm: 1.3896\n",
            "Epoch 1: Train Loss: 0.1700, Test Loss: 0.0843, Train L1 Norm: 0.2280, Test L1 Norm: 0.0460, Train Linf Norm: 7.2161, Test Linf Norm: 0.5051\n",
            "Epoch 2: Train Loss: 0.0567, Test Loss: 0.0272, Train L1 Norm: 0.0792, Test L1 Norm: 0.0296, Train Linf Norm: 2.4898, Test Linf Norm: 0.6184\n",
            "Epoch 3: Train Loss: 0.0420, Test Loss: 0.0335, Train L1 Norm: 0.0584, Test L1 Norm: 0.0283, Train Linf Norm: 1.8373, Test Linf Norm: 0.5644\n",
            "Epoch 4: Train Loss: 0.0342, Test Loss: 0.0170, Train L1 Norm: 0.0587, Test L1 Norm: 0.0172, Train Linf Norm: 1.9946, Test Linf Norm: 0.3461\n",
            "Epoch 5: Train Loss: 0.0298, Test Loss: 0.0243, Train L1 Norm: 0.0447, Test L1 Norm: 0.0181, Train Linf Norm: 1.4421, Test Linf Norm: 0.3002\n",
            "Epoch 6: Train Loss: 0.0269, Test Loss: 0.0185, Train L1 Norm: 0.0472, Test L1 Norm: 0.0160, Train Linf Norm: 1.6138, Test Linf Norm: 0.2786\n",
            "Epoch 7: Train Loss: 0.0239, Test Loss: 0.0202, Train L1 Norm: 0.0384, Test L1 Norm: 0.0155, Train Linf Norm: 1.2727, Test Linf Norm: 0.2521\n",
            "Epoch 8: Train Loss: 0.0225, Test Loss: 0.0187, Train L1 Norm: 0.0350, Test L1 Norm: 0.0163, Train Linf Norm: 1.1548, Test Linf Norm: 0.3361\n",
            "Epoch 9: Train Loss: 0.0209, Test Loss: 0.0119, Train L1 Norm: 0.0338, Test L1 Norm: 0.0129, Train Linf Norm: 1.1222, Test Linf Norm: 0.2653\n",
            "Epoch 10: Train Loss: 0.0197, Test Loss: 0.0102, Train L1 Norm: 0.0351, Test L1 Norm: 0.0121, Train Linf Norm: 1.2150, Test Linf Norm: 0.2581\n",
            "Epoch 11: Train Loss: 0.0187, Test Loss: 0.0123, Train L1 Norm: 0.0323, Test L1 Norm: 0.0115, Train Linf Norm: 1.1018, Test Linf Norm: 0.2280\n",
            "Epoch 12: Train Loss: 0.0179, Test Loss: 0.0206, Train L1 Norm: 0.0300, Test L1 Norm: 0.0154, Train Linf Norm: 1.0086, Test Linf Norm: 0.2322\n",
            "Epoch 13: Train Loss: 0.0170, Test Loss: 0.0106, Train L1 Norm: 0.0352, Test L1 Norm: 0.0117, Train Linf Norm: 1.2786, Test Linf Norm: 0.2098\n",
            "Epoch 14: Train Loss: 0.0164, Test Loss: 0.0135, Train L1 Norm: 0.0271, Test L1 Norm: 0.0122, Train Linf Norm: 0.9018, Test Linf Norm: 0.2523\n",
            "Epoch 15: Train Loss: 0.0157, Test Loss: 0.0109, Train L1 Norm: 0.0301, Test L1 Norm: 0.0107, Train Linf Norm: 1.0604, Test Linf Norm: 0.2229\n",
            "Epoch 16: Train Loss: 0.0152, Test Loss: 0.0086, Train L1 Norm: 0.0273, Test L1 Norm: 0.0097, Train Linf Norm: 0.9435, Test Linf Norm: 0.1890\n",
            "Epoch 17: Train Loss: 0.0147, Test Loss: 0.0126, Train L1 Norm: 0.0275, Test L1 Norm: 0.0107, Train Linf Norm: 0.9575, Test Linf Norm: 0.1862\n",
            "Epoch 18: Train Loss: 0.0142, Test Loss: 0.0148, Train L1 Norm: 0.0281, Test L1 Norm: 0.0106, Train Linf Norm: 0.9982, Test Linf Norm: 0.1863\n",
            "Epoch 19: Train Loss: 0.0140, Test Loss: 0.0112, Train L1 Norm: 0.0263, Test L1 Norm: 0.0101, Train Linf Norm: 0.9242, Test Linf Norm: 0.1804\n",
            "Epoch 20: Train Loss: 0.0134, Test Loss: 0.0148, Train L1 Norm: 0.0252, Test L1 Norm: 0.0121, Train Linf Norm: 0.8808, Test Linf Norm: 0.2277\n",
            "Epoch 21: Train Loss: 0.0133, Test Loss: 0.0082, Train L1 Norm: 0.0260, Test L1 Norm: 0.0091, Train Linf Norm: 0.9220, Test Linf Norm: 0.1821\n",
            "Epoch 22: Train Loss: 0.0131, Test Loss: 0.0194, Train L1 Norm: 0.0236, Test L1 Norm: 0.0150, Train Linf Norm: 0.8146, Test Linf Norm: 0.2415\n",
            "Epoch 23: Train Loss: 0.0126, Test Loss: 0.0262, Train L1 Norm: 0.0228, Test L1 Norm: 0.0167, Train Linf Norm: 0.7848, Test Linf Norm: 0.2967\n",
            "Epoch 24: Train Loss: 0.0123, Test Loss: 0.0180, Train L1 Norm: 0.0235, Test L1 Norm: 0.0128, Train Linf Norm: 0.8215, Test Linf Norm: 0.2062\n",
            "Epoch 25: Train Loss: 0.0122, Test Loss: 0.0162, Train L1 Norm: 0.0245, Test L1 Norm: 0.0124, Train Linf Norm: 0.8783, Test Linf Norm: 0.2286\n",
            "Epoch 26: Train Loss: 0.0121, Test Loss: 0.0155, Train L1 Norm: 0.0225, Test L1 Norm: 0.0123, Train Linf Norm: 0.7840, Test Linf Norm: 0.2432\n",
            "Epoch 27: Train Loss: 0.0117, Test Loss: 0.0080, Train L1 Norm: 0.0224, Test L1 Norm: 0.0086, Train Linf Norm: 0.7896, Test Linf Norm: 0.1754\n",
            "Epoch 28: Train Loss: 0.0115, Test Loss: 0.0070, Train L1 Norm: 0.0205, Test L1 Norm: 0.0077, Train Linf Norm: 0.6998, Test Linf Norm: 0.1553\n",
            "Epoch 29: Train Loss: 0.0113, Test Loss: 0.0101, Train L1 Norm: 0.0188, Test L1 Norm: 0.0094, Train Linf Norm: 0.6245, Test Linf Norm: 0.1769\n",
            "Epoch 30: Train Loss: 0.0112, Test Loss: 0.0188, Train L1 Norm: 0.0211, Test L1 Norm: 0.0134, Train Linf Norm: 0.7339, Test Linf Norm: 0.2556\n",
            "Epoch 31: Train Loss: 0.0110, Test Loss: 0.0110, Train L1 Norm: 0.0221, Test L1 Norm: 0.0085, Train Linf Norm: 0.7858, Test Linf Norm: 0.1397\n",
            "Epoch 32: Train Loss: 0.0107, Test Loss: 0.0134, Train L1 Norm: 0.0206, Test L1 Norm: 0.0101, Train Linf Norm: 0.7223, Test Linf Norm: 0.1701\n",
            "Epoch 33: Train Loss: 0.0106, Test Loss: 0.0103, Train L1 Norm: 0.0192, Test L1 Norm: 0.0088, Train Linf Norm: 0.6596, Test Linf Norm: 0.1504\n",
            "Epoch 34: Train Loss: 0.0105, Test Loss: 0.0121, Train L1 Norm: 0.0173, Test L1 Norm: 0.0117, Train Linf Norm: 0.5705, Test Linf Norm: 0.2346\n",
            "Epoch 35: Train Loss: 0.0103, Test Loss: 0.0068, Train L1 Norm: 0.0206, Test L1 Norm: 0.0072, Train Linf Norm: 0.7318, Test Linf Norm: 0.1412\n",
            "Epoch 36: Train Loss: 0.0100, Test Loss: 0.0100, Train L1 Norm: 0.0186, Test L1 Norm: 0.0087, Train Linf Norm: 0.6436, Test Linf Norm: 0.1732\n",
            "Epoch 37: Train Loss: 0.0101, Test Loss: 0.0077, Train L1 Norm: 0.0196, Test L1 Norm: 0.0078, Train Linf Norm: 0.6877, Test Linf Norm: 0.1498\n",
            "Epoch 38: Train Loss: 0.0100, Test Loss: 0.0072, Train L1 Norm: 0.0185, Test L1 Norm: 0.0081, Train Linf Norm: 0.6393, Test Linf Norm: 0.1580\n",
            "Epoch 39: Train Loss: 0.0098, Test Loss: 0.0080, Train L1 Norm: 0.0181, Test L1 Norm: 0.0079, Train Linf Norm: 0.6241, Test Linf Norm: 0.1631\n",
            "Epoch 40: Train Loss: 0.0097, Test Loss: 0.0086, Train L1 Norm: 0.0172, Test L1 Norm: 0.0073, Train Linf Norm: 0.5794, Test Linf Norm: 0.1334\n",
            "Epoch 41: Train Loss: 0.0095, Test Loss: 0.0160, Train L1 Norm: 0.0172, Test L1 Norm: 0.0095, Train Linf Norm: 0.5874, Test Linf Norm: 0.1542\n",
            "Epoch 42: Train Loss: 0.0095, Test Loss: 0.0127, Train L1 Norm: 0.0178, Test L1 Norm: 0.0091, Train Linf Norm: 0.6174, Test Linf Norm: 0.1630\n",
            "Epoch 43: Train Loss: 0.0094, Test Loss: 0.0181, Train L1 Norm: 0.0165, Test L1 Norm: 0.0108, Train Linf Norm: 0.5518, Test Linf Norm: 0.1737\n",
            "Epoch 44: Train Loss: 0.0093, Test Loss: 0.0095, Train L1 Norm: 0.0163, Test L1 Norm: 0.0090, Train Linf Norm: 0.5464, Test Linf Norm: 0.1925\n",
            "Epoch 45: Train Loss: 0.0093, Test Loss: 0.0075, Train L1 Norm: 0.0169, Test L1 Norm: 0.0075, Train Linf Norm: 0.5821, Test Linf Norm: 0.1414\n",
            "Epoch 46: Train Loss: 0.0090, Test Loss: 0.0062, Train L1 Norm: 0.0169, Test L1 Norm: 0.0064, Train Linf Norm: 0.5840, Test Linf Norm: 0.1239\n",
            "Epoch 47: Train Loss: 0.0092, Test Loss: 0.0080, Train L1 Norm: 0.0164, Test L1 Norm: 0.0080, Train Linf Norm: 0.5611, Test Linf Norm: 0.1667\n",
            "Epoch 48: Train Loss: 0.0089, Test Loss: 0.0094, Train L1 Norm: 0.0158, Test L1 Norm: 0.0086, Train Linf Norm: 0.5332, Test Linf Norm: 0.1738\n",
            "Epoch 49: Train Loss: 0.0088, Test Loss: 0.0071, Train L1 Norm: 0.0153, Test L1 Norm: 0.0075, Train Linf Norm: 0.5139, Test Linf Norm: 0.1520\n",
            "Epoch 50: Train Loss: 0.0088, Test Loss: 0.0066, Train L1 Norm: 0.0150, Test L1 Norm: 0.0066, Train Linf Norm: 0.4993, Test Linf Norm: 0.1302\n",
            "Epoch 51: Train Loss: 0.0088, Test Loss: 0.0082, Train L1 Norm: 0.0149, Test L1 Norm: 0.0068, Train Linf Norm: 0.4947, Test Linf Norm: 0.1206\n",
            "Epoch 52: Train Loss: 0.0086, Test Loss: 0.0259, Train L1 Norm: 0.0154, Test L1 Norm: 0.0153, Train Linf Norm: 0.5210, Test Linf Norm: 0.2499\n",
            "Epoch 53: Train Loss: 0.0084, Test Loss: 0.0067, Train L1 Norm: 0.0149, Test L1 Norm: 0.0066, Train Linf Norm: 0.5039, Test Linf Norm: 0.1253\n",
            "Epoch 54: Train Loss: 0.0084, Test Loss: 0.0194, Train L1 Norm: 0.0148, Test L1 Norm: 0.0114, Train Linf Norm: 0.4977, Test Linf Norm: 0.1675\n",
            "Epoch 55: Train Loss: 0.0085, Test Loss: 0.0075, Train L1 Norm: 0.0141, Test L1 Norm: 0.0072, Train Linf Norm: 0.4632, Test Linf Norm: 0.1409\n",
            "Epoch 56: Train Loss: 0.0084, Test Loss: 0.0132, Train L1 Norm: 0.0147, Test L1 Norm: 0.0087, Train Linf Norm: 0.4909, Test Linf Norm: 0.1414\n",
            "Epoch 57: Train Loss: 0.0082, Test Loss: 0.0099, Train L1 Norm: 0.0150, Test L1 Norm: 0.0081, Train Linf Norm: 0.5096, Test Linf Norm: 0.1578\n",
            "Epoch 58: Train Loss: 0.0056, Test Loss: 0.0084, Train L1 Norm: 0.0132, Test L1 Norm: 0.0071, Train Linf Norm: 0.4633, Test Linf Norm: 0.1276\n",
            "Epoch 59: Train Loss: 0.0055, Test Loss: 0.0068, Train L1 Norm: 0.0131, Test L1 Norm: 0.0064, Train Linf Norm: 0.4638, Test Linf Norm: 0.1205\n",
            "Epoch 60: Train Loss: 0.0055, Test Loss: 0.0058, Train L1 Norm: 0.0137, Test L1 Norm: 0.0064, Train Linf Norm: 0.4928, Test Linf Norm: 0.1327\n",
            "Epoch 61: Train Loss: 0.0055, Test Loss: 0.0055, Train L1 Norm: 0.0130, Test L1 Norm: 0.0061, Train Linf Norm: 0.4597, Test Linf Norm: 0.1276\n",
            "Epoch 62: Train Loss: 0.0055, Test Loss: 0.0055, Train L1 Norm: 0.0127, Test L1 Norm: 0.0063, Train Linf Norm: 0.4424, Test Linf Norm: 0.1328\n",
            "Epoch 63: Train Loss: 0.0055, Test Loss: 0.0055, Train L1 Norm: 0.0133, Test L1 Norm: 0.0061, Train Linf Norm: 0.4738, Test Linf Norm: 0.1241\n",
            "Epoch 64: Train Loss: 0.0054, Test Loss: 0.0055, Train L1 Norm: 0.0128, Test L1 Norm: 0.0061, Train Linf Norm: 0.4508, Test Linf Norm: 0.1255\n",
            "Epoch 65: Train Loss: 0.0054, Test Loss: 0.0053, Train L1 Norm: 0.0125, Test L1 Norm: 0.0059, Train Linf Norm: 0.4399, Test Linf Norm: 0.1191\n",
            "Epoch 66: Train Loss: 0.0054, Test Loss: 0.0056, Train L1 Norm: 0.0126, Test L1 Norm: 0.0062, Train Linf Norm: 0.4395, Test Linf Norm: 0.1259\n",
            "Epoch 67: Train Loss: 0.0054, Test Loss: 0.0054, Train L1 Norm: 0.0120, Test L1 Norm: 0.0063, Train Linf Norm: 0.4158, Test Linf Norm: 0.1284\n",
            "Epoch 68: Train Loss: 0.0054, Test Loss: 0.0054, Train L1 Norm: 0.0126, Test L1 Norm: 0.0061, Train Linf Norm: 0.4439, Test Linf Norm: 0.1265\n",
            "Epoch 69: Train Loss: 0.0054, Test Loss: 0.0055, Train L1 Norm: 0.0123, Test L1 Norm: 0.0061, Train Linf Norm: 0.4342, Test Linf Norm: 0.1236\n",
            "Epoch 70: Train Loss: 0.0053, Test Loss: 0.0057, Train L1 Norm: 0.0120, Test L1 Norm: 0.0060, Train Linf Norm: 0.4194, Test Linf Norm: 0.1157\n",
            "Epoch 71: Train Loss: 0.0053, Test Loss: 0.0052, Train L1 Norm: 0.0125, Test L1 Norm: 0.0060, Train Linf Norm: 0.4432, Test Linf Norm: 0.1241\n",
            "Epoch 72: Train Loss: 0.0053, Test Loss: 0.0054, Train L1 Norm: 0.0124, Test L1 Norm: 0.0058, Train Linf Norm: 0.4360, Test Linf Norm: 0.1129\n",
            "Epoch 73: Train Loss: 0.0053, Test Loss: 0.0064, Train L1 Norm: 0.0123, Test L1 Norm: 0.0064, Train Linf Norm: 0.4332, Test Linf Norm: 0.1320\n",
            "Epoch 74: Train Loss: 0.0053, Test Loss: 0.0059, Train L1 Norm: 0.0125, Test L1 Norm: 0.0061, Train Linf Norm: 0.4454, Test Linf Norm: 0.1232\n",
            "Epoch 75: Train Loss: 0.0052, Test Loss: 0.0144, Train L1 Norm: 0.0127, Test L1 Norm: 0.0092, Train Linf Norm: 0.4530, Test Linf Norm: 0.1649\n",
            "Epoch 76: Train Loss: 0.0052, Test Loss: 0.0052, Train L1 Norm: 0.0117, Test L1 Norm: 0.0058, Train Linf Norm: 0.4038, Test Linf Norm: 0.1162\n",
            "Epoch 77: Train Loss: 0.0052, Test Loss: 0.0052, Train L1 Norm: 0.0120, Test L1 Norm: 0.0057, Train Linf Norm: 0.4231, Test Linf Norm: 0.1137\n",
            "Epoch 78: Train Loss: 0.0052, Test Loss: 0.0051, Train L1 Norm: 0.0118, Test L1 Norm: 0.0057, Train Linf Norm: 0.4127, Test Linf Norm: 0.1147\n",
            "Epoch 79: Train Loss: 0.0052, Test Loss: 0.0057, Train L1 Norm: 0.0119, Test L1 Norm: 0.0057, Train Linf Norm: 0.4179, Test Linf Norm: 0.1067\n",
            "Epoch 80: Train Loss: 0.0052, Test Loss: 0.0058, Train L1 Norm: 0.0119, Test L1 Norm: 0.0060, Train Linf Norm: 0.4158, Test Linf Norm: 0.1225\n",
            "Epoch 81: Train Loss: 0.0052, Test Loss: 0.0052, Train L1 Norm: 0.0118, Test L1 Norm: 0.0057, Train Linf Norm: 0.4114, Test Linf Norm: 0.1123\n",
            "Epoch 82: Train Loss: 0.0051, Test Loss: 0.0083, Train L1 Norm: 0.0119, Test L1 Norm: 0.0072, Train Linf Norm: 0.4182, Test Linf Norm: 0.1422\n",
            "Epoch 83: Train Loss: 0.0051, Test Loss: 0.0059, Train L1 Norm: 0.0119, Test L1 Norm: 0.0061, Train Linf Norm: 0.4195, Test Linf Norm: 0.1250\n",
            "Epoch 84: Train Loss: 0.0051, Test Loss: 0.0052, Train L1 Norm: 0.0117, Test L1 Norm: 0.0057, Train Linf Norm: 0.4082, Test Linf Norm: 0.1138\n",
            "Epoch 85: Train Loss: 0.0051, Test Loss: 0.0052, Train L1 Norm: 0.0117, Test L1 Norm: 0.0058, Train Linf Norm: 0.4099, Test Linf Norm: 0.1173\n",
            "Epoch 86: Train Loss: 0.0051, Test Loss: 0.0051, Train L1 Norm: 0.0113, Test L1 Norm: 0.0059, Train Linf Norm: 0.3899, Test Linf Norm: 0.1258\n",
            "Epoch 87: Train Loss: 0.0051, Test Loss: 0.0069, Train L1 Norm: 0.0112, Test L1 Norm: 0.0062, Train Linf Norm: 0.3862, Test Linf Norm: 0.1144\n",
            "Epoch 88: Train Loss: 0.0051, Test Loss: 0.0052, Train L1 Norm: 0.0115, Test L1 Norm: 0.0058, Train Linf Norm: 0.4023, Test Linf Norm: 0.1170\n",
            "Epoch 89: Train Loss: 0.0050, Test Loss: 0.0055, Train L1 Norm: 0.0113, Test L1 Norm: 0.0061, Train Linf Norm: 0.3922, Test Linf Norm: 0.1275\n",
            "Epoch 90: Train Loss: 0.0046, Test Loss: 0.0049, Train L1 Norm: 0.0113, Test L1 Norm: 0.0056, Train Linf Norm: 0.4020, Test Linf Norm: 0.1120\n",
            "Epoch 91: Train Loss: 0.0046, Test Loss: 0.0052, Train L1 Norm: 0.0112, Test L1 Norm: 0.0055, Train Linf Norm: 0.3929, Test Linf Norm: 0.1045\n",
            "Epoch 92: Train Loss: 0.0046, Test Loss: 0.0050, Train L1 Norm: 0.0113, Test L1 Norm: 0.0055, Train Linf Norm: 0.3993, Test Linf Norm: 0.1100\n",
            "Epoch 93: Train Loss: 0.0046, Test Loss: 0.0049, Train L1 Norm: 0.0110, Test L1 Norm: 0.0055, Train Linf Norm: 0.3871, Test Linf Norm: 0.1103\n",
            "Epoch 94: Train Loss: 0.0046, Test Loss: 0.0050, Train L1 Norm: 0.0110, Test L1 Norm: 0.0055, Train Linf Norm: 0.3841, Test Linf Norm: 0.1102\n",
            "Epoch 95: Train Loss: 0.0046, Test Loss: 0.0049, Train L1 Norm: 0.0109, Test L1 Norm: 0.0055, Train Linf Norm: 0.3783, Test Linf Norm: 0.1117\n",
            "Epoch 96: Train Loss: 0.0046, Test Loss: 0.0050, Train L1 Norm: 0.0107, Test L1 Norm: 0.0056, Train Linf Norm: 0.3727, Test Linf Norm: 0.1129\n",
            "Epoch 97: Train Loss: 0.0046, Test Loss: 0.0050, Train L1 Norm: 0.0109, Test L1 Norm: 0.0055, Train Linf Norm: 0.3839, Test Linf Norm: 0.1108\n",
            "Epoch 98: Train Loss: 0.0046, Test Loss: 0.0050, Train L1 Norm: 0.0107, Test L1 Norm: 0.0056, Train Linf Norm: 0.3712, Test Linf Norm: 0.1123\n",
            "Epoch 99: Train Loss: 0.0046, Test Loss: 0.0049, Train L1 Norm: 0.0109, Test L1 Norm: 0.0054, Train Linf Norm: 0.3804, Test Linf Norm: 0.1066\n",
            "Epoch 100: Train Loss: 0.0045, Test Loss: 0.0050, Train L1 Norm: 0.0109, Test L1 Norm: 0.0056, Train Linf Norm: 0.3801, Test Linf Norm: 0.1136\n",
            "Epoch 101: Train Loss: 0.0045, Test Loss: 0.0054, Train L1 Norm: 0.0107, Test L1 Norm: 0.0056, Train Linf Norm: 0.3700, Test Linf Norm: 0.1131\n",
            "Epoch 102: Train Loss: 0.0045, Test Loss: 0.0048, Train L1 Norm: 0.0108, Test L1 Norm: 0.0055, Train Linf Norm: 0.3754, Test Linf Norm: 0.1112\n",
            "Epoch 103: Train Loss: 0.0045, Test Loss: 0.0049, Train L1 Norm: 0.0107, Test L1 Norm: 0.0054, Train Linf Norm: 0.3722, Test Linf Norm: 0.1047\n",
            "Epoch 104: Train Loss: 0.0045, Test Loss: 0.0050, Train L1 Norm: 0.0107, Test L1 Norm: 0.0055, Train Linf Norm: 0.3699, Test Linf Norm: 0.1092\n",
            "Epoch 105: Train Loss: 0.0045, Test Loss: 0.0048, Train L1 Norm: 0.0105, Test L1 Norm: 0.0054, Train Linf Norm: 0.3544, Test Linf Norm: 0.1094\n",
            "Epoch 106: Train Loss: 0.0045, Test Loss: 0.0049, Train L1 Norm: 0.0105, Test L1 Norm: 0.0056, Train Linf Norm: 0.3624, Test Linf Norm: 0.1165\n",
            "Epoch 107: Train Loss: 0.0045, Test Loss: 0.0050, Train L1 Norm: 0.0105, Test L1 Norm: 0.0056, Train Linf Norm: 0.3615, Test Linf Norm: 0.1109\n",
            "Epoch 108: Train Loss: 0.0045, Test Loss: 0.0048, Train L1 Norm: 0.0108, Test L1 Norm: 0.0055, Train Linf Norm: 0.3799, Test Linf Norm: 0.1130\n",
            "Epoch 109: Train Loss: 0.0045, Test Loss: 0.0048, Train L1 Norm: 0.0106, Test L1 Norm: 0.0054, Train Linf Norm: 0.3703, Test Linf Norm: 0.1095\n",
            "Epoch 110: Train Loss: 0.0045, Test Loss: 0.0049, Train L1 Norm: 0.0105, Test L1 Norm: 0.0055, Train Linf Norm: 0.3665, Test Linf Norm: 0.1107\n",
            "Epoch 111: Train Loss: 0.0045, Test Loss: 0.0048, Train L1 Norm: 0.0105, Test L1 Norm: 0.0055, Train Linf Norm: 0.3663, Test Linf Norm: 0.1106\n",
            "Epoch 112: Train Loss: 0.0045, Test Loss: 0.0048, Train L1 Norm: 0.0103, Test L1 Norm: 0.0054, Train Linf Norm: 0.3548, Test Linf Norm: 0.1101\n",
            "Epoch 113: Train Loss: 0.0045, Test Loss: 0.0048, Train L1 Norm: 0.0102, Test L1 Norm: 0.0055, Train Linf Norm: 0.3498, Test Linf Norm: 0.1117\n",
            "Epoch 114: Train Loss: 0.0045, Test Loss: 0.0048, Train L1 Norm: 0.0105, Test L1 Norm: 0.0054, Train Linf Norm: 0.3637, Test Linf Norm: 0.1082\n",
            "Epoch 115: Train Loss: 0.0044, Test Loss: 0.0048, Train L1 Norm: 0.0103, Test L1 Norm: 0.0057, Train Linf Norm: 0.3564, Test Linf Norm: 0.1184\n",
            "Epoch 116: Train Loss: 0.0044, Test Loss: 0.0048, Train L1 Norm: 0.0103, Test L1 Norm: 0.0054, Train Linf Norm: 0.3577, Test Linf Norm: 0.1074\n",
            "Epoch 117: Train Loss: 0.0044, Test Loss: 0.0048, Train L1 Norm: 0.0103, Test L1 Norm: 0.0053, Train Linf Norm: 0.3551, Test Linf Norm: 0.1043\n",
            "Epoch 118: Train Loss: 0.0044, Test Loss: 0.0048, Train L1 Norm: 0.0104, Test L1 Norm: 0.0054, Train Linf Norm: 0.3617, Test Linf Norm: 0.1063\n",
            "Epoch 119: Train Loss: 0.0044, Test Loss: 0.0047, Train L1 Norm: 0.0104, Test L1 Norm: 0.0054, Train Linf Norm: 0.3617, Test Linf Norm: 0.1102\n",
            "Epoch 120: Train Loss: 0.0044, Test Loss: 0.0048, Train L1 Norm: 0.0103, Test L1 Norm: 0.0054, Train Linf Norm: 0.3568, Test Linf Norm: 0.1089\n",
            "Epoch 121: Train Loss: 0.0044, Test Loss: 0.0048, Train L1 Norm: 0.0104, Test L1 Norm: 0.0055, Train Linf Norm: 0.3620, Test Linf Norm: 0.1110\n",
            "Epoch 122: Train Loss: 0.0044, Test Loss: 0.0047, Train L1 Norm: 0.0102, Test L1 Norm: 0.0054, Train Linf Norm: 0.3519, Test Linf Norm: 0.1112\n",
            "Epoch 123: Train Loss: 0.0044, Test Loss: 0.0050, Train L1 Norm: 0.0105, Test L1 Norm: 0.0054, Train Linf Norm: 0.3645, Test Linf Norm: 0.1074\n",
            "Epoch 124: Train Loss: 0.0044, Test Loss: 0.0048, Train L1 Norm: 0.0105, Test L1 Norm: 0.0053, Train Linf Norm: 0.3674, Test Linf Norm: 0.1061\n",
            "Epoch 125: Train Loss: 0.0044, Test Loss: 0.0049, Train L1 Norm: 0.0101, Test L1 Norm: 0.0054, Train Linf Norm: 0.3498, Test Linf Norm: 0.1082\n",
            "Epoch 126: Train Loss: 0.0044, Test Loss: 0.0048, Train L1 Norm: 0.0103, Test L1 Norm: 0.0054, Train Linf Norm: 0.3585, Test Linf Norm: 0.1073\n",
            "Epoch 127: Train Loss: 0.0044, Test Loss: 0.0048, Train L1 Norm: 0.0099, Test L1 Norm: 0.0054, Train Linf Norm: 0.3401, Test Linf Norm: 0.1069\n",
            "Epoch 128: Train Loss: 0.0044, Test Loss: 0.0048, Train L1 Norm: 0.0102, Test L1 Norm: 0.0054, Train Linf Norm: 0.3564, Test Linf Norm: 0.1091\n",
            "Epoch 129: Train Loss: 0.0044, Test Loss: 0.0050, Train L1 Norm: 0.0105, Test L1 Norm: 0.0056, Train Linf Norm: 0.3705, Test Linf Norm: 0.1151\n",
            "Epoch 130: Train Loss: 0.0044, Test Loss: 0.0048, Train L1 Norm: 0.0101, Test L1 Norm: 0.0054, Train Linf Norm: 0.3500, Test Linf Norm: 0.1098\n",
            "Epoch 131: Train Loss: 0.0043, Test Loss: 0.0047, Train L1 Norm: 0.0101, Test L1 Norm: 0.0053, Train Linf Norm: 0.3527, Test Linf Norm: 0.1072\n",
            "Epoch 132: Train Loss: 0.0043, Test Loss: 0.0047, Train L1 Norm: 0.0100, Test L1 Norm: 0.0054, Train Linf Norm: 0.3503, Test Linf Norm: 0.1091\n",
            "Epoch 133: Train Loss: 0.0043, Test Loss: 0.0047, Train L1 Norm: 0.0100, Test L1 Norm: 0.0053, Train Linf Norm: 0.3447, Test Linf Norm: 0.1055\n",
            "Epoch 134: Train Loss: 0.0043, Test Loss: 0.0047, Train L1 Norm: 0.0099, Test L1 Norm: 0.0053, Train Linf Norm: 0.3453, Test Linf Norm: 0.1068\n",
            "Epoch 135: Train Loss: 0.0042, Test Loss: 0.0047, Train L1 Norm: 0.0100, Test L1 Norm: 0.0054, Train Linf Norm: 0.3486, Test Linf Norm: 0.1096\n",
            "Epoch 136: Train Loss: 0.0043, Test Loss: 0.0047, Train L1 Norm: 0.0099, Test L1 Norm: 0.0053, Train Linf Norm: 0.3407, Test Linf Norm: 0.1061\n",
            "Epoch 137: Train Loss: 0.0043, Test Loss: 0.0047, Train L1 Norm: 0.0098, Test L1 Norm: 0.0053, Train Linf Norm: 0.3391, Test Linf Norm: 0.1080\n",
            "Epoch 138: Train Loss: 0.0042, Test Loss: 0.0047, Train L1 Norm: 0.0101, Test L1 Norm: 0.0053, Train Linf Norm: 0.3515, Test Linf Norm: 0.1065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 12:33:28,415]\u001b[0m Trial 30 finished with value: 0.0053379944372922185 and parameters: {'n_layers': 6, 'n_units_0': 767, 'n_units_1': 1946, 'n_units_2': 1132, 'n_units_3': 858, 'n_units_4': 1433, 'n_units_5': 1021, 'hidden_activation': 'LeakyReLU', 'output_activation': 'ReLU', 'loss': 'MAE', 'optimizer': 'Adagrad', 'lr': 0.00015259273225516585, 'batch_size': 48, 'n_epochs': 139, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.49770639123704336, 'patience': 10, 'threshold': 0.00011180420255457926}. Best is trial 30 with value: 0.0053379944372922185.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 139: Train Loss: 0.0042, Test Loss: 0.0047, Train L1 Norm: 0.0100, Test L1 Norm: 0.0053, Train Linf Norm: 0.3478, Test Linf Norm: 0.1082\n",
            "Epoch 1: Train Loss: 0.1657, Test Loss: 0.0470, Train L1 Norm: 0.4931, Test L1 Norm: 0.0375, Train Linf Norm: 19.9003, Test Linf Norm: 0.7024\n",
            "Epoch 2: Train Loss: 0.0549, Test Loss: 0.0625, Train L1 Norm: 0.1148, Test L1 Norm: 0.0363, Train Linf Norm: 4.1822, Test Linf Norm: 0.4739\n",
            "Epoch 3: Train Loss: 0.0406, Test Loss: 0.0375, Train L1 Norm: 0.0836, Test L1 Norm: 0.0283, Train Linf Norm: 3.0091, Test Linf Norm: 0.4476\n",
            "Epoch 4: Train Loss: 0.0335, Test Loss: 0.0473, Train L1 Norm: 0.0598, Test L1 Norm: 0.0339, Train Linf Norm: 2.0622, Test Linf Norm: 0.6384\n",
            "Epoch 5: Train Loss: 0.0290, Test Loss: 0.0173, Train L1 Norm: 0.0716, Test L1 Norm: 0.0212, Train Linf Norm: 2.7120, Test Linf Norm: 0.5028\n",
            "Epoch 6: Train Loss: 0.0260, Test Loss: 0.0309, Train L1 Norm: 0.0471, Test L1 Norm: 0.0210, Train Linf Norm: 1.6009, Test Linf Norm: 0.3321\n",
            "Epoch 7: Train Loss: 0.0233, Test Loss: 0.0119, Train L1 Norm: 0.0443, Test L1 Norm: 0.0170, Train Linf Norm: 1.5331, Test Linf Norm: 0.4259\n",
            "Epoch 8: Train Loss: 0.0220, Test Loss: 0.0348, Train L1 Norm: 0.0401, Test L1 Norm: 0.0272, Train Linf Norm: 1.3638, Test Linf Norm: 0.5474\n",
            "Epoch 9: Train Loss: 0.0207, Test Loss: 0.0115, Train L1 Norm: 0.0379, Test L1 Norm: 0.0135, Train Linf Norm: 1.2882, Test Linf Norm: 0.2884\n",
            "Epoch 10: Train Loss: 0.0193, Test Loss: 0.0204, Train L1 Norm: 0.0380, Test L1 Norm: 0.0200, Train Linf Norm: 1.3225, Test Linf Norm: 0.4434\n",
            "Epoch 11: Train Loss: 0.0183, Test Loss: 0.0396, Train L1 Norm: 0.0324, Test L1 Norm: 0.0263, Train Linf Norm: 1.0774, Test Linf Norm: 0.4853\n",
            "Epoch 12: Train Loss: 0.0176, Test Loss: 0.0178, Train L1 Norm: 0.0308, Test L1 Norm: 0.0157, Train Linf Norm: 1.0179, Test Linf Norm: 0.3408\n",
            "Epoch 13: Train Loss: 0.0167, Test Loss: 0.0167, Train L1 Norm: 0.0295, Test L1 Norm: 0.0153, Train Linf Norm: 0.9806, Test Linf Norm: 0.3162\n",
            "Epoch 14: Train Loss: 0.0164, Test Loss: 0.0219, Train L1 Norm: 0.0401, Test L1 Norm: 0.0150, Train Linf Norm: 1.4972, Test Linf Norm: 0.2326\n",
            "Epoch 15: Train Loss: 0.0157, Test Loss: 0.0139, Train L1 Norm: 0.0288, Test L1 Norm: 0.0127, Train Linf Norm: 0.9776, Test Linf Norm: 0.2522\n",
            "Epoch 16: Train Loss: 0.0152, Test Loss: 0.0102, Train L1 Norm: 0.0279, Test L1 Norm: 0.0123, Train Linf Norm: 0.9487, Test Linf Norm: 0.2901\n",
            "Epoch 17: Train Loss: 0.0146, Test Loss: 0.0265, Train L1 Norm: 0.0268, Test L1 Norm: 0.0196, Train Linf Norm: 0.9072, Test Linf Norm: 0.3830\n",
            "Epoch 18: Train Loss: 0.0143, Test Loss: 0.0271, Train L1 Norm: 0.0250, Test L1 Norm: 0.0170, Train Linf Norm: 0.8282, Test Linf Norm: 0.2512\n",
            "Epoch 19: Train Loss: 0.0138, Test Loss: 0.0285, Train L1 Norm: 0.0255, Test L1 Norm: 0.0203, Train Linf Norm: 0.8639, Test Linf Norm: 0.4135\n",
            "Epoch 20: Train Loss: 0.0134, Test Loss: 0.0157, Train L1 Norm: 0.0239, Test L1 Norm: 0.0122, Train Linf Norm: 0.8020, Test Linf Norm: 0.2130\n",
            "Epoch 21: Train Loss: 0.0130, Test Loss: 0.0089, Train L1 Norm: 0.0240, Test L1 Norm: 0.0104, Train Linf Norm: 0.8171, Test Linf Norm: 0.2260\n",
            "Epoch 22: Train Loss: 0.0129, Test Loss: 0.0188, Train L1 Norm: 0.0222, Test L1 Norm: 0.0134, Train Linf Norm: 0.7256, Test Linf Norm: 0.2246\n",
            "Epoch 23: Train Loss: 0.0124, Test Loss: 0.0153, Train L1 Norm: 0.0246, Test L1 Norm: 0.0130, Train Linf Norm: 0.8555, Test Linf Norm: 0.2661\n",
            "Epoch 24: Train Loss: 0.0124, Test Loss: 0.0096, Train L1 Norm: 0.0229, Test L1 Norm: 0.0117, Train Linf Norm: 0.7797, Test Linf Norm: 0.2775\n",
            "Epoch 25: Train Loss: 0.0119, Test Loss: 0.0088, Train L1 Norm: 0.0212, Test L1 Norm: 0.0094, Train Linf Norm: 0.7092, Test Linf Norm: 0.1996\n",
            "Epoch 26: Train Loss: 0.0116, Test Loss: 0.0069, Train L1 Norm: 0.0239, Test L1 Norm: 0.0089, Train Linf Norm: 0.8384, Test Linf Norm: 0.1940\n",
            "Epoch 27: Train Loss: 0.0115, Test Loss: 0.0106, Train L1 Norm: 0.0195, Test L1 Norm: 0.0105, Train Linf Norm: 0.6352, Test Linf Norm: 0.2144\n",
            "Epoch 28: Train Loss: 0.0112, Test Loss: 0.0144, Train L1 Norm: 0.0258, Test L1 Norm: 0.0121, Train Linf Norm: 0.9395, Test Linf Norm: 0.2498\n",
            "Epoch 29: Train Loss: 0.0111, Test Loss: 0.0126, Train L1 Norm: 0.0200, Test L1 Norm: 0.0106, Train Linf Norm: 0.6687, Test Linf Norm: 0.2006\n",
            "Epoch 30: Train Loss: 0.0110, Test Loss: 0.0110, Train L1 Norm: 0.0202, Test L1 Norm: 0.0094, Train Linf Norm: 0.6806, Test Linf Norm: 0.1870\n",
            "Epoch 31: Train Loss: 0.0107, Test Loss: 0.0143, Train L1 Norm: 0.0248, Test L1 Norm: 0.0119, Train Linf Norm: 0.9056, Test Linf Norm: 0.2147\n",
            "Epoch 32: Train Loss: 0.0106, Test Loss: 0.0066, Train L1 Norm: 0.0182, Test L1 Norm: 0.0087, Train Linf Norm: 0.5977, Test Linf Norm: 0.1994\n",
            "Epoch 33: Train Loss: 0.0103, Test Loss: 0.0082, Train L1 Norm: 0.0186, Test L1 Norm: 0.0087, Train Linf Norm: 0.6227, Test Linf Norm: 0.1860\n",
            "Epoch 34: Train Loss: 0.0101, Test Loss: 0.0073, Train L1 Norm: 0.0187, Test L1 Norm: 0.0095, Train Linf Norm: 0.6290, Test Linf Norm: 0.2205\n",
            "Epoch 35: Train Loss: 0.0103, Test Loss: 0.0081, Train L1 Norm: 0.0182, Test L1 Norm: 0.0091, Train Linf Norm: 0.5994, Test Linf Norm: 0.1989\n",
            "Epoch 36: Train Loss: 0.0101, Test Loss: 0.0070, Train L1 Norm: 0.0202, Test L1 Norm: 0.0084, Train Linf Norm: 0.7010, Test Linf Norm: 0.1876\n",
            "Epoch 37: Train Loss: 0.0100, Test Loss: 0.0116, Train L1 Norm: 0.0176, Test L1 Norm: 0.0095, Train Linf Norm: 0.5758, Test Linf Norm: 0.1791\n",
            "Epoch 38: Train Loss: 0.0099, Test Loss: 0.0065, Train L1 Norm: 0.0190, Test L1 Norm: 0.0081, Train Linf Norm: 0.6499, Test Linf Norm: 0.1781\n",
            "Epoch 39: Train Loss: 0.0099, Test Loss: 0.0141, Train L1 Norm: 0.0174, Test L1 Norm: 0.0107, Train Linf Norm: 0.5762, Test Linf Norm: 0.1897\n",
            "Epoch 40: Train Loss: 0.0097, Test Loss: 0.0103, Train L1 Norm: 0.0257, Test L1 Norm: 0.0087, Train Linf Norm: 0.9780, Test Linf Norm: 0.1741\n",
            "Epoch 41: Train Loss: 0.0095, Test Loss: 0.0192, Train L1 Norm: 0.0174, Test L1 Norm: 0.0127, Train Linf Norm: 0.5842, Test Linf Norm: 0.2041\n",
            "Epoch 42: Train Loss: 0.0094, Test Loss: 0.0115, Train L1 Norm: 0.0171, Test L1 Norm: 0.0093, Train Linf Norm: 0.5748, Test Linf Norm: 0.1825\n",
            "Epoch 43: Train Loss: 0.0093, Test Loss: 0.0125, Train L1 Norm: 0.0164, Test L1 Norm: 0.0094, Train Linf Norm: 0.5425, Test Linf Norm: 0.1786\n",
            "Epoch 44: Train Loss: 0.0092, Test Loss: 0.0078, Train L1 Norm: 0.0166, Test L1 Norm: 0.0083, Train Linf Norm: 0.5538, Test Linf Norm: 0.1813\n",
            "Epoch 45: Train Loss: 0.0090, Test Loss: 0.0098, Train L1 Norm: 0.0161, Test L1 Norm: 0.0092, Train Linf Norm: 0.5320, Test Linf Norm: 0.1854\n",
            "Epoch 46: Train Loss: 0.0090, Test Loss: 0.0081, Train L1 Norm: 0.0165, Test L1 Norm: 0.0085, Train Linf Norm: 0.5543, Test Linf Norm: 0.1866\n",
            "Epoch 47: Train Loss: 0.0088, Test Loss: 0.0091, Train L1 Norm: 0.0151, Test L1 Norm: 0.0086, Train Linf Norm: 0.4884, Test Linf Norm: 0.1773\n",
            "Epoch 48: Train Loss: 0.0087, Test Loss: 0.0128, Train L1 Norm: 0.0161, Test L1 Norm: 0.0099, Train Linf Norm: 0.5396, Test Linf Norm: 0.1971\n",
            "Epoch 49: Train Loss: 0.0088, Test Loss: 0.0136, Train L1 Norm: 0.0161, Test L1 Norm: 0.0096, Train Linf Norm: 0.5440, Test Linf Norm: 0.1756\n",
            "Epoch 50: Train Loss: 0.0057, Test Loss: 0.0057, Train L1 Norm: 0.0142, Test L1 Norm: 0.0072, Train Linf Norm: 0.4978, Test Linf Norm: 0.1576\n",
            "Epoch 51: Train Loss: 0.0057, Test Loss: 0.0056, Train L1 Norm: 0.0137, Test L1 Norm: 0.0071, Train Linf Norm: 0.4740, Test Linf Norm: 0.1597\n",
            "Epoch 52: Train Loss: 0.0057, Test Loss: 0.0058, Train L1 Norm: 0.0141, Test L1 Norm: 0.0076, Train Linf Norm: 0.4943, Test Linf Norm: 0.1737\n",
            "Epoch 53: Train Loss: 0.0056, Test Loss: 0.0096, Train L1 Norm: 0.0142, Test L1 Norm: 0.0083, Train Linf Norm: 0.5010, Test Linf Norm: 0.1683\n",
            "Epoch 54: Train Loss: 0.0056, Test Loss: 0.0075, Train L1 Norm: 0.0136, Test L1 Norm: 0.0075, Train Linf Norm: 0.4686, Test Linf Norm: 0.1592\n",
            "Epoch 55: Train Loss: 0.0056, Test Loss: 0.0056, Train L1 Norm: 0.0138, Test L1 Norm: 0.0071, Train Linf Norm: 0.4833, Test Linf Norm: 0.1603\n",
            "Epoch 56: Train Loss: 0.0056, Test Loss: 0.0058, Train L1 Norm: 0.0132, Test L1 Norm: 0.0072, Train Linf Norm: 0.4524, Test Linf Norm: 0.1630\n",
            "Epoch 57: Train Loss: 0.0055, Test Loss: 0.0056, Train L1 Norm: 0.0135, Test L1 Norm: 0.0075, Train Linf Norm: 0.4714, Test Linf Norm: 0.1688\n",
            "Epoch 58: Train Loss: 0.0055, Test Loss: 0.0057, Train L1 Norm: 0.0136, Test L1 Norm: 0.0072, Train Linf Norm: 0.4799, Test Linf Norm: 0.1634\n",
            "Epoch 59: Train Loss: 0.0055, Test Loss: 0.0057, Train L1 Norm: 0.0127, Test L1 Norm: 0.0071, Train Linf Norm: 0.4265, Test Linf Norm: 0.1591\n",
            "Epoch 60: Train Loss: 0.0054, Test Loss: 0.0075, Train L1 Norm: 0.0129, Test L1 Norm: 0.0076, Train Linf Norm: 0.4408, Test Linf Norm: 0.1594\n",
            "Epoch 61: Train Loss: 0.0054, Test Loss: 0.0056, Train L1 Norm: 0.0126, Test L1 Norm: 0.0072, Train Linf Norm: 0.4334, Test Linf Norm: 0.1634\n",
            "Epoch 62: Train Loss: 0.0054, Test Loss: 0.0056, Train L1 Norm: 0.0122, Test L1 Norm: 0.0070, Train Linf Norm: 0.4121, Test Linf Norm: 0.1567\n",
            "Epoch 63: Train Loss: 0.0054, Test Loss: 0.0069, Train L1 Norm: 0.0135, Test L1 Norm: 0.0078, Train Linf Norm: 0.4721, Test Linf Norm: 0.1709\n",
            "Epoch 64: Train Loss: 0.0054, Test Loss: 0.0061, Train L1 Norm: 0.0127, Test L1 Norm: 0.0074, Train Linf Norm: 0.4349, Test Linf Norm: 0.1681\n",
            "Epoch 65: Train Loss: 0.0054, Test Loss: 0.0053, Train L1 Norm: 0.0127, Test L1 Norm: 0.0068, Train Linf Norm: 0.4381, Test Linf Norm: 0.1540\n",
            "Epoch 66: Train Loss: 0.0053, Test Loss: 0.0053, Train L1 Norm: 0.0129, Test L1 Norm: 0.0070, Train Linf Norm: 0.4491, Test Linf Norm: 0.1620\n",
            "Epoch 67: Train Loss: 0.0053, Test Loss: 0.0052, Train L1 Norm: 0.0126, Test L1 Norm: 0.0068, Train Linf Norm: 0.4321, Test Linf Norm: 0.1540\n",
            "Epoch 68: Train Loss: 0.0053, Test Loss: 0.0055, Train L1 Norm: 0.0127, Test L1 Norm: 0.0070, Train Linf Norm: 0.4394, Test Linf Norm: 0.1563\n",
            "Epoch 69: Train Loss: 0.0053, Test Loss: 0.0101, Train L1 Norm: 0.0128, Test L1 Norm: 0.0086, Train Linf Norm: 0.4425, Test Linf Norm: 0.1704\n",
            "Epoch 70: Train Loss: 0.0053, Test Loss: 0.0056, Train L1 Norm: 0.0125, Test L1 Norm: 0.0070, Train Linf Norm: 0.4295, Test Linf Norm: 0.1557\n",
            "Epoch 71: Train Loss: 0.0052, Test Loss: 0.0055, Train L1 Norm: 0.0124, Test L1 Norm: 0.0069, Train Linf Norm: 0.4264, Test Linf Norm: 0.1530\n",
            "Epoch 72: Train Loss: 0.0052, Test Loss: 0.0052, Train L1 Norm: 0.0124, Test L1 Norm: 0.0068, Train Linf Norm: 0.4277, Test Linf Norm: 0.1534\n",
            "Epoch 73: Train Loss: 0.0052, Test Loss: 0.0054, Train L1 Norm: 0.0121, Test L1 Norm: 0.0068, Train Linf Norm: 0.4115, Test Linf Norm: 0.1542\n",
            "Epoch 74: Train Loss: 0.0052, Test Loss: 0.0055, Train L1 Norm: 0.0122, Test L1 Norm: 0.0068, Train Linf Norm: 0.4228, Test Linf Norm: 0.1515\n",
            "Epoch 75: Train Loss: 0.0052, Test Loss: 0.0051, Train L1 Norm: 0.0121, Test L1 Norm: 0.0067, Train Linf Norm: 0.4170, Test Linf Norm: 0.1538\n",
            "Epoch 76: Train Loss: 0.0052, Test Loss: 0.0052, Train L1 Norm: 0.0119, Test L1 Norm: 0.0069, Train Linf Norm: 0.4054, Test Linf Norm: 0.1577\n",
            "Epoch 77: Train Loss: 0.0051, Test Loss: 0.0061, Train L1 Norm: 0.0119, Test L1 Norm: 0.0070, Train Linf Norm: 0.4065, Test Linf Norm: 0.1538\n",
            "Epoch 78: Train Loss: 0.0051, Test Loss: 0.0051, Train L1 Norm: 0.0122, Test L1 Norm: 0.0067, Train Linf Norm: 0.4161, Test Linf Norm: 0.1536\n",
            "Epoch 79: Train Loss: 0.0051, Test Loss: 0.0055, Train L1 Norm: 0.0114, Test L1 Norm: 0.0069, Train Linf Norm: 0.3817, Test Linf Norm: 0.1569\n",
            "Epoch 80: Train Loss: 0.0051, Test Loss: 0.0051, Train L1 Norm: 0.0117, Test L1 Norm: 0.0068, Train Linf Norm: 0.3980, Test Linf Norm: 0.1590\n",
            "Epoch 81: Train Loss: 0.0051, Test Loss: 0.0051, Train L1 Norm: 0.0121, Test L1 Norm: 0.0070, Train Linf Norm: 0.4178, Test Linf Norm: 0.1627\n",
            "Epoch 82: Train Loss: 0.0051, Test Loss: 0.0068, Train L1 Norm: 0.0118, Test L1 Norm: 0.0072, Train Linf Norm: 0.4074, Test Linf Norm: 0.1577\n",
            "Epoch 83: Train Loss: 0.0051, Test Loss: 0.0051, Train L1 Norm: 0.0116, Test L1 Norm: 0.0066, Train Linf Norm: 0.3964, Test Linf Norm: 0.1515\n",
            "Epoch 84: Train Loss: 0.0050, Test Loss: 0.0050, Train L1 Norm: 0.0115, Test L1 Norm: 0.0066, Train Linf Norm: 0.3950, Test Linf Norm: 0.1484\n",
            "Epoch 85: Train Loss: 0.0050, Test Loss: 0.0050, Train L1 Norm: 0.0111, Test L1 Norm: 0.0067, Train Linf Norm: 0.3737, Test Linf Norm: 0.1523\n",
            "Epoch 86: Train Loss: 0.0050, Test Loss: 0.0050, Train L1 Norm: 0.0111, Test L1 Norm: 0.0066, Train Linf Norm: 0.3737, Test Linf Norm: 0.1507\n",
            "Epoch 87: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0110, Test L1 Norm: 0.0071, Train Linf Norm: 0.3693, Test Linf Norm: 0.1637\n",
            "Epoch 88: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 0.0115, Test L1 Norm: 0.0067, Train Linf Norm: 0.3919, Test Linf Norm: 0.1592\n",
            "Epoch 89: Train Loss: 0.0049, Test Loss: 0.0049, Train L1 Norm: 0.0118, Test L1 Norm: 0.0065, Train Linf Norm: 0.4058, Test Linf Norm: 0.1487\n",
            "Epoch 90: Train Loss: 0.0050, Test Loss: 0.0051, Train L1 Norm: 0.0115, Test L1 Norm: 0.0069, Train Linf Norm: 0.3929, Test Linf Norm: 0.1600\n",
            "Epoch 91: Train Loss: 0.0049, Test Loss: 0.0052, Train L1 Norm: 0.0119, Test L1 Norm: 0.0066, Train Linf Norm: 0.4139, Test Linf Norm: 0.1508\n",
            "Epoch 92: Train Loss: 0.0049, Test Loss: 0.0048, Train L1 Norm: 0.0114, Test L1 Norm: 0.0066, Train Linf Norm: 0.3879, Test Linf Norm: 0.1524\n",
            "Epoch 93: Train Loss: 0.0049, Test Loss: 0.0066, Train L1 Norm: 0.0111, Test L1 Norm: 0.0069, Train Linf Norm: 0.3770, Test Linf Norm: 0.1505\n",
            "Epoch 94: Train Loss: 0.0049, Test Loss: 0.0056, Train L1 Norm: 0.0111, Test L1 Norm: 0.0067, Train Linf Norm: 0.3721, Test Linf Norm: 0.1490\n",
            "Epoch 95: Train Loss: 0.0049, Test Loss: 0.0049, Train L1 Norm: 0.0115, Test L1 Norm: 0.0066, Train Linf Norm: 0.3965, Test Linf Norm: 0.1524\n",
            "Epoch 96: Train Loss: 0.0048, Test Loss: 0.0049, Train L1 Norm: 0.0110, Test L1 Norm: 0.0065, Train Linf Norm: 0.3740, Test Linf Norm: 0.1511\n",
            "Epoch 97: Train Loss: 0.0049, Test Loss: 0.0068, Train L1 Norm: 0.0109, Test L1 Norm: 0.0070, Train Linf Norm: 0.3665, Test Linf Norm: 0.1516\n",
            "Epoch 98: Train Loss: 0.0048, Test Loss: 0.0050, Train L1 Norm: 0.0110, Test L1 Norm: 0.0067, Train Linf Norm: 0.3723, Test Linf Norm: 0.1558\n",
            "Epoch 99: Train Loss: 0.0048, Test Loss: 0.0063, Train L1 Norm: 0.0113, Test L1 Norm: 0.0068, Train Linf Norm: 0.3866, Test Linf Norm: 0.1507\n",
            "Epoch 100: Train Loss: 0.0048, Test Loss: 0.0059, Train L1 Norm: 0.0112, Test L1 Norm: 0.0070, Train Linf Norm: 0.3817, Test Linf Norm: 0.1591\n",
            "Epoch 101: Train Loss: 0.0048, Test Loss: 0.0048, Train L1 Norm: 0.0109, Test L1 Norm: 0.0065, Train Linf Norm: 0.3698, Test Linf Norm: 0.1510\n",
            "Epoch 102: Train Loss: 0.0048, Test Loss: 0.0061, Train L1 Norm: 0.0103, Test L1 Norm: 0.0069, Train Linf Norm: 0.3405, Test Linf Norm: 0.1535\n",
            "Epoch 103: Train Loss: 0.0048, Test Loss: 0.0058, Train L1 Norm: 0.0111, Test L1 Norm: 0.0067, Train Linf Norm: 0.3817, Test Linf Norm: 0.1496\n",
            "Epoch 104: Train Loss: 0.0048, Test Loss: 0.0050, Train L1 Norm: 0.0110, Test L1 Norm: 0.0066, Train Linf Norm: 0.3737, Test Linf Norm: 0.1540\n",
            "Epoch 105: Train Loss: 0.0048, Test Loss: 0.0048, Train L1 Norm: 0.0107, Test L1 Norm: 0.0064, Train Linf Norm: 0.3594, Test Linf Norm: 0.1497\n",
            "Epoch 106: Train Loss: 0.0047, Test Loss: 0.0048, Train L1 Norm: 0.0109, Test L1 Norm: 0.0065, Train Linf Norm: 0.3700, Test Linf Norm: 0.1501\n",
            "Epoch 107: Train Loss: 0.0047, Test Loss: 0.0047, Train L1 Norm: 0.0110, Test L1 Norm: 0.0066, Train Linf Norm: 0.3780, Test Linf Norm: 0.1570\n",
            "Epoch 108: Train Loss: 0.0047, Test Loss: 0.0083, Train L1 Norm: 0.0097, Test L1 Norm: 0.0075, Train Linf Norm: 0.3162, Test Linf Norm: 0.1539\n",
            "Epoch 109: Train Loss: 0.0047, Test Loss: 0.0050, Train L1 Norm: 0.0106, Test L1 Norm: 0.0064, Train Linf Norm: 0.3599, Test Linf Norm: 0.1486\n",
            "Epoch 110: Train Loss: 0.0047, Test Loss: 0.0052, Train L1 Norm: 0.0106, Test L1 Norm: 0.0067, Train Linf Norm: 0.3577, Test Linf Norm: 0.1521\n",
            "Epoch 111: Train Loss: 0.0047, Test Loss: 0.0052, Train L1 Norm: 0.0105, Test L1 Norm: 0.0065, Train Linf Norm: 0.3530, Test Linf Norm: 0.1493\n",
            "Epoch 112: Train Loss: 0.0047, Test Loss: 0.0053, Train L1 Norm: 0.0109, Test L1 Norm: 0.0064, Train Linf Norm: 0.3744, Test Linf Norm: 0.1468\n",
            "Epoch 113: Train Loss: 0.0047, Test Loss: 0.0050, Train L1 Norm: 0.0111, Test L1 Norm: 0.0064, Train Linf Norm: 0.3836, Test Linf Norm: 0.1472\n",
            "Epoch 114: Train Loss: 0.0047, Test Loss: 0.0056, Train L1 Norm: 0.0106, Test L1 Norm: 0.0067, Train Linf Norm: 0.3569, Test Linf Norm: 0.1521\n",
            "Epoch 115: Train Loss: 0.0047, Test Loss: 0.0062, Train L1 Norm: 0.0104, Test L1 Norm: 0.0070, Train Linf Norm: 0.3490, Test Linf Norm: 0.1539\n",
            "Epoch 116: Train Loss: 0.0047, Test Loss: 0.0052, Train L1 Norm: 0.0105, Test L1 Norm: 0.0066, Train Linf Norm: 0.3559, Test Linf Norm: 0.1533\n",
            "Epoch 117: Train Loss: 0.0046, Test Loss: 0.0059, Train L1 Norm: 0.0109, Test L1 Norm: 0.0065, Train Linf Norm: 0.3734, Test Linf Norm: 0.1454\n",
            "Epoch 118: Train Loss: 0.0046, Test Loss: 0.0054, Train L1 Norm: 0.0103, Test L1 Norm: 0.0065, Train Linf Norm: 0.3430, Test Linf Norm: 0.1470\n",
            "Epoch 119: Train Loss: 0.0042, Test Loss: 0.0046, Train L1 Norm: 0.0103, Test L1 Norm: 0.0063, Train Linf Norm: 0.3541, Test Linf Norm: 0.1489\n",
            "Epoch 120: Train Loss: 0.0042, Test Loss: 0.0045, Train L1 Norm: 0.0101, Test L1 Norm: 0.0063, Train Linf Norm: 0.3437, Test Linf Norm: 0.1480\n",
            "Epoch 121: Train Loss: 0.0042, Test Loss: 0.0046, Train L1 Norm: 0.0103, Test L1 Norm: 0.0063, Train Linf Norm: 0.3511, Test Linf Norm: 0.1454\n",
            "Epoch 122: Train Loss: 0.0042, Test Loss: 0.0048, Train L1 Norm: 0.0102, Test L1 Norm: 0.0063, Train Linf Norm: 0.3474, Test Linf Norm: 0.1462\n",
            "Epoch 123: Train Loss: 0.0042, Test Loss: 0.0055, Train L1 Norm: 0.0103, Test L1 Norm: 0.0066, Train Linf Norm: 0.3522, Test Linf Norm: 0.1491\n",
            "Epoch 124: Train Loss: 0.0042, Test Loss: 0.0045, Train L1 Norm: 0.0101, Test L1 Norm: 0.0063, Train Linf Norm: 0.3438, Test Linf Norm: 0.1471\n",
            "Epoch 125: Train Loss: 0.0042, Test Loss: 0.0047, Train L1 Norm: 0.0102, Test L1 Norm: 0.0063, Train Linf Norm: 0.3490, Test Linf Norm: 0.1465\n",
            "Epoch 126: Train Loss: 0.0042, Test Loss: 0.0045, Train L1 Norm: 0.0100, Test L1 Norm: 0.0062, Train Linf Norm: 0.3393, Test Linf Norm: 0.1448\n",
            "Epoch 127: Train Loss: 0.0042, Test Loss: 0.0046, Train L1 Norm: 0.0103, Test L1 Norm: 0.0063, Train Linf Norm: 0.3512, Test Linf Norm: 0.1468\n",
            "Epoch 128: Train Loss: 0.0042, Test Loss: 0.0045, Train L1 Norm: 0.0103, Test L1 Norm: 0.0064, Train Linf Norm: 0.3574, Test Linf Norm: 0.1501\n",
            "Epoch 129: Train Loss: 0.0042, Test Loss: 0.0045, Train L1 Norm: 0.0101, Test L1 Norm: 0.0062, Train Linf Norm: 0.3459, Test Linf Norm: 0.1465\n",
            "Epoch 130: Train Loss: 0.0042, Test Loss: 0.0045, Train L1 Norm: 0.0103, Test L1 Norm: 0.0062, Train Linf Norm: 0.3541, Test Linf Norm: 0.1461\n",
            "Epoch 131: Train Loss: 0.0042, Test Loss: 0.0048, Train L1 Norm: 0.0101, Test L1 Norm: 0.0064, Train Linf Norm: 0.3455, Test Linf Norm: 0.1495\n",
            "Epoch 132: Train Loss: 0.0042, Test Loss: 0.0046, Train L1 Norm: 0.0103, Test L1 Norm: 0.0063, Train Linf Norm: 0.3561, Test Linf Norm: 0.1489\n",
            "Epoch 133: Train Loss: 0.0042, Test Loss: 0.0045, Train L1 Norm: 0.0100, Test L1 Norm: 0.0063, Train Linf Norm: 0.3425, Test Linf Norm: 0.1481\n",
            "Epoch 134: Train Loss: 0.0041, Test Loss: 0.0045, Train L1 Norm: 0.0098, Test L1 Norm: 0.0062, Train Linf Norm: 0.3333, Test Linf Norm: 0.1461\n",
            "Epoch 135: Train Loss: 0.0041, Test Loss: 0.0045, Train L1 Norm: 0.0101, Test L1 Norm: 0.0063, Train Linf Norm: 0.3463, Test Linf Norm: 0.1489\n",
            "Epoch 136: Train Loss: 0.0041, Test Loss: 0.0047, Train L1 Norm: 0.0098, Test L1 Norm: 0.0063, Train Linf Norm: 0.3323, Test Linf Norm: 0.1468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 12:53:52,684]\u001b[0m Trial 31 finished with value: 0.006315543460845947 and parameters: {'n_layers': 6, 'n_units_0': 776, 'n_units_1': 1915, 'n_units_2': 1077, 'n_units_3': 826, 'n_units_4': 1444, 'n_units_5': 1193, 'hidden_activation': 'LeakyReLU', 'output_activation': 'ReLU', 'loss': 'MAE', 'optimizer': 'Adagrad', 'lr': 0.00014729588814823102, 'batch_size': 48, 'n_epochs': 137, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.4984730612362001, 'patience': 10, 'threshold': 0.00010174486981975581}. Best is trial 30 with value: 0.0053379944372922185.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 137: Train Loss: 0.0041, Test Loss: 0.0049, Train L1 Norm: 0.0100, Test L1 Norm: 0.0063, Train Linf Norm: 0.3431, Test Linf Norm: 0.1474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 12:54:02,101]\u001b[0m Trial 32 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.1646, Test Loss: 0.1142, Train L1 Norm: 0.2973, Test L1 Norm: 0.0944, Train Linf Norm: 10.4370, Test Linf Norm: 1.9199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 12:54:10,791]\u001b[0m Trial 33 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 3.3922, Test Loss: 3.4125, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 12:54:13,954]\u001b[0m Trial 34 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.4549, Test Loss: 0.1707, Train L1 Norm: 0.6582, Test L1 Norm: 0.0855, Train Linf Norm: 104.8961, Test Linf Norm: 2.5350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 12:54:19,970]\u001b[0m Trial 35 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 1.0254, Test Loss: 0.4507, Train L1 Norm: 1.4000, Test L1 Norm: 0.4768, Train Linf Norm: 42.4018, Test Linf Norm: 11.4324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 12:54:27,640]\u001b[0m Trial 36 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 4.6880, Test Loss: 0.0442, Train L1 Norm: 0.7534, Test L1 Norm: 0.2904, Train Linf Norm: 19.6422, Test Linf Norm: 9.0118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 12:54:30,124]\u001b[0m Trial 37 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.1967, Test Loss: 0.0314, Train L1 Norm: 1.5022, Test L1 Norm: 0.1708, Train Linf Norm: 312.6136, Test Linf Norm: 11.8439\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 12:54:43,364]\u001b[0m Trial 38 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 3.3920, Test Loss: 3.4125, Train L1 Norm: 1.0007, Test L1 Norm: 1.0000, Train Linf Norm: 1.0055, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 12:54:49,265]\u001b[0m Trial 39 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 20.4332, Test Loss: 20.6405, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 12:54:51,245]\u001b[0m Trial 40 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 2.7939, Test Loss: 2.5972, Train L1 Norm: 1.1069, Test L1 Norm: 1.0033, Train Linf Norm: 79.2331, Test Linf Norm: 47.4269\n",
            "Epoch 1: Train Loss: 0.1731, Test Loss: 0.0594, Train L1 Norm: 0.1748, Test L1 Norm: 0.0375, Train Linf Norm: 4.7418, Test Linf Norm: 0.4651\n",
            "Epoch 2: Train Loss: 0.0577, Test Loss: 0.0377, Train L1 Norm: 0.0848, Test L1 Norm: 0.0298, Train Linf Norm: 2.8080, Test Linf Norm: 0.5951\n",
            "Epoch 3: Train Loss: 0.0432, Test Loss: 0.0288, Train L1 Norm: 0.0574, Test L1 Norm: 0.0213, Train Linf Norm: 1.7870, Test Linf Norm: 0.3831\n",
            "Epoch 4: Train Loss: 0.0354, Test Loss: 0.0471, Train L1 Norm: 0.0612, Test L1 Norm: 0.0254, Train Linf Norm: 2.1361, Test Linf Norm: 0.3215\n",
            "Epoch 5: Train Loss: 0.0309, Test Loss: 0.0252, Train L1 Norm: 0.0459, Test L1 Norm: 0.0192, Train Linf Norm: 1.4954, Test Linf Norm: 0.3298\n",
            "Epoch 6: Train Loss: 0.0275, Test Loss: 0.0224, Train L1 Norm: 0.0402, Test L1 Norm: 0.0176, Train Linf Norm: 1.2880, Test Linf Norm: 0.3275\n",
            "Epoch 7: Train Loss: 0.0254, Test Loss: 0.0398, Train L1 Norm: 0.0522, Test L1 Norm: 0.0236, Train Linf Norm: 1.9281, Test Linf Norm: 0.3705\n",
            "Epoch 8: Train Loss: 0.0237, Test Loss: 0.0197, Train L1 Norm: 0.0372, Test L1 Norm: 0.0167, Train Linf Norm: 1.2352, Test Linf Norm: 0.2743\n",
            "Epoch 9: Train Loss: 0.0219, Test Loss: 0.0105, Train L1 Norm: 0.0363, Test L1 Norm: 0.0116, Train Linf Norm: 1.2343, Test Linf Norm: 0.2560\n",
            "Epoch 10: Train Loss: 0.0205, Test Loss: 0.0167, Train L1 Norm: 0.0363, Test L1 Norm: 0.0141, Train Linf Norm: 1.2615, Test Linf Norm: 0.2757\n",
            "Epoch 11: Train Loss: 0.0195, Test Loss: 0.0109, Train L1 Norm: 0.0317, Test L1 Norm: 0.0122, Train Linf Norm: 1.0691, Test Linf Norm: 0.2546\n",
            "Epoch 12: Train Loss: 0.0186, Test Loss: 0.0116, Train L1 Norm: 0.0307, Test L1 Norm: 0.0124, Train Linf Norm: 1.0401, Test Linf Norm: 0.2714\n",
            "Epoch 13: Train Loss: 0.0176, Test Loss: 0.0193, Train L1 Norm: 0.0325, Test L1 Norm: 0.0142, Train Linf Norm: 1.1499, Test Linf Norm: 0.2661\n",
            "Epoch 14: Train Loss: 0.0170, Test Loss: 0.0250, Train L1 Norm: 0.0287, Test L1 Norm: 0.0172, Train Linf Norm: 0.9788, Test Linf Norm: 0.3150\n",
            "Epoch 15: Train Loss: 0.0164, Test Loss: 0.0242, Train L1 Norm: 0.0268, Test L1 Norm: 0.0163, Train Linf Norm: 0.9007, Test Linf Norm: 0.2986\n",
            "Epoch 16: Train Loss: 0.0158, Test Loss: 0.0083, Train L1 Norm: 0.0255, Test L1 Norm: 0.0103, Train Linf Norm: 0.8507, Test Linf Norm: 0.2213\n",
            "Epoch 17: Train Loss: 0.0153, Test Loss: 0.0133, Train L1 Norm: 0.0234, Test L1 Norm: 0.0123, Train Linf Norm: 0.7605, Test Linf Norm: 0.2244\n",
            "Epoch 18: Train Loss: 0.0147, Test Loss: 0.0205, Train L1 Norm: 0.0244, Test L1 Norm: 0.0127, Train Linf Norm: 0.8266, Test Linf Norm: 0.2173\n",
            "Epoch 19: Train Loss: 0.0147, Test Loss: 0.0130, Train L1 Norm: 0.0244, Test L1 Norm: 0.0113, Train Linf Norm: 0.8284, Test Linf Norm: 0.2189\n",
            "Epoch 20: Train Loss: 0.0141, Test Loss: 0.0114, Train L1 Norm: 0.0275, Test L1 Norm: 0.0103, Train Linf Norm: 0.9883, Test Linf Norm: 0.2193\n",
            "Epoch 21: Train Loss: 0.0136, Test Loss: 0.0217, Train L1 Norm: 0.0234, Test L1 Norm: 0.0131, Train Linf Norm: 0.7993, Test Linf Norm: 0.2280\n",
            "Epoch 22: Train Loss: 0.0134, Test Loss: 0.0121, Train L1 Norm: 0.0220, Test L1 Norm: 0.0106, Train Linf Norm: 0.7403, Test Linf Norm: 0.2119\n",
            "Epoch 23: Train Loss: 0.0131, Test Loss: 0.0128, Train L1 Norm: 0.0243, Test L1 Norm: 0.0123, Train Linf Norm: 0.8537, Test Linf Norm: 0.2305\n",
            "Epoch 24: Train Loss: 0.0128, Test Loss: 0.0082, Train L1 Norm: 0.0209, Test L1 Norm: 0.0084, Train Linf Norm: 0.7062, Test Linf Norm: 0.1802\n",
            "Epoch 25: Train Loss: 0.0125, Test Loss: 0.0132, Train L1 Norm: 0.0219, Test L1 Norm: 0.0105, Train Linf Norm: 0.7530, Test Linf Norm: 0.1976\n",
            "Epoch 26: Train Loss: 0.0123, Test Loss: 0.0161, Train L1 Norm: 0.0205, Test L1 Norm: 0.0121, Train Linf Norm: 0.6936, Test Linf Norm: 0.2348\n",
            "Epoch 27: Train Loss: 0.0121, Test Loss: 0.0188, Train L1 Norm: 0.0214, Test L1 Norm: 0.0135, Train Linf Norm: 0.7411, Test Linf Norm: 0.2211\n",
            "Epoch 28: Train Loss: 0.0119, Test Loss: 0.0081, Train L1 Norm: 0.0188, Test L1 Norm: 0.0087, Train Linf Norm: 0.6235, Test Linf Norm: 0.1872\n",
            "Epoch 29: Train Loss: 0.0114, Test Loss: 0.0162, Train L1 Norm: 0.0204, Test L1 Norm: 0.0118, Train Linf Norm: 0.7026, Test Linf Norm: 0.2281\n",
            "Epoch 30: Train Loss: 0.0114, Test Loss: 0.0165, Train L1 Norm: 0.0195, Test L1 Norm: 0.0117, Train Linf Norm: 0.6646, Test Linf Norm: 0.2196\n",
            "Epoch 31: Train Loss: 0.0111, Test Loss: 0.0257, Train L1 Norm: 0.0188, Test L1 Norm: 0.0143, Train Linf Norm: 0.6326, Test Linf Norm: 0.2297\n",
            "Epoch 32: Train Loss: 0.0110, Test Loss: 0.0127, Train L1 Norm: 0.0196, Test L1 Norm: 0.0101, Train Linf Norm: 0.6752, Test Linf Norm: 0.1905\n",
            "Epoch 33: Train Loss: 0.0108, Test Loss: 0.0144, Train L1 Norm: 0.0195, Test L1 Norm: 0.0103, Train Linf Norm: 0.6793, Test Linf Norm: 0.1759\n",
            "Epoch 34: Train Loss: 0.0108, Test Loss: 0.0196, Train L1 Norm: 0.0184, Test L1 Norm: 0.0136, Train Linf Norm: 0.6238, Test Linf Norm: 0.2479\n",
            "Epoch 35: Train Loss: 0.0106, Test Loss: 0.0070, Train L1 Norm: 0.0190, Test L1 Norm: 0.0079, Train Linf Norm: 0.6590, Test Linf Norm: 0.1730\n",
            "Epoch 36: Train Loss: 0.0104, Test Loss: 0.0109, Train L1 Norm: 0.0184, Test L1 Norm: 0.0091, Train Linf Norm: 0.6334, Test Linf Norm: 0.1751\n",
            "Epoch 37: Train Loss: 0.0104, Test Loss: 0.0108, Train L1 Norm: 0.0193, Test L1 Norm: 0.0093, Train Linf Norm: 0.6776, Test Linf Norm: 0.1778\n",
            "Epoch 38: Train Loss: 0.0101, Test Loss: 0.0078, Train L1 Norm: 0.0178, Test L1 Norm: 0.0081, Train Linf Norm: 0.6096, Test Linf Norm: 0.1659\n",
            "Epoch 39: Train Loss: 0.0100, Test Loss: 0.0185, Train L1 Norm: 0.0176, Test L1 Norm: 0.0114, Train Linf Norm: 0.6018, Test Linf Norm: 0.1959\n",
            "Epoch 40: Train Loss: 0.0098, Test Loss: 0.0065, Train L1 Norm: 0.0179, Test L1 Norm: 0.0074, Train Linf Norm: 0.6204, Test Linf Norm: 0.1649\n",
            "Epoch 41: Train Loss: 0.0099, Test Loss: 0.0079, Train L1 Norm: 0.0175, Test L1 Norm: 0.0073, Train Linf Norm: 0.6008, Test Linf Norm: 0.1506\n",
            "Epoch 42: Train Loss: 0.0098, Test Loss: 0.0065, Train L1 Norm: 0.0169, Test L1 Norm: 0.0072, Train Linf Norm: 0.5743, Test Linf Norm: 0.1500\n",
            "Epoch 43: Train Loss: 0.0095, Test Loss: 0.0127, Train L1 Norm: 0.0156, Test L1 Norm: 0.0091, Train Linf Norm: 0.5157, Test Linf Norm: 0.1690\n",
            "Epoch 44: Train Loss: 0.0094, Test Loss: 0.0104, Train L1 Norm: 0.0170, Test L1 Norm: 0.0085, Train Linf Norm: 0.5913, Test Linf Norm: 0.1619\n",
            "Epoch 45: Train Loss: 0.0094, Test Loss: 0.0073, Train L1 Norm: 0.0166, Test L1 Norm: 0.0071, Train Linf Norm: 0.5704, Test Linf Norm: 0.1481\n",
            "Epoch 46: Train Loss: 0.0091, Test Loss: 0.0170, Train L1 Norm: 0.0168, Test L1 Norm: 0.0114, Train Linf Norm: 0.5853, Test Linf Norm: 0.1893\n",
            "Epoch 47: Train Loss: 0.0091, Test Loss: 0.0113, Train L1 Norm: 0.0159, Test L1 Norm: 0.0080, Train Linf Norm: 0.5360, Test Linf Norm: 0.1441\n",
            "Epoch 48: Train Loss: 0.0090, Test Loss: 0.0081, Train L1 Norm: 0.0154, Test L1 Norm: 0.0076, Train Linf Norm: 0.5195, Test Linf Norm: 0.1529\n",
            "Epoch 49: Train Loss: 0.0089, Test Loss: 0.0085, Train L1 Norm: 0.0160, Test L1 Norm: 0.0074, Train Linf Norm: 0.5526, Test Linf Norm: 0.1522\n",
            "Epoch 50: Train Loss: 0.0089, Test Loss: 0.0058, Train L1 Norm: 0.0151, Test L1 Norm: 0.0064, Train Linf Norm: 0.5060, Test Linf Norm: 0.1348\n",
            "Epoch 51: Train Loss: 0.0087, Test Loss: 0.0112, Train L1 Norm: 0.0166, Test L1 Norm: 0.0091, Train Linf Norm: 0.5826, Test Linf Norm: 0.1677\n",
            "Epoch 52: Train Loss: 0.0087, Test Loss: 0.0060, Train L1 Norm: 0.0153, Test L1 Norm: 0.0067, Train Linf Norm: 0.5238, Test Linf Norm: 0.1466\n",
            "Epoch 53: Train Loss: 0.0085, Test Loss: 0.0088, Train L1 Norm: 0.0142, Test L1 Norm: 0.0076, Train Linf Norm: 0.4747, Test Linf Norm: 0.1514\n",
            "Epoch 54: Train Loss: 0.0084, Test Loss: 0.0111, Train L1 Norm: 0.0148, Test L1 Norm: 0.0079, Train Linf Norm: 0.5025, Test Linf Norm: 0.1488\n",
            "Epoch 55: Train Loss: 0.0085, Test Loss: 0.0061, Train L1 Norm: 0.0141, Test L1 Norm: 0.0069, Train Linf Norm: 0.4710, Test Linf Norm: 0.1424\n",
            "Epoch 56: Train Loss: 0.0084, Test Loss: 0.0104, Train L1 Norm: 0.0148, Test L1 Norm: 0.0075, Train Linf Norm: 0.5078, Test Linf Norm: 0.1342\n",
            "Epoch 57: Train Loss: 0.0084, Test Loss: 0.0062, Train L1 Norm: 0.0147, Test L1 Norm: 0.0069, Train Linf Norm: 0.4988, Test Linf Norm: 0.1476\n",
            "Epoch 58: Train Loss: 0.0082, Test Loss: 0.0053, Train L1 Norm: 0.0158, Test L1 Norm: 0.0065, Train Linf Norm: 0.5563, Test Linf Norm: 0.1412\n",
            "Epoch 59: Train Loss: 0.0083, Test Loss: 0.0083, Train L1 Norm: 0.0147, Test L1 Norm: 0.0071, Train Linf Norm: 0.5036, Test Linf Norm: 0.1385\n",
            "Epoch 60: Train Loss: 0.0082, Test Loss: 0.0055, Train L1 Norm: 0.0146, Test L1 Norm: 0.0063, Train Linf Norm: 0.4975, Test Linf Norm: 0.1400\n",
            "Epoch 61: Train Loss: 0.0079, Test Loss: 0.0152, Train L1 Norm: 0.0143, Test L1 Norm: 0.0097, Train Linf Norm: 0.4932, Test Linf Norm: 0.1581\n",
            "Epoch 62: Train Loss: 0.0080, Test Loss: 0.0051, Train L1 Norm: 0.0141, Test L1 Norm: 0.0058, Train Linf Norm: 0.4796, Test Linf Norm: 0.1222\n",
            "Epoch 63: Train Loss: 0.0081, Test Loss: 0.0055, Train L1 Norm: 0.0149, Test L1 Norm: 0.0059, Train Linf Norm: 0.5173, Test Linf Norm: 0.1216\n",
            "Epoch 64: Train Loss: 0.0079, Test Loss: 0.0057, Train L1 Norm: 0.0137, Test L1 Norm: 0.0059, Train Linf Norm: 0.4602, Test Linf Norm: 0.1189\n",
            "Epoch 65: Train Loss: 0.0078, Test Loss: 0.0062, Train L1 Norm: 0.0150, Test L1 Norm: 0.0059, Train Linf Norm: 0.5230, Test Linf Norm: 0.1124\n",
            "Epoch 66: Train Loss: 0.0078, Test Loss: 0.0052, Train L1 Norm: 0.0135, Test L1 Norm: 0.0065, Train Linf Norm: 0.4564, Test Linf Norm: 0.1485\n",
            "Epoch 67: Train Loss: 0.0076, Test Loss: 0.0074, Train L1 Norm: 0.0133, Test L1 Norm: 0.0071, Train Linf Norm: 0.4495, Test Linf Norm: 0.1384\n",
            "Epoch 68: Train Loss: 0.0077, Test Loss: 0.0059, Train L1 Norm: 0.0140, Test L1 Norm: 0.0059, Train Linf Norm: 0.4841, Test Linf Norm: 0.1213\n",
            "Epoch 69: Train Loss: 0.0076, Test Loss: 0.0062, Train L1 Norm: 0.0143, Test L1 Norm: 0.0064, Train Linf Norm: 0.5003, Test Linf Norm: 0.1368\n",
            "Epoch 70: Train Loss: 0.0075, Test Loss: 0.0055, Train L1 Norm: 0.0128, Test L1 Norm: 0.0061, Train Linf Norm: 0.4293, Test Linf Norm: 0.1303\n",
            "Epoch 71: Train Loss: 0.0075, Test Loss: 0.0087, Train L1 Norm: 0.0125, Test L1 Norm: 0.0073, Train Linf Norm: 0.4131, Test Linf Norm: 0.1386\n",
            "Epoch 72: Train Loss: 0.0046, Test Loss: 0.0047, Train L1 Norm: 0.0120, Test L1 Norm: 0.0057, Train Linf Norm: 0.4355, Test Linf Norm: 0.1242\n",
            "Epoch 73: Train Loss: 0.0046, Test Loss: 0.0046, Train L1 Norm: 0.0121, Test L1 Norm: 0.0058, Train Linf Norm: 0.4394, Test Linf Norm: 0.1281\n",
            "Epoch 74: Train Loss: 0.0046, Test Loss: 0.0047, Train L1 Norm: 0.0118, Test L1 Norm: 0.0057, Train Linf Norm: 0.4213, Test Linf Norm: 0.1253\n",
            "Epoch 75: Train Loss: 0.0046, Test Loss: 0.0052, Train L1 Norm: 0.0122, Test L1 Norm: 0.0060, Train Linf Norm: 0.4470, Test Linf Norm: 0.1295\n",
            "Epoch 76: Train Loss: 0.0046, Test Loss: 0.0057, Train L1 Norm: 0.0117, Test L1 Norm: 0.0057, Train Linf Norm: 0.4240, Test Linf Norm: 0.1132\n",
            "Epoch 77: Train Loss: 0.0046, Test Loss: 0.0047, Train L1 Norm: 0.0127, Test L1 Norm: 0.0057, Train Linf Norm: 0.4697, Test Linf Norm: 0.1262\n",
            "Epoch 78: Train Loss: 0.0045, Test Loss: 0.0054, Train L1 Norm: 0.0121, Test L1 Norm: 0.0057, Train Linf Norm: 0.4434, Test Linf Norm: 0.1184\n",
            "Epoch 79: Train Loss: 0.0045, Test Loss: 0.0054, Train L1 Norm: 0.0121, Test L1 Norm: 0.0057, Train Linf Norm: 0.4417, Test Linf Norm: 0.1169\n",
            "Epoch 80: Train Loss: 0.0045, Test Loss: 0.0046, Train L1 Norm: 0.0118, Test L1 Norm: 0.0054, Train Linf Norm: 0.4276, Test Linf Norm: 0.1117\n",
            "Epoch 81: Train Loss: 0.0045, Test Loss: 0.0046, Train L1 Norm: 0.0123, Test L1 Norm: 0.0057, Train Linf Norm: 0.4521, Test Linf Norm: 0.1258\n",
            "Epoch 82: Train Loss: 0.0045, Test Loss: 0.0046, Train L1 Norm: 0.0119, Test L1 Norm: 0.0056, Train Linf Norm: 0.4351, Test Linf Norm: 0.1218\n",
            "Epoch 83: Train Loss: 0.0044, Test Loss: 0.0057, Train L1 Norm: 0.0118, Test L1 Norm: 0.0059, Train Linf Norm: 0.4324, Test Linf Norm: 0.1221\n",
            "Epoch 84: Train Loss: 0.0044, Test Loss: 0.0047, Train L1 Norm: 0.0121, Test L1 Norm: 0.0055, Train Linf Norm: 0.4457, Test Linf Norm: 0.1180\n",
            "Epoch 85: Train Loss: 0.0044, Test Loss: 0.0046, Train L1 Norm: 0.0116, Test L1 Norm: 0.0055, Train Linf Norm: 0.4212, Test Linf Norm: 0.1193\n",
            "Epoch 86: Train Loss: 0.0044, Test Loss: 0.0048, Train L1 Norm: 0.0119, Test L1 Norm: 0.0056, Train Linf Norm: 0.4370, Test Linf Norm: 0.1188\n",
            "Epoch 87: Train Loss: 0.0044, Test Loss: 0.0067, Train L1 Norm: 0.0116, Test L1 Norm: 0.0061, Train Linf Norm: 0.4200, Test Linf Norm: 0.1200\n",
            "Epoch 88: Train Loss: 0.0044, Test Loss: 0.0045, Train L1 Norm: 0.0118, Test L1 Norm: 0.0056, Train Linf Norm: 0.4297, Test Linf Norm: 0.1227\n",
            "Epoch 89: Train Loss: 0.0044, Test Loss: 0.0050, Train L1 Norm: 0.0116, Test L1 Norm: 0.0054, Train Linf Norm: 0.4183, Test Linf Norm: 0.1115\n",
            "Epoch 90: Train Loss: 0.0044, Test Loss: 0.0045, Train L1 Norm: 0.0118, Test L1 Norm: 0.0053, Train Linf Norm: 0.4316, Test Linf Norm: 0.1130\n",
            "Epoch 91: Train Loss: 0.0043, Test Loss: 0.0044, Train L1 Norm: 0.0113, Test L1 Norm: 0.0053, Train Linf Norm: 0.4062, Test Linf Norm: 0.1133\n",
            "Epoch 92: Train Loss: 0.0044, Test Loss: 0.0046, Train L1 Norm: 0.0114, Test L1 Norm: 0.0053, Train Linf Norm: 0.4147, Test Linf Norm: 0.1116\n",
            "Epoch 93: Train Loss: 0.0043, Test Loss: 0.0046, Train L1 Norm: 0.0116, Test L1 Norm: 0.0056, Train Linf Norm: 0.4239, Test Linf Norm: 0.1209\n",
            "Epoch 94: Train Loss: 0.0043, Test Loss: 0.0045, Train L1 Norm: 0.0112, Test L1 Norm: 0.0058, Train Linf Norm: 0.4042, Test Linf Norm: 0.1286\n",
            "Epoch 95: Train Loss: 0.0043, Test Loss: 0.0045, Train L1 Norm: 0.0118, Test L1 Norm: 0.0053, Train Linf Norm: 0.4331, Test Linf Norm: 0.1109\n",
            "Epoch 96: Train Loss: 0.0043, Test Loss: 0.0047, Train L1 Norm: 0.0113, Test L1 Norm: 0.0054, Train Linf Norm: 0.4070, Test Linf Norm: 0.1149\n",
            "Epoch 97: Train Loss: 0.0043, Test Loss: 0.0052, Train L1 Norm: 0.0111, Test L1 Norm: 0.0054, Train Linf Norm: 0.4015, Test Linf Norm: 0.1074\n",
            "Epoch 98: Train Loss: 0.0043, Test Loss: 0.0044, Train L1 Norm: 0.0114, Test L1 Norm: 0.0051, Train Linf Norm: 0.4138, Test Linf Norm: 0.1063\n",
            "Epoch 99: Train Loss: 0.0043, Test Loss: 0.0045, Train L1 Norm: 0.0112, Test L1 Norm: 0.0053, Train Linf Norm: 0.4065, Test Linf Norm: 0.1105\n",
            "Epoch 100: Train Loss: 0.0043, Test Loss: 0.0047, Train L1 Norm: 0.0112, Test L1 Norm: 0.0054, Train Linf Norm: 0.4048, Test Linf Norm: 0.1133\n",
            "Epoch 101: Train Loss: 0.0043, Test Loss: 0.0060, Train L1 Norm: 0.0107, Test L1 Norm: 0.0057, Train Linf Norm: 0.3832, Test Linf Norm: 0.1144\n",
            "Epoch 102: Train Loss: 0.0043, Test Loss: 0.0050, Train L1 Norm: 0.0112, Test L1 Norm: 0.0055, Train Linf Norm: 0.4048, Test Linf Norm: 0.1137\n",
            "Epoch 103: Train Loss: 0.0042, Test Loss: 0.0069, Train L1 Norm: 0.0107, Test L1 Norm: 0.0064, Train Linf Norm: 0.3810, Test Linf Norm: 0.1283\n",
            "Epoch 104: Train Loss: 0.0042, Test Loss: 0.0074, Train L1 Norm: 0.0110, Test L1 Norm: 0.0062, Train Linf Norm: 0.3974, Test Linf Norm: 0.1204\n",
            "Epoch 105: Train Loss: 0.0042, Test Loss: 0.0046, Train L1 Norm: 0.0109, Test L1 Norm: 0.0053, Train Linf Norm: 0.3966, Test Linf Norm: 0.1135\n",
            "Epoch 106: Train Loss: 0.0042, Test Loss: 0.0053, Train L1 Norm: 0.0109, Test L1 Norm: 0.0054, Train Linf Norm: 0.3920, Test Linf Norm: 0.1088\n",
            "Epoch 107: Train Loss: 0.0042, Test Loss: 0.0046, Train L1 Norm: 0.0109, Test L1 Norm: 0.0054, Train Linf Norm: 0.3938, Test Linf Norm: 0.1174\n",
            "Epoch 108: Train Loss: 0.0039, Test Loss: 0.0044, Train L1 Norm: 0.0108, Test L1 Norm: 0.0052, Train Linf Norm: 0.3947, Test Linf Norm: 0.1083\n",
            "Epoch 109: Train Loss: 0.0039, Test Loss: 0.0042, Train L1 Norm: 0.0108, Test L1 Norm: 0.0053, Train Linf Norm: 0.3952, Test Linf Norm: 0.1149\n",
            "Epoch 110: Train Loss: 0.0039, Test Loss: 0.0043, Train L1 Norm: 0.0107, Test L1 Norm: 0.0052, Train Linf Norm: 0.3921, Test Linf Norm: 0.1116\n",
            "Epoch 111: Train Loss: 0.0039, Test Loss: 0.0043, Train L1 Norm: 0.0108, Test L1 Norm: 0.0053, Train Linf Norm: 0.3937, Test Linf Norm: 0.1139\n",
            "Epoch 112: Train Loss: 0.0039, Test Loss: 0.0044, Train L1 Norm: 0.0106, Test L1 Norm: 0.0052, Train Linf Norm: 0.3829, Test Linf Norm: 0.1086\n",
            "Epoch 113: Train Loss: 0.0039, Test Loss: 0.0043, Train L1 Norm: 0.0106, Test L1 Norm: 0.0051, Train Linf Norm: 0.3860, Test Linf Norm: 0.1089\n",
            "Epoch 114: Train Loss: 0.0039, Test Loss: 0.0043, Train L1 Norm: 0.0108, Test L1 Norm: 0.0051, Train Linf Norm: 0.3936, Test Linf Norm: 0.1062\n",
            "Epoch 115: Train Loss: 0.0039, Test Loss: 0.0042, Train L1 Norm: 0.0106, Test L1 Norm: 0.0052, Train Linf Norm: 0.3832, Test Linf Norm: 0.1121\n",
            "Epoch 116: Train Loss: 0.0039, Test Loss: 0.0043, Train L1 Norm: 0.0106, Test L1 Norm: 0.0052, Train Linf Norm: 0.3848, Test Linf Norm: 0.1093\n",
            "Epoch 117: Train Loss: 0.0039, Test Loss: 0.0042, Train L1 Norm: 0.0109, Test L1 Norm: 0.0052, Train Linf Norm: 0.3990, Test Linf Norm: 0.1103\n",
            "Epoch 118: Train Loss: 0.0039, Test Loss: 0.0042, Train L1 Norm: 0.0108, Test L1 Norm: 0.0051, Train Linf Norm: 0.3967, Test Linf Norm: 0.1087\n",
            "Epoch 119: Train Loss: 0.0038, Test Loss: 0.0045, Train L1 Norm: 0.0107, Test L1 Norm: 0.0052, Train Linf Norm: 0.3869, Test Linf Norm: 0.1100\n",
            "Epoch 120: Train Loss: 0.0038, Test Loss: 0.0043, Train L1 Norm: 0.0108, Test L1 Norm: 0.0051, Train Linf Norm: 0.3994, Test Linf Norm: 0.1095\n",
            "Epoch 121: Train Loss: 0.0038, Test Loss: 0.0043, Train L1 Norm: 0.0107, Test L1 Norm: 0.0051, Train Linf Norm: 0.3919, Test Linf Norm: 0.1053\n",
            "Epoch 122: Train Loss: 0.0038, Test Loss: 0.0043, Train L1 Norm: 0.0106, Test L1 Norm: 0.0051, Train Linf Norm: 0.3842, Test Linf Norm: 0.1065\n",
            "Epoch 123: Train Loss: 0.0038, Test Loss: 0.0042, Train L1 Norm: 0.0108, Test L1 Norm: 0.0052, Train Linf Norm: 0.3941, Test Linf Norm: 0.1109\n",
            "Epoch 124: Train Loss: 0.0038, Test Loss: 0.0042, Train L1 Norm: 0.0106, Test L1 Norm: 0.0051, Train Linf Norm: 0.3856, Test Linf Norm: 0.1091\n",
            "Epoch 125: Train Loss: 0.0038, Test Loss: 0.0044, Train L1 Norm: 0.0107, Test L1 Norm: 0.0052, Train Linf Norm: 0.3910, Test Linf Norm: 0.1082\n",
            "Epoch 126: Train Loss: 0.0038, Test Loss: 0.0042, Train L1 Norm: 0.0105, Test L1 Norm: 0.0051, Train Linf Norm: 0.3816, Test Linf Norm: 0.1072\n",
            "Epoch 127: Train Loss: 0.0038, Test Loss: 0.0043, Train L1 Norm: 0.0107, Test L1 Norm: 0.0051, Train Linf Norm: 0.3942, Test Linf Norm: 0.1073\n",
            "Epoch 128: Train Loss: 0.0038, Test Loss: 0.0043, Train L1 Norm: 0.0107, Test L1 Norm: 0.0053, Train Linf Norm: 0.3926, Test Linf Norm: 0.1146\n",
            "Epoch 129: Train Loss: 0.0038, Test Loss: 0.0042, Train L1 Norm: 0.0107, Test L1 Norm: 0.0051, Train Linf Norm: 0.3912, Test Linf Norm: 0.1102\n",
            "Epoch 130: Train Loss: 0.0038, Test Loss: 0.0042, Train L1 Norm: 0.0107, Test L1 Norm: 0.0050, Train Linf Norm: 0.3932, Test Linf Norm: 0.1058\n",
            "Epoch 131: Train Loss: 0.0038, Test Loss: 0.0043, Train L1 Norm: 0.0105, Test L1 Norm: 0.0050, Train Linf Norm: 0.3841, Test Linf Norm: 0.1045\n",
            "Epoch 132: Train Loss: 0.0038, Test Loss: 0.0042, Train L1 Norm: 0.0105, Test L1 Norm: 0.0050, Train Linf Norm: 0.3851, Test Linf Norm: 0.1067\n",
            "Epoch 133: Train Loss: 0.0038, Test Loss: 0.0048, Train L1 Norm: 0.0106, Test L1 Norm: 0.0054, Train Linf Norm: 0.3893, Test Linf Norm: 0.1114\n",
            "Epoch 134: Train Loss: 0.0038, Test Loss: 0.0042, Train L1 Norm: 0.0105, Test L1 Norm: 0.0050, Train Linf Norm: 0.3847, Test Linf Norm: 0.1062\n",
            "Epoch 135: Train Loss: 0.0038, Test Loss: 0.0043, Train L1 Norm: 0.0106, Test L1 Norm: 0.0050, Train Linf Norm: 0.3883, Test Linf Norm: 0.1050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 13:12:31,521]\u001b[0m Trial 41 finished with value: 0.0050436803916469215 and parameters: {'n_layers': 5, 'n_units_0': 1493, 'n_units_1': 1845, 'n_units_2': 571, 'n_units_3': 725, 'n_units_4': 1838, 'hidden_activation': 'LeakyReLU', 'output_activation': 'ReLU', 'loss': 'MAE', 'optimizer': 'Adagrad', 'lr': 0.0001755001869006059, 'batch_size': 48, 'n_epochs': 136, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.423570919194432, 'patience': 8, 'threshold': 0.0001755168655943945}. Best is trial 41 with value: 0.0050436803916469215.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 136: Train Loss: 0.0038, Test Loss: 0.0042, Train L1 Norm: 0.0108, Test L1 Norm: 0.0050, Train Linf Norm: 0.3966, Test Linf Norm: 0.1065\n",
            "Epoch 1: Train Loss: 0.1616, Test Loss: 0.0477, Train L1 Norm: 0.2809, Test L1 Norm: 0.0581, Train Linf Norm: 9.7559, Test Linf Norm: 1.3961\n",
            "Epoch 2: Train Loss: 0.0492, Test Loss: 0.0274, Train L1 Norm: 0.0879, Test L1 Norm: 0.0296, Train Linf Norm: 3.0085, Test Linf Norm: 0.6161\n",
            "Epoch 3: Train Loss: 0.0360, Test Loss: 0.0194, Train L1 Norm: 0.0685, Test L1 Norm: 0.0245, Train Linf Norm: 2.3514, Test Linf Norm: 0.5513\n",
            "Epoch 4: Train Loss: 0.0299, Test Loss: 0.0279, Train L1 Norm: 0.0581, Test L1 Norm: 0.0249, Train Linf Norm: 2.0114, Test Linf Norm: 0.5025\n",
            "Epoch 5: Train Loss: 0.0259, Test Loss: 0.0357, Train L1 Norm: 0.0558, Test L1 Norm: 0.0219, Train Linf Norm: 1.9996, Test Linf Norm: 0.3239\n",
            "Epoch 6: Train Loss: 0.0231, Test Loss: 0.0516, Train L1 Norm: 0.0506, Test L1 Norm: 0.0273, Train Linf Norm: 1.8232, Test Linf Norm: 0.3438\n",
            "Epoch 7: Train Loss: 0.0212, Test Loss: 0.0150, Train L1 Norm: 0.0460, Test L1 Norm: 0.0159, Train Linf Norm: 1.6375, Test Linf Norm: 0.3158\n",
            "Epoch 8: Train Loss: 0.0195, Test Loss: 0.0278, Train L1 Norm: 0.0418, Test L1 Norm: 0.0210, Train Linf Norm: 1.4746, Test Linf Norm: 0.3716\n",
            "Epoch 9: Train Loss: 0.0185, Test Loss: 0.0109, Train L1 Norm: 0.0424, Test L1 Norm: 0.0134, Train Linf Norm: 1.5266, Test Linf Norm: 0.2723\n",
            "Epoch 10: Train Loss: 0.0173, Test Loss: 0.0137, Train L1 Norm: 0.0427, Test L1 Norm: 0.0142, Train Linf Norm: 1.5691, Test Linf Norm: 0.2788\n",
            "Epoch 11: Train Loss: 0.0166, Test Loss: 0.0107, Train L1 Norm: 0.0411, Test L1 Norm: 0.0133, Train Linf Norm: 1.5085, Test Linf Norm: 0.2756\n",
            "Epoch 12: Train Loss: 0.0155, Test Loss: 0.0115, Train L1 Norm: 0.0373, Test L1 Norm: 0.0132, Train Linf Norm: 1.3587, Test Linf Norm: 0.2745\n",
            "Epoch 13: Train Loss: 0.0150, Test Loss: 0.0143, Train L1 Norm: 0.0366, Test L1 Norm: 0.0136, Train Linf Norm: 1.3357, Test Linf Norm: 0.2665\n",
            "Epoch 14: Train Loss: 0.0146, Test Loss: 0.0089, Train L1 Norm: 0.0366, Test L1 Norm: 0.0118, Train Linf Norm: 1.3502, Test Linf Norm: 0.2533\n",
            "Epoch 15: Train Loss: 0.0140, Test Loss: 0.0145, Train L1 Norm: 0.0338, Test L1 Norm: 0.0136, Train Linf Norm: 1.2275, Test Linf Norm: 0.2690\n",
            "Epoch 16: Train Loss: 0.0134, Test Loss: 0.0091, Train L1 Norm: 0.0335, Test L1 Norm: 0.0123, Train Linf Norm: 1.2292, Test Linf Norm: 0.2739\n",
            "Epoch 17: Train Loss: 0.0130, Test Loss: 0.0087, Train L1 Norm: 0.0335, Test L1 Norm: 0.0117, Train Linf Norm: 1.2407, Test Linf Norm: 0.2570\n",
            "Epoch 18: Train Loss: 0.0129, Test Loss: 0.0163, Train L1 Norm: 0.0300, Test L1 Norm: 0.0143, Train Linf Norm: 1.0685, Test Linf Norm: 0.2795\n",
            "Epoch 19: Train Loss: 0.0124, Test Loss: 0.0103, Train L1 Norm: 0.0323, Test L1 Norm: 0.0119, Train Linf Norm: 1.1966, Test Linf Norm: 0.2519\n",
            "Epoch 20: Train Loss: 0.0120, Test Loss: 0.0102, Train L1 Norm: 0.0319, Test L1 Norm: 0.0115, Train Linf Norm: 1.1927, Test Linf Norm: 0.2439\n",
            "Epoch 21: Train Loss: 0.0119, Test Loss: 0.0091, Train L1 Norm: 0.0285, Test L1 Norm: 0.0114, Train Linf Norm: 1.0299, Test Linf Norm: 0.2528\n",
            "Epoch 22: Train Loss: 0.0117, Test Loss: 0.0132, Train L1 Norm: 0.0303, Test L1 Norm: 0.0122, Train Linf Norm: 1.1235, Test Linf Norm: 0.2439\n",
            "Epoch 23: Train Loss: 0.0113, Test Loss: 0.0085, Train L1 Norm: 0.0304, Test L1 Norm: 0.0105, Train Linf Norm: 1.1402, Test Linf Norm: 0.2268\n",
            "Epoch 24: Train Loss: 0.0111, Test Loss: 0.0088, Train L1 Norm: 0.0272, Test L1 Norm: 0.0106, Train Linf Norm: 0.9835, Test Linf Norm: 0.2351\n",
            "Epoch 25: Train Loss: 0.0110, Test Loss: 0.0151, Train L1 Norm: 0.0283, Test L1 Norm: 0.0128, Train Linf Norm: 1.0425, Test Linf Norm: 0.2485\n",
            "Epoch 26: Train Loss: 0.0107, Test Loss: 0.0090, Train L1 Norm: 0.0263, Test L1 Norm: 0.0101, Train Linf Norm: 0.9553, Test Linf Norm: 0.2158\n",
            "Epoch 27: Train Loss: 0.0105, Test Loss: 0.0082, Train L1 Norm: 0.0275, Test L1 Norm: 0.0110, Train Linf Norm: 1.0158, Test Linf Norm: 0.2430\n",
            "Epoch 28: Train Loss: 0.0104, Test Loss: 0.0094, Train L1 Norm: 0.0262, Test L1 Norm: 0.0102, Train Linf Norm: 0.9625, Test Linf Norm: 0.2103\n",
            "Epoch 29: Train Loss: 0.0101, Test Loss: 0.0076, Train L1 Norm: 0.0274, Test L1 Norm: 0.0104, Train Linf Norm: 1.0209, Test Linf Norm: 0.2282\n",
            "Epoch 30: Train Loss: 0.0100, Test Loss: 0.0183, Train L1 Norm: 0.0269, Test L1 Norm: 0.0140, Train Linf Norm: 1.0008, Test Linf Norm: 0.2616\n",
            "Epoch 31: Train Loss: 0.0100, Test Loss: 0.0151, Train L1 Norm: 0.0256, Test L1 Norm: 0.0123, Train Linf Norm: 0.9419, Test Linf Norm: 0.2328\n",
            "Epoch 32: Train Loss: 0.0098, Test Loss: 0.0134, Train L1 Norm: 0.0266, Test L1 Norm: 0.0111, Train Linf Norm: 0.9918, Test Linf Norm: 0.2154\n",
            "Epoch 33: Train Loss: 0.0096, Test Loss: 0.0099, Train L1 Norm: 0.0262, Test L1 Norm: 0.0098, Train Linf Norm: 0.9804, Test Linf Norm: 0.1966\n",
            "Epoch 34: Train Loss: 0.0095, Test Loss: 0.0127, Train L1 Norm: 0.0263, Test L1 Norm: 0.0115, Train Linf Norm: 0.9908, Test Linf Norm: 0.2255\n",
            "Epoch 35: Train Loss: 0.0093, Test Loss: 0.0072, Train L1 Norm: 0.0240, Test L1 Norm: 0.0092, Train Linf Norm: 0.8848, Test Linf Norm: 0.2036\n",
            "Epoch 36: Train Loss: 0.0093, Test Loss: 0.0068, Train L1 Norm: 0.0247, Test L1 Norm: 0.0089, Train Linf Norm: 0.9179, Test Linf Norm: 0.1962\n",
            "Epoch 37: Train Loss: 0.0092, Test Loss: 0.0065, Train L1 Norm: 0.0252, Test L1 Norm: 0.0086, Train Linf Norm: 0.9428, Test Linf Norm: 0.1879\n",
            "Epoch 38: Train Loss: 0.0089, Test Loss: 0.0067, Train L1 Norm: 0.0254, Test L1 Norm: 0.0089, Train Linf Norm: 0.9624, Test Linf Norm: 0.1967\n",
            "Epoch 39: Train Loss: 0.0088, Test Loss: 0.0066, Train L1 Norm: 0.0230, Test L1 Norm: 0.0085, Train Linf Norm: 0.8452, Test Linf Norm: 0.1866\n",
            "Epoch 40: Train Loss: 0.0089, Test Loss: 0.0098, Train L1 Norm: 0.0247, Test L1 Norm: 0.0099, Train Linf Norm: 0.9289, Test Linf Norm: 0.2030\n",
            "Epoch 41: Train Loss: 0.0088, Test Loss: 0.0092, Train L1 Norm: 0.0245, Test L1 Norm: 0.0094, Train Linf Norm: 0.9188, Test Linf Norm: 0.1947\n",
            "Epoch 42: Train Loss: 0.0086, Test Loss: 0.0142, Train L1 Norm: 0.0239, Test L1 Norm: 0.0120, Train Linf Norm: 0.8979, Test Linf Norm: 0.2268\n",
            "Epoch 43: Train Loss: 0.0085, Test Loss: 0.0098, Train L1 Norm: 0.0232, Test L1 Norm: 0.0093, Train Linf Norm: 0.8662, Test Linf Norm: 0.1889\n",
            "Epoch 44: Train Loss: 0.0084, Test Loss: 0.0142, Train L1 Norm: 0.0235, Test L1 Norm: 0.0108, Train Linf Norm: 0.8840, Test Linf Norm: 0.1942\n",
            "Epoch 45: Train Loss: 0.0084, Test Loss: 0.0076, Train L1 Norm: 0.0233, Test L1 Norm: 0.0094, Train Linf Norm: 0.8703, Test Linf Norm: 0.2025\n",
            "Epoch 46: Train Loss: 0.0083, Test Loss: 0.0151, Train L1 Norm: 0.0238, Test L1 Norm: 0.0114, Train Linf Norm: 0.9002, Test Linf Norm: 0.2080\n",
            "Epoch 47: Train Loss: 0.0060, Test Loss: 0.0060, Train L1 Norm: 0.0222, Test L1 Norm: 0.0080, Train Linf Norm: 0.8615, Test Linf Norm: 0.1747\n",
            "Epoch 48: Train Loss: 0.0060, Test Loss: 0.0061, Train L1 Norm: 0.0222, Test L1 Norm: 0.0079, Train Linf Norm: 0.8635, Test Linf Norm: 0.1729\n",
            "Epoch 49: Train Loss: 0.0059, Test Loss: 0.0067, Train L1 Norm: 0.0228, Test L1 Norm: 0.0082, Train Linf Norm: 0.8929, Test Linf Norm: 0.1748\n",
            "Epoch 50: Train Loss: 0.0059, Test Loss: 0.0061, Train L1 Norm: 0.0224, Test L1 Norm: 0.0078, Train Linf Norm: 0.8681, Test Linf Norm: 0.1686\n",
            "Epoch 51: Train Loss: 0.0059, Test Loss: 0.0060, Train L1 Norm: 0.0215, Test L1 Norm: 0.0079, Train Linf Norm: 0.8322, Test Linf Norm: 0.1730\n",
            "Epoch 52: Train Loss: 0.0059, Test Loss: 0.0064, Train L1 Norm: 0.0214, Test L1 Norm: 0.0080, Train Linf Norm: 0.8273, Test Linf Norm: 0.1740\n",
            "Epoch 53: Train Loss: 0.0058, Test Loss: 0.0083, Train L1 Norm: 0.0214, Test L1 Norm: 0.0084, Train Linf Norm: 0.8255, Test Linf Norm: 0.1698\n",
            "Epoch 54: Train Loss: 0.0058, Test Loss: 0.0062, Train L1 Norm: 0.0217, Test L1 Norm: 0.0079, Train Linf Norm: 0.8386, Test Linf Norm: 0.1677\n",
            "Epoch 55: Train Loss: 0.0058, Test Loss: 0.0058, Train L1 Norm: 0.0213, Test L1 Norm: 0.0080, Train Linf Norm: 0.8203, Test Linf Norm: 0.1798\n",
            "Epoch 56: Train Loss: 0.0058, Test Loss: 0.0059, Train L1 Norm: 0.0215, Test L1 Norm: 0.0077, Train Linf Norm: 0.8284, Test Linf Norm: 0.1679\n",
            "Epoch 57: Train Loss: 0.0057, Test Loss: 0.0059, Train L1 Norm: 0.0215, Test L1 Norm: 0.0078, Train Linf Norm: 0.8341, Test Linf Norm: 0.1701\n",
            "Epoch 58: Train Loss: 0.0057, Test Loss: 0.0058, Train L1 Norm: 0.0209, Test L1 Norm: 0.0078, Train Linf Norm: 0.8072, Test Linf Norm: 0.1710\n",
            "Epoch 59: Train Loss: 0.0057, Test Loss: 0.0059, Train L1 Norm: 0.0214, Test L1 Norm: 0.0077, Train Linf Norm: 0.8323, Test Linf Norm: 0.1671\n",
            "Epoch 60: Train Loss: 0.0057, Test Loss: 0.0068, Train L1 Norm: 0.0210, Test L1 Norm: 0.0079, Train Linf Norm: 0.8104, Test Linf Norm: 0.1659\n",
            "Epoch 61: Train Loss: 0.0057, Test Loss: 0.0058, Train L1 Norm: 0.0212, Test L1 Norm: 0.0077, Train Linf Norm: 0.8188, Test Linf Norm: 0.1675\n",
            "Epoch 62: Train Loss: 0.0056, Test Loss: 0.0056, Train L1 Norm: 0.0211, Test L1 Norm: 0.0076, Train Linf Norm: 0.8153, Test Linf Norm: 0.1650\n",
            "Epoch 63: Train Loss: 0.0056, Test Loss: 0.0059, Train L1 Norm: 0.0210, Test L1 Norm: 0.0077, Train Linf Norm: 0.8168, Test Linf Norm: 0.1657\n",
            "Epoch 64: Train Loss: 0.0056, Test Loss: 0.0063, Train L1 Norm: 0.0212, Test L1 Norm: 0.0076, Train Linf Norm: 0.8258, Test Linf Norm: 0.1637\n",
            "Epoch 65: Train Loss: 0.0056, Test Loss: 0.0062, Train L1 Norm: 0.0217, Test L1 Norm: 0.0078, Train Linf Norm: 0.8502, Test Linf Norm: 0.1695\n",
            "Epoch 66: Train Loss: 0.0056, Test Loss: 0.0061, Train L1 Norm: 0.0215, Test L1 Norm: 0.0076, Train Linf Norm: 0.8370, Test Linf Norm: 0.1626\n",
            "Epoch 67: Train Loss: 0.0056, Test Loss: 0.0056, Train L1 Norm: 0.0207, Test L1 Norm: 0.0075, Train Linf Norm: 0.8060, Test Linf Norm: 0.1642\n",
            "Epoch 68: Train Loss: 0.0055, Test Loss: 0.0058, Train L1 Norm: 0.0213, Test L1 Norm: 0.0076, Train Linf Norm: 0.8284, Test Linf Norm: 0.1681\n",
            "Epoch 69: Train Loss: 0.0055, Test Loss: 0.0055, Train L1 Norm: 0.0207, Test L1 Norm: 0.0074, Train Linf Norm: 0.8025, Test Linf Norm: 0.1621\n",
            "Epoch 70: Train Loss: 0.0055, Test Loss: 0.0056, Train L1 Norm: 0.0211, Test L1 Norm: 0.0075, Train Linf Norm: 0.8181, Test Linf Norm: 0.1632\n",
            "Epoch 71: Train Loss: 0.0055, Test Loss: 0.0061, Train L1 Norm: 0.0208, Test L1 Norm: 0.0075, Train Linf Norm: 0.8121, Test Linf Norm: 0.1578\n",
            "Epoch 72: Train Loss: 0.0054, Test Loss: 0.0066, Train L1 Norm: 0.0207, Test L1 Norm: 0.0077, Train Linf Norm: 0.8029, Test Linf Norm: 0.1631\n",
            "Epoch 73: Train Loss: 0.0055, Test Loss: 0.0055, Train L1 Norm: 0.0205, Test L1 Norm: 0.0073, Train Linf Norm: 0.7924, Test Linf Norm: 0.1569\n",
            "Epoch 74: Train Loss: 0.0055, Test Loss: 0.0054, Train L1 Norm: 0.0206, Test L1 Norm: 0.0073, Train Linf Norm: 0.7952, Test Linf Norm: 0.1607\n",
            "Epoch 75: Train Loss: 0.0054, Test Loss: 0.0054, Train L1 Norm: 0.0206, Test L1 Norm: 0.0072, Train Linf Norm: 0.8023, Test Linf Norm: 0.1570\n",
            "Epoch 76: Train Loss: 0.0054, Test Loss: 0.0059, Train L1 Norm: 0.0207, Test L1 Norm: 0.0075, Train Linf Norm: 0.8033, Test Linf Norm: 0.1617\n",
            "Epoch 77: Train Loss: 0.0054, Test Loss: 0.0056, Train L1 Norm: 0.0206, Test L1 Norm: 0.0074, Train Linf Norm: 0.8059, Test Linf Norm: 0.1606\n",
            "Epoch 78: Train Loss: 0.0054, Test Loss: 0.0059, Train L1 Norm: 0.0204, Test L1 Norm: 0.0075, Train Linf Norm: 0.7898, Test Linf Norm: 0.1600\n",
            "Epoch 79: Train Loss: 0.0053, Test Loss: 0.0061, Train L1 Norm: 0.0206, Test L1 Norm: 0.0074, Train Linf Norm: 0.8044, Test Linf Norm: 0.1563\n",
            "Epoch 80: Train Loss: 0.0054, Test Loss: 0.0055, Train L1 Norm: 0.0201, Test L1 Norm: 0.0074, Train Linf Norm: 0.7828, Test Linf Norm: 0.1626\n",
            "Epoch 81: Train Loss: 0.0053, Test Loss: 0.0054, Train L1 Norm: 0.0202, Test L1 Norm: 0.0072, Train Linf Norm: 0.7825, Test Linf Norm: 0.1569\n",
            "Epoch 82: Train Loss: 0.0053, Test Loss: 0.0055, Train L1 Norm: 0.0203, Test L1 Norm: 0.0072, Train Linf Norm: 0.7905, Test Linf Norm: 0.1564\n",
            "Epoch 83: Train Loss: 0.0053, Test Loss: 0.0054, Train L1 Norm: 0.0198, Test L1 Norm: 0.0074, Train Linf Norm: 0.7689, Test Linf Norm: 0.1652\n",
            "Epoch 84: Train Loss: 0.0053, Test Loss: 0.0057, Train L1 Norm: 0.0199, Test L1 Norm: 0.0073, Train Linf Norm: 0.7723, Test Linf Norm: 0.1580\n",
            "Epoch 85: Train Loss: 0.0050, Test Loss: 0.0053, Train L1 Norm: 0.0201, Test L1 Norm: 0.0072, Train Linf Norm: 0.7843, Test Linf Norm: 0.1608\n",
            "Epoch 86: Train Loss: 0.0050, Test Loss: 0.0054, Train L1 Norm: 0.0201, Test L1 Norm: 0.0071, Train Linf Norm: 0.7842, Test Linf Norm: 0.1541\n",
            "Epoch 87: Train Loss: 0.0050, Test Loss: 0.0053, Train L1 Norm: 0.0199, Test L1 Norm: 0.0072, Train Linf Norm: 0.7749, Test Linf Norm: 0.1591\n",
            "Epoch 88: Train Loss: 0.0050, Test Loss: 0.0052, Train L1 Norm: 0.0198, Test L1 Norm: 0.0070, Train Linf Norm: 0.7748, Test Linf Norm: 0.1530\n",
            "Epoch 89: Train Loss: 0.0050, Test Loss: 0.0053, Train L1 Norm: 0.0200, Test L1 Norm: 0.0071, Train Linf Norm: 0.7790, Test Linf Norm: 0.1558\n",
            "Epoch 90: Train Loss: 0.0050, Test Loss: 0.0053, Train L1 Norm: 0.0197, Test L1 Norm: 0.0070, Train Linf Norm: 0.7663, Test Linf Norm: 0.1526\n",
            "Epoch 91: Train Loss: 0.0050, Test Loss: 0.0052, Train L1 Norm: 0.0199, Test L1 Norm: 0.0072, Train Linf Norm: 0.7810, Test Linf Norm: 0.1582\n",
            "Epoch 92: Train Loss: 0.0050, Test Loss: 0.0053, Train L1 Norm: 0.0198, Test L1 Norm: 0.0070, Train Linf Norm: 0.7729, Test Linf Norm: 0.1528\n",
            "Epoch 93: Train Loss: 0.0050, Test Loss: 0.0053, Train L1 Norm: 0.0200, Test L1 Norm: 0.0071, Train Linf Norm: 0.7823, Test Linf Norm: 0.1555\n",
            "Epoch 94: Train Loss: 0.0050, Test Loss: 0.0053, Train L1 Norm: 0.0198, Test L1 Norm: 0.0070, Train Linf Norm: 0.7721, Test Linf Norm: 0.1522\n",
            "Epoch 95: Train Loss: 0.0050, Test Loss: 0.0052, Train L1 Norm: 0.0200, Test L1 Norm: 0.0070, Train Linf Norm: 0.7802, Test Linf Norm: 0.1520\n",
            "Epoch 96: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0198, Test L1 Norm: 0.0070, Train Linf Norm: 0.7739, Test Linf Norm: 0.1519\n",
            "Epoch 97: Train Loss: 0.0049, Test Loss: 0.0052, Train L1 Norm: 0.0197, Test L1 Norm: 0.0071, Train Linf Norm: 0.7718, Test Linf Norm: 0.1563\n",
            "Epoch 98: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0195, Test L1 Norm: 0.0070, Train Linf Norm: 0.7616, Test Linf Norm: 0.1512\n",
            "Epoch 99: Train Loss: 0.0049, Test Loss: 0.0054, Train L1 Norm: 0.0199, Test L1 Norm: 0.0070, Train Linf Norm: 0.7778, Test Linf Norm: 0.1502\n",
            "Epoch 100: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0198, Test L1 Norm: 0.0070, Train Linf Norm: 0.7757, Test Linf Norm: 0.1512\n",
            "Epoch 101: Train Loss: 0.0049, Test Loss: 0.0052, Train L1 Norm: 0.0196, Test L1 Norm: 0.0069, Train Linf Norm: 0.7637, Test Linf Norm: 0.1500\n",
            "Epoch 102: Train Loss: 0.0049, Test Loss: 0.0052, Train L1 Norm: 0.0196, Test L1 Norm: 0.0070, Train Linf Norm: 0.7653, Test Linf Norm: 0.1523\n",
            "Epoch 103: Train Loss: 0.0049, Test Loss: 0.0052, Train L1 Norm: 0.0194, Test L1 Norm: 0.0069, Train Linf Norm: 0.7550, Test Linf Norm: 0.1510\n",
            "Epoch 104: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0197, Test L1 Norm: 0.0069, Train Linf Norm: 0.7718, Test Linf Norm: 0.1502\n",
            "Epoch 105: Train Loss: 0.0049, Test Loss: 0.0052, Train L1 Norm: 0.0194, Test L1 Norm: 0.0070, Train Linf Norm: 0.7558, Test Linf Norm: 0.1510\n",
            "Epoch 106: Train Loss: 0.0049, Test Loss: 0.0051, Train L1 Norm: 0.0196, Test L1 Norm: 0.0069, Train Linf Norm: 0.7694, Test Linf Norm: 0.1504\n",
            "Epoch 107: Train Loss: 0.0049, Test Loss: 0.0053, Train L1 Norm: 0.0194, Test L1 Norm: 0.0071, Train Linf Norm: 0.7580, Test Linf Norm: 0.1546\n",
            "Epoch 108: Train Loss: 0.0049, Test Loss: 0.0056, Train L1 Norm: 0.0196, Test L1 Norm: 0.0072, Train Linf Norm: 0.7693, Test Linf Norm: 0.1548\n",
            "Epoch 109: Train Loss: 0.0049, Test Loss: 0.0051, Train L1 Norm: 0.0195, Test L1 Norm: 0.0069, Train Linf Norm: 0.7602, Test Linf Norm: 0.1503\n",
            "Epoch 110: Train Loss: 0.0049, Test Loss: 0.0052, Train L1 Norm: 0.0195, Test L1 Norm: 0.0069, Train Linf Norm: 0.7604, Test Linf Norm: 0.1508\n",
            "Epoch 111: Train Loss: 0.0049, Test Loss: 0.0051, Train L1 Norm: 0.0191, Test L1 Norm: 0.0069, Train Linf Norm: 0.7461, Test Linf Norm: 0.1491\n",
            "Epoch 112: Train Loss: 0.0049, Test Loss: 0.0052, Train L1 Norm: 0.0192, Test L1 Norm: 0.0069, Train Linf Norm: 0.7450, Test Linf Norm: 0.1508\n",
            "Epoch 113: Train Loss: 0.0049, Test Loss: 0.0052, Train L1 Norm: 0.0192, Test L1 Norm: 0.0070, Train Linf Norm: 0.7473, Test Linf Norm: 0.1509\n",
            "Epoch 114: Train Loss: 0.0049, Test Loss: 0.0051, Train L1 Norm: 0.0191, Test L1 Norm: 0.0069, Train Linf Norm: 0.7444, Test Linf Norm: 0.1490\n",
            "Epoch 115: Train Loss: 0.0048, Test Loss: 0.0058, Train L1 Norm: 0.0193, Test L1 Norm: 0.0070, Train Linf Norm: 0.7545, Test Linf Norm: 0.1482\n",
            "Epoch 116: Train Loss: 0.0048, Test Loss: 0.0052, Train L1 Norm: 0.0195, Test L1 Norm: 0.0070, Train Linf Norm: 0.7617, Test Linf Norm: 0.1536\n",
            "Epoch 117: Train Loss: 0.0048, Test Loss: 0.0051, Train L1 Norm: 0.0197, Test L1 Norm: 0.0068, Train Linf Norm: 0.7707, Test Linf Norm: 0.1491\n",
            "Epoch 118: Train Loss: 0.0048, Test Loss: 0.0054, Train L1 Norm: 0.0195, Test L1 Norm: 0.0069, Train Linf Norm: 0.7643, Test Linf Norm: 0.1488\n",
            "Epoch 119: Train Loss: 0.0048, Test Loss: 0.0051, Train L1 Norm: 0.0193, Test L1 Norm: 0.0068, Train Linf Norm: 0.7501, Test Linf Norm: 0.1490\n",
            "Epoch 120: Train Loss: 0.0048, Test Loss: 0.0053, Train L1 Norm: 0.0191, Test L1 Norm: 0.0069, Train Linf Norm: 0.7427, Test Linf Norm: 0.1503\n",
            "Epoch 121: Train Loss: 0.0048, Test Loss: 0.0051, Train L1 Norm: 0.0189, Test L1 Norm: 0.0069, Train Linf Norm: 0.7374, Test Linf Norm: 0.1501\n",
            "Epoch 122: Train Loss: 0.0048, Test Loss: 0.0051, Train L1 Norm: 0.0192, Test L1 Norm: 0.0070, Train Linf Norm: 0.7470, Test Linf Norm: 0.1533\n",
            "Epoch 123: Train Loss: 0.0048, Test Loss: 0.0051, Train L1 Norm: 0.0193, Test L1 Norm: 0.0068, Train Linf Norm: 0.7521, Test Linf Norm: 0.1471\n",
            "Epoch 124: Train Loss: 0.0048, Test Loss: 0.0051, Train L1 Norm: 0.0192, Test L1 Norm: 0.0068, Train Linf Norm: 0.7516, Test Linf Norm: 0.1476\n",
            "Epoch 125: Train Loss: 0.0048, Test Loss: 0.0053, Train L1 Norm: 0.0195, Test L1 Norm: 0.0069, Train Linf Norm: 0.7625, Test Linf Norm: 0.1494\n",
            "Epoch 126: Train Loss: 0.0048, Test Loss: 0.0051, Train L1 Norm: 0.0191, Test L1 Norm: 0.0070, Train Linf Norm: 0.7477, Test Linf Norm: 0.1553\n",
            "Epoch 127: Train Loss: 0.0048, Test Loss: 0.0051, Train L1 Norm: 0.0191, Test L1 Norm: 0.0068, Train Linf Norm: 0.7440, Test Linf Norm: 0.1479\n",
            "Epoch 128: Train Loss: 0.0048, Test Loss: 0.0051, Train L1 Norm: 0.0192, Test L1 Norm: 0.0068, Train Linf Norm: 0.7517, Test Linf Norm: 0.1480\n",
            "Epoch 129: Train Loss: 0.0048, Test Loss: 0.0052, Train L1 Norm: 0.0190, Test L1 Norm: 0.0068, Train Linf Norm: 0.7373, Test Linf Norm: 0.1464\n",
            "Epoch 130: Train Loss: 0.0048, Test Loss: 0.0051, Train L1 Norm: 0.0189, Test L1 Norm: 0.0067, Train Linf Norm: 0.7315, Test Linf Norm: 0.1458\n",
            "Epoch 131: Train Loss: 0.0048, Test Loss: 0.0051, Train L1 Norm: 0.0192, Test L1 Norm: 0.0068, Train Linf Norm: 0.7486, Test Linf Norm: 0.1481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 13:28:15,171]\u001b[0m Trial 42 finished with value: 0.0067928646018728615 and parameters: {'n_layers': 5, 'n_units_0': 808, 'n_units_1': 1939, 'n_units_2': 597, 'n_units_3': 733, 'n_units_4': 1769, 'hidden_activation': 'LeakyReLU', 'output_activation': 'ReLU', 'loss': 'MAE', 'optimizer': 'Adagrad', 'lr': 0.00012194072348862521, 'batch_size': 48, 'n_epochs': 132, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.44624499812413243, 'patience': 8, 'threshold': 0.00018043581136551048}. Best is trial 41 with value: 0.0050436803916469215.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 132: Train Loss: 0.0048, Test Loss: 0.0051, Train L1 Norm: 0.0192, Test L1 Norm: 0.0068, Train Linf Norm: 0.7505, Test Linf Norm: 0.1469\n",
            "Epoch 1: Train Loss: 0.2298, Test Loss: 0.1013, Train L1 Norm: 0.3012, Test L1 Norm: 0.0533, Train Linf Norm: 17.7254, Test Linf Norm: 1.0337\n",
            "Epoch 2: Train Loss: 0.0687, Test Loss: 0.0428, Train L1 Norm: 0.0827, Test L1 Norm: 0.0367, Train Linf Norm: 4.4347, Test Linf Norm: 1.2156\n",
            "Epoch 3: Train Loss: 0.0504, Test Loss: 0.0658, Train L1 Norm: 0.0657, Test L1 Norm: 0.0454, Train Linf Norm: 3.6577, Test Linf Norm: 1.2459\n",
            "Epoch 4: Train Loss: 0.0421, Test Loss: 0.0249, Train L1 Norm: 0.0576, Test L1 Norm: 0.0254, Train Linf Norm: 3.2471, Test Linf Norm: 0.8500\n",
            "Epoch 5: Train Loss: 0.0352, Test Loss: 0.0614, Train L1 Norm: 0.0581, Test L1 Norm: 0.0346, Train Linf Norm: 3.6058, Test Linf Norm: 0.6232\n",
            "Epoch 6: Train Loss: 0.0317, Test Loss: 0.0333, Train L1 Norm: 0.0501, Test L1 Norm: 0.0237, Train Linf Norm: 3.0347, Test Linf Norm: 0.5622\n",
            "Epoch 7: Train Loss: 0.0290, Test Loss: 0.0301, Train L1 Norm: 0.0443, Test L1 Norm: 0.0213, Train Linf Norm: 2.6146, Test Linf Norm: 0.5286\n",
            "Epoch 8: Train Loss: 0.0261, Test Loss: 0.0285, Train L1 Norm: 0.0411, Test L1 Norm: 0.0254, Train Linf Norm: 2.4334, Test Linf Norm: 0.8340\n",
            "Epoch 9: Train Loss: 0.0243, Test Loss: 0.0626, Train L1 Norm: 0.0386, Test L1 Norm: 0.0308, Train Linf Norm: 2.2795, Test Linf Norm: 0.5686\n",
            "Epoch 10: Train Loss: 0.0230, Test Loss: 0.0206, Train L1 Norm: 0.0366, Test L1 Norm: 0.0213, Train Linf Norm: 2.1773, Test Linf Norm: 0.7196\n",
            "Epoch 11: Train Loss: 0.0220, Test Loss: 0.0209, Train L1 Norm: 0.0381, Test L1 Norm: 0.0168, Train Linf Norm: 2.3370, Test Linf Norm: 0.4700\n",
            "Epoch 12: Train Loss: 0.0205, Test Loss: 0.0304, Train L1 Norm: 0.0358, Test L1 Norm: 0.0202, Train Linf Norm: 2.1701, Test Linf Norm: 0.5046\n",
            "Epoch 13: Train Loss: 0.0202, Test Loss: 0.0169, Train L1 Norm: 0.0352, Test L1 Norm: 0.0158, Train Linf Norm: 2.1664, Test Linf Norm: 0.4746\n",
            "Epoch 14: Train Loss: 0.0189, Test Loss: 0.0292, Train L1 Norm: 0.0358, Test L1 Norm: 0.0197, Train Linf Norm: 2.2753, Test Linf Norm: 0.4805\n",
            "Epoch 15: Train Loss: 0.0179, Test Loss: 0.0159, Train L1 Norm: 0.0334, Test L1 Norm: 0.0156, Train Linf Norm: 2.0751, Test Linf Norm: 0.4516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 13:29:09,578]\u001b[0m Trial 43 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16: Train Loss: 0.0177, Test Loss: 0.0119, Train L1 Norm: 0.0332, Test L1 Norm: 0.0132, Train Linf Norm: 2.0894, Test Linf Norm: 0.4142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 13:29:14,779]\u001b[0m Trial 44 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 3.3922, Test Loss: 3.4125, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.1750, Test Loss: 0.0790, Train L1 Norm: 0.4519, Test L1 Norm: 0.0531, Train Linf Norm: 18.1501, Test Linf Norm: 0.9566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 13:29:29,069]\u001b[0m Trial 45 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 0.0619, Test Loss: 0.1166, Train L1 Norm: 0.0717, Test L1 Norm: 0.0617, Train Linf Norm: 2.1816, Test Linf Norm: 0.9477\n",
            "Epoch 1: Train Loss: 0.1435, Test Loss: 0.0458, Train L1 Norm: 0.1943, Test L1 Norm: 0.0362, Train Linf Norm: 6.1371, Test Linf Norm: 0.6806\n",
            "Epoch 2: Train Loss: 0.0541, Test Loss: 0.0349, Train L1 Norm: 0.0949, Test L1 Norm: 0.0308, Train Linf Norm: 3.3052, Test Linf Norm: 0.5962\n",
            "Epoch 3: Train Loss: 0.0396, Test Loss: 0.0227, Train L1 Norm: 0.0893, Test L1 Norm: 0.0243, Train Linf Norm: 3.3322, Test Linf Norm: 0.5549\n",
            "Epoch 4: Train Loss: 0.0329, Test Loss: 0.0265, Train L1 Norm: 0.0510, Test L1 Norm: 0.0212, Train Linf Norm: 1.6351, Test Linf Norm: 0.3644\n",
            "Epoch 5: Train Loss: 0.0284, Test Loss: 0.0262, Train L1 Norm: 0.0636, Test L1 Norm: 0.0235, Train Linf Norm: 2.3352, Test Linf Norm: 0.5149\n",
            "Epoch 6: Train Loss: 0.0254, Test Loss: 0.0232, Train L1 Norm: 0.0484, Test L1 Norm: 0.0183, Train Linf Norm: 1.6830, Test Linf Norm: 0.3315\n",
            "Epoch 7: Train Loss: 0.0233, Test Loss: 0.0354, Train L1 Norm: 0.0515, Test L1 Norm: 0.0259, Train Linf Norm: 1.8710, Test Linf Norm: 0.4756\n",
            "Epoch 8: Train Loss: 0.0216, Test Loss: 0.0230, Train L1 Norm: 0.0554, Test L1 Norm: 0.0197, Train Linf Norm: 2.1069, Test Linf Norm: 0.4028\n",
            "Epoch 9: Train Loss: 0.0202, Test Loss: 0.0137, Train L1 Norm: 0.0471, Test L1 Norm: 0.0143, Train Linf Norm: 1.7486, Test Linf Norm: 0.2988\n",
            "Epoch 10: Train Loss: 0.0189, Test Loss: 0.0457, Train L1 Norm: 0.0382, Test L1 Norm: 0.0311, Train Linf Norm: 1.3374, Test Linf Norm: 0.5937\n",
            "Epoch 11: Train Loss: 0.0181, Test Loss: 0.0190, Train L1 Norm: 0.0419, Test L1 Norm: 0.0190, Train Linf Norm: 1.3741, Test Linf Norm: 0.4216\n",
            "Epoch 12: Train Loss: 0.0170, Test Loss: 0.0116, Train L1 Norm: 0.0354, Test L1 Norm: 0.0133, Train Linf Norm: 1.2483, Test Linf Norm: 0.2859\n",
            "Epoch 13: Train Loss: 0.0165, Test Loss: 0.0182, Train L1 Norm: 0.0354, Test L1 Norm: 0.0188, Train Linf Norm: 1.2606, Test Linf Norm: 0.4546\n",
            "Epoch 14: Train Loss: 0.0159, Test Loss: 0.0101, Train L1 Norm: 0.0363, Test L1 Norm: 0.0135, Train Linf Norm: 1.3105, Test Linf Norm: 0.3303\n",
            "Epoch 15: Train Loss: 0.0153, Test Loss: 0.0114, Train L1 Norm: 0.0464, Test L1 Norm: 0.0146, Train Linf Norm: 1.8226, Test Linf Norm: 0.3693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 13:31:07,745]\u001b[0m Trial 46 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16: Train Loss: 0.0147, Test Loss: 0.0236, Train L1 Norm: 0.0319, Test L1 Norm: 0.0191, Train Linf Norm: 1.1322, Test Linf Norm: 0.3964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 13:31:09,049]\u001b[0m Trial 47 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 2.0035, Test Loss: 0.2914, Train L1 Norm: 1.8983, Test L1 Norm: 0.3074, Train Linf Norm: 1149.3442, Test Linf Norm: 66.9537\n",
            "Epoch 1: Train Loss: 0.1327, Test Loss: 0.0672, Train L1 Norm: 0.1424, Test L1 Norm: 0.0379, Train Linf Norm: 2.7961, Test Linf Norm: 0.3806\n",
            "Epoch 2: Train Loss: 0.0467, Test Loss: 0.0201, Train L1 Norm: 0.0646, Test L1 Norm: 0.0198, Train Linf Norm: 1.4244, Test Linf Norm: 0.3058\n",
            "Epoch 3: Train Loss: 0.0349, Test Loss: 0.0422, Train L1 Norm: 0.0467, Test L1 Norm: 0.0254, Train Linf Norm: 1.0068, Test Linf Norm: 0.3067\n",
            "Epoch 4: Train Loss: 0.0286, Test Loss: 0.0410, Train L1 Norm: 0.0432, Test L1 Norm: 0.0254, Train Linf Norm: 0.9785, Test Linf Norm: 0.3152\n",
            "Epoch 5: Train Loss: 0.0249, Test Loss: 0.0263, Train L1 Norm: 0.0347, Test L1 Norm: 0.0174, Train Linf Norm: 0.7568, Test Linf Norm: 0.2318\n",
            "Epoch 6: Train Loss: 0.0225, Test Loss: 0.0141, Train L1 Norm: 0.0498, Test L1 Norm: 0.0139, Train Linf Norm: 1.2729, Test Linf Norm: 0.2228\n",
            "Epoch 7: Train Loss: 0.0203, Test Loss: 0.0131, Train L1 Norm: 0.0323, Test L1 Norm: 0.0137, Train Linf Norm: 0.7437, Test Linf Norm: 0.2287\n",
            "Epoch 8: Train Loss: 0.0184, Test Loss: 0.0160, Train L1 Norm: 0.0328, Test L1 Norm: 0.0138, Train Linf Norm: 0.7855, Test Linf Norm: 0.2116\n",
            "Epoch 9: Train Loss: 0.0172, Test Loss: 0.0133, Train L1 Norm: 0.0300, Test L1 Norm: 0.0123, Train Linf Norm: 0.7107, Test Linf Norm: 0.2009\n",
            "Epoch 10: Train Loss: 0.0158, Test Loss: 0.0162, Train L1 Norm: 0.0271, Test L1 Norm: 0.0142, Train Linf Norm: 0.6330, Test Linf Norm: 0.2034\n",
            "Epoch 11: Train Loss: 0.0149, Test Loss: 0.0108, Train L1 Norm: 0.0226, Test L1 Norm: 0.0109, Train Linf Norm: 0.5064, Test Linf Norm: 0.1774\n",
            "Epoch 12: Train Loss: 0.0136, Test Loss: 0.0093, Train L1 Norm: 0.0243, Test L1 Norm: 0.0101, Train Linf Norm: 0.5730, Test Linf Norm: 0.1655\n",
            "Epoch 13: Train Loss: 0.0129, Test Loss: 0.0125, Train L1 Norm: 0.0236, Test L1 Norm: 0.0109, Train Linf Norm: 0.5575, Test Linf Norm: 0.1652\n",
            "Epoch 14: Train Loss: 0.0122, Test Loss: 0.0110, Train L1 Norm: 0.0226, Test L1 Norm: 0.0101, Train Linf Norm: 0.5345, Test Linf Norm: 0.1608\n",
            "Epoch 15: Train Loss: 0.0115, Test Loss: 0.0083, Train L1 Norm: 0.0222, Test L1 Norm: 0.0099, Train Linf Norm: 0.5350, Test Linf Norm: 0.1745\n",
            "Epoch 16: Train Loss: 0.0108, Test Loss: 0.0099, Train L1 Norm: 0.0233, Test L1 Norm: 0.0112, Train Linf Norm: 0.5726, Test Linf Norm: 0.1882\n",
            "Epoch 17: Train Loss: 0.0104, Test Loss: 0.0106, Train L1 Norm: 0.0236, Test L1 Norm: 0.0105, Train Linf Norm: 0.5921, Test Linf Norm: 0.1711\n",
            "Epoch 18: Train Loss: 0.0099, Test Loss: 0.0167, Train L1 Norm: 0.0200, Test L1 Norm: 0.0124, Train Linf Norm: 0.4814, Test Linf Norm: 0.1873\n",
            "Epoch 19: Train Loss: 0.0095, Test Loss: 0.0074, Train L1 Norm: 0.0226, Test L1 Norm: 0.0089, Train Linf Norm: 0.5709, Test Linf Norm: 0.1581\n",
            "Epoch 20: Train Loss: 0.0090, Test Loss: 0.0070, Train L1 Norm: 0.0183, Test L1 Norm: 0.0082, Train Linf Norm: 0.4383, Test Linf Norm: 0.1384\n",
            "Epoch 21: Train Loss: 0.0086, Test Loss: 0.0070, Train L1 Norm: 0.0213, Test L1 Norm: 0.0086, Train Linf Norm: 0.5406, Test Linf Norm: 0.1491\n",
            "Epoch 22: Train Loss: 0.0083, Test Loss: 0.0072, Train L1 Norm: 0.0189, Test L1 Norm: 0.0080, Train Linf Norm: 0.4647, Test Linf Norm: 0.1309\n",
            "Epoch 23: Train Loss: 0.0080, Test Loss: 0.0071, Train L1 Norm: 0.0185, Test L1 Norm: 0.0079, Train Linf Norm: 0.4565, Test Linf Norm: 0.1297\n",
            "Epoch 24: Train Loss: 0.0078, Test Loss: 0.0071, Train L1 Norm: 0.0178, Test L1 Norm: 0.0079, Train Linf Norm: 0.4380, Test Linf Norm: 0.1321\n",
            "Epoch 25: Train Loss: 0.0075, Test Loss: 0.0068, Train L1 Norm: 0.0163, Test L1 Norm: 0.0079, Train Linf Norm: 0.3937, Test Linf Norm: 0.1328\n",
            "Epoch 26: Train Loss: 0.0074, Test Loss: 0.0078, Train L1 Norm: 0.0215, Test L1 Norm: 0.0081, Train Linf Norm: 0.5602, Test Linf Norm: 0.1301\n",
            "Epoch 27: Train Loss: 0.0072, Test Loss: 0.0123, Train L1 Norm: 0.0167, Test L1 Norm: 0.0091, Train Linf Norm: 0.4088, Test Linf Norm: 0.1366\n",
            "Epoch 28: Train Loss: 0.0071, Test Loss: 0.0076, Train L1 Norm: 0.0157, Test L1 Norm: 0.0077, Train Linf Norm: 0.3768, Test Linf Norm: 0.1233\n",
            "Epoch 29: Train Loss: 0.0070, Test Loss: 0.0066, Train L1 Norm: 0.0164, Test L1 Norm: 0.0082, Train Linf Norm: 0.4032, Test Linf Norm: 0.1431\n",
            "Epoch 30: Train Loss: 0.0068, Test Loss: 0.0063, Train L1 Norm: 0.0162, Test L1 Norm: 0.0075, Train Linf Norm: 0.3981, Test Linf Norm: 0.1282\n",
            "Epoch 31: Train Loss: 0.0068, Test Loss: 0.0072, Train L1 Norm: 0.0156, Test L1 Norm: 0.0080, Train Linf Norm: 0.3804, Test Linf Norm: 0.1389\n",
            "Epoch 32: Train Loss: 0.0067, Test Loss: 0.0079, Train L1 Norm: 0.0147, Test L1 Norm: 0.0081, Train Linf Norm: 0.3559, Test Linf Norm: 0.1365\n",
            "Epoch 33: Train Loss: 0.0067, Test Loss: 0.0062, Train L1 Norm: 0.0153, Test L1 Norm: 0.0075, Train Linf Norm: 0.3735, Test Linf Norm: 0.1284\n",
            "Epoch 34: Train Loss: 0.0066, Test Loss: 0.0061, Train L1 Norm: 0.0148, Test L1 Norm: 0.0074, Train Linf Norm: 0.3565, Test Linf Norm: 0.1260\n",
            "Epoch 35: Train Loss: 0.0066, Test Loss: 0.0064, Train L1 Norm: 0.0149, Test L1 Norm: 0.0076, Train Linf Norm: 0.3605, Test Linf Norm: 0.1301\n",
            "Epoch 36: Train Loss: 0.0066, Test Loss: 0.0066, Train L1 Norm: 0.0147, Test L1 Norm: 0.0078, Train Linf Norm: 0.3555, Test Linf Norm: 0.1350\n",
            "Epoch 37: Train Loss: 0.0066, Test Loss: 0.0101, Train L1 Norm: 0.0144, Test L1 Norm: 0.0095, Train Linf Norm: 0.3444, Test Linf Norm: 0.1545\n",
            "Epoch 38: Train Loss: 0.0066, Test Loss: 0.0065, Train L1 Norm: 0.0161, Test L1 Norm: 0.0073, Train Linf Norm: 0.4023, Test Linf Norm: 0.1204\n",
            "Epoch 39: Train Loss: 0.0066, Test Loss: 0.0063, Train L1 Norm: 0.0140, Test L1 Norm: 0.0071, Train Linf Norm: 0.3344, Test Linf Norm: 0.1179\n",
            "Epoch 40: Train Loss: 0.0067, Test Loss: 0.0068, Train L1 Norm: 0.0136, Test L1 Norm: 0.0072, Train Linf Norm: 0.3210, Test Linf Norm: 0.1207\n",
            "Epoch 41: Train Loss: 0.0068, Test Loss: 0.0083, Train L1 Norm: 0.0144, Test L1 Norm: 0.0078, Train Linf Norm: 0.3456, Test Linf Norm: 0.1250\n",
            "Epoch 42: Train Loss: 0.0069, Test Loss: 0.0066, Train L1 Norm: 0.0134, Test L1 Norm: 0.0070, Train Linf Norm: 0.3129, Test Linf Norm: 0.1113\n",
            "Epoch 43: Train Loss: 0.0070, Test Loss: 0.0061, Train L1 Norm: 0.0131, Test L1 Norm: 0.0068, Train Linf Norm: 0.3036, Test Linf Norm: 0.1138\n",
            "Epoch 44: Train Loss: 0.0071, Test Loss: 0.0113, Train L1 Norm: 0.0139, Test L1 Norm: 0.0081, Train Linf Norm: 0.3261, Test Linf Norm: 0.1146\n",
            "Epoch 45: Train Loss: 0.0072, Test Loss: 0.0087, Train L1 Norm: 0.0137, Test L1 Norm: 0.0077, Train Linf Norm: 0.3222, Test Linf Norm: 0.1155\n",
            "Epoch 46: Train Loss: 0.0073, Test Loss: 0.0102, Train L1 Norm: 0.0126, Test L1 Norm: 0.0082, Train Linf Norm: 0.2854, Test Linf Norm: 0.1185\n",
            "Epoch 47: Train Loss: 0.0075, Test Loss: 0.0074, Train L1 Norm: 0.0143, Test L1 Norm: 0.0071, Train Linf Norm: 0.3402, Test Linf Norm: 0.1145\n",
            "Epoch 48: Train Loss: 0.0076, Test Loss: 0.0071, Train L1 Norm: 0.0148, Test L1 Norm: 0.0072, Train Linf Norm: 0.3530, Test Linf Norm: 0.1199\n",
            "Epoch 49: Train Loss: 0.0077, Test Loss: 0.0060, Train L1 Norm: 0.0140, Test L1 Norm: 0.0072, Train Linf Norm: 0.3286, Test Linf Norm: 0.1265\n",
            "Epoch 50: Train Loss: 0.0079, Test Loss: 0.0128, Train L1 Norm: 0.0128, Test L1 Norm: 0.0089, Train Linf Norm: 0.2860, Test Linf Norm: 0.1290\n",
            "Epoch 51: Train Loss: 0.0081, Test Loss: 0.0099, Train L1 Norm: 0.0149, Test L1 Norm: 0.0079, Train Linf Norm: 0.3524, Test Linf Norm: 0.1218\n",
            "Epoch 52: Train Loss: 0.0082, Test Loss: 0.0073, Train L1 Norm: 0.0134, Test L1 Norm: 0.0071, Train Linf Norm: 0.3015, Test Linf Norm: 0.1145\n",
            "Epoch 53: Train Loss: 0.0082, Test Loss: 0.0115, Train L1 Norm: 0.0140, Test L1 Norm: 0.0091, Train Linf Norm: 0.3219, Test Linf Norm: 0.1303\n",
            "Epoch 54: Train Loss: 0.0084, Test Loss: 0.0059, Train L1 Norm: 0.0140, Test L1 Norm: 0.0070, Train Linf Norm: 0.3209, Test Linf Norm: 0.1192\n",
            "Epoch 55: Train Loss: 0.0084, Test Loss: 0.0073, Train L1 Norm: 0.0136, Test L1 Norm: 0.0073, Train Linf Norm: 0.3084, Test Linf Norm: 0.1186\n",
            "Epoch 56: Train Loss: 0.0086, Test Loss: 0.0099, Train L1 Norm: 0.0129, Test L1 Norm: 0.0082, Train Linf Norm: 0.2842, Test Linf Norm: 0.1272\n",
            "Epoch 57: Train Loss: 0.0086, Test Loss: 0.0119, Train L1 Norm: 0.0152, Test L1 Norm: 0.0093, Train Linf Norm: 0.3596, Test Linf Norm: 0.1427\n",
            "Epoch 58: Train Loss: 0.0086, Test Loss: 0.0381, Train L1 Norm: 0.0143, Test L1 Norm: 0.0188, Train Linf Norm: 0.3283, Test Linf Norm: 0.1941\n",
            "Epoch 59: Train Loss: 0.0085, Test Loss: 0.0145, Train L1 Norm: 0.0128, Test L1 Norm: 0.0090, Train Linf Norm: 0.2806, Test Linf Norm: 0.1179\n",
            "Epoch 60: Train Loss: 0.0085, Test Loss: 0.0085, Train L1 Norm: 0.0113, Test L1 Norm: 0.0076, Train Linf Norm: 0.2354, Test Linf Norm: 0.1203\n",
            "Epoch 61: Train Loss: 0.0084, Test Loss: 0.0095, Train L1 Norm: 0.0120, Test L1 Norm: 0.0077, Train Linf Norm: 0.2581, Test Linf Norm: 0.1053\n",
            "Epoch 62: Train Loss: 0.0085, Test Loss: 0.0058, Train L1 Norm: 0.0121, Test L1 Norm: 0.0066, Train Linf Norm: 0.2636, Test Linf Norm: 0.1117\n",
            "Epoch 63: Train Loss: 0.0083, Test Loss: 0.0113, Train L1 Norm: 0.0121, Test L1 Norm: 0.0085, Train Linf Norm: 0.2628, Test Linf Norm: 0.1225\n",
            "Epoch 64: Train Loss: 0.0084, Test Loss: 0.0056, Train L1 Norm: 0.0113, Test L1 Norm: 0.0062, Train Linf Norm: 0.2392, Test Linf Norm: 0.1013\n",
            "Epoch 65: Train Loss: 0.0082, Test Loss: 0.0063, Train L1 Norm: 0.0112, Test L1 Norm: 0.0071, Train Linf Norm: 0.2393, Test Linf Norm: 0.1245\n",
            "Epoch 66: Train Loss: 0.0080, Test Loss: 0.0146, Train L1 Norm: 0.0119, Test L1 Norm: 0.0088, Train Linf Norm: 0.2617, Test Linf Norm: 0.1110\n",
            "Epoch 67: Train Loss: 0.0078, Test Loss: 0.0073, Train L1 Norm: 0.0109, Test L1 Norm: 0.0070, Train Linf Norm: 0.2329, Test Linf Norm: 0.1110\n",
            "Epoch 68: Train Loss: 0.0077, Test Loss: 0.0059, Train L1 Norm: 0.0110, Test L1 Norm: 0.0072, Train Linf Norm: 0.2352, Test Linf Norm: 0.1299\n",
            "Epoch 69: Train Loss: 0.0075, Test Loss: 0.0067, Train L1 Norm: 0.0115, Test L1 Norm: 0.0065, Train Linf Norm: 0.2542, Test Linf Norm: 0.1047\n",
            "Epoch 70: Train Loss: 0.0073, Test Loss: 0.0060, Train L1 Norm: 0.0119, Test L1 Norm: 0.0061, Train Linf Norm: 0.2674, Test Linf Norm: 0.0958\n",
            "Epoch 71: Train Loss: 0.0072, Test Loss: 0.0063, Train L1 Norm: 0.0116, Test L1 Norm: 0.0067, Train Linf Norm: 0.2596, Test Linf Norm: 0.1100\n",
            "Epoch 72: Train Loss: 0.0070, Test Loss: 0.0056, Train L1 Norm: 0.0117, Test L1 Norm: 0.0062, Train Linf Norm: 0.2675, Test Linf Norm: 0.1059\n",
            "Epoch 73: Train Loss: 0.0068, Test Loss: 0.0077, Train L1 Norm: 0.0115, Test L1 Norm: 0.0069, Train Linf Norm: 0.2622, Test Linf Norm: 0.1066\n",
            "Epoch 74: Train Loss: 0.0066, Test Loss: 0.0066, Train L1 Norm: 0.0132, Test L1 Norm: 0.0069, Train Linf Norm: 0.3166, Test Linf Norm: 0.1149\n",
            "Epoch 75: Train Loss: 0.0064, Test Loss: 0.0084, Train L1 Norm: 0.0106, Test L1 Norm: 0.0072, Train Linf Norm: 0.2371, Test Linf Norm: 0.1085\n",
            "Epoch 76: Train Loss: 0.0063, Test Loss: 0.0095, Train L1 Norm: 0.0101, Test L1 Norm: 0.0074, Train Linf Norm: 0.2249, Test Linf Norm: 0.1109\n",
            "Epoch 77: Train Loss: 0.0060, Test Loss: 0.0081, Train L1 Norm: 0.0091, Test L1 Norm: 0.0067, Train Linf Norm: 0.1926, Test Linf Norm: 0.1025\n",
            "Epoch 78: Train Loss: 0.0059, Test Loss: 0.0061, Train L1 Norm: 0.0089, Test L1 Norm: 0.0060, Train Linf Norm: 0.1878, Test Linf Norm: 0.0973\n",
            "Epoch 79: Train Loss: 0.0058, Test Loss: 0.0057, Train L1 Norm: 0.0108, Test L1 Norm: 0.0069, Train Linf Norm: 0.2502, Test Linf Norm: 0.1235\n",
            "Epoch 80: Train Loss: 0.0055, Test Loss: 0.0049, Train L1 Norm: 0.0089, Test L1 Norm: 0.0066, Train Linf Norm: 0.1930, Test Linf Norm: 0.1186\n",
            "Epoch 81: Train Loss: 0.0055, Test Loss: 0.0062, Train L1 Norm: 0.0089, Test L1 Norm: 0.0060, Train Linf Norm: 0.1940, Test Linf Norm: 0.0944\n",
            "Epoch 82: Train Loss: 0.0053, Test Loss: 0.0050, Train L1 Norm: 0.0096, Test L1 Norm: 0.0060, Train Linf Norm: 0.2181, Test Linf Norm: 0.1021\n",
            "Epoch 83: Train Loss: 0.0052, Test Loss: 0.0051, Train L1 Norm: 0.0095, Test L1 Norm: 0.0057, Train Linf Norm: 0.2163, Test Linf Norm: 0.0952\n",
            "Epoch 84: Train Loss: 0.0051, Test Loss: 0.0050, Train L1 Norm: 0.0090, Test L1 Norm: 0.0062, Train Linf Norm: 0.2012, Test Linf Norm: 0.1059\n",
            "Epoch 85: Train Loss: 0.0050, Test Loss: 0.0051, Train L1 Norm: 0.0087, Test L1 Norm: 0.0058, Train Linf Norm: 0.1924, Test Linf Norm: 0.0983\n",
            "Epoch 86: Train Loss: 0.0050, Test Loss: 0.0057, Train L1 Norm: 0.0092, Test L1 Norm: 0.0059, Train Linf Norm: 0.2100, Test Linf Norm: 0.0975\n",
            "Epoch 87: Train Loss: 0.0049, Test Loss: 0.0048, Train L1 Norm: 0.0084, Test L1 Norm: 0.0063, Train Linf Norm: 0.1850, Test Linf Norm: 0.1122\n",
            "Epoch 88: Train Loss: 0.0048, Test Loss: 0.0047, Train L1 Norm: 0.0087, Test L1 Norm: 0.0057, Train Linf Norm: 0.1935, Test Linf Norm: 0.0956\n",
            "Epoch 89: Train Loss: 0.0047, Test Loss: 0.0053, Train L1 Norm: 0.0084, Test L1 Norm: 0.0057, Train Linf Norm: 0.1873, Test Linf Norm: 0.0933\n",
            "Epoch 90: Train Loss: 0.0047, Test Loss: 0.0054, Train L1 Norm: 0.0082, Test L1 Norm: 0.0058, Train Linf Norm: 0.1802, Test Linf Norm: 0.0949\n",
            "Epoch 91: Train Loss: 0.0047, Test Loss: 0.0053, Train L1 Norm: 0.0084, Test L1 Norm: 0.0057, Train Linf Norm: 0.1862, Test Linf Norm: 0.0949\n",
            "Epoch 92: Train Loss: 0.0047, Test Loss: 0.0047, Train L1 Norm: 0.0084, Test L1 Norm: 0.0055, Train Linf Norm: 0.1872, Test Linf Norm: 0.0930\n",
            "Epoch 93: Train Loss: 0.0046, Test Loss: 0.0050, Train L1 Norm: 0.0082, Test L1 Norm: 0.0061, Train Linf Norm: 0.1835, Test Linf Norm: 0.1054\n",
            "Epoch 94: Train Loss: 0.0046, Test Loss: 0.0057, Train L1 Norm: 0.0081, Test L1 Norm: 0.0057, Train Linf Norm: 0.1797, Test Linf Norm: 0.0923\n",
            "Epoch 95: Train Loss: 0.0046, Test Loss: 0.0055, Train L1 Norm: 0.0083, Test L1 Norm: 0.0057, Train Linf Norm: 0.1832, Test Linf Norm: 0.0949\n",
            "Epoch 96: Train Loss: 0.0046, Test Loss: 0.0046, Train L1 Norm: 0.0079, Test L1 Norm: 0.0055, Train Linf Norm: 0.1705, Test Linf Norm: 0.0946\n",
            "Epoch 97: Train Loss: 0.0046, Test Loss: 0.0046, Train L1 Norm: 0.0078, Test L1 Norm: 0.0055, Train Linf Norm: 0.1695, Test Linf Norm: 0.0928\n",
            "Epoch 98: Train Loss: 0.0046, Test Loss: 0.0046, Train L1 Norm: 0.0078, Test L1 Norm: 0.0055, Train Linf Norm: 0.1693, Test Linf Norm: 0.0932\n",
            "Epoch 99: Train Loss: 0.0046, Test Loss: 0.0047, Train L1 Norm: 0.0076, Test L1 Norm: 0.0054, Train Linf Norm: 0.1636, Test Linf Norm: 0.0900\n",
            "Epoch 100: Train Loss: 0.0046, Test Loss: 0.0048, Train L1 Norm: 0.0072, Test L1 Norm: 0.0055, Train Linf Norm: 0.1495, Test Linf Norm: 0.0907\n",
            "Epoch 101: Train Loss: 0.0046, Test Loss: 0.0049, Train L1 Norm: 0.0080, Test L1 Norm: 0.0059, Train Linf Norm: 0.1741, Test Linf Norm: 0.1047\n",
            "Epoch 102: Train Loss: 0.0047, Test Loss: 0.0047, Train L1 Norm: 0.0078, Test L1 Norm: 0.0053, Train Linf Norm: 0.1668, Test Linf Norm: 0.0878\n",
            "Epoch 103: Train Loss: 0.0048, Test Loss: 0.0055, Train L1 Norm: 0.0084, Test L1 Norm: 0.0059, Train Linf Norm: 0.1861, Test Linf Norm: 0.1007\n",
            "Epoch 104: Train Loss: 0.0049, Test Loss: 0.0049, Train L1 Norm: 0.0086, Test L1 Norm: 0.0054, Train Linf Norm: 0.1920, Test Linf Norm: 0.0879\n",
            "Epoch 105: Train Loss: 0.0049, Test Loss: 0.0046, Train L1 Norm: 0.0081, Test L1 Norm: 0.0057, Train Linf Norm: 0.1757, Test Linf Norm: 0.0995\n",
            "Epoch 106: Train Loss: 0.0050, Test Loss: 0.0051, Train L1 Norm: 0.0078, Test L1 Norm: 0.0055, Train Linf Norm: 0.1658, Test Linf Norm: 0.0887\n",
            "Epoch 107: Train Loss: 0.0051, Test Loss: 0.0050, Train L1 Norm: 0.0073, Test L1 Norm: 0.0056, Train Linf Norm: 0.1482, Test Linf Norm: 0.0918\n",
            "Epoch 108: Train Loss: 0.0051, Test Loss: 0.0050, Train L1 Norm: 0.0084, Test L1 Norm: 0.0058, Train Linf Norm: 0.1841, Test Linf Norm: 0.1004\n",
            "Epoch 109: Train Loss: 0.0052, Test Loss: 0.0057, Train L1 Norm: 0.0078, Test L1 Norm: 0.0058, Train Linf Norm: 0.1631, Test Linf Norm: 0.0928\n",
            "Epoch 110: Train Loss: 0.0053, Test Loss: 0.0047, Train L1 Norm: 0.0080, Test L1 Norm: 0.0053, Train Linf Norm: 0.1694, Test Linf Norm: 0.0861\n",
            "Epoch 111: Train Loss: 0.0054, Test Loss: 0.0050, Train L1 Norm: 0.0080, Test L1 Norm: 0.0054, Train Linf Norm: 0.1689, Test Linf Norm: 0.0899\n",
            "Epoch 112: Train Loss: 0.0055, Test Loss: 0.0098, Train L1 Norm: 0.0095, Test L1 Norm: 0.0074, Train Linf Norm: 0.2138, Test Linf Norm: 0.1098\n",
            "Epoch 113: Train Loss: 0.0056, Test Loss: 0.0047, Train L1 Norm: 0.0083, Test L1 Norm: 0.0054, Train Linf Norm: 0.1761, Test Linf Norm: 0.0909\n",
            "Epoch 114: Train Loss: 0.0057, Test Loss: 0.0047, Train L1 Norm: 0.0087, Test L1 Norm: 0.0054, Train Linf Norm: 0.1878, Test Linf Norm: 0.0905\n",
            "Epoch 115: Train Loss: 0.0058, Test Loss: 0.0054, Train L1 Norm: 0.0090, Test L1 Norm: 0.0062, Train Linf Norm: 0.1950, Test Linf Norm: 0.1029\n",
            "Epoch 116: Train Loss: 0.0059, Test Loss: 0.0051, Train L1 Norm: 0.0083, Test L1 Norm: 0.0066, Train Linf Norm: 0.1728, Test Linf Norm: 0.1176\n",
            "Epoch 117: Train Loss: 0.0060, Test Loss: 0.0064, Train L1 Norm: 0.0083, Test L1 Norm: 0.0064, Train Linf Norm: 0.1727, Test Linf Norm: 0.1097\n",
            "Epoch 118: Train Loss: 0.0061, Test Loss: 0.0047, Train L1 Norm: 0.0080, Test L1 Norm: 0.0055, Train Linf Norm: 0.1624, Test Linf Norm: 0.0932\n",
            "Epoch 119: Train Loss: 0.0062, Test Loss: 0.0095, Train L1 Norm: 0.0085, Test L1 Norm: 0.0064, Train Linf Norm: 0.1781, Test Linf Norm: 0.0879\n",
            "Epoch 120: Train Loss: 0.0062, Test Loss: 0.0084, Train L1 Norm: 0.0080, Test L1 Norm: 0.0062, Train Linf Norm: 0.1625, Test Linf Norm: 0.0906\n",
            "Epoch 121: Train Loss: 0.0062, Test Loss: 0.0055, Train L1 Norm: 0.0083, Test L1 Norm: 0.0056, Train Linf Norm: 0.1689, Test Linf Norm: 0.0907\n",
            "Epoch 122: Train Loss: 0.0063, Test Loss: 0.0046, Train L1 Norm: 0.0084, Test L1 Norm: 0.0054, Train Linf Norm: 0.1735, Test Linf Norm: 0.0915\n",
            "Epoch 123: Train Loss: 0.0063, Test Loss: 0.0066, Train L1 Norm: 0.0082, Test L1 Norm: 0.0067, Train Linf Norm: 0.1673, Test Linf Norm: 0.1100\n",
            "Epoch 124: Train Loss: 0.0063, Test Loss: 0.0135, Train L1 Norm: 0.0087, Test L1 Norm: 0.0123, Train Linf Norm: 0.1832, Test Linf Norm: 0.2113\n",
            "Epoch 125: Train Loss: 0.0063, Test Loss: 0.0064, Train L1 Norm: 0.0071, Test L1 Norm: 0.0055, Train Linf Norm: 0.1319, Test Linf Norm: 0.0856\n",
            "Epoch 126: Train Loss: 0.0063, Test Loss: 0.0048, Train L1 Norm: 0.0075, Test L1 Norm: 0.0055, Train Linf Norm: 0.1451, Test Linf Norm: 0.0916\n",
            "Epoch 127: Train Loss: 0.0062, Test Loss: 0.0046, Train L1 Norm: 0.0076, Test L1 Norm: 0.0058, Train Linf Norm: 0.1491, Test Linf Norm: 0.1036\n",
            "Epoch 128: Train Loss: 0.0061, Test Loss: 0.0046, Train L1 Norm: 0.0085, Test L1 Norm: 0.0052, Train Linf Norm: 0.1787, Test Linf Norm: 0.0846\n",
            "Epoch 129: Train Loss: 0.0060, Test Loss: 0.0064, Train L1 Norm: 0.0074, Test L1 Norm: 0.0060, Train Linf Norm: 0.1459, Test Linf Norm: 0.0979\n",
            "Epoch 130: Train Loss: 0.0059, Test Loss: 0.0059, Train L1 Norm: 0.0079, Test L1 Norm: 0.0060, Train Linf Norm: 0.1627, Test Linf Norm: 0.0997\n",
            "Epoch 131: Train Loss: 0.0059, Test Loss: 0.0048, Train L1 Norm: 0.0068, Test L1 Norm: 0.0051, Train Linf Norm: 0.1261, Test Linf Norm: 0.0812\n",
            "Epoch 132: Train Loss: 0.0057, Test Loss: 0.0055, Train L1 Norm: 0.0072, Test L1 Norm: 0.0055, Train Linf Norm: 0.1425, Test Linf Norm: 0.0864\n",
            "Epoch 133: Train Loss: 0.0055, Test Loss: 0.0050, Train L1 Norm: 0.0082, Test L1 Norm: 0.0054, Train Linf Norm: 0.1747, Test Linf Norm: 0.0896\n",
            "Epoch 134: Train Loss: 0.0054, Test Loss: 0.0058, Train L1 Norm: 0.0075, Test L1 Norm: 0.0061, Train Linf Norm: 0.1548, Test Linf Norm: 0.1044\n",
            "Epoch 135: Train Loss: 0.0054, Test Loss: 0.0049, Train L1 Norm: 0.0075, Test L1 Norm: 0.0051, Train Linf Norm: 0.1541, Test Linf Norm: 0.0820\n",
            "Epoch 136: Train Loss: 0.0053, Test Loss: 0.0043, Train L1 Norm: 0.0073, Test L1 Norm: 0.0056, Train Linf Norm: 0.1498, Test Linf Norm: 0.0962\n",
            "Epoch 137: Train Loss: 0.0051, Test Loss: 0.0058, Train L1 Norm: 0.0062, Test L1 Norm: 0.0060, Train Linf Norm: 0.1159, Test Linf Norm: 0.0964\n",
            "Epoch 138: Train Loss: 0.0050, Test Loss: 0.0044, Train L1 Norm: 0.0069, Test L1 Norm: 0.0051, Train Linf Norm: 0.1392, Test Linf Norm: 0.0836\n",
            "Epoch 139: Train Loss: 0.0048, Test Loss: 0.0048, Train L1 Norm: 0.0075, Test L1 Norm: 0.0051, Train Linf Norm: 0.1580, Test Linf Norm: 0.0816\n",
            "Epoch 140: Train Loss: 0.0047, Test Loss: 0.0082, Train L1 Norm: 0.0067, Test L1 Norm: 0.0067, Train Linf Norm: 0.1353, Test Linf Norm: 0.1009\n",
            "Epoch 141: Train Loss: 0.0047, Test Loss: 0.0047, Train L1 Norm: 0.0067, Test L1 Norm: 0.0054, Train Linf Norm: 0.1351, Test Linf Norm: 0.0893\n",
            "Epoch 142: Train Loss: 0.0046, Test Loss: 0.0050, Train L1 Norm: 0.0067, Test L1 Norm: 0.0053, Train Linf Norm: 0.1402, Test Linf Norm: 0.0862\n",
            "Epoch 143: Train Loss: 0.0045, Test Loss: 0.0043, Train L1 Norm: 0.0070, Test L1 Norm: 0.0051, Train Linf Norm: 0.1503, Test Linf Norm: 0.0874\n",
            "Epoch 144: Train Loss: 0.0044, Test Loss: 0.0042, Train L1 Norm: 0.0072, Test L1 Norm: 0.0050, Train Linf Norm: 0.1545, Test Linf Norm: 0.0836\n",
            "Epoch 145: Train Loss: 0.0043, Test Loss: 0.0045, Train L1 Norm: 0.0063, Test L1 Norm: 0.0052, Train Linf Norm: 0.1281, Test Linf Norm: 0.0869\n",
            "Epoch 146: Train Loss: 0.0042, Test Loss: 0.0047, Train L1 Norm: 0.0061, Test L1 Norm: 0.0050, Train Linf Norm: 0.1227, Test Linf Norm: 0.0793\n",
            "Epoch 147: Train Loss: 0.0042, Test Loss: 0.0051, Train L1 Norm: 0.0068, Test L1 Norm: 0.0052, Train Linf Norm: 0.1451, Test Linf Norm: 0.0843\n",
            "Epoch 148: Train Loss: 0.0041, Test Loss: 0.0045, Train L1 Norm: 0.0064, Test L1 Norm: 0.0049, Train Linf Norm: 0.1335, Test Linf Norm: 0.0798\n",
            "Epoch 149: Train Loss: 0.0041, Test Loss: 0.0046, Train L1 Norm: 0.0062, Test L1 Norm: 0.0050, Train Linf Norm: 0.1259, Test Linf Norm: 0.0811\n",
            "Epoch 150: Train Loss: 0.0040, Test Loss: 0.0046, Train L1 Norm: 0.0061, Test L1 Norm: 0.0049, Train Linf Norm: 0.1249, Test Linf Norm: 0.0784\n",
            "Epoch 151: Train Loss: 0.0040, Test Loss: 0.0044, Train L1 Norm: 0.0055, Test L1 Norm: 0.0050, Train Linf Norm: 0.1049, Test Linf Norm: 0.0817\n",
            "Epoch 152: Train Loss: 0.0040, Test Loss: 0.0044, Train L1 Norm: 0.0063, Test L1 Norm: 0.0051, Train Linf Norm: 0.1312, Test Linf Norm: 0.0861\n",
            "Epoch 153: Train Loss: 0.0039, Test Loss: 0.0041, Train L1 Norm: 0.0061, Test L1 Norm: 0.0050, Train Linf Norm: 0.1234, Test Linf Norm: 0.0833\n",
            "Epoch 154: Train Loss: 0.0039, Test Loss: 0.0046, Train L1 Norm: 0.0061, Test L1 Norm: 0.0050, Train Linf Norm: 0.1278, Test Linf Norm: 0.0794\n",
            "Epoch 155: Train Loss: 0.0039, Test Loss: 0.0042, Train L1 Norm: 0.0065, Test L1 Norm: 0.0048, Train Linf Norm: 0.1384, Test Linf Norm: 0.0789\n",
            "Epoch 156: Train Loss: 0.0039, Test Loss: 0.0044, Train L1 Norm: 0.0060, Test L1 Norm: 0.0049, Train Linf Norm: 0.1232, Test Linf Norm: 0.0795\n",
            "Epoch 157: Train Loss: 0.0039, Test Loss: 0.0075, Train L1 Norm: 0.0063, Test L1 Norm: 0.0059, Train Linf Norm: 0.1319, Test Linf Norm: 0.0890\n",
            "Epoch 158: Train Loss: 0.0039, Test Loss: 0.0041, Train L1 Norm: 0.0056, Test L1 Norm: 0.0051, Train Linf Norm: 0.1106, Test Linf Norm: 0.0883\n",
            "Epoch 159: Train Loss: 0.0039, Test Loss: 0.0044, Train L1 Norm: 0.0063, Test L1 Norm: 0.0049, Train Linf Norm: 0.1329, Test Linf Norm: 0.0806\n",
            "Epoch 160: Train Loss: 0.0039, Test Loss: 0.0042, Train L1 Norm: 0.0063, Test L1 Norm: 0.0049, Train Linf Norm: 0.1315, Test Linf Norm: 0.0835\n",
            "Epoch 161: Train Loss: 0.0039, Test Loss: 0.0041, Train L1 Norm: 0.0056, Test L1 Norm: 0.0051, Train Linf Norm: 0.1088, Test Linf Norm: 0.0880\n",
            "Epoch 162: Train Loss: 0.0039, Test Loss: 0.0043, Train L1 Norm: 0.0062, Test L1 Norm: 0.0049, Train Linf Norm: 0.1306, Test Linf Norm: 0.0800\n",
            "Epoch 163: Train Loss: 0.0040, Test Loss: 0.0045, Train L1 Norm: 0.0056, Test L1 Norm: 0.0050, Train Linf Norm: 0.1093, Test Linf Norm: 0.0820\n",
            "Epoch 164: Train Loss: 0.0040, Test Loss: 0.0060, Train L1 Norm: 0.0065, Test L1 Norm: 0.0054, Train Linf Norm: 0.1370, Test Linf Norm: 0.0809\n",
            "Epoch 165: Train Loss: 0.0041, Test Loss: 0.0047, Train L1 Norm: 0.0067, Test L1 Norm: 0.0051, Train Linf Norm: 0.1457, Test Linf Norm: 0.0829\n",
            "Epoch 166: Train Loss: 0.0041, Test Loss: 0.0043, Train L1 Norm: 0.0058, Test L1 Norm: 0.0048, Train Linf Norm: 0.1155, Test Linf Norm: 0.0762\n",
            "Epoch 167: Train Loss: 0.0041, Test Loss: 0.0047, Train L1 Norm: 0.0065, Test L1 Norm: 0.0051, Train Linf Norm: 0.1387, Test Linf Norm: 0.0829\n",
            "Epoch 168: Train Loss: 0.0042, Test Loss: 0.0051, Train L1 Norm: 0.0055, Test L1 Norm: 0.0056, Train Linf Norm: 0.1035, Test Linf Norm: 0.0909\n",
            "Epoch 169: Train Loss: 0.0042, Test Loss: 0.0047, Train L1 Norm: 0.0062, Test L1 Norm: 0.0050, Train Linf Norm: 0.1279, Test Linf Norm: 0.0845\n",
            "Epoch 170: Train Loss: 0.0043, Test Loss: 0.0046, Train L1 Norm: 0.0065, Test L1 Norm: 0.0059, Train Linf Norm: 0.1350, Test Linf Norm: 0.1022\n",
            "Epoch 171: Train Loss: 0.0044, Test Loss: 0.0050, Train L1 Norm: 0.0057, Test L1 Norm: 0.0052, Train Linf Norm: 0.1107, Test Linf Norm: 0.0871\n",
            "Epoch 172: Train Loss: 0.0045, Test Loss: 0.0045, Train L1 Norm: 0.0061, Test L1 Norm: 0.0049, Train Linf Norm: 0.1204, Test Linf Norm: 0.0807\n",
            "Epoch 173: Train Loss: 0.0046, Test Loss: 0.0069, Train L1 Norm: 0.0061, Test L1 Norm: 0.0061, Train Linf Norm: 0.1210, Test Linf Norm: 0.0942\n",
            "Epoch 174: Train Loss: 0.0046, Test Loss: 0.0053, Train L1 Norm: 0.0067, Test L1 Norm: 0.0052, Train Linf Norm: 0.1389, Test Linf Norm: 0.0863\n",
            "Epoch 175: Train Loss: 0.0047, Test Loss: 0.0054, Train L1 Norm: 0.0067, Test L1 Norm: 0.0060, Train Linf Norm: 0.1377, Test Linf Norm: 0.0999\n",
            "Epoch 176: Train Loss: 0.0048, Test Loss: 0.0046, Train L1 Norm: 0.0063, Test L1 Norm: 0.0051, Train Linf Norm: 0.1244, Test Linf Norm: 0.0852\n",
            "Epoch 177: Train Loss: 0.0049, Test Loss: 0.0051, Train L1 Norm: 0.0065, Test L1 Norm: 0.0057, Train Linf Norm: 0.1302, Test Linf Norm: 0.0975\n",
            "Epoch 178: Train Loss: 0.0050, Test Loss: 0.0063, Train L1 Norm: 0.0068, Test L1 Norm: 0.0053, Train Linf Norm: 0.1365, Test Linf Norm: 0.0802\n",
            "Epoch 179: Train Loss: 0.0050, Test Loss: 0.0090, Train L1 Norm: 0.0061, Test L1 Norm: 0.0066, Train Linf Norm: 0.1154, Test Linf Norm: 0.0957\n",
            "Epoch 180: Train Loss: 0.0051, Test Loss: 0.0058, Train L1 Norm: 0.0063, Test L1 Norm: 0.0053, Train Linf Norm: 0.1219, Test Linf Norm: 0.0862\n",
            "Epoch 181: Train Loss: 0.0052, Test Loss: 0.0104, Train L1 Norm: 0.0060, Test L1 Norm: 0.0063, Train Linf Norm: 0.1095, Test Linf Norm: 0.0822\n",
            "Epoch 182: Train Loss: 0.0052, Test Loss: 0.0043, Train L1 Norm: 0.0067, Test L1 Norm: 0.0049, Train Linf Norm: 0.1329, Test Linf Norm: 0.0813\n",
            "Epoch 183: Train Loss: 0.0052, Test Loss: 0.0050, Train L1 Norm: 0.0070, Test L1 Norm: 0.0050, Train Linf Norm: 0.1433, Test Linf Norm: 0.0780\n",
            "Epoch 184: Train Loss: 0.0053, Test Loss: 0.0044, Train L1 Norm: 0.0060, Test L1 Norm: 0.0050, Train Linf Norm: 0.1102, Test Linf Norm: 0.0841\n",
            "Epoch 185: Train Loss: 0.0053, Test Loss: 0.0052, Train L1 Norm: 0.0058, Test L1 Norm: 0.0049, Train Linf Norm: 0.1031, Test Linf Norm: 0.0751\n",
            "Epoch 186: Train Loss: 0.0053, Test Loss: 0.0055, Train L1 Norm: 0.0071, Test L1 Norm: 0.0051, Train Linf Norm: 0.1449, Test Linf Norm: 0.0803\n",
            "Epoch 187: Train Loss: 0.0053, Test Loss: 0.0044, Train L1 Norm: 0.0067, Test L1 Norm: 0.0050, Train Linf Norm: 0.1319, Test Linf Norm: 0.0807\n",
            "Epoch 188: Train Loss: 0.0052, Test Loss: 0.0057, Train L1 Norm: 0.0067, Test L1 Norm: 0.0056, Train Linf Norm: 0.1341, Test Linf Norm: 0.0918\n",
            "Epoch 189: Train Loss: 0.0052, Test Loss: 0.0043, Train L1 Norm: 0.0064, Test L1 Norm: 0.0053, Train Linf Norm: 0.1227, Test Linf Norm: 0.0925\n",
            "Epoch 190: Train Loss: 0.0052, Test Loss: 0.0058, Train L1 Norm: 0.0059, Test L1 Norm: 0.0063, Train Linf Norm: 0.1101, Test Linf Norm: 0.1021\n",
            "Epoch 191: Train Loss: 0.0052, Test Loss: 0.0062, Train L1 Norm: 0.0062, Test L1 Norm: 0.0055, Train Linf Norm: 0.1183, Test Linf Norm: 0.0807\n",
            "Epoch 192: Train Loss: 0.0051, Test Loss: 0.0050, Train L1 Norm: 0.0062, Test L1 Norm: 0.0051, Train Linf Norm: 0.1203, Test Linf Norm: 0.0812\n",
            "Epoch 193: Train Loss: 0.0049, Test Loss: 0.0063, Train L1 Norm: 0.0055, Test L1 Norm: 0.0054, Train Linf Norm: 0.0981, Test Linf Norm: 0.0821\n",
            "Epoch 194: Train Loss: 0.0049, Test Loss: 0.0077, Train L1 Norm: 0.0057, Test L1 Norm: 0.0060, Train Linf Norm: 0.1066, Test Linf Norm: 0.0848\n",
            "Epoch 195: Train Loss: 0.0048, Test Loss: 0.0045, Train L1 Norm: 0.0067, Test L1 Norm: 0.0046, Train Linf Norm: 0.1377, Test Linf Norm: 0.0720\n",
            "Epoch 196: Train Loss: 0.0047, Test Loss: 0.0041, Train L1 Norm: 0.0062, Test L1 Norm: 0.0046, Train Linf Norm: 0.1240, Test Linf Norm: 0.0757\n",
            "Epoch 197: Train Loss: 0.0046, Test Loss: 0.0043, Train L1 Norm: 0.0060, Test L1 Norm: 0.0049, Train Linf Norm: 0.1174, Test Linf Norm: 0.0815\n",
            "Epoch 198: Train Loss: 0.0045, Test Loss: 0.0061, Train L1 Norm: 0.0059, Test L1 Norm: 0.0052, Train Linf Norm: 0.1157, Test Linf Norm: 0.0806\n",
            "Epoch 199: Train Loss: 0.0044, Test Loss: 0.0042, Train L1 Norm: 0.0055, Test L1 Norm: 0.0047, Train Linf Norm: 0.1022, Test Linf Norm: 0.0752\n",
            "Epoch 200: Train Loss: 0.0043, Test Loss: 0.0054, Train L1 Norm: 0.0057, Test L1 Norm: 0.0050, Train Linf Norm: 0.1097, Test Linf Norm: 0.0750\n",
            "Epoch 201: Train Loss: 0.0043, Test Loss: 0.0057, Train L1 Norm: 0.0068, Test L1 Norm: 0.0052, Train Linf Norm: 0.1460, Test Linf Norm: 0.0826\n",
            "Epoch 202: Train Loss: 0.0041, Test Loss: 0.0042, Train L1 Norm: 0.0055, Test L1 Norm: 0.0046, Train Linf Norm: 0.1080, Test Linf Norm: 0.0747\n",
            "Epoch 203: Train Loss: 0.0041, Test Loss: 0.0044, Train L1 Norm: 0.0074, Test L1 Norm: 0.0048, Train Linf Norm: 0.1668, Test Linf Norm: 0.0795\n",
            "Epoch 204: Train Loss: 0.0040, Test Loss: 0.0050, Train L1 Norm: 0.0052, Test L1 Norm: 0.0047, Train Linf Norm: 0.1000, Test Linf Norm: 0.0729\n",
            "Epoch 205: Train Loss: 0.0039, Test Loss: 0.0043, Train L1 Norm: 0.0053, Test L1 Norm: 0.0046, Train Linf Norm: 0.1041, Test Linf Norm: 0.0726\n",
            "Epoch 206: Train Loss: 0.0039, Test Loss: 0.0039, Train L1 Norm: 0.0053, Test L1 Norm: 0.0047, Train Linf Norm: 0.1028, Test Linf Norm: 0.0813\n",
            "Epoch 207: Train Loss: 0.0038, Test Loss: 0.0054, Train L1 Norm: 0.0057, Test L1 Norm: 0.0057, Train Linf Norm: 0.1180, Test Linf Norm: 0.0927\n",
            "Epoch 208: Train Loss: 0.0037, Test Loss: 0.0039, Train L1 Norm: 0.0053, Test L1 Norm: 0.0047, Train Linf Norm: 0.1036, Test Linf Norm: 0.0782\n",
            "Epoch 209: Train Loss: 0.0037, Test Loss: 0.0043, Train L1 Norm: 0.0057, Test L1 Norm: 0.0046, Train Linf Norm: 0.1157, Test Linf Norm: 0.0748\n",
            "Epoch 210: Train Loss: 0.0037, Test Loss: 0.0043, Train L1 Norm: 0.0056, Test L1 Norm: 0.0049, Train Linf Norm: 0.1152, Test Linf Norm: 0.0830\n",
            "Epoch 211: Train Loss: 0.0036, Test Loss: 0.0039, Train L1 Norm: 0.0049, Test L1 Norm: 0.0044, Train Linf Norm: 0.0925, Test Linf Norm: 0.0704\n",
            "Epoch 212: Train Loss: 0.0036, Test Loss: 0.0039, Train L1 Norm: 0.0054, Test L1 Norm: 0.0048, Train Linf Norm: 0.1084, Test Linf Norm: 0.0822\n",
            "Epoch 213: Train Loss: 0.0036, Test Loss: 0.0040, Train L1 Norm: 0.0051, Test L1 Norm: 0.0044, Train Linf Norm: 0.1012, Test Linf Norm: 0.0707\n",
            "Epoch 214: Train Loss: 0.0036, Test Loss: 0.0051, Train L1 Norm: 0.0048, Test L1 Norm: 0.0048, Train Linf Norm: 0.0920, Test Linf Norm: 0.0755\n",
            "Epoch 215: Train Loss: 0.0035, Test Loss: 0.0039, Train L1 Norm: 0.0050, Test L1 Norm: 0.0045, Train Linf Norm: 0.0976, Test Linf Norm: 0.0733\n",
            "Epoch 216: Train Loss: 0.0035, Test Loss: 0.0039, Train L1 Norm: 0.0055, Test L1 Norm: 0.0044, Train Linf Norm: 0.1135, Test Linf Norm: 0.0708\n",
            "Epoch 217: Train Loss: 0.0035, Test Loss: 0.0040, Train L1 Norm: 0.0052, Test L1 Norm: 0.0044, Train Linf Norm: 0.1038, Test Linf Norm: 0.0700\n",
            "Epoch 218: Train Loss: 0.0035, Test Loss: 0.0040, Train L1 Norm: 0.0048, Test L1 Norm: 0.0044, Train Linf Norm: 0.0924, Test Linf Norm: 0.0715\n",
            "Epoch 219: Train Loss: 0.0035, Test Loss: 0.0041, Train L1 Norm: 0.0048, Test L1 Norm: 0.0047, Train Linf Norm: 0.0916, Test Linf Norm: 0.0786\n",
            "Epoch 220: Train Loss: 0.0035, Test Loss: 0.0040, Train L1 Norm: 0.0049, Test L1 Norm: 0.0047, Train Linf Norm: 0.0964, Test Linf Norm: 0.0781\n",
            "Epoch 221: Train Loss: 0.0035, Test Loss: 0.0038, Train L1 Norm: 0.0052, Test L1 Norm: 0.0045, Train Linf Norm: 0.1036, Test Linf Norm: 0.0749\n",
            "Epoch 222: Train Loss: 0.0035, Test Loss: 0.0039, Train L1 Norm: 0.0048, Test L1 Norm: 0.0045, Train Linf Norm: 0.0932, Test Linf Norm: 0.0724\n",
            "Epoch 223: Train Loss: 0.0035, Test Loss: 0.0038, Train L1 Norm: 0.0053, Test L1 Norm: 0.0044, Train Linf Norm: 0.1080, Test Linf Norm: 0.0718\n",
            "Epoch 224: Train Loss: 0.0035, Test Loss: 0.0039, Train L1 Norm: 0.0052, Test L1 Norm: 0.0044, Train Linf Norm: 0.1056, Test Linf Norm: 0.0702\n",
            "Epoch 225: Train Loss: 0.0036, Test Loss: 0.0038, Train L1 Norm: 0.0049, Test L1 Norm: 0.0046, Train Linf Norm: 0.0951, Test Linf Norm: 0.0754\n",
            "Epoch 226: Train Loss: 0.0036, Test Loss: 0.0039, Train L1 Norm: 0.0051, Test L1 Norm: 0.0044, Train Linf Norm: 0.1026, Test Linf Norm: 0.0702\n",
            "Epoch 227: Train Loss: 0.0036, Test Loss: 0.0042, Train L1 Norm: 0.0052, Test L1 Norm: 0.0045, Train Linf Norm: 0.1033, Test Linf Norm: 0.0727\n",
            "Epoch 228: Train Loss: 0.0037, Test Loss: 0.0038, Train L1 Norm: 0.0052, Test L1 Norm: 0.0044, Train Linf Norm: 0.1020, Test Linf Norm: 0.0710\n",
            "Epoch 229: Train Loss: 0.0037, Test Loss: 0.0050, Train L1 Norm: 0.0045, Test L1 Norm: 0.0049, Train Linf Norm: 0.0805, Test Linf Norm: 0.0792\n",
            "Epoch 230: Train Loss: 0.0038, Test Loss: 0.0039, Train L1 Norm: 0.0058, Test L1 Norm: 0.0046, Train Linf Norm: 0.1199, Test Linf Norm: 0.0743\n",
            "Epoch 231: Train Loss: 0.0038, Test Loss: 0.0042, Train L1 Norm: 0.0056, Test L1 Norm: 0.0046, Train Linf Norm: 0.1142, Test Linf Norm: 0.0766\n",
            "Epoch 232: Train Loss: 0.0039, Test Loss: 0.0042, Train L1 Norm: 0.0056, Test L1 Norm: 0.0044, Train Linf Norm: 0.1154, Test Linf Norm: 0.0692\n",
            "Epoch 233: Train Loss: 0.0039, Test Loss: 0.0040, Train L1 Norm: 0.0051, Test L1 Norm: 0.0045, Train Linf Norm: 0.0961, Test Linf Norm: 0.0735\n",
            "Epoch 234: Train Loss: 0.0040, Test Loss: 0.0039, Train L1 Norm: 0.0050, Test L1 Norm: 0.0044, Train Linf Norm: 0.0946, Test Linf Norm: 0.0721\n",
            "Epoch 235: Train Loss: 0.0041, Test Loss: 0.0050, Train L1 Norm: 0.0050, Test L1 Norm: 0.0046, Train Linf Norm: 0.0907, Test Linf Norm: 0.0700\n",
            "Epoch 236: Train Loss: 0.0042, Test Loss: 0.0044, Train L1 Norm: 0.0060, Test L1 Norm: 0.0045, Train Linf Norm: 0.1251, Test Linf Norm: 0.0718\n",
            "Epoch 237: Train Loss: 0.0042, Test Loss: 0.0038, Train L1 Norm: 0.0051, Test L1 Norm: 0.0054, Train Linf Norm: 0.0944, Test Linf Norm: 0.0987\n",
            "Epoch 238: Train Loss: 0.0043, Test Loss: 0.0049, Train L1 Norm: 0.0055, Test L1 Norm: 0.0051, Train Linf Norm: 0.1074, Test Linf Norm: 0.0821\n",
            "Epoch 239: Train Loss: 0.0044, Test Loss: 0.0042, Train L1 Norm: 0.0052, Test L1 Norm: 0.0046, Train Linf Norm: 0.0982, Test Linf Norm: 0.0757\n",
            "Epoch 240: Train Loss: 0.0044, Test Loss: 0.0038, Train L1 Norm: 0.0055, Test L1 Norm: 0.0046, Train Linf Norm: 0.1054, Test Linf Norm: 0.0763\n",
            "Epoch 241: Train Loss: 0.0045, Test Loss: 0.0057, Train L1 Norm: 0.0058, Test L1 Norm: 0.0063, Train Linf Norm: 0.1154, Test Linf Norm: 0.1076\n",
            "Epoch 242: Train Loss: 0.0046, Test Loss: 0.0039, Train L1 Norm: 0.0054, Test L1 Norm: 0.0047, Train Linf Norm: 0.1013, Test Linf Norm: 0.0777\n",
            "Epoch 243: Train Loss: 0.0045, Test Loss: 0.0051, Train L1 Norm: 0.0058, Test L1 Norm: 0.0049, Train Linf Norm: 0.1148, Test Linf Norm: 0.0784\n",
            "Epoch 244: Train Loss: 0.0047, Test Loss: 0.0081, Train L1 Norm: 0.0054, Test L1 Norm: 0.0064, Train Linf Norm: 0.1005, Test Linf Norm: 0.0961\n",
            "Epoch 245: Train Loss: 0.0047, Test Loss: 0.0063, Train L1 Norm: 0.0055, Test L1 Norm: 0.0049, Train Linf Norm: 0.1038, Test Linf Norm: 0.0700\n",
            "Epoch 246: Train Loss: 0.0047, Test Loss: 0.0069, Train L1 Norm: 0.0054, Test L1 Norm: 0.0054, Train Linf Norm: 0.0945, Test Linf Norm: 0.0788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 14:10:09,856]\u001b[0m Trial 48 finished with value: 0.004501680308952928 and parameters: {'n_layers': 5, 'n_units_0': 473, 'n_units_1': 1830, 'n_units_2': 378, 'n_units_3': 1044, 'n_units_4': 1283, 'hidden_activation': 'LeakyReLU', 'output_activation': 'ReLU', 'loss': 'MAE', 'optimizer': 'Adagrad', 'lr': 0.00035263524264307194, 'batch_size': 32, 'n_epochs': 247, 'scheduler': 'CosineAnnealingLR', 't_max_fraction': 0.12661432741796588, 'eta_min': 0.0001684336013288058}. Best is trial 48 with value: 0.004501680308952928.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 247: Train Loss: 0.0047, Test Loss: 0.0048, Train L1 Norm: 0.0056, Test L1 Norm: 0.0045, Train Linf Norm: 0.1054, Test Linf Norm: 0.0684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 14:10:18,370]\u001b[0m Trial 49 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.0416, Test Loss: 0.0029, Train L1 Norm: 1.2294, Test L1 Norm: 0.1759, Train Linf Norm: 36.5304, Test Linf Norm: 4.5997\n",
            "Epoch 1: Train Loss: 0.1305, Test Loss: 0.0381, Train L1 Norm: 0.2157, Test L1 Norm: 0.0437, Train Linf Norm: 4.8477, Test Linf Norm: 0.6964\n",
            "Epoch 2: Train Loss: 0.0392, Test Loss: 0.0240, Train L1 Norm: 0.0866, Test L1 Norm: 0.0291, Train Linf Norm: 2.0562, Test Linf Norm: 0.4520\n",
            "Epoch 3: Train Loss: 0.0286, Test Loss: 0.0312, Train L1 Norm: 0.0617, Test L1 Norm: 0.0327, Train Linf Norm: 1.4452, Test Linf Norm: 0.4977\n",
            "Epoch 4: Train Loss: 0.0237, Test Loss: 0.0331, Train L1 Norm: 0.0523, Test L1 Norm: 0.0291, Train Linf Norm: 1.2193, Test Linf Norm: 0.4217\n",
            "Epoch 5: Train Loss: 0.0211, Test Loss: 0.0172, Train L1 Norm: 0.0469, Test L1 Norm: 0.0209, Train Linf Norm: 1.1005, Test Linf Norm: 0.3320\n",
            "Epoch 6: Train Loss: 0.0192, Test Loss: 0.0142, Train L1 Norm: 0.0443, Test L1 Norm: 0.0190, Train Linf Norm: 1.0514, Test Linf Norm: 0.3068\n",
            "Epoch 7: Train Loss: 0.0179, Test Loss: 0.0198, Train L1 Norm: 0.0394, Test L1 Norm: 0.0210, Train Linf Norm: 0.9152, Test Linf Norm: 0.3264\n",
            "Epoch 8: Train Loss: 0.0167, Test Loss: 0.0122, Train L1 Norm: 0.0372, Test L1 Norm: 0.0171, Train Linf Norm: 0.8651, Test Linf Norm: 0.2759\n",
            "Epoch 9: Train Loss: 0.0161, Test Loss: 0.0193, Train L1 Norm: 0.0332, Test L1 Norm: 0.0196, Train Linf Norm: 0.7524, Test Linf Norm: 0.2855\n",
            "Epoch 10: Train Loss: 0.0153, Test Loss: 0.0325, Train L1 Norm: 0.0353, Test L1 Norm: 0.0238, Train Linf Norm: 0.8298, Test Linf Norm: 0.3135\n",
            "Epoch 11: Train Loss: 0.0148, Test Loss: 0.0107, Train L1 Norm: 0.0348, Test L1 Norm: 0.0157, Train Linf Norm: 0.8259, Test Linf Norm: 0.2471\n",
            "Epoch 12: Train Loss: 0.0146, Test Loss: 0.0107, Train L1 Norm: 0.0309, Test L1 Norm: 0.0150, Train Linf Norm: 0.7048, Test Linf Norm: 0.2463\n",
            "Epoch 13: Train Loss: 0.0139, Test Loss: 0.0124, Train L1 Norm: 0.0313, Test L1 Norm: 0.0166, Train Linf Norm: 0.7287, Test Linf Norm: 0.2798\n",
            "Epoch 14: Train Loss: 0.0136, Test Loss: 0.0098, Train L1 Norm: 0.0306, Test L1 Norm: 0.0148, Train Linf Norm: 0.7079, Test Linf Norm: 0.2557\n",
            "Epoch 15: Train Loss: 0.0135, Test Loss: 0.0100, Train L1 Norm: 0.0316, Test L1 Norm: 0.0148, Train Linf Norm: 0.7532, Test Linf Norm: 0.2514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 14:13:11,129]\u001b[0m Trial 50 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16: Train Loss: 0.0132, Test Loss: 0.0125, Train L1 Norm: 0.0307, Test L1 Norm: 0.0157, Train Linf Norm: 0.7252, Test Linf Norm: 0.2501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 14:13:19,935]\u001b[0m Trial 51 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 3.3922, Test Loss: 3.4125, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.2616, Test Loss: 0.0808, Train L1 Norm: 0.7490, Test L1 Norm: 0.0611, Train Linf Norm: 59.7477, Test Linf Norm: 2.0916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 14:13:28,895]\u001b[0m Trial 52 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 0.0890, Test Loss: 0.0869, Train L1 Norm: 0.1420, Test L1 Norm: 0.0492, Train Linf Norm: 9.4052, Test Linf Norm: 0.6387\n",
            "Epoch 1: Train Loss: 0.1873, Test Loss: 0.0609, Train L1 Norm: 0.5202, Test L1 Norm: 0.0618, Train Linf Norm: 27.4988, Test Linf Norm: 1.8436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-21 14:13:38,785]\u001b[0m Trial 53 pruned. \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 0.0649, Test Loss: 0.0682, Train L1 Norm: 0.1196, Test L1 Norm: 0.0610, Train Linf Norm: 5.5674, Test Linf Norm: 1.7979\n",
            "Epoch 1: Train Loss: 0.1385, Test Loss: 0.0480, Train L1 Norm: 0.3085, Test L1 Norm: 0.0372, Train Linf Norm: 7.9704, Test Linf Norm: 0.4250\n",
            "Epoch 2: Train Loss: 0.0473, Test Loss: 0.0510, Train L1 Norm: 0.0886, Test L1 Norm: 0.0398, Train Linf Norm: 2.1689, Test Linf Norm: 0.5783\n",
            "Epoch 3: Train Loss: 0.0355, Test Loss: 0.0156, Train L1 Norm: 0.0608, Test L1 Norm: 0.0177, Train Linf Norm: 1.4381, Test Linf Norm: 0.2629\n",
            "Epoch 4: Train Loss: 0.0286, Test Loss: 0.0319, Train L1 Norm: 0.0581, Test L1 Norm: 0.0220, Train Linf Norm: 1.4445, Test Linf Norm: 0.2716\n",
            "Epoch 5: Train Loss: 0.0251, Test Loss: 0.0188, Train L1 Norm: 0.0447, Test L1 Norm: 0.0158, Train Linf Norm: 1.0653, Test Linf Norm: 0.2190\n",
            "Epoch 6: Train Loss: 0.0218, Test Loss: 0.0119, Train L1 Norm: 0.0370, Test L1 Norm: 0.0130, Train Linf Norm: 0.8531, Test Linf Norm: 0.2015\n",
            "Epoch 7: Train Loss: 0.0201, Test Loss: 0.0264, Train L1 Norm: 0.0332, Test L1 Norm: 0.0188, Train Linf Norm: 0.7542, Test Linf Norm: 0.2354\n",
            "Epoch 8: Train Loss: 0.0182, Test Loss: 0.0120, Train L1 Norm: 0.0339, Test L1 Norm: 0.0126, Train Linf Norm: 0.8052, Test Linf Norm: 0.2024\n",
            "Epoch 9: Train Loss: 0.0166, Test Loss: 0.0121, Train L1 Norm: 0.0342, Test L1 Norm: 0.0129, Train Linf Norm: 0.8352, Test Linf Norm: 0.2075\n",
            "Epoch 10: Train Loss: 0.0153, Test Loss: 0.0120, Train L1 Norm: 0.0296, Test L1 Norm: 0.0126, Train Linf Norm: 0.7073, Test Linf Norm: 0.1963\n",
            "Epoch 11: Train Loss: 0.0143, Test Loss: 0.0140, Train L1 Norm: 0.0286, Test L1 Norm: 0.0128, Train Linf Norm: 0.6830, Test Linf Norm: 0.1967\n",
            "Epoch 12: Train Loss: 0.0132, Test Loss: 0.0093, Train L1 Norm: 0.0273, Test L1 Norm: 0.0107, Train Linf Norm: 0.6614, Test Linf Norm: 0.1750\n",
            "Epoch 13: Train Loss: 0.0124, Test Loss: 0.0149, Train L1 Norm: 0.0257, Test L1 Norm: 0.0130, Train Linf Norm: 0.6206, Test Linf Norm: 0.1879\n",
            "Epoch 14: Train Loss: 0.0111, Test Loss: 0.0139, Train L1 Norm: 0.0257, Test L1 Norm: 0.0126, Train Linf Norm: 0.6297, Test Linf Norm: 0.1817\n",
            "Epoch 15: Train Loss: 0.0105, Test Loss: 0.0075, Train L1 Norm: 0.0243, Test L1 Norm: 0.0096, Train Linf Norm: 0.5969, Test Linf Norm: 0.1571\n",
            "Epoch 16: Train Loss: 0.0099, Test Loss: 0.0130, Train L1 Norm: 0.0280, Test L1 Norm: 0.0119, Train Linf Norm: 0.7217, Test Linf Norm: 0.1806\n",
            "Epoch 17: Train Loss: 0.0094, Test Loss: 0.0073, Train L1 Norm: 0.0232, Test L1 Norm: 0.0094, Train Linf Norm: 0.5770, Test Linf Norm: 0.1502\n",
            "Epoch 18: Train Loss: 0.0089, Test Loss: 0.0136, Train L1 Norm: 0.0218, Test L1 Norm: 0.0114, Train Linf Norm: 0.5336, Test Linf Norm: 0.1675\n",
            "Epoch 19: Train Loss: 0.0082, Test Loss: 0.0068, Train L1 Norm: 0.0212, Test L1 Norm: 0.0091, Train Linf Norm: 0.5246, Test Linf Norm: 0.1484\n",
            "Epoch 20: Train Loss: 0.0079, Test Loss: 0.0064, Train L1 Norm: 0.0215, Test L1 Norm: 0.0090, Train Linf Norm: 0.5362, Test Linf Norm: 0.1503\n",
            "Epoch 21: Train Loss: 0.0075, Test Loss: 0.0080, Train L1 Norm: 0.0202, Test L1 Norm: 0.0089, Train Linf Norm: 0.4998, Test Linf Norm: 0.1372\n",
            "Epoch 22: Train Loss: 0.0072, Test Loss: 0.0068, Train L1 Norm: 0.0205, Test L1 Norm: 0.0092, Train Linf Norm: 0.5142, Test Linf Norm: 0.1556\n",
            "Epoch 23: Train Loss: 0.0069, Test Loss: 0.0063, Train L1 Norm: 0.0200, Test L1 Norm: 0.0081, Train Linf Norm: 0.5013, Test Linf Norm: 0.1275\n",
            "Epoch 24: Train Loss: 0.0067, Test Loss: 0.0069, Train L1 Norm: 0.0198, Test L1 Norm: 0.0083, Train Linf Norm: 0.5003, Test Linf Norm: 0.1316\n",
            "Epoch 25: Train Loss: 0.0065, Test Loss: 0.0063, Train L1 Norm: 0.0194, Test L1 Norm: 0.0083, Train Linf Norm: 0.4843, Test Linf Norm: 0.1349\n",
            "Epoch 26: Train Loss: 0.0062, Test Loss: 0.0062, Train L1 Norm: 0.0195, Test L1 Norm: 0.0079, Train Linf Norm: 0.4943, Test Linf Norm: 0.1244\n",
            "Epoch 27: Train Loss: 0.0061, Test Loss: 0.0060, Train L1 Norm: 0.0197, Test L1 Norm: 0.0079, Train Linf Norm: 0.4993, Test Linf Norm: 0.1246\n",
            "Epoch 28: Train Loss: 0.0060, Test Loss: 0.0060, Train L1 Norm: 0.0191, Test L1 Norm: 0.0081, Train Linf Norm: 0.4850, Test Linf Norm: 0.1337\n",
            "Epoch 29: Train Loss: 0.0059, Test Loss: 0.0065, Train L1 Norm: 0.0187, Test L1 Norm: 0.0079, Train Linf Norm: 0.4701, Test Linf Norm: 0.1219\n",
            "Epoch 30: Train Loss: 0.0058, Test Loss: 0.0060, Train L1 Norm: 0.0186, Test L1 Norm: 0.0081, Train Linf Norm: 0.4715, Test Linf Norm: 0.1333\n",
            "Epoch 31: Train Loss: 0.0057, Test Loss: 0.0058, Train L1 Norm: 0.0188, Test L1 Norm: 0.0077, Train Linf Norm: 0.4796, Test Linf Norm: 0.1223\n",
            "Epoch 32: Train Loss: 0.0056, Test Loss: 0.0064, Train L1 Norm: 0.0184, Test L1 Norm: 0.0078, Train Linf Norm: 0.4655, Test Linf Norm: 0.1208\n",
            "Epoch 33: Train Loss: 0.0056, Test Loss: 0.0057, Train L1 Norm: 0.0187, Test L1 Norm: 0.0077, Train Linf Norm: 0.4755, Test Linf Norm: 0.1213\n",
            "Epoch 34: Train Loss: 0.0055, Test Loss: 0.0058, Train L1 Norm: 0.0186, Test L1 Norm: 0.0076, Train Linf Norm: 0.4726, Test Linf Norm: 0.1198\n",
            "Epoch 35: Train Loss: 0.0055, Test Loss: 0.0057, Train L1 Norm: 0.0184, Test L1 Norm: 0.0076, Train Linf Norm: 0.4685, Test Linf Norm: 0.1196\n",
            "Epoch 36: Train Loss: 0.0055, Test Loss: 0.0058, Train L1 Norm: 0.0184, Test L1 Norm: 0.0078, Train Linf Norm: 0.4687, Test Linf Norm: 0.1234\n",
            "Epoch 37: Train Loss: 0.0054, Test Loss: 0.0057, Train L1 Norm: 0.0183, Test L1 Norm: 0.0076, Train Linf Norm: 0.4654, Test Linf Norm: 0.1198\n",
            "Epoch 38: Train Loss: 0.0054, Test Loss: 0.0057, Train L1 Norm: 0.0181, Test L1 Norm: 0.0076, Train Linf Norm: 0.4593, Test Linf Norm: 0.1195\n",
            "Epoch 39: Train Loss: 0.0054, Test Loss: 0.0057, Train L1 Norm: 0.0183, Test L1 Norm: 0.0077, Train Linf Norm: 0.4656, Test Linf Norm: 0.1227\n",
            "Epoch 40: Train Loss: 0.0054, Test Loss: 0.0057, Train L1 Norm: 0.0183, Test L1 Norm: 0.0076, Train Linf Norm: 0.4677, Test Linf Norm: 0.1205\n",
            "Epoch 41: Train Loss: 0.0054, Test Loss: 0.0058, Train L1 Norm: 0.0182, Test L1 Norm: 0.0076, Train Linf Norm: 0.4618, Test Linf Norm: 0.1201\n",
            "Epoch 42: Train Loss: 0.0054, Test Loss: 0.0057, Train L1 Norm: 0.0177, Test L1 Norm: 0.0078, Train Linf Norm: 0.4465, Test Linf Norm: 0.1259\n",
            "Epoch 43: Train Loss: 0.0054, Test Loss: 0.0057, Train L1 Norm: 0.0179, Test L1 Norm: 0.0076, Train Linf Norm: 0.4521, Test Linf Norm: 0.1195\n",
            "Epoch 44: Train Loss: 0.0054, Test Loss: 0.0057, Train L1 Norm: 0.0179, Test L1 Norm: 0.0076, Train Linf Norm: 0.4526, Test Linf Norm: 0.1199\n",
            "Epoch 45: Train Loss: 0.0054, Test Loss: 0.0057, Train L1 Norm: 0.0178, Test L1 Norm: 0.0075, Train Linf Norm: 0.4515, Test Linf Norm: 0.1186\n",
            "Epoch 46: Train Loss: 0.0054, Test Loss: 0.0056, Train L1 Norm: 0.0180, Test L1 Norm: 0.0075, Train Linf Norm: 0.4554, Test Linf Norm: 0.1184\n",
            "Epoch 47: Train Loss: 0.0055, Test Loss: 0.0060, Train L1 Norm: 0.0182, Test L1 Norm: 0.0076, Train Linf Norm: 0.4618, Test Linf Norm: 0.1190\n",
            "Epoch 48: Train Loss: 0.0055, Test Loss: 0.0057, Train L1 Norm: 0.0179, Test L1 Norm: 0.0076, Train Linf Norm: 0.4540, Test Linf Norm: 0.1220\n",
            "Epoch 49: Train Loss: 0.0055, Test Loss: 0.0057, Train L1 Norm: 0.0174, Test L1 Norm: 0.0076, Train Linf Norm: 0.4380, Test Linf Norm: 0.1209\n",
            "Epoch 50: Train Loss: 0.0055, Test Loss: 0.0057, Train L1 Norm: 0.0174, Test L1 Norm: 0.0078, Train Linf Norm: 0.4379, Test Linf Norm: 0.1263\n",
            "Epoch 51: Train Loss: 0.0056, Test Loss: 0.0055, Train L1 Norm: 0.0173, Test L1 Norm: 0.0074, Train Linf Norm: 0.4342, Test Linf Norm: 0.1163\n",
            "Epoch 52: Train Loss: 0.0056, Test Loss: 0.0057, Train L1 Norm: 0.0177, Test L1 Norm: 0.0075, Train Linf Norm: 0.4469, Test Linf Norm: 0.1185\n",
            "Epoch 53: Train Loss: 0.0057, Test Loss: 0.0058, Train L1 Norm: 0.0172, Test L1 Norm: 0.0077, Train Linf Norm: 0.4303, Test Linf Norm: 0.1237\n",
            "Epoch 54: Train Loss: 0.0058, Test Loss: 0.0056, Train L1 Norm: 0.0176, Test L1 Norm: 0.0077, Train Linf Norm: 0.4432, Test Linf Norm: 0.1268\n",
            "Epoch 55: Train Loss: 0.0059, Test Loss: 0.0062, Train L1 Norm: 0.0172, Test L1 Norm: 0.0074, Train Linf Norm: 0.4287, Test Linf Norm: 0.1134\n",
            "Epoch 56: Train Loss: 0.0060, Test Loss: 0.0062, Train L1 Norm: 0.0175, Test L1 Norm: 0.0077, Train Linf Norm: 0.4371, Test Linf Norm: 0.1204\n",
            "Epoch 57: Train Loss: 0.0062, Test Loss: 0.0056, Train L1 Norm: 0.0178, Test L1 Norm: 0.0076, Train Linf Norm: 0.4460, Test Linf Norm: 0.1224\n",
            "Epoch 58: Train Loss: 0.0063, Test Loss: 0.0072, Train L1 Norm: 0.0171, Test L1 Norm: 0.0076, Train Linf Norm: 0.4251, Test Linf Norm: 0.1159\n",
            "Epoch 59: Train Loss: 0.0065, Test Loss: 0.0074, Train L1 Norm: 0.0163, Test L1 Norm: 0.0077, Train Linf Norm: 0.3981, Test Linf Norm: 0.1173\n",
            "Epoch 60: Train Loss: 0.0067, Test Loss: 0.0058, Train L1 Norm: 0.0169, Test L1 Norm: 0.0077, Train Linf Norm: 0.4145, Test Linf Norm: 0.1287\n",
            "Epoch 61: Train Loss: 0.0069, Test Loss: 0.0055, Train L1 Norm: 0.0177, Test L1 Norm: 0.0071, Train Linf Norm: 0.4379, Test Linf Norm: 0.1119\n",
            "Epoch 62: Train Loss: 0.0071, Test Loss: 0.0058, Train L1 Norm: 0.0177, Test L1 Norm: 0.0072, Train Linf Norm: 0.4373, Test Linf Norm: 0.1129\n",
            "Epoch 63: Train Loss: 0.0073, Test Loss: 0.0057, Train L1 Norm: 0.0160, Test L1 Norm: 0.0073, Train Linf Norm: 0.3809, Test Linf Norm: 0.1164\n",
            "Epoch 64: Train Loss: 0.0075, Test Loss: 0.0126, Train L1 Norm: 0.0173, Test L1 Norm: 0.0110, Train Linf Norm: 0.4212, Test Linf Norm: 0.1606\n",
            "Epoch 65: Train Loss: 0.0076, Test Loss: 0.0076, Train L1 Norm: 0.0174, Test L1 Norm: 0.0088, Train Linf Norm: 0.4238, Test Linf Norm: 0.1389\n",
            "Epoch 66: Train Loss: 0.0079, Test Loss: 0.0060, Train L1 Norm: 0.0172, Test L1 Norm: 0.0072, Train Linf Norm: 0.4154, Test Linf Norm: 0.1122\n",
            "Epoch 67: Train Loss: 0.0080, Test Loss: 0.0072, Train L1 Norm: 0.0174, Test L1 Norm: 0.0077, Train Linf Norm: 0.4240, Test Linf Norm: 0.1170\n",
            "Epoch 68: Train Loss: 0.0081, Test Loss: 0.0065, Train L1 Norm: 0.0171, Test L1 Norm: 0.0077, Train Linf Norm: 0.4114, Test Linf Norm: 0.1186\n",
            "Epoch 69: Train Loss: 0.0082, Test Loss: 0.0064, Train L1 Norm: 0.0166, Test L1 Norm: 0.0073, Train Linf Norm: 0.3925, Test Linf Norm: 0.1103\n",
            "Epoch 70: Train Loss: 0.0084, Test Loss: 0.0069, Train L1 Norm: 0.0159, Test L1 Norm: 0.0074, Train Linf Norm: 0.3721, Test Linf Norm: 0.1109\n",
            "Epoch 71: Train Loss: 0.0084, Test Loss: 0.0068, Train L1 Norm: 0.0182, Test L1 Norm: 0.0073, Train Linf Norm: 0.4486, Test Linf Norm: 0.1120\n",
            "Epoch 72: Train Loss: 0.0086, Test Loss: 0.0107, Train L1 Norm: 0.0195, Test L1 Norm: 0.0080, Train Linf Norm: 0.4867, Test Linf Norm: 0.1114\n",
            "Epoch 73: Train Loss: 0.0084, Test Loss: 0.0319, Train L1 Norm: 0.0165, Test L1 Norm: 0.0174, Train Linf Norm: 0.3949, Test Linf Norm: 0.1975\n",
            "Epoch 74: Train Loss: 0.0084, Test Loss: 0.0124, Train L1 Norm: 0.0186, Test L1 Norm: 0.0089, Train Linf Norm: 0.4588, Test Linf Norm: 0.1201\n",
            "Epoch 75: Train Loss: 0.0084, Test Loss: 0.0137, Train L1 Norm: 0.0158, Test L1 Norm: 0.0099, Train Linf Norm: 0.3691, Test Linf Norm: 0.1367\n",
            "Epoch 76: Train Loss: 0.0083, Test Loss: 0.0135, Train L1 Norm: 0.0161, Test L1 Norm: 0.0085, Train Linf Norm: 0.3819, Test Linf Norm: 0.1121\n",
            "Epoch 77: Train Loss: 0.0084, Test Loss: 0.0071, Train L1 Norm: 0.0200, Test L1 Norm: 0.0071, Train Linf Norm: 0.5010, Test Linf Norm: 0.1074\n",
            "Epoch 78: Train Loss: 0.0082, Test Loss: 0.0075, Train L1 Norm: 0.0151, Test L1 Norm: 0.0077, Train Linf Norm: 0.3508, Test Linf Norm: 0.1229\n",
            "Epoch 79: Train Loss: 0.0080, Test Loss: 0.0194, Train L1 Norm: 0.0150, Test L1 Norm: 0.0114, Train Linf Norm: 0.3496, Test Linf Norm: 0.1376\n",
            "Epoch 80: Train Loss: 0.0080, Test Loss: 0.0063, Train L1 Norm: 0.0154, Test L1 Norm: 0.0067, Train Linf Norm: 0.3631, Test Linf Norm: 0.1019\n",
            "Epoch 81: Train Loss: 0.0078, Test Loss: 0.0056, Train L1 Norm: 0.0137, Test L1 Norm: 0.0077, Train Linf Norm: 0.3126, Test Linf Norm: 0.1282\n",
            "Epoch 82: Train Loss: 0.0075, Test Loss: 0.0059, Train L1 Norm: 0.0150, Test L1 Norm: 0.0070, Train Linf Norm: 0.3539, Test Linf Norm: 0.1098\n",
            "Epoch 83: Train Loss: 0.0074, Test Loss: 0.0068, Train L1 Norm: 0.0141, Test L1 Norm: 0.0077, Train Linf Norm: 0.3299, Test Linf Norm: 0.1246\n",
            "Epoch 84: Train Loss: 0.0073, Test Loss: 0.0060, Train L1 Norm: 0.0140, Test L1 Norm: 0.0066, Train Linf Norm: 0.3259, Test Linf Norm: 0.1023\n",
            "Epoch 85: Train Loss: 0.0070, Test Loss: 0.0052, Train L1 Norm: 0.0136, Test L1 Norm: 0.0067, Train Linf Norm: 0.3201, Test Linf Norm: 0.1094\n",
            "Epoch 86: Train Loss: 0.0067, Test Loss: 0.0076, Train L1 Norm: 0.0142, Test L1 Norm: 0.0072, Train Linf Norm: 0.3398, Test Linf Norm: 0.1128\n",
            "Epoch 87: Train Loss: 0.0066, Test Loss: 0.0077, Train L1 Norm: 0.0143, Test L1 Norm: 0.0072, Train Linf Norm: 0.3451, Test Linf Norm: 0.1091\n",
            "Epoch 88: Train Loss: 0.0063, Test Loss: 0.0129, Train L1 Norm: 0.0137, Test L1 Norm: 0.0091, Train Linf Norm: 0.3285, Test Linf Norm: 0.1210\n",
            "Epoch 89: Train Loss: 0.0061, Test Loss: 0.0052, Train L1 Norm: 0.0133, Test L1 Norm: 0.0065, Train Linf Norm: 0.3174, Test Linf Norm: 0.1047\n",
            "Epoch 90: Train Loss: 0.0060, Test Loss: 0.0060, Train L1 Norm: 0.0126, Test L1 Norm: 0.0065, Train Linf Norm: 0.2966, Test Linf Norm: 0.1020\n",
            "Epoch 91: Train Loss: 0.0058, Test Loss: 0.0055, Train L1 Norm: 0.0137, Test L1 Norm: 0.0065, Train Linf Norm: 0.3329, Test Linf Norm: 0.1051\n",
            "Epoch 92: Train Loss: 0.0056, Test Loss: 0.0049, Train L1 Norm: 0.0127, Test L1 Norm: 0.0068, Train Linf Norm: 0.3036, Test Linf Norm: 0.1161\n",
            "Epoch 93: Train Loss: 0.0054, Test Loss: 0.0097, Train L1 Norm: 0.0124, Test L1 Norm: 0.0080, Train Linf Norm: 0.2977, Test Linf Norm: 0.1092\n",
            "Epoch 94: Train Loss: 0.0052, Test Loss: 0.0048, Train L1 Norm: 0.0131, Test L1 Norm: 0.0063, Train Linf Norm: 0.3201, Test Linf Norm: 0.1031\n",
            "Epoch 95: Train Loss: 0.0051, Test Loss: 0.0047, Train L1 Norm: 0.0122, Test L1 Norm: 0.0063, Train Linf Norm: 0.2938, Test Linf Norm: 0.1022\n",
            "Epoch 96: Train Loss: 0.0049, Test Loss: 0.0122, Train L1 Norm: 0.0120, Test L1 Norm: 0.0091, Train Linf Norm: 0.2864, Test Linf Norm: 0.1307\n",
            "Epoch 97: Train Loss: 0.0048, Test Loss: 0.0049, Train L1 Norm: 0.0121, Test L1 Norm: 0.0063, Train Linf Norm: 0.2942, Test Linf Norm: 0.1008\n",
            "Epoch 98: Train Loss: 0.0047, Test Loss: 0.0050, Train L1 Norm: 0.0124, Test L1 Norm: 0.0066, Train Linf Norm: 0.3049, Test Linf Norm: 0.1082\n",
            "Epoch 99: Train Loss: 0.0046, Test Loss: 0.0046, Train L1 Norm: 0.0114, Test L1 Norm: 0.0060, Train Linf Norm: 0.2731, Test Linf Norm: 0.0966\n",
            "Epoch 100: Train Loss: 0.0046, Test Loss: 0.0048, Train L1 Norm: 0.0123, Test L1 Norm: 0.0060, Train Linf Norm: 0.3013, Test Linf Norm: 0.0946\n",
            "Epoch 101: Train Loss: 0.0045, Test Loss: 0.0054, Train L1 Norm: 0.0122, Test L1 Norm: 0.0063, Train Linf Norm: 0.2983, Test Linf Norm: 0.0978\n",
            "Epoch 102: Train Loss: 0.0044, Test Loss: 0.0047, Train L1 Norm: 0.0115, Test L1 Norm: 0.0060, Train Linf Norm: 0.2801, Test Linf Norm: 0.0970\n",
            "Epoch 103: Train Loss: 0.0044, Test Loss: 0.0046, Train L1 Norm: 0.0117, Test L1 Norm: 0.0060, Train Linf Norm: 0.2844, Test Linf Norm: 0.0953\n",
            "Epoch 104: Train Loss: 0.0043, Test Loss: 0.0046, Train L1 Norm: 0.0120, Test L1 Norm: 0.0059, Train Linf Norm: 0.2953, Test Linf Norm: 0.0947\n",
            "Epoch 105: Train Loss: 0.0043, Test Loss: 0.0046, Train L1 Norm: 0.0113, Test L1 Norm: 0.0059, Train Linf Norm: 0.2733, Test Linf Norm: 0.0938\n"
          ]
        }
      ],
      "source": [
        "if OPTIMIZE:\n",
        "    # Creating a study object with Optuna with TPE sampler and median pruner \n",
        "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner())\n",
        "\n",
        "    # Running Optuna with 100 trials when we are optimizing.\n",
        "    study.optimize(objective, n_trials=N_TRIALS)\n",
        "\n",
        "    # Printing the best trial information\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "    print(\"  Value: \", trial.value)\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(f\"    {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmMfE9_dUZiS"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phyiHlWEUZiT"
      },
      "outputs": [],
      "source": [
        "# Creating the best network and optimizer using the best hyperparameters\n",
        "if OPTIMIZE:\n",
        "    net, \\\n",
        "    loss_fn, \\\n",
        "    optimizer, \\\n",
        "    batch_size, \\\n",
        "    n_epochs, \\\n",
        "    scheduler, \\\n",
        "    loss_name, \\\n",
        "    optimizer_name, \\\n",
        "    scheduler_name, \\\n",
        "    n_units, \\\n",
        "    n_layers, \\\n",
        "    hidden_activation, \\\n",
        "    output_activation, \\\n",
        "    lr = create_model(trial, optimize=True)\n",
        "# Creating the network with predefined hyperparameters\n",
        "else:\n",
        "    net, \\\n",
        "    loss_fn, \\\n",
        "    optimizer, \\\n",
        "    batch_size, \\\n",
        "    n_epochs, \\\n",
        "    scheduler, \\\n",
        "    loss_name, \\\n",
        "    optimizer_name, \\\n",
        "    scheduler_name, \\\n",
        "    n_units, \\\n",
        "    n_layers, \\\n",
        "    hidden_activation, \\\n",
        "    output_activation, \\\n",
        "    lr = create_model(trial=None, optimize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yq-oY81UZiU"
      },
      "outputs": [],
      "source": [
        "print(\"loss_fn:\", loss_fn)\n",
        "print(\"batch_size:\", batch_size)\n",
        "print(\"n_epochs:\", n_epochs)\n",
        "print(\"scheduler:\", scheduler)\n",
        "print(\"loss_name:\", loss_name)\n",
        "print(\"optimizer_name:\", optimizer_name)\n",
        "print(\"scheduler_name:\", scheduler_name)\n",
        "print(\"n_units:\", n_units)\n",
        "print(\"n_layers:\", n_layers)\n",
        "print(\"hidden_activation:\", hidden_activation)\n",
        "print(\"output_activation:\", output_activation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7aLWdZyUZiW"
      },
      "outputs": [],
      "source": [
        "# Training and evaluating the network using the train_and_eval function\n",
        "train_losses, test_losses, train_metrics, test_metrics = train_and_eval(\n",
        "    net, loss_fn, optimizer, batch_size, n_epochs, scheduler\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akNucrgMUZiW"
      },
      "source": [
        "## Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHsrs2Y-UZic"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# save the network to a .pth file\n",
        "torch.save(net.state_dict(), \"net.pth\")\n",
        "save_file(\"net.pth\")\n",
        "\n",
        "# save the optimizer to a .pth file\n",
        "torch.save(optimizer.state_dict(), \"optimizer.pth\")\n",
        "save_file(\"optimizer.pth\")\n",
        "\n",
        "# save the scheduler to a .pth file if it is not None\n",
        "if scheduler is not None:\n",
        "  torch.save(scheduler.state_dict(), \"scheduler.pth\")\n",
        "  save_file(\"scheduler.pth\")\n",
        "\n",
        "# create a dictionary to store the rest of the variables\n",
        "var_dict = {\n",
        "  \"batch_size\": batch_size,\n",
        "  \"n_epochs\": n_epochs,\n",
        "  \"loss_name\": loss_name,\n",
        "  \"optimizer_name\": optimizer_name,\n",
        "  \"scheduler_name\": scheduler_name,\n",
        "  \"n_units\": n_units,\n",
        "  \"n_layers\": n_layers,\n",
        "  \"hidden_activation_name\": hidden_activation.__class__.__name__,\n",
        "  \"output_activation_name\": output_activation.__class__.__name__,\n",
        "  \"lr\": lr,\n",
        "}\n",
        "\n",
        "# save the dictionary to a .json file\n",
        "with open(\"var_dict.json\", \"w\") as f:\n",
        "  json.dump(var_dict, f)\n",
        "save_file(\"var_dict.json\")\n",
        "\n",
        "# Saving the output of the training using pandas\n",
        "train_df = pd.DataFrame(\n",
        "    {\n",
        "        \"train_loss\": train_losses,\n",
        "        \"test_loss\": test_losses,\n",
        "        \"train_l1_norm\": [m[\"l1_norm\"] for m in train_metrics],\n",
        "        \"test_l1_norm\": [m[\"l1_norm\"] for m in test_metrics],\n",
        "        \"train_linf_norm\": [m[\"linf_norm\"] for m in train_metrics],\n",
        "        \"test_linf_norm\": [m[\"linf_norm\"] for m in test_metrics],\n",
        "    }\n",
        ")\n",
        "train_df.to_csv(\"train_output.csv\", index=False)\n",
        "save_file(\"train_output.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU23l7dIUZie"
      },
      "source": [
        "## Visualizing the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cippWZS6UZie"
      },
      "outputs": [],
      "source": [
        "# Plotting the losses and metrics for the best network \n",
        "plt.figure(figsize=(12, 8))\n",
        "#plt.subplot(2, 2, 1)\n",
        "#plt.plot(train_losses, label=\"Train Loss\")\n",
        "#plt.plot(test_losses, label=\"Test Loss\")\n",
        "#plt.xlabel(\"Epoch\")\n",
        "#plt.ylabel(\"Loss\")\n",
        "#plt.legend()\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot([m[\"l1_norm\"] for m in train_metrics], label=\"Train L1 Norm\")\n",
        "plt.plot([m[\"l1_norm\"] for m in test_metrics], label=\"Test L1 Norm\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"L1 Norm\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-3, 1e2)\n",
        "plt.legend()\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot([m[\"linf_norm\"] for m in train_metrics], label=\"Train Linf Norm\")\n",
        "plt.plot([m[\"linf_norm\"] for m in test_metrics], label=\"Test Linf Norm\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Linf Norm\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-3, 1e2)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Added plotting MSE of training data and MSE of test data in one plot \n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(train_losses,label=\"training data\")\n",
        "plt.plot(test_losses,label=\"test data\")\n",
        "#if scheduler is not None:\n",
        "#    plt.plot([scheduler.get_last_lr()[0] for _ in range(n_epochs)], label=\"Learning rate\") \n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-7, 1e0)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiEDutxIUZig"
      },
      "source": [
        "## Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7Mj990wUZih"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# load the dictionary from the .json file\n",
        "with open(\"var_dict.json\", \"r\") as f:\n",
        "  var_dict_loaded = json.load(f)\n",
        "\n",
        "# extract the variables from the dictionary\n",
        "batch_size_loaded = var_dict_loaded[\"batch_size\"]\n",
        "n_epochs_loaded = var_dict_loaded[\"n_epochs\"]\n",
        "loss_name_loaded = var_dict_loaded[\"loss_name\"]\n",
        "optimizer_name_loaded = var_dict_loaded[\"optimizer_name\"]\n",
        "scheduler_name_loaded = var_dict_loaded[\"scheduler_name\"]\n",
        "n_units_loaded = var_dict_loaded[\"n_units\"]\n",
        "n_layers_loaded = var_dict_loaded[\"n_layers\"]\n",
        "hidden_activation_name_loaded = var_dict_loaded[\"hidden_activation_name\"]\n",
        "output_activation_name_loaded = var_dict_loaded[\"output_activation_name\"]\n",
        "lr_loaded = var_dict_loaded[\"lr\"]\n",
        "\n",
        "# create the activation functions from their names\n",
        "if hidden_activation_name_loaded == \"ReLU\":\n",
        "  hidden_activation_loaded = nn.ReLU()\n",
        "elif hidden_activation_name_loaded == \"LeakyReLU\":\n",
        "  hidden_activation_loaded = nn.LeakyReLU() \n",
        "elif hidden_activation_name_loaded == \"ELU\":\n",
        "  hidden_activation_loaded = nn.ELU() \n",
        "elif hidden_activation_name_loaded == \"Tanh\":\n",
        "  hidden_activation_loaded = nn.Tanh()\n",
        "else:\n",
        "  hidden_activation_loaded = nn.Sigmoid()\n",
        "\n",
        "if output_activation_name_loaded == \"ReLU\":\n",
        "    output_activation_loaded = nn.ReLU()\n",
        "elif output_activation_name_loaded == \"Softplus\":\n",
        "    output_activation_loaded = nn.Softplus()\n",
        "else:\n",
        "    output_activation_loaded = nn.Identity()\n",
        "\n",
        "\n",
        "\n",
        "# load the network from the .pth file\n",
        "net_loaded = Net(n_layers_loaded, n_units_loaded, hidden_activation_loaded, output_activation_loaded).to(device)\n",
        "if torch.cuda.is_available():\n",
        " net_loaded.load_state_dict(torch.load(\"net.pth\"))\n",
        "else: \n",
        "  net_loaded.load_state_dict(torch.load(\"net.pth\", map_location=torch.device('cpu')))\n",
        "\n",
        "# create the loss function from its name\n",
        "if loss_name_loaded == \"MSE\":\n",
        "  loss_fn_loaded = nn.MSELoss()\n",
        "elif loss_name_loaded == \"MAE\":\n",
        "  loss_fn_loaded = nn.L1Loss()\n",
        "elif loss_name_loaded == \"Huber\":\n",
        "  loss_fn_loaded = nn.SmoothL1Loss() \n",
        "else:\n",
        "  # create the log-cosh loss function\n",
        "  def log_cosh_loss_loaded(y_pred, y_true):\n",
        "    return torch.mean(torch.log(torch.cosh(y_pred - y_true)))\n",
        "  loss_fn_loaded = log_cosh_loss_loaded\n",
        "\n",
        "# load the optimizer from the .pth file\n",
        "if torch.cuda.is_available():\n",
        "  optimizer_loaded_state_dict = torch.load(\"optimizer.pth\")\n",
        "else:\n",
        "  optimizer_loaded_state_dict = torch.load(\"optimizer.pth\", map_location=torch.device('cpu'))\n",
        "\n",
        "if optimizer_name_loaded == \"SGD\":\n",
        "  # Added getting the weight decay and momentum parameters from the state dict\n",
        "  weight_decay_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"weight_decay\"]\n",
        "  momentum_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"momentum\"]\n",
        "  optimizer_loaded = optim.SGD(net_loaded.parameters(), lr=lr_loaded, weight_decay=weight_decay_loaded, momentum=momentum_loaded)\n",
        "elif optimizer_name_loaded == \"Adam\":\n",
        "  # Added getting the weight decay and beta parameters from the state dict\n",
        "  weight_decay_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"weight_decay\"]\n",
        "  beta1_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"betas\"][0]\n",
        "  beta2_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"betas\"][1]\n",
        "  optimizer_loaded = optim.Adam(net_loaded.parameters(), lr=lr_loaded, weight_decay=weight_decay_loaded, betas=(beta1_loaded, beta2_loaded))\n",
        "elif optimizer_name_loaded == \"RMSprop\":\n",
        "  optimizer_loaded = optim.RMSprop(net_loaded.parameters(), lr=lr_loaded)\n",
        "else:\n",
        "  # Added loading the Adagrad optimizer\n",
        "  optimizer_loaded = optim.Adagrad(net_loaded.parameters(), lr=lr_loaded)\n",
        "optimizer_loaded.load_state_dict(optimizer_loaded_state_dict)\n",
        "\n",
        "# load the scheduler from the .pth file\n",
        "if torch.cuda.is_available():\n",
        "  scheduler_loaded_state_dict = torch.load(\"scheduler.pth\")\n",
        "else: \n",
        "  scheduler_loaded_state_dict = torch.load(\"scheduler.pth\", map_location=torch.device('cpu'))\n",
        "\n",
        "if scheduler_name_loaded == \"StepLR\":\n",
        "  # Added getting the step_size and gamma parameters from the state dict\n",
        "  step_size_loaded = scheduler_loaded_state_dict[\"step_size\"]\n",
        "  gamma_loaded = scheduler_loaded_state_dict[\"gamma\"]\n",
        "  scheduler_loaded = optim.lr_scheduler.StepLR(optimizer_loaded, step_size=step_size_loaded, gamma=gamma_loaded)\n",
        "elif scheduler_name_loaded == \"ExponentialLR\":\n",
        "  # Added getting the gamma parameter from the state dict\n",
        "  gamma_loaded = scheduler_loaded_state_dict[\"gamma\"]\n",
        "  scheduler_loaded = optim.lr_scheduler.ExponentialLR(optimizer_loaded, gamma=gamma_loaded)\n",
        "elif scheduler_name_loaded == \"CosineAnnealingLR\":\n",
        "  # Added getting the T_max parameter from the state dict\n",
        "  T_max_loaded = scheduler_loaded_state_dict[\"T_max\"]\n",
        "  scheduler_loaded = optim.lr_scheduler.CosineAnnealingLR(optimizer_loaded, T_max=T_max_loaded)\n",
        "elif scheduler_name_loaded == \"ReduceLROnPlateau\":\n",
        "  # Added getting the mode, factor, patience, threshold and min_lr parameters from the state dict\n",
        "  mode_loaded = scheduler_loaded_state_dict[\"mode\"]\n",
        "  factor_loaded = scheduler_loaded_state_dict[\"factor\"]\n",
        "  patience_loaded = scheduler_loaded_state_dict[\"patience\"]\n",
        "  threshold_loaded = scheduler_loaded_state_dict[\"threshold\"]\n",
        "  min_lr_loaded = scheduler_loaded_state_dict[\"min_lrs\"][0]\n",
        "  scheduler_loaded = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                    optimizer_loaded, mode=mode_loaded, factor=factor_loaded, patience=patience_loaded, threshold=threshold_loaded, min_lr=min_lr_loaded\n",
        "                )\n",
        "# elif scheduler_name_loaded == \"OneCycleLR\":\n",
        "#   max_lr_loaded = scheduler_loaded_state_dict[\"max_lr\"]\n",
        "#   epochs_loaded = scheduler_loaded_state_dict[\"epochs\"]\n",
        "#   steps_per_epoch_loaded = scheduler_loaded_state_dict[\"steps_per_epoch\"]\n",
        "#   pct_start_loaded = scheduler_loaded_state_dict[\"pct_start\"]\n",
        "#   max_lr_loaded = scheduler_loaded_state_dict[\"max_lr\"]\n",
        "#   scheduler_loaded = optim.lr_scheduler.OneCycleLR(\n",
        "#                     optimizer_loaded, max_lr=max_lr_loaded, epochs=epochs_loaded, steps_per_epoch=steps_per_epoch_loaded, pct_start=pct_start_loaded\n",
        "#                 )\n",
        "else:\n",
        "  scheduler_loaded = None\n",
        "\n",
        "if scheduler_loaded is not None:\n",
        "  # Added loading the state dict to the scheduler_loaded\n",
        "  scheduler_loaded.load_state_dict(scheduler_loaded_state_dict)\n",
        "\n",
        "# Loading the output of the training using pandas\n",
        "train_df_loaded = pd.read_csv(\"train_output.csv\")\n",
        "train_losses_loaded = train_df_loaded[\"train_loss\"].tolist()\n",
        "test_losses_loaded = train_df_loaded[\"test_loss\"].tolist()\n",
        "train_metrics_loaded = [\n",
        "    {\n",
        "        \"l1_norm\": train_df_loaded[\"train_l1_norm\"][i],\n",
        "        \"linf_norm\": train_df_loaded[\"train_linf_norm\"][i],\n",
        "    }\n",
        "    for i in range(len(train_df_loaded))\n",
        "]\n",
        "test_metrics_loaded = [\n",
        "    {\n",
        "        \"l1_norm\": train_df_loaded[\"test_l1_norm\"][i],\n",
        "        \"linf_norm\": train_df_loaded[\"test_linf_norm\"][i],\n",
        "    }\n",
        "    for i in range(len(train_df_loaded))\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ_fcj7zUZii"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "batch_size_loaded\n",
        "n_epochs_loaded\n",
        "loss_name_loaded\n",
        "optimizer_name_loaded\n",
        "scheduler_name_loaded\n",
        "n_units_loaded\n",
        "n_layers_loaded\n",
        "hidden_activation_name_loaded\n",
        "output_activation_name_loaded\n",
        "lr_loaded\n",
        "hidden_activation_loaded\n",
        "output_activation_loaded\n",
        "net_loaded\n",
        "net_loaded.__dict__ # print the subparameters of the network\n",
        "loss_fn_loaded\n",
        "optimizer_loaded\n",
        "optimizer_loaded.__dict__ # print the subparameters of the optimizer\n",
        "scheduler_loaded\n",
        "scheduler_loaded.__dict__ # print the subparameters of the scheduler\n",
        "train_losses_loaded\n",
        "test_losses_loaded\n",
        "train_metrics_loaded\n",
        "test_metrics_loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0B0SHa5SvExY"
      },
      "outputs": [],
      "source": [
        "train_losses_loaded[-1]\n",
        "test_losses_loaded[-1]\n",
        "test_metrics_loaded[-1]['l1_norm']\n",
        "test_metrics_loaded[-1]['linf_norm']\n",
        "print(f'Error is {test_metrics_loaded[-1][\"l1_norm\"] / (3.84e-4)} times bigger than in Dieselhorst et al.')\n",
        "print(f'Error is {test_metrics_loaded[-1][\"linf_norm\"] / (8.14e-3)} times bigger than in Dieselhorst et al.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj2XBdtmvExY"
      },
      "source": [
        "### Visualize loaded results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwLGR1aSUZik"
      },
      "source": [
        "Let us verify correct loading of the train and test metrics by visualizing them again but now through the loaded values. Likewise for the train and test losses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXiNgLsmUZil"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"last_expr_or_assign\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgq4WfSiUZil"
      },
      "outputs": [],
      "source": [
        "# Plotting the losses and metrics for the best network plt.figure(figsize=(12, \n",
        "#plt.subplot(2, 2, 1)\n",
        "#plt.plot(train_losses_loaded, label=\"Train Loss\")\n",
        "#plt.plot(test_losses_loaded, label=\"Test Loss\")\n",
        "#plt.xlabel(\"Epoch\")\n",
        "#plt.ylabel(\"Loss\")\n",
        "#plt.legend()\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot([m[\"l1_norm\"] for m in train_metrics_loaded], label=\"Train L1 Norm\")\n",
        "plt.plot([m[\"l1_norm\"] for m in test_metrics_loaded], label=\"Test L1 Norm\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"L1 Norm\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-3, 1e2)\n",
        "plt.legend()\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot([m[\"linf_norm\"] for m in train_metrics_loaded], label=\"Train Linf Norm\")\n",
        "plt.plot([m[\"linf_norm\"] for m in test_metrics_loaded], label=\"Test Linf Norm\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Linf Norm\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-3, 1e2)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Added plotting MSE of training data and MSE of test data in one plot \n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(train_losses_loaded,label=\"training data\")\n",
        "plt.plot(test_losses_loaded,label=\"test data\")\n",
        "#if scheduler is not None:\n",
        "#    plt.plot([scheduler.get_last_lr()[0] for _ in range(n_epochs)], label=\"Learning rate\") \n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-7, 1e0)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkgLqJ_UUZim"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EZQMUK8vExY"
      },
      "source": [
        "## Counting the number of parameters in the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWToFZmnvExZ"
      },
      "outputs": [],
      "source": [
        "net_loaded.eval()\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {count_parameters(net_loaded)} parameters.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxuzVSnlUZin"
      },
      "source": [
        "## Evaluating the network on arbirary input\n",
        "### Comparing `net` and `net_loaded`\n",
        "\n",
        "We compare `net` and `net_loaded` to confirm correct loading of the network. Note that `net` is only available if we have trained the model in this session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0PLAA0DUZin"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "print(list(net.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NZ8iVA7UZio"
      },
      "outputs": [],
      "source": [
        "print(list(net_loaded.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXLYbm8uUZio"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "# Set the network to evaluation mode\n",
        "net.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InGW0Xq6UZip"
      },
      "outputs": [],
      "source": [
        "rho_example, vx_example, vy_example, vz_example, epsilon_example = sample_primitive_variables(20)\n",
        "\n",
        "# Create arbitrary input\n",
        "inputs =  generate_input_data(rho_example, vx_example, vy_example, vz_example, epsilon_example)\n",
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVa1upmFUZip"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "# Pass the inputs to the network and get the outputs\n",
        "outputs = [net(input) for input in inputs]\n",
        "# Print the outputs\n",
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih9p2bosUZiq"
      },
      "outputs": [],
      "source": [
        "# Set the network to evaluation mode\n",
        "net_loaded.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-Xjfo7VUZir"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Pass the inputs to the network and get the outputs\n",
        "outputs = [net_loaded(input) for input in inputs]\n",
        "# Print the outputs\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjpIvdybUZis"
      },
      "source": [
        "## Porting the model to C++"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMlEd4RoUZis"
      },
      "outputs": [],
      "source": [
        "import torch.jit\n",
        "\n",
        "# Creating a dummy input tensor of shape (1, 5) to trace the model\n",
        "dummy_input = torch.randn(1, 5).to(device)\n",
        "dummy_input\n",
        "\n",
        "# Ensure that net_loaded is in evaluation mode.\n",
        "net_loaded.eval()\n",
        "\n",
        "# Tracing the model using the torch.jit.trace function\n",
        "traced_model = torch.jit.trace(net_loaded, dummy_input)\n",
        "\n",
        "# Saving the traced model to a file named \"net.pt\"\n",
        "traced_model.save(\"net.pt\")\n",
        "save_file(\"net.pt\")\n",
        "\n",
        "example_input_to_validate_correct_export_and_import = generate_input_data(*sample_primitive_variables(1))\n",
        "example_input_to_validate_correct_export_and_import\n",
        "net_loaded(example_input_to_validate_correct_export_and_import)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "bsc",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{"convo":[{"bot":false,"text":"\n      Is it correct that Google colab does not save files when asked to do so in the notebook? E.g. I tried to save my neural network as net.pth in my notebook in colab, but I don't see the file. Or is it saved to a specific location? In any case, how can I save results produced in the notebook in google colab?\n      \n      \n    "},{"bot":true,"text":"<p>Google Colab does save files when asked to do so in the notebook, but the files are not stored permanently on the Colab virtual machine. They will be deleted when the session ends or the runtime disconnects. <a class=\"tooltip-target\" data-citationid=\"4e05f2d2-4814-ab37-ff7f-a45bfaa9458c-6-group\" h=\"ID=SERP,5026.1\">To save your files permanently, you need to save them to an external storage such as Google Drive</a><a href=\"https://stackoverflow.com/questions/64808087/how-do-i-save-files-from-google-colab-to-google-drive\" class=\"ac-anchor sup-target\" target=\"_blank\" data-citationid=\"4e05f2d2-4814-ab37-ff7f-a45bfaa9458c-2\" aria-label=\"How do I save files from Google Colab to Google Drive\" h=\"ID=SERP,5026.1\"><sup>1</sup></a><a href=\"https://www.youtube.com/watch?v=e8Syou92H8U\" class=\"ac-anchor sup-target\" target=\"_blank\" data-citationid=\"4e05f2d2-4814-ab37-ff7f-a45bfaa9458c-4\" aria-label=\"How to Save and Download files from Google Colab - YouTube\" h=\"ID=SERP,5026.1\"><sup>2</sup></a><a href=\"https://www.picozu.com/saving-images-to-google-drive-from-colab\" class=\"ac-anchor sup-target\" target=\"_blank\" data-citationid=\"4e05f2d2-4814-ab37-ff7f-a45bfaa9458c-6\" aria-label=\"Saving Images To Google Drive From Colab â€“ Picozu\" h=\"ID=SERP,5026.1\"><sup>3</sup></a>.</p>\n<p>There are a few ways to save files to Google Drive from Colab. <a class=\"tooltip-target\" data-citationid=\"4e05f2d2-4814-ab37-ff7f-a45bfaa9458c-10-group\" h=\"ID=SERP,5026.1\">One way is to <strong>mount Google Drive</strong> and provide the path to the folder where you want to save your files</a><a href=\"https://stackoverflow.com/questions/64808087/how-do-i-save-files-from-google-colab-to-google-drive\" class=\"ac-anchor sup-target\" target=\"_blank\" data-citationid=\"4e05f2d2-4814-ab37-ff7f-a45bfaa9458c-10\" aria-label=\"How do I save files from Google Colab to Google Drive\" h=\"ID=SERP,5026.1\"><sup>1</sup></a>. For example:</p>\n\n      <div class=\"code-block\">\n        <pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span> google.colab <span class=\"hljs-keyword\">import</span> drive\ndrive.mount(<span class=\"hljs-string\">'/content/drive'</span>)\n<span class=\"hljs-comment\"># Save your neural network as net.pth</span>\ntorch.save(net.state_dict(), <span class=\"hljs-string\">'/content/drive/My Drive/net.pth'</span>)\n</code></pre>\n\n        <button type=\"button\" class=\"js-code-copy\" data-clipboard-text=\"from google.colab import drive\ndrive.mount('/content/drive')\n# Save your neural network as net.pth\ntorch.save(net.state_dict(), '/content/drive/My Drive/net.pth')\n\">\n          <svg class=\"copy-icon\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\">\n            <path d=\"M5.5028 4.62704L5.5 6.75V17.2542C5.5 19.0491 6.95507 20.5042 8.75 20.5042L17.3663 20.5045C17.0573 21.3782 16.224 22.0042 15.2444 22.0042H8.75C6.12665 22.0042 4 19.8776 4 17.2542V6.75C4 5.76929 4.62745 4.93512 5.5028 4.62704ZM17.75 2C18.9926 2 20 3.00736 20 4.25V17.25C20 18.4926 18.9926 19.5 17.75 19.5H8.75C7.50736 19.5 6.5 18.4926 6.5 17.25V4.25C6.5 3.00736 7.50736 2 8.75 2H17.75ZM17.75 3.5H8.75C8.33579 3.5 8 3.83579 8 4.25V17.25C8 17.6642 8.33579 18 8.75 18H17.75C18.1642 18 18.5 17.6642 18.5 17.25V4.25C18.5 3.83579 18.1642 3.5 17.75 3.5Z\"></path>\n          </svg>\n          <span class=\"tooltip\">Copy</span>\n        </button>\n      </div>\n      <p><a class=\"tooltip-target\" data-citationid=\"4e05f2d2-4814-ab37-ff7f-a45bfaa9458c-26-group\" h=\"ID=SERP,5026.1\">Another way is to use <strong>PyDrive</strong> to upload your files to Google Drive</a><a href=\"https://stackoverflow.com/questions/52717164/save-files-pictures-in-google-colaboratory\" class=\"ac-anchor sup-target\" target=\"_blank\" data-citationid=\"4e05f2d2-4814-ab37-ff7f-a45bfaa9458c-26\" aria-label=\"Save files/pictures in Google Colaboratory - Stack Overflow\" h=\"ID=SERP,5026.1\"><sup>4</sup></a>. For example:</p>\n\n      <div class=\"code-block\">\n        <pre><code class=\"language-python\">!pip install -U -q PyDrive\n<span class=\"hljs-keyword\">from</span> pydrive.auth <span class=\"hljs-keyword\">import</span> GoogleAuth\n<span class=\"hljs-keyword\">from</span> pydrive.drive <span class=\"hljs-keyword\">import</span> GoogleDrive\n<span class=\"hljs-keyword\">from</span> google.colab <span class=\"hljs-keyword\">import</span> auth\n<span class=\"hljs-keyword\">from</span> oauth2client.client <span class=\"hljs-keyword\">import</span> GoogleCredentials\n\n<span class=\"hljs-comment\"># Authenticate and create the PyDrive client.</span>\nauth.authenticate_user()\ngauth = GoogleAuth()\ngauth.credentials = GoogleCredentials.get_application_default()\ndrive = GoogleDrive(gauth)\n\n<span class=\"hljs-comment\"># Save your neural network as net.pth</span>\ntorch.save(net.state_dict(), <span class=\"hljs-string\">'net.pth'</span>)\n<span class=\"hljs-comment\"># Upload net.pth to Google Drive</span>\nuploaded = drive.CreateFile({<span class=\"hljs-string\">'title'</span>: <span class=\"hljs-string\">'net.pth'</span>})\nuploaded.SetContentFile(<span class=\"hljs-string\">'net.pth'</span>)\nuploaded.Upload()\n</code></pre>\n\n        <button type=\"button\" class=\"js-code-copy\" data-clipboard-text=\"!pip install -U -q PyDrive\nfrom pydrive.auth import GoogleAuth\nfrom pydrive.drive import GoogleDrive\nfrom google.colab import auth\nfrom oauth2client.client import GoogleCredentials\n\n# Authenticate and create the PyDrive client.\nauth.authenticate_user()\ngauth = GoogleAuth()\ngauth.credentials = GoogleCredentials.get_application_default()\ndrive = GoogleDrive(gauth)\n\n# Save your neural network as net.pth\ntorch.save(net.state_dict(), 'net.pth')\n# Upload net.pth to Google Drive\nuploaded = drive.CreateFile({'title': 'net.pth'})\nuploaded.SetContentFile('net.pth')\nuploaded.Upload()\n\">\n          <svg class=\"copy-icon\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\">\n            <path d=\"M5.5028 4.62704L5.5 6.75V17.2542C5.5 19.0491 6.95507 20.5042 8.75 20.5042L17.3663 20.5045C17.0573 21.3782 16.224 22.0042 15.2444 22.0042H8.75C6.12665 22.0042 4 19.8776 4 17.2542V6.75C4 5.76929 4.62745 4.93512 5.5028 4.62704ZM17.75 2C18.9926 2 20 3.00736 20 4.25V17.25C20 18.4926 18.9926 19.5 17.75 19.5H8.75C7.50736 19.5 6.5 18.4926 6.5 17.25V4.25C6.5 3.00736 7.50736 2 8.75 2H17.75ZM17.75 3.5H8.75C8.33579 3.5 8 3.83579 8 4.25V17.25C8 17.6642 8.33579 18 8.75 18H17.75C18.1642 18 18.5 17.6642 18.5 17.25V4.25C18.5 3.83579 18.1642 3.5 17.75 3.5Z\"></path>\n          </svg>\n          <span class=\"tooltip\">Copy</span>\n        </button>\n      </div>\n      <p>I hope this helps you with saving your results produced in the notebook in Google Colab.ðŸ˜Š</p>\n"},{"bot":false,"text":"\n      When I go to an earlier version of the file, should I be able to see the saved files again or should I not?\n      \n      \n    "},{"bot":true,"text":"<p>If you go to an earlier version of the file, you should be able to see the saved files again, as long as they were saved before the version you are going back to. However, this will also revert any changes you made to the file after that version, so be careful not to lose your work. You can always use the <strong>File</strong> menu and select <strong>Revision history</strong> to see the different versions of your file and restore them if needed.</p>\n"},{"bot":false,"text":"\n      I don't see my saved files when I go back to an earlier version of the file that should in fact have the file\n      \n      \n    "},{"bot":true,"text":"<p>Thatâ€™s strange. Are you sure you saved the files to the same folder as the notebook file? If not, they might be in a different location on your Google Drive. You can use the <strong>Files</strong> panel on the left side of the Colab interface to browse your Google Drive and find your files. Alternatively, you can use the <strong>Search in Drive</strong> option in the <strong>File</strong> menu to look for your files by name or type.</p>\n"},{"bot":false,"text":"\n      Please modify my saving code to save to the drive if it is mounted else save as shown here\n\n```python\nimport json\nimport pandas as pd\n\n# save the network to a .pth file\ntorch.save(net.state_dict(), \"net.pth\")\n\n# save the optimizer to a .pth file\ntorch.save(optimizer.state_dict(), \"optimizer.pth\")\n\n# save the scheduler to a .pth file if it is not None\nif scheduler is not None:\n  torch.save(scheduler.state_dict(), \"scheduler.pth\")\n\n# create a dictionary to store the rest of the variables\nvar_dict = {\n  \"batch_size\": batch_size,\n  \"n_epochs\": n_epochs,\n  \"loss_name\": loss_name,\n  \"optimizer_name\": optimizer_name,\n  \"scheduler_name\": scheduler_name,\n  \"n_units\": n_units,\n  \"n_layers\": n_layers,\n  \"hidden_activation_name\": hidden_activation.__class__.__name__,\n  \"output_activation_name\": output_activation.__class__.__name__,\n  \"lr\": lr,\n}\n\n# save the dictionary to a .json file\nwith open(\"var_dict.json\", \"w\") as f:\n  json.dump(var_dict, f)\n\n# Saving the output of the training using pandas\ntrain_df = pd.DataFrame(\n    {\n        \"train_loss\": train_losses,\n        \"test_loss\": test_losses,\n        \"train_l1_norm\": [m[\"l1_norm\"] for m in train_metrics],\n        \"test_l1_norm\": [m[\"l1_norm\"] for m in test_metrics],\n        \"train_linf_norm\": [m[\"linf_norm\"] for m in train_metrics],\n        \"test_linf_norm\": [m[\"linf_norm\"] for m in test_metrics],\n    }\n)\ntrain_df.to_csv(\"train_output.csv\", index=False)\n\n```\n      \n      \n    "},{"bot":true,"text":"<p>I can try to help you with rewriting your saving code. Here is one possible way to modify it to save to the drive if it is mounted else save as shown here:</p>\n\n      <div class=\"code-block\">\n        <pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span> json\n<span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd\n<span class=\"hljs-keyword\">import</span> os\n\n<span class=\"hljs-comment\"># check if the drive is mounted</span>\ndrive_mounted = os.path.exists(<span class=\"hljs-string\">\"/content/drive\"</span>)\n\n<span class=\"hljs-comment\"># define a function to save a file to the drive or the current directory</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">save_file</span>(<span class=\"hljs-params\">file_name, file_object</span>):\n  <span class=\"hljs-keyword\">if</span> drive_mounted:\n    <span class=\"hljs-comment\"># save the file to the drive folder</span>\n    drive_folder = <span class=\"hljs-string\">\"/content/drive/My Drive\"</span> <span class=\"hljs-comment\"># change this to your desired folder</span>\n    file_path = os.path.join(drive_folder, file_name)\n  <span class=\"hljs-keyword\">else</span>:\n    <span class=\"hljs-comment\"># save the file to the current directory</span>\n    file_path = file_name\n  <span class=\"hljs-comment\"># write the file object to the file path</span>\n  <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(file_path, <span class=\"hljs-string\">\"wb\"</span>) <span class=\"hljs-keyword\">as</span> f:\n    f.write(file_object)\n\n<span class=\"hljs-comment\"># save the network to a .pth file</span>\nnet_file = torch.save(net.state_dict(), <span class=\"hljs-string\">\"net.pth\"</span>)\nsave_file(<span class=\"hljs-string\">\"net.pth\"</span>, net_file)\n\n<span class=\"hljs-comment\"># save the optimizer to a .pth file</span>\noptimizer_file = torch.save(optimizer.state_dict(), <span class=\"hljs-string\">\"optimizer.pth\"</span>)\nsave_file(<span class=\"hljs-string\">\"optimizer.pth\"</span>, optimizer_file)\n\n<span class=\"hljs-comment\"># save the scheduler to a .pth file if it is not None</span>\n<span class=\"hljs-keyword\">if</span> scheduler <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n  scheduler_file = torch.save(scheduler.state_dict(), <span class=\"hljs-string\">\"scheduler.pth\"</span>)\n  save_file(<span class=\"hljs-string\">\"scheduler.pth\"</span>, scheduler_file)\n\n<span class=\"hljs-comment\"># create a dictionary to store the rest of the variables</span>\nvar_dict = {\n  <span class=\"hljs-string\">\"batch_size\"</span>: batch_size,\n  <span class=\"hljs-string\">\"n_epochs\"</span>: n_epochs,\n  <span class=\"hljs-string\">\"loss_name\"</span>: loss_name,\n  <span class=\"hljs-string\">\"optimizer_name\"</span>: optimizer_name,\n  <span class=\"hljs-string\">\"scheduler_name\"</span>: scheduler_name,\n  <span class=\"hljs-string\">\"n_units\"</span>: n_units,\n  <span class=\"hljs-string\">\"n_layers\"</span>: n_layers,\n  <span class=\"hljs-string\">\"hidden_activation_name\"</span>: hidden_activation.__class__.__name__,\n  <span class=\"hljs-string\">\"output_activation_name\"</span>: output_activation.__class__.__name__,\n  <span class=\"hljs-string\">\"lr\"</span>: lr,\n}\n\n<span class=\"hljs-comment\"># save the dictionary to a .json file</span>\nvar_dict_file = json.dumps(var_dict)\nsave_file(<span class=\"hljs-string\">\"var_dict.json\"</span>, var_dict_file)\n\n<span class=\"hljs-comment\"># Saving the output of the training using pandas</span>\ntrain_df = pd.DataFrame(\n    {\n        <span class=\"hljs-string\">\"train_loss\"</span>: train_losses,\n        <span class=\"hljs-string\">\"test_loss\"</span>: test_losses,\n        <span class=\"hljs-string\">\"train_l1_norm\"</span>: [m[<span class=\"hljs-string\">\"l1_norm\"</span>] <span class=\"hljs-keyword\">for</span> m <span class=\"hljs-keyword\">in</span> train_metrics],\n        <span class=\"hljs-string\">\"test_l1_norm\"</span>: [m[<span class=\"hljs-string\">\"l1_norm\"</span>] <span class=\"hljs-keyword\">for</span> m <span class=\"hljs-keyword\">in</span> test_metrics],\n        <span class=\"hljs-string\">\"train_linf_norm\"</span>: [m[<span class=\"hljs-string\">\"linf_norm\"</span>] <span class=\"hljs-keyword\">for</span> m <span class=\"hljs-keyword\">in</span> train_metrics],\n        <span class=\"hljs-string\">\"test_linf_norm\"</span>: [m[<span class=\"hljs-string\">\"linf_norm\"</span>] <span class=\"hljs-keyword\">for</span> m <span class=\"hljs-keyword\">in</span> test_metrics],\n    }\n)\ntrain_df_file = train_df.to_csv(<span class=\"hljs-string\">\"train_output.csv\"</span>, index=<span class=\"hljs-literal\">False</span>)\nsave_file(<span class=\"hljs-string\">\"train_output.csv\"</span>, train_df_file)\n</code></pre>\n\n        <button type=\"button\" class=\"js-code-copy\" data-clipboard-text=\"import json\nimport pandas as pd\nimport os\n\n# check if the drive is mounted\ndrive_mounted = os.path.exists(&quot;/content/drive&quot;)\n\n# define a function to save a file to the drive or the current directory\ndef save_file(file_name, file_object):\n  if drive_mounted:\n    # save the file to the drive folder\n    drive_folder = &quot;/content/drive/My Drive&quot; # change this to your desired folder\n    file_path = os.path.join(drive_folder, file_name)\n  else:\n    # save the file to the current directory\n    file_path = file_name\n  # write the file object to the file path\n  with open(file_path, &quot;wb&quot;) as f:\n    f.write(file_object)\n\n# save the network to a .pth file\nnet_file = torch.save(net.state_dict(), &quot;net.pth&quot;)\nsave_file(&quot;net.pth&quot;, net_file)\n\n# save the optimizer to a .pth file\noptimizer_file = torch.save(optimizer.state_dict(), &quot;optimizer.pth&quot;)\nsave_file(&quot;optimizer.pth&quot;, optimizer_file)\n\n# save the scheduler to a .pth file if it is not None\nif scheduler is not None:\n  scheduler_file = torch.save(scheduler.state_dict(), &quot;scheduler.pth&quot;)\n  save_file(&quot;scheduler.pth&quot;, scheduler_file)\n\n# create a dictionary to store the rest of the variables\nvar_dict = {\n  &quot;batch_size&quot;: batch_size,\n  &quot;n_epochs&quot;: n_epochs,\n  &quot;loss_name&quot;: loss_name,\n  &quot;optimizer_name&quot;: optimizer_name,\n  &quot;scheduler_name&quot;: scheduler_name,\n  &quot;n_units&quot;: n_units,\n  &quot;n_layers&quot;: n_layers,\n  &quot;hidden_activation_name&quot;: hidden_activation.__class__.__name__,\n  &quot;output_activation_name&quot;: output_activation.__class__.__name__,\n  &quot;lr&quot;: lr,\n}\n\n# save the dictionary to a .json file\nvar_dict_file = json.dumps(var_dict)\nsave_file(&quot;var_dict.json&quot;, var_dict_file)\n\n# Saving the output of the training using pandas\ntrain_df = pd.DataFrame(\n    {\n        &quot;train_loss&quot;: train_losses,\n        &quot;test_loss&quot;: test_losses,\n        &quot;train_l1_norm&quot;: [m[&quot;l1_norm&quot;] for m in train_metrics],\n        &quot;test_l1_norm&quot;: [m[&quot;l1_norm&quot;] for m in test_metrics],\n        &quot;train_linf_norm&quot;: [m[&quot;linf_norm&quot;] for m in train_metrics],\n        &quot;test_linf_norm&quot;: [m[&quot;linf_norm&quot;] for m in test_metrics],\n    }\n)\ntrain_df_file = train_df.to_csv(&quot;train_output.csv&quot;, index=False)\nsave_file(&quot;train_output.csv&quot;, train_df_file)\n\">\n          <svg class=\"copy-icon\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\">\n            <path d=\"M5.5028 4.62704L5.5 6.75V17.2542C5.5 19.0491 6.95507 20.5042 8.75 20.5042L17.3663 20.5045C17.0573 21.3782 16.224 22.0042 15.2444 22.0042H8.75C6.12665 22.0042 4 19.8776 4 17.2542V6.75C4 5.76929 4.62745 4.93512 5.5028 4.62704ZM17.75 2C18.9926 2 20 3.00736 20 4.25V17.25C20 18.4926 18.9926 19.5 17.75 19.5H8.75C7.50736 19.5 6.5 18.4926 6.5 17.25V4.25C6.5 3.00736 7.50736 2 8.75 2H17.75ZM17.75 3.5H8.75C8.33579 3.5 8 3.83579 8 4.25V17.25C8 17.6642 8.33579 18 8.75 18H17.75C18.1642 18 18.5 17.6642 18.5 17.25V4.25C18.5 3.83579 18.1642 3.5 17.75 3.5Z\"></path>\n          </svg>\n          <span class=\"tooltip\">Copy</span>\n        </button>\n      </div>\n      <p>I hope this code works for you. Please note that I have not tested it and you may need to make some adjustments.ðŸ˜Š</p>\n"},{"bot":false,"text":"\n      I get TypeError: a bytes-like object is required, not 'NoneType' in `---> 24 save_file(\"net.pth\", net_file)` in `---> 20 f.write(file_object)`\n      \n      \n    "},{"bot":true,"text":"<p>It seems that the <code>torch.save</code> function does not return a bytes-like object, but rather writes the file directly to the given file name. My apologies for the mistake. In that case, you can modify the <code>save_file</code> function to take only the file name as an argument and use <code>shutil.copyfile</code> to copy the file to the drive folder. For example:</p>\n\n      <div class=\"code-block\">\n        <pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span> json\n<span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd\n<span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">import</span> shutil\n\n<span class=\"hljs-comment\"># check if the drive is mounted</span>\ndrive_mounted = os.path.exists(<span class=\"hljs-string\">\"/content/drive\"</span>)\n\n<span class=\"hljs-comment\"># define a function to save a file to the drive or the current directory</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">save_file</span>(<span class=\"hljs-params\">file_name</span>):\n  <span class=\"hljs-keyword\">if</span> drive_mounted:\n    <span class=\"hljs-comment\"># save the file to the drive folder</span>\n    drive_folder = <span class=\"hljs-string\">\"/content/drive/My Drive\"</span> <span class=\"hljs-comment\"># change this to your desired folder</span>\n    file_path = os.path.join(drive_folder, file_name)\n    <span class=\"hljs-comment\"># copy the file from the current directory to the drive folder</span>\n    shutil.copyfile(file_name, file_path)\n  <span class=\"hljs-keyword\">else</span>:\n    <span class=\"hljs-comment\"># do nothing as the file is already in the current directory</span>\n    <span class=\"hljs-keyword\">pass</span>\n\n<span class=\"hljs-comment\"># save the network to a .pth file</span>\ntorch.save(net.state_dict(), <span class=\"hljs-string\">\"net.pth\"</span>)\nsave_file(<span class=\"hljs-string\">\"net.pth\"</span>)\n\n<span class=\"hljs-comment\"># save the optimizer to a .pth file</span>\ntorch.save(optimizer.state_dict(), <span class=\"hljs-string\">\"optimizer.pth\"</span>)\nsave_file(<span class=\"hljs-string\">\"optimizer.pth\"</span>)\n\n<span class=\"hljs-comment\"># save the scheduler to a .pth file if it is not None</span>\n<span class=\"hljs-keyword\">if</span> scheduler <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n  torch.save(scheduler.state_dict(), <span class=\"hljs-string\">\"scheduler.pth\"</span>)\n  save_file(<span class=\"hljs-string\">\"scheduler.pth\"</span>)\n\n<span class=\"hljs-comment\"># create a dictionary to store the rest of the variables</span>\nvar_dict = {\n  <span class=\"hljs-string\">\"batch_size\"</span>: batch_size,\n  <span class=\"hljs-string\">\"n_epochs\"</span>: n_epochs,\n  <span class=\"hljs-string\">\"loss_name\"</span>: loss_name,\n  <span class=\"hljs-string\">\"optimizer_name\"</span>: optimizer_name,\n  <span class=\"hljs-string\">\"scheduler_name\"</span>: scheduler_name,\n  <span class=\"hljs-string\">\"n_units\"</span>: n_units,\n  <span class=\"hljs-string\">\"n_layers\"</span>: n_layers,\n  <span class=\"hljs-string\">\"hidden_activation_name\"</span>: hidden_activation.__class__.__name__,\n  <span class=\"hljs-string\">\"output_activation_name\"</span>: output_activation.__class__.__name__,\n  <span class=\"hljs-string\">\"lr\"</span>: lr,\n}\n\n<span class=\"hljs-comment\"># save the dictionary to a .json file</span>\n<span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">\"var_dict.json\"</span>, <span class=\"hljs-string\">\"w\"</span>) <span class=\"hljs-keyword\">as</span> f:\n  json.dump(var_dict, f)\nsave_file(<span class=\"hljs-string\">\"var_dict.json\"</span>)\n\n<span class=\"hljs-comment\"># Saving the output of the training using pandas</span>\ntrain_df = pd.DataFrame(\n    {\n        <span class=\"hljs-string\">\"train_loss\"</span>: train_losses,\n        <span class=\"hljs-string\">\"test_loss\"</span>: test_losses,\n        <span class=\"hljs-string\">\"train_l1_norm\"</span>: [m[<span class=\"hljs-string\">\"l1_norm\"</span>] <span class=\"hljs-keyword\">for</span> m <span class=\"hljs-keyword\">in</span> train_metrics],\n        <span class=\"hljs-string\">\"test_l1_norm\"</span>: [m[<span class=\"hljs-string\">\"l1_norm\"</span>] <span class=\"hljs-keyword\">for</span> m <span class=\"hljs-keyword\">in</span> test_metrics],\n        <span class=\"hljs-string\">\"train_linf_norm\"</span>: [m[<span class=\"hljs-string\">\"linf_norm\"</span>] <span class=\"hljs-keyword\">for</span> m <span class=\"hljs-keyword\">in</span> train_metrics],\n        <span class=\"hljs-string\">\"test_linf_norm\"</span>: [m[<span class=\"hljs-string\">\"linf_norm\"</span>] <span class=\"hljs-keyword\">for</span> m <span class=\"hljs-keyword\">in</span> test_metrics],\n    }\n)\ntrain_df.to_csv(<span class=\"hljs-string\">\"train_output.csv\"</span>, index=<span class=\"hljs-literal\">False</span>)\nsave_file(<span class=\"hljs-string\">\"train_output.csv\"</span>)\n</code></pre>\n\n        <button type=\"button\" class=\"js-code-copy\" data-clipboard-text=\"import json\nimport pandas as pd\nimport os\nimport shutil\n\n# check if the drive is mounted\ndrive_mounted = os.path.exists(&quot;/content/drive&quot;)\n\n# define a function to save a file to the drive or the current directory\ndef save_file(file_name):\n  if drive_mounted:\n    # save the file to the drive folder\n    drive_folder = &quot;/content/drive/My Drive&quot; # change this to your desired folder\n    file_path = os.path.join(drive_folder, file_name)\n    # copy the file from the current directory to the drive folder\n    shutil.copyfile(file_name, file_path)\n  else:\n    # do nothing as the file is already in the current directory\n    pass\n\n# save the network to a .pth file\ntorch.save(net.state_dict(), &quot;net.pth&quot;)\nsave_file(&quot;net.pth&quot;)\n\n# save the optimizer to a .pth file\ntorch.save(optimizer.state_dict(), &quot;optimizer.pth&quot;)\nsave_file(&quot;optimizer.pth&quot;)\n\n# save the scheduler to a .pth file if it is not None\nif scheduler is not None:\n  torch.save(scheduler.state_dict(), &quot;scheduler.pth&quot;)\n  save_file(&quot;scheduler.pth&quot;)\n\n# create a dictionary to store the rest of the variables\nvar_dict = {\n  &quot;batch_size&quot;: batch_size,\n  &quot;n_epochs&quot;: n_epochs,\n  &quot;loss_name&quot;: loss_name,\n  &quot;optimizer_name&quot;: optimizer_name,\n  &quot;scheduler_name&quot;: scheduler_name,\n  &quot;n_units&quot;: n_units,\n  &quot;n_layers&quot;: n_layers,\n  &quot;hidden_activation_name&quot;: hidden_activation.__class__.__name__,\n  &quot;output_activation_name&quot;: output_activation.__class__.__name__,\n  &quot;lr&quot;: lr,\n}\n\n# save the dictionary to a .json file\nwith open(&quot;var_dict.json&quot;, &quot;w&quot;) as f:\n  json.dump(var_dict, f)\nsave_file(&quot;var_dict.json&quot;)\n\n# Saving the output of the training using pandas\ntrain_df = pd.DataFrame(\n    {\n        &quot;train_loss&quot;: train_losses,\n        &quot;test_loss&quot;: test_losses,\n        &quot;train_l1_norm&quot;: [m[&quot;l1_norm&quot;] for m in train_metrics],\n        &quot;test_l1_norm&quot;: [m[&quot;l1_norm&quot;] for m in test_metrics],\n        &quot;train_linf_norm&quot;: [m[&quot;linf_norm&quot;] for m in train_metrics],\n        &quot;test_linf_norm&quot;: [m[&quot;linf_norm&quot;] for m in test_metrics],\n    }\n)\ntrain_df.to_csv(&quot;train_output.csv&quot;, index=False)\nsave_file(&quot;train_output.csv&quot;)\n\">\n          <svg class=\"copy-icon\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\">\n            <path d=\"M5.5028 4.62704L5.5 6.75V17.2542C5.5 19.0491 6.95507 20.5042 8.75 20.5042L17.3663 20.5045C17.0573 21.3782 16.224 22.0042 15.2444 22.0042H8.75C6.12665 22.0042 4 19.8776 4 17.2542V6.75C4 5.76929 4.62745 4.93512 5.5028 4.62704ZM17.75 2C18.9926 2 20 3.00736 20 4.25V17.25C20 18.4926 18.9926 19.5 17.75 19.5H8.75C7.50736 19.5 6.5 18.4926 6.5 17.25V4.25C6.5 3.00736 7.50736 2 8.75 2H17.75ZM17.75 3.5H8.75C8.33579 3.5 8 3.83579 8 4.25V17.25C8 17.6642 8.33579 18 8.75 18H17.75C18.1642 18 18.5 17.6642 18.5 17.25V4.25C18.5 3.83579 18.1642 3.5 17.75 3.5Z\"></path>\n          </svg>\n          <span class=\"tooltip\">Copy</span>\n        </button>\n      </div>\n      <p>I hope this code works better for you. Please note that I have not tested it and you may need to make some adjustments.ðŸ˜Š</p>\n"},{"bot":false,"text":"\n      What is the fucntion of the last if statement and the line after it here? Do you think it should be there or is it a mistake? \n\n```python\nimport json\nimport pandas as pd\n\n# load the dictionary from the .json file\nwith open(\"var_dict.json\", \"r\") as f:\n  var_dict_loaded = json.load(f)\n\n# extract the variables from the dictionary\nbatch_size_loaded = var_dict_loaded[\"batch_size\"]\nn_epochs_loaded = var_dict_loaded[\"n_epochs\"]\nloss_name_loaded = var_dict_loaded[\"loss_name\"]\noptimizer_name_loaded = var_dict_loaded[\"optimizer_name\"]\nscheduler_name_loaded = var_dict_loaded[\"scheduler_name\"]\nn_units_loaded = var_dict_loaded[\"n_units\"]\nn_layers_loaded = var_dict_loaded[\"n_layers\"]\nhidden_activation_name_loaded = var_dict_loaded[\"hidden_activation_name\"]\noutput_activation_name_loaded = var_dict_loaded[\"output_activation_name\"]\nlr_loaded = var_dict_loaded[\"lr\"]\n\n# create the activation functions from their names\nif hidden_activation_name_loaded == \"ReLU\":\n  hidden_activation_loaded = nn.ReLU()\nelif hidden_activation_name_loaded == \"LeakyReLU\":\n  hidden_activation_loaded = nn.LeakyReLU() \nelif hidden_activation_name_loaded == \"ELU\":\n  hidden_activation_loaded = nn.ELU() \nelif hidden_activation_name_loaded == \"Tanh\":\n  hidden_activation_loaded = nn.Tanh()\nelse:\n  hidden_activation_loaded = nn.Sigmoid()\n\nif output_activation_name_loaded == \"ReLU\":\n  output_activation_loaded = nn.ReLU()\nelse:\n  output_activation_loaded = nn.Identity()\n\n# load the network from the .pth file\nnet_loaded = Net(n_layers_loaded, n_units_loaded, hidden_activation_loaded, output_activation_loaded).to(device)\nnet_loaded.load_state_dict(torch.load(\"net.pth\"))\n\n# create the loss function from its name\nif loss_name_loaded == \"MSE\":\n  loss_fn_loaded = nn.MSELoss()\nelif loss_name_loaded == \"MAE\":\n  loss_fn_loaded = nn.L1Loss()\nelif loss_name_loaded == \"Huber\":\n  loss_fn_loaded = nn.SmoothL1Loss() \nelse:\n  # create the log-cosh loss function\n  def log_cosh_loss_loaded(y_pred, y_true):\n    return torch.mean(torch.log(torch.cosh(y_pred - y_true)))\n  loss_fn_loaded = log_cosh_loss_loaded\n\n# load the optimizer from the .pth file\nif optimizer_name_loaded == \"SGD\":\n  optimizer_loaded = optim.SGD(net_loaded.parameters(), lr=lr_loaded)\nelif optimizer_name_loaded == \"Adam\":\n  optimizer_loaded = optim.Adam(net_loaded.parameters(), lr=lr_loaded)\nelif optimizer_name_loaded == \"RMSprop\":\n  optimizer_loaded = optim.RMSprop(net_loaded.parameters(), lr=lr_loaded)\nelse:\n  # Added loading the Adagrad optimizer\n  optimizer_loaded = optim.Adagrad(net_loaded.parameters(), lr=lr_loaded)\noptimizer_loaded.load_state_dict(torch.load(\"optimizer.pth\"))\n\n# load the scheduler from the .pth file\nscheduler_loaded_state_dict = torch.load(\"scheduler.pth\")\nif scheduler_name_loaded == \"StepLR\":\n  # Added getting the step_size and gamma parameters from the state dict\n  step_size = scheduler_loaded_state_dict[\"step_size\"]\n  gamma = scheduler_loaded_state_dict[\"gamma\"]\n  scheduler_loaded = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\nelif scheduler_name_loaded == \"ExponentialLR\":\n  # Added getting the gamma parameter from the state dict\n  gamma = scheduler_loaded_state_dict[\"gamma\"]\n  scheduler_loaded = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\nelif scheduler_name_loaded == \"CosineAnnealingLR\":\n  # Added getting the T_max parameter from the state dict\n  T_max = scheduler_loaded_state_dict[\"T_max\"]\n  scheduler_loaded = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max)\nelif scheduler_name_loaded == \"ReduceLROnPlateau\":\n  # Added getting the mode, factor, patience, threshold and min_lr parameters from the state dict\n  mode = scheduler_loaded_state_dict[\"mode\"]\n  factor = scheduler_loaded_state_dict[\"factor\"]\n  patience = scheduler_loaded_state_dict[\"patience\"]\n  threshold = scheduler_loaded_state_dict[\"threshold\"]\n  min_lr = scheduler_loaded_state_dict[\"min_lrs\"][0]\n  scheduler_loaded = optim.lr_scheduler.ReduceLROnPlateau(\n                    optimizer, mode=mode, factor=factor, patience=patience, threshold=threshold, min_lr=min_lr\n                )\nelse:\n  scheduler_loaded = None\n\nif scheduler_loaded is not None:\n  # Added loading the state dict to the scheduler_loaded\n  scheduler_loaded.load_state_dict(scheduler_state_dict)\n```\n      \n      \n    "},{"bot":true,"text":"<p>The last if statement and the line after it are checking if the scheduler_loaded is not None, and if so, loading the state dict to the scheduler_loaded. This is necessary because the scheduler_state_dict variable was defined earlier when loading the .pth file, but it was not applied to the scheduler_loaded yet. Without this step, the scheduler_loaded would not have the correct state and parameters that were saved before. Therefore, I think it is not a mistake, but a required step to load the scheduler correctly.ðŸ˜Š</p>\n"},{"bot":false,"text":"\n      Why does it come after the clause\n\n```python\nif scheduler_name_loaded == \"StepLR\":\n  scheduler_loaded = optim.lr_scheduler.StepLR(optimizer_loaded, step_size=10, gamma=0.1)\nelif scheduler_name_loaded == \"ExponentialLR\":\n  scheduler_loaded = optim.lr_scheduler.ExponentialLR(optimizer_loaded, gamma=0.9)\nelif scheduler_name_loaded == \"CosineAnnealingLR\":\n  scheduler_loaded = optim.lr_scheduler.CosineAnnealingLR(optimizer_loaded, T_max=10)\nelif scheduler_name_loaded == \"ReduceLROnPlateau\":\n  scheduler_loaded = optim.lr_scheduler.ReduceLROnPlateau(\n                    optimizer_loaded, mode=\"min\", factor=0.5, patience=5, threshold=0.0005, min_lr=1e-6\n                )\nelse:\n  scheduler_loaded = None\n```\n?\n      \n      \n    "},{"bot":true,"text":"<p>It comes after the clause because the clause is creating a new scheduler object based on the scheduler name, but it is not loading the state dict that was saved before. The state dict contains information such as the current epoch, the last epoch, and the base lrs that were used when saving the scheduler. These information are important to resume the training from where it left off. Therefore, after creating the scheduler object, we need to load the state dict to the scheduler object to make sure it has the correct state and parameters.ðŸ˜Š</p>\n"},{"bot":false,"text":"\n      Could you modifiy the code\n\n```python\nimport json\nimport pandas as pd\n\n# load the dictionary from the .json file\nwith open(\"var_dict.json\", \"r\") as f:\n  var_dict_loaded = json.load(f)\n\n# extract the variables from the dictionary\nbatch_size_loaded = var_dict_loaded[\"batch_size\"]\nn_epochs_loaded = var_dict_loaded[\"n_epochs\"]\nloss_name_loaded = var_dict_loaded[\"loss_name\"]\noptimizer_name_loaded = var_dict_loaded[\"optimizer_name\"]\nscheduler_name_loaded = var_dict_loaded[\"scheduler_name\"]\nn_units_loaded = var_dict_loaded[\"n_units\"]\nn_layers_loaded = var_dict_loaded[\"n_layers\"]\nhidden_activation_name_loaded = var_dict_loaded[\"hidden_activation_name\"]\noutput_activation_name_loaded = var_dict_loaded[\"output_activation_name\"]\nlr_loaded = var_dict_loaded[\"lr\"]\n\n# create the activation functions from their names\nif hidden_activation_name_loaded == \"ReLU\":\n  hidden_activation_loaded = nn.ReLU()\nelif hidden_activation_name_loaded == \"LeakyReLU\":\n  hidden_activation_loaded = nn.LeakyReLU() \nelif hidden_activation_name_loaded == \"ELU\":\n  hidden_activation_loaded = nn.ELU() \nelif hidden_activation_name_loaded == \"Tanh\":\n  hidden_activation_loaded = nn.Tanh()\nelse:\n  hidden_activation_loaded = nn.Sigmoid()\n\nif output_activation_name_loaded == \"ReLU\":\n  output_activation_loaded = nn.ReLU()\nelse:\n  output_activation_loaded = nn.Identity()\n\n# load the network from the .pth file\nnet_loaded = Net(n_layers_loaded, n_units_loaded, hidden_activation_loaded, output_activation_loaded).to(device)\nnet_loaded.load_state_dict(torch.load(\"net.pth\"))\n\n# create the loss function from its name\nif loss_name_loaded == \"MSE\":\n  loss_fn_loaded = nn.MSELoss()\nelif loss_name_loaded == \"MAE\":\n  loss_fn_loaded = nn.L1Loss()\nelif loss_name_loaded == \"Huber\":\n  loss_fn_loaded = nn.SmoothL1Loss() \nelse:\n  # create the log-cosh loss function\n  def log_cosh_loss_loaded(y_pred, y_true):\n    return torch.mean(torch.log(torch.cosh(y_pred - y_true)))\n  loss_fn_loaded = log_cosh_loss_loaded\n\n# load the optimizer from the .pth file\nif optimizer_name_loaded == \"SGD\":\n  optimizer_loaded = optim.SGD(net_loaded.parameters(), lr=lr_loaded)\nelif optimizer_name_loaded == \"Adam\":\n  optimizer_loaded = optim.Adam(net_loaded.parameters(), lr=lr_loaded)\nelif optimizer_name_loaded == \"RMSprop\":\n  optimizer_loaded = optim.RMSprop(net_loaded.parameters(), lr=lr_loaded)\nelse:\n  # Added loading the Adagrad optimizer\n  optimizer_loaded = optim.Adagrad(net_loaded.parameters(), lr=lr_loaded)\noptimizer_loaded.load_state_dict(torch.load(\"optimizer.pth\"))\n\nif scheduler_name_loaded == \"StepLR\":\n  scheduler_loaded = optim.lr_scheduler.StepLR(optimizer_loaded, step_size=10, gamma=0.1)\nelif scheduler_name_loaded == \"ExponentialLR\":\n  scheduler_loaded = optim.lr_scheduler.ExponentialLR(optimizer_loaded, gamma=0.9)\nelif scheduler_name_loaded == \"CosineAnnealingLR\":\n  scheduler_loaded = optim.lr_scheduler.CosineAnnealingLR(optimizer_loaded, T_max=10)\nelif scheduler_name_loaded == \"ReduceLROnPlateau\":\n  scheduler_loaded = optim.lr_scheduler.ReduceLROnPlateau(\n                    optimizer_loaded, mode=\"min\", factor=0.5, patience=5, threshold=0.0005, min_lr=1e-6\n                )\nelse:\n  scheduler_loaded = None\n\nif scheduler_loaded is not None:\n  scheduler_loaded.load_state_dict(torch.load(\"scheduler.pth\"))\n            \n\n# Loading the output of the training using pandas\ntrain_df_loaded = pd.read_csv(\"train_output.csv\")\ntrain_losses_loaded = train_df_loaded[\"train_loss\"].tolist()\ntest_losses_loaded = train_df_loaded[\"test_loss\"].tolist()\ntrain_metrics_loaded = [\n    {\n        \"l1_norm\": train_df_loaded[\"train_l1_norm\"][i],\n        \"linf_norm\": train_df_loaded[\"train_linf_norm\"][i],\n    }\n    for i in range(len(train_df_loaded))\n]\ntest_metrics_loaded = [\n    {\n        \"l1_norm\": train_df_loaded[\"test_l1_norm\"][i],\n        \"linf_norm\": train_df_loaded[\"test_linf_norm\"][i],\n    }\n    for i in range(len(train_df_loaded))\n]\n```\nsuch that from the scheduler.pth file the correct subparameters of the scheduler are loaded? Currently, I load e.g. for ReduceLROnPlateau some predefined subparameters, but I know that I can retrieve the actual values from the scheduler.pth file, and likewise for the other possible schedules. Please implement this.\n      \n      \n    "},{"bot":true,"text":"<p>I can try to help you with modifying your code. Here is one possible way to load the correct subparameters of the scheduler from the scheduler.pth file:</p>\n\n      <div class=\"code-block\">\n        <pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span> json\n<span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd\n\n<span class=\"hljs-comment\"># load the dictionary from the .json file</span>\n<span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">\"var_dict.json\"</span>, <span class=\"hljs-string\">\"r\"</span>) <span class=\"hljs-keyword\">as</span> f:\n  var_dict_loaded = json.load(f)\n\n<span class=\"hljs-comment\"># extract the variables from the dictionary</span>\nbatch_size_loaded = var_dict_loaded[<span class=\"hljs-string\">\"batch_size\"</span>]\nn_epochs_loaded = var_dict_loaded[<span class=\"hljs-string\">\"n_epochs\"</span>]\nloss_name_loaded = var_dict_loaded[<span class=\"hljs-string\">\"loss_name\"</span>]\noptimizer_name_loaded = var_dict_loaded[<span class=\"hljs-string\">\"optimizer_name\"</span>]\nscheduler_name_loaded = var_dict_loaded[<span class=\"hljs-string\">\"scheduler_name\"</span>]\nn_units_loaded = var_dict_loaded[<span class=\"hljs-string\">\"n_units\"</span>]\nn_layers_loaded = var_dict_loaded[<span class=\"hljs-string\">\"n_layers\"</span>]\nhidden_activation_name_loaded = var_dict_loaded[<span class=\"hljs-string\">\"hidden_activation_name\"</span>]\noutput_activation_name_loaded = var_dict_loaded[<span class=\"hljs-string\">\"output_activation_name\"</span>]\nlr_loaded = var_dict_loaded[<span class=\"hljs-string\">\"lr\"</span>]\n\n<span class=\"hljs-comment\"># create the activation functions from their names</span>\n<span class=\"hljs-keyword\">if</span> hidden_activation_name_loaded == <span class=\"hljs-string\">\"ReLU\"</span>:\n  hidden_activation_loaded = nn.ReLU()\n<span class=\"hljs-keyword\">elif</span> hidden_activation_name_loaded == <span class=\"hljs-string\">\"LeakyReLU\"</span>:\n  hidden_activation_loaded = nn.LeakyReLU() \n<span class=\"hljs-keyword\">elif</span> hidden_activation_name_loaded == <span class=\"hljs-string\">\"ELU\"</span>:\n  hidden_activation_loaded = nn.ELU() \n<span class=\"hljs-keyword\">elif</span> hidden_activation_name_loaded == <span class=\"hljs-string\">\"Tanh\"</span>:\n  hidden_activation_loaded = nn.Tanh()\n<span class=\"hljs-keyword\">else</span>:\n  hidden_activation_loaded = nn.Sigmoid()\n\n<span class=\"hljs-keyword\">if</span> output_activation_name_loaded == <span class=\"hljs-string\">\"ReLU\"</span>:\n  output_activation_loaded = nn.ReLU()\n<span class=\"hljs-keyword\">else</span>:\n  output_activation_loaded = nn.Identity()\n\n<span class=\"hljs-comment\"># load the network from the .pth file</span>\nnet_loaded = Net(n_layers_loaded, n_units_loaded, hidden_activation_loaded, output_activation_loaded).to(device)\nnet_loaded.load_state_dict(torch.load(<span class=\"hljs-string\">\"net.pth\"</span>))\n\n<span class=\"hljs-comment\"># create the loss function from its name</span>\n<span class=\"hljs-keyword\">if</span> loss_name_loaded == <span class=\"hljs-string\">\"MSE\"</span>:\n  loss_fn_loaded = nn.MSELoss()\n<span class=\"hljs-keyword\">elif</span> loss_name_loaded == <span class=\"hljs-string\">\"MAE\"</span>:\n  loss_fn_loaded = nn.L1Loss()\n<span class=\"hljs-keyword\">elif</span> loss_name_loaded == <span class=\"hljs-string\">\"Huber\"</span>:\n  loss_fn_loaded = nn.SmoothL1Loss() \n<span class=\"hljs-keyword\">else</span>:\n  <span class=\"hljs-comment\"># create the log-cosh loss function</span>\n  <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">log_cosh_loss</span>(<span class=\"hljs-params\">y_pred, y_true</span>):\n    <span class=\"hljs-keyword\">return</span> torch.mean(torch.log(torch.cosh(y_pred - y_true)))\n  loss_fn = log_cosh_loss\n\n<span class=\"hljs-comment\"># load the optimizer from the .pth file</span>\n<span class=\"hljs-keyword\">if</span> optimizer_name == <span class=\"hljs-string\">\"SGD\"</span>:\n  optimizer = optim.SGD(net.parameters(), lr=lr)\n<span class=\"hljs-keyword\">elif</span> optimizer_name == <span class=\"hljs-string\">\"Adam\"</span>:\n  optimizer = optim.Adam(net.parameters(), lr=lr)\n<span class=\"hljs-keyword\">elif</span> optimizer_name == <span class=\"hljs-string\">\"RMSprop\"</span>:\n  optimizer = optim.RMSprop(net.parameters(), lr=lr)\n<span class=\"hljs-keyword\">else</span>:\n  <span class=\"hljs-comment\"># Added loading the Adagrad optimizer</span>\n  optimizer = optim.Adagrad(net.parameters(), lr=lr)\noptimizer.load_state_dict(torch.load(<span class=\"hljs-string\">\"optimizer.pth\"</span>))\n\n<span class=\"hljs-comment\"># load the scheduler state dict from the .pth file</span>\nscheduler_state_dict = torch.load(<span class=\"hljs-string\">\"scheduler.pth\"</span>)\n\n<span class=\"hljs-comment\"># create the scheduler from its name and state dict</span>\n<span class=\"hljs-keyword\">if</span> scheduler_name == <span class=\"hljs-string\">\"StepLR\"</span>:\n  <span class=\"hljs-comment\"># Added getting the step_size and gamma parameters from the state dict</span>\n  step_size = scheduler_state_dict[<span class=\"hljs-string\">\"step_size\"</span>]\n  gamma = scheduler_state_dict[<span class=\"hljs-string\">\"gamma\"</span>]\n  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n<span class=\"hljs-keyword\">elif</span> scheduler_name == <span class=\"hljs-string\">\"ExponentialLR\"</span>:\n  <span class=\"hljs-comment\"># Added getting the gamma parameter from the state dict</span>\n  gamma = scheduler_state_dict[<span class=\"hljs-string\">\"gamma\"</span>]\n  scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n<span class=\"hljs-keyword\">elif</span> scheduler_name == <span class=\"hljs-string\">\"CosineAnnealingLR\"</span>:\n  <span class=\"hljs-comment\"># Added getting the T_max parameter from the state dict</span>\n  T_max = scheduler_state_dict[<span class=\"hljs-string\">\"T_max\"</span>]\n  scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max)\n<span class=\"hljs-keyword\">elif</span> scheduler_name == <span class=\"hljs-string\">\"ReduceLROnPlateau\"</span>:\n  <span class=\"hljs-comment\"># Added getting the mode, factor, patience, threshold and min_lr parameters from the state dict</span>\n  mode = scheduler_state_dict[<span class=\"hljs-string\">\"mode\"</span>]\n  factor = scheduler_state_dict[<span class=\"hljs-string\">\"factor\"</span>]\n  patience = scheduler_state_dict[<span class=\"hljs-string\">\"patience\"</span>]\n  threshold = scheduler_state_dict[<span class=\"hljs-string\">\"threshold\"</span>]\n  min_lr = scheduler_state_dict[<span class=\"hljs-string\">\"min_lrs\"</span>][<span class=\"hljs-number\">0</span>]\n  scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n                    optimizer, mode=mode, factor=factor, patience=patience, threshold=threshold, min_lr=min_lr\n                )\n<span class=\"hljs-keyword\">else</span>:\n  scheduler = <span class=\"hljs-literal\">None</span>\n\n<span class=\"hljs-keyword\">if</span> scheduler <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n  <span class=\"hljs-comment\"># Added loading the state dict to the scheduler</span>\n  scheduler.load_state_dict(scheduler_state_dict)\n\n<span class=\"hljs-comment\"># Loading the output of the training using pandas</span>\ntrain_df_loaded = pd.read_csv(<span class=\"hljs-string\">\"train_output.csv\"</span>)\ntrain_losses</code></pre>\n\n        <button type=\"button\" class=\"js-code-copy\" data-clipboard-text=\"import json\nimport pandas as pd\n\n# load the dictionary from the .json file\nwith open(&quot;var_dict.json&quot;, &quot;r&quot;) as f:\n  var_dict_loaded = json.load(f)\n\n# extract the variables from the dictionary\nbatch_size_loaded = var_dict_loaded[&quot;batch_size&quot;]\nn_epochs_loaded = var_dict_loaded[&quot;n_epochs&quot;]\nloss_name_loaded = var_dict_loaded[&quot;loss_name&quot;]\noptimizer_name_loaded = var_dict_loaded[&quot;optimizer_name&quot;]\nscheduler_name_loaded = var_dict_loaded[&quot;scheduler_name&quot;]\nn_units_loaded = var_dict_loaded[&quot;n_units&quot;]\nn_layers_loaded = var_dict_loaded[&quot;n_layers&quot;]\nhidden_activation_name_loaded = var_dict_loaded[&quot;hidden_activation_name&quot;]\noutput_activation_name_loaded = var_dict_loaded[&quot;output_activation_name&quot;]\nlr_loaded = var_dict_loaded[&quot;lr&quot;]\n\n# create the activation functions from their names\nif hidden_activation_name_loaded == &quot;ReLU&quot;:\n  hidden_activation_loaded = nn.ReLU()\nelif hidden_activation_name_loaded == &quot;LeakyReLU&quot;:\n  hidden_activation_loaded = nn.LeakyReLU() \nelif hidden_activation_name_loaded == &quot;ELU&quot;:\n  hidden_activation_loaded = nn.ELU() \nelif hidden_activation_name_loaded == &quot;Tanh&quot;:\n  hidden_activation_loaded = nn.Tanh()\nelse:\n  hidden_activation_loaded = nn.Sigmoid()\n\nif output_activation_name_loaded == &quot;ReLU&quot;:\n  output_activation_loaded = nn.ReLU()\nelse:\n  output_activation_loaded = nn.Identity()\n\n# load the network from the .pth file\nnet_loaded = Net(n_layers_loaded, n_units_loaded, hidden_activation_loaded, output_activation_loaded).to(device)\nnet_loaded.load_state_dict(torch.load(&quot;net.pth&quot;))\n\n# create the loss function from its name\nif loss_name_loaded == &quot;MSE&quot;:\n  loss_fn_loaded = nn.MSELoss()\nelif loss_name_loaded == &quot;MAE&quot;:\n  loss_fn_loaded = nn.L1Loss()\nelif loss_name_loaded == &quot;Huber&quot;:\n  loss_fn_loaded = nn.SmoothL1Loss() \nelse:\n  # create the log-cosh loss function\n  def log_cosh_loss(y_pred, y_true):\n    return torch.mean(torch.log(torch.cosh(y_pred - y_true)))\n  loss_fn = log_cosh_loss\n\n# load the optimizer from the .pth file\nif optimizer_name == &quot;SGD&quot;:\n  optimizer = optim.SGD(net.parameters(), lr=lr)\nelif optimizer_name == &quot;Adam&quot;:\n  optimizer = optim.Adam(net.parameters(), lr=lr)\nelif optimizer_name == &quot;RMSprop&quot;:\n  optimizer = optim.RMSprop(net.parameters(), lr=lr)\nelse:\n  # Added loading the Adagrad optimizer\n  optimizer = optim.Adagrad(net.parameters(), lr=lr)\noptimizer.load_state_dict(torch.load(&quot;optimizer.pth&quot;))\n\n# load the scheduler state dict from the .pth file\nscheduler_state_dict = torch.load(&quot;scheduler.pth&quot;)\n\n# create the scheduler from its name and state dict\nif scheduler_name == &quot;StepLR&quot;:\n  # Added getting the step_size and gamma parameters from the state dict\n  step_size = scheduler_state_dict[&quot;step_size&quot;]\n  gamma = scheduler_state_dict[&quot;gamma&quot;]\n  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\nelif scheduler_name == &quot;ExponentialLR&quot;:\n  # Added getting the gamma parameter from the state dict\n  gamma = scheduler_state_dict[&quot;gamma&quot;]\n  scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\nelif scheduler_name == &quot;CosineAnnealingLR&quot;:\n  # Added getting the T_max parameter from the state dict\n  T_max = scheduler_state_dict[&quot;T_max&quot;]\n  scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max)\nelif scheduler_name == &quot;ReduceLROnPlateau&quot;:\n  # Added getting the mode, factor, patience, threshold and min_lr parameters from the state dict\n  mode = scheduler_state_dict[&quot;mode&quot;]\n  factor = scheduler_state_dict[&quot;factor&quot;]\n  patience = scheduler_state_dict[&quot;patience&quot;]\n  threshold = scheduler_state_dict[&quot;threshold&quot;]\n  min_lr = scheduler_state_dict[&quot;min_lrs&quot;][0]\n  scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n                    optimizer, mode=mode, factor=factor, patience=patience, threshold=threshold, min_lr=min_lr\n                )\nelse:\n  scheduler = None\n\nif scheduler is not None:\n  # Added loading the state dict to the scheduler\n  scheduler.load_state_dict(scheduler_state_dict)\n\n# Loading the output of the training using pandas\ntrain_df_loaded = pd.read_csv(&quot;train_output.csv&quot;)\ntrain_losses\">\n          <svg class=\"copy-icon\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\">\n            <path d=\"M5.5028 4.62704L5.5 6.75V17.2542C5.5 19.0491 6.95507 20.5042 8.75 20.5042L17.3663 20.5045C17.0573 21.3782 16.224 22.0042 15.2444 22.0042H8.75C6.12665 22.0042 4 19.8776 4 17.2542V6.75C4 5.76929 4.62745 4.93512 5.5028 4.62704ZM17.75 2C18.9926 2 20 3.00736 20 4.25V17.25C20 18.4926 18.9926 19.5 17.75 19.5H8.75C7.50736 19.5 6.5 18.4926 6.5 17.25V4.25C6.5 3.00736 7.50736 2 8.75 2H17.75ZM17.75 3.5H8.75C8.33579 3.5 8 3.83579 8 4.25V17.25C8 17.6642 8.33579 18 8.75 18H17.75C18.1642 18 18.5 17.6642 18.5 17.25V4.25C18.5 3.83579 18.1642 3.5 17.75 3.5Z\"></path>\n          </svg>\n          <span class=\"tooltip\">Copy</span>\n        </button>\n      </div>\n      "},{"bot":false,"text":"\n      Please continue\n      \n      \n    "},{"bot":true,"text":"<p>Iâ€™m sorry, I donâ€™t understand what you want me to continue. Do you want me to continue modifying the code or explaining the code? Or do you want me to help you with something else? Please be more specific.ðŸ˜Š</p>\n"},{"bot":false,"text":"\n      Having loaded the scheduler again, how do I print its subparameters?\n      \n      \n    "},{"bot":true,"text":"<p>To print the subparameters of the scheduler, you can use the <code>__dict__</code> attribute of the scheduler object. This will return a dictionary of the attributes and values of the scheduler. For example:</p>\n\n      <div class=\"code-block\">\n        <pre><code class=\"language-python\"><span class=\"hljs-comment\"># print the subparameters of the scheduler</span>\n<span class=\"hljs-built_in\">print</span>(scheduler.__dict__)\n</code></pre>\n\n        <button type=\"button\" class=\"js-code-copy\" data-clipboard-text=\"# print the subparameters of the scheduler\nprint(scheduler.__dict__)\n\">\n          <svg class=\"copy-icon\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\">\n            <path d=\"M5.5028 4.62704L5.5 6.75V17.2542C5.5 19.0491 6.95507 20.5042 8.75 20.5042L17.3663 20.5045C17.0573 21.3782 16.224 22.0042 15.2444 22.0042H8.75C6.12665 22.0042 4 19.8776 4 17.2542V6.75C4 5.76929 4.62745 4.93512 5.5028 4.62704ZM17.75 2C18.9926 2 20 3.00736 20 4.25V17.25C20 18.4926 18.9926 19.5 17.75 19.5H8.75C7.50736 19.5 6.5 18.4926 6.5 17.25V4.25C6.5 3.00736 7.50736 2 8.75 2H17.75ZM17.75 3.5H8.75C8.33579 3.5 8 3.83579 8 4.25V17.25C8 17.6642 8.33579 18 8.75 18H17.75C18.1642 18 18.5 17.6642 18.5 17.25V4.25C18.5 3.83579 18.1642 3.5 17.75 3.5Z\"></path>\n          </svg>\n          <span class=\"tooltip\">Copy</span>\n        </button>\n      </div>\n      <p>This will output something like this (depending on the type and state of the scheduler):</p>\n\n      <div class=\"code-block\">\n        <pre><code class=\"language-python\">{<span class=\"hljs-string\">'optimizer'</span>: SGD (\nParameter Group <span class=\"hljs-number\">0</span>\n    dampening: <span class=\"hljs-number\">0</span>\n    lr: <span class=\"hljs-number\">0.01</span>\n    momentum: <span class=\"hljs-number\">0</span>\n    nesterov: <span class=\"hljs-literal\">False</span>\n    weight_decay: <span class=\"hljs-number\">0</span>\n), <span class=\"hljs-string\">'step_size'</span>: <span class=\"hljs-number\">10</span>, <span class=\"hljs-string\">'gamma'</span>: <span class=\"hljs-number\">0.1</span>, <span class=\"hljs-string\">'base_lrs'</span>: [<span class=\"hljs-number\">0.01</span>], <span class=\"hljs-string\">'last_epoch'</span>: <span class=\"hljs-number\">9</span>, <span class=\"hljs-string\">'_step_count'</span>: <span class=\"hljs-number\">10</span>, <span class=\"hljs-string\">'verbose'</span>: <span class=\"hljs-literal\">False</span>}\n</code></pre>\n\n        <button type=\"button\" class=\"js-code-copy\" data-clipboard-text=\"{'optimizer': SGD (\nParameter Group 0\n    dampening: 0\n    lr: 0.01\n    momentum: 0\n    nesterov: False\n    weight_decay: 0\n), 'step_size': 10, 'gamma': 0.1, 'base_lrs': [0.01], 'last_epoch': 9, '_step_count': 10, 'verbose': False}\n\">\n          <svg class=\"copy-icon\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\">\n            <path d=\"M5.5028 4.62704L5.5 6.75V17.2542C5.5 19.0491 6.95507 20.5042 8.75 20.5042L17.3663 20.5045C17.0573 21.3782 16.224 22.0042 15.2444 22.0042H8.75C6.12665 22.0042 4 19.8776 4 17.2542V6.75C4 5.76929 4.62745 4.93512 5.5028 4.62704ZM17.75 2C18.9926 2 20 3.00736 20 4.25V17.25C20 18.4926 18.9926 19.5 17.75 19.5H8.75C7.50736 19.5 6.5 18.4926 6.5 17.25V4.25C6.5 3.00736 7.50736 2 8.75 2H17.75ZM17.75 3.5H8.75C8.33579 3.5 8 3.83579 8 4.25V17.25C8 17.6642 8.33579 18 8.75 18H17.75C18.1642 18 18.5 17.6642 18.5 17.25V4.25C18.5 3.83579 18.1642 3.5 17.75 3.5Z\"></path>\n          </svg>\n          <span class=\"tooltip\">Copy</span>\n        </button>\n      </div>\n      <p>You can also access specific subparameters by using dot notation. For example:</p>\n\n      <div class=\"code-block\">\n        <pre><code class=\"language-python\"><span class=\"hljs-comment\"># print the step size of the scheduler</span>\n<span class=\"hljs-built_in\">print</span>(scheduler.step_size)\n</code></pre>\n\n        <button type=\"button\" class=\"js-code-copy\" data-clipboard-text=\"# print the step size of the scheduler\nprint(scheduler.step_size)\n\">\n          <svg class=\"copy-icon\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\">\n            <path d=\"M5.5028 4.62704L5.5 6.75V17.2542C5.5 19.0491 6.95507 20.5042 8.75 20.5042L17.3663 20.5045C17.0573 21.3782 16.224 22.0042 15.2444 22.0042H8.75C6.12665 22.0042 4 19.8776 4 17.2542V6.75C4 5.76929 4.62745 4.93512 5.5028 4.62704ZM17.75 2C18.9926 2 20 3.00736 20 4.25V17.25C20 18.4926 18.9926 19.5 17.75 19.5H8.75C7.50736 19.5 6.5 18.4926 6.5 17.25V4.25C6.5 3.00736 7.50736 2 8.75 2H17.75ZM17.75 3.5H8.75C8.33579 3.5 8 3.83579 8 4.25V17.25C8 17.6642 8.33579 18 8.75 18H17.75C18.1642 18 18.5 17.6642 18.5 17.25V4.25C18.5 3.83579 18.1642 3.5 17.75 3.5Z\"></path>\n          </svg>\n          <span class=\"tooltip\">Copy</span>\n        </button>\n      </div>\n      <p>This will output something like this:</p>\n\n      <div class=\"code-block\">\n        <pre><code class=\"language-python\"><span class=\"hljs-number\">10</span>\n</code></pre>\n\n        <button type=\"button\" class=\"js-code-copy\" data-clipboard-text=\"10\n\">\n          <svg class=\"copy-icon\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\">\n            <path d=\"M5.5028 4.62704L5.5 6.75V17.2542C5.5 19.0491 6.95507 20.5042 8.75 20.5042L17.3663 20.5045C17.0573 21.3782 16.224 22.0042 15.2444 22.0042H8.75C6.12665 22.0042 4 19.8776 4 17.2542V6.75C4 5.76929 4.62745 4.93512 5.5028 4.62704ZM17.75 2C18.9926 2 20 3.00736 20 4.25V17.25C20 18.4926 18.9926 19.5 17.75 19.5H8.75C7.50736 19.5 6.5 18.4926 6.5 17.25V4.25C6.5 3.00736 7.50736 2 8.75 2H17.75ZM17.75 3.5H8.75C8.33579 3.5 8 3.83579 8 4.25V17.25C8 17.6642 8.33579 18 8.75 18H17.75C18.1642 18 18.5 17.6642 18.5 17.25V4.25C18.5 3.83579 18.1642 3.5 17.75 3.5Z\"></path>\n          </svg>\n          <span class=\"tooltip\">Copy</span>\n        </button>\n      </div>\n      <p>I hope this helps you with printing the subparameters of the scheduler.ðŸ˜Š</p>\n"}],"date":"14 May 2023","favorite":false,"id":"fbW61my4pf2w8RZTiB32U4bREI4yO8G30Bbw","time":"8:12 am","title":"\n      Is it correct that Google colab does not save files when asked to do so in the notebook? E.g. I tried to save my neural network as net.pth in my notebook in colab, but I don't see the file. Or is it saved to a specific location? In any case, how can I save results produced in the notebook in google colab?\n      \n      \n    ","unified_id":false}
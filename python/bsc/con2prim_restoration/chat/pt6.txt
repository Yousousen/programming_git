{"convo":[{"bot":false,"text":"\n      I have implemented a neural network to learn conservative-to-primitive \nconversion in relativistic hydrodynamics. My input data are the three \nconservative variables, namely the rest mass density $\\rho$, the velocity in the \nx-direction $v_x$ and the specific internal energy $\\epsilon$. The output of the \nneural network is just the primitive variable pressure $p$; all the other \nprimtive variables can be calculated out of combinations of $p$ and conservative \nvariables, but we are not concerned with this further calculation.\n\nI would like to ask you to help me lower the errors in the neural network. \nSpecifically, I would like to get the L1 norm error down from being an order \n10^-2 to it being at least at most an order 10^-4 error, and I would like to get \nthe Linf norm error down from being an order 10^-1 to it being at most an order\n10^-3 error. Would you be able to help me with this?\n\nI first implemented z-score normalization as follows\n\n```python\n\n# Computing the mean and standard deviation of each input variable from the training set\nD_mean = torch.mean(x_train[:, 0])\nD_std = torch.std(x_train[:, 0])\nSx_mean = torch.mean(x_train[:, 1])\nSx_std = torch.std(x_train[:, 1])\ntau_mean = torch.mean(x_train[:, 2])\ntau_std = torch.std(x_train[:, 2])\n\n# Applying z-score normalization to both train and test sets using the statistics from the training set\nx_train[:, 0] = torch.sub(x_train[:, 0], D_mean).div(D_std)\nx_train[:, 1] = torch.sub(x_train[:, 1], Sx_mean).div(Sx_std)\nx_train[:, 2] = torch.sub(x_train[:, 2], tau_mean).div(tau_std)\nx_test[:, 0] = torch.sub(x_test[:, 0], D_mean).div(D_std)\nx_test[:, 1] = torch.sub(x_test[:, 1], Sx_mean).div(Sx_std)\nx_test[:, 2] = torch.sub(x_test[:, 2], tau_mean).div(tau_std)\n\n```\n\nAs part of the following code to generate the data for my NN\n\n```python\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport optuna\nimport tensorboardX as tbx\n\n# Checking if GPU is available and setting the device accordingly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# ## Constants and flags to set\n# Defining some constants and parameters for convenience.\n\n# In[62]:\n\n\nN_TRIALS = 150 # Number of trials for hyperparameter optimization\nOPTIMIZE = False # Whether to optimize the hyperparameters or to use predetermined values from Dieseldorst et al..\n\n# I try out here the values as obtained in Optuna run 5, but I will increase the number of epochs.\n# N_LAYERS_NO_OPT = 3\n# N_UNITS_NO_OPT = [78, 193, 99]\n# HIDDEN_ACTIVATION_NAME_NO_OPT = \"ReLU\"\n# OUTPUT_ACTIVATION_NAME_NO_OPT = \"Linear\"\n# LOSS_NAME_NO_OPT = \"MSE\"\n# OPTIMIZER_NAME_NO_OPT = \"Adam\"\n# LR_NO_OPT = 0.00036516467819506355\n# BATCH_SIZE_NO_OPT = 170\n# N_EPOCHS_NO_OPT = 400\n# SCHEDULER_NAME_NO_OPT = \"ReduceLROnPlateau\"\n\n# Dieselhorst hyperparameters\nN_LAYERS_NO_OPT = 2\nN_UNITS_NO_OPT = [600, 200]\nHIDDEN_ACTIVATION_NAME_NO_OPT = \"Sigmoid\"\nOUTPUT_ACTIVATION_NAME_NO_OPT = \"ReLU\"\nLOSS_NAME_NO_OPT = \"MSE\"\nOPTIMIZER_NAME_NO_OPT = \"Adam\"\nLR_NO_OPT = 6e-3\nBATCH_SIZE_NO_OPT = 32\nN_EPOCHS_NO_OPT = 400\nSCHEDULER_NAME_NO_OPT = \"ReduceLROnPlateau\"\n\n\nc = 1  # Speed of light (used in compute_conserved_variables and sample_primitive_variables functions)\ngamma = 5 / 3  # Adiabatic index (used in eos_analytic function)\nn_train_samples = 80000 # Number of training samples (used in generate_input_data and generate_labels functions)\nn_test_samples = 10000 # Number of test samples (used in generate_input_data and generate_labels functions)\nrho_interval = (0, 10.1) # Sampling interval for rest-mass density (used in sample_primitive_variables function)\nvx_interval = (0, 0.721 * c) # Sampling interval for velocity in x-direction (used in sample_primitive_variables function)\nepsilon_interval = (0, 2.02) # Sampling interval for specific internal energy (used in sample_primitive_variables function)\n\nnp.random.seed(2) # Uncomment for pseudorandom data.\n\n\n# ## Generating the data\n\n# In[63]:\n\n\n# Defining an analytic equation of state (EOS) for an ideal gas\ndef eos_analytic(rho, epsilon):\n    # Adding some assertions to check that the input tensors are valid and have \n    the expected shape and type assert isinstance(rho, torch.Tensor), \"rho must \n    be a torch.Tensor\"\n    assert isinstance(epsilon, torch.Tensor), \"epsilon must be a torch.Tensor\"\n    assert rho.shape == epsilon.shape, \"rho and epsilon must have the same shape\"\n    assert rho.ndim == 1, \"rho and epsilon must be one-dimensional tensors\"\n    assert rho.dtype == torch.float32, \"rho and epsilon must have dtype torch.float32\"\n\n    return (gamma - 1) * rho * epsilon\n\n\n# Defining a function that samples primitive variables from uniform distributions\ndef sample_primitive_variables(n_samples):\n    # Sampling from uniform distributions with intervals matching Dieseldorst et \n    al.\n    rho = np.random.uniform(*rho_interval, size=n_samples)  # Rest-mass density\n    vx = np.random.uniform(*vx_interval, size=n_samples)  # Velocity in x-direction\n    epsilon = np.random.uniform(*epsilon_interval, size=n_samples)  # Specific internal energy\n\n    # Returning the primitive variables\n    return rho, vx, epsilon\n\n\n# Defining a function that computes conserved variables from primitive variables\ndef compute_conserved_variables(rho, vx, epsilon):\n\n    # Computing the pressure from the primitive variables using the EOS\n    p = eos_analytic(rho, epsilon)\n    # Computing the Lorentz factor from the velocity.\n    W = 1 / torch.sqrt(1 - vx ** 2 / c ** 2)\n    # Specific enthalpy\n    h = 1 + epsilon + p / rho  \n\n    # Computing the conserved variables from the primitive variables\n    D = rho * W  # Conserved density\n    Sx = rho * h * W ** 2 * vx  # Conserved momentum in x-direction\n    tau = rho * h * W ** 2 - p - D  # Conserved energy density\n\n    # Returning the conserved variables\n    return D, Sx, tau\n\n# Defining a function that generates input data (conserved variables) from given samples of primitive variables\ndef generate_input_data(rho, vx, epsilon):\n    # Converting the numpy arrays to torch tensors and moving them to the device\n    rho = torch.tensor(rho, dtype=torch.float32).to(device)\n    vx = torch.tensor(vx, dtype=torch.float32).to(device)\n    epsilon = torch.tensor(epsilon, dtype=torch.float32).to(device)\n\n    # Computing the conserved variables using the compute_conserved_variables function\n    D, Sx, tau = compute_conserved_variables(rho, vx, epsilon)\n\n    # Stacking the conserved variables into a torch tensor\n    x = torch.stack([D, Sx, tau], axis=1)\n\n    # Returning the input data tensor\n    return x\n\n# Defining a function that generates output data (labels) from given samples of primitive variables\ndef generate_labels(rho, epsilon):\n    # Converting the numpy arrays to torch tensors and moving them to the device\n    rho = torch.tensor(rho, dtype=torch.float32).to(device)\n    epsilon = torch.tensor(epsilon, dtype=torch.float32).to(device)\n\n    # Computing the pressure from the primitive variables using the EOS\n    p = eos_analytic(rho, epsilon)\n\n    # Returning the output data tensor\n    return p\n\n\n# Sampling the primitive variables using the sample_primitive_variables function\n\n# In[64]:\n\n\nrho_train, vx_train, epsilon_train = sample_primitive_variables(n_train_samples)\nrho_test, vx_test, epsilon_test = sample_primitive_variables(n_test_samples)\n\n\n```\n\nHowever, this in fact increased the L1 norm error by an order of magnitude while it made the Linf norm error slightly worse by a value of ~0.1; these increases in the errors for the L1 norm and Linf norm hold for both the training and the test data.\n      \n      \n    "}],"date":"14 May 2023","favorite":false,"id":"JKTZKtzMrPvVCbyXlIuew7tzat4nd5uEgno5","time":"10:21 am","title":"\n      I have implemented a neural network to learn conservative-to-primitive \nconversion in relativistic hydrodynamics. My input data are the three \nconservative variables, namely the rest mass density $\\rho$, the velocity in the \nx-direction $v_x$ and the specific internal energy $\\epsilon$. The output of the \nneural network is just the primitive variable pressure $p$; all the other \nprimtive variables can be calculated out of combinations of $p$ and conservative \nvariables, but we are not concerned with this further calculation.\n\nI would like to ask you to help me lower the errors in the neural network. \nSpecifically, I would like to get the L1 norm error down from being an order \n10^-2 to it being at least at most an order 10^-4 error, and I would like to get \nthe Linf norm error down from being an order 10^-1 to it being at most an order\n10^-3 error. Would you be able to help me with this?\n\nI first implemented z-score normalization as follows\n\n```python\n\n# Computing the mean and standard deviation of each input variable from the training set\nD_mean = torch.mean(x_train[:, 0])\nD_std = torch.std(x_train[:, 0])\nSx_mean = torch.mean(x_train[:, 1])\nSx_std = torch.std(x_train[:, 1])\ntau_mean = torch.mean(x_train[:, 2])\ntau_std = torch.std(x_train[:, 2])\n\n# Applying z-score normalization to both train and test sets using the statistics from the training set\nx_train[:, 0] = torch.sub(x_train[:, 0], D_mean).div(D_std)\nx_train[:, 1] = torch.sub(x_train[:, 1], Sx_mean).div(Sx_std)\nx_train[:, 2] = torch.sub(x_train[:, 2], tau_mean).div(tau_std)\nx_test[:, 0] = torch.sub(x_test[:, 0], D_mean).div(D_std)\nx_test[:, 1] = torch.sub(x_test[:, 1], Sx_mean).div(Sx_std)\nx_test[:, 2] = torch.sub(x_test[:, 2], tau_mean).div(tau_std)\n\n```\n\nAs part of the following code to generate the data for my NN\n\n```python\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport optuna\nimport tensorboardX as tbx\n\n# Checking if GPU is available and setting the device accordingly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# ## Constants and flags to set\n# Defining some constants and parameters for convenience.\n\n# In[62]:\n\n\nN_TRIALS = 150 # Number of trials for hyperparameter optimization\nOPTIMIZE = False # Whether to optimize the hyperparameters or to use predetermined values from Dieseldorst et al..\n\n# I try out here the values as obtained in Optuna run 5, but I will increase the number of epochs.\n# N_LAYERS_NO_OPT = 3\n# N_UNITS_NO_OPT = [78, 193, 99]\n# HIDDEN_ACTIVATION_NAME_NO_OPT = \"ReLU\"\n# OUTPUT_ACTIVATION_NAME_NO_OPT = \"Linear\"\n# LOSS_NAME_NO_OPT = \"MSE\"\n# OPTIMIZER_NAME_NO_OPT = \"Adam\"\n# LR_NO_OPT = 0.00036516467819506355\n# BATCH_SIZE_NO_OPT = 170\n# N_EPOCHS_NO_OPT = 400\n# SCHEDULER_NAME_NO_OPT = \"ReduceLROnPlateau\"\n\n# Dieselhorst hyperparameters\nN_LAYERS_NO_OPT = 2\nN_UNITS_NO_OPT = [600, 200]\nHIDDEN_ACTIVATION_NAME_NO_OPT = \"Sigmoid\"\nOUTPUT_ACTIVATION_NAME_NO_OPT = \"ReLU\"\nLOSS_NAME_NO_OPT = \"MSE\"\nOPTIMIZER_NAME_NO_OPT = \"Adam\"\nLR_NO_OPT = 6e-3\nBATCH_SIZE_NO_OPT = 32\nN_EPOCHS_NO_OPT = 400\nSCHEDULER_NAME_NO_OPT = \"ReduceLROnPlateau\"\n\n\nc = 1  # Speed of light (used in compute_conserved_variables and sample_primitive_variables functions)\ngamma = 5 / 3  # Adiabatic index (used in eos_analytic function)\nn_train_samples = 80000 # Number of training samples (used in generate_input_data and generate_labels functions)\nn_test_samples = 10000 # Number of test samples (used in generate_input_data and generate_labels functions)\nrho_interval = (0, 10.1) # Sampling interval for rest-mass density (used in sample_primitive_variables function)\nvx_interval = (0, 0.721 * c) # Sampling interval for velocity in x-direction (used in sample_primitive_variables function)\nepsilon_interval = (0, 2.02) # Sampling interval for specific internal energy (used in sample_primitive_variables function)\n\nnp.random.seed(2) # Uncomment for pseudorandom data.\n\n\n# ## Generating the data\n\n# In[63]:\n\n\n# Defining an analytic equation of state (EOS) for an ideal gas\ndef eos_analytic(rho, epsilon):\n    # Adding some assertions to check that the input tensors are valid and have \n    the expected shape and type assert isinstance(rho, torch.Tensor), \"rho must \n    be a torch.Tensor\"\n    assert isinstance(epsilon, torch.Tensor), \"epsilon must be a torch.Tensor\"\n    assert rho.shape == epsilon.shape, \"rho and epsilon must have the same shape\"\n    assert rho.ndim == 1, \"rho and epsilon must be one-dimensional tensors\"\n    assert rho.dtype == torch.float32, \"rho and epsilon must have dtype torch.float32\"\n\n    return (gamma - 1) * rho * epsilon\n\n\n# Defining a function that samples primitive variables from uniform distributions\ndef sample_primitive_variables(n_samples):\n    # Sampling from uniform distributions with intervals matching Dieseldorst et \n    al.\n    rho = np.random.uniform(*rho_interval, size=n_samples)  # Rest-mass density\n    vx = np.random.uniform(*vx_interval, size=n_samples)  # Velocity in x-direction\n    epsilon = np.random.uniform(*epsilon_interval, size=n_samples)  # Specific internal energy\n\n    # Returning the primitive variables\n    return rho, vx, epsilon\n\n\n# Defining a function that computes conserved variables from primitive variables\ndef compute_conserved_variables(rho, vx, epsilon):\n\n    # Computing the pressure from the primitive variables using the EOS\n    p = eos_analytic(rho, epsilon)\n    # Computing the Lorentz factor from the velocity.\n    W = 1 / torch.sqrt(1 - vx ** 2 / c ** 2)\n    # Specific enthalpy\n    h = 1 + epsilon + p / rho  \n\n    # Computing the conserved variables from the primitive variables\n    D = rho * W  # Conserved density\n    Sx = rho * h * W ** 2 * vx  # Conserved momentum in x-direction\n    tau = rho * h * W ** 2 - p - D  # Conserved energy density\n\n    # Returning the conserved variables\n    return D, Sx, tau\n\n# Defining a function that generates input data (conserved variables) from given samples of primitive variables\ndef generate_input_data(rho, vx, epsilon):\n    # Converting the numpy arrays to torch tensors and moving them to the device\n    rho = torch.tensor(rho, dtype=torch.float32).to(device)\n    vx = torch.tensor(vx, dtype=torch.float32).to(device)\n    epsilon = torch.tensor(epsilon, dtype=torch.float32).to(device)\n\n    # Computing the conserved variables using the compute_conserved_variables function\n    D, Sx, tau = compute_conserved_variables(rho, vx, epsilon)\n\n    # Stacking the conserved variables into a torch tensor\n    x = torch.stack([D, Sx, tau], axis=1)\n\n    # Returning the input data tensor\n    return x\n\n# Defining a function that generates output data (labels) from given samples of primitive variables\ndef generate_labels(rho, epsilon):\n    # Converting the numpy arrays to torch tensors and moving them to the device\n    rho = torch.tensor(rho, dtype=torch.float32).to(device)\n    epsilon = torch.tensor(epsilon, dtype=torch.float32).to(device)\n\n    # Computing the pressure from the primitive variables using the EOS\n    p = eos_analytic(rho, epsilon)\n\n    # Returning the output data tensor\n    return p\n\n\n# Sampling the primitive variables using the sample_primitive_variables function\n\n# In[64]:\n\n\nrho_train, vx_train, epsilon_train = sample_primitive_variables(n_train_samples)\nrho_test, vx_test, epsilon_test = sample_primitive_variables(n_test_samples)\n\n\n```\n\nHowever, this in fact increased the L1 norm error by an order of magnitude while it made the Linf norm error slightly worse by a value of ~0.1; these increases in the errors for the L1 norm and Linf norm hold for both the training and the test data.\n      \n      \n    ","unified_id":false}
{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network to learn conservative-to-primitive conversion in relativistic hydrodynamics\n",
    "We use Optuna to do a type of Bayesian optimization of the hyperparameters of the model. We then train the model using these hyperparameters to recover the primitive pressure from the conserved variables.\n",
    "\n",
    "Use this first cell to convert this notebook to a python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bvptr\\anaconda3\\envs\\bsc\\Lib\\site-packages\\traitlets\\traitlets.py:2548: FutureWarning: Supporting extra quotes around strings is deprecated in traitlets 5.0. You can use 'remove_cell' instead of \"'remove_cell'\" if you require traitlets >=5.\n",
      "  warn(\n",
      "[NbConvertApp] Converting notebook pt2.ipynb to script\n",
      "[NbConvertApp] Writing 31488 bytes to pt2.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert pt2.ipynb --TagRemovePreprocessor.enabled=True --TagRemovePreprocessor.remove_cell_tags='{\"remove_cell\"}' --to script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import optuna\n",
    "import tensorboardX as tbx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if GPU is available and setting the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Defining some constants or parameters for convenience\n",
    "c = 1  # Speed of light (used in compute_conserved_variables and sample_primitive_variables functions)\n",
    "gamma = 5 / 3  # Adiabatic index (used in eos_analytic function)\n",
    "n_train_samples = 80000 # Number of training samples (used in generate_input_data and generate_labels functions)\n",
    "n_test_samples = 10000 # Number of test samples (used in generate_input_data and generate_labels functions)\n",
    "rho_interval = (0, 10.1) # Sampling interval for rest-mass density (used in sample_primitive_variables function)\n",
    "vx_interval = (0, 0.721 * c) # Sampling interval for velocity in x-direction (used in sample_primitive_variables function)\n",
    "epsilon_interval = (0, 2.02) # Sampling interval for specific internal energy (used in sample_primitive_variables function)\n",
    "\n",
    "# Uncomment for pseudorandom data.\n",
    "np.random.seed(0)\n",
    "\n",
    "# Defining an analytic equation of state (EOS) for an ideal gas\n",
    "def eos_analytic(rho, epsilon):\n",
    "    \"\"\"Computes the pressure from rest-mass density and specific internal energy using an analytic EOS.\n",
    "\n",
    "    Args:\n",
    "        rho (torch.Tensor): The rest-mass density tensor of shape (n_samples,).\n",
    "        epsilon (torch.Tensor): The specific internal energy tensor of shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The pressure tensor of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    # Adding some assertions to check that the input tensors are valid and have the expected shape and type \n",
    "    assert isinstance(rho, torch.Tensor), \"rho must be a torch.Tensor\"\n",
    "    assert isinstance(epsilon, torch.Tensor), \"epsilon must be a torch.Tensor\"\n",
    "    assert rho.shape == epsilon.shape, \"rho and epsilon must have the same shape\"\n",
    "    assert rho.ndim == 1, \"rho and epsilon must be one-dimensional tensors\"\n",
    "    assert rho.dtype == torch.float32, \"rho and epsilon must have dtype torch.float32\"\n",
    "\n",
    "    return (gamma - 1) * rho * epsilon\n",
    "\n",
    "\n",
    "# Defining a function that samples primitive variables from uniform distributions\n",
    "def sample_primitive_variables(n_samples):\n",
    "    \"\"\"Samples primitive variables from uniform distributions.\n",
    "\n",
    "    Args:\n",
    "        n_samples (int): The number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of (rho, vx, epsilon), where rho is rest-mass density,\n",
    "            vx is velocity in x-direction,\n",
    "            epsilon is specific internal energy,\n",
    "            each being a numpy array of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    # Sampling from uniform distributions with intervals matching Dieseldorst et al.\n",
    "    rho = np.random.uniform(*rho_interval, size=n_samples)  # Rest-mass density\n",
    "    vx = np.random.uniform(*vx_interval, size=n_samples)  # Velocity in x-direction\n",
    "    epsilon = np.random.uniform(*epsilon_interval, size=n_samples)  # Specific internal energy\n",
    "\n",
    "    # Returning the primitive variables\n",
    "    return rho, vx, epsilon\n",
    "\n",
    "\n",
    "# Defining a function that computes conserved variables from primitive variables\n",
    "def compute_conserved_variables(rho, vx, epsilon):\n",
    "    \"\"\"Computes conserved variables from primitive variables.\n",
    "\n",
    "    Args:\n",
    "        rho (torch.Tensor): The rest-mass density tensor of shape (n_samples,).\n",
    "        vx (torch.Tensor): The velocity in x-direction tensor of shape (n_samples,).\n",
    "        epsilon (torch.Tensor): The specific internal energy tensor of shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of (D, Sx, tau), where D is conserved density,\n",
    "            Sx is conserved momentum in x-direction,\n",
    "            tau is conserved energy density,\n",
    "            each being a torch tensor of shape (n_samples,).\n",
    "    \"\"\"\n",
    "\n",
    "    # Computing the pressure from the primitive variables using the EOS\n",
    "    p = eos_analytic(rho, epsilon)\n",
    "    # Computing the Lorentz factor from the velocity.\n",
    "    W = 1 / torch.sqrt(1 - vx ** 2 / c ** 2)\n",
    "    # Specific enthalpy\n",
    "    h = 1 + epsilon + p / rho  \n",
    "\n",
    "    # Computing the conserved variables from the primitive variables\n",
    "    D = rho * W  # Conserved density\n",
    "    Sx = rho * h * W ** 2 * vx  # Conserved momentum in x-direction\n",
    "    tau = rho * h * W ** 2 - p - D  # Conserved energy density\n",
    "\n",
    "    # Returning the conserved variables\n",
    "    return D, Sx, tau\n",
    "\n",
    "\n",
    "# Defining a function that generates input data (conserved variables) from random samples of primitive variables\n",
    "def generate_input_data(n_samples):\n",
    "    \"\"\"Generates input data (conserved variables) from random samples of primitive variables.\n",
    "\n",
    "    Args:\n",
    "        n_samples (int): The number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The input data tensor of shape (n_samples, 3).\n",
    "    \"\"\"\n",
    "    # Sampling the primitive variables using the sample_primitive_variables function\n",
    "    rho, vx, epsilon = sample_primitive_variables(n_samples)\n",
    "\n",
    "    # Converting the numpy arrays to torch tensors and moving them to the device\n",
    "    rho = torch.tensor(rho, dtype=torch.float32).to(device)\n",
    "    vx = torch.tensor(vx, dtype=torch.float32).to(device)\n",
    "    epsilon = torch.tensor(epsilon, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Computing the conserved variables using the compute_conserved_variables function\n",
    "    D, Sx, tau = compute_conserved_variables(rho, vx, epsilon)\n",
    "\n",
    "    # Stacking the conserved variables into a torch tensor\n",
    "    x = torch.stack([D, Sx, tau], axis=1)\n",
    "\n",
    "    # Returning the input data tensor\n",
    "    return x\n",
    "\n",
    "# Defining a function that generates output data (labels) from random samples of primitive variables\n",
    "def generate_labels(n_samples):\n",
    "    \"\"\"Generates output data (labels) from random samples of primitive variables.\n",
    "\n",
    "    Args:\n",
    "        n_samples (int): The number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The output data tensor of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    # Sampling the primitive variables using the sample_primitive_variables function\n",
    "    rho, _, epsilon = sample_primitive_variables(n_samples)\n",
    "\n",
    "    # Converting the numpy arrays to torch tensors and moving them to the device\n",
    "    rho = torch.tensor(rho, dtype=torch.float32).to(device)\n",
    "    epsilon = torch.tensor(epsilon, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Computing the pressure from the primitive variables using the EOS\n",
    "    p = eos_analytic(rho, epsilon)\n",
    "\n",
    "    # Returning the output data tensor\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x400 with 0 Axes>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(array([16008., 10447.,  8413.,  6973.,  5797.,  5255.,  4566.,  3807.,\n",
       "         3448.,  2976.,  2544.,  2202.,  1843.,  1586.,  1301.,  1045.,\n",
       "          784.,   538.,   339.,   128.]),\n",
       " array([1.2271406e-05, 6.7581248e-01, 1.3516127e+00, 2.0274129e+00,\n",
       "        2.7032132e+00, 3.3790135e+00, 4.0548139e+00, 4.7306137e+00,\n",
       "        5.4064140e+00, 6.0822144e+00, 6.7580147e+00, 7.4338150e+00,\n",
       "        8.1096153e+00, 8.7854156e+00, 9.4612160e+00, 1.0137015e+01,\n",
       "        1.0812816e+01, 1.1488616e+01, 1.2164416e+01, 1.2840217e+01,\n",
       "        1.3516017e+01], dtype=float32),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'y_train')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Frequency')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(array([2035., 1215., 1028.,  837.,  693.,  683.,  570.,  504.,  461.,\n",
       "         370.,  304.,  274.,  230.,  217.,  180.,  167.,  101.,   68.,\n",
       "          43.,   20.]),\n",
       " array([1.02815415e-04, 6.66673183e-01, 1.33324361e+00, 1.99981403e+00,\n",
       "        2.66638446e+00, 3.33295465e+00, 3.99952507e+00, 4.66609573e+00,\n",
       "        5.33266592e+00, 5.99923611e+00, 6.66580677e+00, 7.33237696e+00,\n",
       "        7.99894762e+00, 8.66551781e+00, 9.33208847e+00, 9.99865818e+00,\n",
       "        1.06652288e+01, 1.13317995e+01, 1.19983702e+01, 1.26649399e+01,\n",
       "        1.33315105e+01], dtype=float32),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'y_test')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Frequency')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: torch.Size([80000, 3])\n",
      "Shape of y_train: torch.Size([80000])\n",
      "Shape of x_test: torch.Size([10000, 3])\n",
      "Shape of y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generating the input and output data for train and test sets using the functions defined\n",
    "# Using the same number of samples as Dieseldorst et al.\n",
    "x_train = generate_input_data(n_train_samples)\n",
    "y_train = generate_labels(n_train_samples) \n",
    "x_test = generate_input_data(n_test_samples) \n",
    "y_test = generate_labels(n_test_samples) \n",
    "\n",
    "# Checking if our output is always positive by plotting a histogram of y_train and y_test tensors \n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y_train.cpu().numpy(), bins=20)\n",
    "plt.xlabel(\"y_train\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(y_test.cpu().numpy(), bins=20)\n",
    "plt.xlabel(\"y_test\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Checking the shapes of the data tensors\n",
    "print(\"Shape of x_train:\", x_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of x_test:\", x_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a class for the network\n",
    "class Net(nn.Module):\n",
    "    \"\"\"A class for creating a network with a\n",
    "    variable number of hidden layers and units.\n",
    "\n",
    "    Attributes:\n",
    "        n_layers (int): The number of hidden layers in the network.\n",
    "        n_units (list): A list of integers representing the number of units in each hidden layer.\n",
    "        hidden_activation (torch.nn.Module): The activation function for the hidden layers.\n",
    "        output_activation (torch.nn.Module): The activation function for the output layer.\n",
    "        layers (torch.nn.ModuleList): A list of linear layers in the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_layers, n_units, hidden_activation, output_activation):\n",
    "        \"\"\"Initializes the network with the given hyperparameters.\n",
    "\n",
    "        Args:\n",
    "            n_layers (int): The number of hidden layers in the network.\n",
    "            n_units (list): A list of integers representing the number of units in each hidden layer.\n",
    "            hidden_activation (torch.nn.Module): The activation function for the hidden layers.\n",
    "            output_activation (torch.nn.Module): The activation function for the output layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_units = n_units\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.output_activation = output_activation\n",
    "\n",
    "        # Creating a list of linear layers with different numbers of units for each layer\n",
    "        self.layers = nn.ModuleList([nn.Linear(3, n_units[0])])\n",
    "        for i in range(1, n_layers):\n",
    "            self.layers.append(nn.Linear(n_units[i - 1], n_units[i]))\n",
    "        self.layers.append(nn.Linear(n_units[-1], 1))\n",
    "\n",
    "        # Adding some assertions to check that the input arguments are valid # CHANGED: Added assertions and comments\n",
    "        assert isinstance(n_layers, int) and n_layers > 0, \"n_layers must be a positive integer\"\n",
    "        assert isinstance(n_units, list) and len(n_units) == n_layers, \"n_units must be a list of length n_layers\"\n",
    "        assert all(isinstance(n, int) and n > 0 for n in n_units), \"n_units must contain positive integers\"\n",
    "        assert isinstance(hidden_activation, nn.Module), \"hidden_activation must be a torch.nn.Module\"\n",
    "        assert isinstance(output_activation, nn.Module), \"output_activation must be a torch.nn.Module\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Performs a forward pass on the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, 3).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape (batch_size, 1).\n",
    "        \"\"\"\n",
    "        # Looping over the hidden layers and applying the linear transformation and the activation function\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.hidden_activation(layer(x))\n",
    "        # Applying the linear transformation and the activation function on the output layer\n",
    "        x = self.output_activation(self.layers[-1](x))\n",
    "\n",
    "        # Returning the output tensor\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model and search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to create a trial network and optimizer\n",
    "def create_model(trial):\n",
    "    \"\"\"Creates a trial network and optimizer based on the sampled hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): The trial object that contains the hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of (net, loss_fn, optimizer, batch_size, n_epochs,\n",
    "            scheduler, loss_name, optimizer_name, scheduler_name,\n",
    "            n_units, n_layers, hidden_activation, output_activation),\n",
    "            where net is the trial network,\n",
    "            loss_fn is the loss function,\n",
    "            optimizer is the optimizer,\n",
    "            batch_size is the batch size,\n",
    "            n_epochs is the number of epochs,\n",
    "            scheduler is the learning rate scheduler,\n",
    "            loss_name is the name of the loss function,\n",
    "            optimizer_name is the name of the optimizer,\n",
    "            scheduler_name is the name of the scheduler,\n",
    "            n_units is a list of integers representing\n",
    "            the number of units in each hidden layer,\n",
    "            n_layers is an integer representing the number of hidden layers in the network,\n",
    "            hidden_activation is a torch.nn.Module representing the activation function for the hidden layers,\n",
    "            output_activation is a torch.nn.Module representing the activation function for the output layer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sampling the hyperparameters from the search space\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    n_units = [trial.suggest_int(f\"n_units_{i}\", 16, 256) for i in range(n_layers)] \n",
    "    hidden_activation_name = trial.suggest_categorical(\n",
    "        \"hidden_activation\", [\"ReLU\", \"LeakyReLU\", \"ELU\", \"Tanh\", \"Sigmoid\"]\n",
    "    )\n",
    "    output_activation_name = trial.suggest_categorical(\n",
    "        \"output_activation\", [\"Linear\", \"ReLU\"]\n",
    "    ) \n",
    "    loss_name = trial.suggest_categorical(\n",
    "        \"loss\", [\"MSE\", \"MAE\", \"Huber\", \"LogCosh\"] \n",
    "    )\n",
    "    optimizer_name = trial.suggest_categorical(\n",
    "        \"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\", \"Adagrad\"] \n",
    "    )\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2) \n",
    "    batch_size = trial.suggest_int(\"batch_size\", 32, 256) #\n",
    "    n_epochs = trial.suggest_int(\"n_epochs\", 50, 100) \n",
    "    scheduler_name = trial.suggest_categorical(\n",
    "        \"scheduler\",\n",
    "        [\"None\", \"CosineAnnealingLR\", \"ReduceLROnPlateau\", \"StepLR\", \"ExponentialLR\"],\n",
    "    )\n",
    "\n",
    "    # Creating the activation functions from their names\n",
    "    if hidden_activation_name == \"ReLU\":\n",
    "        hidden_activation = nn.ReLU()\n",
    "    elif hidden_activation_name == \"LeakyReLU\":\n",
    "        hidden_activation = nn.LeakyReLU() \n",
    "    elif hidden_activation_name == \"ELU\":\n",
    "        hidden_activation = nn.ELU() \n",
    "    elif hidden_activation_name == \"Tanh\":\n",
    "        hidden_activation = nn.Tanh()\n",
    "    else:\n",
    "        hidden_activation = nn.Sigmoid()\n",
    "\n",
    "    if output_activation_name == \"ReLU\":\n",
    "        output_activation = nn.ReLU()\n",
    "    else:\n",
    "        output_activation = nn.Identity()\n",
    "\n",
    "    # Creating the loss function from its name\n",
    "    if loss_name == \"MSE\":\n",
    "        loss_fn = nn.MSELoss()\n",
    "    elif loss_name == \"MAE\":\n",
    "        loss_fn = nn.L1Loss()\n",
    "    elif loss_name == \"Huber\":\n",
    "        loss_fn = nn.SmoothL1Loss() \n",
    "    else:\n",
    "        # Creating the log-cosh loss function\n",
    "        def log_cosh_loss(y_pred, y_true):\n",
    "            \"\"\"Computes the log-cosh loss between the predicted and true values.\n",
    "\n",
    "            Args:\n",
    "                y_pred (torch.Tensor): The predicted values tensor of shape (batch_size, 1).\n",
    "                y_true (torch.Tensor): The true values tensor of shape (batch_size, 1).\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: The log-cosh loss tensor of shape ().\n",
    "            \"\"\"\n",
    "            return torch.mean(torch.log(torch.cosh(y_pred - y_true)))\n",
    "            \n",
    "        loss_fn = log_cosh_loss\n",
    "\n",
    "    # Creating the network with the sampled hyperparameters\n",
    "    net = Net(\n",
    "        n_layers, n_units, hidden_activation, output_activation\n",
    "    ).to(device)\n",
    "\n",
    "    # Creating the optimizer from its name\n",
    "    if optimizer_name == \"SGD\":\n",
    "        optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr=lr)\n",
    "    else:\n",
    "        # Added creating the Adagrad optimizer\n",
    "        optimizer = optim.Adagrad(net.parameters(), lr=lr)\n",
    "\n",
    "    # Creating the learning rate scheduler from its name\n",
    "    if scheduler_name == \"StepLR\":\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif scheduler_name == \"ExponentialLR\":\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    elif scheduler_name == \"CosineAnnealingLR\":\n",
    "        # Added creating the CosineAnnealingLR scheduler\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "    elif scheduler_name == \"ReduceLROnPlateau\":\n",
    "        # Added creating the ReduceLROnPlateau scheduler\n",
    "        # Creating the ReduceLROnPlateau scheduler with a threshold value of 0.01\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.1, patience=10, threshold=0.01\n",
    "        )\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    # Returning all variables needed for saving and loading # CHANGED: Added a return statement to return all variables\n",
    "    return net, loss_fn, optimizer, batch_size, n_epochs, scheduler, loss_name, optimizer_name, scheduler_name, n_units, n_layers, hidden_activation, output_activation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## The training and evaluation loop\n",
    "\n",
    " We first define a couple of functions used in the training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that computes loss and metrics for a given batch\n",
    "def compute_loss_and_metrics(y_pred, y_true, loss_fn):\n",
    "    \"\"\"Computes loss and metrics for a given batch.\n",
    "\n",
    "    Args:\n",
    "        y_pred (torch.Tensor): The predicted pressure tensor of shape (batch_size, 1).\n",
    "        y_true (torch.Tensor): The true pressure tensor of shape (batch_size,).\n",
    "        loss_fn (torch.nn.Module or function): The loss function to use.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of (loss, l1_norm), where loss is a scalar tensor,\n",
    "            l1_norm is L1 norm for relative error of pressure,\n",
    "            each being a scalar tensor.\n",
    "            linf_norm is Linf norm for relative error of pressure.\n",
    "    \"\"\"\n",
    "    # Reshaping the target tensor to match the input tensor\n",
    "    y_true = y_true.view(-1, 1)\n",
    "\n",
    "    # Computing the loss using the loss function\n",
    "    loss = loss_fn(y_pred, y_true)\n",
    "\n",
    "    # Computing the relative error of pressure\n",
    "    rel_error = torch.abs((y_pred - y_true) / y_true)\n",
    "\n",
    "    # Computing the L1 norm for the relative error of pressure\n",
    "    l1_norm = torch.mean(rel_error) \n",
    "    # Computing the Linf norm for the relative error of pressure\n",
    "    linf_norm = torch.max(rel_error) \n",
    "\n",
    "    # Returning the loss and metrics\n",
    "    return loss, l1_norm, linf_norm\n",
    "\n",
    "\n",
    "# Defining a function that updates the learning rate scheduler with validation loss if applicable\n",
    "def update_scheduler(scheduler, test_loss):\n",
    "    \"\"\"Updates the learning rate scheduler with validation loss if applicable.\n",
    "\n",
    "    Args:\n",
    "        scheduler (torch.optim.lr_scheduler._LRScheduler or None): The learning rate scheduler to use.\n",
    "        test_loss (float): The validation loss to use.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Checking if scheduler is not None\n",
    "    if scheduler is not None:\n",
    "        # Checking if scheduler is ReduceLROnPlateau\n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            # Updating the scheduler with test_loss\n",
    "            scheduler.step(test_loss)\n",
    "        else:\n",
    "            # Updating the scheduler without test_loss\n",
    "            scheduler.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the actual training and evaluation loop,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to train and evaluate a network\n",
    "def train_and_eval(net, loss_fn, optimizer, batch_size, n_epochs, scheduler, trial=None):\n",
    "    \"\"\"Trains and evaluates a network.\n",
    "\n",
    "    Args:\n",
    "        net (torch.nn.Module): The network to train and evaluate.\n",
    "        loss_fn (torch.nn.Module or function): The loss function.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer.\n",
    "        batch_size (int): The batch size.\n",
    "        n_epochs (int): The number of epochs.\n",
    "        scheduler (torch.optim.lr_scheduler._LRScheduler or None): The learning rate scheduler.\n",
    "    Returns:\n",
    "        tuple: A tuple of (train_losses, test_losses, train_metrics, test_metrics), where\n",
    "            train_losses is a list of training losses for each epoch,\n",
    "            test_losses is a list of validation losses for each epoch,\n",
    "            train_metrics is a list of dictionaries containing training metrics for each epoch,\n",
    "            test_metrics is a list of dictionaries containing validation metrics for each epoch.\n",
    "    \"\"\"\n",
    "    # Creating data loaders for train and test sets\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(x_test, y_test), batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Initializing lists to store the losses and metrics for each epoch\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_metrics = []\n",
    "    test_metrics = []\n",
    "\n",
    "    # Creating a SummaryWriter object to log data for tensorboard\n",
    "    writer = tbx.SummaryWriter()\n",
    "\n",
    "    # Looping over the epochs\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Setting the network to training mode\n",
    "        net.train()\n",
    "\n",
    "        # Initializing variables to store the total loss and metrics for the train set\n",
    "        train_loss = 0.0\n",
    "        train_l1_norm = 0.0\n",
    "        train_linf_norm = 0.0\n",
    "\n",
    "        # Looping over the batches in the train set\n",
    "        for x_batch, y_batch in train_loader:\n",
    "\n",
    "            # Moving the batch tensors to the device\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            # Zeroing the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Performing a forward pass and computing the loss and metrics\n",
    "            y_pred = net(x_batch)\n",
    "            loss, l1_norm, linf_norm = compute_loss_and_metrics(\n",
    "                y_pred, y_batch, loss_fn\n",
    "            )\n",
    "\n",
    "\n",
    "            # Performing a backward pass and updating the weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Updating the total loss and metrics for the train set\n",
    "            train_loss += loss.item() * x_batch.size(0)\n",
    "            train_l1_norm += l1_norm.item() * x_batch.size(0)\n",
    "            train_linf_norm += linf_norm.item() * x_batch.size(0)\n",
    "\n",
    "        # Computing the average loss and metrics for the train set\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_l1_norm /= len(train_loader.dataset)\n",
    "        train_linf_norm /= len(train_loader.dataset)\n",
    "\n",
    "        # Appending the average loss and metrics for the train set to the lists\n",
    "        train_losses.append(train_loss)\n",
    "        train_metrics.append(\n",
    "            {\n",
    "                \"l1_norm\": train_l1_norm,\n",
    "                \"linf_norm\": train_linf_norm,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Logging the average loss and metrics for the train set to tensorboard\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"L1 norm/train\", train_l1_norm, epoch)\n",
    "        writer.add_scalar(\"Linf norm/train\", train_linf_norm, epoch)\n",
    "\n",
    "        # Setting the network to evaluation mode\n",
    "        net.eval()\n",
    "\n",
    "        # Initializing variables to store the total loss and metrics for the test set\n",
    "        test_loss = 0.0\n",
    "        test_l1_norm = 0.0\n",
    "        test_linf_norm = 0.0\n",
    "\n",
    "        # Looping over the batches in the test set\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in test_loader:\n",
    "\n",
    "                # Moving the batch tensors to the device\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                # Performing a forward pass and computing the loss and metrics\n",
    "                y_pred = net(x_batch)\n",
    "                loss, l1_norm, linf_norm = compute_loss_and_metrics(\n",
    "                    y_pred, y_batch, loss_fn\n",
    "                )\n",
    "\n",
    "\n",
    "                # Updating the total loss and metrics for the test set\n",
    "                test_loss += loss.item() * x_batch.size(0)\n",
    "                test_l1_norm += l1_norm.item() * x_batch.size(0)\n",
    "                test_linf_norm += linf_norm.item() * x_batch.size(0)\n",
    "\n",
    "        # Computing the average loss and metrics for the test set\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_l1_norm /= len(test_loader.dataset)\n",
    "        test_linf_norm /= len(test_loader.dataset)\n",
    "\n",
    "        # Appending the average loss and metrics for the test set to the lists\n",
    "        test_losses.append(test_loss)\n",
    "        test_metrics.append(\n",
    "            {\n",
    "                \"l1_norm\": test_l1_norm,\n",
    "                \"linf_norm\": test_linf_norm,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Logging the average loss and metrics for the test set to tensorboard\n",
    "        writer.add_scalar(\"Loss/test\", test_loss, epoch)\n",
    "        writer.add_scalar(\"L1 norm/test\", test_l1_norm, epoch)\n",
    "        writer.add_scalar(\"Linf norm/test\", test_linf_norm, epoch)\n",
    "\n",
    "        # Printing the average loss and metrics for both sets for this epoch\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, \"\n",
    "            f\"Train L1 Norm: {train_l1_norm:.4f}, Test L1 Norm: {test_l1_norm:.4f}, \"\n",
    "            f\"Train Linf Norm: {train_linf_norm:.4f}, Test Linf Norm: {test_linf_norm:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Updating the learning rate scheduler with validation loss if applicable\n",
    "        update_scheduler(scheduler, test_loss)\n",
    "\n",
    "        # Reporting the intermediate metric value to Optuna if trial is not None\n",
    "        if trial is not None:\n",
    "            trial.report(test_metrics[-1][\"l1_norm\"], epoch)\n",
    "\n",
    "            # Checking if the trial should be pruned based on the intermediate value if trial is not None\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "    # Closing the SummaryWriter object\n",
    "    writer.close()\n",
    "\n",
    "    # Returning the losses and metrics lists\n",
    "    return train_losses, test_losses, train_metrics, test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The objective function and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining an objective function for Optuna to minimize\n",
    "def objective(trial):\n",
    "    \"\"\"Defines an objective function for Optuna to minimize.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): The trial object that contains the hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        float: The validation L1 norm to minimize.\n",
    "    \"\"\"\n",
    "    # Creating a trial network and optimizer using the create_model function\n",
    "    net, \\\n",
    "    loss_fn, \\\n",
    "    optimizer, \\\n",
    "    batch_size, \\\n",
    "    n_epochs, \\\n",
    "    scheduler, \\\n",
    "    loss_name, \\\n",
    "    optimizer_name, \\\n",
    "    scheduler_name, \\\n",
    "    n_units, \\\n",
    "    n_layers, \\\n",
    "    hidden_activation, \\\n",
    "    output_activation = create_model(trial)\n",
    "\n",
    "    # Training and evaluating the network using the train_and_eval function\n",
    "    _, _, _, test_metrics = train_and_eval(\n",
    "        net, loss_fn, optimizer, batch_size, n_epochs, scheduler, trial\n",
    "    )\n",
    "\n",
    "    # Returning the last validation L1 norm as the objective value to minimize\n",
    "    return test_metrics[-1][\"l1_norm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-07 07:50:24,430]\u001b[0m A new study created in memory with name: no-name-9f015af9-4ddb-4782-a6be-9401752b3bd8\u001b[0m\n",
      "/tmp/ipykernel_26905/2547659134.py:43: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2) # CHANGED: Used a log-uniform distribution with a range of 1e-4 to 1e-2 for the learning rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.8061, Test Loss: 1.7699, Train L1 Norm: 16.5193, Test L1 Norm: 12.4400, Train Linf Norm: 825.3101, Test Linf Norm: 546.3081\n",
      "Epoch 2: Train Loss: 1.7824, Test Loss: 1.7654, Train L1 Norm: 17.2991, Test L1 Norm: 11.9039, Train Linf Norm: 870.0799, Test Linf Norm: 523.0028\n",
      "Epoch 3: Train Loss: 1.7810, Test Loss: 1.7668, Train L1 Norm: 17.3108, Test L1 Norm: 11.4184, Train Linf Norm: 888.4560, Test Linf Norm: 501.2407\n",
      "Epoch 4: Train Loss: 1.7805, Test Loss: 1.7704, Train L1 Norm: 17.1827, Test L1 Norm: 11.2749, Train Linf Norm: 884.6797, Test Linf Norm: 493.7141\n",
      "Epoch 5: Train Loss: 1.7800, Test Loss: 1.7660, Train L1 Norm: 17.1862, Test L1 Norm: 11.4963, Train Linf Norm: 891.6751, Test Linf Norm: 504.1000\n",
      "Epoch 6: Train Loss: 1.7787, Test Loss: 1.7729, Train L1 Norm: 17.2076, Test L1 Norm: 10.8978, Train Linf Norm: 876.5932, Test Linf Norm: 476.7930\n",
      "Epoch 7: Train Loss: 1.7790, Test Loss: 1.7653, Train L1 Norm: 17.3719, Test L1 Norm: 12.3621, Train Linf Norm: 895.7439, Test Linf Norm: 542.9986\n",
      "Epoch 8: Train Loss: 1.7787, Test Loss: 1.7643, Train L1 Norm: 17.3324, Test L1 Norm: 12.1102, Train Linf Norm: 886.8958, Test Linf Norm: 530.9763\n",
      "Epoch 9: Train Loss: 1.7785, Test Loss: 1.7646, Train L1 Norm: 17.3152, Test L1 Norm: 11.8539, Train Linf Norm: 882.9730, Test Linf Norm: 519.3144\n",
      "Epoch 10: Train Loss: 1.7783, Test Loss: 1.7655, Train L1 Norm: 17.2960, Test L1 Norm: 11.4658, Train Linf Norm: 889.0093, Test Linf Norm: 502.4769\n",
      "Epoch 11: Train Loss: 1.7781, Test Loss: 1.7638, Train L1 Norm: 17.3006, Test L1 Norm: 12.0889, Train Linf Norm: 896.1065, Test Linf Norm: 530.6662\n",
      "Epoch 12: Train Loss: 1.7777, Test Loss: 1.7639, Train L1 Norm: 17.2619, Test L1 Norm: 11.8692, Train Linf Norm: 872.9008, Test Linf Norm: 520.3589\n",
      "Epoch 13: Train Loss: 1.7775, Test Loss: 1.7640, Train L1 Norm: 17.2829, Test L1 Norm: 12.1394, Train Linf Norm: 883.9029, Test Linf Norm: 532.1467\n",
      "Epoch 14: Train Loss: 1.7776, Test Loss: 1.7639, Train L1 Norm: 17.3132, Test L1 Norm: 11.9441, Train Linf Norm: 891.0321, Test Linf Norm: 523.7211\n",
      "Epoch 15: Train Loss: 1.7772, Test Loss: 1.7642, Train L1 Norm: 17.1537, Test L1 Norm: 12.2402, Train Linf Norm: 885.6435, Test Linf Norm: 536.6971\n",
      "Epoch 16: Train Loss: 1.7772, Test Loss: 1.7648, Train L1 Norm: 17.2099, Test L1 Norm: 11.7063, Train Linf Norm: 882.3835, Test Linf Norm: 512.8011\n",
      "Epoch 17: Train Loss: 1.7770, Test Loss: 1.7653, Train L1 Norm: 17.2817, Test L1 Norm: 11.6235, Train Linf Norm: 902.8110, Test Linf Norm: 509.0493\n",
      "Epoch 18: Train Loss: 1.7773, Test Loss: 1.7640, Train L1 Norm: 17.1516, Test L1 Norm: 11.8069, Train Linf Norm: 880.0315, Test Linf Norm: 517.2669\n",
      "Epoch 19: Train Loss: 1.7771, Test Loss: 1.7643, Train L1 Norm: 17.2382, Test L1 Norm: 12.2056, Train Linf Norm: 884.6607, Test Linf Norm: 535.2828\n",
      "Epoch 20: Train Loss: 1.7771, Test Loss: 1.7649, Train L1 Norm: 17.3185, Test L1 Norm: 11.5380, Train Linf Norm: 890.1516, Test Linf Norm: 505.3005\n",
      "Epoch 21: Train Loss: 1.7769, Test Loss: 1.7640, Train L1 Norm: 17.2012, Test L1 Norm: 12.0266, Train Linf Norm: 890.3425, Test Linf Norm: 527.3094\n",
      "Epoch 22: Train Loss: 1.7769, Test Loss: 1.7638, Train L1 Norm: 17.2595, Test L1 Norm: 11.8122, Train Linf Norm: 886.4998, Test Linf Norm: 517.5646\n",
      "Epoch 23: Train Loss: 1.7768, Test Loss: 1.7640, Train L1 Norm: 17.2358, Test L1 Norm: 12.1593, Train Linf Norm: 877.0312, Test Linf Norm: 533.0691\n",
      "Epoch 24: Train Loss: 1.7768, Test Loss: 1.7669, Train L1 Norm: 17.2649, Test L1 Norm: 11.3664, Train Linf Norm: 880.9213, Test Linf Norm: 497.5595\n",
      "Epoch 25: Train Loss: 1.7768, Test Loss: 1.7640, Train L1 Norm: 17.2485, Test L1 Norm: 11.7328, Train Linf Norm: 889.6722, Test Linf Norm: 513.9232\n",
      "Epoch 26: Train Loss: 1.7767, Test Loss: 1.7647, Train L1 Norm: 17.2276, Test L1 Norm: 11.5933, Train Linf Norm: 883.2935, Test Linf Norm: 507.6291\n",
      "Epoch 27: Train Loss: 1.7768, Test Loss: 1.7640, Train L1 Norm: 17.2010, Test L1 Norm: 11.7206, Train Linf Norm: 884.7965, Test Linf Norm: 513.4099\n",
      "Epoch 28: Train Loss: 1.7765, Test Loss: 1.7640, Train L1 Norm: 17.1663, Test L1 Norm: 12.1902, Train Linf Norm: 877.3459, Test Linf Norm: 534.3972\n",
      "Epoch 29: Train Loss: 1.7767, Test Loss: 1.7636, Train L1 Norm: 17.2365, Test L1 Norm: 11.9704, Train Linf Norm: 879.1208, Test Linf Norm: 524.5354\n",
      "Epoch 30: Train Loss: 1.7766, Test Loss: 1.7638, Train L1 Norm: 17.2207, Test L1 Norm: 11.8190, Train Linf Norm: 878.6269, Test Linf Norm: 517.7872\n",
      "Epoch 31: Train Loss: 1.7766, Test Loss: 1.7637, Train L1 Norm: 17.2077, Test L1 Norm: 11.8694, Train Linf Norm: 884.6982, Test Linf Norm: 520.0173\n",
      "Epoch 32: Train Loss: 1.7765, Test Loss: 1.7636, Train L1 Norm: 17.1889, Test L1 Norm: 11.9197, Train Linf Norm: 865.2353, Test Linf Norm: 522.3105\n",
      "Epoch 33: Train Loss: 1.7766, Test Loss: 1.7636, Train L1 Norm: 17.2512, Test L1 Norm: 11.8959, Train Linf Norm: 892.1315, Test Linf Norm: 521.2506\n",
      "Epoch 34: Train Loss: 1.7765, Test Loss: 1.7637, Train L1 Norm: 17.1815, Test L1 Norm: 12.0041, Train Linf Norm: 883.6766, Test Linf Norm: 526.0733\n",
      "Epoch 35: Train Loss: 1.7764, Test Loss: 1.7638, Train L1 Norm: 17.1679, Test L1 Norm: 12.0886, Train Linf Norm: 892.4362, Test Linf Norm: 529.8246\n",
      "Epoch 36: Train Loss: 1.7765, Test Loss: 1.7636, Train L1 Norm: 17.2389, Test L1 Norm: 11.9725, Train Linf Norm: 876.7341, Test Linf Norm: 524.6247\n",
      "Epoch 37: Train Loss: 1.7765, Test Loss: 1.7637, Train L1 Norm: 17.2247, Test L1 Norm: 11.8288, Train Linf Norm: 879.5583, Test Linf Norm: 518.1983\n",
      "Epoch 38: Train Loss: 1.7765, Test Loss: 1.7638, Train L1 Norm: 17.2290, Test L1 Norm: 11.7878, Train Linf Norm: 880.0810, Test Linf Norm: 516.3776\n",
      "Epoch 39: Train Loss: 1.7765, Test Loss: 1.7636, Train L1 Norm: 17.1799, Test L1 Norm: 11.9039, Train Linf Norm: 880.5765, Test Linf Norm: 521.5695\n",
      "Epoch 40: Train Loss: 1.7765, Test Loss: 1.7637, Train L1 Norm: 17.2409, Test L1 Norm: 11.8159, Train Linf Norm: 890.4336, Test Linf Norm: 517.6347\n",
      "Epoch 41: Train Loss: 1.7765, Test Loss: 1.7639, Train L1 Norm: 17.2395, Test L1 Norm: 11.7397, Train Linf Norm: 887.1039, Test Linf Norm: 514.2287\n",
      "Epoch 42: Train Loss: 1.7765, Test Loss: 1.7637, Train L1 Norm: 17.1540, Test L1 Norm: 11.8660, Train Linf Norm: 882.0531, Test Linf Norm: 519.8806\n",
      "Epoch 43: Train Loss: 1.7764, Test Loss: 1.7638, Train L1 Norm: 17.2495, Test L1 Norm: 11.7860, Train Linf Norm: 886.3796, Test Linf Norm: 516.2974\n",
      "Epoch 44: Train Loss: 1.7764, Test Loss: 1.7636, Train L1 Norm: 17.1715, Test L1 Norm: 11.8934, Train Linf Norm: 875.7139, Test Linf Norm: 521.0948\n",
      "Epoch 45: Train Loss: 1.7764, Test Loss: 1.7638, Train L1 Norm: 17.2510, Test L1 Norm: 11.8002, Train Linf Norm: 887.0852, Test Linf Norm: 516.9226\n",
      "Epoch 46: Train Loss: 1.7764, Test Loss: 1.7637, Train L1 Norm: 17.1918, Test L1 Norm: 11.8422, Train Linf Norm: 884.9905, Test Linf Norm: 518.7989\n",
      "Epoch 47: Train Loss: 1.7764, Test Loss: 1.7637, Train L1 Norm: 17.2127, Test L1 Norm: 11.8477, Train Linf Norm: 845.5536, Test Linf Norm: 519.0458\n",
      "Epoch 48: Train Loss: 1.7764, Test Loss: 1.7637, Train L1 Norm: 17.1999, Test L1 Norm: 11.8683, Train Linf Norm: 891.9552, Test Linf Norm: 519.9667\n",
      "Epoch 49: Train Loss: 1.7764, Test Loss: 1.7637, Train L1 Norm: 17.2209, Test L1 Norm: 11.8567, Train Linf Norm: 887.6405, Test Linf Norm: 519.4491\n",
      "Epoch 50: Train Loss: 1.7764, Test Loss: 1.7637, Train L1 Norm: 17.2300, Test L1 Norm: 11.8418, Train Linf Norm: 878.7641, Test Linf Norm: 518.7828\n",
      "Epoch 51: Train Loss: 1.7764, Test Loss: 1.7637, Train L1 Norm: 17.1991, Test L1 Norm: 11.8591, Train Linf Norm: 883.7787, Test Linf Norm: 519.5569\n",
      "Epoch 52: Train Loss: 1.7764, Test Loss: 1.7637, Train L1 Norm: 17.2136, Test L1 Norm: 11.8616, Train Linf Norm: 879.5896, Test Linf Norm: 519.6643\n",
      "Epoch 53: Train Loss: 1.7764, Test Loss: 1.7637, Train L1 Norm: 17.2113, Test L1 Norm: 11.8681, Train Linf Norm: 885.0683, Test Linf Norm: 519.9568\n",
      "Epoch 54: Train Loss: 1.7764, Test Loss: 1.7637, Train L1 Norm: 17.2513, Test L1 Norm: 11.8426, Train Linf Norm: 892.7870, Test Linf Norm: 518.8159\n",
      "Epoch 55: Train Loss: 1.7764, Test Loss: 1.7637, Train L1 Norm: 17.2013, Test L1 Norm: 11.8532, Train Linf Norm: 881.0999, Test Linf Norm: 519.2913\n",
      "Epoch 56: Train Loss: 1.7764, Test Loss: 1.7637, Train L1 Norm: 17.2068, Test L1 Norm: 11.8580, Train Linf Norm: 885.2113, Test Linf Norm: 519.5058\n",
      "Epoch 57: Train Loss: 1.7764, Test Loss: 1.7637, Train L1 Norm: 17.2116, Test L1 Norm: 11.8623, Train Linf Norm: 888.5608, Test Linf Norm: 519.6951\n",
      "Epoch 58: Train Loss: 1.7764, Test Loss: 1.7637, Train L1 Norm: 17.2639, Test L1 Norm: 11.8399, Train Linf Norm: 892.2765, Test Linf Norm: 518.6977\n",
      "Epoch 59: Train Loss: 1.7764, Test Loss: 1.7637, Train L1 Norm: 17.1870, Test L1 Norm: 11.8550, Train Linf Norm: 869.4911, Test Linf Norm: 519.3717\n",
      "Epoch 60: Train Loss: 1.7764, Test Loss: 1.7637, Train L1 Norm: 17.2041, Test L1 Norm: 11.8599, Train Linf Norm: 864.4184, Test Linf Norm: 519.5877\n",
      "Epoch 61: Train Loss: 1.7764, Test Loss: 1.7637, Train L1 Norm: 17.2101, Test L1 Norm: 11.8640, Train Linf Norm: 868.4580, Test Linf Norm: 519.7706\n",
      "Epoch 62: Train Loss: 1.7764, Test Loss: 1.7637, Train L1 Norm: 17.2423, Test L1 Norm: 11.8545, Train Linf Norm: 877.7046, Test Linf Norm: 519.3488\n",
      "Epoch 63: Train Loss: 1.7764, Test Loss: 1.7637, Train L1 Norm: 17.2161, Test L1 Norm: 11.8520, Train Linf Norm: 875.2053, Test Linf Norm: 519.2356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-07 07:54:41,940]\u001b[0m Trial 0 finished with value: 11.850316779553891 and parameters: {'n_layers': 1, 'n_units_0': 156, 'hidden_activation': 'ELU', 'output_activation': 'Linear', 'loss': 'LogCosh', 'optimizer': 'RMSprop', 'lr': 0.0003089908850240646, 'batch_size': 68, 'n_epochs': 64, 'scheduler': 'ExponentialLR'}. Best is trial 0 with value: 11.850316779553891.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: Train Loss: 1.7764, Test Loss: 1.7637, Train L1 Norm: 17.2160, Test L1 Norm: 11.8503, Train Linf Norm: 880.4566, Test Linf Norm: 519.1609\n",
      "Epoch 1: Train Loss: 2.0393, Test Loss: 1.9363, Train L1 Norm: 12.2307, Test L1 Norm: 9.6840, Train Linf Norm: 1052.4683, Test Linf Norm: 732.8882\n",
      "Epoch 2: Train Loss: 1.9435, Test Loss: 1.9116, Train L1 Norm: 13.9685, Test L1 Norm: 9.9680, Train Linf Norm: 1189.1559, Test Linf Norm: 749.0114\n",
      "Epoch 3: Train Loss: 1.9232, Test Loss: 1.8933, Train L1 Norm: 14.2966, Test L1 Norm: 10.4427, Train Linf Norm: 1217.5653, Test Linf Norm: 782.7494\n",
      "Epoch 4: Train Loss: 1.9067, Test Loss: 1.8783, Train L1 Norm: 14.3671, Test L1 Norm: 10.5289, Train Linf Norm: 1216.1462, Test Linf Norm: 787.3127\n",
      "Epoch 5: Train Loss: 1.8923, Test Loss: 1.8660, Train L1 Norm: 14.4795, Test L1 Norm: 10.3329, Train Linf Norm: 1234.9336, Test Linf Norm: 770.0749\n",
      "Epoch 6: Train Loss: 1.8798, Test Loss: 1.8539, Train L1 Norm: 14.7134, Test L1 Norm: 10.6431, Train Linf Norm: 1266.2723, Test Linf Norm: 792.4371\n",
      "Epoch 7: Train Loss: 1.8685, Test Loss: 1.8472, Train L1 Norm: 14.7329, Test L1 Norm: 10.1265, Train Linf Norm: 1228.6747, Test Linf Norm: 750.4380\n",
      "Epoch 8: Train Loss: 1.8588, Test Loss: 1.8352, Train L1 Norm: 14.9268, Test L1 Norm: 10.6164, Train Linf Norm: 1287.9264, Test Linf Norm: 786.7555\n",
      "Epoch 9: Train Loss: 1.8500, Test Loss: 1.8270, Train L1 Norm: 14.9999, Test L1 Norm: 10.8705, Train Linf Norm: 1296.3798, Test Linf Norm: 804.7268\n",
      "Epoch 10: Train Loss: 1.8420, Test Loss: 1.8214, Train L1 Norm: 15.1690, Test L1 Norm: 10.5462, Train Linf Norm: 1313.4343, Test Linf Norm: 778.1079\n",
      "Epoch 11: Train Loss: 1.8376, Test Loss: 1.8196, Train L1 Norm: 15.1312, Test L1 Norm: 10.7836, Train Linf Norm: 1303.8211, Test Linf Norm: 796.3804\n",
      "Epoch 12: Train Loss: 1.8368, Test Loss: 1.8190, Train L1 Norm: 15.2056, Test L1 Norm: 10.7727, Train Linf Norm: 1280.7036, Test Linf Norm: 795.3754\n",
      "Epoch 13: Train Loss: 1.8362, Test Loss: 1.8184, Train L1 Norm: 15.1929, Test L1 Norm: 10.7855, Train Linf Norm: 1305.9846, Test Linf Norm: 796.2085\n",
      "Epoch 14: Train Loss: 1.8355, Test Loss: 1.8177, Train L1 Norm: 15.2219, Test L1 Norm: 10.8108, Train Linf Norm: 1323.4720, Test Linf Norm: 798.0089\n",
      "Epoch 15: Train Loss: 1.8348, Test Loss: 1.8170, Train L1 Norm: 15.2120, Test L1 Norm: 10.9203, Train Linf Norm: 1313.6269, Test Linf Norm: 806.3454\n",
      "Epoch 16: Train Loss: 1.8342, Test Loss: 1.8165, Train L1 Norm: 15.2631, Test L1 Norm: 10.8136, Train Linf Norm: 1301.2903, Test Linf Norm: 797.9104\n",
      "Epoch 17: Train Loss: 1.8336, Test Loss: 1.8158, Train L1 Norm: 15.2470, Test L1 Norm: 10.8784, Train Linf Norm: 1289.9472, Test Linf Norm: 802.7805\n",
      "Epoch 18: Train Loss: 1.8329, Test Loss: 1.8152, Train L1 Norm: 15.2700, Test L1 Norm: 10.8638, Train Linf Norm: 1282.0519, Test Linf Norm: 801.4948\n",
      "Epoch 19: Train Loss: 1.8322, Test Loss: 1.8149, Train L1 Norm: 15.2552, Test L1 Norm: 10.7567, Train Linf Norm: 1295.1344, Test Linf Norm: 793.0334\n",
      "Epoch 20: Train Loss: 1.8317, Test Loss: 1.8141, Train L1 Norm: 15.2636, Test L1 Norm: 10.8515, Train Linf Norm: 1267.1346, Test Linf Norm: 800.2311\n",
      "Epoch 21: Train Loss: 1.8313, Test Loss: 1.8140, Train L1 Norm: 15.2879, Test L1 Norm: 10.8517, Train Linf Norm: 1296.6403, Test Linf Norm: 800.2327\n",
      "Epoch 22: Train Loss: 1.8312, Test Loss: 1.8140, Train L1 Norm: 15.2851, Test L1 Norm: 10.8528, Train Linf Norm: 1297.3022, Test Linf Norm: 800.3016\n",
      "Epoch 23: Train Loss: 1.8311, Test Loss: 1.8139, Train L1 Norm: 15.2775, Test L1 Norm: 10.8584, Train Linf Norm: 1322.3836, Test Linf Norm: 800.7219\n",
      "Epoch 24: Train Loss: 1.8311, Test Loss: 1.8138, Train L1 Norm: 15.2953, Test L1 Norm: 10.8581, Train Linf Norm: 1306.3753, Test Linf Norm: 800.6839\n",
      "Epoch 25: Train Loss: 1.8310, Test Loss: 1.8138, Train L1 Norm: 15.2948, Test L1 Norm: 10.8570, Train Linf Norm: 1320.0718, Test Linf Norm: 800.5843\n",
      "Epoch 26: Train Loss: 1.8310, Test Loss: 1.8137, Train L1 Norm: 15.2994, Test L1 Norm: 10.8540, Train Linf Norm: 1303.5604, Test Linf Norm: 800.3318\n",
      "Epoch 27: Train Loss: 1.8309, Test Loss: 1.8137, Train L1 Norm: 15.2965, Test L1 Norm: 10.8510, Train Linf Norm: 1307.0257, Test Linf Norm: 800.0880\n",
      "Epoch 28: Train Loss: 1.8308, Test Loss: 1.8136, Train L1 Norm: 15.3008, Test L1 Norm: 10.8499, Train Linf Norm: 1296.2917, Test Linf Norm: 799.9839\n",
      "Epoch 29: Train Loss: 1.8308, Test Loss: 1.8136, Train L1 Norm: 15.2992, Test L1 Norm: 10.8480, Train Linf Norm: 1300.2952, Test Linf Norm: 799.8224\n",
      "Epoch 30: Train Loss: 1.8307, Test Loss: 1.8135, Train L1 Norm: 15.2896, Test L1 Norm: 10.8516, Train Linf Norm: 1299.9359, Test Linf Norm: 800.0877\n",
      "Epoch 31: Train Loss: 1.8307, Test Loss: 1.8135, Train L1 Norm: 15.2918, Test L1 Norm: 10.8518, Train Linf Norm: 1293.5307, Test Linf Norm: 800.1015\n",
      "Epoch 32: Train Loss: 1.8307, Test Loss: 1.8135, Train L1 Norm: 15.2909, Test L1 Norm: 10.8520, Train Linf Norm: 1302.2964, Test Linf Norm: 800.1168\n",
      "Epoch 33: Train Loss: 1.8307, Test Loss: 1.8135, Train L1 Norm: 15.2905, Test L1 Norm: 10.8523, Train Linf Norm: 1301.0437, Test Linf Norm: 800.1365\n",
      "Epoch 34: Train Loss: 1.8307, Test Loss: 1.8135, Train L1 Norm: 15.2910, Test L1 Norm: 10.8525, Train Linf Norm: 1279.1130, Test Linf Norm: 800.1484\n",
      "Epoch 35: Train Loss: 1.8307, Test Loss: 1.8135, Train L1 Norm: 15.2927, Test L1 Norm: 10.8527, Train Linf Norm: 1313.0946, Test Linf Norm: 800.1653\n",
      "Epoch 36: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2903, Test L1 Norm: 10.8530, Train Linf Norm: 1281.4869, Test Linf Norm: 800.1877\n",
      "Epoch 37: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2926, Test L1 Norm: 10.8532, Train Linf Norm: 1304.5916, Test Linf Norm: 800.1998\n",
      "Epoch 38: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2927, Test L1 Norm: 10.8534, Train Linf Norm: 1322.7112, Test Linf Norm: 800.2143\n",
      "Epoch 39: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2940, Test L1 Norm: 10.8535, Train Linf Norm: 1305.7213, Test Linf Norm: 800.2225\n",
      "Epoch 40: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2942, Test L1 Norm: 10.8536, Train Linf Norm: 1299.8543, Test Linf Norm: 800.2279\n",
      "Epoch 41: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2933, Test L1 Norm: 10.8536, Train Linf Norm: 1298.2874, Test Linf Norm: 800.2298\n",
      "Epoch 42: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2937, Test L1 Norm: 10.8536, Train Linf Norm: 1294.9821, Test Linf Norm: 800.2305\n",
      "Epoch 43: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8536, Train Linf Norm: 1327.0745, Test Linf Norm: 800.2300\n",
      "Epoch 44: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2937, Test L1 Norm: 10.8536, Train Linf Norm: 1303.4072, Test Linf Norm: 800.2289\n",
      "Epoch 45: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2935, Test L1 Norm: 10.8537, Train Linf Norm: 1313.6783, Test Linf Norm: 800.2306\n",
      "Epoch 46: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2937, Test L1 Norm: 10.8537, Train Linf Norm: 1286.9005, Test Linf Norm: 800.2305\n",
      "Epoch 47: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2935, Test L1 Norm: 10.8537, Train Linf Norm: 1321.8605, Test Linf Norm: 800.2323\n",
      "Epoch 48: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2937, Test L1 Norm: 10.8537, Train Linf Norm: 1315.7060, Test Linf Norm: 800.2325\n",
      "Epoch 49: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2937, Test L1 Norm: 10.8537, Train Linf Norm: 1321.2503, Test Linf Norm: 800.2317\n",
      "Epoch 50: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1311.9045, Test Linf Norm: 800.2325\n",
      "Epoch 51: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1326.3482, Test Linf Norm: 800.2325\n",
      "Epoch 52: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1302.8690, Test Linf Norm: 800.2318\n",
      "Epoch 53: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1296.3689, Test Linf Norm: 800.2324\n",
      "Epoch 54: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1315.4782, Test Linf Norm: 800.2317\n",
      "Epoch 55: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1320.5204, Test Linf Norm: 800.2317\n",
      "Epoch 56: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1312.7227, Test Linf Norm: 800.2312\n",
      "Epoch 57: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1294.6123, Test Linf Norm: 800.2316\n",
      "Epoch 58: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1300.7191, Test Linf Norm: 800.2316\n",
      "Epoch 59: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1336.0448, Test Linf Norm: 800.2318\n",
      "Epoch 60: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1333.1039, Test Linf Norm: 800.2322\n",
      "Epoch 61: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1287.1997, Test Linf Norm: 800.2322\n",
      "Epoch 62: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1309.2298, Test Linf Norm: 800.2322\n",
      "Epoch 63: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1308.4628, Test Linf Norm: 800.2322\n",
      "Epoch 64: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1337.5886, Test Linf Norm: 800.2321\n",
      "Epoch 65: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1308.8842, Test Linf Norm: 800.2321\n",
      "Epoch 66: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1306.5665, Test Linf Norm: 800.2321\n",
      "Epoch 67: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1319.6625, Test Linf Norm: 800.2321\n",
      "Epoch 68: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1301.0388, Test Linf Norm: 800.2321\n",
      "Epoch 69: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1301.6224, Test Linf Norm: 800.2321\n",
      "Epoch 70: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1292.6612, Test Linf Norm: 800.2321\n",
      "Epoch 71: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1282.9194, Test Linf Norm: 800.2321\n",
      "Epoch 72: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1315.6692, Test Linf Norm: 800.2321\n",
      "Epoch 73: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1319.6421, Test Linf Norm: 800.2321\n",
      "Epoch 74: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1278.5177, Test Linf Norm: 800.2321\n",
      "Epoch 75: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1305.0163, Test Linf Norm: 800.2321\n",
      "Epoch 76: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1306.0049, Test Linf Norm: 800.2321\n",
      "Epoch 77: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1301.5673, Test Linf Norm: 800.2321\n",
      "Epoch 78: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1290.6976, Test Linf Norm: 800.2321\n",
      "Epoch 79: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1287.8679, Test Linf Norm: 800.2321\n",
      "Epoch 80: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1310.6008, Test Linf Norm: 800.2321\n",
      "Epoch 81: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1302.7017, Test Linf Norm: 800.2321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-07 07:59:31,260]\u001b[0m Trial 1 finished with value: 10.853677866363526 and parameters: {'n_layers': 1, 'n_units_0': 157, 'hidden_activation': 'LeakyReLU', 'output_activation': 'ReLU', 'loss': 'LogCosh', 'optimizer': 'SGD', 'lr': 0.0001670161972243837, 'batch_size': 122, 'n_epochs': 82, 'scheduler': 'StepLR'}. Best is trial 1 with value: 10.853677866363526.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82: Train Loss: 1.8306, Test Loss: 1.8135, Train L1 Norm: 15.2936, Test L1 Norm: 10.8537, Train Linf Norm: 1300.5195, Test Linf Norm: 800.2321\n",
      "Epoch 1: Train Loss: 2.9543, Test Loss: 2.9353, Train L1 Norm: 1.0001, Test L1 Norm: 0.9999, Train Linf Norm: 1.0562, Test Linf Norm: 1.0055\n",
      "Epoch 2: Train Loss: 2.9543, Test Loss: 2.9353, Train L1 Norm: 1.0001, Test L1 Norm: 0.9999, Train Linf Norm: 1.0756, Test Linf Norm: 1.0120\n",
      "Epoch 3: Train Loss: 2.9542, Test Loss: 2.9353, Train L1 Norm: 1.0002, Test L1 Norm: 0.9999, Train Linf Norm: 1.0889, Test Linf Norm: 1.0191\n",
      "Epoch 4: Train Loss: 2.9542, Test Loss: 2.9353, Train L1 Norm: 1.0002, Test L1 Norm: 0.9998, Train Linf Norm: 1.1034, Test Linf Norm: 1.0271\n",
      "Epoch 5: Train Loss: 2.9542, Test Loss: 2.9353, Train L1 Norm: 1.0002, Test L1 Norm: 0.9998, Train Linf Norm: 1.1224, Test Linf Norm: 1.0362\n",
      "Epoch 6: Train Loss: 2.9542, Test Loss: 2.9352, Train L1 Norm: 1.0002, Test L1 Norm: 0.9998, Train Linf Norm: 1.1395, Test Linf Norm: 1.0467\n",
      "Epoch 7: Train Loss: 2.9542, Test Loss: 2.9352, Train L1 Norm: 1.0003, Test L1 Norm: 0.9997, Train Linf Norm: 1.1693, Test Linf Norm: 1.0590\n",
      "Epoch 8: Train Loss: 2.9541, Test Loss: 2.9351, Train L1 Norm: 1.0003, Test L1 Norm: 0.9996, Train Linf Norm: 1.1992, Test Linf Norm: 1.0733\n",
      "Epoch 9: Train Loss: 2.9540, Test Loss: 2.9350, Train L1 Norm: 1.0004, Test L1 Norm: 0.9995, Train Linf Norm: 1.2508, Test Linf Norm: 1.0905\n",
      "Epoch 10: Train Loss: 2.9539, Test Loss: 2.9349, Train L1 Norm: 1.0005, Test L1 Norm: 0.9994, Train Linf Norm: 1.3154, Test Linf Norm: 1.1113\n",
      "Epoch 11: Train Loss: 2.9538, Test Loss: 2.9347, Train L1 Norm: 1.0008, Test L1 Norm: 0.9992, Train Linf Norm: 1.4207, Test Linf Norm: 1.1369\n",
      "Epoch 12: Train Loss: 2.9536, Test Loss: 2.9344, Train L1 Norm: 1.0011, Test L1 Norm: 0.9987, Train Linf Norm: 1.5758, Test Linf Norm: 1.1855\n",
      "Epoch 13: Train Loss: 2.9534, Test Loss: 2.9344, Train L1 Norm: 1.0013, Test L1 Norm: 0.9986, Train Linf Norm: 1.6896, Test Linf Norm: 1.1910\n",
      "Epoch 14: Train Loss: 2.9533, Test Loss: 2.9344, Train L1 Norm: 1.0013, Test L1 Norm: 0.9986, Train Linf Norm: 1.7173, Test Linf Norm: 1.1966\n",
      "Epoch 15: Train Loss: 2.9533, Test Loss: 2.9343, Train L1 Norm: 1.0014, Test L1 Norm: 0.9985, Train Linf Norm: 1.7391, Test Linf Norm: 1.2023\n",
      "Epoch 16: Train Loss: 2.9533, Test Loss: 2.9343, Train L1 Norm: 1.0014, Test L1 Norm: 0.9984, Train Linf Norm: 1.7660, Test Linf Norm: 1.2082\n",
      "Epoch 17: Train Loss: 2.9532, Test Loss: 2.9342, Train L1 Norm: 1.0014, Test L1 Norm: 0.9984, Train Linf Norm: 1.7820, Test Linf Norm: 1.2154\n",
      "Epoch 18: Train Loss: 2.9532, Test Loss: 2.9342, Train L1 Norm: 1.0015, Test L1 Norm: 0.9984, Train Linf Norm: 1.8110, Test Linf Norm: 1.2522\n",
      "Epoch 19: Train Loss: 2.9531, Test Loss: 2.9341, Train L1 Norm: 1.0015, Test L1 Norm: 0.9984, Train Linf Norm: 1.8412, Test Linf Norm: 1.2917\n",
      "Epoch 20: Train Loss: 2.9531, Test Loss: 2.9340, Train L1 Norm: 1.0016, Test L1 Norm: 0.9985, Train Linf Norm: 1.8822, Test Linf Norm: 1.3329\n",
      "Epoch 21: Train Loss: 2.9530, Test Loss: 2.9340, Train L1 Norm: 1.0017, Test L1 Norm: 0.9986, Train Linf Norm: 1.9225, Test Linf Norm: 1.3753\n",
      "Epoch 22: Train Loss: 2.9529, Test Loss: 2.9339, Train L1 Norm: 1.0018, Test L1 Norm: 0.9987, Train Linf Norm: 1.9775, Test Linf Norm: 1.4193\n",
      "Epoch 23: Train Loss: 2.9529, Test Loss: 2.9338, Train L1 Norm: 1.0021, Test L1 Norm: 0.9988, Train Linf Norm: 2.0528, Test Linf Norm: 1.4646\n",
      "Epoch 24: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0022, Test L1 Norm: 0.9988, Train Linf Norm: 2.0995, Test Linf Norm: 1.4687\n",
      "Epoch 25: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0022, Test L1 Norm: 0.9988, Train Linf Norm: 2.1021, Test Linf Norm: 1.4729\n",
      "Epoch 26: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0022, Test L1 Norm: 0.9988, Train Linf Norm: 2.1100, Test Linf Norm: 1.4771\n",
      "Epoch 27: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0022, Test L1 Norm: 0.9988, Train Linf Norm: 2.1033, Test Linf Norm: 1.4813\n",
      "Epoch 28: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0023, Test L1 Norm: 0.9989, Train Linf Norm: 2.1216, Test Linf Norm: 1.4855\n",
      "Epoch 29: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0023, Test L1 Norm: 0.9989, Train Linf Norm: 2.0832, Test Linf Norm: 1.4897\n",
      "Epoch 30: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0023, Test L1 Norm: 0.9989, Train Linf Norm: 2.1327, Test Linf Norm: 1.4939\n",
      "Epoch 31: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0023, Test L1 Norm: 0.9989, Train Linf Norm: 2.1396, Test Linf Norm: 1.4988\n",
      "Epoch 32: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0023, Test L1 Norm: 0.9989, Train Linf Norm: 2.1410, Test Linf Norm: 1.5037\n",
      "Epoch 33: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.1535, Test Linf Norm: 1.5087\n",
      "Epoch 34: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.1556, Test Linf Norm: 1.5137\n",
      "Epoch 35: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.1197, Test Linf Norm: 1.5140\n",
      "Epoch 36: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.1680, Test Linf Norm: 1.5144\n",
      "Epoch 37: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.1682, Test Linf Norm: 1.5148\n",
      "Epoch 38: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.1589, Test Linf Norm: 1.5152\n",
      "Epoch 39: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.1699, Test Linf Norm: 1.5156\n",
      "Epoch 40: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.1554, Test Linf Norm: 1.5160\n",
      "Epoch 41: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.1690, Test Linf Norm: 1.5163\n",
      "Epoch 42: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.1638, Test Linf Norm: 1.5167\n",
      "Epoch 43: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.1015, Test Linf Norm: 1.5171\n",
      "Epoch 44: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.1712, Test Linf Norm: 1.5175\n",
      "Epoch 45: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.1670, Test Linf Norm: 1.5179\n",
      "Epoch 46: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.1698, Test Linf Norm: 1.5179\n",
      "Epoch 47: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.1721, Test Linf Norm: 1.5179\n",
      "Epoch 48: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.0835, Test Linf Norm: 1.5179\n",
      "Epoch 49: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.1326, Test Linf Norm: 1.5179\n",
      "Epoch 50: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.1515, Test Linf Norm: 1.5179\n",
      "Epoch 51: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.1641, Test Linf Norm: 1.5179\n",
      "Epoch 52: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.1407, Test Linf Norm: 1.5179\n",
      "Epoch 53: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.1660, Test Linf Norm: 1.5179\n",
      "Epoch 54: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.1727, Test Linf Norm: 1.5179\n",
      "Epoch 55: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.1659, Test Linf Norm: 1.5179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-07 08:02:08,733]\u001b[0m Trial 2 finished with value: 0.9989154116511345 and parameters: {'n_layers': 2, 'n_units_0': 79, 'n_units_1': 193, 'hidden_activation': 'LeakyReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'SGD', 'lr': 0.0001304170374026519, 'batch_size': 233, 'n_epochs': 56, 'scheduler': 'ReduceLROnPlateau'}. Best is trial 2 with value: 0.9989154116511345.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: Train Loss: 2.9528, Test Loss: 2.9338, Train L1 Norm: 1.0024, Test L1 Norm: 0.9989, Train Linf Norm: 2.1741, Test Linf Norm: 1.5179\n",
      "Epoch 1: Train Loss: 1.8247, Test Loss: 1.7788, Train L1 Norm: 15.0653, Test L1 Norm: 11.3699, Train Linf Norm: 2174.9655, Test Linf Norm: 1213.4413\n",
      "Epoch 2: Train Loss: 1.7908, Test Loss: 1.7721, Train L1 Norm: 17.0295, Test L1 Norm: 11.6029, Train Linf Norm: 2494.1654, Test Linf Norm: 1241.0643\n",
      "Epoch 3: Train Loss: 1.7857, Test Loss: 1.7694, Train L1 Norm: 17.1367, Test L1 Norm: 11.5968, Train Linf Norm: 2418.3921, Test Linf Norm: 1241.9817\n",
      "Epoch 4: Train Loss: 1.7833, Test Loss: 1.7680, Train L1 Norm: 17.1605, Test L1 Norm: 11.6540, Train Linf Norm: 2328.2037, Test Linf Norm: 1249.0601\n",
      "Epoch 5: Train Loss: 1.7821, Test Loss: 1.7672, Train L1 Norm: 17.2095, Test L1 Norm: 11.6698, Train Linf Norm: 2492.8787, Test Linf Norm: 1252.0308\n",
      "Epoch 6: Train Loss: 1.7813, Test Loss: 1.7667, Train L1 Norm: 17.2070, Test L1 Norm: 11.7836, Train Linf Norm: 2484.9866, Test Linf Norm: 1264.7171\n",
      "Epoch 7: Train Loss: 1.7808, Test Loss: 1.7666, Train L1 Norm: 17.2706, Test L1 Norm: 11.6563, Train Linf Norm: 2448.0296, Test Linf Norm: 1250.9321\n",
      "Epoch 8: Train Loss: 1.7804, Test Loss: 1.7664, Train L1 Norm: 17.2676, Test L1 Norm: 11.6629, Train Linf Norm: 2559.7364, Test Linf Norm: 1251.9969\n",
      "Epoch 9: Train Loss: 1.7801, Test Loss: 1.7661, Train L1 Norm: 17.2589, Test L1 Norm: 11.7033, Train Linf Norm: 2500.7251, Test Linf Norm: 1256.6844\n",
      "Epoch 10: Train Loss: 1.7799, Test Loss: 1.7658, Train L1 Norm: 17.2372, Test L1 Norm: 11.8543, Train Linf Norm: 2456.1447, Test Linf Norm: 1273.0840\n",
      "Epoch 11: Train Loss: 1.7796, Test Loss: 1.7659, Train L1 Norm: 17.3303, Test L1 Norm: 11.6700, Train Linf Norm: 2562.0120, Test Linf Norm: 1253.2710\n",
      "Epoch 12: Train Loss: 1.7795, Test Loss: 1.7657, Train L1 Norm: 17.2392, Test L1 Norm: 11.7297, Train Linf Norm: 2486.1674, Test Linf Norm: 1259.8446\n",
      "Epoch 13: Train Loss: 1.7793, Test Loss: 1.7656, Train L1 Norm: 17.2058, Test L1 Norm: 11.7505, Train Linf Norm: 2475.3407, Test Linf Norm: 1262.0845\n",
      "Epoch 14: Train Loss: 1.7793, Test Loss: 1.7656, Train L1 Norm: 17.2346, Test L1 Norm: 11.7615, Train Linf Norm: 2460.6097, Test Linf Norm: 1263.2735\n",
      "Epoch 15: Train Loss: 1.7793, Test Loss: 1.7656, Train L1 Norm: 17.2284, Test L1 Norm: 11.7731, Train Linf Norm: 2488.6342, Test Linf Norm: 1264.5174\n",
      "Epoch 16: Train Loss: 1.7793, Test Loss: 1.7656, Train L1 Norm: 17.2434, Test L1 Norm: 11.7810, Train Linf Norm: 2485.8330, Test Linf Norm: 1265.3722\n",
      "Epoch 17: Train Loss: 1.7793, Test Loss: 1.7655, Train L1 Norm: 17.2726, Test L1 Norm: 11.7798, Train Linf Norm: 2470.3095, Test Linf Norm: 1265.2436\n",
      "Epoch 18: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2532, Test L1 Norm: 11.7830, Train Linf Norm: 2478.6554, Test Linf Norm: 1265.5894\n",
      "Epoch 19: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2710, Test L1 Norm: 11.7834, Train Linf Norm: 2461.1732, Test Linf Norm: 1265.6343\n",
      "Epoch 20: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2622, Test L1 Norm: 11.7845, Train Linf Norm: 2558.2296, Test Linf Norm: 1265.7480\n",
      "Epoch 21: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2711, Test L1 Norm: 11.7841, Train Linf Norm: 2515.0232, Test Linf Norm: 1265.7069\n",
      "Epoch 22: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2705, Test L1 Norm: 11.7832, Train Linf Norm: 2481.9521, Test Linf Norm: 1265.6170\n",
      "Epoch 23: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2814, Test L1 Norm: 11.7810, Train Linf Norm: 2374.9464, Test Linf Norm: 1265.3738\n",
      "Epoch 24: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2621, Test L1 Norm: 11.7811, Train Linf Norm: 2537.5596, Test Linf Norm: 1265.3886\n",
      "Epoch 25: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2616, Test L1 Norm: 11.7811, Train Linf Norm: 2428.5547, Test Linf Norm: 1265.3939\n",
      "Epoch 26: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2623, Test L1 Norm: 11.7814, Train Linf Norm: 2493.4093, Test Linf Norm: 1265.4233\n",
      "Epoch 27: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2626, Test L1 Norm: 11.7815, Train Linf Norm: 2458.8750, Test Linf Norm: 1265.4368\n",
      "Epoch 28: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2616, Test L1 Norm: 11.7817, Train Linf Norm: 2467.7018, Test Linf Norm: 1265.4579\n",
      "Epoch 29: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2606, Test L1 Norm: 11.7818, Train Linf Norm: 2512.7748, Test Linf Norm: 1265.4703\n",
      "Epoch 30: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2621, Test L1 Norm: 11.7820, Train Linf Norm: 2433.0192, Test Linf Norm: 1265.4901\n",
      "Epoch 31: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2625, Test L1 Norm: 11.7822, Train Linf Norm: 2519.5117, Test Linf Norm: 1265.5115\n",
      "Epoch 32: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2630, Test L1 Norm: 11.7823, Train Linf Norm: 2500.9314, Test Linf Norm: 1265.5217\n",
      "Epoch 33: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2635, Test L1 Norm: 11.7825, Train Linf Norm: 2506.8842, Test Linf Norm: 1265.5386\n",
      "Epoch 34: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2632, Test L1 Norm: 11.7826, Train Linf Norm: 2476.4209, Test Linf Norm: 1265.5473\n",
      "Epoch 35: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2632, Test L1 Norm: 11.7826, Train Linf Norm: 2497.3968, Test Linf Norm: 1265.5499\n",
      "Epoch 36: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2634, Test L1 Norm: 11.7826, Train Linf Norm: 2489.8993, Test Linf Norm: 1265.5506\n",
      "Epoch 37: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2634, Test L1 Norm: 11.7826, Train Linf Norm: 2475.1162, Test Linf Norm: 1265.5512\n",
      "Epoch 38: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2634, Test L1 Norm: 11.7826, Train Linf Norm: 2529.7342, Test Linf Norm: 1265.5523\n",
      "Epoch 39: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2633, Test L1 Norm: 11.7826, Train Linf Norm: 2422.7797, Test Linf Norm: 1265.5530\n",
      "Epoch 40: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2633, Test L1 Norm: 11.7826, Train Linf Norm: 2524.4273, Test Linf Norm: 1265.5542\n",
      "Epoch 41: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2633, Test L1 Norm: 11.7826, Train Linf Norm: 2546.4515, Test Linf Norm: 1265.5541\n",
      "Epoch 42: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2633, Test L1 Norm: 11.7826, Train Linf Norm: 2419.2879, Test Linf Norm: 1265.5562\n",
      "Epoch 43: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2634, Test L1 Norm: 11.7827, Train Linf Norm: 2499.9988, Test Linf Norm: 1265.5578\n",
      "Epoch 44: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2633, Test L1 Norm: 11.7827, Train Linf Norm: 2451.5031, Test Linf Norm: 1265.5591\n",
      "Epoch 45: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2634, Test L1 Norm: 11.7827, Train Linf Norm: 2493.1567, Test Linf Norm: 1265.5597\n",
      "Epoch 46: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2634, Test L1 Norm: 11.7827, Train Linf Norm: 2472.1743, Test Linf Norm: 1265.5596\n",
      "Epoch 47: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2634, Test L1 Norm: 11.7827, Train Linf Norm: 2488.0899, Test Linf Norm: 1265.5596\n",
      "Epoch 48: Train Loss: 1.7792, Test Loss: 1.7655, Train L1 Norm: 17.2634, Test L1 Norm: 11.7827, Train Linf Norm: 2525.5724, Test Linf Norm: 1265.5594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-05-07 08:05:10,382]\u001b[0m Trial 3 failed with parameters: {'n_layers': 2, 'n_units_0': 104, 'n_units_1': 88, 'hidden_activation': 'Tanh', 'output_activation': 'Linear', 'loss': 'LogCosh', 'optimizer': 'Adagrad', 'lr': 0.0007878983783182211, 'batch_size': 225, 'n_epochs': 89, 'scheduler': 'ReduceLROnPlateau'} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yousousen/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_26905/4122804367.py\", line 27, in objective\n",
      "    _, _, _, test_metrics = train_and_eval(\n",
      "  File \"/tmp/ipykernel_26905/3615620923.py\", line 109, in train_and_eval\n",
      "    y_pred = net(x_batch)\n",
      "  File \"/home/yousousen/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_26905/609318184.py\", line 53, in forward\n",
      "    x = self.hidden_activation(layer(x))\n",
      "  File \"/home/yousousen/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/yousousen/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2023-05-07 08:05:10,388]\u001b[0m Trial 3 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m, sampler\u001b[39m=\u001b[39moptuna\u001b[39m.\u001b[39msamplers\u001b[39m.\u001b[39mTPESampler(), pruner\u001b[39m=\u001b[39moptuna\u001b[39m.\u001b[39mpruners\u001b[39m.\u001b[39mMedianPruner())\n\u001b[1;32m      4\u001b[0m \u001b[39m# Running Optuna with 100 trials without sampler and pruner arguments\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Printing the best trial information\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest trial:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     _optimize(\n\u001b[1;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    435\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     12\u001b[0m net, \\\n\u001b[1;32m     13\u001b[0m loss_fn, \\\n\u001b[1;32m     14\u001b[0m optimizer, \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m hidden_activation, \\\n\u001b[1;32m     24\u001b[0m output_activation \u001b[39m=\u001b[39m create_model(trial)\n\u001b[1;32m     26\u001b[0m \u001b[39m# Training and evaluating the network using the train_and_eval function\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m _, _, _, test_metrics \u001b[39m=\u001b[39m train_and_eval(\n\u001b[1;32m     28\u001b[0m     net, loss_fn, optimizer, batch_size, n_epochs, scheduler, trial\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[39m# Returning the last validation L1 norm as the objective value to minimize\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mreturn\u001b[39;00m test_metrics[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39ml1_norm\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[8], line 109\u001b[0m, in \u001b[0;36mtrain_and_eval\u001b[0;34m(net, loss_fn, optimizer, batch_size, n_epochs, scheduler, trial)\u001b[0m\n\u001b[1;32m    106\u001b[0m y_batch \u001b[39m=\u001b[39m y_batch\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    108\u001b[0m \u001b[39m# Performing a forward pass and computing the loss and metrics\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m y_pred \u001b[39m=\u001b[39m net(x_batch)\n\u001b[1;32m    110\u001b[0m loss, l1_norm, linf_norm \u001b[39m=\u001b[39m compute_loss_and_metrics(\n\u001b[1;32m    111\u001b[0m     y_pred, y_batch, loss_fn\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    115\u001b[0m \u001b[39m# Updating the total loss and metrics for the test set\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[5], line 53\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m# Looping over the hidden layers and applying the linear transformation and the activation function\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[0;32m---> 53\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_activation(layer(x))\n\u001b[1;32m     54\u001b[0m \u001b[39m# Applying the linear transformation and the activation function on the output layer\u001b[39;00m\n\u001b[1;32m     55\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_activation(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m](x))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Creating a study object with Optuna with TPE sampler and median pruner \n",
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner())\n",
    "\n",
    "# Running Optuna with 100 trials without sampler and pruner arguments\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Printing the best trial information\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the best network and optimizer using the best hyperparameters\n",
    "net, \\\n",
    "loss_fn, \\\n",
    "optimizer, \\\n",
    "batch_size, \\\n",
    "n_epochs, \\\n",
    "scheduler, \\\n",
    "loss_name, \\\n",
    "optimizer_name, \\\n",
    "scheduler_name, \\\n",
    "n_units, \\\n",
    "n_layers, \\\n",
    "hidden_activation, \\\n",
    "output_activation = create_model(trial)\n",
    "\n",
    "\n",
    "\n",
    "# Training and evaluating the best network using the train_and_eval function\n",
    "train_losses, test_losses, train_metrics, test_metrics = train_and_eval(\n",
    "    net, loss_fn, optimizer, batch_size, n_epochs, scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the losses and metrics for the best network \n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot([m[\"l1_norm\"] for m in train_metrics], label=\"Train L1 Norm\")\n",
    "plt.plot([m[\"l1_norm\"] for m in test_metrics], label=\"Test L1 Norm\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"L1 Norm\")\n",
    "plt.legend()\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot([m[\"linf_norm\"] for m in train_metrics], label=\"Train Linf Norm\")\n",
    "plt.plot([m[\"linf_norm\"] for m in test_metrics], label=\"Test Linf Norm\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Linf Norm\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Added plotting MSE of training data and MSE of test data in one plot \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_losses,label=\"MSE of training data\")\n",
    "plt.plot(test_losses,label=\"MSE of test data\")\n",
    "if scheduler is not None:\n",
    "    plt.plot([scheduler.get_last_lr()[0] for _ in range(n_epochs)], label=\"Learning rate\") \n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Saving the best network state dictionary using torch.save\n",
    "torch.save(net.state_dict(), \"best_net.pth\")\n",
    "\n",
    "# Saving the loss function name using pickle\n",
    "with open(\"loss_fn.pkl\", \"wb\") as f:\n",
    "    pickle.dump(loss_name, f)\n",
    "\n",
    "# Saving the optimizer name and parameters using pickle\n",
    "with open(\"optimizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump((optimizer_name, optimizer.state_dict()), f)\n",
    "\n",
    "# Saving the best number of epochs using pickle\n",
    "with open(\"n_epochs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(n_epochs, f)\n",
    "\n",
    "# Saving the scheduler name and parameters using pickle\n",
    "with open(\"scheduler.pkl\", \"wb\") as f:\n",
    "    pickle.dump((scheduler_name, scheduler.state_dict()), f)\n",
    "\n",
    "# Saving the number of units for each hidden layer using pickle\n",
    "with open(\"n_units.pkl\", \"wb\") as f:\n",
    "    pickle.dump(n_units, f)\n",
    "\n",
    "# Saving the output of create_model using pickle\n",
    "with open(\"create_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump((net, loss_fn, optimizer, batch_size, n_epochs, scheduler), f)\n",
    "\n",
    "# Saving the output of the training using pandas\n",
    "train_df = pd.DataFrame(\n",
    "    {\n",
    "        \"train_loss\": train_losses,\n",
    "        \"test_loss\": test_losses,\n",
    "        \"train_l1_norm\": [m[\"l1_norm\"] for m in train_metrics],\n",
    "        \"test_l1_norm\": [m[\"l1_norm\"] for m in test_metrics],\n",
    "        \"train_linf_norm\": [m[\"linf_norm\"] for m in train_metrics],\n",
    "        \"test_linf_norm\": [m[\"linf_norm\"] for m in test_metrics],\n",
    "    }\n",
    ")\n",
    "train_df.to_csv(\"train_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the best network state dictionary using torch.load\n",
    "state_dict = torch.load(\"best_net.pth\")\n",
    "\n",
    "# Loading the state dictionary into a new network instance using net.load_state_dict\n",
    "new_net = Net(n_layers, n_units, hidden_activation, output_activation).to(device)\n",
    "new_net.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Loading the loss function name using pickle\n",
    "with open(\"loss_fn.pkl\", \"rb\") as f:\n",
    "    loss_name = pickle.load(f)\n",
    "\n",
    "# Loading the optimizer name and parameters using pickle\n",
    "with open(\"optimizer.pkl\", \"rb\") as f:\n",
    "    optimizer_name, optimizer_state_dict = pickle.load(f)\n",
    "\n",
    "# Loading the best number of epochs using pickle\n",
    "with open(\"n_epochs.pkl\", \"rb\") as f:\n",
    "    n_epochs = pickle.load(f)\n",
    "\n",
    "# Loading the scheduler name and parameters using pickle\n",
    "with open(\"scheduler.pkl\", \"rb\") as f:\n",
    "    scheduler_name, scheduler_state_dict = pickle.load(f)\n",
    "\n",
    "# Loading the number of units for each hidden layer using pickle\n",
    "with open(\"n_units.pkl\", \"rb\") as f:\n",
    "    n_units = pickle.load(f)\n",
    "\n",
    "# Loading the output of create_model using pickle\n",
    "with open(\"create_model.pkl\", \"rb\") as f:\n",
    "    net, loss_fn, optimizer, batch_size, n_epochs, scheduler = pickle.load(f)\n",
    "\n",
    "# Loading the output of the training using pandas\n",
    "train_df = pd.read_csv(\"train_output.csv\")\n",
    "train_losses = train_df[\"train_loss\"].tolist()\n",
    "test_losses = train_df[\"test_loss\"].tolist()\n",
    "train_metrics = [\n",
    "    {\n",
    "        \"l1_norm\": train_df[\"train_l1_norm\"][i],\n",
    "        \"linf_norm\": train_df[\"train_linf_norm\"][i],\n",
    "    }\n",
    "    for i in range(len(train_df))\n",
    "]\n",
    "test_metrics = [\n",
    "    {\n",
    "        \"l1_norm\": train_df[\"test_l1_norm\"][i],\n",
    "        \"linf_norm\": train_df[\"test_linf_norm\"][i],\n",
    "    }\n",
    "    for i in range(len(train_df))\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

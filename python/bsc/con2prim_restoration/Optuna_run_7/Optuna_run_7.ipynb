{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A6J9DL8_MPs"
      },
      "source": [
        "# Neural network to learn conservative-to-primitive conversion in relativistic hydrodynamics\n",
        "We use Optuna to do a type of Bayesian optimization of the hyperparameters of the model. We then train the model using these hyperparameters to recover the primitive pressure from the conserved variables.\n",
        "\n",
        "Use this first cell to convert this notebook to a python script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRwi3muQ_MPy",
        "outputId": "ec908eaf-fbf7-4dc2-97b7-db9a456dbab6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "!jupyter nbconvert pt7.ipynb --TagRemovePreprocessor.enabled=True --TagRemovePreprocessor.remove_cell_tags='{\"remove_cell\"}' --to script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdwWLwVk_MP1"
      },
      "source": [
        "Next some cells for working on google colab,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3Dcn9y6_MP2",
        "outputId": "b631ae7b-7174-4e67-c0b8-0f0e24c63ebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.12.2)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.10.4)\n",
            "Requirement already satisfied: cmaes>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from optuna) (0.9.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.4.3)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.40.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "#%%script echo skipping\n",
        "\n",
        "!pip install optuna tensorboard tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s39rR8QF_MP2",
        "outputId": "265c03cb-6b21-4e15-d791-34b1edb7309e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#%%script echo skipping\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDps7B_j_MP3"
      },
      "outputs": [],
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import optuna\n",
        "import tensorboardX as tbx\n",
        "\n",
        "# Checking if GPU is available and setting the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UBWFIrK_MP3"
      },
      "source": [
        "## Constants and flags to set\n",
        "Defining some constants and parameters for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfR3J7Rg_MP4"
      },
      "outputs": [],
      "source": [
        "\n",
        "N_TRIALS = 200 # Number of trials for hyperparameter optimization\n",
        "OPTIMIZE = True # Whether to optimize the hyperparameters or to use predetermined values from Dieseldorst et al..\n",
        "\n",
        "# I try out here the values as obtained in Optuna run 5, but I will increase the number of epochs.\n",
        "# N_LAYERS_NO_OPT = 3\n",
        "# N_UNITS_NO_OPT = [78, 193, 99]\n",
        "# HIDDEN_ACTIVATION_NAME_NO_OPT = \"ReLU\"\n",
        "# OUTPUT_ACTIVATION_NAME_NO_OPT = \"Linear\"\n",
        "# LOSS_NAME_NO_OPT = \"MSE\"\n",
        "# OPTIMIZER_NAME_NO_OPT = \"Adam\"\n",
        "# LR_NO_OPT = 0.00036516467819506355\n",
        "# BATCH_SIZE_NO_OPT = 170\n",
        "# N_EPOCHS_NO_OPT = 400\n",
        "# SCHEDULER_NAME_NO_OPT = \"ReduceLROnPlateau\"\n",
        "\n",
        "# Dieselhorst hyperparameters\n",
        "N_LAYERS_NO_OPT = 2\n",
        "N_UNITS_NO_OPT = [600, 200]\n",
        "HIDDEN_ACTIVATION_NAME_NO_OPT = \"Sigmoid\"\n",
        "OUTPUT_ACTIVATION_NAME_NO_OPT = \"ReLU\"\n",
        "LOSS_NAME_NO_OPT = \"MSE\"\n",
        "OPTIMIZER_NAME_NO_OPT = \"Adam\"\n",
        "LR_NO_OPT = 6e-3\n",
        "BATCH_SIZE_NO_OPT = 32\n",
        "N_EPOCHS_NO_OPT = 400\n",
        "SCHEDULER_NAME_NO_OPT = \"ReduceLROnPlateau\"\n",
        "\n",
        "\n",
        "c = 1  # Speed of light (used in compute_conserved_variables and sample_primitive_variables functions)\n",
        "gamma = 5 / 3  # Adiabatic index (used in eos_analytic function)\n",
        "n_train_samples = 80000 # Number of training samples (used in generate_input_data and generate_labels functions)\n",
        "n_test_samples = 10000 # Number of test samples (used in generate_input_data and generate_labels functions)\n",
        "rho_interval = (0, 10.1) # Sampling interval for rest-mass density (used in sample_primitive_variables function)\n",
        "vx_interval = (0, 0.721 * c) # Sampling interval for velocity in x-direction (used in sample_primitive_variables function)\n",
        "epsilon_interval = (0, 2.02) # Sampling interval for specific internal energy (used in sample_primitive_variables function)\n",
        "\n",
        "np.random.seed(4) # Uncomment for pseudorandom data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyurwhCp_MP5"
      },
      "source": [
        "## Generating the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAWVYdaf_MP6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Defining an analytic equation of state (EOS) for an ideal gas\n",
        "def eos_analytic(rho, epsilon):\n",
        "    \"\"\"Computes the pressure from rest-mass density and specific internal energy using an analytic EOS.\n",
        "\n",
        "    Args:\n",
        "        rho (torch.Tensor): The rest-mass density tensor of shape (n_samples,).\n",
        "        epsilon (torch.Tensor): The specific internal energy tensor of shape (n_samples,).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The pressure tensor of shape (n_samples,).\n",
        "    \"\"\"\n",
        "    # Adding some assertions to check that the input tensors are valid and have the expected shape and type \n",
        "    assert isinstance(rho, torch.Tensor), \"rho must be a torch.Tensor\"\n",
        "    assert isinstance(epsilon, torch.Tensor), \"epsilon must be a torch.Tensor\"\n",
        "    assert rho.shape == epsilon.shape, \"rho and epsilon must have the same shape\"\n",
        "    assert rho.ndim == 1, \"rho and epsilon must be one-dimensional tensors\"\n",
        "    assert rho.dtype == torch.float32, \"rho and epsilon must have dtype torch.float32\"\n",
        "\n",
        "    return (gamma - 1) * rho * epsilon\n",
        "\n",
        "\n",
        "# Defining a function that samples primitive variables from uniform distributions\n",
        "def sample_primitive_variables(n_samples):\n",
        "    \"\"\"Samples primitive variables from uniform distributions.\n",
        "\n",
        "    Args:\n",
        "        n_samples (int): The number of samples to generate.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple of (rho, vx, epsilon), where rho is rest-mass density,\n",
        "            vx is velocity in x-direction,\n",
        "            epsilon is specific internal energy,\n",
        "            each being a numpy array of shape (n_samples,).\n",
        "    \"\"\"\n",
        "    # Sampling from uniform distributions with intervals matching Dieseldorst et al.\n",
        "    rho = np.random.uniform(*rho_interval, size=n_samples)  # Rest-mass density\n",
        "    vx = np.random.uniform(*vx_interval, size=n_samples)  # Velocity in x-direction\n",
        "    epsilon = np.random.uniform(*epsilon_interval, size=n_samples)  # Specific internal energy\n",
        "\n",
        "    # Returning the primitive variables\n",
        "    return rho, vx, epsilon\n",
        "\n",
        "\n",
        "# Defining a function that computes conserved variables from primitive variables\n",
        "def compute_conserved_variables(rho, vx, epsilon):\n",
        "    \"\"\"Computes conserved variables from primitive variables.\n",
        "\n",
        "    Args:\n",
        "        rho (torch.Tensor): The rest-mass density tensor of shape (n_samples,).\n",
        "        vx (torch.Tensor): The velocity in x-direction tensor of shape (n_samples,).\n",
        "        epsilon (torch.Tensor): The specific internal energy tensor of shape (n_samples,).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple of (D, Sx, tau), where D is conserved density,\n",
        "            Sx is conserved momentum in x-direction,\n",
        "            tau is conserved energy density,\n",
        "            each being a torch tensor of shape (n_samples,).\n",
        "    \"\"\"\n",
        "\n",
        "    # Computing the pressure from the primitive variables using the EOS\n",
        "    p = eos_analytic(rho, epsilon)\n",
        "    # Computing the Lorentz factor from the velocity.\n",
        "    W = 1 / torch.sqrt(1 - vx ** 2 / c ** 2)\n",
        "    # Specific enthalpy\n",
        "    h = 1 + epsilon + p / rho  \n",
        "\n",
        "    # Computing the conserved variables from the primitive variables\n",
        "    D = rho * W  # Conserved density\n",
        "    Sx = rho * h * W ** 2 * vx  # Conserved momentum in x-direction\n",
        "    tau = rho * h * W ** 2 - p - D  # Conserved energy density\n",
        "\n",
        "    # Returning the conserved variables\n",
        "    return D, Sx, tau\n",
        "\n",
        "# Defining a function that generates input data (conserved variables) from given samples of primitive variables\n",
        "def generate_input_data(rho, vx, epsilon):\n",
        "    # Converting the numpy arrays to torch tensors and moving them to the device\n",
        "    rho = torch.tensor(rho, dtype=torch.float32).to(device)\n",
        "    vx = torch.tensor(vx, dtype=torch.float32).to(device)\n",
        "    epsilon = torch.tensor(epsilon, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Computing the conserved variables using the compute_conserved_variables function\n",
        "    D, Sx, tau = compute_conserved_variables(rho, vx, epsilon)\n",
        "\n",
        "    # Stacking the conserved variables into a torch tensor\n",
        "    x = torch.stack([D, Sx, tau], axis=1)\n",
        "\n",
        "    # Returning the input data tensor\n",
        "    return x\n",
        "\n",
        "# Defining a function that generates output data (labels) from given samples of primitive variables\n",
        "def generate_labels(rho, epsilon):\n",
        "    # Converting the numpy arrays to torch tensors and moving them to the device\n",
        "    rho = torch.tensor(rho, dtype=torch.float32).to(device)\n",
        "    epsilon = torch.tensor(epsilon, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Computing the pressure from the primitive variables using the EOS\n",
        "    p = eos_analytic(rho, epsilon)\n",
        "\n",
        "    # Returning the output data tensor\n",
        "    return p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyV8eTje_MP7"
      },
      "source": [
        "Sampling the primitive variables using the sample_primitive_variables function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x70cNiU6_MP7"
      },
      "outputs": [],
      "source": [
        "rho_train, vx_train, epsilon_train = sample_primitive_variables(n_train_samples)\n",
        "rho_test, vx_test, epsilon_test = sample_primitive_variables(n_test_samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLrgWF1s_MP8"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"last_expr_or_assign\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "EkMYUplJ_MP8",
        "outputId": "5bbd10f9-47c5-4c59-f6f1-012f7b5d44b1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1200x400 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plotting the histograms of rho, vx and epsilon\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(rho_train, bins=20)\n",
        "plt.xlabel(\"rho\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "#plt.yscale(\"log\")\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.hist(vx_train, bins=20)\n",
        "plt.xlabel(\"vx\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "#plt.yscale(\"log\")\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(epsilon_train, bins=20)\n",
        "plt.xlabel(\"epsilon\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "#plt.yscale(\"log\")\n",
        "plt.suptitle(\"Primitive variables\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jWkERUe_MP9",
        "outputId": "05dd39b9-d3cb-4d13-a3a7-8eccea70d8ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([3.7178, 2.2458, 6.2395,  ..., 1.1359, 2.7856, 0.9368], device='cuda:0')"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Generating the input and output data for train and test sets.\n",
        "x_train = generate_input_data(rho_train, vx_train, epsilon_train)\n",
        "y_train = generate_labels(rho_train, epsilon_train) \n",
        "x_test = generate_input_data(rho_test, vx_test, epsilon_test) \n",
        "y_test = generate_labels(rho_test, epsilon_test) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "RxNDvSRK_MP-",
        "outputId": "2643788e-769f-4607-997e-0f7aaef2e596"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAGMCAYAAAALJhESAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABRAklEQVR4nO3deXxU5dn/8e9ASCBAwp6ALGHXKCYKIUWlBIyySQVc0KokwEMVB0UjVrAV5KkVFMUoz1S0LUS0VsQi2lJRCZtVhLCDkU3DJiRskpAgCUzO7w9/mTJkITOZnNk+79drXjL3uc8515kzk8u55j73sRiGYQgAAAAAAAAwUR1vBwAAAAAAAIDgQ1EKAAAAAAAApqMoBQAAAAAAANNRlAIAAAAAAIDpKEoBAAAAAADAdBSlAAAAAAAAYDqKUgAAAAAAADAdRSkAAAAAAACYjqIUAAAAAAAATEdRCgAAOMnIyJDFYtH+/fu9HYpDTEyMbrvtNo9us7CwUP/zP/+j6OhoWSwWPfbYYx7dPv6rovdUUlKSkpKSTI/FW/sFAADlUZQCAMBNZV+0N27c6O1QJElnz57Vs88+q9WrV3sthnfffVfp6ele278rnn/+eWVkZGjChAl6++239cADD3g7JHhIdna2nn32WZ8qrAIAgPJCvB0AAADwjLNnz2rGjBmS5LWRIO+++6527tzpF6OOVq5cqV/84heaPn26t0MJSp999lmtbTs7O1szZsxQUlKSYmJiTNsvAABwDSOlAABAUDp27JiaNGnise1duHBBJSUlHttebSoqKvJ2CAoNDVVoaGjQ7BcAAJRHUQoAAA9KTU1Vo0aN9MMPP2j48OFq1KiRWrZsqcmTJ8tutzv67d+/XxaLRS+99JJeeeUVdejQQQ0aNFC/fv20c+dOp21WNgdOamqqYxTI/v371bJlS0nSjBkzZLFYZLFY9Oyzz1YZ7zfffKMBAwaoQYMGatu2rZ577jmVlpaW6/fRRx9p6NChatOmjcLCwtS5c2f94Q9/cDqmpKQkLVu2TAcOHHDsvyy+kpISTZs2TT179lRkZKQaNmyovn37atWqVdV4Vf/rs88+U3x8vOrXr6/Y2FgtWbKkXJ/Tp0/rscceU7t27RQWFqYuXbrohRdecBzX6tWrZbFYlJOTo2XLljliLbvU69ixYxo3bpyioqJUv359xcXF6a233nLax8XnLz09XZ07d1ZYWJiys7MlSbt27dKdd96pZs2aqX79+urVq5c+/vjjyx7fxdt98803HdtNSEhQVlZWuf4rV65U37591bBhQzVp0kS33367vv32W6c+zz77rCwWi7Kzs/XrX/9aTZs21U033STpv3N1rV69Wr169VKDBg3Uo0cPxyWgS5YsUY8ePVS/fn317NlTW7Zscdr29u3blZqaqk6dOql+/fqKjo7W2LFjdfLkycse66Xv65iYGMe5uPRRFs+BAwf08MMPq3v37mrQoIGaN2+uu+66y+kyvYyMDN11112SpP79+5fbRkWfJ1fPeXXODQAAuDwu3wMAwMPsdrsGDhyoxMREvfTSS1qxYoVefvllde7cWRMmTHDqu3DhQp05c0ZWq1Xnzp3Tq6++qgEDBmjHjh2Kioqq9j5btmyp119/XRMmTNCIESM0cuRISdK1115b6Tq5ubnq37+/Lly4oClTpqhhw4Z688031aBBg3J9MzIy1KhRI6WlpalRo0ZauXKlpk2bpoKCAs2ePVuS9Lvf/U75+fk6fPiwXnnlFUlSo0aNJEkFBQX6y1/+onvvvVfjx4/XmTNn9Ne//lUDBw7Uhg0bFB8ff9lj3Lt3r0aNGqWHHnpIKSkpWrBgge666y4tX75ct9xyi6SfL2Hs16+ffvjhBz344INq3769vvrqK02dOlVHjx5Venq6rrrqKr399tt6/PHH1bZtWz3xxBOO1/Cnn35SUlKS9u3bp4kTJ6pjx45avHixUlNTdfr0aU2aNMkppgULFujcuXP6zW9+o7CwMDVr1kzffPONbrzxRl1xxRWO1/X999/X8OHD9Y9//EMjRoy47LG+++67OnPmjB588EFZLBa9+OKLGjlypL7//nvVq1dPkrRixQoNHjxYnTp10rPPPquffvpJc+fO1Y033qjNmzeXu2ztrrvuUteuXfX888/LMAxH+759+/TrX/9aDz74oO6//3699NJLGjZsmObNm6enn35aDz/8sCRp5syZuvvuu7V7927VqfPz75qff/65vv/+e40ZM0bR0dH65ptv9Oabb+qbb77R119/LYvFctljLZOenq7CwkKntldeeUVbt25V8+bNJUlZWVn66quvdM8996ht27bav3+/Xn/9dSUlJSk7O1vh4eH65S9/qUcffVSvvfaann76aV111VWS5PjvpVw959U5NwAAoJoMAADglgULFhiSjKysLEdbSkqKIcn43//9X6e+1113ndGzZ0/H85ycHEOS0aBBA+Pw4cOO9vXr1xuSjMcff9zR1q9fP6Nfv37l9p+SkmJ06NDB8fz48eOGJGP69OnViv+xxx4zJBnr1693tB07dsyIjIw0JBk5OTmO9rNnz5Zb/8EHHzTCw8ONc+fOOdqGDh3qFFOZCxcuGMXFxU5tP/74oxEVFWWMHTv2srF26NDBkGT84x//cLTl5+cbrVu3Nq677jpH2x/+8AejYcOGxp49e5zWnzJlilG3bl3j4MGDTtscOnSoU7/09HRDkvHOO+842kpKSow+ffoYjRo1MgoKCgzD+O/5i4iIMI4dO+a0jZtvvtno0aOH0+tSWlpq3HDDDUbXrl2rPM6y7TZv3tw4deqUo/2jjz4yJBn//Oc/HW3x8fFGq1atjJMnTzratm3bZtSpU8cYPXq0o2369OmGJOPee+8tt7+y1/Wrr75ytH366aeO9+aBAwcc7W+88YYhyVi1apWjraL3xd///ndDkrF27VpHW9ln5eL3VGXv6zLvv/9+uc9SRftbt26dIclYuHCho23x4sXlYq1sv66e8+qcGwAAUD1cvgcAQC146KGHnJ737dtX33//fbl+w4cP1xVXXOF43rt3byUmJurf//53rcf473//W7/4xS/Uu3dvR1vLli113333let78eipM2fO6MSJE+rbt6/Onj2rXbt2XXZfdevWdczjU1paqlOnTunChQvq1auXNm/eXK1427Rp4zTKKCIiQqNHj9aWLVuUm5srSVq8eLH69u2rpk2b6sSJE45HcnKy7Ha71q5dW+U+/v3vfys6Olr33nuvo61evXp69NFHVVhYqDVr1jj1v+OOOxyXTUrSqVOntHLlSt19992O1+nEiRM6efKkBg4cqL179+qHH3647LGOGjVKTZs2dTzv27evJDneQ0ePHtXWrVuVmpqqZs2aOfpde+21uuWWWyp8/1z6niwTGxurPn36OJ4nJiZKkgYMGKD27duXa7/4fXzx++LcuXM6ceKEfvGLX0hStc9rRbKzszV27Fjdfvvt+v3vf1/h/s6fP6+TJ0+qS5cuatKkidv7c/WcX+7cAACA6uPyPQAAPKx+/fpOhQpJatq0qX788cdyfbt27VqurVu3bnr//fdrLb4yBw4ccBQaLta9e/dybd98841+//vfa+XKlSooKHBalp+fX639vfXWW3r55Ze1a9cunT9/3tHesWPHaq3fpUuXcpeDdevWTdLP8/1ER0dr79692r59e7nXv8yxY8eq3MeBAwfUtWtXx+VpZcou/Tpw4IBT+6Wx79u3T4Zh6JlnntEzzzxTaQwXFyIrcnExSJKjCFL2HiqLo6JzddVVV+nTTz9VUVGRGjZsWGmsle0rMjJSktSuXbsK2y9+H586dUozZszQe++9V+61re774lIFBQUaOXKkrrjiCi1cuNDpnP/000+aOXOmFixYoB9++MHpMkR39+fqOb/cuQEAANVHUQoAAA+rW7euR7dnsVicvnyXuXiS8dp0+vRp9evXTxEREfrf//1fde7cWfXr19fmzZv11FNPVTgx+qXeeecdpaamavjw4XryySfVqlUr1a1bVzNnztR3333nsVhLS0t1yy236Le//W2Fy8uKWJ5y6fxbZa/F5MmTNXDgwArX6dKly2W3W9l7qKL3QXVVNFdYVfuqTgx33323vvrqKz355JOKj49Xo0aNVFpaqkGDBlXrfVGR1NRUHTlyRBs2bFBERITTskceeUQLFizQY489pj59+igyMlIWi0X33HOP2/tzVW2cGwAAghVFKQAAvGjv3r3l2vbs2eM0SXXTpk0rvDTo0hEcrkwqLUkdOnSocP+7d+92er569WqdPHlSS5Ys0S9/+UtHe05OTrl1K4vhgw8+UKdOnbRkyRKnPtOnT692vGWjkC5ef8+ePZLkeL06d+6swsJCJScnV3u7F+vQoYO2b9+u0tJSp5EzZZcodujQocr1O3XqJOnny7/cjaG6cUrlz5X0c6wtWrRwGiVVG3788UdlZmZqxowZmjZtmqO9ovdUdc2aNUtLly7VkiVLdOWVV5Zb/sEHHyglJUUvv/yyo+3cuXM6ffq0Uz9XPgs1PecAAMB9zCkFAIAXLV261GmOoQ0bNmj9+vUaPHiwo61z587atWuXjh8/7mjbtm2bvvzyS6dthYeHS1K5L+iVGTJkiL7++mtt2LDB0Xb8+HH97W9/c+pXNjLk4pEgJSUl+tOf/lRumw0bNqzwMqqKtrF+/XqtW7euWrFK0pEjR/Thhx86nhcUFGjhwoWKj49XdHS0pJ9H7qxbt06ffvppufVPnz6tCxcuVLmPIUOGKDc3V4sWLXK0XbhwQXPnzlWjRo3Ur1+/Ktdv1aqVkpKS9MYbb+jo0aPlll98DmuidevWio+P11tvveV0vnfu3KnPPvtMQ4YM8ch+qlLROZV+voueO1asWKHf//73+t3vfqfhw4dXus9L9zd37txyowbLCnLV+SzU9JwDAAD3MVIKAAAv6tKli2666SZNmDBBxcXFSk9PV/PmzZ0uPxs7dqzmzJmjgQMHaty4cTp27JjmzZunq6++2ml+pwYNGig2NlaLFi1St27d1KxZM11zzTW65pprKtz3b3/7W7399tsaNGiQJk2apIYNG+rNN990jBwpc8MNN6hp06ZKSUnRo48+KovForfffrvCy5V69uypRYsWKS0tTQkJCWrUqJGGDRum2267TUuWLNGIESM0dOhQ5eTkaN68eYqNjVVhYWG1Xqtu3bpp3LhxysrKUlRUlObPn6+8vDwtWLDA0efJJ5/Uxx9/rNtuu02pqanq2bOnioqKtGPHDn3wwQfav3+/WrRoUek+fvOb3+iNN95QamqqNm3apJiYGH3wwQf68ssvlZ6ersaNG182TpvNpptuukk9evTQ+PHj1alTJ+Xl5WndunU6fPiwtm3bVq3jvZzZs2dr8ODB6tOnj8aNG6effvpJc+fOVWRkpJ599lmP7KMqERER+uUvf6kXX3xR58+f1xVXXKHPPvuswhF01XHvvfeqZcuW6tq1q9555x2nZbfccouioqJ022236e2331ZkZKRiY2O1bt06rVixQs2bN3fqHx8fr7p16+qFF15Qfn6+wsLCNGDAALVq1arcfj1xzgEAgHsoSgEA4EWjR49WnTp1lJ6ermPHjql37976v//7P7Vu3drR56qrrtLChQs1bdo0paWlKTY2Vm+//bbeffddrV692ml7f/nLX/TII4/o8ccfV0lJiaZPn15pUap169ZatWqVHnnkEc2aNUvNmzfXQw89pDZt2mjcuHGOfs2bN9e//vUvPfHEE/r973+vpk2b6v7779fNN99cbt6khx9+WFu3btWCBQv0yiuvqEOHDho2bJhSU1OVm5urN954Q59++qliY2P1zjvvaPHixeWOoTJdu3bV3Llz9eSTT2r37t3q2LGjFi1a5BRDeHi41qxZo+eff16LFy/WwoULFRERoW7dumnGjBmOybor06BBA61evVpTpkzRW2+9pYKCAnXv3l0LFixQampqteKMjY3Vxo0bNWPGDGVkZOjkyZNq1aqVrrvuOqfL3GoqOTlZy5cv1/Tp0zVt2jTVq1dP/fr10wsvvFDtyeNr6t1339Ujjzwim80mwzB066236pNPPlGbNm1c3taJEyckSSkpKeWWrVq1SlFRUXr11VdVt25d/e1vf9O5c+d04403asWKFeXeh9HR0Zo3b55mzpypcePGyW63a9WqVRUWpTxxzgEAgHssBrMyAgBguv3796tjx46aPXu2Jk+e7O1wAAAAANMxpxQAAAAAAABMR1EKAAAAAAAApqMoBQAAAAAAANMxpxQAAAAAAABMx0gpAAAAAAAAmI6iFAAAAAAAAExHUQoAAAAAAACmoygFAAAAAAAA01GUAgAAAAAAgOkoSgEAAAAAAMB0FKUAAAAAAABgOopSAAAAAAAAMB1FKQAAAAAAAJiOohQAAAAAAABMR1EKAAAAAAAApqMoBQAAAAAAANNRlAIAAAAAAIDpKEoBAAAAAADAdBSlAAAAAAAAYDqKUgAAAAAAADAdRSkAAAAAAACYjqIUAAAAAAAATEdRCgAAAAAAAKYL8XYA3lRaWqojR46ocePGslgs3g4HAExnGIbOnDmjNm3aqE4dfqeoCjkDQLAjZ1QfOQNAsKtuzgjqotSRI0fUrl07b4cBAF536NAhtW3b1tth+DRyBgD8jJxxeeQMAPjZ5XJGUBelGjduLOnnFykiIsLL0QCA+QoKCtSuXTvH30NUjpwBINiRM6qPnAEg2FU3ZwR1UapsKG1ERATJAkBQ49KCyyNnAMDPyBmXR84AgJ9dLmdwMTgAAFWw2WyKjY1VQkKCt0MBAAAAAgpFKQAAqmC1WpWdna2srCxvhwIAAAAElKAsSvGrNwAAAAAAgHcFZVGKX70BAAAAAAC8KyiLUgAAAAAAAPAuilIAAAAA4AFMEwIArqEoBQAAAAAewDQhAOAailIAAFSBX70BAACA2kFRCgCAKvCrNwAAAFA7KEoBAAAAAADAdCHeDiBYxUxZ5tZ6+2cN9XAkAAAzuPt3X+JvPwAEG74rAAgWjJQCAAAAAACA6RgpVQM1+dXbG/v0xi8nxOub+6zpft3Fr3cAAAAAgDIUpYKIvxWI/I2/FSkBAAAAAPAmilLwWVxLDwAAAABA4ArKopTNZpPNZpPdbvd2KH7Dn0bk+FOsAHwfOQMAAACoHUE50bnValV2draysrK8HQoAwMeRMwAAAIDaEZRFKQAAAAAAAHgXRSkAAAAA8ACbzabY2FglJCR4OxQA8AsUpQAAAADAA7jkGwBcQ1EKAAAAAAAApqMoBQAAAAAAANNRlAIAAAAAAIDpKEoBAAAAAADAdBSlAAAAAAAAYDqKUgAAAAAAADBdiLcDAADAl9lsNtlsNtntdm+HAgBAlWKmLHN73f2zhnowEgCoHkZKAQBQBavVquzsbGVlZXk7FAAAACCgUJQCAAAAAACA6ShKAQAAAAAAwHQUpQAAAAAAAGC6oCxK2Ww2xcbGKiEhwduhAAAAAAAABKWgLEoxaS0AAAAAAIB3BWVRCgAAAAAAAN4V4u0AAABA1WKmLHNrvf2zhno4EgAAAMBzGCkFAAAAABU4e/asOnTooMmTJ3s7FAAISIyUAgAAAIAK/PGPf9QvfvELb4dhCndH5UqMzAXgPkZKAQAAAMAl9u7dq127dmnw4MHeDgUAAhZFKQAAAAABZe3atRo2bJjatGkji8WipUuXlutjs9kUExOj+vXrKzExURs2bHBaPnnyZM2cOdOkiAEgOFGUAgAAABBQioqKFBcXJ5vNVuHyRYsWKS0tTdOnT9fmzZsVFxengQMH6tixY5Kkjz76SN26dVO3bt2qtb/i4mIVFBQ4PQAAl8ecUgAAAAACyuDBg6u87G7OnDkaP368xowZI0maN2+eli1bpvnz52vKlCn6+uuv9d5772nx4sUqLCzU+fPnFRERoWnTplW4vZkzZ2rGjBm1ciwAEMgYKQUACCrcSQkAgltJSYk2bdqk5ORkR1udOnWUnJysdevWSfq5yHTo0CHt379fL730ksaPH19pQUqSpk6dqvz8fMfj0KFDtX4cABAIGCkFAAgqwXQnJQBAeSdOnJDdbldUVJRTe1RUlHbt2uXWNsPCwhQWFuaJ8AAgqFCUAgAEjbI7KQ0bNkw7d+70djgAAD+Qmprq7RAAIGBx+R4AwC9wJyUAgCe0aNFCdevWVV5enlN7Xl6eoqOja7Rtm82m2NhYJSQk1Gg7ABAsKEoBAPwCd1ICAHhCaGioevbsqczMTEdbaWmpMjMz1adPnxpt22q1Kjs7W1lZWTUNEwCCApfvAQD8AndSAgBUV2Fhofbt2+d4npOTo61bt6pZs2Zq37690tLSlJKSol69eql3795KT09XUVGRI4fANTFTlrm13v5ZQz0cCQB/Q1EKAOD3yu6kNHXqVEdbRXdSKrt0LyMjQzt37rzsnZTS0tIczwsKCtSuXbtaOgIAgCdt3LhR/fv3dzwv+3uekpKijIwMjRo1SsePH9e0adOUm5ur+Ph4LV++vNzk566y2Wyy2Wyy2+012g4ABAuKUgBM4+6vaBK/pKFq3EmpYnzmAASrpKQkGYZRZZ+JEydq4sSJHt2v1WqV1WpVQUGBIiMjPbptAAhEFKUAAEHHlTsp8as3AAAAUDuY6BwA4Pdq805KTFoLAAAA1A6KUgAAv1ebd1ICAAAAUDuC8vI9LsUAAP/DnZQAAL6O7xkA4JqgLEoxASEA+B/upAQA8HV8zwAA1wRlUQoA4H+4kxIAAAAQWJhTCgAAAAAAAKZjpBQAAFXg8j0AAGpHzJRlbq+7f9ZQD0YCwFsYKQUAQBWsVquys7OVlZXl7VAAAD7OZrMpNjZWCQkJ3g4FAPwCRSkAAAAA8AB+yAAA11CUAgAAAAAAgOkoSgEAAAAAAMB0THQOAEAVgnWic3cnn2XiWQAAAFQXI6UAAKgC84MAAKqLic4BwDUUpQAAAADAA/ghAwBcQ1EKAAAAAAAApqMoBQAAAAAAANNRlAIAoArMDwIAAADUDu6+BwBAFaxWq6xWqwoKChQZGentcAAAgNy/S6zEnWIBX8JIKQAAAAAAAJiOohQAAAAAeACXfAOAayhKAQAAAIAHWK1WZWdnKysry9uhAIBfYE4pAADgMczxAQAAgOpipBQAAFXgUgwAAACgdlCUAgCgClyKAQAAANQOilIAAAAAAAAwHUUpAAAAAAAAmI6iFAAAAAAAAExHUQoAAAAAAACmoygFAAAAAAAA01GUAgAAAAAPsNlsio2NVUJCgrdDAQC/EOLtAAAA8GU2m002m012u93boQAAfJzVapXValVBQYEiIyO9HQ4qETNlmVvr7Z811MORAAjKkVL8ggEAqC6r1ars7GxlZWV5OxQAAAAgoATlSCl+wQAAwPe4+8u1xK/XAAAA/igoR0oBAAAAAADAuyhKAQAAAAAAwHQUpQAAAAAAAGA6ilIAAAAAAAAwXVBOdA7A/3DrXgAAAAAILIyUAgAAAAAAgOkoSgEAAAAAAMB0FKUAAAAAAABgOuaUAgAAfo955wAAAPwPI6UAAKiCzWZTbGysEhISvB0KAMDHkTMAwDUUpQAAqILValV2draysrK8HQoAwMeRMwDANVy+BwAAAADAZbh7qbjE5eJAZRgpBQAAAAAAANNRlAIAAAAAAIDpKEoBAAAAAADAdBSlAAAAAAAAYDqKUgAAAAAAADAdd98DAABBizspAQAAeA8jpQAAAAAAAGA6ilIAAAAAAAAwHUUpAAAAAAAAmI6iFAAAAAAAAExHUQoAAAAAAACmoygFAAAAAAAA04V4OwAAAMxw+vRpJScn68KFC7pw4YImTZqk8ePHezssAAAQBGKmLHNrvf2zhno4EsC3UJQCAASFxo0ba+3atQoPD1dRUZGuueYajRw5Us2bN/d2aAAAAEBQoigFAAgKdevWVXh4uCSpuLhYhmHIMAwvRwV/5u6v3hK/fAO+jtG1AGAO5pQCAPiFtWvXatiwYWrTpo0sFouWLl1aro/NZlNMTIzq16+vxMREbdiwwWn56dOnFRcXp7Zt2+rJJ59UixYtTIoeAOBPykbXbt26VevXr9fzzz+vkydPejssAAg4FKUAAH6hqKhIcXFxstlsFS5ftGiR0tLSNH36dG3evFlxcXEaOHCgjh075ujTpEkTbdu2TTk5OXr33XeVl5dX6f6Ki4tVUFDg9AAABAdG1wKAOdwqSn3//feejgMAEKA8lTMGDx6s5557TiNGjKhw+Zw5czR+/HiNGTNGsbGxmjdvnsLDwzV//vxyfaOiohQXF6cvvvii0v3NnDlTkZGRjke7du08chwAgMp5KmcwuhYA/INbRakuXbqof//+euedd3Tu3DlPxwQACCBm5IySkhJt2rRJycnJjrY6deooOTlZ69atkyTl5eXpzJkzkqT8/HytXbtW3bt3r3SbU6dOVX5+vuNx6NChWokdAPBfnsoZjK4FAP/gVlFq8+bNuvbaa5WWlqbo6Gg9+OCD5X5ZAABAMidnnDhxQna7XVFRUU7tUVFRys3NlSQdOHBAffv2VVxcnPr27atHHnlEPXr0qHSbYWFhioiIcHoAAGqXp3IGo2sBwD+4VZSKj4/Xq6++qiNHjmj+/Pk6evSobrrpJl1zzTWaM2eOjh8/7uk4AQB+yldyRu/evbV161Zt27ZN27dv14MPPlit9Ww2m2JjY5WQkFDLEQIAzMgZjK4FAN9Ro4nOQ0JCNHLkSC1evFgvvPCC9u3bp8mTJ6tdu3YaPXq0jh496qk4AQB+rjZzRosWLVS3bt1yl1bk5eUpOjq6RnFbrVZlZ2crKyurRtsBAFRfbeYMRtcCgO8IqcnKGzdu1Pz58/Xee++pYcOGmjx5ssaNG6fDhw9rxowZuv3227msDwAgqXZzRmhoqHr27KnMzEwNHz5cklRaWqrMzExNnDjRg0cBeEbMlGVurbd/1lAPRwL4Jm9/zygbXesqm80mm80mu93u+aAAIAC5VZSaM2eOFixYoN27d2vIkCFauHChhgwZojp1fh541bFjR2VkZCgmJsaTsQIA/JCnckZhYaH27dvneJ6Tk6OtW7eqWbNmat++vdLS0pSSkqJevXqpd+/eSk9PV1FRkcaMGVOj+PmCAQDmMeN7Rm2PrrVarSooKFBkZGSNtgUAwcCtotTrr7+usWPHKjU1Va1bt66wT6tWrfTXv/61RsEBAPyfp3LGxo0b1b9/f8fztLQ0SVJKSooyMjI0atQoHT9+XNOmTVNubq7i4+O1fPnycpdnuIovGABgHjO+ZzC6Fv7E3ZG1EqNr4R/cKkrt3bv3sn1CQ0OVkpLizuYBwGNI5N7nqZyRlJQkwzCq7DNx4kS+UACAH/NUzvDW6FoAgGvcKkotWLBAjRo10l133eXUvnjxYp09e5ZiFADAgZwBAKguT+UMb42u5ZJvAHCNW3ffmzlzplq0aFGuvVWrVnr++edrHBQAIHD4e86w2WyKjY1VQkKCt0MBgIDnqZxRNrr20kdGRoajz8SJE3XgwAEVFxdr/fr1SkxMrHH83LEVAFzjVlHq4MGD6tixY7n2Dh066ODBgzUOCgAQOPw9Z/AFAwDM4+85AwDgGreKUq1atdL27dvLtW/btk3NmzevcVAAgMBBzgAAVBc5AwCCi1tFqXvvvVePPvqoVq1aJbvdLrvdrpUrV2rSpEm65557PB0jAMCPkTMAANXl7zmDS74BwDVuTXT+hz/8Qfv379fNN9+skJCfN1FaWqrRo0ebPj/I6dOnlZycrAsXLujChQuaNGmSxo8fb2oMAIDK+VLOcAeT1gKAefw9Z1itVlmtVhUUFCgyMtLb4QCAz7MYl7u/dhX27Nmjbdu2qUGDBurRo4c6dOjgydiqxW63q7i4WOHh4SoqKtI111yjjRs3Vmt4b1myyM/PV0REhMv7rsmt5gH4vv2zhno7hFpX07+DrvCFnFET5Az4gmD4uwTfRc6oPnIG/B35BjVV3b+Dbo2UKtOtWzd169atJpuosbp16yo8PFySVFxc7LizBgDAt/hCzgD8XU2+qPIFA/6EnAEAwcGtopTdbldGRoYyMzN17NgxlZaWOi1fuXJltbe1du1azZ49W5s2bdLRo0f14Ycfavjw4U59bDabZs+erdzcXMXFxWnu3Lnq3bu3Y/np06fVr18/7d27V7Nnz67wNrIAAO/wZM4AAAQ2cgYABBe3ilKTJk1SRkaGhg4dqmuuuUYWi8XtAIqKihQXF6exY8dq5MiR5ZYvWrRIaWlpmjdvnhITE5Wenq6BAwdq9+7datWqlSSpSZMm2rZtm/Ly8jRy5EjdeeedioqKKret4uJiFRcXO54XFBS4HTcAoHo8mTMAAIHN33MG8xACgGvcKkq99957ev/99zVkyJAaBzB48GANHjy40uVz5szR+PHjNWbMGEnSvHnztGzZMs2fP19Tpkxx6hsVFaW4uDh98cUXuvPOO8tta+bMmZoxY0aNYwYAVJ8ncwYAILD5e85gonMAcE0dd1YKDQ1Vly5dPB1LOSUlJdq0aZOSk5MdbXXq1FFycrLWrVsnScrLy9OZM2ckSfn5+Vq7dq26d+9e4famTp2q/Px8x+PQoUO1fgwAEOzMyhm1hdt7A4B5/D1nAABc41ZR6oknntCrr75a6xOKnzhxQna7vdyleFFRUcrNzZUkHThwQH379lVcXJz69u2rRx55RD169Khwe2FhYYqIiHB6AABql1k5o7ZYrVZlZ2crKyvL26EAQMDz95wBAHCNW5fv/ec//9GqVav0ySef6Oqrr1a9evWcli9ZssQjwVVH7969tXXrVtP2BwBwjS/lDACAbyNnAEBwcaso1aRJE40YMcLTsZTTokUL1a1bV3l5eU7teXl5io6OrvX9AwBqzqycAQDwf+QMAAgubhWlFixY4Ok4KhQaGqqePXsqMzNTw4cPlySVlpYqMzNTEydONCUGAEDNmJUzAFQtZsoyt9bbP2uohyMBKufvOYO77wGAa9yaU0qSLly4oBUrVuiNN95wTDR+5MgRFRYWurSdwsJCbd261XEJXk5OjrZu3aqDBw9KktLS0vTnP/9Zb731lr799ltNmDBBRUVFjrvxAQB8n6dyBgAg8PlzzmAeQgBwjVsjpQ4cOKBBgwbp4MGDKi4u1i233KLGjRvrhRdeUHFxsebNm1ftbW3cuFH9+/d3PE9LS5MkpaSkKCMjQ6NGjdLx48c1bdo05ebmKj4+XsuXLy83+TkAwDd5Mmd4A796A4B5/D1nAABc49ZIqUmTJqlXr1768ccf1aBBA0f7iBEjlJmZ6dK2kpKSZBhGuUdGRoajz8SJE3XgwAEVFxdr/fr1SkxMdCdsB27vDQDm8WTO8AZ+9QYA8/h7zgAAuMatkVJffPGFvvrqK4WGhjq1x8TE6IcffvBIYLXJarXKarWqoKBAkZGR3g4HAAKav+cMAIB5yBkAEFzcGilVWlpa4WUMhw8fVuPGjWscFAAgcJAzAADVRc4AgODi1kipW2+9Venp6XrzzTclSRaLRYWFhZo+fbqGDBni0QABAP6NnAEAqC5yBuAbuGMrzOJWUerll1/WwIEDFRsbq3PnzunXv/619u7dqxYtWujvf/+7p2MEAPgxcgYAoLr8PWdwcwwAcI1bRam2bdtq27Zteu+997R9+3YVFhZq3Lhxuu+++5wmJAQAgJwBAKguf88ZzF0LAK5xqyglSSEhIbr//vs9GQsAIECRMwAA1UXOAIDg4VZRauHChVUuHz16tFvBAAACj7/nDC7FAADz+HvOAAC4xmIYhuHqSk2bNnV6fv78eZ09e1ahoaEKDw/XqVOnPBZgbbj4C8aePXuUn5+viIgIl7fj7uRvAPxDMEzUWHZ5gbt/B6vD33NGmZq+VuQMBKNg+DsaTMgZ1UfOQLDi7z7KVPfvYB13Nv7jjz86PQoLC7V7927ddNNNfjEBodVqVXZ2trKysrwdCgAEPH/PGQAA85AzACC4uFWUqkjXrl01a9YsTZo0yVObBAAEKHIGAKC6yBkAELg8VpSSfp6U8MiRI57cJAAgQJEzAADVRc4AgMDk1kTnH3/8sdNzwzB09OhR/d///Z9uvPFGjwQGAAgM5AwAQHWRMwAguLhVlBo+fLjTc4vFopYtW2rAgAF6+eWXPREXACBAkDMAANXl7zmDO7YCgGvcKkqVlpZ6Og4A8Dnu3vmGu444I2cAAKrL33OG1WqV1Wp13HUKCDY1uXMk/w8dnDw6pxQAAAAAAABQHW6NlEpLS6t23zlz5rizCwBAgCBnAACqi5wBAMHFraLUli1btGXLFp0/f17du3eXJO3Zs0d169bV9ddf7+hnsVg8EyUAwG+RMwAA1UXOAIDg4lZRatiwYWrcuLHeeustNW3aVJL0448/asyYMerbt6+eeOIJjwbpaUxACADm8fecAQAwDzkDAIKLxTAMw9WVrrjiCn322We6+uqrndp37typW2+9VUeOHPFYgLWpbALC/Px8RUREuLx+TSZxAxC4/GmSxpr+HawOf88ZF/+QsWfPHnIGYBJ/+lsaLMgZ1cf3DMB1/N0PLNX9O+jWROcFBQU6fvx4ufbjx4/rzJkz7mwSABCg/D1nWK1WZWdnKysry9uhAEDA8/ecAQBwjVtFqREjRmjMmDFasmSJDh8+rMOHD+sf//iHxo0bp5EjR3o6RgCAHyNnAACqi5wBAMHFrTml5s2bp8mTJ+vXv/61zp8///OGQkI0btw4zZ4926MBAgD8GzkDAFBd5AwACC5uFaXCw8P1pz/9SbNnz9Z3330nSercubMaNmzo0eAAAP6PnAEAqC5yBgAEF7eKUmWOHj2qo0eP6pe//KUaNGggwzC4PSsAoELkDABAdZEzgOBTkwn+mSTdf7lVlDp58qTuvvturVq1ShaLRXv37lWnTp00btw4NW3aVC+//LKn4wQA+ClyBgB3uPvlhC8m/o2cAQDBxa2Jzh9//HHVq1dPBw8eVHh4uKN91KhRWr58uceCAwD4P3IGAKC6/D1n2Gw2xcbGKiEhwduhAIBfcGuk1GeffaZPP/1Ubdu2dWrv2rWrDhw44JHAAACBgZwBAKguf88ZVqtVVqtVBQUFioyM9HY4AODz3BopVVRU5PTLRZlTp04pLCysxkEBAAIHOQMAUF3kDAAILm4Vpfr27auFCxc6nlssFpWWlurFF19U//79PRZcbWFYLQCYx99zBgDAPOQMAAgubl2+9+KLL+rmm2/Wxo0bVVJSot/+9rf65ptvdOrUKX355ZeejtHjGFYLAObx95wBADAPOQMAgotbI6WuueYa7dmzRzfddJNuv/12FRUVaeTIkdqyZYs6d+7s6RgBAH6MnAEAqC5yBgAEF5dHSp0/f16DBg3SvHnz9Lvf/a42YgIABAhyBgCgusgZABB8XB4pVa9ePW3fvr02YgEABBhfyxmHDh1SUlKSYmNjde2112rx4sXeDgkA8P/5Ws4AANQ+ty7fu//++/XXv/7V07EAAAKQL+WMkJAQpaenKzs7W5999pkee+wxFRUVeTssAMD/50s5AwBQ+9ya6PzChQuaP3++VqxYoZ49e6phw4ZOy+fMmeOR4AAA/s+Xckbr1q3VunVrSVJ0dLRatGihU6dOlYsJAOAdvpQzAPiPmCnL3Fpv/6yhHo4ErnKpKPX9998rJiZGO3fu1PXXXy9J2rNnj1Mfi8XiuegAAH6rNnLG2rVrNXv2bG3atElHjx7Vhx9+qOHDhzv1sdlsmj17tnJzcxUXF6e5c+eqd+/e5ba1adMm2e12tWvXzrUDAwB4HN8zACA4uVSU6tq1q44ePapVq1ZJkkaNGqXXXntNUVFRtRIcAMB/1UbOKCoqUlxcnMaOHauRI0eWW75o0SKlpaVp3rx5SkxMVHp6ugYOHKjdu3erVatWjn6nTp3S6NGj9ec//7nSfRUXF6u4uNjxvKCgwO24AQBV43sGAAQnl+aUMgzD6fknn3zCXBwAgArVRs4YPHiwnnvuOY0YMaLC5XPmzNH48eM1ZswYxcbGat68eQoPD9f8+fMdfYqLizV8+HBNmTJFN9xwQ6X7mjlzpiIjIx0PRlQBQO3hewYABCe35pQqc2nyAACgMrWdM0pKSrRp0yZNnTrV0VanTh0lJydr3bp1jhhSU1M1YMAAPfDAA1Vub+rUqUpLS3M8LygooDAF+AF35xWRmFvEl/A9AwCCg0sjpSwWS7lrubm2GwBQEbNzxokTJ2S328td6hEVFaXc3FxJ0pdffqlFixZp6dKlio+PV3x8vHbs2FHh9sLCwhQREeH0AADUDr5nAEBwcmmkVNkvzGFhYZKkc+fO6aGHHip3V4wlS5Z4LkIAgF/yxZxx0003qbS01KV1bDabbDab7HZ7LUUFAPDFnAEAqH0uFaVSUlKcnt9///0eDcYsfMEAgNpnds5o0aKF6tatq7y8PKf2vLw8RUdHu71dq9Uqq9WqgoICRUZG1jRMAEAFAuV7BgDANS4VpRYsWFBbcZiKLxgAUPvMzhmhoaHq2bOnMjMzNXz4cElSaWmpMjMzNXHiRFNjAQC4xte+Zxw6dEgPPPCAjh07ppCQED3zzDO66667vB0WAAScGk10DgCAmQoLC7Vv3z7H85ycHG3dulXNmjVT+/btlZaWppSUFPXq1Uu9e/dWenq6ioqKNGbMGC9GDQDwNyEhIUpPT1d8fLxyc3PVs2dPDRkypNzlhACAmqEoBQDwGxs3blT//v0dz8vujpeSkqKMjAyNGjVKx48f17Rp05Sbm6v4+HgtX7683OTnruCSbwAIPq1bt1br1q0lSdHR0WrRooVOnTpFUQoAPMylu+8BAOBNSUlJMgyj3CMjI8PRZ+LEiTpw4ICKi4u1fv16JSYm1mifVqtV2dnZysrKqmH0AACzrF27VsOGDVObNm1ksVi0dOnScn1sNptiYmJUv359JSYmasOGDRVua9OmTbLb7WrXrl0tRw0AwYeiFAAAAICAUlRUpLi4ONlstgqXL1q0SGlpaZo+fbo2b96suLg4DRw4UMeOHXPqd+rUKY0ePVpvvvlmlfsrLi5WQUGB0wMAcHkUpQAAAAAElMGDB+u5557TiBEjKlw+Z84cjR8/XmPGjFFsbKzmzZun8PBwzZ8/39GnuLhYw4cP15QpU3TDDTdUub+ZM2cqMjLS8WBUFQBUD0UpAACqYLPZFBsbq4SEBG+HAgDwgJKSEm3atEnJycmOtjp16ig5OVnr1q2TJBmGodTUVA0YMEAPPPDAZbc5depU5efnOx6HDh2qtfgBIJBQlAIAoArMKQUAgeXEiROy2+3lboIRFRWl3NxcSdKXX36pRYsWaenSpYqPj1d8fLx27NhR6TbDwsIUERHh9AAAXB533wMAAACAi9x0000qLS31dhgAEPAYKQUAAAAgaLRo0UJ169ZVXl6eU3teXp6io6NrtG0u+QYA1zBSCgCAKthsNtlsNtntdm+HAqCWxUxZ5va6+2cN9WAkqE2hoaHq2bOnMjMzNXz4cElSaWmpMjMzNXHixBpt22q1ymq1qqCgQJGRkR6IFkBt4u++91GUAgCgCnzBAAD/U1hYqH379jme5+TkaOvWrWrWrJnat2+vtLQ0paSkqFevXurdu7fS09NVVFSkMWPGeDFqAAg+FKUAAAAABJSNGzeqf//+judpaWmSpJSUFGVkZGjUqFE6fvy4pk2bptzcXMXHx2v58uXlJj8HANQuilIAAAAAAkpSUpIMw6iyz8SJE2t8ud6luOQbAFzDROcAAAAA4AFWq1XZ2dnKysrydigA4BeCsijFXTEAANVFzgAAAABqR1BevsektQBqE3fxCCzkDAAAAKB2BOVIKQAAAADwNEbXAoBrKEoBAAAAgAcwpxQAuIaiFAAAAAAAAEwXlHNKAQAAAAAAuMvdeWSZQ9YZI6UAAAAAAABgOopSAABUgUlrAQDVRc4AANdQlAIAoApMWgsAqC5yBgC4hqIUAAAAAAAATEdRCgAAAAAAAKajKAUAAAAAAADTUZQCAAAAAACA6ShKAQAAAIAHcPc9AHANRSkAAAAA8ADuvgcArqEoBQBAFfjVGwAAAKgdFKUAAKgCv3oDAAAAtSPE2wEAAAAA/i5myjK31ts/a6iHIwEAwH8wUgoAAAAAAACmoygFAAAAAAAA01GUAgAAAAAP4OYYAOCaoCxKkSwAAAAAeBo3xwAA1wRlUYpkAQAAAAAA4F1BWZQCAAAAAACAd1GUAgAAAAAAgOkoSgEAAAAAAMB0Id4OAAAAX2az2WSz2WS3270dCoAAFDNlmdvr7p811IORAABgPkZKAQBQBW6OAQAAANQOilIAAAAAAAAwHUUpAAAAAPAAm82m2NhYJSQkeDsUAPALzCkFAAAAAB5gtVpltVpVUFCgyMhIb4cDwAcxl6AzRkoBAAAAAADAdBSlAAAAAAAAYDqKUgAAAAAAADAdRSkAAAAAAACYjqIUAAAAAAAATEdRCgAAAAAAAKajKAUAAAAAAADTUZQCAAAAAACA6ShKAQAAAAAAwHQUpQAAAAAAAGA6ilIAgKAyYsQINW3aVHfeeae3QwEAAACCGkUpAEBQmTRpkhYuXOjtMAAAAchmsyk2NlYJCQneDgUA/AJFKQBAUElKSlLjxo29HQYAIABZrVZlZ2crKyvL26EAgF+gKAUA8Btr167VsGHD1KZNG1ksFi1durRcH5vNppiYGNWvX1+JiYnasGGD+YECAAAAuCyKUgAAv1FUVKS4uDjZbLYKly9atEhpaWmaPn26Nm/erLi4OA0cOFDHjh1zeV/FxcUqKChwegAAAADwHIpSAAC/MXjwYD333HMaMWJEhcvnzJmj8ePHa8yYMYqNjdW8efMUHh6u+fPnu7yvmTNnKjIy0vFo165dTcMHAAAAcBGKUgCAgFBSUqJNmzYpOTnZ0VanTh0lJydr3bp1Lm9v6tSpys/PdzwOHTrkyXABAACAoBfi7QAAAPCEEydOyG63Kyoqyqk9KipKu3btcjxPTk7Wtm3bVFRUpLZt22rx4sXq06dPue2FhYUpLCys1uMGAAAAghVFKQBAUFmxYoVL/W02m2w2m+x2ey1FBAAAAFxezJRlbq+7f9ZQD0biOUF5+Z7NZlNsbKwSEhK8HQoAwENatGihunXrKi8vz6k9Ly9P0dHRbm+X23sDAAAAtSMoi1J8wQCAwBMaGqqePXsqMzPT0VZaWqrMzMwKL88DAAAA4F1cvgcA8BuFhYXat2+f43lOTo62bt2qZs2aqX379kpLS1NKSop69eql3r17Kz09XUVFRRozZozb++TyPQAAAKB2UJQCAPiNjRs3qn///o7naWlpkqSUlBRlZGRo1KhROn78uKZNm6bc3FzFx8dr+fLl5SY/d4XVapXValVBQYEiIyNrfAwAAAAAfkZRCgDgN5KSkmQYRpV9Jk6cqIkTJ5oUEQAAAAB3BeWcUgAAAAAAAPAuilIAAFSBO7YCAAAAtYOiFAAAVeCOrQAAAEDtoCgFAAAAAAAA01GUAgAAAIBLjBgxQk2bNtWdd97p7VAAIGBRlAIAAACAS0yaNEkLFy70dhgAENAoSgEAUAUmOgeA4JSUlKTGjRt7OwwACGgUpQAAqAITnQOA/1m7dq2GDRumNm3ayGKxaOnSpeX62Gw2xcTEqH79+kpMTNSGDRvMDxQAghxFKQAAAAABpaioSHFxcbLZbBUuX7RokdLS0jR9+nRt3rxZcXFxGjhwoI4dO+bW/oqLi1VQUOD0AABcHkUpAAAAAAFl8ODBeu655zRixIgKl8+ZM0fjx4/XmDFjFBsbq3nz5ik8PFzz5893a38zZ85UZGSk49GuXbuahA8AQYOiFAAAAICgUVJSok2bNik5OdnRVqdOHSUnJ2vdunVubXPq1KnKz893PA4dOuSpcAEgoIV4OwAAAHyZzWaTzWaT3W73digAAA84ceKE7Ha7oqKinNqjoqK0a9cux/Pk5GRt27ZNRUVFatu2rRYvXqw+ffpUuM2wsDCFhYXVatwAEIgoSgEAUAWr1Sqr1aqCggJFRkZ6OxwAgElWrFjh8jr8kAEAruHyPQAAAABBo0WLFqpbt67y8vKc2vPy8hQdHV2jbXPHVgBwDUUpAAAAAEEjNDRUPXv2VGZmpqOttLRUmZmZlV6eBwCoHVy+BwAAACCgFBYWat++fY7nOTk52rp1q5o1a6b27dsrLS1NKSkp6tWrl3r37q309HQVFRVpzJgxXowaAIIPRSkAAAAAAWXjxo3q37+/43laWpokKSUlRRkZGRo1apSOHz+uadOmKTc3V/Hx8Vq+fHm5yc9dxZxSAOAailIAAFSBLxgA4H+SkpJkGEaVfSZOnKiJEyd6dL/cHAMAXMOcUgAAVIFJawEAAIDaQVEKAAAAAAAApqMoBQAAAAAeYLPZFBsbq4SEBG+HAgB+gaIUAAAAAHgAl3wDgGsoSgEAAAAAAMB0FKUAAAAAAABgOopSAAAAAAAAMF2ItwMAAPxXzJRlbq+7f9ZQD0aCMjabTTabTXa73duhAIATcobvIWcA8FXu5ozazheMlAIAoApMWgsAqC5yBgC4hqIUAAAAAAAATEdRCgAAAAAAAKajKAUAAAAAAADTUZQCAAAAAACA6ShKAQAAAIAH2Gw2xcbGKiEhwduhAIBfoCgFAAAAAB7A3fcAwDUUpQAAAAAAAGA6ilIAAAAAAAAwHUUpAAAAAAAAmI6iFAAAAAAAAExHUQoAgCpwJyUAAACgdoR4OwBvMgxDklRQUODW+qXFZz0ZDgDUiDt/y8rWKft7iPKsVqusVqvy8/PVpEkTcgaAgEDOqB02m002m00XLlyQxPcMAP7P3b9j1c0ZFiOIs8rhw4fVrl07b4cBAF536NAhtW3b1tth+DRyBgD8jJxxeeQMAPjZ5XJGUBelSktLdeTIETVu3FgWi8WldQsKCtSuXTsdOnRIERERtRShOQLlWALlOKTAOZZAOQ4pcI7l0uMwDENnzpxRmzZtVKcOV3RXJVhzhj/HLvl3/P4cu+Tf8RN7xcgZ1ResOaMMx+A7AuE4OAbf4cpxVDdnBPXle3Xq1KnxrzwRERF+/aa6WKAcS6AchxQ4xxIoxyEFzrFcfByRkZFejsY/BHvO8OfYJf+O359jl/w7fmIvj5xRPcGeM8pwDL4jEI6DY/Ad1T2O6uQMfuIAAAAAAACA6ShKAQAAAAAAwHQUpdwUFham6dOnKywszNuh1FigHEugHIcUOMcSKMchBc6xBMpx+Bt/ft39OXbJv+P359gl/46f2OFNgXAOOQbfEQjHwTH4jto4jqCe6BwAAAAAAADewUgpAAAAAAAAmI6iFAAAAAAAAExHUQoAAAAAAACmoygFAAAAAAAA01GUqoLNZlNMTIzq16+vxMREbdiwocr+ixcv1pVXXqn69eurR48e+ve//21SpJWbOXOmEhIS1LhxY7Vq1UrDhw/X7t27q1wnIyNDFovF6VG/fn2TIq7Ys88+Wy6mK6+8ssp1fPF8SFJMTEy5Y7FYLLJarRX295XzsXbtWg0bNkxt2rSRxWLR0qVLnZYbhqFp06apdevWatCggZKTk7V3797LbtfVz5knVHUs58+f11NPPaUePXqoYcOGatOmjUaPHq0jR45UuU133qO1eRySlJqaWi6mQYMGXXa73jgngcxfXs/a+oyboTq57ty5c7JarWrevLkaNWqkO+64Q3l5eV6K+L9ef/11XXvttYqIiFBERIT69OmjTz75xLHcV+OuyKxZs2SxWPTYY4852nw5/sv93fbl2Mv88MMPuv/++9W8eXM1aNBAPXr00MaNGx3Lfflzi4r5S84o48+5o4w/55AygZRLyvhbTikTCLlFMje/UJSqxKJFi5SWlqbp06dr8+bNiouL08CBA3Xs2LEK+3/11Ve69957NW7cOG3ZskXDhw/X8OHDtXPnTpMjd7ZmzRpZrVZ9/fXX+vzzz3X+/HndeuutKioqqnK9iIgIHT161PE4cOCASRFX7uqrr3aK6T//+U+lfX31fEhSVlaW03F8/vnnkqS77rqr0nV84XwUFRUpLi5ONputwuUvvviiXnvtNc2bN0/r169Xw4YNNXDgQJ07d67Sbbr6OfOUqo7l7Nmz2rx5s5555hlt3rxZS5Ys0e7du/WrX/3qstt15T3qCZc7J5I0aNAgp5j+/ve/V7lNb52TQOVPr2dtfMbNUp1c9/jjj+uf//ynFi9erDVr1ujIkSMaOXKkF6P+Wdu2bTVr1ixt2rRJGzdu1IABA3T77bfrm2++keS7cV8qKytLb7zxhq699lqndl+Pv6q/274e+48//qgbb7xR9erV0yeffKLs7Gy9/PLLatq0qaOPL39uUZ4/5Ywy/pw7yvhzDikTKLmkjL/mlDL+nFskL+QXAxXq3bu3YbVaHc/tdrvRpk0bY+bMmRX2v/vuu42hQ4c6tSUmJhoPPvhgrcbpqmPHjhmSjDVr1lTaZ8GCBUZkZKR5QVXD9OnTjbi4uGr395fzYRiGMWnSJKNz585GaWlphct98XxIMj788EPH89LSUiM6OtqYPXu2o+306dNGWFiY8fe//73S7bj6OasNlx5LRTZs2GBIMg4cOFBpH1ffo55W0XGkpKQYt99+u0vb8YVzEkj89fX01GfcWy7NdadPnzbq1atnLF682NHn22+/NSQZ69at81aYlWratKnxl7/8xW/iPnPmjNG1a1fj888/N/r162dMmjTJMAzff92r+rvt67EbhmE89dRTxk033VTpcn/73MJ/c0YZf88dZfw9h5Txt1xSxl9zShl/zy2GYX5+YaRUBUpKSrRp0yYlJyc72urUqaPk5GStW7euwnXWrVvn1F+SBg4cWGl/b8nPz5ckNWvWrMp+hYWF6tChg9q1a+dUZfemvXv3qk2bNurUqZPuu+8+HTx4sNK+/nI+SkpK9M4772js2LGyWCyV9vPF83GxnJwc5ebmOr3mkZGRSkxMrPQ1d+dz5i35+fmyWCxq0qRJlf1ceY+aZfXq1WrVqpW6d++uCRMm6OTJk5X29adz4g8C6fV05zPuTZfmuk2bNun8+fNO8V955ZVq3769T8Vvt9v13nvvqaioSH369PGbuK1Wq4YOHVou7/pD/JX93faH2D/++GP16tVLd911l1q1aqXrrrtOf/7znx3L/e1zG+wCKWeU8df3oL/mkDL+mkvK+HNOKePPuUUyP79QlKrAiRMnZLfbFRUV5dQeFRWl3NzcCtfJzc11qb83lJaW6rHHHtONN96oa665ptJ+3bt31/z58/XRRx/pnXfeUWlpqW644QYdPnzYxGidJSYmKiMjQ8uXL9frr7+unJwc9e3bV2fOnKmwvz+cD0launSpTp8+rdTU1Er7+OL5uFTZ6+rKa+7O58wbzp07p6eeekr33nuvIiIiKu3n6nvUDIMGDdLChQuVmZmpF154QWvWrNHgwYNlt9sr7O8v58RfBNLr6c5n3FsqynW5ubkKDQ0tV1j2lfh37NihRo0aKSwsTA899JA+/PBDxcbG+nzckvTee+9p8+bNmjlzZrllvh5/VX+3fT12Sfr+++/1+uuvq2vXrvr00081YcIEPfroo3rrrbck+dfnFoGVM8r443vQH3NIGX/OJWX8OaeU8ffcIpmfX0JqHjL8hdVq1c6dOy87z02fPn3Up08fx/MbbrhBV111ld544w394Q9/qO0wKzR48GDHv6+99lolJiaqQ4cOev/99zVu3DivxOQJf/3rXzV48GC1adOm0j6+eD6Cxfnz53X33XfLMAy9/vrrVfb1xffoPffc4/h3jx49dO2116pz585avXq1br75Zq/EBNS26uY6X9K9e3dt3bpV+fn5+uCDD5SSkqI1a9Z4O6zLOnTokCZNmqTPP//c6zdEcUdVf7cbNGjgxciqp7S0VL169dLzzz8vSbruuuu0c+dOzZs3TykpKV6ODvBP/phDyvhrLinj7zmljL/nFsn8/MJIqQq0aNFCdevWLTcLfl5enqKjoytcJzo62qX+Zps4caL+9a9/adWqVWrbtq1L69arV0/XXXed9u3bV0vRua5Jkybq1q1bpTH5+vmQpAMHDmjFihX6n//5H5fW88XzUfa6uvKau/M5M1NZQerAgQP6/PPPqxwlVZHLvUe9oVOnTmrRokWlMfn6OfE3gfR6uvMZ94bKcl10dLRKSkp0+vRpp/6+En9oaKi6dOminj17aubMmYqLi9Orr77q83Fv2rRJx44d0/XXX6+QkBCFhIRozZo1eu211xQSEqKoqCifjv9SF//d9vXXXpJat26t2NhYp7arrrrKcZmIv3xu8bNAyhll/O096K85pIy/5pIygZZTyvhbbpHMzy8UpSoQGhqqnj17KjMz09FWWlqqzMxMpxErF+vTp49Tf0n6/PPPK+1vFsMwNHHiRH344YdauXKlOnbs6PI27Ha7duzYodatW9dChO4pLCzUd999V2lMvno+LrZgwQK1atVKQ4cOdWk9XzwfHTt2VHR0tNNrXlBQoPXr11f6mrvzOTNLWUFq7969WrFihZo3b+7yNi73HvWGw4cP6+TJk5XG5MvnxB8F0uvpzmfcTJfLdT179lS9evWc4t+9e7cOHjzoE/FfqrS0VMXFxT4f980336wdO3Zo69atjkevXr103333Of7ty/Ff6uK/277+2kvSjTfeWO629Xv27FGHDh0k+f7nFs4CKWeU8Zf3YKDlkDL+kkvKBFpOKeNvuUXyQn5xeWr0IPHee+8ZYWFhRkZGhpGdnW385je/MZo0aWLk5uYahmEYDzzwgDFlyhRH/y+//NIICQkxXnrpJePbb781pk+fbtSrV8/YsWOHtw7BMAzDmDBhghEZGWmsXr3aOHr0qONx9uxZR59Lj2XGjBnGp59+anz33XfGpk2bjHvuuceoX7++8c0333jjEAzDMIwnnnjCWL16tZGTk2N8+eWXRnJystGiRQvj2LFjhmH4z/koY7fbjfbt2xtPPfVUuWW+ej7OnDljbNmyxdiyZYshyZgzZ46xZcsWxx3pZs2aZTRp0sT46KOPjO3btxu333670bFjR+Onn35ybGPAgAHG3LlzHc8v9znzxrGUlJQYv/rVr4y2bdsaW7dudfrcFBcXV3osl3uPmn0cZ86cMSZPnmysW7fOyMnJMVasWGFcf/31RteuXY1z585VehzeOieByp9eT098xr2lOrnuoYceMtq3b2+sXLnS2Lhxo9GnTx+jT58+Xoz6Z1OmTDHWrFlj5OTkGNu3bzemTJliWCwW47PPPjMMw3fjrszFd0oyDN+O/3J/t305dsP4+c6wISEhxh//+Edj7969xt/+9jcjPDzceOeddxx9fPlzi/L8KWeU8efcUcafc0iZQMslZfwpp5Tx99xiGObnF4pSVZg7d67Rvn17IzQ01Ojdu7fx9ddfO5b169fPSElJcer//vvvG926dTNCQ0ONq6++2li2bJnJEZcnqcLHggULHH0uPZbHHnvMcdxRUVHGkCFDjM2bN5sf/EVGjRpltG7d2ggNDTWuuOIKY9SoUca+ffscy/3lfJT59NNPDUnG7t27yy3z1fOxatWqCt9LZbGWlpYazzzzjBEVFWWEhYUZN998c7nj69ChgzF9+nSntqo+Z944lpycnEo/N6tWrar0WC73HjX7OM6ePWvceuutRsuWLY169eoZHTp0MMaPH1/uf2x95ZwEMn95PT3xGfeW6uS6n376yXj44YeNpk2bGuHh4caIESOMo0ePei/o/2/s2LFGhw4djNDQUKNly5bGzTff7PgSYRi+G3dlLv0C4cvxX+7vti/HXuaf//yncc011xhhYWHGlVdeabz55ptOy335c4uK+UvOKOPPuaOMP+eQMoGWS8r4U04pEwi5xTDMzS8WwzAM18dXAQAAAAAAAO5jTikAAAAAAACYjqIUAAAAAAAATEdRCgAAAAAAAKajKAUAAAAAAADTUZQCAAAAAACA6ShKAQAAAAAAwHQUpQAAAAAAAGA6ilIAAAAAAAAwHUUpAAAAAAAAmI6iFGCy1NRUWSwWWSwW1atXT1FRUbrllls0f/58lZaWejs8AIAPOH78uCZMmKD27dsrLCxM0dHRGjhwoL788ktvhwYA8LKkpCQ99thj3g4D8IgQbwcABKNBgwZpwYIFstvtysvL0/LlyzVp0iR98MEH+vjjjxUSwkcTAILZHXfcoZKSEr311lvq1KmT8vLylJmZqZMnT3o7NAAAAI9hpBTgBWW/el9xxRW6/vrr9fTTT+ujjz7SJ598ooyMDG+HBwDwotOnT+uLL77QCy+8oP79+6tDhw7q3bu3pk6dql/96ldavXq1QkND9cUXXzjWefHFF9WqVSvl5eV5MXIAQG1LTU3VmjVr9Oqrrzquvvjuu+80btw4dezYUQ0aNFD37t316quvOq1X0eiq4cOHKzU11bzggQpQlAJ8xIABAxQXF6clS5Z4OxQAgBc1atRIjRo10tKlS1VcXFxuedkXiwceeED5+fnasmWLnnnmGf3lL39RVFSUFyIGAJjl1VdfVZ8+fTR+/HgdPXpUR48eVdu2bdW2bVstXrxY2dnZmjZtmp5++mm9//773g4XuCyKUoAPufLKK7V//35vhwEA8KKQkBBlZGTorbfeUpMmTXTjjTfq6aef1vbt2x19nnvuOTVt2lS/+c1vdP/99yslJUW/+tWvvBg1AMAMkZGRCg0NVXh4uKKjoxUdHa2wsDDNmDFDvXr1UseOHXXfffdpzJgxFKXgFyhKAT7EMAxZLBZvhwEA8LI77rhDR44c0ccff6xBgwZp9erVuv766x2XeIeGhupvf/ub/vGPf+jcuXN65ZVXvBswAMCrbDabevbsqZYtW6pRo0Z68803dfDgQW+HBVwWRSnAh3z77bfq2LGjt8MAAPiA+vXr65ZbbtEzzzyjr776SqmpqZo+fbpj+VdffSVJOnXqlE6dOuWtMAEAXvbee+9p8uTJGjdunD777DNt3bpVY8aMUUlJiaNPnTp1ZBiG03rnz583O1SgHIpSgI9YuXKlduzYoTvuuMPboQAAfFBsbKyKiookSd99950ef/xx/fnPf1ZiYqJSUlJUWlrq5QgBAGYIDQ2V3W53PP/yyy91ww036OGHH9Z1112nLl266LvvvnNap2XLljp69Kjjud1u186dO02LGagMRSnAC4qLi5Wbm6sffvhBmzdv1vPPP6/bb79dt912m0aPHu3t8AAAXnTy5EkNGDBA77zzjrZv366cnBwtXrxYL774om6//XbZ7Xbdf//9GjhwoMaMGaMFCxZo+/btevnll70dOgDABDExMVq/fr3279+vEydOqGvXrtq4caM+/fRT7dmzR88884yysrKc1hkwYICWLVumZcuWadeuXZowYYJOnz7tnQMALhLi7QCAYLR8+XK1bt1aISEhatq0qeLi4vTaa68pJSVFdepQKwaAYNaoUSMlJibqlVde0Xfffafz58+rXbt2Gj9+vJ5++mn98Y9/1IEDB/Svf/1LktS6dWu9+eabuvfee3XrrbcqLi7Oy0cAAKhNkydPVkpKimJjY/XTTz9p165d2rJli0aNGiWLxaJ7771XDz/8sD755BPHOmPHjtW2bds0evRohYSE6PHHH1f//v29eBTAzyzGpReWAgAAAAAAALWMIRkAAAAAAAAwHUUpAAAAAAAAmI6iFAAAAAAAAExHUQoAAAAAAACmoygFAAAAAAAA01GUAgAAAAAAgOkoSgEAAAAAAMB0FKUAAAAAAABgOopSAAAAAAAAMB1FKQAAAAAAAJiOohQAAAAAAABM9/8AAIPfw+DbzCsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1200x400 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plotting the histograms of the input data before normalization\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(x_train[:, 0].cpu().numpy(), bins=20)\n",
        "plt.xlabel(\"D\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.yscale(\"log\")\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.hist(x_train[:, 1].cpu().numpy(), bins=20)\n",
        "plt.xlabel(\"Sx\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.yscale(\"log\")\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(x_train[:, 2].cpu().numpy(), bins=20)\n",
        "plt.xlabel(\"tau\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.yscale(\"log\")\n",
        "plt.suptitle(\"Input data before normalization\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZqNHwos_MP-"
      },
      "source": [
        "Perform z-score normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSninCqK_MP_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Computing the mean and standard deviation of each input variable from the training set\n",
        "# D_mean = torch.mean(x_train[:, 0])\n",
        "# D_std = torch.std(x_train[:, 0])\n",
        "# Sx_mean = torch.mean(x_train[:, 1])\n",
        "# Sx_std = torch.std(x_train[:, 1])\n",
        "# tau_mean = torch.mean(x_train[:, 2])\n",
        "# tau_std = torch.std(x_train[:, 2])\n",
        "\n",
        "# # Applying z-score normalization to both train and test sets using the statistics from the training set\n",
        "# x_train[:, 0] = torch.sub(x_train[:, 0], D_mean).div(D_std)\n",
        "# x_train[:, 1] = torch.sub(x_train[:, 1], Sx_mean).div(Sx_std)\n",
        "# x_train[:, 2] = torch.sub(x_train[:, 2], tau_mean).div(tau_std)\n",
        "# x_test[:, 0] = torch.sub(x_test[:, 0], D_mean).div(D_std)\n",
        "# x_test[:, 1] = torch.sub(x_test[:, 1], Sx_mean).div(Sx_std)\n",
        "# x_test[:, 2] = torch.sub(x_test[:, 2], tau_mean).div(tau_std)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "yxoJWxUz_MP_",
        "outputId": "40adccee-186d-4ebf-812b-8cd3d54dbcd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(5.6435, device='cuda:0')\n",
            "tensor(3.3454, device='cuda:0')\n",
            "tensor(6.8819, device='cuda:0')\n",
            "tensor(8.1340, device='cuda:0')\n",
            "tensor(8.0449, device='cuda:0')\n",
            "tensor(7.8764, device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1200x400 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "# Verifying that the means and the stds of the input data are close to 0 and 1 respectively.\n",
        "print(torch.mean(x_train[:, 0]))\n",
        "print(torch.std(x_train[:, 0]))\n",
        "print(torch.mean(x_train[:, 1]))\n",
        "print(torch.std(x_train[:, 1]))\n",
        "print(torch.mean(x_train[:, 2]))\n",
        "print(torch.std(x_train[:, 2]))\n",
        "\n",
        "# Plotting the histograms of the input data after normalization\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(x_train[:, 0].cpu().numpy(), bins=20)\n",
        "plt.xlabel(\"D\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.yscale(\"log\")\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.hist(x_train[:, 1].cpu().numpy(), bins=20)\n",
        "plt.xlabel(\"Sx\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.yscale(\"log\")\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(x_train[:, 2].cpu().numpy(), bins=20)\n",
        "plt.xlabel(\"tau\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.yscale(\"log\")\n",
        "plt.suptitle(\"Input data after normalization\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkwBQWKY_MP_"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "zp847o04_MQA",
        "outputId": "794b5bb6-a132-4d63-bce3-c83e2df94cdb"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAGGCAYAAADmRxfNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAvKElEQVR4nO3dfXRU9Z3H8c9ASHgwCYVIHspDWGuw4SEoJFmoVpBUDBw00GIUhRAQbTtpaSNbw/YIutoFRXOidtZ0PUKKnpWHXcWeTcVihKKIEMKDD1kRaAgoeQARQoIGnLn7B4eRmAfIvTeZmcz7dc6c473zu3e+Nzfxez787r3jMAzDEAAAAABY0M3XBQAAAAAIfAQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgWYivC/A1j8ejY8eOKTw8XA6Hw9flAIBPGIahM2fOKC4uTt268W9OEv0BAKT29YegDxbHjh3ToEGDfF0GAPiFo0ePauDAgb4uwy/QHwDgW1fSH4I+WISHh0u68MOKiIjwcTUA4Bt1dXUaNGiQ9/+JoD8AgNS+/hC0wcLlcsnlcsntdkuSIiIiaBwAgh6X/Hzr4s+C/gAAV9YfgvZCWqfTqfLycpWWlvq6FAAAACDgBW2wAAAAAGAfggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAXMLlcikxMVHJycm+LgUAAorDMAzD10X4Ul1dnSIjI3X69GlFRES0a9v4vGLTn3t4+VTT2wKA3az8v7CrsvozMdsj6A8A/El7/l/IjAUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAs6zLB4uzZsxoyZIgWLVrk61IAAACAoNNlgsUf/vAH/fM//7OvywAAAACCUpcIFgcOHNAnn3yi9PR0X5cCAAAABCWfB4utW7dq2rRpiouLk8Ph0IYNG5qNcblcio+PV8+ePZWamqqdO3c2eX/RokVatmxZJ1UMAAAA4LtCfF1AQ0ODkpKSNG/ePM2YMaPZ+2vXrlVubq4KCwuVmpqqgoICTZ48Wfv379eAAQP0+uuvKyEhQQkJCXrvvfd8cATmxOcVm9728PKpNlYCAPAn9AcAgcrnwSI9Pb3NS5jy8/O1YMECZWdnS5IKCwtVXFyslStXKi8vT++//77WrFmj9evXq76+XufPn1dERISWLFnS4v4aGxvV2NjoXa6rq7P3gAAAAIAg5PNLodpy7tw5lZWVKS0tzbuuW7duSktL0/bt2yVJy5Yt09GjR3X48GE99dRTWrBgQauh4uL4yMhI72vQoEEdfhwAAABAV+fXweLEiRNyu92Kjo5usj46OlrV1dWm9rl48WKdPn3a+zp69KgdpQIAAABBzeeXQtlp7ty5lx0TFhamsLCwji8GAAAACCJ+PWMRFRWl7t27q6ampsn6mpoaxcTE+KgqAAAAAN/l18EiNDRUY8aMUUlJiXedx+NRSUmJxo0b58PKAAAAAFzK55dC1dfX6+DBg97liooK7d27V/369dPgwYOVm5urrKwsjR07VikpKSooKFBDQ4P3KVEAAAAAfM/nwWLXrl2aOHGidzk3N1eSlJWVpaKiImVmZur48eNasmSJqqurNXr0aG3cuLHZDd3t5XK55HK55Ha7Le0HAAAAgB8EiwkTJsgwjDbH5OTkKCcnx9bPdTqdcjqdqqurU2RkpK37BgD43qlTp5SWlqZvvvlG33zzjRYuXKgFCxb4uiwA6LJ8HiwAAOgI4eHh2rp1q3r37q2GhgaNGDFCM2bMUP/+/X1dGgB0SX598zYAAGZ1795dvXv3liQ1NjbKMIzLzpADAMxjxgIA4Je2bt2qFStWqKysTFVVVXrttdeUkZHRZIzL5dKKFStUXV2tpKQkPffcc0pJSfG+f+rUKd188806cOCAVqxYoaioqE4+is4Vn1dsetvDy6faWAmAYESwCEBmGwdNA0AgaWhoUFJSkubNm6cZM2Y0e3/t2rXKzc1VYWGhUlNTVVBQoMmTJ2v//v0aMGCAJKlv377at2+fampqNGPGDP3sZz+z/PAPAEDLuBQKAOCX0tPT9fjjj2v69Oktvp+fn68FCxYoOztbiYmJKiwsVO/evbVy5cpmY6Ojo5WUlKR33nmn1c9rbGxUXV1dkxcA4MoRLAAAAefcuXMqKytTWlqad123bt2Ulpam7du3S5Jqamp05swZSdLp06e1detWDRs2rNV9Llu2TJGRkd7XoEGDOvYgAKCLCdpg4XK5lJiYqOTkZF+XAgBopxMnTsjtdje7rCk6OlrV1dWSpMrKSt10001KSkrSTTfdpF/96lcaOXJkq/tcvHixTp8+7X0dPXq0Q48BALqaoL3Hgu+xAICuLSUlRXv37r3i8WFhYQoLC+u4ggCgiwvaGQsAQOCKiopS9+7dVVNT02R9TU2NYmJifFQVAAQ3ggUAIOCEhoZqzJgxKikp8a7zeDwqKSnRuHHjfFgZAASvoL0UCgDg3+rr63Xw4EHvckVFhfbu3at+/fpp8ODBys3NVVZWlsaOHauUlBQVFBSooaFB2dnZPqwaAIIXwQIA4Jd27dqliRMnepdzc3MlSVlZWSoqKlJmZqaOHz+uJUuWqLq6WqNHj9bGjRstf0+Fy+WSy+WS2+22tB8ACDYECwCAX5owYYIMw2hzTE5OjnJycmz9XB7uAQDmcI8FAAAAAMuCdsYiGKe64/OKTW97ePlUGysBAABAVxO0MxZOp1Pl5eUqLS31dSkAAABAwAvaYAEAAADAPkF7KRQAAPiW2ctluVQWwEXMWAAAAACwjGABAMAlXC6XEhMTlZyc7OtSACCgECwAALgED/cAAHMIFgAAAAAs4+ZtAABgGt+RBOCioJ2x4BpaAAAAwD5BGyy4hhYAAACwT9AGCwAAAAD24R4LXBGuoQUAAEBbmLEAAOAS3IMHAOYQLAAAuAT34AGAOQQLAAAAAJYRLAAAAABYxs3bAADAJ3gwCNC1ECzQ4cw2DpoGAABA4OBSKAAAAACWBW2w4HGCAAAAgH2CNljwOEEAAADAPkEbLAAAaAkz2gBgDjdvAwBwCafTKafTqbq6OkVGRvq6HLSCB4MA/ocZCwAAAACWMWMBv8XzzQEAAAIHMxYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAy4I2WLhcLiUmJio5OdnXpQAA/Aj9AQDMCdpg4XQ6VV5ertLSUl+XAgDwI/QHADAnxNcFAB0hPq/Y9LaHl0+1sRIAgD+hPwAdJ2hnLAAAAADYh2ABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMh43C3yH2UcR8hhCAOjaeFQt0DZmLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAC7hcrmUmJio5ORkX5cCAAGFm7cBALiE0+mU0+lUXV2dIiMjfV0Oughu/EYwYMYCAAAAgGUECwAAAACWBW2w4BpaAAAAwD5Be48F19DCblw/CwAAglnQzlgAAAAAsA/BAgAAAIBlBAsAAAAAlhEsAAAAAFgWtDdvAwAABAKzDwfhwSDobMxYAAAAALCMYAEAAADAMi6FAvwA34EBAAACHTMWAAAAACwjWAAAAACwjGABAAAAwDJTweIf//iH3XUAALoIegQABCdTweIHP/iBJk6cqJdffllff/213TUBAAJYoPcIl8ulxMREJScn+7oUAAgopoLF7t27NWrUKOXm5iomJkYPPPCAdu7caXdtAIAAFOg9wul0qry8XKWlpb4uBQACiqlgMXr0aD3zzDM6duyYVq5cqaqqKt14440aMWKE8vPzdfz4cbvrBAAECHoEAAQnSzdvh4SEaMaMGVq/fr2eeOIJHTx4UIsWLdKgQYM0Z84cVVVV2VUnACDA0CMAILhYCha7du3SL3/5S8XGxio/P1+LFi3SoUOHtGnTJh07dkx33HGHXXUCAAIMPQIAgoupb97Oz8/XqlWrtH//fk2ZMkWrV6/WlClT1K3bhZwydOhQFRUVKT4+3s5aAQABgB4BAMHJVLB4/vnnNW/ePM2dO1exsbEtjhkwYIBefPFFS8UBuLz4vGJT2x1ePtXmSoAL6BEAEJxMBYsDBw5cdkxoaKiysrLM7B4AEMDoEQAQnEwFi1WrVumqq67SzJkzm6xfv369zp49S7MAgCBGjwD8g9kZbYlZbZhj6ubtZcuWKSoqqtn6AQMG6N///d8tFwUACFz0CAAITqaCxZEjRzR06NBm64cMGaIjR45YLgoAELjoEQAQnEwFiwEDBuiDDz5otn7fvn3q37+/5aI6g8vlUmJiopKTk31dCgB0KV2hRwAA2s9UsLj77rv161//Wps3b5bb7Zbb7dbbb7+thQsX6q677rK7xg7hdDpVXl6u0tJSX5cCAF1KV+gRAID2M3Xz9mOPPabDhw9r0qRJCgm5sAuPx6M5c+Zw/SwABDl6BAAEJ1PBIjQ0VGvXrtVjjz2mffv2qVevXho5cqSGDBlid30AgABDjwCA4GQqWFyUkJCghIQEu2oBAHQh9AgACC6mgoXb7VZRUZFKSkpUW1srj8fT5P23337bluIAAIGHHgEAwclUsFi4cKGKioo0depUjRgxQg6Hw+66AAABih4BBD6+XA9mmAoWa9as0bp16zRlyhS76wHQSWga6Cj0CAAITqYeNxsaGqof/OAHdtcCAOgC6BEAEJxMBYsHH3xQzzzzjAzDsLseAECAo0cAQHAydSnUu+++q82bN+uNN97Q8OHD1aNHjybvv/rqq7YUBwAIPPQIAAhOpoJF3759NX36dLtrAQB0AfQIAAhOpoLFqlWr7K4DANBF0CMAIDiZ/oK8b775Rlu2bNGhQ4c0a9YshYeH69ixY4qIiNBVV11lZ40A/AxPlMLl0COA4GW2R9AfAp+pYFFZWanbbrtNR44cUWNjo37yk58oPDxcTzzxhBobG1VYWGh3nQCAABHoPcLlcsnlcsntdvu6FAAIKKaeCrVw4UKNHTtWX375pXr16uVdP336dJWUlNhWHAAg8AR6j3A6nSovL1dpaamvSwGAgGJqxuKdd97Re++9p9DQ0Cbr4+Pj9fnnn9tSGAAgMNEjACA4mZqx8Hg8LU4Rf/bZZwoPD7dcFAAgcNEjACA4mQoWt956qwoKCrzLDodD9fX1Wrp0qaZMmWJXbQCAAESPAIDgZOpSqKefflqTJ09WYmKivv76a82aNUsHDhxQVFSUXnnlFbtrBAAEEHoEAAQnU8Fi4MCB2rdvn9asWaMPPvhA9fX1mj9/vu65554mN+oBAIIPPQIAgpPp77EICQnRvffea2ctAIAugh4BAMHHVLBYvXp1m+/PmTPHVDEAgMBHjwCA4GQqWCxcuLDJ8vnz53X27FmFhoaqd+/eNA0ACGL0CAAITqaeCvXll182edXX12v//v268cYbuTEPAIIcPQIAgpPpeyy+69prr9Xy5ct177336pNPPrFrtwC6mPi8YtPbHl4+1cZK0JnoEQDQ9ZmasWhNSEiIjh07ZucuAQBdBD0CALo2UzMWf/nLX5osG4ahqqoq/fGPf9SPfvQjWwoDAAQmegQABCdTwSIjI6PJssPh0NVXX61bbrlFTz/9tB11AQACFD0CAIKTqWDh8XjsrgMA0EXQIwAgONl6jwUAAACA4GRqxiI3N/eKx+bn55v5CABAgKJHAEBwMhUs9uzZoz179uj8+fMaNmyYJOnTTz9V9+7ddcMNN3jHORwOe6oEAAQMegQABCdTwWLatGkKDw/Xn//8Z33ve9+TdOELkbKzs3XTTTfpwQcftLVIAEDgoEcAMIPvOQp8pu6xePrpp7Vs2TJvw5Ck733ve3r88cd54gcABDl6BAAEJ1PBoq6uTsePH2+2/vjx4zpz5ozlogAAgYseAQDByVSwmD59urKzs/Xqq6/qs88+02effab/+Z//0fz58zVjxgy7a2zTqVOnNHbsWI0ePVojRozQCy+80KmfDwBoyp96BACg85i6x6KwsFCLFi3SrFmzdP78+Qs7CgnR/PnztWLFClsLvJzw8HBt3bpVvXv3VkNDg0aMGKEZM2aof//+nVoHAOACf+oRAIDOYypY9O7dW//xH/+hFStW6NChQ5Kka665Rn369LG1uCvRvXt39e7dW5LU2NgowzBkGEan1wEAuMCfegQAoPOYChYXVVVVqaqqSj/+8Y/Vq1cvGYbR7scHbt26VStWrFBZWZmqqqr02muvKSMjo8kYl8ulFStWqLq6WklJSXruueeUkpLiff/UqVO6+eabdeDAAa1YsUJRUVFWDgsAYAM7egQAXAmeKOUfTN1j8cUXX2jSpElKSEjQlClTVFVVJUmaP39+ux8j2NDQoKSkJLlcrhbfX7t2rXJzc7V06VLt3r1bSUlJmjx5smpra71j+vbtq3379qmiokL/9V//pZqaGjOHBQCwgZ09AgAQOEwFi9/+9rfq0aOHjhw54r0MSZIyMzO1cePGdu0rPT1djz/+uKZPn97i+/n5+VqwYIGys7OVmJiowsJC9e7dWytXrmw2Njo6WklJSXrnnXfad0AAANvY2SMAAIHD1KVQf/vb3/Tmm29q4MCBTdZfe+21qqystKUwSTp37pzKysq0ePFi77pu3bopLS1N27dvlyTV1NSod+/eCg8P1+nTp7V161b94he/aHWfjY2Namxs9C7X1dXZVi8AoPN6BADAv5gKFg0NDU3+FeqikydPKiwszHJRF504cUJut1vR0dFN1kdHR+uTTz6RJFVWVur+++/33rT9q1/9SiNHjmx1n8uWLdOjjz5qW40AOo/Za2i5frZzdVaPAAD4F1OXQt10001avXq1d9nhcMjj8ejJJ5/UxIkTbSvuSqSkpGjv3r3at2+fPvjgAz3wwANtjl+8eLFOnz7tfR09erSTKgWA4OBPPQIA0HlMzVg8+eSTmjRpknbt2qVz587pd7/7nT7++GOdPHlS27Zts624qKgode/evdnN2DU1NYqJiTG1z7CwMP7FDAA6UGf1CACAfzE1YzFixAh9+umnuvHGG3XHHXeooaFBM2bM0J49e3TNNdfYVlxoaKjGjBmjkpIS7zqPx6OSkhKNGzfOts8BANins3oEAMC/tHvG4vz587rttttUWFio3//+95YLqK+v18GDB73LFRUV2rt3r/r166fBgwcrNzdXWVlZGjt2rFJSUlRQUKCGhgZlZ2db/mwAgL3s7hEAgMDR7mDRo0cPffDBB7YVsGvXribX3Obm5kqSsrKyVFRUpMzMTB0/flxLlixRdXW1Ro8erY0bNza7oRsA4Ht29wgAQOAwdSnUvffeqxdffNGWAiZMmOB9otOlr6KiIu+YnJwcVVZWqrGxUTt27FBqaqrlz3W5XEpMTFRycrLlfQEAvmVnjwAABA5TN29/8803Wrlypd566y2NGTNGffr0afJ+fn6+LcV1JKfTKafTqbq6OkVGRvq6HADoMrpCjwAAtF+7gsU//vEPxcfH66OPPtINN9wgSfr000+bjHE4HPZVBwAIGP7WI44eParZs2ertrZWISEhevjhhzVz5sxO+3wAgYHvSLJPu4LFtddeq6qqKm3evFmSlJmZqWeffZb7HQAAftcjQkJCVFBQoNGjR6u6ulpjxozRlClTms2gAADs0a5gYRhGk+U33nhDDQ0NthYEAAhM/tYjYmNjFRsbK0mKiYlRVFSUTp48SbAAgA5i6ubti77bRAAAuMhqj9i6daumTZumuLg4ORwObdiwodkYl8ul+Ph49ezZU6mpqdq5c2eL+yorK5Pb7dagQYMs1QQAaF27ZiwcDkez62O5pwKAvzN7/azENbTtYXePaGhoUFJSkubNm6cZM2Y0e3/t2rXKzc1VYWGhUlNTVVBQoMmTJ2v//v0aMGCAd9zJkyc1Z84cvfDCC6ZrAQBcXrsvhZo7d67CwsIkSV9//bV+/vOfN5tWfvXVV+2rsIO4XC65XC653W5flwIAXYLdPSI9PV3p6emtvp+fn68FCxZ4vzC1sLBQxcXFWrlypfLy8iRJjY2NysjIUF5ensaPH9/m5zU2NqqxsdG7XFdXd0V1AgAuaFewyMrKarJ877332lpMZ+JxswBgr87sEefOnVNZWZkWL17sXdetWzelpaVp+/btkr4NOrfccotmz5592X0uW7ZMjz76aIfVDABdXbuCxapVqzqqDgBAgOvMHnHixAm53e5mT5yKjo7WJ598Iknatm2b1q5dq1GjRnnvz3jppZc0cuTIFve5ePFi5ebmepfr6uq4JwMA2sHUF+QBAODvbrzxRnk8niseHxYW5r2MCwDQfpaeCgUAgC9ERUWpe/fuqqmpabK+pqZGMTExPqoKAIIbwQIAEHBCQ0M1ZswYlZSUeNd5PB6VlJRo3LhxPqwMAIIXl0IBAPxSfX29Dh486F2uqKjQ3r171a9fPw0ePFi5ubnKysrS2LFjlZKSooKCAjU0NHifEgUA6FwECwCAX9q1a5cmTpzoXb54Y3VWVpaKioqUmZmp48ePa8mSJaqurtbo0aO1cePGZjd0txePIwcAc4I2WNA4AMC/TZgw4bLf3p2Tk6OcnBxbP5fHkQOAOUF7j4XT6VR5eblKS0t9XQoAAAAQ8II2WAAAAACwD8ECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAgEu4XC4lJiYqOTnZ16UAQEAhWAAAcAkeRw4A5hAsAAAAAFgWtMGCqW4AAADAPkEbLJjqBgAAAOwTtMECAAAAgH0IFgAAAAAsI1gAAAAAsCzE1wUAgD+Lzys2ve3h5VNtrASdxeVyyeVyye12+7oUAAgozFgAAHAJHu4BAOYwYwEAAAC0EzPazTFjAQAAAMAyggUAAAAAywgWAAAAACwL2mDhcrmUmJio5ORkX5cCAAAABLygDRY89QMAAACwT9AGCwAAAAD2IVgAAHAJLpUFAHMIFgAAXIJLZQHAHIIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLgjZYuFwuJSYmKjk52delAAAAAAEvaIOF0+lUeXm5SktLfV0KAMCP8A9PAGBO0AYLAABawj88AYA5BAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUhvi4AAAAACCbxecWmtz28fKqNldiLGQsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACW8VQoAOggZp/64c9P/AAAoDXMWAAAcAmXy6XExEQlJyf7uhQACCgECwAALuF0OlVeXq7S0lJflwIAAYVgAQAAAMCyoA0WTHUDAAAA9gnaYMFUNwAAAGCfoA0WAAAAAOxDsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJYRLAAAAABYRrAAAAAAYBnBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAA4BIul0uJiYlKTk72dSkAEFAIFgAAXMLpdKq8vFylpaW+LgUAAgrBAgAAAIBlBAsAAAAAlhEsAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGBZ0AYLvlkVAAAAsE/QBgu+WRUAAACwT9AGCwAAAAD2IVgAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAHAJl8ulxMREJScn+7oUAAgoBAsAAC7hdDpVXl6u0tJSX5cCAAGFYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwDKCBQAAAADLCBYAAAAALCNYAAAAALAsxNcFAACais8rNr3t4eVTbawEAOBvzPaIzugPzFgAAAAAsIxgAQAAAMAyggUAAAAAywgWAAAAACwjWAAAAACwjGABAAAAwLKADxZHjx7VhAkTlJiYqFGjRmn9+vW+LgkAAAAIOgH/PRYhISEqKCjQ6NGjVV1drTFjxmjKlCnq06ePr0sDAAAAgkbAB4vY2FjFxsZKkmJiYhQVFaWTJ08SLAAAAIBO5PNLobZu3app06YpLi5ODodDGzZsaDbG5XIpPj5ePXv2VGpqqnbu3NnivsrKyuR2uzVo0KAOrhoAAADApXweLBoaGpSUlCSXy9Xi+2vXrlVubq6WLl2q3bt3KykpSZMnT1ZtbW2TcSdPntScOXP0n//5n51RNgAAAIBL+PxSqPT0dKWnp7f6fn5+vhYsWKDs7GxJUmFhoYqLi7Vy5Url5eVJkhobG5WRkaG8vDyNHz++zc9rbGxUY2Ojd7murs6GowAAAACCm89nLNpy7tw5lZWVKS0tzbuuW7duSktL0/bt2yVJhmFo7ty5uuWWWzR79uzL7nPZsmWKjIz0vrhsCgAAALDO5zMWbTlx4oTcbreio6ObrI+OjtYnn3wiSdq2bZvWrl2rUaNGee/PeOmllzRy5MgW97l48WLl5uZ6l0+fPq3BgwebmrnwNJ5t9zYA0JHMzsJe3M4wDDvLCWgXfxZmf6b0CAD+pDP6g18Hiytx4403yuPxXPH4sLAwhYWFeZcv/rCYuQDQFUQWWNv+zJkzioyMtKWWQHfmzBlJ9AcAXUNn9Ae/DhZRUVHq3r27ampqmqyvqalRTEyMLZ8RFxeno0ePKjw8XA6H44q3q6ur06BBg3T06FFFRETYUos/6KrHJXXdY+O4Aou/HpdhGDpz5ozi4uJ8XYrfMNsfJP89z1ZxXIGnqx4bx9V52tMf/DpYhIaGasyYMSopKVFGRoYkyePxqKSkRDk5ObZ8Rrdu3TRw4EDT20dERPjNibdTVz0uqeseG8cVWPzxuJipaMpqf5D88zzbgeMKPF312DiuznGl/cHnwaK+vl4HDx70LldUVGjv3r3q16+fBg8erNzcXGVlZWns2LFKSUlRQUGBGhoavE+JAgAAAOB7Pg8Wu3bt0sSJE73LF2+szsrKUlFRkTIzM3X8+HEtWbJE1dXVGj16tDZu3Njshm4AAAAAvuPzYDFhwoTL3mWek5Nj26VPdgkLC9PSpUub3AjeFXTV45K67rFxXIGlqx4Xmuqq55njCjxd9dg4Lv/kMHi2IAAAAACL/PoL8gAAAAAEBoIFAAAAAMsIFgAAAAAsI1i0weVyKT4+Xj179lRqaqp27tzZ5vj169fruuuuU8+ePTVy5Ej99a9/7aRKr9yyZcuUnJys8PBwDRgwQBkZGdq/f3+b2xQVFcnhcDR59ezZs5MqvjKPPPJIsxqvu+66NrcJhPMVHx/f7LgcDoecTmeL4/31XG3dulXTpk1TXFycHA6HNmzY0OR9wzC0ZMkSxcbGqlevXkpLS9OBAwcuu9/2/o3ara3jOn/+vB566CGNHDlSffr0UVxcnObMmaNjx461uU8zv8vwja7WI+gP3/L3c3URPaJt9IjOR7Boxdq1a5Wbm6ulS5dq9+7dSkpK0uTJk1VbW9vi+Pfee09333235s+frz179igjI0MZGRn66KOPOrnytv3973+X0+nU+++/r02bNun8+fO69dZb1dDQ0OZ2ERERqqqq8r4qKys7qeIrN3z48CY1vvvuu62ODZTzVVpa2uSYNm3aJEmaOXNmq9v447lqaGhQUlKSXC5Xi+8/+eSTevbZZ1VYWKgdO3aoT58+mjx5sr7++utW99nev9GO0NZxnT17Vrt379bDDz+s3bt369VXX9X+/ft1++23X3a/7fldhm90xR5Bf7ggEM7VRfQIeoTf9QgDLUpJSTGcTqd32e12G3FxccayZctaHH/nnXcaU6dObbIuNTXVeOCBBzq0Tqtqa2sNScbf//73VsesWrXKiIyM7LyiTFi6dKmRlJR0xeMD9XwtXLjQuOaaawyPx9Pi+4FwriQZr732mnfZ4/EYMTExxooVK7zrTp06ZYSFhRmvvPJKq/tp799oR/vucbVk586dhiSjsrKy1THt/V2GbwRDj6A/fMvfz9VF9Ihv0SN8gxmLFpw7d05lZWVKS0vzruvWrZvS0tK0ffv2FrfZvn17k/GSNHny5FbH+4vTp09Lkvr169fmuPr6eg0ZMkSDBg3SHXfcoY8//rgzymuXAwcOKC4uTv/0T/+ke+65R0eOHGl1bCCer3Pnzunll1/WvHnz5HA4Wh0XCOfqUhUVFaqurm5yPiIjI5Wamtrq+TDzN+oPTp8+LYfDob59+7Y5rj2/y+h8wdIj6A/f8vdzJdEjLkWP8B2CRQtOnDght9vd7Nu9o6OjVV1d3eI21dXV7RrvDzwej37zm9/oRz/6kUaMGNHquGHDhmnlypV6/fXX9fLLL8vj8Wj8+PH67LPPOrHatqWmpqqoqEgbN27U888/r4qKCt100006c+ZMi+MD8Xxt2LBBp06d0ty5c1sdEwjn6rsu/szbcz7M/I362tdff62HHnpId999tyIiIlod197fZXS+YOgR9IfAOVcX0SO+RY/wHZ9/8zZ8x+l06qOPPrrstXnjxo3TuHHjvMvjx4/XD3/4Q/3pT3/SY4891tFlXpH09HTvf48aNUqpqakaMmSI1q1bp/nz5/uwMvu8+OKLSk9PV1xcXKtjAuFcBaPz58/rzjvvlGEYev7559scGwy/y/B/9IfAQ48IXF2pRzBj0YKoqCh1795dNTU1TdbX1NQoJiamxW1iYmLaNd7XcnJy9L//+7/avHmzBg4c2K5te/Tooeuvv14HDx7soOqs69u3rxISElqtMdDOV2Vlpd566y3dd9997douEM7VxZ95e86Hmb9RX7nYMCorK7Vp06Y2/yWqJZf7XUbn6+o9gv4QOOfqInpEU/QI3yFYtCA0NFRjxoxRSUmJd53H41FJSUmTpH+pcePGNRkvSZs2bWp1vK8YhqGcnBy99tprevvttzV06NB278PtduvDDz9UbGxsB1Roj/r6eh06dKjVGgPlfF20atUqDRgwQFOnTm3XdoFwroYOHaqYmJgm56Ourk47duxo9XyY+Rv1hYsN48CBA3rrrbfUv3//du/jcr/L6HxdtUfQHy4IhHP1XfSIpugRPuTbe8f915o1a4ywsDCjqKjIKC8vN+6//36jb9++RnV1tWEYhjF79mwjLy/PO37btm1GSEiI8dRTTxn/93//ZyxdutTo0aOH8eGHH/rqEFr0i1/8woiMjDS2bNliVFVVeV9nz571jvnusT366KPGm2++aRw6dMgoKysz7rrrLqNnz57Gxx9/7ItDaNGDDz5obNmyxaioqDC2bdtmpKWlGVFRUUZtba1hGIF7vgzjwpMsBg8ebDz00EPN3guUc3XmzBljz549xp49ewxJRn5+vrFnzx7vky+WL19u9O3b13j99deNDz74wLjjjjuMoUOHGl999ZV3H7fccovx3HPPeZcv9zfq6+M6d+6ccfvttxsDBw409u7d2+TvrbGxsdXjutzvMvxDV+wR9IcLAuFcXYoecQE9wj96BMGiDc8995wxePBgIzQ01EhJSTHef/9973s333yzkZWV1WT8unXrjISEBCM0NNQYPny4UVxc3MkVX56kFl+rVq3yjvnusf3mN7/x/hyio6ONKVOmGLt37+784tuQmZlpxMbGGqGhocb3v/99IzMz0zh48KD3/UA9X4ZhGG+++aYhydi/f3+z9wLlXG3evLnF37uLtXs8HuPhhx82oqOjjbCwMGPSpEnNjnfIkCHG0qVLm6xr62+0M7R1XBUVFa3+vW3evLnV47rc7zL8R1frEfSHb/n7uboUPeICeoR/cBiGYdg9CwIAAAAguHCPBQAAAADLCBYAAAAALCNYAAAAALCMYAEAAADAMoIFAAAAAMsIFgAAAAAsI1gAAAAAsIxgAQAAAMAyggUQgObOnauMjAxflwEAAOBFsAA6wSOPPKLRo0fbtr9nnnlGRUVFtu0PAOB7dvcKSSoqKlLfvn1t3SfQmhBfFwDgW+fPn1ePHj0uOy4yMrITqgEAALhyzFgAV2j16tXq37+/Ghsbm6zPyMjQ7NmzW92uqKhIjz76qPbt2yeHwyGHw+GdbXA4HHr++ed1++23q0+fPvrDH/4gt9ut+fPna+jQoerVq5eGDRumZ555psk+v3sp1IQJE/TrX/9av/vd79SvXz/FxMTokUcesevQAQBXqCN6xalTp3Tffffp6quvVkREhG655Rbt27fPu+2+ffs0ceJEhYeHKyIiQmPGjNGuXbu0ZcsWZWdn6/Tp09590hvQkQgWwBWaOXOm3G63/vKXv3jX1dbWqri4WPPmzWt1u8zMTD344IMaPny4qqqqVFVVpczMTO/7jzzyiKZPn64PP/xQ8+bNk8fj0cCBA7V+/XqVl5dryZIl+td//VetW7euzfr+/Oc/q0+fPtqxY4eefPJJ/du//Zs2bdpk/cABAFesI3rFzJkzVVtbqzfeeENlZWW64YYbNGnSJJ08eVKSdM8992jgwIEqLS1VWVmZ8vLy1KNHD40fP14FBQWKiIjw7nPRokUd+wNAUONSKOAK9erVS7NmzdKqVas0c+ZMSdLLL7+swYMHa8KECW1ud9VVVykkJEQxMTHN3p81a5ays7ObrHv00Ue9/z106FBt375d69at05133tnq54waNUpLly6VJF177bX64x//qJKSEv3kJz9pz2ECACywu1e8++672rlzp2praxUWFiZJeuqpp7Rhwwb993//t+6//34dOXJE//Iv/6LrrrtO0oUecFFkZKQcDkeL/QewG8ECaIcFCxYoOTlZn3/+ub7//e+rqKhIc+fOlcPhML3PsWPHNlvncrm0cuVKHTlyRF999ZXOnTt32Rv6Ro0a1WQ5NjZWtbW1pusCAJhjZ6/Yt2+f6uvr1b9//ybrv/rqKx06dEiSlJubq/vuu08vvfSS0tLSNHPmTF1zzTW2HAvQHgQLoB2uv/56JSUlafXq1br11lv18ccfq7i42NI++/Tp02R5zZo1WrRokZ5++mmNGzdO4eHhWrFihXbs2NHmfr5707fD4ZDH47FUGwCg/ezsFfX19YqNjdWWLVuavXfxaU+PPPKIZs2apeLiYr3xxhtaunSp1qxZo+nTp1s4CqD9CBZAO913330qKCjQ559/rrS0NA0aNOiy24SGhsrtdl/R/rdt26bx48frl7/8pXfdxX+VAgAEBrt6xQ033KDq6mqFhIQoPj6+1W0TEhKUkJCg3/72t7r77ru1atUqTZ8+vV39B7CKm7eBdpo1a5Y+++wzvfDCC23eiHep+Ph4VVRUaO/evTpx4kSzp4Vc6tprr9WuXbv05ptv6tNPP9XDDz+s0tJSu8oHAHQCu3pFWlqaxo0bp4yMDP3tb3/T4cOH9d577+n3v/+9du3apa+++ko5OTnasmWLKisrtW3bNpWWluqHP/yhd5/19fUqKSnRiRMndPbs2Y48bAQ5ggXQTpGRkfrpT3+qq6666oq//fqnP/2pbrvtNk2cOFFXX321XnnllVbHPvDAA5oxY4YyMzOVmpqqL774osnsBQDA/9nVKxwOh/7617/qxz/+sbKzs5WQkKC77rpLlZWVio6OVvfu3fXFF19ozpw5SkhI0J133qn09HTvQ0DGjx+vn//858rMzNTVV1+tJ598sgOPGsHOYRiG4esigEAzadIkDR8+XM8++6yvSwEA+Cl6BYINwQJohy+//FJbtmzRz372M5WXl2vYsGG+LgkA4GfoFQhW3LwNtMP111+vL7/8Uk888USTRjF8+HBVVla2uM2f/vQn3XPPPZ1VIgDAx+gVCFbMWAA2qKys1Pnz51t8Lzo6WuHh4Z1cEQDA39Ar0NURLAAAAABYxlOhAAAAAFhGsAAAAABgGcECAAAAgGUECwAAAACWESwAAAAAWEawAAAAAGAZwQIAAACAZQQLAAAAAJb9PwmIgyye1xCPAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 800x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Checking if our output is always positive by plotting a histogram of y_train and y_test tensors \n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(y_train.cpu().numpy(), bins=20) # must be cpu here.\n",
        "plt.xlabel(\"y_train\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.yscale(\"log\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(y_test.cpu().numpy(), bins=20) # must be cpu here\n",
        "plt.xlabel(\"y_test\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.yscale(\"log\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Fslp2S0_MQA"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roVkgAVS_MQA"
      },
      "source": [
        "## Defining the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0dgTKR0_MQB"
      },
      "outputs": [],
      "source": [
        "# Defining a class for the network\n",
        "class Net(nn.Module):\n",
        "    \"\"\"A class for creating a network with a\n",
        "    variable number of hidden layers and units.\n",
        "\n",
        "    Attributes:\n",
        "        n_layers (int): The number of hidden layers in the network.\n",
        "        n_units (list): A list of integers representing the number of units in each hidden layer.\n",
        "        hidden_activation (torch.nn.Module): The activation function for the hidden layers.\n",
        "        output_activation (torch.nn.Module): The activation function for the output layer.\n",
        "        layers (torch.nn.ModuleList): A list of linear layers in the network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_layers, n_units, hidden_activation, output_activation):\n",
        "        \"\"\"Initializes the network with the given hyperparameters.\n",
        "\n",
        "        Args:\n",
        "            n_layers (int): The number of hidden layers in the network.\n",
        "            n_units (list): A list of integers representing the number of units in each hidden layer.\n",
        "            hidden_activation (torch.nn.Module): The activation function for the hidden layers.\n",
        "            output_activation (torch.nn.Module): The activation function for the output layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.n_units = n_units\n",
        "        self.hidden_activation = hidden_activation\n",
        "        self.output_activation = output_activation\n",
        "\n",
        "        # Creating a list of linear layers with different numbers of units for each layer\n",
        "        self.layers = nn.ModuleList([nn.Linear(3, n_units[0])])\n",
        "        for i in range(1, n_layers):\n",
        "            self.layers.append(nn.Linear(n_units[i - 1], n_units[i]))\n",
        "        self.layers.append(nn.Linear(n_units[-1], 1))\n",
        "\n",
        "        # Adding some assertions to check that the input arguments are valid\n",
        "        assert isinstance(n_layers, int) and n_layers > 0, \"n_layers must be a positive integer\"\n",
        "        assert isinstance(n_units, list) and len(n_units) == n_layers, \"n_units must be a list of length n_layers\"\n",
        "        assert all(isinstance(n, int) and n > 0 for n in n_units), \"n_units must contain positive integers\"\n",
        "        assert isinstance(hidden_activation, nn.Module), \"hidden_activation must be a torch.nn.Module\"\n",
        "        assert isinstance(output_activation, nn.Module), \"output_activation must be a torch.nn.Module\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Performs a forward pass on the input tensor.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input tensor of shape (batch_size, 3).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The output tensor of shape (batch_size, 1).\n",
        "        \"\"\"\n",
        "        # Looping over the hidden layers and applying the linear transformation and the activation function\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = self.hidden_activation(layer(x))\n",
        "        # Applying the linear transformation and the activation function on the output layer\n",
        "        x = self.output_activation(self.layers[-1](x))\n",
        "\n",
        "        # Returning the output tensor\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVO7KNgK_MQB"
      },
      "source": [
        "## Defining the model and search space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sInSI1G_MQB"
      },
      "outputs": [],
      "source": [
        "# Defining a function to create a trial network and optimizer\n",
        "def create_model(trial, optimize):\n",
        "    \"\"\"Creates a trial network and optimizer based on the sampled hyperparameters.\n",
        "\n",
        "    Args:\n",
        "        trial (optuna.trial.Trial): The trial object that contains the hyperparameters.\n",
        "        optimize (boolean): Whether to optimize the hyperparameters or to use predefined values.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple of (net, loss_fn, optimizer, batch_size, n_epochs,\n",
        "            scheduler, loss_name, optimizer_name, scheduler_name,\n",
        "            n_units, n_layers, hidden_activation, output_activation),\n",
        "            where net is the trial network,\n",
        "            loss_fn is the loss function,\n",
        "            optimizer is the optimizer,\n",
        "            batch_size is the batch size,\n",
        "            n_epochs is the number of epochs,\n",
        "            scheduler is the learning rate scheduler,\n",
        "            loss_name is the name of the loss function,\n",
        "            optimizer_name is the name of the optimizer,\n",
        "            scheduler_name is the name of the scheduler,\n",
        "            n_units is a list of integers representing\n",
        "            the number of units in each hidden layer,\n",
        "            n_layers is an integer representing the number of hidden layers in the network,\n",
        "            hidden_activation is a torch.nn.Module representing the activation function for the hidden layers,\n",
        "            output_activation is a torch.nn.Module representing the activation function for the output layer,\n",
        "            lr is the (initial) learning rate.\n",
        "    \"\"\"\n",
        "    # If optimize is True, sample the hyperparameters from the search space\n",
        "    if optimize:\n",
        "        # Sampling the hyperparameters from the search space\n",
        "        n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
        "        n_units = [trial.suggest_int(f\"n_units_{i}\", 16, 1048) for i in range(n_layers)] \n",
        "        hidden_activation_name = trial.suggest_categorical(\n",
        "            \"hidden_activation\", [\"ReLU\", \"LeakyReLU\", \"ELU\", \"Tanh\", \"Sigmoid\"]\n",
        "        )\n",
        "        output_activation_name = trial.suggest_categorical(\n",
        "            #\"output_activation\", [\"Linear\", \"ReLU\", \"Softplus\"]\n",
        "            # Assuming pressure cannot be negative, linear output activation is not an option.\n",
        "            \"output_activation\", [\"ReLU\", \"Softplus\"]\n",
        "        ) \n",
        "        loss_name = trial.suggest_categorical(\n",
        "            \"loss\", [\"MSE\", \"MAE\", \"Huber\", \"LogCosh\"] \n",
        "        )\n",
        "        optimizer_name = trial.suggest_categorical(\n",
        "            \"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\", \"Adagrad\"] \n",
        "        )\n",
        "        lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2) \n",
        "        batch_size = trial.suggest_int(\"batch_size\", 16, 256)\n",
        "        n_epochs = trial.suggest_int(\"n_epochs\", 10, 200) \n",
        "        scheduler_name = trial.suggest_categorical(\n",
        "            \"scheduler\",\n",
        "            [\"None\", \"CosineAnnealingLR\", \"ReduceLROnPlateau\", \"StepLR\", \"ExponentialLR\",\"OneCycleLR\"],\n",
        "        )\n",
        "\n",
        "    # If optimize is False, use the predefined values\n",
        "    else:\n",
        "        # Setting the hyperparameters to the predefined values\n",
        "        n_layers = N_LAYERS_NO_OPT\n",
        "        n_units = N_UNITS_NO_OPT\n",
        "        hidden_activation_name = HIDDEN_ACTIVATION_NAME_NO_OPT\n",
        "        output_activation_name = OUTPUT_ACTIVATION_NAME_NO_OPT\n",
        "        loss_name = LOSS_NAME_NO_OPT\n",
        "        optimizer_name = OPTIMIZER_NAME_NO_OPT\n",
        "        lr = LR_NO_OPT\n",
        "        batch_size = BATCH_SIZE_NO_OPT\n",
        "        n_epochs = N_EPOCHS_NO_OPT\n",
        "        scheduler_name = SCHEDULER_NAME_NO_OPT\n",
        "\n",
        "\n",
        "    # Creating the activation functions from their names\n",
        "    if hidden_activation_name == \"ReLU\":\n",
        "        hidden_activation = nn.ReLU()\n",
        "    elif hidden_activation_name == \"LeakyReLU\":\n",
        "        hidden_activation = nn.LeakyReLU() \n",
        "    elif hidden_activation_name == \"ELU\":\n",
        "        hidden_activation = nn.ELU() \n",
        "    elif hidden_activation_name == \"Tanh\":\n",
        "        hidden_activation = nn.Tanh()\n",
        "    else:\n",
        "        hidden_activation = nn.Sigmoid()\n",
        "\n",
        "    if output_activation_name == \"ReLU\":\n",
        "        output_activation = nn.ReLU()\n",
        "    elif output_activation_name == \"Softplus\":\n",
        "        output_activation = nn.Softplus()\n",
        "    else:\n",
        "        output_activation = nn.Identity()\n",
        "\n",
        "    # Creating the loss function from its name\n",
        "    if loss_name == \"MSE\":\n",
        "        loss_fn = nn.MSELoss()\n",
        "    elif loss_name == \"MAE\":\n",
        "        loss_fn = nn.L1Loss()\n",
        "    elif loss_name == \"Huber\":\n",
        "        loss_fn = nn.SmoothL1Loss() \n",
        "    else:\n",
        "        # Creating the log-cosh loss function\n",
        "        def log_cosh_loss(y_pred, y_true):\n",
        "            return torch.mean(torch.log(torch.cosh(y_pred - y_true)))\n",
        "            \n",
        "        loss_fn = log_cosh_loss\n",
        "\n",
        "    # Creating the network with the sampled hyperparameters\n",
        "    net = Net(\n",
        "        n_layers, n_units, hidden_activation, output_activation\n",
        "    ).to(device)\n",
        "\n",
        "    # Creating the optimizer from its name\n",
        "    if optimizer_name == \"SGD\":\n",
        "        # Added sampling the weight decay and momentum for SGD\n",
        "        weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
        "        momentum = trial.suggest_uniform(\"momentum\", 0.0, 0.99)\n",
        "        optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
        "    elif optimizer_name == \"Adam\":\n",
        "        # Added sampling the weight decay and beta parameters for Adam\n",
        "        weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
        "        beta1 = trial.suggest_uniform(\"beta1\", 0.5, 0.99)\n",
        "        beta2 = trial.suggest_uniform(\"beta2\", 0.9, 0.999)\n",
        "        optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay, betas=(beta1, beta2))\n",
        "    elif optimizer_name == \"RMSprop\":\n",
        "        optimizer = optim.RMSprop(net.parameters(), lr=lr)\n",
        "    else:\n",
        "        # Added creating the Adagrad optimizer\n",
        "        optimizer = optim.Adagrad(net.parameters(), lr=lr)\n",
        "\n",
        "    # Creating the learning rate scheduler from its name\n",
        "    if scheduler_name == \"StepLR\":\n",
        "        # Added sampling the step_size and gamma for StepLR\n",
        "        step_size = trial.suggest_int(\"step_size\", 5, 15)\n",
        "        gamma = trial.suggest_uniform(\"gamma\", 0.05, 0.5)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "    elif scheduler_name == \"ExponentialLR\":\n",
        "        # Added sampling the gamma for ExponentialLR\n",
        "        gamma = trial.suggest_uniform(\"gamma\", 0.8, 0.99)\n",
        "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
        "    elif scheduler_name == \"CosineAnnealingLR\":\n",
        "        # Added sampling the T_max for CosineAnnealingLR\n",
        "        T_max = trial.suggest_int(\"T_max\", 5, 15)\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max)\n",
        "    elif scheduler_name == \"ReduceLROnPlateau\":\n",
        "        # Added sampling the factor, patience and threshold for ReduceLROnPlateau\n",
        "        factor = trial.suggest_uniform(\"factor\", 0.1, 0.9)\n",
        "        patience = trial.suggest_int(\"patience\", 5, 15)\n",
        "        threshold = trial.suggest_loguniform(\"threshold\", 1e-4, 1e-2)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode=\"min\", factor=factor, patience=patience, threshold=threshold\n",
        "        )\n",
        "    # # Added using OneCycleLR scheduler as an option\n",
        "    # elif scheduler_name == \"OneCycleLR\":\n",
        "    #         # Added sampling the max_lr and pct_start for OneCycleLR\n",
        "    #         max_lr = trial.suggest_loguniform(\"max_lr\", lr, 10 * lr) \n",
        "    #         pct_start = trial.suggest_uniform(\"pct_start\", 0.1, 0.9)\n",
        "    #         scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "    #             optimizer,\n",
        "    #             max_lr=max_lr,\n",
        "    #             epochs=n_epochs,\n",
        "    #             steps_per_epoch=len(train_loader),\n",
        "    #             pct_start=pct_start,\n",
        "    #         )\n",
        "    else:\n",
        "        scheduler = None\n",
        "\n",
        "    # Returning all variables needed for saving and loading\n",
        "    return net, loss_fn, optimizer, batch_size, n_epochs, scheduler, loss_name, optimizer_name, scheduler_name, n_units, n_layers, hidden_activation, output_activation, lr\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5idaz2iN_MQC"
      },
      "source": [
        " ## The training and evaluation loop\n",
        "\n",
        " We first define a couple of functions used in the training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0giq3C5k_MQC"
      },
      "outputs": [],
      "source": [
        "# Defining a function that computes loss and metrics for a given batch\n",
        "def compute_loss_and_metrics(y_pred, y_true, loss_fn):\n",
        "    \"\"\"Computes loss and metrics for a given batch.\n",
        "\n",
        "    Args:\n",
        "        y_pred (torch.Tensor): The predicted pressure tensor of shape (batch_size, 1).\n",
        "        y_true (torch.Tensor): The true pressure tensor of shape (batch_size,).\n",
        "        loss_fn (torch.nn.Module or function): The loss function to use.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple of (loss, l1_norm), where loss is a scalar tensor,\n",
        "            l1_norm is L1 norm for relative error of pressure,\n",
        "            each being a scalar tensor.\n",
        "            linf_norm is Linf norm for relative error of pressure.\n",
        "    \"\"\"\n",
        "    # Reshaping the target tensor to match the input tensor\n",
        "    y_true = y_true.view(-1, 1)\n",
        "\n",
        "    # Computing the loss using the loss function\n",
        "    loss = loss_fn(y_pred, y_true)\n",
        "\n",
        "    # Computing the relative error of pressure\n",
        "    rel_error = torch.abs((y_pred - y_true) / y_true)\n",
        "\n",
        "    # Computing the L1 norm for the relative error of pressure\n",
        "    l1_norm = torch.mean(rel_error) \n",
        "    # Computing the Linf norm for the relative error of pressure\n",
        "    linf_norm = torch.max(rel_error) \n",
        "\n",
        "    # Returning the loss and metrics\n",
        "    return loss, l1_norm, linf_norm\n",
        "\n",
        "\n",
        "# Defining a function that updates the learning rate scheduler with validation loss if applicable\n",
        "def update_scheduler(scheduler, test_loss):\n",
        "    \"\"\"Updates the learning rate scheduler with validation loss if applicable.\n",
        "\n",
        "    Args:\n",
        "        scheduler (torch.optim.lr_scheduler._LRScheduler or None): The learning rate scheduler to use.\n",
        "        test_loss (float): The validation loss to use.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Checking if scheduler is not None\n",
        "    if scheduler is not None:\n",
        "        # Checking if scheduler is ReduceLROnPlateau\n",
        "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
        "            # Updating the scheduler with test_loss\n",
        "            scheduler.step(test_loss)\n",
        "        else:\n",
        "            # Updating the scheduler without test_loss\n",
        "            scheduler.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1UU7F8l_MQD"
      },
      "source": [
        "Now for the actual training and evaluation loop,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K40MijxO_MQD"
      },
      "outputs": [],
      "source": [
        "# Defining a function to train and evaluate a network\n",
        "def train_and_eval(net, loss_fn, optimizer, batch_size, n_epochs, scheduler, trial=None):\n",
        "    \"\"\"Trains and evaluates a network.\n",
        "\n",
        "    Args:\n",
        "        net (torch.nn.Module): The network to train and evaluate.\n",
        "        loss_fn (torch.nn.Module or function): The loss function.\n",
        "        optimizer (torch.optim.Optimizer): The optimizer.\n",
        "        batch_size (int): The batch size.\n",
        "        n_epochs (int): The number of epochs.\n",
        "        scheduler (torch.optim.lr_scheduler._LRScheduler or None): The learning rate scheduler.\n",
        "    Returns:\n",
        "        tuple: A tuple of (train_losses, test_losses, train_metrics, test_metrics), where\n",
        "            train_losses is a list of training losses for each epoch,\n",
        "            test_losses is a list of validation losses for each epoch,\n",
        "            train_metrics is a list of dictionaries containing training metrics for each epoch,\n",
        "            test_metrics is a list of dictionaries containing validation metrics for each epoch.\n",
        "    \"\"\"\n",
        "    # Creating data loaders for train and test sets\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.TensorDataset(x_test, y_test), batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Initializing lists to store the losses and metrics for each epoch\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    train_metrics = []\n",
        "    test_metrics = []\n",
        "\n",
        "    # Creating a SummaryWriter object to log data for tensorboard\n",
        "    writer = tbx.SummaryWriter()\n",
        "\n",
        "    # Looping over the epochs\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # Setting the network to training mode\n",
        "        net.train()\n",
        "\n",
        "        # Initializing variables to store the total loss and metrics for the train set\n",
        "        train_loss = 0.0\n",
        "        train_l1_norm = 0.0\n",
        "        train_linf_norm = 0.0\n",
        "\n",
        "        # Looping over the batches in the train set\n",
        "        for x_batch, y_batch in train_loader:\n",
        "\n",
        "            # Moving the batch tensors to the device\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            # Zeroing the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Performing a forward pass and computing the loss and metrics\n",
        "            y_pred = net(x_batch)\n",
        "            loss, l1_norm, linf_norm = compute_loss_and_metrics(\n",
        "                y_pred, y_batch, loss_fn\n",
        "            )\n",
        "\n",
        "\n",
        "            # Performing a backward pass and updating the weights\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Updating the total loss and metrics for the train set\n",
        "            train_loss += loss.item() * x_batch.size(0)\n",
        "            train_l1_norm += l1_norm.item() * x_batch.size(0)\n",
        "            train_linf_norm += linf_norm.item() * x_batch.size(0)\n",
        "\n",
        "        # Computing the average loss and metrics for the train set\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_l1_norm /= len(train_loader.dataset)\n",
        "        train_linf_norm /= len(train_loader.dataset)\n",
        "\n",
        "        # Appending the average loss and metrics for the train set to the lists\n",
        "        train_losses.append(train_loss)\n",
        "        train_metrics.append(\n",
        "            {\n",
        "                \"l1_norm\": train_l1_norm,\n",
        "                \"linf_norm\": train_linf_norm,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Logging the average loss and metrics for the train set to tensorboard\n",
        "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
        "        writer.add_scalar(\"L1 norm/train\", train_l1_norm, epoch)\n",
        "        writer.add_scalar(\"Linf norm/train\", train_linf_norm, epoch)\n",
        "\n",
        "        # Setting the network to evaluation mode\n",
        "        net.eval()\n",
        "\n",
        "        # Initializing variables to store the total loss and metrics for the test set\n",
        "        test_loss = 0.0\n",
        "        test_l1_norm = 0.0\n",
        "        test_linf_norm = 0.0\n",
        "\n",
        "        # Looping over the batches in the test set\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in test_loader:\n",
        "\n",
        "                # Moving the batch tensors to the device\n",
        "                x_batch = x_batch.to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "\n",
        "                # Performing a forward pass and computing the loss and metrics\n",
        "                y_pred = net(x_batch)\n",
        "                loss, l1_norm, linf_norm = compute_loss_and_metrics(\n",
        "                    y_pred, y_batch, loss_fn\n",
        "                )\n",
        "\n",
        "\n",
        "                # Updating the total loss and metrics for the test set\n",
        "                test_loss += loss.item() * x_batch.size(0)\n",
        "                test_l1_norm += l1_norm.item() * x_batch.size(0)\n",
        "                test_linf_norm += linf_norm.item() * x_batch.size(0)\n",
        "\n",
        "        # Computing the average loss and metrics for the test set\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        test_l1_norm /= len(test_loader.dataset)\n",
        "        test_linf_norm /= len(test_loader.dataset)\n",
        "\n",
        "        # Appending the average loss and metrics for the test set to the lists\n",
        "        test_losses.append(test_loss)\n",
        "        test_metrics.append(\n",
        "            {\n",
        "                \"l1_norm\": test_l1_norm,\n",
        "                \"linf_norm\": test_linf_norm,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Logging the average loss and metrics for the test set to tensorboard\n",
        "        writer.add_scalar(\"Loss/test\", test_loss, epoch)\n",
        "        writer.add_scalar(\"L1 norm/test\", test_l1_norm, epoch)\n",
        "        writer.add_scalar(\"Linf norm/test\", test_linf_norm, epoch)\n",
        "\n",
        "        # Printing the average loss and metrics for both sets for this epoch\n",
        "        print(\n",
        "            f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, \"\n",
        "            f\"Train L1 Norm: {train_l1_norm:.4f}, Test L1 Norm: {test_l1_norm:.4f}, \"\n",
        "            f\"Train Linf Norm: {train_linf_norm:.4f}, Test Linf Norm: {test_linf_norm:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Updating the learning rate scheduler with validation loss if applicable\n",
        "        update_scheduler(scheduler, test_loss)\n",
        "\n",
        "        # Reporting the intermediate metric value to Optuna if trial is not None\n",
        "        if trial is not None:\n",
        "            trial.report(test_metrics[-1][\"l1_norm\"], epoch)\n",
        "\n",
        "            # Checking if the trial should be pruned based on the intermediate value if trial is not None\n",
        "            if trial.should_prune():\n",
        "                raise optuna.TrialPruned()\n",
        "\n",
        "    # Closing the SummaryWriter object\n",
        "    writer.close()\n",
        "\n",
        "    # Returning the losses and metrics lists\n",
        "    return train_losses, test_losses, train_metrics, test_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ow4gXb1_MQD"
      },
      "source": [
        "## The objective function and hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwwXv8Fe_MQE"
      },
      "outputs": [],
      "source": [
        "# Defining an objective function for Optuna to minimize\n",
        "def objective(trial):\n",
        "    \"\"\"Defines an objective function for Optuna to minimize.\n",
        "\n",
        "    Args:\n",
        "        trial (optuna.trial.Trial): The trial object that contains the hyperparameters.\n",
        "\n",
        "    Returns:\n",
        "        float: The validation L1 norm to minimize.\n",
        "    \"\"\"\n",
        "    # Creating a trial network and optimizer using the create_model function\n",
        "    net, \\\n",
        "    loss_fn, \\\n",
        "    optimizer, \\\n",
        "    batch_size, \\\n",
        "    n_epochs, \\\n",
        "    scheduler, \\\n",
        "    loss_name, \\\n",
        "    optimizer_name, \\\n",
        "    scheduler_name, \\\n",
        "    n_units, \\\n",
        "    n_layers, \\\n",
        "    hidden_activation, \\\n",
        "    output_activation, \\\n",
        "    lr = create_model(trial, optimize=True)\n",
        "\n",
        "    # Training and evaluating the network using the train_and_eval function\n",
        "    _, _, _, test_metrics = train_and_eval(\n",
        "        net, loss_fn, optimizer, batch_size, n_epochs, scheduler, trial\n",
        "    )\n",
        "\n",
        "    # Returning the last validation L1 norm as the objective value to minimize\n",
        "    return test_metrics[-1][\"l1_norm\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "II0T_X4e_MQE",
        "outputId": "6768dfab-8079-4b9f-aaf7-f8c363cb13b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 11:30:34,150]\u001b[0m A new study created in memory with name: no-name-10a8ef05-1f56-44d3-978a-2ea81640a346\u001b[0m\n",
            "<ipython-input-59-d4ca07a22bfc>:48: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "<ipython-input-59-d4ca07a22bfc>:112: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
            "<ipython-input-59-d4ca07a22bfc>:113: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  momentum = trial.suggest_uniform(\"momentum\", 0.0, 0.99)\n",
            "<ipython-input-59-d4ca07a22bfc>:131: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  gamma = trial.suggest_uniform(\"gamma\", 0.05, 0.5)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.1558, Test Loss: 0.0282, Train L1 Norm: 5.2583, Test L1 Norm: 3.0869, Train Linf Norm: 109.4276, Test Linf Norm: 64.0458\n",
            "Epoch 2: Train Loss: 0.0211, Test Loss: 0.0162, Train L1 Norm: 4.0590, Test L1 Norm: 2.9351, Train Linf Norm: 85.9637, Test Linf Norm: 61.2421\n",
            "Epoch 3: Train Loss: 0.0143, Test Loss: 0.0123, Train L1 Norm: 3.8091, Test L1 Norm: 2.8137, Train Linf Norm: 79.7502, Test Linf Norm: 58.8752\n",
            "Epoch 4: Train Loss: 0.0116, Test Loss: 0.0104, Train L1 Norm: 3.5787, Test L1 Norm: 2.7120, Train Linf Norm: 75.9517, Test Linf Norm: 56.8599\n",
            "Epoch 5: Train Loss: 0.0099, Test Loss: 0.0092, Train L1 Norm: 3.4082, Test L1 Norm: 2.6242, Train Linf Norm: 72.4110, Test Linf Norm: 55.0943\n",
            "Epoch 6: Train Loss: 0.0089, Test Loss: 0.0082, Train L1 Norm: 3.2620, Test L1 Norm: 2.5577, Train Linf Norm: 69.3201, Test Linf Norm: 53.7701\n",
            "Epoch 7: Train Loss: 0.0081, Test Loss: 0.0076, Train L1 Norm: 3.1550, Test L1 Norm: 2.4959, Train Linf Norm: 66.9583, Test Linf Norm: 52.5137\n",
            "Epoch 8: Train Loss: 0.0075, Test Loss: 0.0071, Train L1 Norm: 3.0420, Test L1 Norm: 2.4451, Train Linf Norm: 64.7623, Test Linf Norm: 51.4897\n",
            "Epoch 9: Train Loss: 0.0069, Test Loss: 0.0066, Train L1 Norm: 2.9617, Test L1 Norm: 2.3905, Train Linf Norm: 63.0091, Test Linf Norm: 50.3702\n",
            "Epoch 10: Train Loss: 0.0065, Test Loss: 0.0063, Train L1 Norm: 2.8926, Test L1 Norm: 2.3530, Train Linf Norm: 61.4901, Test Linf Norm: 49.6147\n",
            "Epoch 11: Train Loss: 0.0062, Test Loss: 0.0059, Train L1 Norm: 2.8266, Test L1 Norm: 2.3172, Train Linf Norm: 59.9042, Test Linf Norm: 48.8869\n",
            "Epoch 12: Train Loss: 0.0059, Test Loss: 0.0057, Train L1 Norm: 2.7695, Test L1 Norm: 2.2781, Train Linf Norm: 58.8854, Test Linf Norm: 48.0756\n",
            "Epoch 13: Train Loss: 0.0056, Test Loss: 0.0054, Train L1 Norm: 2.7176, Test L1 Norm: 2.2485, Train Linf Norm: 57.5659, Test Linf Norm: 47.4765\n",
            "Epoch 14: Train Loss: 0.0054, Test Loss: 0.0052, Train L1 Norm: 2.6714, Test L1 Norm: 2.2270, Train Linf Norm: 56.8567, Test Linf Norm: 47.0456\n",
            "Epoch 15: Train Loss: 0.0052, Test Loss: 0.0051, Train L1 Norm: 2.6510, Test L1 Norm: 2.2206, Train Linf Norm: 56.4697, Test Linf Norm: 46.9102\n",
            "Epoch 16: Train Loss: 0.0052, Test Loss: 0.0051, Train L1 Norm: 2.6468, Test L1 Norm: 2.2170, Train Linf Norm: 56.3152, Test Linf Norm: 46.8334\n",
            "Epoch 17: Train Loss: 0.0052, Test Loss: 0.0051, Train L1 Norm: 2.6428, Test L1 Norm: 2.2154, Train Linf Norm: 56.1711, Test Linf Norm: 46.8020\n",
            "Epoch 18: Train Loss: 0.0052, Test Loss: 0.0051, Train L1 Norm: 2.6398, Test L1 Norm: 2.2111, Train Linf Norm: 55.9018, Test Linf Norm: 46.7101\n",
            "Epoch 19: Train Loss: 0.0051, Test Loss: 0.0051, Train L1 Norm: 2.6350, Test L1 Norm: 2.2119, Train Linf Norm: 56.1497, Test Linf Norm: 46.7327\n",
            "Epoch 20: Train Loss: 0.0051, Test Loss: 0.0050, Train L1 Norm: 2.6317, Test L1 Norm: 2.2082, Train Linf Norm: 56.1045, Test Linf Norm: 46.6544\n",
            "Epoch 21: Train Loss: 0.0051, Test Loss: 0.0050, Train L1 Norm: 2.6282, Test L1 Norm: 2.2059, Train Linf Norm: 55.4528, Test Linf Norm: 46.6094\n",
            "Epoch 22: Train Loss: 0.0051, Test Loss: 0.0050, Train L1 Norm: 2.6238, Test L1 Norm: 2.2022, Train Linf Norm: 55.8402, Test Linf Norm: 46.5312\n",
            "Epoch 23: Train Loss: 0.0051, Test Loss: 0.0050, Train L1 Norm: 2.6205, Test L1 Norm: 2.2009, Train Linf Norm: 55.4334, Test Linf Norm: 46.5064\n",
            "Epoch 24: Train Loss: 0.0051, Test Loss: 0.0050, Train L1 Norm: 2.6165, Test L1 Norm: 2.1981, Train Linf Norm: 55.6348, Test Linf Norm: 46.4470\n",
            "Epoch 25: Train Loss: 0.0050, Test Loss: 0.0050, Train L1 Norm: 2.6133, Test L1 Norm: 2.1956, Train Linf Norm: 55.2326, Test Linf Norm: 46.3955\n",
            "Epoch 26: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.6089, Test L1 Norm: 2.1947, Train Linf Norm: 55.4504, Test Linf Norm: 46.3803\n",
            "Epoch 27: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.6055, Test L1 Norm: 2.1899, Train Linf Norm: 55.4370, Test Linf Norm: 46.2786\n",
            "Epoch 28: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.6017, Test L1 Norm: 2.1903, Train Linf Norm: 55.2923, Test Linf Norm: 46.2898\n",
            "Epoch 29: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.6005, Test L1 Norm: 2.1890, Train Linf Norm: 55.0988, Test Linf Norm: 46.2609\n",
            "Epoch 30: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.6000, Test L1 Norm: 2.1889, Train Linf Norm: 55.3981, Test Linf Norm: 46.2600\n",
            "Epoch 31: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.5995, Test L1 Norm: 2.1883, Train Linf Norm: 55.1321, Test Linf Norm: 46.2460\n",
            "Epoch 32: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.5991, Test L1 Norm: 2.1885, Train Linf Norm: 55.3616, Test Linf Norm: 46.2524\n",
            "Epoch 33: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.5987, Test L1 Norm: 2.1878, Train Linf Norm: 55.0433, Test Linf Norm: 46.2364\n",
            "Epoch 34: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.5982, Test L1 Norm: 2.1872, Train Linf Norm: 55.1961, Test Linf Norm: 46.2241\n",
            "Epoch 35: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.5979, Test L1 Norm: 2.1875, Train Linf Norm: 55.1918, Test Linf Norm: 46.2304\n",
            "Epoch 36: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.5975, Test L1 Norm: 2.1865, Train Linf Norm: 55.1822, Test Linf Norm: 46.2100\n",
            "Epoch 37: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.5971, Test L1 Norm: 2.1867, Train Linf Norm: 55.2941, Test Linf Norm: 46.2130\n",
            "Epoch 38: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.5968, Test L1 Norm: 2.1862, Train Linf Norm: 55.0784, Test Linf Norm: 46.2042\n",
            "Epoch 39: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.5964, Test L1 Norm: 2.1859, Train Linf Norm: 55.0885, Test Linf Norm: 46.1979\n",
            "Epoch 40: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.5960, Test L1 Norm: 2.1855, Train Linf Norm: 55.2218, Test Linf Norm: 46.1889\n",
            "Epoch 41: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.5957, Test L1 Norm: 2.1856, Train Linf Norm: 55.1000, Test Linf Norm: 46.1916\n",
            "Epoch 42: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.5953, Test L1 Norm: 2.1853, Train Linf Norm: 55.0705, Test Linf Norm: 46.1861\n",
            "Epoch 43: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.5951, Test L1 Norm: 2.1852, Train Linf Norm: 55.1970, Test Linf Norm: 46.1836\n",
            "Epoch 44: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.5951, Test L1 Norm: 2.1852, Train Linf Norm: 55.3294, Test Linf Norm: 46.1826\n",
            "Epoch 45: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.5951, Test L1 Norm: 2.1851, Train Linf Norm: 55.0465, Test Linf Norm: 46.1815\n",
            "Epoch 46: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.5950, Test L1 Norm: 2.1851, Train Linf Norm: 55.3081, Test Linf Norm: 46.1805\n",
            "Epoch 47: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.5950, Test L1 Norm: 2.1851, Train Linf Norm: 55.3004, Test Linf Norm: 46.1796\n",
            "Epoch 48: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.5950, Test L1 Norm: 2.1850, Train Linf Norm: 55.1455, Test Linf Norm: 46.1792\n",
            "Epoch 49: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.5949, Test L1 Norm: 2.1850, Train Linf Norm: 55.1144, Test Linf Norm: 46.1789\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 11:37:11,853]\u001b[0m Trial 0 finished with value: 2.185002766711917 and parameters: {'n_layers': 3, 'n_units_0': 477, 'n_units_1': 435, 'n_units_2': 725, 'hidden_activation': 'ELU', 'output_activation': 'Softplus', 'loss': 'LogCosh', 'optimizer': 'SGD', 'lr': 0.00013395298585734257, 'batch_size': 23, 'n_epochs': 50, 'scheduler': 'StepLR', 'weight_decay': 0.008091838194815433, 'momentum': 0.12022219948227084, 'step_size': 14, 'gamma': 0.09778985777032034}. Best is trial 0 with value: 2.185002766711917.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50: Train Loss: 0.0050, Test Loss: 0.0049, Train L1 Norm: 2.5949, Test L1 Norm: 2.1850, Train Linf Norm: 55.1929, Test Linf Norm: 46.1785\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-59-d4ca07a22bfc>:135: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  gamma = trial.suggest_uniform(\"gamma\", 0.8, 0.99)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 2: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 3: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 4: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 5: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 6: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 7: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 8: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 9: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 10: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 11: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 12: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 13: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 14: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 15: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 16: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 17: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 18: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 19: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 20: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 21: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 22: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 23: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 24: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 25: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 26: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 27: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 28: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 29: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 30: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 31: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 32: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 33: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 34: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 35: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 36: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 37: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 38: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 39: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 40: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 41: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 42: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 43: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 44: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 45: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 46: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 47: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 48: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 49: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 50: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 51: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 52: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 53: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 54: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 55: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 56: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 57: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 58: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 59: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 60: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 61: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 62: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 63: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 64: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 65: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 66: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 67: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 68: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 69: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 70: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 71: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 72: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 73: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 74: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 75: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 76: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 77: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 78: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 79: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 80: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 81: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 82: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 83: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 84: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 85: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 86: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 87: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 88: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 89: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 90: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 91: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 92: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 93: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 94: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 95: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 96: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 97: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 98: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 99: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 100: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 101: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 102: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 103: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 104: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 105: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 106: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 107: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 108: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 109: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 110: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 111: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 112: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 113: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 114: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 115: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 116: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 117: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 118: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 119: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 120: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 121: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 122: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 123: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 124: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 125: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 126: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 127: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 128: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 129: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 130: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 131: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 132: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 133: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 134: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 135: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 136: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 137: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 138: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 139: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 140: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 141: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 142: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 143: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 144: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 11:59:43,154]\u001b[0m Trial 1 finished with value: 1.0 and parameters: {'n_layers': 2, 'n_units_0': 927, 'n_units_1': 727, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'LogCosh', 'optimizer': 'SGD', 'lr': 0.0002958118390136455, 'batch_size': 17, 'n_epochs': 145, 'scheduler': 'ExponentialLR', 'weight_decay': 0.00031145375888763855, 'momentum': 0.016000712361557257, 'gamma': 0.8432553546797719}. Best is trial 1 with value: 1.0.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 145: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.3340, Test Loss: 0.1172, Train L1 Norm: 5.0279, Test L1 Norm: 2.9740, Train Linf Norm: 929.2649, Test Linf Norm: 541.1802\n",
            "Epoch 2: Train Loss: 0.0897, Test Loss: 0.0261, Train L1 Norm: 4.0312, Test L1 Norm: 2.8378, Train Linf Norm: 775.2511, Test Linf Norm: 528.7887\n",
            "Epoch 3: Train Loss: 0.0343, Test Loss: 0.1572, Train L1 Norm: 3.6809, Test L1 Norm: 2.5607, Train Linf Norm: 707.2398, Test Linf Norm: 464.3237\n",
            "Epoch 4: Train Loss: 0.0284, Test Loss: 0.0163, Train L1 Norm: 3.4695, Test L1 Norm: 2.7213, Train Linf Norm: 622.9084, Test Linf Norm: 517.7378\n",
            "Epoch 5: Train Loss: 0.0206, Test Loss: 0.0103, Train L1 Norm: 3.2561, Test L1 Norm: 2.5833, Train Linf Norm: 598.4866, Test Linf Norm: 494.3331\n",
            "Epoch 6: Train Loss: 0.0125, Test Loss: 0.0275, Train L1 Norm: 3.1091, Test L1 Norm: 2.4025, Train Linf Norm: 594.9364, Test Linf Norm: 455.7557\n",
            "Epoch 7: Train Loss: 0.0174, Test Loss: 0.0080, Train L1 Norm: 2.9777, Test L1 Norm: 2.4137, Train Linf Norm: 561.8671, Test Linf Norm: 465.1612\n",
            "Epoch 8: Train Loss: 0.0094, Test Loss: 0.0095, Train L1 Norm: 2.8707, Test L1 Norm: 2.4106, Train Linf Norm: 556.3487, Test Linf Norm: 466.9826\n",
            "Epoch 9: Train Loss: 0.0107, Test Loss: 0.0069, Train L1 Norm: 2.7640, Test L1 Norm: 2.3175, Train Linf Norm: 529.2432, Test Linf Norm: 449.7579\n",
            "Epoch 10: Train Loss: 0.0110, Test Loss: 0.0062, Train L1 Norm: 2.6838, Test L1 Norm: 2.2915, Train Linf Norm: 505.4600, Test Linf Norm: 446.3721\n",
            "Epoch 11: Train Loss: 0.0085, Test Loss: 0.0608, Train L1 Norm: 2.6114, Test L1 Norm: 2.4522, Train Linf Norm: 492.7672, Test Linf Norm: 474.2780\n",
            "Epoch 12: Train Loss: 0.0117, Test Loss: 0.0060, Train L1 Norm: 2.5662, Test L1 Norm: 2.1984, Train Linf Norm: 491.2133, Test Linf Norm: 429.2086\n",
            "Epoch 13: Train Loss: 0.0081, Test Loss: 0.0053, Train L1 Norm: 2.5048, Test L1 Norm: 2.1778, Train Linf Norm: 475.9961, Test Linf Norm: 426.4315\n",
            "Epoch 14: Train Loss: 0.0071, Test Loss: 0.0050, Train L1 Norm: 2.4620, Test L1 Norm: 2.1397, Train Linf Norm: 435.0701, Test Linf Norm: 419.4945\n",
            "Epoch 15: Train Loss: 0.0062, Test Loss: 0.0073, Train L1 Norm: 2.4023, Test L1 Norm: 2.1446, Train Linf Norm: 449.7727, Test Linf Norm: 421.0022\n",
            "Epoch 16: Train Loss: 0.0056, Test Loss: 0.0067, Train L1 Norm: 2.3599, Test L1 Norm: 2.1154, Train Linf Norm: 452.3193, Test Linf Norm: 415.9460\n",
            "Epoch 17: Train Loss: 0.0073, Test Loss: 0.0164, Train L1 Norm: 2.3240, Test L1 Norm: 2.0189, Train Linf Norm: 442.4752, Test Linf Norm: 393.0419\n",
            "Epoch 18: Train Loss: 0.0067, Test Loss: 0.0043, Train L1 Norm: 2.2908, Test L1 Norm: 2.0398, Train Linf Norm: 439.4625, Test Linf Norm: 402.0593\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:00:08,287]\u001b[0m Trial 2 finished with value: 2.086806902372837 and parameters: {'n_layers': 2, 'n_units_0': 268, 'n_units_1': 786, 'hidden_activation': 'ELU', 'output_activation': 'Softplus', 'loss': 'Huber', 'optimizer': 'SGD', 'lr': 0.0015917410763812573, 'batch_size': 248, 'n_epochs': 19, 'scheduler': 'OneCycleLR', 'weight_decay': 0.0002633163839472179, 'momentum': 0.07565486648828688}. Best is trial 1 with value: 1.0.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Train Loss: 0.0060, Test Loss: 0.0130, Train L1 Norm: 2.2528, Test L1 Norm: 2.0868, Train Linf Norm: 425.4863, Test Linf Norm: 410.6135\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-59-d4ca07a22bfc>:143: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  factor = trial.suggest_uniform(\"factor\", 0.1, 0.9)\n",
            "<ipython-input-59-d4ca07a22bfc>:145: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  threshold = trial.suggest_loguniform(\"threshold\", 1e-4, 1e-2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 3.0362, Test Loss: 2.9773, Train L1 Norm: 1.2518, Test L1 Norm: 0.9995, Train Linf Norm: 17.2294, Test Linf Norm: 1.0000\n",
            "Epoch 2: Train Loss: 2.9503, Test Loss: 2.9772, Train L1 Norm: 1.0007, Test L1 Norm: 1.0000, Train Linf Norm: 1.2101, Test Linf Norm: 1.1876\n",
            "Epoch 3: Train Loss: 2.9700, Test Loss: 2.9774, Train L1 Norm: 1.0085, Test L1 Norm: 0.9997, Train Linf Norm: 1.1815, Test Linf Norm: 1.0000\n",
            "Epoch 4: Train Loss: 2.9842, Test Loss: 2.9774, Train L1 Norm: 1.0159, Test L1 Norm: 1.0000, Train Linf Norm: 1.3262, Test Linf Norm: 1.0000\n",
            "Epoch 5: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 6: Train Loss: 2.9529, Test Loss: 2.9755, Train L1 Norm: 1.0031, Test L1 Norm: 0.9989, Train Linf Norm: 1.1360, Test Linf Norm: 1.0000\n",
            "Epoch 7: Train Loss: 2.9524, Test Loss: 2.9774, Train L1 Norm: 1.0001, Test L1 Norm: 1.0000, Train Linf Norm: 1.0220, Test Linf Norm: 1.0000\n",
            "Epoch 8: Train Loss: 2.9566, Test Loss: 2.9774, Train L1 Norm: 1.0149, Test L1 Norm: 0.9999, Train Linf Norm: 1.9789, Test Linf Norm: 1.0000\n",
            "Epoch 9: Train Loss: 2.9621, Test Loss: 2.9325, Train L1 Norm: 1.0039, Test L1 Norm: 0.9925, Train Linf Norm: 1.0737, Test Linf Norm: 1.0000\n",
            "Epoch 10: Train Loss: 2.9509, Test Loss: 2.9774, Train L1 Norm: 0.9997, Test L1 Norm: 1.0000, Train Linf Norm: 1.0245, Test Linf Norm: 1.0000\n",
            "Epoch 11: Train Loss: 2.9561, Test Loss: 2.9773, Train L1 Norm: 1.0490, Test L1 Norm: 1.0015, Train Linf Norm: 4.1433, Test Linf Norm: 1.1898\n",
            "Epoch 12: Train Loss: 2.9529, Test Loss: 2.9771, Train L1 Norm: 1.0483, Test L1 Norm: 1.0772, Train Linf Norm: 4.7160, Test Linf Norm: 7.2447\n",
            "Epoch 13: Train Loss: 2.9572, Test Loss: 2.9774, Train L1 Norm: 1.0127, Test L1 Norm: 1.0000, Train Linf Norm: 1.7108, Test Linf Norm: 1.0000\n",
            "Epoch 14: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0002, Test L1 Norm: 1.0000, Train Linf Norm: 1.0093, Test Linf Norm: 1.0000\n",
            "Epoch 15: Train Loss: 2.9560, Test Loss: 2.9774, Train L1 Norm: 1.0032, Test L1 Norm: 1.0000, Train Linf Norm: 1.1189, Test Linf Norm: 1.0000\n",
            "Epoch 16: Train Loss: 2.9588, Test Loss: 2.9769, Train L1 Norm: 1.0091, Test L1 Norm: 0.9994, Train Linf Norm: 1.5491, Test Linf Norm: 1.0000\n",
            "Epoch 17: Train Loss: 2.9539, Test Loss: 2.9771, Train L1 Norm: 1.1083, Test L1 Norm: 0.9991, Train Linf Norm: 9.1016, Test Linf Norm: 1.0000\n",
            "Epoch 18: Train Loss: 2.9535, Test Loss: 2.9774, Train L1 Norm: 1.0002, Test L1 Norm: 1.0000, Train Linf Norm: 1.0697, Test Linf Norm: 1.0000\n",
            "Epoch 19: Train Loss: 2.9584, Test Loss: 2.9773, Train L1 Norm: 1.0085, Test L1 Norm: 0.9994, Train Linf Norm: 1.5059, Test Linf Norm: 1.0000\n",
            "Epoch 20: Train Loss: 2.9527, Test Loss: 2.9773, Train L1 Norm: 1.0012, Test L1 Norm: 0.9980, Train Linf Norm: 1.1555, Test Linf Norm: 1.0389\n",
            "Epoch 21: Train Loss: 2.9581, Test Loss: 2.9773, Train L1 Norm: 1.0047, Test L1 Norm: 0.9996, Train Linf Norm: 1.3492, Test Linf Norm: 1.0000\n",
            "Epoch 22: Train Loss: 2.9537, Test Loss: 2.9774, Train L1 Norm: 1.0014, Test L1 Norm: 0.9998, Train Linf Norm: 1.0747, Test Linf Norm: 1.0266\n",
            "Epoch 23: Train Loss: 2.9603, Test Loss: 2.9725, Train L1 Norm: 1.0279, Test L1 Norm: 0.9978, Train Linf Norm: 2.7229, Test Linf Norm: 1.0000\n",
            "Epoch 24: Train Loss: 2.9502, Test Loss: 2.9774, Train L1 Norm: 0.9993, Test L1 Norm: 0.9999, Train Linf Norm: 1.0091, Test Linf Norm: 1.0000\n",
            "Epoch 25: Train Loss: 2.8969, Test Loss: 2.9468, Train L1 Norm: 1.0746, Test L1 Norm: 0.9794, Train Linf Norm: 8.0630, Test Linf Norm: 2.7138\n",
            "Epoch 26: Train Loss: 2.9035, Test Loss: 1.4885, Train L1 Norm: 0.9904, Test L1 Norm: 0.6398, Train Linf Norm: 1.5765, Test Linf Norm: 1.2844\n",
            "Epoch 27: Train Loss: 2.4109, Test Loss: 5.2035, Train L1 Norm: 0.8735, Test L1 Norm: 1.6528, Train Linf Norm: 4.3483, Test Linf Norm: 17.0514\n",
            "Epoch 28: Train Loss: 1.7278, Test Loss: 2.9277, Train L1 Norm: 0.6964, Test L1 Norm: 1.4748, Train Linf Norm: 6.4110, Test Linf Norm: 34.8867\n",
            "Epoch 29: Train Loss: 1.5387, Test Loss: 0.5617, Train L1 Norm: 0.6155, Test L1 Norm: 0.3171, Train Linf Norm: 4.4239, Test Linf Norm: 3.6540\n",
            "Epoch 30: Train Loss: 1.4240, Test Loss: 0.4818, Train L1 Norm: 0.5981, Test L1 Norm: 0.3021, Train Linf Norm: 5.9397, Test Linf Norm: 5.0394\n",
            "Epoch 31: Train Loss: 1.3213, Test Loss: 0.4175, Train L1 Norm: 0.5694, Test L1 Norm: 0.3292, Train Linf Norm: 6.0429, Test Linf Norm: 9.1390\n",
            "Epoch 32: Train Loss: 1.2186, Test Loss: 2.2593, Train L1 Norm: 0.5123, Test L1 Norm: 0.7635, Train Linf Norm: 4.0317, Test Linf Norm: 1.7133\n",
            "Epoch 33: Train Loss: 1.1381, Test Loss: 2.2477, Train L1 Norm: 0.4974, Test L1 Norm: 0.7538, Train Linf Norm: 4.5903, Test Linf Norm: 1.4390\n",
            "Epoch 34: Train Loss: 1.0450, Test Loss: 1.2881, Train L1 Norm: 0.4664, Test L1 Norm: 0.7392, Train Linf Norm: 4.1908, Test Linf Norm: 18.0192\n",
            "Epoch 35: Train Loss: 0.9634, Test Loss: 0.5977, Train L1 Norm: 0.4462, Test L1 Norm: 0.2940, Train Linf Norm: 4.4966, Test Linf Norm: 4.6343\n",
            "Epoch 36: Train Loss: 0.9009, Test Loss: 0.9656, Train L1 Norm: 0.4316, Test L1 Norm: 0.4570, Train Linf Norm: 4.7083, Test Linf Norm: 3.7215\n",
            "Epoch 37: Train Loss: 0.8464, Test Loss: 0.8498, Train L1 Norm: 0.4104, Test L1 Norm: 0.4236, Train Linf Norm: 4.3611, Test Linf Norm: 4.0515\n",
            "Epoch 38: Train Loss: 0.7701, Test Loss: 0.8261, Train L1 Norm: 0.4192, Test L1 Norm: 0.3830, Train Linf Norm: 6.7570, Test Linf Norm: 4.9509\n",
            "Epoch 39: Train Loss: 0.7254, Test Loss: 1.4430, Train L1 Norm: 0.3634, Test L1 Norm: 0.5702, Train Linf Norm: 3.5661, Test Linf Norm: 2.9185\n",
            "Epoch 40: Train Loss: 0.6528, Test Loss: 1.8610, Train L1 Norm: 0.3559, Test L1 Norm: 1.5292, Train Linf Norm: 4.4648, Test Linf Norm: 57.6904\n",
            "Epoch 41: Train Loss: 0.6161, Test Loss: 0.2834, Train L1 Norm: 0.3421, Test L1 Norm: 0.2072, Train Linf Norm: 4.2524, Test Linf Norm: 4.8854\n",
            "Epoch 42: Train Loss: 0.5735, Test Loss: 0.0468, Train L1 Norm: 0.3273, Test L1 Norm: 0.1830, Train Linf Norm: 4.1506, Test Linf Norm: 9.0886\n",
            "Epoch 43: Train Loss: 0.5364, Test Loss: 0.1558, Train L1 Norm: 0.3139, Test L1 Norm: 0.1366, Train Linf Norm: 3.8739, Test Linf Norm: 2.7070\n",
            "Epoch 44: Train Loss: 0.4954, Test Loss: 1.7876, Train L1 Norm: 0.3025, Test L1 Norm: 0.6274, Train Linf Norm: 3.9177, Test Linf Norm: 2.0651\n",
            "Epoch 45: Train Loss: 0.4576, Test Loss: 0.0602, Train L1 Norm: 0.2906, Test L1 Norm: 0.1898, Train Linf Norm: 3.9371, Test Linf Norm: 4.8702\n",
            "Epoch 46: Train Loss: 0.4110, Test Loss: 0.0802, Train L1 Norm: 0.2735, Test L1 Norm: 0.1957, Train Linf Norm: 3.8304, Test Linf Norm: 4.6835\n",
            "Epoch 47: Train Loss: 0.3562, Test Loss: 0.2543, Train L1 Norm: 0.2618, Test L1 Norm: 0.2445, Train Linf Norm: 4.1621, Test Linf Norm: 7.1248\n",
            "Epoch 48: Train Loss: 0.3138, Test Loss: 0.1006, Train L1 Norm: 0.2479, Test L1 Norm: 0.2803, Train Linf Norm: 4.3740, Test Linf Norm: 10.8886\n",
            "Epoch 49: Train Loss: 0.2744, Test Loss: 0.0067, Train L1 Norm: 0.2341, Test L1 Norm: 0.1890, Train Linf Norm: 4.2576, Test Linf Norm: 11.5772\n",
            "Epoch 50: Train Loss: 0.2428, Test Loss: 0.2218, Train L1 Norm: 0.2306, Test L1 Norm: 0.2840, Train Linf Norm: 4.8758, Test Linf Norm: 10.5968\n",
            "Epoch 51: Train Loss: 0.2185, Test Loss: 0.6861, Train L1 Norm: 0.2170, Test L1 Norm: 0.3748, Train Linf Norm: 4.3344, Test Linf Norm: 5.7101\n",
            "Epoch 52: Train Loss: 0.2015, Test Loss: 0.0111, Train L1 Norm: 0.2076, Test L1 Norm: 0.1473, Train Linf Norm: 4.1757, Test Linf Norm: 7.5661\n",
            "Epoch 53: Train Loss: 0.1865, Test Loss: 0.0254, Train L1 Norm: 0.2051, Test L1 Norm: 0.1752, Train Linf Norm: 4.5703, Test Linf Norm: 9.6035\n",
            "Epoch 54: Train Loss: 0.1697, Test Loss: 1.0373, Train L1 Norm: 0.1904, Test L1 Norm: 0.4459, Train Linf Norm: 3.9497, Test Linf Norm: 3.2101\n",
            "Epoch 55: Train Loss: 0.1518, Test Loss: 0.5751, Train L1 Norm: 0.1758, Test L1 Norm: 0.3551, Train Linf Norm: 3.5658, Test Linf Norm: 7.0392\n",
            "Epoch 56: Train Loss: 0.1357, Test Loss: 0.0102, Train L1 Norm: 0.1738, Test L1 Norm: 0.1283, Train Linf Norm: 3.9590, Test Linf Norm: 7.2530\n",
            "Epoch 57: Train Loss: 0.1211, Test Loss: 0.0117, Train L1 Norm: 0.1615, Test L1 Norm: 0.1317, Train Linf Norm: 3.5957, Test Linf Norm: 7.1336\n",
            "Epoch 58: Train Loss: 0.1067, Test Loss: 0.0262, Train L1 Norm: 0.1554, Test L1 Norm: 0.1522, Train Linf Norm: 3.5639, Test Linf Norm: 8.1422\n",
            "Epoch 59: Train Loss: 0.0896, Test Loss: 0.0181, Train L1 Norm: 0.1470, Test L1 Norm: 0.1419, Train Linf Norm: 3.7919, Test Linf Norm: 6.8083\n",
            "Epoch 60: Train Loss: 0.0774, Test Loss: 0.7780, Train L1 Norm: 0.1346, Test L1 Norm: 0.3829, Train Linf Norm: 3.6350, Test Linf Norm: 5.3083\n",
            "Epoch 61: Train Loss: 0.0666, Test Loss: 0.0016, Train L1 Norm: 0.1291, Test L1 Norm: 0.1366, Train Linf Norm: 3.5848, Test Linf Norm: 8.5773\n",
            "Epoch 62: Train Loss: 0.0565, Test Loss: 0.0624, Train L1 Norm: 0.1221, Test L1 Norm: 0.2122, Train Linf Norm: 3.5356, Test Linf Norm: 10.5369\n",
            "Epoch 63: Train Loss: 0.0471, Test Loss: 0.0593, Train L1 Norm: 0.1139, Test L1 Norm: 0.1818, Train Linf Norm: 3.3493, Test Linf Norm: 7.7939\n",
            "Epoch 64: Train Loss: 0.0395, Test Loss: 0.0531, Train L1 Norm: 0.1087, Test L1 Norm: 0.1814, Train Linf Norm: 3.4579, Test Linf Norm: 7.8775\n",
            "Epoch 65: Train Loss: 0.0346, Test Loss: 0.0015, Train L1 Norm: 0.1078, Test L1 Norm: 0.1100, Train Linf Norm: 3.6980, Test Linf Norm: 6.7800\n",
            "Epoch 66: Train Loss: 0.0286, Test Loss: 0.0039, Train L1 Norm: 0.1012, Test L1 Norm: 0.1544, Train Linf Norm: 3.6693, Test Linf Norm: 9.2099\n",
            "Epoch 67: Train Loss: 0.0256, Test Loss: 0.0012, Train L1 Norm: 0.0958, Test L1 Norm: 0.1304, Train Linf Norm: 3.5886, Test Linf Norm: 8.3825\n",
            "Epoch 68: Train Loss: 0.0247, Test Loss: 0.0182, Train L1 Norm: 0.0973, Test L1 Norm: 0.1439, Train Linf Norm: 3.6567, Test Linf Norm: 7.1448\n",
            "Epoch 69: Train Loss: 0.0235, Test Loss: 0.0073, Train L1 Norm: 0.0962, Test L1 Norm: 0.1371, Train Linf Norm: 3.6015, Test Linf Norm: 8.2212\n",
            "Epoch 70: Train Loss: 0.0207, Test Loss: 0.0014, Train L1 Norm: 0.0973, Test L1 Norm: 0.1694, Train Linf Norm: 3.9126, Test Linf Norm: 10.6268\n",
            "Epoch 71: Train Loss: 0.0204, Test Loss: 0.1036, Train L1 Norm: 0.0959, Test L1 Norm: 0.1851, Train Linf Norm: 3.9346, Test Linf Norm: 6.9571\n",
            "Epoch 72: Train Loss: 0.0191, Test Loss: 0.0162, Train L1 Norm: 0.0936, Test L1 Norm: 0.1843, Train Linf Norm: 3.9395, Test Linf Norm: 10.3081\n",
            "Epoch 73: Train Loss: 0.0181, Test Loss: 0.0028, Train L1 Norm: 0.0957, Test L1 Norm: 0.1289, Train Linf Norm: 4.0723, Test Linf Norm: 6.4278\n",
            "Epoch 74: Train Loss: 0.0172, Test Loss: 0.0684, Train L1 Norm: 0.0963, Test L1 Norm: 0.2246, Train Linf Norm: 4.1992, Test Linf Norm: 10.7647\n",
            "Epoch 75: Train Loss: 0.0172, Test Loss: 0.0011, Train L1 Norm: 0.0954, Test L1 Norm: 0.0959, Train Linf Norm: 4.0694, Test Linf Norm: 5.5106\n",
            "Epoch 76: Train Loss: 0.0153, Test Loss: 0.0023, Train L1 Norm: 0.0915, Test L1 Norm: 0.1405, Train Linf Norm: 4.0367, Test Linf Norm: 8.8366\n",
            "Epoch 77: Train Loss: 0.0148, Test Loss: 0.0012, Train L1 Norm: 0.0920, Test L1 Norm: 0.1667, Train Linf Norm: 4.1199, Test Linf Norm: 11.0792\n",
            "Epoch 78: Train Loss: 0.0142, Test Loss: 0.0037, Train L1 Norm: 0.0913, Test L1 Norm: 0.1302, Train Linf Norm: 4.0433, Test Linf Norm: 7.8191\n",
            "Epoch 79: Train Loss: 0.0122, Test Loss: 0.0013, Train L1 Norm: 0.0886, Test L1 Norm: 0.1117, Train Linf Norm: 3.9840, Test Linf Norm: 6.9601\n",
            "Epoch 80: Train Loss: 0.0118, Test Loss: 0.0031, Train L1 Norm: 0.0886, Test L1 Norm: 0.1342, Train Linf Norm: 4.0602, Test Linf Norm: 8.4397\n",
            "Epoch 81: Train Loss: 0.0103, Test Loss: 0.0008, Train L1 Norm: 0.0874, Test L1 Norm: 0.1327, Train Linf Norm: 4.0403, Test Linf Norm: 8.6407\n",
            "Epoch 82: Train Loss: 0.0095, Test Loss: 0.0005, Train L1 Norm: 0.0848, Test L1 Norm: 0.1203, Train Linf Norm: 3.9660, Test Linf Norm: 7.8894\n",
            "Epoch 83: Train Loss: 0.0093, Test Loss: 0.0004, Train L1 Norm: 0.0839, Test L1 Norm: 0.1078, Train Linf Norm: 3.9998, Test Linf Norm: 7.0537\n",
            "Epoch 84: Train Loss: 0.0081, Test Loss: 0.0384, Train L1 Norm: 0.0824, Test L1 Norm: 0.1731, Train Linf Norm: 4.0158, Test Linf Norm: 8.3388\n",
            "Epoch 85: Train Loss: 0.0080, Test Loss: 0.0009, Train L1 Norm: 0.0822, Test L1 Norm: 0.1034, Train Linf Norm: 3.9711, Test Linf Norm: 6.4621\n",
            "Epoch 86: Train Loss: 0.0074, Test Loss: 0.0070, Train L1 Norm: 0.0861, Test L1 Norm: 0.1527, Train Linf Norm: 4.3310, Test Linf Norm: 8.9922\n",
            "Epoch 87: Train Loss: 0.0073, Test Loss: 0.0016, Train L1 Norm: 0.0815, Test L1 Norm: 0.1171, Train Linf Norm: 4.0457, Test Linf Norm: 7.5082\n",
            "Epoch 88: Train Loss: 0.0072, Test Loss: 0.0019, Train L1 Norm: 0.0774, Test L1 Norm: 0.1136, Train Linf Norm: 3.7356, Test Linf Norm: 7.2299\n",
            "Epoch 89: Train Loss: 0.0066, Test Loss: 0.0007, Train L1 Norm: 0.0795, Test L1 Norm: 0.1181, Train Linf Norm: 3.9929, Test Linf Norm: 7.7978\n",
            "Epoch 90: Train Loss: 0.0063, Test Loss: 0.0009, Train L1 Norm: 0.0800, Test L1 Norm: 0.1384, Train Linf Norm: 3.9334, Test Linf Norm: 8.6437\n",
            "Epoch 91: Train Loss: 0.0058, Test Loss: 0.0008, Train L1 Norm: 0.0833, Test L1 Norm: 0.1015, Train Linf Norm: 4.2887, Test Linf Norm: 5.3906\n",
            "Epoch 92: Train Loss: 0.0058, Test Loss: 0.0013, Train L1 Norm: 0.0827, Test L1 Norm: 0.1546, Train Linf Norm: 4.2079, Test Linf Norm: 10.1124\n",
            "Epoch 93: Train Loss: 0.0056, Test Loss: 0.0007, Train L1 Norm: 0.0773, Test L1 Norm: 0.1239, Train Linf Norm: 3.8866, Test Linf Norm: 8.0981\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:03:34,409]\u001b[0m Trial 3 finished with value: 0.11888195726890118 and parameters: {'n_layers': 1, 'n_units_0': 544, 'hidden_activation': 'ReLU', 'output_activation': 'Softplus', 'loss': 'Huber', 'optimizer': 'RMSprop', 'lr': 0.006471778379983815, 'batch_size': 74, 'n_epochs': 94, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.545982637252452, 'patience': 14, 'threshold': 0.0033499949266035793}. Best is trial 3 with value: 0.11888195726890118.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 94: Train Loss: 0.0053, Test Loss: 0.0004, Train L1 Norm: 0.0788, Test L1 Norm: 0.1189, Train Linf Norm: 4.0051, Test Linf Norm: 7.8284\n",
            "Epoch 1: Train Loss: 0.2372, Test Loss: 0.1274, Train L1 Norm: 3.8849, Test L1 Norm: 2.5766, Train Linf Norm: 495.6760, Test Linf Norm: 329.8956\n",
            "Epoch 2: Train Loss: 0.1035, Test Loss: 0.1212, Train L1 Norm: 3.0219, Test L1 Norm: 2.4436, Train Linf Norm: 384.9891, Test Linf Norm: 315.2024\n",
            "Epoch 3: Train Loss: 0.0887, Test Loss: 0.1017, Train L1 Norm: 2.8292, Test L1 Norm: 2.3388, Train Linf Norm: 362.9044, Test Linf Norm: 302.9570\n",
            "Epoch 4: Train Loss: 0.0807, Test Loss: 0.0912, Train L1 Norm: 2.7111, Test L1 Norm: 2.2765, Train Linf Norm: 350.5675, Test Linf Norm: 295.9222\n",
            "Epoch 5: Train Loss: 0.0754, Test Loss: 0.0726, Train L1 Norm: 2.6244, Test L1 Norm: 2.2392, Train Linf Norm: 335.3828, Test Linf Norm: 292.2272\n",
            "Epoch 6: Train Loss: 0.0710, Test Loss: 0.0670, Train L1 Norm: 2.5443, Test L1 Norm: 2.1909, Train Linf Norm: 327.3743, Test Linf Norm: 286.4207\n",
            "Epoch 7: Train Loss: 0.0676, Test Loss: 0.0655, Train L1 Norm: 2.4673, Test L1 Norm: 2.1561, Train Linf Norm: 316.9298, Test Linf Norm: 282.3334\n",
            "Epoch 8: Train Loss: 0.0647, Test Loss: 0.0635, Train L1 Norm: 2.4197, Test L1 Norm: 2.0919, Train Linf Norm: 312.5289, Test Linf Norm: 274.1249\n",
            "Epoch 9: Train Loss: 0.0625, Test Loss: 0.0585, Train L1 Norm: 2.3620, Test L1 Norm: 2.0644, Train Linf Norm: 308.0034, Test Linf Norm: 270.9793\n",
            "Epoch 10: Train Loss: 0.0604, Test Loss: 0.0614, Train L1 Norm: 2.3214, Test L1 Norm: 2.0439, Train Linf Norm: 297.0574, Test Linf Norm: 268.5497\n",
            "Epoch 11: Train Loss: 0.0579, Test Loss: 0.0541, Train L1 Norm: 2.2763, Test L1 Norm: 1.9956, Train Linf Norm: 295.9271, Test Linf Norm: 262.5191\n",
            "Epoch 12: Train Loss: 0.0559, Test Loss: 0.0520, Train L1 Norm: 2.2341, Test L1 Norm: 1.9754, Train Linf Norm: 289.8604, Test Linf Norm: 260.2422\n",
            "Epoch 13: Train Loss: 0.0539, Test Loss: 0.0647, Train L1 Norm: 2.2010, Test L1 Norm: 1.9699, Train Linf Norm: 257.5556, Test Linf Norm: 259.4984\n",
            "Epoch 14: Train Loss: 0.0521, Test Loss: 0.0559, Train L1 Norm: 2.1622, Test L1 Norm: 1.9375, Train Linf Norm: 275.4865, Test Linf Norm: 255.6500\n",
            "Epoch 15: Train Loss: 0.0503, Test Loss: 0.0467, Train L1 Norm: 2.1338, Test L1 Norm: 1.8902, Train Linf Norm: 273.5604, Test Linf Norm: 249.6926\n",
            "Epoch 16: Train Loss: 0.0487, Test Loss: 0.0531, Train L1 Norm: 2.1079, Test L1 Norm: 1.8855, Train Linf Norm: 270.7362, Test Linf Norm: 249.1328\n",
            "Epoch 17: Train Loss: 0.0472, Test Loss: 0.0454, Train L1 Norm: 2.0802, Test L1 Norm: 1.8490, Train Linf Norm: 269.5311, Test Linf Norm: 244.5469\n",
            "Epoch 18: Train Loss: 0.0460, Test Loss: 0.0429, Train L1 Norm: 2.0565, Test L1 Norm: 1.8276, Train Linf Norm: 264.6832, Test Linf Norm: 241.8544\n",
            "Epoch 19: Train Loss: 0.0444, Test Loss: 0.0426, Train L1 Norm: 2.0305, Test L1 Norm: 1.7984, Train Linf Norm: 258.6733, Test Linf Norm: 238.0403\n",
            "Epoch 20: Train Loss: 0.0432, Test Loss: 0.0487, Train L1 Norm: 2.0095, Test L1 Norm: 1.7787, Train Linf Norm: 260.7054, Test Linf Norm: 235.3178\n",
            "Epoch 21: Train Loss: 0.0422, Test Loss: 0.0406, Train L1 Norm: 1.9879, Test L1 Norm: 1.7706, Train Linf Norm: 259.0063, Test Linf Norm: 234.6619\n",
            "Epoch 22: Train Loss: 0.0410, Test Loss: 0.0395, Train L1 Norm: 1.9646, Test L1 Norm: 1.7419, Train Linf Norm: 255.9402, Test Linf Norm: 230.8866\n",
            "Epoch 23: Train Loss: 0.0401, Test Loss: 0.0376, Train L1 Norm: 1.9429, Test L1 Norm: 1.7326, Train Linf Norm: 252.0508, Test Linf Norm: 229.8388\n",
            "Epoch 24: Train Loss: 0.0392, Test Loss: 0.0413, Train L1 Norm: 1.9258, Test L1 Norm: 1.7286, Train Linf Norm: 247.5341, Test Linf Norm: 229.3698\n",
            "Epoch 25: Train Loss: 0.0382, Test Loss: 0.0383, Train L1 Norm: 1.9082, Test L1 Norm: 1.6924, Train Linf Norm: 243.0277, Test Linf Norm: 224.5951\n",
            "Epoch 26: Train Loss: 0.0374, Test Loss: 0.0357, Train L1 Norm: 1.8849, Test L1 Norm: 1.6843, Train Linf Norm: 243.0821, Test Linf Norm: 223.6972\n",
            "Epoch 27: Train Loss: 0.0365, Test Loss: 0.0406, Train L1 Norm: 1.8685, Test L1 Norm: 1.6802, Train Linf Norm: 240.2268, Test Linf Norm: 223.2089\n",
            "Epoch 28: Train Loss: 0.0358, Test Loss: 0.0417, Train L1 Norm: 1.8492, Test L1 Norm: 1.6509, Train Linf Norm: 238.8086, Test Linf Norm: 219.2405\n",
            "Epoch 29: Train Loss: 0.0351, Test Loss: 0.0325, Train L1 Norm: 1.8329, Test L1 Norm: 1.6410, Train Linf Norm: 232.8326, Test Linf Norm: 218.2958\n",
            "Epoch 30: Train Loss: 0.0343, Test Loss: 0.0399, Train L1 Norm: 1.8181, Test L1 Norm: 1.6408, Train Linf Norm: 236.4975, Test Linf Norm: 218.1997\n",
            "Epoch 31: Train Loss: 0.0337, Test Loss: 0.0337, Train L1 Norm: 1.7997, Test L1 Norm: 1.6185, Train Linf Norm: 224.4800, Test Linf Norm: 215.4766\n",
            "Epoch 32: Train Loss: 0.0332, Test Loss: 0.0312, Train L1 Norm: 1.7846, Test L1 Norm: 1.6014, Train Linf Norm: 233.5118, Test Linf Norm: 213.3254\n",
            "Epoch 33: Train Loss: 0.0327, Test Loss: 0.0354, Train L1 Norm: 1.7664, Test L1 Norm: 1.6013, Train Linf Norm: 224.8961, Test Linf Norm: 213.3318\n",
            "Epoch 34: Train Loss: 0.0320, Test Loss: 0.0333, Train L1 Norm: 1.7537, Test L1 Norm: 1.5716, Train Linf Norm: 226.1919, Test Linf Norm: 209.4166\n",
            "Epoch 35: Train Loss: 0.0316, Test Loss: 0.0301, Train L1 Norm: 1.7378, Test L1 Norm: 1.5686, Train Linf Norm: 226.4090, Test Linf Norm: 209.2596\n",
            "Epoch 36: Train Loss: 0.0309, Test Loss: 0.0313, Train L1 Norm: 1.7244, Test L1 Norm: 1.5512, Train Linf Norm: 224.4411, Test Linf Norm: 206.9318\n",
            "Epoch 37: Train Loss: 0.0306, Test Loss: 0.0284, Train L1 Norm: 1.7085, Test L1 Norm: 1.5416, Train Linf Norm: 222.8260, Test Linf Norm: 205.8521\n",
            "Epoch 38: Train Loss: 0.0300, Test Loss: 0.0326, Train L1 Norm: 1.6944, Test L1 Norm: 1.5411, Train Linf Norm: 218.3507, Test Linf Norm: 205.8089\n",
            "Epoch 39: Train Loss: 0.0297, Test Loss: 0.0290, Train L1 Norm: 1.6817, Test L1 Norm: 1.5266, Train Linf Norm: 217.9406, Test Linf Norm: 204.0195\n",
            "Epoch 40: Train Loss: 0.0292, Test Loss: 0.0289, Train L1 Norm: 1.6683, Test L1 Norm: 1.5123, Train Linf Norm: 215.4646, Test Linf Norm: 202.1843\n",
            "Epoch 41: Train Loss: 0.0287, Test Loss: 0.0312, Train L1 Norm: 1.6563, Test L1 Norm: 1.4954, Train Linf Norm: 216.9811, Test Linf Norm: 199.8666\n",
            "Epoch 42: Train Loss: 0.0283, Test Loss: 0.0295, Train L1 Norm: 1.6434, Test L1 Norm: 1.4924, Train Linf Norm: 211.8924, Test Linf Norm: 199.6523\n",
            "Epoch 43: Train Loss: 0.0281, Test Loss: 0.0297, Train L1 Norm: 1.6317, Test L1 Norm: 1.4786, Train Linf Norm: 212.7917, Test Linf Norm: 197.7962\n",
            "Epoch 44: Train Loss: 0.0276, Test Loss: 0.0301, Train L1 Norm: 1.6203, Test L1 Norm: 1.4803, Train Linf Norm: 212.6686, Test Linf Norm: 198.1607\n",
            "Epoch 45: Train Loss: 0.0272, Test Loss: 0.0300, Train L1 Norm: 1.6092, Test L1 Norm: 1.4740, Train Linf Norm: 207.2841, Test Linf Norm: 197.3743\n",
            "Epoch 46: Train Loss: 0.0256, Test Loss: 0.0253, Train L1 Norm: 1.6029, Test L1 Norm: 1.4619, Train Linf Norm: 206.6506, Test Linf Norm: 195.8666\n",
            "Epoch 47: Train Loss: 0.0255, Test Loss: 0.0253, Train L1 Norm: 1.5989, Test L1 Norm: 1.4602, Train Linf Norm: 207.6808, Test Linf Norm: 195.6707\n",
            "Epoch 48: Train Loss: 0.0254, Test Loss: 0.0253, Train L1 Norm: 1.5956, Test L1 Norm: 1.4578, Train Linf Norm: 208.4659, Test Linf Norm: 195.3626\n",
            "Epoch 49: Train Loss: 0.0253, Test Loss: 0.0249, Train L1 Norm: 1.5919, Test L1 Norm: 1.4547, Train Linf Norm: 206.9773, Test Linf Norm: 194.9863\n",
            "Epoch 50: Train Loss: 0.0252, Test Loss: 0.0249, Train L1 Norm: 1.5884, Test L1 Norm: 1.4510, Train Linf Norm: 206.9302, Test Linf Norm: 194.5041\n",
            "Epoch 51: Train Loss: 0.0251, Test Loss: 0.0252, Train L1 Norm: 1.5850, Test L1 Norm: 1.4467, Train Linf Norm: 206.6390, Test Linf Norm: 193.9228\n",
            "Epoch 52: Train Loss: 0.0250, Test Loss: 0.0250, Train L1 Norm: 1.5819, Test L1 Norm: 1.4476, Train Linf Norm: 205.6181, Test Linf Norm: 194.1067\n",
            "Epoch 53: Train Loss: 0.0249, Test Loss: 0.0246, Train L1 Norm: 1.5780, Test L1 Norm: 1.4450, Train Linf Norm: 206.6184, Test Linf Norm: 193.7744\n",
            "Epoch 54: Train Loss: 0.0248, Test Loss: 0.0245, Train L1 Norm: 1.5752, Test L1 Norm: 1.4416, Train Linf Norm: 203.6525, Test Linf Norm: 193.3372\n",
            "Epoch 55: Train Loss: 0.0247, Test Loss: 0.0244, Train L1 Norm: 1.5716, Test L1 Norm: 1.4387, Train Linf Norm: 204.9059, Test Linf Norm: 192.9719\n",
            "Epoch 56: Train Loss: 0.0246, Test Loss: 0.0243, Train L1 Norm: 1.5689, Test L1 Norm: 1.4358, Train Linf Norm: 204.1267, Test Linf Norm: 192.6084\n",
            "Epoch 57: Train Loss: 0.0245, Test Loss: 0.0248, Train L1 Norm: 1.5650, Test L1 Norm: 1.4331, Train Linf Norm: 204.4156, Test Linf Norm: 192.2452\n",
            "Epoch 58: Train Loss: 0.0244, Test Loss: 0.0243, Train L1 Norm: 1.5623, Test L1 Norm: 1.4316, Train Linf Norm: 203.2349, Test Linf Norm: 192.0908\n",
            "Epoch 59: Train Loss: 0.0244, Test Loss: 0.0241, Train L1 Norm: 1.5588, Test L1 Norm: 1.4282, Train Linf Norm: 202.6080, Test Linf Norm: 191.6562\n",
            "Epoch 60: Train Loss: 0.0243, Test Loss: 0.0246, Train L1 Norm: 1.5552, Test L1 Norm: 1.4247, Train Linf Norm: 195.7779, Test Linf Norm: 191.1846\n",
            "Epoch 61: Train Loss: 0.0242, Test Loss: 0.0251, Train L1 Norm: 1.5526, Test L1 Norm: 1.4264, Train Linf Norm: 202.5522, Test Linf Norm: 191.4522\n",
            "Epoch 62: Train Loss: 0.0241, Test Loss: 0.0238, Train L1 Norm: 1.5495, Test L1 Norm: 1.4217, Train Linf Norm: 203.3179, Test Linf Norm: 190.8485\n",
            "Epoch 63: Train Loss: 0.0240, Test Loss: 0.0237, Train L1 Norm: 1.5462, Test L1 Norm: 1.4187, Train Linf Norm: 203.7608, Test Linf Norm: 190.4730\n",
            "Epoch 64: Train Loss: 0.0239, Test Loss: 0.0237, Train L1 Norm: 1.5437, Test L1 Norm: 1.4156, Train Linf Norm: 202.0266, Test Linf Norm: 190.0731\n",
            "Epoch 65: Train Loss: 0.0238, Test Loss: 0.0237, Train L1 Norm: 1.5405, Test L1 Norm: 1.4144, Train Linf Norm: 201.5141, Test Linf Norm: 189.9270\n",
            "Epoch 66: Train Loss: 0.0238, Test Loss: 0.0236, Train L1 Norm: 1.5371, Test L1 Norm: 1.4118, Train Linf Norm: 200.6845, Test Linf Norm: 189.6013\n",
            "Epoch 67: Train Loss: 0.0237, Test Loss: 0.0234, Train L1 Norm: 1.5346, Test L1 Norm: 1.4101, Train Linf Norm: 198.6456, Test Linf Norm: 189.4042\n",
            "Epoch 68: Train Loss: 0.0236, Test Loss: 0.0234, Train L1 Norm: 1.5309, Test L1 Norm: 1.4066, Train Linf Norm: 198.9991, Test Linf Norm: 188.9310\n",
            "Epoch 69: Train Loss: 0.0235, Test Loss: 0.0233, Train L1 Norm: 1.5285, Test L1 Norm: 1.4040, Train Linf Norm: 199.3770, Test Linf Norm: 188.6140\n",
            "Epoch 70: Train Loss: 0.0234, Test Loss: 0.0235, Train L1 Norm: 1.5254, Test L1 Norm: 1.4044, Train Linf Norm: 198.6704, Test Linf Norm: 188.6967\n",
            "Epoch 71: Train Loss: 0.0234, Test Loss: 0.0231, Train L1 Norm: 1.5229, Test L1 Norm: 1.4003, Train Linf Norm: 196.0351, Test Linf Norm: 188.1596\n",
            "Epoch 72: Train Loss: 0.0233, Test Loss: 0.0230, Train L1 Norm: 1.5196, Test L1 Norm: 1.3974, Train Linf Norm: 197.8389, Test Linf Norm: 187.7831\n",
            "Epoch 73: Train Loss: 0.0232, Test Loss: 0.0229, Train L1 Norm: 1.5172, Test L1 Norm: 1.3957, Train Linf Norm: 194.1629, Test Linf Norm: 187.5842\n",
            "Epoch 74: Train Loss: 0.0231, Test Loss: 0.0229, Train L1 Norm: 1.5141, Test L1 Norm: 1.3947, Train Linf Norm: 197.0141, Test Linf Norm: 187.4657\n",
            "Epoch 75: Train Loss: 0.0231, Test Loss: 0.0229, Train L1 Norm: 1.5115, Test L1 Norm: 1.3913, Train Linf Norm: 196.5555, Test Linf Norm: 187.0312\n",
            "Epoch 76: Train Loss: 0.0230, Test Loss: 0.0227, Train L1 Norm: 1.5084, Test L1 Norm: 1.3889, Train Linf Norm: 187.7251, Test Linf Norm: 186.7117\n",
            "Epoch 77: Train Loss: 0.0229, Test Loss: 0.0239, Train L1 Norm: 1.5060, Test L1 Norm: 1.3896, Train Linf Norm: 196.3247, Test Linf Norm: 186.8212\n",
            "Epoch 78: Train Loss: 0.0229, Test Loss: 0.0226, Train L1 Norm: 1.5029, Test L1 Norm: 1.3855, Train Linf Norm: 197.6905, Test Linf Norm: 186.3051\n",
            "Epoch 79: Train Loss: 0.0228, Test Loss: 0.0226, Train L1 Norm: 1.5006, Test L1 Norm: 1.3835, Train Linf Norm: 197.9731, Test Linf Norm: 186.0524\n",
            "Epoch 80: Train Loss: 0.0227, Test Loss: 0.0226, Train L1 Norm: 1.4973, Test L1 Norm: 1.3810, Train Linf Norm: 194.9072, Test Linf Norm: 185.7271\n",
            "Epoch 81: Train Loss: 0.0226, Test Loss: 0.0225, Train L1 Norm: 1.4950, Test L1 Norm: 1.3762, Train Linf Norm: 192.1137, Test Linf Norm: 185.0801\n",
            "Epoch 82: Train Loss: 0.0226, Test Loss: 0.0224, Train L1 Norm: 1.4927, Test L1 Norm: 1.3750, Train Linf Norm: 194.8040, Test Linf Norm: 184.9487\n",
            "Epoch 83: Train Loss: 0.0225, Test Loss: 0.0222, Train L1 Norm: 1.4899, Test L1 Norm: 1.3746, Train Linf Norm: 195.3340, Test Linf Norm: 184.9207\n",
            "Epoch 84: Train Loss: 0.0224, Test Loss: 0.0230, Train L1 Norm: 1.4872, Test L1 Norm: 1.3711, Train Linf Norm: 195.0115, Test Linf Norm: 184.4411\n",
            "Epoch 85: Train Loss: 0.0224, Test Loss: 0.0224, Train L1 Norm: 1.4849, Test L1 Norm: 1.3707, Train Linf Norm: 195.5853, Test Linf Norm: 184.4269\n",
            "Epoch 86: Train Loss: 0.0223, Test Loss: 0.0230, Train L1 Norm: 1.4820, Test L1 Norm: 1.3661, Train Linf Norm: 193.6793, Test Linf Norm: 183.7841\n",
            "Epoch 87: Train Loss: 0.0223, Test Loss: 0.0230, Train L1 Norm: 1.4795, Test L1 Norm: 1.3678, Train Linf Norm: 193.4763, Test Linf Norm: 184.0599\n",
            "Epoch 88: Train Loss: 0.0222, Test Loss: 0.0219, Train L1 Norm: 1.4767, Test L1 Norm: 1.3641, Train Linf Norm: 193.5076, Test Linf Norm: 183.5887\n",
            "Epoch 89: Train Loss: 0.0221, Test Loss: 0.0227, Train L1 Norm: 1.4741, Test L1 Norm: 1.3633, Train Linf Norm: 188.9095, Test Linf Norm: 183.4884\n",
            "Epoch 90: Train Loss: 0.0221, Test Loss: 0.0218, Train L1 Norm: 1.4719, Test L1 Norm: 1.3599, Train Linf Norm: 193.3298, Test Linf Norm: 183.0535\n",
            "Epoch 91: Train Loss: 0.0220, Test Loss: 0.0217, Train L1 Norm: 1.4693, Test L1 Norm: 1.3575, Train Linf Norm: 192.6004, Test Linf Norm: 182.7471\n",
            "Epoch 92: Train Loss: 0.0219, Test Loss: 0.0220, Train L1 Norm: 1.4671, Test L1 Norm: 1.3548, Train Linf Norm: 192.9005, Test Linf Norm: 182.3921\n",
            "Epoch 93: Train Loss: 0.0219, Test Loss: 0.0217, Train L1 Norm: 1.4644, Test L1 Norm: 1.3527, Train Linf Norm: 192.2289, Test Linf Norm: 182.1296\n",
            "Epoch 94: Train Loss: 0.0218, Test Loss: 0.0216, Train L1 Norm: 1.4622, Test L1 Norm: 1.3507, Train Linf Norm: 187.8064, Test Linf Norm: 181.8809\n",
            "Epoch 95: Train Loss: 0.0218, Test Loss: 0.0219, Train L1 Norm: 1.4597, Test L1 Norm: 1.3509, Train Linf Norm: 191.4615, Test Linf Norm: 181.9175\n",
            "Epoch 96: Train Loss: 0.0217, Test Loss: 0.0214, Train L1 Norm: 1.4574, Test L1 Norm: 1.3473, Train Linf Norm: 191.7628, Test Linf Norm: 181.4525\n",
            "Epoch 97: Train Loss: 0.0216, Test Loss: 0.0214, Train L1 Norm: 1.4547, Test L1 Norm: 1.3475, Train Linf Norm: 184.2880, Test Linf Norm: 181.5074\n",
            "Epoch 98: Train Loss: 0.0216, Test Loss: 0.0218, Train L1 Norm: 1.4526, Test L1 Norm: 1.3457, Train Linf Norm: 186.5609, Test Linf Norm: 181.2719\n",
            "Epoch 99: Train Loss: 0.0215, Test Loss: 0.0222, Train L1 Norm: 1.4506, Test L1 Norm: 1.3408, Train Linf Norm: 191.3013, Test Linf Norm: 180.5900\n",
            "Epoch 100: Train Loss: 0.0215, Test Loss: 0.0213, Train L1 Norm: 1.4481, Test L1 Norm: 1.3397, Train Linf Norm: 187.2900, Test Linf Norm: 180.4999\n",
            "Epoch 101: Train Loss: 0.0214, Test Loss: 0.0215, Train L1 Norm: 1.4458, Test L1 Norm: 1.3394, Train Linf Norm: 180.8977, Test Linf Norm: 180.4683\n",
            "Epoch 102: Train Loss: 0.0214, Test Loss: 0.0211, Train L1 Norm: 1.4430, Test L1 Norm: 1.3374, Train Linf Norm: 188.6636, Test Linf Norm: 180.2182\n",
            "Epoch 103: Train Loss: 0.0213, Test Loss: 0.0213, Train L1 Norm: 1.4411, Test L1 Norm: 1.3355, Train Linf Norm: 188.5487, Test Linf Norm: 179.9681\n",
            "Epoch 104: Train Loss: 0.0213, Test Loss: 0.0212, Train L1 Norm: 1.4388, Test L1 Norm: 1.3321, Train Linf Norm: 189.0684, Test Linf Norm: 179.5174\n",
            "Epoch 105: Train Loss: 0.0212, Test Loss: 0.0210, Train L1 Norm: 1.4366, Test L1 Norm: 1.3318, Train Linf Norm: 187.4702, Test Linf Norm: 179.5101\n",
            "Epoch 106: Train Loss: 0.0212, Test Loss: 0.0209, Train L1 Norm: 1.4343, Test L1 Norm: 1.3277, Train Linf Norm: 186.0212, Test Linf Norm: 178.9563\n",
            "Epoch 107: Train Loss: 0.0211, Test Loss: 0.0210, Train L1 Norm: 1.4319, Test L1 Norm: 1.3273, Train Linf Norm: 186.3755, Test Linf Norm: 178.9095\n",
            "Epoch 108: Train Loss: 0.0210, Test Loss: 0.0209, Train L1 Norm: 1.4298, Test L1 Norm: 1.3258, Train Linf Norm: 187.1675, Test Linf Norm: 178.7413\n",
            "Epoch 109: Train Loss: 0.0210, Test Loss: 0.0208, Train L1 Norm: 1.4276, Test L1 Norm: 1.3236, Train Linf Norm: 186.7667, Test Linf Norm: 178.4479\n",
            "Epoch 110: Train Loss: 0.0209, Test Loss: 0.0212, Train L1 Norm: 1.4254, Test L1 Norm: 1.3234, Train Linf Norm: 184.1872, Test Linf Norm: 178.4423\n",
            "Epoch 111: Train Loss: 0.0209, Test Loss: 0.0208, Train L1 Norm: 1.4233, Test L1 Norm: 1.3195, Train Linf Norm: 185.7143, Test Linf Norm: 177.9130\n",
            "Epoch 112: Train Loss: 0.0208, Test Loss: 0.0206, Train L1 Norm: 1.4211, Test L1 Norm: 1.3184, Train Linf Norm: 186.8105, Test Linf Norm: 177.7915\n",
            "Epoch 113: Train Loss: 0.0208, Test Loss: 0.0211, Train L1 Norm: 1.4189, Test L1 Norm: 1.3173, Train Linf Norm: 187.2716, Test Linf Norm: 177.6500\n",
            "Epoch 114: Train Loss: 0.0207, Test Loss: 0.0205, Train L1 Norm: 1.4171, Test L1 Norm: 1.3151, Train Linf Norm: 183.4076, Test Linf Norm: 177.3826\n",
            "Epoch 115: Train Loss: 0.0207, Test Loss: 0.0217, Train L1 Norm: 1.4148, Test L1 Norm: 1.3117, Train Linf Norm: 185.5085, Test Linf Norm: 176.8828\n",
            "Epoch 116: Train Loss: 0.0206, Test Loss: 0.0204, Train L1 Norm: 1.4128, Test L1 Norm: 1.3123, Train Linf Norm: 183.9921, Test Linf Norm: 177.0224\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:07:03,782]\u001b[0m Trial 4 finished with value: 1.3105429499037564 and parameters: {'n_layers': 2, 'n_units_0': 347, 'n_units_1': 50, 'hidden_activation': 'ELU', 'output_activation': 'Softplus', 'loss': 'MAE', 'optimizer': 'Adagrad', 'lr': 0.0006115769175605397, 'batch_size': 159, 'n_epochs': 117, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.33429561612706304, 'patience': 7, 'threshold': 0.007681597415202573}. Best is trial 3 with value: 0.11888195726890118.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 117: Train Loss: 0.0206, Test Loss: 0.0207, Train L1 Norm: 1.4105, Test L1 Norm: 1.3105, Train Linf Norm: 184.7867, Test Linf Norm: 176.7923\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-59-d4ca07a22bfc>:117: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
            "<ipython-input-59-d4ca07a22bfc>:118: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  beta1 = trial.suggest_uniform(\"beta1\", 0.5, 0.99)\n",
            "<ipython-input-59-d4ca07a22bfc>:119: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  beta2 = trial.suggest_uniform(\"beta2\", 0.9, 0.999)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.1916, Test Loss: 0.1025, Train L1 Norm: 0.2335, Test L1 Norm: 0.1277, Train Linf Norm: 6.9781, Test Linf Norm: 3.0678\n",
            "Epoch 2: Train Loss: 0.0834, Test Loss: 0.0690, Train L1 Norm: 0.1645, Test L1 Norm: 0.1031, Train Linf Norm: 5.8601, Test Linf Norm: 2.4469\n",
            "Epoch 3: Train Loss: 0.0855, Test Loss: 0.0819, Train L1 Norm: 0.1505, Test L1 Norm: 0.1009, Train Linf Norm: 5.1029, Test Linf Norm: 1.3341\n",
            "Epoch 4: Train Loss: 0.0851, Test Loss: 0.2027, Train L1 Norm: 0.1106, Test L1 Norm: 0.5347, Train Linf Norm: 2.5021, Test Linf Norm: 28.2717\n",
            "Epoch 5: Train Loss: 0.0849, Test Loss: 0.0773, Train L1 Norm: 0.1431, Test L1 Norm: 0.0891, Train Linf Norm: 4.9175, Test Linf Norm: 1.5280\n",
            "Epoch 6: Train Loss: 0.0844, Test Loss: 0.0404, Train L1 Norm: 0.1261, Test L1 Norm: 0.0476, Train Linf Norm: 3.5455, Test Linf Norm: 1.2905\n",
            "Epoch 7: Train Loss: 0.0901, Test Loss: 0.0758, Train L1 Norm: 0.1163, Test L1 Norm: 0.0694, Train Linf Norm: 2.5950, Test Linf Norm: 1.0928\n",
            "Epoch 8: Train Loss: 0.0818, Test Loss: 0.0719, Train L1 Norm: 0.1338, Test L1 Norm: 0.1793, Train Linf Norm: 4.4008, Test Linf Norm: 9.1634\n",
            "Epoch 9: Train Loss: 0.0877, Test Loss: 0.0520, Train L1 Norm: 0.1466, Test L1 Norm: 0.2336, Train Linf Norm: 4.9792, Test Linf Norm: 12.3608\n",
            "Epoch 10: Train Loss: 0.0883, Test Loss: 0.0529, Train L1 Norm: 0.1486, Test L1 Norm: 0.0861, Train Linf Norm: 5.1230, Test Linf Norm: 1.0737\n",
            "Epoch 11: Train Loss: 0.0872, Test Loss: 0.1675, Train L1 Norm: 0.1561, Test L1 Norm: 0.1146, Train Linf Norm: 5.7123, Test Linf Norm: 0.9880\n",
            "Epoch 12: Train Loss: 0.0841, Test Loss: 0.0546, Train L1 Norm: 0.1222, Test L1 Norm: 0.0900, Train Linf Norm: 3.7127, Test Linf Norm: 1.9228\n",
            "Epoch 13: Train Loss: 0.0849, Test Loss: 0.1177, Train L1 Norm: 0.1372, Test L1 Norm: 0.0859, Train Linf Norm: 4.6381, Test Linf Norm: 1.1484\n",
            "Epoch 14: Train Loss: 0.0278, Test Loss: 0.0200, Train L1 Norm: 0.0545, Test L1 Norm: 0.0382, Train Linf Norm: 1.9247, Test Linf Norm: 1.2734\n",
            "Epoch 15: Train Loss: 0.0260, Test Loss: 0.0327, Train L1 Norm: 0.0469, Test L1 Norm: 0.0371, Train Linf Norm: 1.3681, Test Linf Norm: 0.9881\n",
            "Epoch 16: Train Loss: 0.0273, Test Loss: 0.0255, Train L1 Norm: 0.0535, Test L1 Norm: 0.0436, Train Linf Norm: 1.8669, Test Linf Norm: 1.2081\n",
            "Epoch 17: Train Loss: 0.0261, Test Loss: 0.0256, Train L1 Norm: 0.0567, Test L1 Norm: 0.0652, Train Linf Norm: 2.1438, Test Linf Norm: 1.3697\n",
            "Epoch 18: Train Loss: 0.0254, Test Loss: 0.0255, Train L1 Norm: 0.0597, Test L1 Norm: 0.0437, Train Linf Norm: 2.3293, Test Linf Norm: 1.0658\n",
            "Epoch 19: Train Loss: 0.0255, Test Loss: 0.0279, Train L1 Norm: 0.0496, Test L1 Norm: 0.0405, Train Linf Norm: 1.6291, Test Linf Norm: 1.0489\n",
            "Epoch 20: Train Loss: 0.0270, Test Loss: 0.0200, Train L1 Norm: 0.0540, Test L1 Norm: 0.0333, Train Linf Norm: 1.8445, Test Linf Norm: 0.7879\n",
            "Epoch 21: Train Loss: 0.0278, Test Loss: 0.0235, Train L1 Norm: 0.0527, Test L1 Norm: 0.0379, Train Linf Norm: 1.6969, Test Linf Norm: 0.8274\n",
            "Epoch 22: Train Loss: 0.0271, Test Loss: 0.0402, Train L1 Norm: 0.0534, Test L1 Norm: 0.0350, Train Linf Norm: 1.7532, Test Linf Norm: 0.7961\n",
            "Epoch 23: Train Loss: 0.0286, Test Loss: 0.0305, Train L1 Norm: 0.0596, Test L1 Norm: 0.0385, Train Linf Norm: 2.1631, Test Linf Norm: 0.8887\n",
            "Epoch 24: Train Loss: 0.0268, Test Loss: 0.0221, Train L1 Norm: 0.0541, Test L1 Norm: 0.0368, Train Linf Norm: 1.9063, Test Linf Norm: 0.8619\n",
            "Epoch 25: Train Loss: 0.0287, Test Loss: 0.0510, Train L1 Norm: 0.0731, Test L1 Norm: 0.0525, Train Linf Norm: 3.3458, Test Linf Norm: 0.8313\n",
            "Epoch 26: Train Loss: 0.0289, Test Loss: 0.0247, Train L1 Norm: 0.0495, Test L1 Norm: 0.0327, Train Linf Norm: 1.4814, Test Linf Norm: 0.9449\n",
            "Epoch 27: Train Loss: 0.0268, Test Loss: 0.0166, Train L1 Norm: 0.0511, Test L1 Norm: 0.0343, Train Linf Norm: 1.6553, Test Linf Norm: 1.1583\n",
            "Epoch 28: Train Loss: 0.0294, Test Loss: 0.0185, Train L1 Norm: 0.0577, Test L1 Norm: 0.0396, Train Linf Norm: 2.0159, Test Linf Norm: 1.6357\n",
            "Epoch 29: Train Loss: 0.0274, Test Loss: 0.0318, Train L1 Norm: 0.0559, Test L1 Norm: 0.0385, Train Linf Norm: 1.9725, Test Linf Norm: 0.8490\n",
            "Epoch 30: Train Loss: 0.0287, Test Loss: 0.0494, Train L1 Norm: 0.0637, Test L1 Norm: 0.0612, Train Linf Norm: 2.4910, Test Linf Norm: 0.8835\n",
            "Epoch 31: Train Loss: 0.0283, Test Loss: 0.0347, Train L1 Norm: 0.0752, Test L1 Norm: 0.0351, Train Linf Norm: 3.3835, Test Linf Norm: 0.7673\n",
            "Epoch 32: Train Loss: 0.0261, Test Loss: 0.0195, Train L1 Norm: 0.0495, Test L1 Norm: 0.0292, Train Linf Norm: 1.6258, Test Linf Norm: 0.8421\n",
            "Epoch 33: Train Loss: 0.0293, Test Loss: 0.0333, Train L1 Norm: 0.0572, Test L1 Norm: 0.0388, Train Linf Norm: 2.0106, Test Linf Norm: 1.0518\n",
            "Epoch 34: Train Loss: 0.0269, Test Loss: 0.0212, Train L1 Norm: 0.0831, Test L1 Norm: 0.0336, Train Linf Norm: 4.2576, Test Linf Norm: 0.9132\n",
            "Epoch 35: Train Loss: 0.0143, Test Loss: 0.0131, Train L1 Norm: 0.0406, Test L1 Norm: 0.0274, Train Linf Norm: 1.6373, Test Linf Norm: 0.8316\n",
            "Epoch 36: Train Loss: 0.0142, Test Loss: 0.0180, Train L1 Norm: 0.0349, Test L1 Norm: 0.0315, Train Linf Norm: 1.1676, Test Linf Norm: 0.9272\n",
            "Epoch 37: Train Loss: 0.0132, Test Loss: 0.0136, Train L1 Norm: 0.0332, Test L1 Norm: 0.0287, Train Linf Norm: 1.0833, Test Linf Norm: 0.8011\n",
            "Epoch 38: Train Loss: 0.0135, Test Loss: 0.0166, Train L1 Norm: 0.0375, Test L1 Norm: 0.0292, Train Linf Norm: 1.3977, Test Linf Norm: 0.8185\n",
            "Epoch 39: Train Loss: 0.0137, Test Loss: 0.0169, Train L1 Norm: 0.0341, Test L1 Norm: 0.0314, Train Linf Norm: 1.1272, Test Linf Norm: 0.8358\n",
            "Epoch 40: Train Loss: 0.0134, Test Loss: 0.0114, Train L1 Norm: 0.0337, Test L1 Norm: 0.0279, Train Linf Norm: 1.1258, Test Linf Norm: 0.8185\n",
            "Epoch 41: Train Loss: 0.0135, Test Loss: 0.0139, Train L1 Norm: 0.0360, Test L1 Norm: 0.0323, Train Linf Norm: 1.2944, Test Linf Norm: 1.0866\n",
            "Epoch 42: Train Loss: 0.0131, Test Loss: 0.0160, Train L1 Norm: 0.0334, Test L1 Norm: 0.0301, Train Linf Norm: 1.1109, Test Linf Norm: 0.8344\n",
            "Epoch 43: Train Loss: 0.0132, Test Loss: 0.0136, Train L1 Norm: 0.0371, Test L1 Norm: 0.0291, Train Linf Norm: 1.3977, Test Linf Norm: 0.9511\n",
            "Epoch 44: Train Loss: 0.0127, Test Loss: 0.0134, Train L1 Norm: 0.0334, Test L1 Norm: 0.0290, Train Linf Norm: 1.0984, Test Linf Norm: 0.8248\n",
            "Epoch 45: Train Loss: 0.0130, Test Loss: 0.0129, Train L1 Norm: 0.0339, Test L1 Norm: 0.0288, Train Linf Norm: 1.1327, Test Linf Norm: 0.8191\n",
            "Epoch 46: Train Loss: 0.0129, Test Loss: 0.0146, Train L1 Norm: 0.0342, Test L1 Norm: 0.0278, Train Linf Norm: 1.1670, Test Linf Norm: 0.8109\n",
            "Epoch 47: Train Loss: 0.0126, Test Loss: 0.0148, Train L1 Norm: 0.0332, Test L1 Norm: 0.0287, Train Linf Norm: 1.0903, Test Linf Norm: 0.8408\n",
            "Epoch 48: Train Loss: 0.0108, Test Loss: 0.0103, Train L1 Norm: 0.0316, Test L1 Norm: 0.0263, Train Linf Norm: 1.0921, Test Linf Norm: 0.8273\n",
            "Epoch 49: Train Loss: 0.0107, Test Loss: 0.0107, Train L1 Norm: 0.0319, Test L1 Norm: 0.0270, Train Linf Norm: 1.1067, Test Linf Norm: 0.8157\n",
            "Epoch 50: Train Loss: 0.0106, Test Loss: 0.0105, Train L1 Norm: 0.0311, Test L1 Norm: 0.0262, Train Linf Norm: 1.0569, Test Linf Norm: 0.8173\n",
            "Epoch 51: Train Loss: 0.0107, Test Loss: 0.0103, Train L1 Norm: 0.0313, Test L1 Norm: 0.0261, Train Linf Norm: 1.0452, Test Linf Norm: 0.8143\n",
            "Epoch 52: Train Loss: 0.0107, Test Loss: 0.0113, Train L1 Norm: 0.0309, Test L1 Norm: 0.0268, Train Linf Norm: 1.0320, Test Linf Norm: 0.8209\n",
            "Epoch 53: Train Loss: 0.0107, Test Loss: 0.0112, Train L1 Norm: 0.0319, Test L1 Norm: 0.0280, Train Linf Norm: 1.0972, Test Linf Norm: 0.8239\n",
            "Epoch 54: Train Loss: 0.0106, Test Loss: 0.0104, Train L1 Norm: 0.0311, Test L1 Norm: 0.0267, Train Linf Norm: 1.0605, Test Linf Norm: 0.8288\n",
            "Epoch 55: Train Loss: 0.0106, Test Loss: 0.0103, Train L1 Norm: 0.0312, Test L1 Norm: 0.0266, Train Linf Norm: 1.0640, Test Linf Norm: 0.8167\n",
            "Epoch 56: Train Loss: 0.0105, Test Loss: 0.0108, Train L1 Norm: 0.0317, Test L1 Norm: 0.0266, Train Linf Norm: 1.1068, Test Linf Norm: 0.8302\n",
            "Epoch 57: Train Loss: 0.0105, Test Loss: 0.0102, Train L1 Norm: 0.0311, Test L1 Norm: 0.0259, Train Linf Norm: 1.0550, Test Linf Norm: 0.8091\n",
            "Epoch 58: Train Loss: 0.0105, Test Loss: 0.0103, Train L1 Norm: 0.0312, Test L1 Norm: 0.0265, Train Linf Norm: 1.0675, Test Linf Norm: 0.8114\n",
            "Epoch 59: Train Loss: 0.0104, Test Loss: 0.0103, Train L1 Norm: 0.0310, Test L1 Norm: 0.0267, Train Linf Norm: 1.0628, Test Linf Norm: 0.8260\n",
            "Epoch 60: Train Loss: 0.0103, Test Loss: 0.0103, Train L1 Norm: 0.0311, Test L1 Norm: 0.0268, Train Linf Norm: 1.0809, Test Linf Norm: 0.8101\n",
            "Epoch 61: Train Loss: 0.0104, Test Loss: 0.0099, Train L1 Norm: 0.0306, Test L1 Norm: 0.0265, Train Linf Norm: 1.0282, Test Linf Norm: 0.8106\n",
            "Epoch 62: Train Loss: 0.0104, Test Loss: 0.0109, Train L1 Norm: 0.0309, Test L1 Norm: 0.0259, Train Linf Norm: 1.0531, Test Linf Norm: 0.8080\n",
            "Epoch 63: Train Loss: 0.0103, Test Loss: 0.0102, Train L1 Norm: 0.0308, Test L1 Norm: 0.0274, Train Linf Norm: 1.0472, Test Linf Norm: 0.8177\n",
            "Epoch 64: Train Loss: 0.0104, Test Loss: 0.0109, Train L1 Norm: 0.0310, Test L1 Norm: 0.0269, Train Linf Norm: 1.0667, Test Linf Norm: 0.8216\n",
            "Epoch 65: Train Loss: 0.0103, Test Loss: 0.0100, Train L1 Norm: 0.0309, Test L1 Norm: 0.0262, Train Linf Norm: 1.0580, Test Linf Norm: 0.8124\n",
            "Epoch 66: Train Loss: 0.0103, Test Loss: 0.0101, Train L1 Norm: 0.0311, Test L1 Norm: 0.0267, Train Linf Norm: 1.0753, Test Linf Norm: 0.8150\n",
            "Epoch 67: Train Loss: 0.0102, Test Loss: 0.0102, Train L1 Norm: 0.0313, Test L1 Norm: 0.0257, Train Linf Norm: 1.0922, Test Linf Norm: 0.8086\n",
            "Epoch 68: Train Loss: 0.0102, Test Loss: 0.0098, Train L1 Norm: 0.0307, Test L1 Norm: 0.0260, Train Linf Norm: 1.0458, Test Linf Norm: 0.8124\n",
            "Epoch 69: Train Loss: 0.0103, Test Loss: 0.0099, Train L1 Norm: 0.0316, Test L1 Norm: 0.0263, Train Linf Norm: 1.0945, Test Linf Norm: 0.8151\n",
            "Epoch 70: Train Loss: 0.0102, Test Loss: 0.0098, Train L1 Norm: 0.0306, Test L1 Norm: 0.0263, Train Linf Norm: 1.0421, Test Linf Norm: 0.8107\n",
            "Epoch 71: Train Loss: 0.0101, Test Loss: 0.0101, Train L1 Norm: 0.0309, Test L1 Norm: 0.0260, Train Linf Norm: 1.0718, Test Linf Norm: 0.8128\n",
            "Epoch 72: Train Loss: 0.0101, Test Loss: 0.0101, Train L1 Norm: 0.0314, Test L1 Norm: 0.0257, Train Linf Norm: 1.1048, Test Linf Norm: 0.8090\n",
            "Epoch 73: Train Loss: 0.0101, Test Loss: 0.0098, Train L1 Norm: 0.0300, Test L1 Norm: 0.0260, Train Linf Norm: 0.9904, Test Linf Norm: 0.8155\n",
            "Epoch 74: Train Loss: 0.0101, Test Loss: 0.0097, Train L1 Norm: 0.0312, Test L1 Norm: 0.0263, Train Linf Norm: 1.0834, Test Linf Norm: 0.8214\n",
            "Epoch 75: Train Loss: 0.0100, Test Loss: 0.0097, Train L1 Norm: 0.0303, Test L1 Norm: 0.0257, Train Linf Norm: 1.0231, Test Linf Norm: 0.8165\n",
            "Epoch 76: Train Loss: 0.0101, Test Loss: 0.0108, Train L1 Norm: 0.0308, Test L1 Norm: 0.0269, Train Linf Norm: 1.0848, Test Linf Norm: 0.8382\n",
            "Epoch 77: Train Loss: 0.0101, Test Loss: 0.0106, Train L1 Norm: 0.0308, Test L1 Norm: 0.0266, Train Linf Norm: 1.0643, Test Linf Norm: 0.8224\n",
            "Epoch 78: Train Loss: 0.0101, Test Loss: 0.0096, Train L1 Norm: 0.0307, Test L1 Norm: 0.0260, Train Linf Norm: 1.0553, Test Linf Norm: 0.8128\n",
            "Epoch 79: Train Loss: 0.0100, Test Loss: 0.0101, Train L1 Norm: 0.0303, Test L1 Norm: 0.0263, Train Linf Norm: 1.0412, Test Linf Norm: 0.8124\n",
            "Epoch 80: Train Loss: 0.0100, Test Loss: 0.0097, Train L1 Norm: 0.0306, Test L1 Norm: 0.0262, Train Linf Norm: 1.0525, Test Linf Norm: 0.8098\n",
            "Epoch 81: Train Loss: 0.0100, Test Loss: 0.0101, Train L1 Norm: 0.0305, Test L1 Norm: 0.0264, Train Linf Norm: 1.0493, Test Linf Norm: 0.8172\n",
            "Epoch 82: Train Loss: 0.0099, Test Loss: 0.0102, Train L1 Norm: 0.0313, Test L1 Norm: 0.0255, Train Linf Norm: 1.1016, Test Linf Norm: 0.8161\n",
            "Epoch 83: Train Loss: 0.0099, Test Loss: 0.0098, Train L1 Norm: 0.0304, Test L1 Norm: 0.0257, Train Linf Norm: 1.0406, Test Linf Norm: 0.8023\n",
            "Epoch 84: Train Loss: 0.0099, Test Loss: 0.0097, Train L1 Norm: 0.0303, Test L1 Norm: 0.0259, Train Linf Norm: 1.0324, Test Linf Norm: 0.8222\n",
            "Epoch 85: Train Loss: 0.0100, Test Loss: 0.0102, Train L1 Norm: 0.0309, Test L1 Norm: 0.0261, Train Linf Norm: 1.0702, Test Linf Norm: 0.8064\n",
            "Epoch 86: Train Loss: 0.0095, Test Loss: 0.0097, Train L1 Norm: 0.0299, Test L1 Norm: 0.0259, Train Linf Norm: 1.0198, Test Linf Norm: 0.8090\n",
            "Epoch 87: Train Loss: 0.0094, Test Loss: 0.0094, Train L1 Norm: 0.0302, Test L1 Norm: 0.0257, Train Linf Norm: 1.0356, Test Linf Norm: 0.8152\n",
            "Epoch 88: Train Loss: 0.0094, Test Loss: 0.0096, Train L1 Norm: 0.0300, Test L1 Norm: 0.0257, Train Linf Norm: 1.0424, Test Linf Norm: 0.8145\n",
            "Epoch 89: Train Loss: 0.0094, Test Loss: 0.0094, Train L1 Norm: 0.0301, Test L1 Norm: 0.0259, Train Linf Norm: 1.0380, Test Linf Norm: 0.8134\n",
            "Epoch 90: Train Loss: 0.0094, Test Loss: 0.0094, Train L1 Norm: 0.0303, Test L1 Norm: 0.0257, Train Linf Norm: 1.0544, Test Linf Norm: 0.8123\n",
            "Epoch 91: Train Loss: 0.0094, Test Loss: 0.0097, Train L1 Norm: 0.0302, Test L1 Norm: 0.0261, Train Linf Norm: 1.0530, Test Linf Norm: 0.8164\n",
            "Epoch 92: Train Loss: 0.0094, Test Loss: 0.0094, Train L1 Norm: 0.0302, Test L1 Norm: 0.0254, Train Linf Norm: 1.0610, Test Linf Norm: 0.8103\n",
            "Epoch 93: Train Loss: 0.0094, Test Loss: 0.0094, Train L1 Norm: 0.0301, Test L1 Norm: 0.0255, Train Linf Norm: 1.0376, Test Linf Norm: 0.8117\n",
            "Epoch 94: Train Loss: 0.0094, Test Loss: 0.0094, Train L1 Norm: 0.0301, Test L1 Norm: 0.0259, Train Linf Norm: 1.0439, Test Linf Norm: 0.8133\n",
            "Epoch 95: Train Loss: 0.0093, Test Loss: 0.0094, Train L1 Norm: 0.0300, Test L1 Norm: 0.0258, Train Linf Norm: 1.0319, Test Linf Norm: 0.8123\n",
            "Epoch 96: Train Loss: 0.0093, Test Loss: 0.0093, Train L1 Norm: 0.0301, Test L1 Norm: 0.0257, Train Linf Norm: 1.0491, Test Linf Norm: 0.8156\n",
            "Epoch 97: Train Loss: 0.0093, Test Loss: 0.0094, Train L1 Norm: 0.0301, Test L1 Norm: 0.0258, Train Linf Norm: 1.0375, Test Linf Norm: 0.8147\n",
            "Epoch 98: Train Loss: 0.0093, Test Loss: 0.0094, Train L1 Norm: 0.0302, Test L1 Norm: 0.0259, Train Linf Norm: 1.0607, Test Linf Norm: 0.8124\n",
            "Epoch 99: Train Loss: 0.0093, Test Loss: 0.0093, Train L1 Norm: 0.0300, Test L1 Norm: 0.0258, Train Linf Norm: 1.0387, Test Linf Norm: 0.8153\n",
            "Epoch 100: Train Loss: 0.0093, Test Loss: 0.0093, Train L1 Norm: 0.0301, Test L1 Norm: 0.0258, Train Linf Norm: 1.0561, Test Linf Norm: 0.8121\n",
            "Epoch 101: Train Loss: 0.0093, Test Loss: 0.0093, Train L1 Norm: 0.0304, Test L1 Norm: 0.0257, Train Linf Norm: 1.0621, Test Linf Norm: 0.8126\n",
            "Epoch 102: Train Loss: 0.0093, Test Loss: 0.0094, Train L1 Norm: 0.0301, Test L1 Norm: 0.0255, Train Linf Norm: 1.0360, Test Linf Norm: 0.8131\n",
            "Epoch 103: Train Loss: 0.0093, Test Loss: 0.0093, Train L1 Norm: 0.0299, Test L1 Norm: 0.0258, Train Linf Norm: 1.0379, Test Linf Norm: 0.8151\n",
            "Epoch 104: Train Loss: 0.0092, Test Loss: 0.0093, Train L1 Norm: 0.0301, Test L1 Norm: 0.0258, Train Linf Norm: 1.0430, Test Linf Norm: 0.8145\n",
            "Epoch 105: Train Loss: 0.0092, Test Loss: 0.0093, Train L1 Norm: 0.0301, Test L1 Norm: 0.0257, Train Linf Norm: 1.0480, Test Linf Norm: 0.8142\n",
            "Epoch 106: Train Loss: 0.0092, Test Loss: 0.0093, Train L1 Norm: 0.0300, Test L1 Norm: 0.0257, Train Linf Norm: 1.0534, Test Linf Norm: 0.8136\n",
            "Epoch 107: Train Loss: 0.0092, Test Loss: 0.0093, Train L1 Norm: 0.0301, Test L1 Norm: 0.0257, Train Linf Norm: 1.0489, Test Linf Norm: 0.8151\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:11:39,590]\u001b[0m Trial 5 finished with value: 0.02577756353840232 and parameters: {'n_layers': 2, 'n_units_0': 138, 'n_units_1': 71, 'hidden_activation': 'Tanh', 'output_activation': 'ReLU', 'loss': 'MAE', 'optimizer': 'Adam', 'lr': 0.0028981760687089996, 'batch_size': 80, 'n_epochs': 108, 'scheduler': 'ReduceLROnPlateau', 'weight_decay': 0.004561101425900267, 'beta1': 0.9343122102113666, 'beta2': 0.9382812134576558, 'factor': 0.21271303955399237, 'patience': 6, 'threshold': 0.0026230393601939504}. Best is trial 5 with value: 0.02577756353840232.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 108: Train Loss: 0.0092, Test Loss: 0.0093, Train L1 Norm: 0.0301, Test L1 Norm: 0.0258, Train Linf Norm: 1.0581, Test Linf Norm: 0.8144\n",
            "Epoch 1: Train Loss: 0.7151, Test Loss: 0.0116, Train L1 Norm: 0.8212, Test L1 Norm: 0.4202, Train Linf Norm: 24.9960, Test Linf Norm: 13.7528\n",
            "Epoch 2: Train Loss: 0.0352, Test Loss: 0.0323, Train L1 Norm: 0.3924, Test L1 Norm: 0.3661, Train Linf Norm: 12.5090, Test Linf Norm: 11.5798\n",
            "Epoch 3: Train Loss: 0.0142, Test Loss: 0.0022, Train L1 Norm: 0.3210, Test L1 Norm: 0.2645, Train Linf Norm: 10.2610, Test Linf Norm: 8.5082\n",
            "Epoch 4: Train Loss: 0.0074, Test Loss: 0.0010, Train L1 Norm: 0.2799, Test L1 Norm: 0.2652, Train Linf Norm: 9.1526, Test Linf Norm: 8.9399\n",
            "Epoch 5: Train Loss: 0.0040, Test Loss: 0.0009, Train L1 Norm: 0.2524, Test L1 Norm: 0.2547, Train Linf Norm: 8.2731, Test Linf Norm: 8.5904\n",
            "Epoch 6: Train Loss: 0.0025, Test Loss: 0.0132, Train L1 Norm: 0.2365, Test L1 Norm: 0.2461, Train Linf Norm: 7.8499, Test Linf Norm: 7.8809\n",
            "Epoch 7: Train Loss: 0.0017, Test Loss: 0.0005, Train L1 Norm: 0.2255, Test L1 Norm: 0.2096, Train Linf Norm: 7.5161, Test Linf Norm: 7.0813\n",
            "Epoch 8: Train Loss: 0.0010, Test Loss: 0.0001, Train L1 Norm: 0.2172, Test L1 Norm: 0.2087, Train Linf Norm: 7.2663, Test Linf Norm: 7.1745\n",
            "Epoch 9: Train Loss: 0.0007, Test Loss: 0.0043, Train L1 Norm: 0.2087, Test L1 Norm: 0.2290, Train Linf Norm: 7.0018, Test Linf Norm: 7.3949\n",
            "Epoch 10: Train Loss: 0.0004, Test Loss: 0.0001, Train L1 Norm: 0.2016, Test L1 Norm: 0.1945, Train Linf Norm: 6.7805, Test Linf Norm: 6.6718\n",
            "Epoch 11: Train Loss: 0.0003, Test Loss: 0.0006, Train L1 Norm: 0.1956, Test L1 Norm: 0.1853, Train Linf Norm: 6.5908, Test Linf Norm: 6.1411\n",
            "Epoch 12: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.1889, Test L1 Norm: 0.1792, Train Linf Norm: 6.3999, Test Linf Norm: 6.1430\n",
            "Epoch 13: Train Loss: 0.0001, Test Loss: 0.0004, Train L1 Norm: 0.1853, Test L1 Norm: 0.1798, Train Linf Norm: 6.2712, Test Linf Norm: 6.0999\n",
            "Epoch 14: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.1814, Test L1 Norm: 0.1738, Train Linf Norm: 6.1816, Test Linf Norm: 5.9822\n",
            "Epoch 15: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.1772, Test L1 Norm: 0.1672, Train Linf Norm: 6.0234, Test Linf Norm: 5.6752\n",
            "Epoch 16: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.1741, Test L1 Norm: 0.1679, Train Linf Norm: 5.9505, Test Linf Norm: 5.7806\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.1718, Test L1 Norm: 0.1686, Train Linf Norm: 5.8714, Test Linf Norm: 5.8030\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.1692, Test L1 Norm: 0.1664, Train Linf Norm: 5.7415, Test Linf Norm: 5.7582\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.1671, Test L1 Norm: 0.1634, Train Linf Norm: 5.7104, Test Linf Norm: 5.6500\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.1645, Test L1 Norm: 0.1638, Train Linf Norm: 5.5753, Test Linf Norm: 5.6716\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:13:16,468]\u001b[0m Trial 6 finished with value: 0.16164853193033488 and parameters: {'n_layers': 2, 'n_units_0': 501, 'n_units_1': 932, 'hidden_activation': 'LeakyReLU', 'output_activation': 'Softplus', 'loss': 'MSE', 'optimizer': 'RMSprop', 'lr': 0.00043715980037579435, 'batch_size': 36, 'n_epochs': 21, 'scheduler': 'ExponentialLR', 'gamma': 0.8192801800449905}. Best is trial 5 with value: 0.02577756353840232.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.1630, Test L1 Norm: 0.1616, Train Linf Norm: 5.5759, Test Linf Norm: 5.5910\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:13:19,323]\u001b[0m Trial 7 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0422, Test Loss: 0.0092, Train L1 Norm: 3.5094, Test L1 Norm: 2.1321, Train Linf Norm: 216.1248, Test Linf Norm: 130.9485\n",
            "Epoch 1: Train Loss: 0.1122, Test Loss: 0.1810, Train L1 Norm: 0.5571, Test L1 Norm: 0.3728, Train Linf Norm: 21.9629, Test Linf Norm: 1.0000\n",
            "Epoch 2: Train Loss: 0.0539, Test Loss: 0.0104, Train L1 Norm: 0.2777, Test L1 Norm: 0.1089, Train Linf Norm: 5.3108, Test Linf Norm: 1.0131\n",
            "Epoch 3: Train Loss: 0.0387, Test Loss: 0.0089, Train L1 Norm: 0.5628, Test L1 Norm: 0.0830, Train Linf Norm: 32.8567, Test Linf Norm: 1.3687\n",
            "Epoch 4: Train Loss: 0.0284, Test Loss: 0.0398, Train L1 Norm: 0.2673, Test L1 Norm: 0.1553, Train Linf Norm: 7.8568, Test Linf Norm: 1.0850\n",
            "Epoch 5: Train Loss: 0.0201, Test Loss: 0.0056, Train L1 Norm: 0.6611, Test L1 Norm: 0.0739, Train Linf Norm: 45.1386, Test Linf Norm: 1.1321\n",
            "Epoch 6: Train Loss: 0.0136, Test Loss: 0.0026, Train L1 Norm: 0.2153, Test L1 Norm: 0.0597, Train Linf Norm: 7.0625, Test Linf Norm: 1.1879\n",
            "Epoch 7: Train Loss: 0.0082, Test Loss: 0.0025, Train L1 Norm: 0.3081, Test L1 Norm: 0.0920, Train Linf Norm: 17.1749, Test Linf Norm: 1.0700\n",
            "Epoch 8: Train Loss: 0.0047, Test Loss: 0.0013, Train L1 Norm: 0.1699, Test L1 Norm: 0.0582, Train Linf Norm: 6.8932, Test Linf Norm: 1.8157\n",
            "Epoch 9: Train Loss: 0.0023, Test Loss: 0.0010, Train L1 Norm: 0.1845, Test L1 Norm: 0.0691, Train Linf Norm: 10.7093, Test Linf Norm: 2.9038\n",
            "Epoch 10: Train Loss: 0.0011, Test Loss: 0.0008, Train L1 Norm: 0.1012, Test L1 Norm: 0.0422, Train Linf Norm: 5.1508, Test Linf Norm: 1.0992\n",
            "Epoch 11: Train Loss: 0.0007, Test Loss: 0.0006, Train L1 Norm: 0.0691, Test L1 Norm: 0.0377, Train Linf Norm: 3.4818, Test Linf Norm: 0.9165\n",
            "Epoch 12: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0545, Test L1 Norm: 0.0377, Train Linf Norm: 2.2497, Test Linf Norm: 0.9165\n",
            "Epoch 13: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0680, Test L1 Norm: 0.0376, Train Linf Norm: 3.3771, Test Linf Norm: 0.9239\n",
            "Epoch 14: Train Loss: 0.0010, Test Loss: 0.0007, Train L1 Norm: 0.0928, Test L1 Norm: 0.0368, Train Linf Norm: 4.3945, Test Linf Norm: 1.1500\n",
            "Epoch 15: Train Loss: 0.0021, Test Loss: 0.0082, Train L1 Norm: 0.1537, Test L1 Norm: 0.1711, Train Linf Norm: 7.4238, Test Linf Norm: 9.5940\n",
            "Epoch 16: Train Loss: 0.0047, Test Loss: 0.0067, Train L1 Norm: 0.2171, Test L1 Norm: 0.4695, Train Linf Norm: 10.0152, Test Linf Norm: 29.8666\n",
            "Epoch 17: Train Loss: 0.0082, Test Loss: 0.0017, Train L1 Norm: 0.3068, Test L1 Norm: 0.0700, Train Linf Norm: 16.7806, Test Linf Norm: 1.1557\n",
            "Epoch 18: Train Loss: 0.0130, Test Loss: 0.0305, Train L1 Norm: 0.3868, Test L1 Norm: 0.1867, Train Linf Norm: 22.0873, Test Linf Norm: 1.0000\n",
            "Epoch 19: Train Loss: 0.0164, Test Loss: 0.0074, Train L1 Norm: 0.4508, Test L1 Norm: 0.1251, Train Linf Norm: 26.1554, Test Linf Norm: 1.0000\n",
            "Epoch 20: Train Loss: 0.0195, Test Loss: 0.0447, Train L1 Norm: 0.2550, Test L1 Norm: 0.1788, Train Linf Norm: 8.4475, Test Linf Norm: 1.0000\n",
            "Epoch 21: Train Loss: 0.0204, Test Loss: 0.0261, Train L1 Norm: 0.2564, Test L1 Norm: 0.1946, Train Linf Norm: 8.1606, Test Linf Norm: 1.1517\n",
            "Epoch 22: Train Loss: 0.0188, Test Loss: 0.0297, Train L1 Norm: 0.2423, Test L1 Norm: 0.2155, Train Linf Norm: 7.7709, Test Linf Norm: 1.0000\n",
            "Epoch 23: Train Loss: 0.0172, Test Loss: 0.0145, Train L1 Norm: 0.2075, Test L1 Norm: 0.1803, Train Linf Norm: 5.4428, Test Linf Norm: 1.0000\n",
            "Epoch 24: Train Loss: 0.0157, Test Loss: 0.0055, Train L1 Norm: 0.1995, Test L1 Norm: 0.1019, Train Linf Norm: 5.0174, Test Linf Norm: 2.0859\n",
            "Epoch 25: Train Loss: 0.0132, Test Loss: 0.0098, Train L1 Norm: 0.1933, Test L1 Norm: 0.1534, Train Linf Norm: 4.9447, Test Linf Norm: 2.8316\n",
            "Epoch 26: Train Loss: 0.0097, Test Loss: 0.0241, Train L1 Norm: 0.2662, Test L1 Norm: 0.1225, Train Linf Norm: 12.5876, Test Linf Norm: 2.3125\n",
            "Epoch 27: Train Loss: 0.0071, Test Loss: 0.0025, Train L1 Norm: 0.2057, Test L1 Norm: 0.0929, Train Linf Norm: 8.3113, Test Linf Norm: 2.3280\n",
            "Epoch 28: Train Loss: 0.0052, Test Loss: 0.0169, Train L1 Norm: 0.1917, Test L1 Norm: 1.0051, Train Linf Norm: 7.5200, Test Linf Norm: 62.1162\n",
            "Epoch 29: Train Loss: 0.0033, Test Loss: 0.0013, Train L1 Norm: 0.1588, Test L1 Norm: 0.0798, Train Linf Norm: 5.6530, Test Linf Norm: 2.1164\n",
            "Epoch 30: Train Loss: 0.0018, Test Loss: 0.0004, Train L1 Norm: 0.1354, Test L1 Norm: 0.0550, Train Linf Norm: 5.6112, Test Linf Norm: 1.2381\n",
            "Epoch 31: Train Loss: 0.0009, Test Loss: 0.0016, Train L1 Norm: 0.1209, Test L1 Norm: 0.2592, Train Linf Norm: 6.0419, Test Linf Norm: 14.3338\n",
            "Epoch 32: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.0732, Test L1 Norm: 0.0394, Train Linf Norm: 3.2091, Test Linf Norm: 1.0778\n",
            "Epoch 33: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0509, Test L1 Norm: 0.0362, Train Linf Norm: 2.0054, Test Linf Norm: 1.0344\n",
            "Epoch 34: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0463, Test L1 Norm: 0.0362, Train Linf Norm: 1.8459, Test Linf Norm: 1.0344\n",
            "Epoch 35: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0505, Test L1 Norm: 0.0356, Train Linf Norm: 2.0195, Test Linf Norm: 1.1015\n",
            "Epoch 36: Train Loss: 0.0005, Test Loss: 0.0004, Train L1 Norm: 0.0687, Test L1 Norm: 0.0500, Train Linf Norm: 2.7941, Test Linf Norm: 0.9939\n",
            "Epoch 37: Train Loss: 0.0010, Test Loss: 0.0011, Train L1 Norm: 0.1249, Test L1 Norm: 0.0506, Train Linf Norm: 6.3076, Test Linf Norm: 1.1084\n",
            "Epoch 38: Train Loss: 0.0018, Test Loss: 0.0050, Train L1 Norm: 0.1416, Test L1 Norm: 0.5668, Train Linf Norm: 5.8412, Test Linf Norm: 36.3129\n",
            "Epoch 39: Train Loss: 0.0033, Test Loss: 0.0087, Train L1 Norm: 0.1726, Test L1 Norm: 0.1100, Train Linf Norm: 6.8042, Test Linf Norm: 1.7565\n",
            "Epoch 40: Train Loss: 0.0050, Test Loss: 0.0008, Train L1 Norm: 0.7112, Test L1 Norm: 0.0782, Train Linf Norm: 51.7788, Test Linf Norm: 0.9990\n",
            "Epoch 41: Train Loss: 0.0070, Test Loss: 0.0034, Train L1 Norm: 0.1761, Test L1 Norm: 0.1383, Train Linf Norm: 5.1081, Test Linf Norm: 1.0000\n",
            "Epoch 42: Train Loss: 0.0083, Test Loss: 0.0022, Train L1 Norm: 0.2157, Test L1 Norm: 0.0832, Train Linf Norm: 7.7933, Test Linf Norm: 1.5948\n",
            "Epoch 43: Train Loss: 0.0094, Test Loss: 0.0337, Train L1 Norm: 0.1869, Test L1 Norm: 2.1328, Train Linf Norm: 5.3498, Test Linf Norm: 139.8543\n",
            "Epoch 44: Train Loss: 0.0099, Test Loss: 0.0299, Train L1 Norm: 0.2327, Test L1 Norm: 0.1699, Train Linf Norm: 9.3074, Test Linf Norm: 4.1645\n",
            "Epoch 45: Train Loss: 0.0103, Test Loss: 0.0038, Train L1 Norm: 0.1894, Test L1 Norm: 0.1064, Train Linf Norm: 5.5702, Test Linf Norm: 1.1805\n",
            "Epoch 46: Train Loss: 0.0088, Test Loss: 0.0015, Train L1 Norm: 0.1752, Test L1 Norm: 0.0867, Train Linf Norm: 4.7373, Test Linf Norm: 1.1089\n",
            "Epoch 47: Train Loss: 0.0079, Test Loss: 0.0057, Train L1 Norm: 0.1740, Test L1 Norm: 0.1391, Train Linf Norm: 5.1122, Test Linf Norm: 2.1332\n",
            "Epoch 48: Train Loss: 0.0066, Test Loss: 0.0073, Train L1 Norm: 0.3312, Test L1 Norm: 0.1598, Train Linf Norm: 19.3206, Test Linf Norm: 1.8294\n",
            "Epoch 49: Train Loss: 0.0053, Test Loss: 0.0040, Train L1 Norm: 0.1653, Test L1 Norm: 0.0943, Train Linf Norm: 5.2821, Test Linf Norm: 1.2285\n",
            "Epoch 50: Train Loss: 0.0038, Test Loss: 0.0007, Train L1 Norm: 0.1714, Test L1 Norm: 0.0728, Train Linf Norm: 6.3987, Test Linf Norm: 1.0615\n",
            "Epoch 51: Train Loss: 0.0024, Test Loss: 0.0005, Train L1 Norm: 0.1636, Test L1 Norm: 0.0610, Train Linf Norm: 7.0766, Test Linf Norm: 1.4608\n",
            "Epoch 52: Train Loss: 0.0014, Test Loss: 0.0015, Train L1 Norm: 0.1359, Test L1 Norm: 0.0892, Train Linf Norm: 5.9571, Test Linf Norm: 1.0493\n",
            "Epoch 53: Train Loss: 0.0007, Test Loss: 0.0003, Train L1 Norm: 0.1318, Test L1 Norm: 0.0516, Train Linf Norm: 7.1378, Test Linf Norm: 1.0446\n",
            "Epoch 54: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.0618, Test L1 Norm: 0.0575, Train Linf Norm: 2.3122, Test Linf Norm: 2.9351\n",
            "Epoch 55: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0509, Test L1 Norm: 0.0562, Train Linf Norm: 2.0837, Test Linf Norm: 2.9600\n",
            "Epoch 56: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0532, Test L1 Norm: 0.0562, Train Linf Norm: 2.4863, Test Linf Norm: 2.9600\n",
            "Epoch 57: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0477, Test L1 Norm: 0.0677, Train Linf Norm: 1.8483, Test Linf Norm: 3.9665\n",
            "Epoch 58: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.0668, Test L1 Norm: 0.0417, Train Linf Norm: 2.7800, Test Linf Norm: 1.0791\n",
            "Epoch 59: Train Loss: 0.0007, Test Loss: 0.0005, Train L1 Norm: 0.0883, Test L1 Norm: 0.0875, Train Linf Norm: 3.3404, Test Linf Norm: 5.2079\n",
            "Epoch 60: Train Loss: 0.0014, Test Loss: 0.0056, Train L1 Norm: 0.1384, Test L1 Norm: 0.1530, Train Linf Norm: 6.1495, Test Linf Norm: 1.0000\n",
            "Epoch 61: Train Loss: 0.0025, Test Loss: 0.0013, Train L1 Norm: 0.2056, Test L1 Norm: 0.1012, Train Linf Norm: 10.7162, Test Linf Norm: 1.0000\n",
            "Epoch 62: Train Loss: 0.0040, Test Loss: 0.0093, Train L1 Norm: 0.2135, Test L1 Norm: 0.1549, Train Linf Norm: 9.8015, Test Linf Norm: 1.0000\n",
            "Epoch 63: Train Loss: 0.0053, Test Loss: 0.0021, Train L1 Norm: 0.2016, Test L1 Norm: 0.0992, Train Linf Norm: 7.7085, Test Linf Norm: 2.8048\n",
            "Epoch 64: Train Loss: 0.0067, Test Loss: 0.0017, Train L1 Norm: 0.2361, Test L1 Norm: 0.1057, Train Linf Norm: 10.0731, Test Linf Norm: 1.2190\n",
            "Epoch 65: Train Loss: 0.0077, Test Loss: 0.0021, Train L1 Norm: 0.4064, Test L1 Norm: 0.1095, Train Linf Norm: 25.4944, Test Linf Norm: 1.0000\n",
            "Epoch 66: Train Loss: 0.0081, Test Loss: 0.0025, Train L1 Norm: 0.1940, Test L1 Norm: 0.1292, Train Linf Norm: 6.0671, Test Linf Norm: 4.9252\n",
            "Epoch 67: Train Loss: 0.0080, Test Loss: 0.0020, Train L1 Norm: 0.1892, Test L1 Norm: 0.0880, Train Linf Norm: 5.7655, Test Linf Norm: 1.8807\n",
            "Epoch 68: Train Loss: 0.0077, Test Loss: 0.0013, Train L1 Norm: 0.1658, Test L1 Norm: 0.0930, Train Linf Norm: 4.1153, Test Linf Norm: 1.0897\n",
            "Epoch 69: Train Loss: 0.0066, Test Loss: 0.0022, Train L1 Norm: 0.1762, Test L1 Norm: 0.1263, Train Linf Norm: 5.3055, Test Linf Norm: 3.9775\n",
            "Epoch 70: Train Loss: 0.0058, Test Loss: 0.0053, Train L1 Norm: 0.1554, Test L1 Norm: 0.0824, Train Linf Norm: 4.0182, Test Linf Norm: 0.9955\n",
            "Epoch 71: Train Loss: 0.0044, Test Loss: 0.0016, Train L1 Norm: 0.1606, Test L1 Norm: 0.1090, Train Linf Norm: 4.8910, Test Linf Norm: 1.1908\n",
            "Epoch 72: Train Loss: 0.0033, Test Loss: 0.0014, Train L1 Norm: 0.1783, Test L1 Norm: 0.0942, Train Linf Norm: 7.3574, Test Linf Norm: 1.1725\n",
            "Epoch 73: Train Loss: 0.0021, Test Loss: 0.0006, Train L1 Norm: 0.1415, Test L1 Norm: 0.0700, Train Linf Norm: 5.1701, Test Linf Norm: 1.2040\n",
            "Epoch 74: Train Loss: 0.0013, Test Loss: 0.0011, Train L1 Norm: 0.1126, Test L1 Norm: 0.0684, Train Linf Norm: 4.1725, Test Linf Norm: 0.9912\n",
            "Epoch 75: Train Loss: 0.0007, Test Loss: 0.0003, Train L1 Norm: 0.0857, Test L1 Norm: 0.0373, Train Linf Norm: 3.1784, Test Linf Norm: 1.0286\n",
            "Epoch 76: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0556, Test L1 Norm: 0.0386, Train Linf Norm: 1.8869, Test Linf Norm: 1.0087\n",
            "Epoch 77: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0476, Test L1 Norm: 0.0368, Train Linf Norm: 1.8531, Test Linf Norm: 1.0203\n",
            "Epoch 78: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0457, Test L1 Norm: 0.0368, Train Linf Norm: 1.6862, Test Linf Norm: 1.0203\n",
            "Epoch 79: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0472, Test L1 Norm: 0.0389, Train Linf Norm: 1.8578, Test Linf Norm: 1.3820\n",
            "Epoch 80: Train Loss: 0.0003, Test Loss: 0.0005, Train L1 Norm: 0.0669, Test L1 Norm: 0.0487, Train Linf Norm: 2.8843, Test Linf Norm: 1.7053\n",
            "Epoch 81: Train Loss: 0.0006, Test Loss: 0.0002, Train L1 Norm: 0.1147, Test L1 Norm: 0.0362, Train Linf Norm: 5.5210, Test Linf Norm: 1.0271\n",
            "Epoch 82: Train Loss: 0.0013, Test Loss: 0.0025, Train L1 Norm: 0.1379, Test L1 Norm: 0.4461, Train Linf Norm: 5.9362, Test Linf Norm: 27.4965\n",
            "Epoch 83: Train Loss: 0.0021, Test Loss: 0.0007, Train L1 Norm: 0.1750, Test L1 Norm: 0.0848, Train Linf Norm: 7.7046, Test Linf Norm: 2.8971\n",
            "Epoch 84: Train Loss: 0.0033, Test Loss: 0.0054, Train L1 Norm: 0.2076, Test L1 Norm: 0.1414, Train Linf Norm: 9.5743, Test Linf Norm: 1.3466\n",
            "Epoch 85: Train Loss: 0.0048, Test Loss: 0.0009, Train L1 Norm: 0.4946, Test L1 Norm: 0.0836, Train Linf Norm: 34.1483, Test Linf Norm: 1.0303\n",
            "Epoch 86: Train Loss: 0.0057, Test Loss: 0.0324, Train L1 Norm: 0.3110, Test L1 Norm: 2.2099, Train Linf Norm: 17.6330, Test Linf Norm: 146.4013\n",
            "Epoch 87: Train Loss: 0.0071, Test Loss: 0.0077, Train L1 Norm: 0.2192, Test L1 Norm: 0.0919, Train Linf Norm: 8.6694, Test Linf Norm: 1.4839\n",
            "Epoch 88: Train Loss: 0.0073, Test Loss: 0.0017, Train L1 Norm: 0.1788, Test L1 Norm: 0.0869, Train Linf Norm: 5.0209, Test Linf Norm: 1.0000\n",
            "Epoch 89: Train Loss: 0.0075, Test Loss: 0.0021, Train L1 Norm: 0.1685, Test L1 Norm: 0.1537, Train Linf Norm: 4.2429, Test Linf Norm: 6.5215\n",
            "Epoch 90: Train Loss: 0.0071, Test Loss: 0.0021, Train L1 Norm: 0.1884, Test L1 Norm: 0.1167, Train Linf Norm: 6.3862, Test Linf Norm: 1.0741\n",
            "Epoch 91: Train Loss: 0.0067, Test Loss: 0.0032, Train L1 Norm: 0.4002, Test L1 Norm: 0.0732, Train Linf Norm: 25.2711, Test Linf Norm: 1.0427\n",
            "Epoch 92: Train Loss: 0.0054, Test Loss: 0.0013, Train L1 Norm: 0.1738, Test L1 Norm: 0.0675, Train Linf Norm: 5.5400, Test Linf Norm: 1.2456\n",
            "Epoch 93: Train Loss: 0.0042, Test Loss: 0.0074, Train L1 Norm: 0.2232, Test L1 Norm: 0.1693, Train Linf Norm: 10.6130, Test Linf Norm: 1.0000\n",
            "Epoch 94: Train Loss: 0.0029, Test Loss: 0.0045, Train L1 Norm: 0.3373, Test L1 Norm: 0.1107, Train Linf Norm: 22.1323, Test Linf Norm: 1.3393\n",
            "Epoch 95: Train Loss: 0.0019, Test Loss: 0.0027, Train L1 Norm: 0.1414, Test L1 Norm: 0.1131, Train Linf Norm: 5.3676, Test Linf Norm: 1.0324\n",
            "Epoch 96: Train Loss: 0.0012, Test Loss: 0.0016, Train L1 Norm: 0.1703, Test L1 Norm: 0.1077, Train Linf Norm: 9.1533, Test Linf Norm: 1.0301\n",
            "Epoch 97: Train Loss: 0.0006, Test Loss: 0.0002, Train L1 Norm: 0.0821, Test L1 Norm: 0.0384, Train Linf Norm: 2.9428, Test Linf Norm: 1.2403\n",
            "Epoch 98: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0588, Test L1 Norm: 0.0390, Train Linf Norm: 2.1291, Test Linf Norm: 1.3884\n",
            "Epoch 99: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0528, Test L1 Norm: 0.0378, Train Linf Norm: 2.2778, Test Linf Norm: 1.1084\n",
            "Epoch 100: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0490, Test L1 Norm: 0.0378, Train Linf Norm: 1.9839, Test Linf Norm: 1.1084\n",
            "Epoch 101: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0506, Test L1 Norm: 0.0407, Train Linf Norm: 2.0892, Test Linf Norm: 1.0750\n",
            "Epoch 102: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0644, Test L1 Norm: 0.0374, Train Linf Norm: 2.5509, Test Linf Norm: 1.0982\n",
            "Epoch 103: Train Loss: 0.0006, Test Loss: 0.0002, Train L1 Norm: 0.1053, Test L1 Norm: 0.0398, Train Linf Norm: 4.9363, Test Linf Norm: 1.1638\n",
            "Epoch 104: Train Loss: 0.0012, Test Loss: 0.0008, Train L1 Norm: 0.1790, Test L1 Norm: 0.0793, Train Linf Norm: 9.8988, Test Linf Norm: 1.0147\n",
            "Epoch 105: Train Loss: 0.0021, Test Loss: 0.0031, Train L1 Norm: 0.2076, Test L1 Norm: 0.1134, Train Linf Norm: 10.7436, Test Linf Norm: 1.0459\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:17:00,974]\u001b[0m Trial 8 finished with value: 0.10073578403890134 and parameters: {'n_layers': 1, 'n_units_0': 543, 'hidden_activation': 'Sigmoid', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adam', 'lr': 0.009979258878951836, 'batch_size': 90, 'n_epochs': 106, 'scheduler': 'CosineAnnealingLR', 'weight_decay': 1.3953755881969121e-05, 'beta1': 0.6842972078589464, 'beta2': 0.9266984716952403, 'T_max': 11}. Best is trial 5 with value: 0.02577756353840232.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 106: Train Loss: 0.0031, Test Loss: 0.0022, Train L1 Norm: 0.1890, Test L1 Norm: 0.1007, Train Linf Norm: 7.6474, Test Linf Norm: 1.0391\n",
            "Epoch 1: Train Loss: 0.0695, Test Loss: 0.0076, Train L1 Norm: 0.3800, Test L1 Norm: 0.0482, Train Linf Norm: 56.9341, Test Linf Norm: 2.0314\n",
            "Epoch 2: Train Loss: 0.0033, Test Loss: 0.0021, Train L1 Norm: 0.0661, Test L1 Norm: 0.0449, Train Linf Norm: 8.7187, Test Linf Norm: 4.8248\n",
            "Epoch 3: Train Loss: 0.0012, Test Loss: 0.0007, Train L1 Norm: 0.0412, Test L1 Norm: 0.0237, Train Linf Norm: 4.4430, Test Linf Norm: 1.0989\n",
            "Epoch 4: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0339, Test L1 Norm: 0.0222, Train Linf Norm: 3.3790, Test Linf Norm: 1.0924\n",
            "Epoch 5: Train Loss: 0.0004, Test Loss: 0.0003, Train L1 Norm: 0.0305, Test L1 Norm: 0.0204, Train Linf Norm: 2.9685, Test Linf Norm: 0.9932\n",
            "Epoch 6: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0285, Test L1 Norm: 0.0190, Train Linf Norm: 2.7736, Test Linf Norm: 1.0072\n",
            "Epoch 7: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0264, Test L1 Norm: 0.0180, Train Linf Norm: 2.6186, Test Linf Norm: 0.9835\n",
            "Epoch 8: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0250, Test L1 Norm: 0.0178, Train Linf Norm: 2.3756, Test Linf Norm: 1.0089\n",
            "Epoch 9: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0247, Test L1 Norm: 0.0176, Train Linf Norm: 2.4310, Test Linf Norm: 1.0128\n",
            "Epoch 10: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0244, Test L1 Norm: 0.0173, Train Linf Norm: 2.4239, Test Linf Norm: 1.0152\n",
            "Epoch 11: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0242, Test L1 Norm: 0.0173, Train Linf Norm: 2.4095, Test Linf Norm: 1.0246\n",
            "Epoch 12: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0242, Test L1 Norm: 0.0173, Train Linf Norm: 2.3551, Test Linf Norm: 1.0251\n",
            "Epoch 13: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0242, Test L1 Norm: 0.0173, Train Linf Norm: 2.4402, Test Linf Norm: 1.0251\n",
            "Epoch 14: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0242, Test L1 Norm: 0.0173, Train Linf Norm: 2.3639, Test Linf Norm: 1.0230\n",
            "Epoch 15: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0242, Test L1 Norm: 0.0172, Train Linf Norm: 2.4366, Test Linf Norm: 1.0245\n",
            "Epoch 16: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0241, Test L1 Norm: 0.0172, Train Linf Norm: 2.3662, Test Linf Norm: 1.0241\n",
            "Epoch 17: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0240, Test L1 Norm: 0.0171, Train Linf Norm: 2.4428, Test Linf Norm: 1.0438\n",
            "Epoch 18: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0238, Test L1 Norm: 0.0168, Train Linf Norm: 2.4595, Test Linf Norm: 1.0156\n",
            "Epoch 19: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0236, Test L1 Norm: 0.0166, Train Linf Norm: 2.3917, Test Linf Norm: 1.0400\n",
            "Epoch 20: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0225, Test L1 Norm: 0.0160, Train Linf Norm: 2.2603, Test Linf Norm: 1.0047\n",
            "Epoch 21: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0224, Test L1 Norm: 0.0164, Train Linf Norm: 2.3626, Test Linf Norm: 1.1430\n",
            "Epoch 22: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0222, Test L1 Norm: 0.0155, Train Linf Norm: 2.3544, Test Linf Norm: 1.0391\n",
            "Epoch 23: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0212, Test L1 Norm: 0.0152, Train Linf Norm: 2.0873, Test Linf Norm: 1.0265\n",
            "Epoch 24: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0208, Test L1 Norm: 0.0153, Train Linf Norm: 2.1540, Test Linf Norm: 1.0893\n",
            "Epoch 25: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0199, Test L1 Norm: 0.0146, Train Linf Norm: 2.0352, Test Linf Norm: 0.9794\n",
            "Epoch 26: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0195, Test L1 Norm: 0.0141, Train Linf Norm: 1.9308, Test Linf Norm: 0.9376\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0186, Test L1 Norm: 0.0138, Train Linf Norm: 1.7939, Test Linf Norm: 0.9288\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0182, Test L1 Norm: 0.0137, Train Linf Norm: 1.7639, Test Linf Norm: 0.9730\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0175, Test L1 Norm: 0.0134, Train Linf Norm: 1.6697, Test Linf Norm: 0.9234\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0171, Test L1 Norm: 0.0132, Train Linf Norm: 1.6528, Test Linf Norm: 0.9252\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0169, Test L1 Norm: 0.0130, Train Linf Norm: 1.5975, Test Linf Norm: 0.9133\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0166, Test L1 Norm: 0.0131, Train Linf Norm: 1.5872, Test Linf Norm: 0.9393\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0164, Test L1 Norm: 0.0129, Train Linf Norm: 1.5650, Test Linf Norm: 0.8962\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0164, Test L1 Norm: 0.0128, Train Linf Norm: 1.5954, Test Linf Norm: 0.8852\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0163, Test L1 Norm: 0.0128, Train Linf Norm: 1.5270, Test Linf Norm: 0.8920\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0164, Test L1 Norm: 0.0128, Train Linf Norm: 1.5175, Test Linf Norm: 0.8938\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0164, Test L1 Norm: 0.0128, Train Linf Norm: 1.5439, Test Linf Norm: 0.8938\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0163, Test L1 Norm: 0.0128, Train Linf Norm: 1.5105, Test Linf Norm: 0.8914\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0163, Test L1 Norm: 0.0128, Train Linf Norm: 1.5539, Test Linf Norm: 0.8918\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0163, Test L1 Norm: 0.0128, Train Linf Norm: 1.5295, Test Linf Norm: 0.8868\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0162, Test L1 Norm: 0.0127, Train Linf Norm: 1.5660, Test Linf Norm: 0.8883\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0161, Test L1 Norm: 0.0127, Train Linf Norm: 1.5288, Test Linf Norm: 0.8677\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0159, Test L1 Norm: 0.0127, Train Linf Norm: 1.4778, Test Linf Norm: 0.8739\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0159, Test L1 Norm: 0.0126, Train Linf Norm: 1.5281, Test Linf Norm: 0.9077\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0158, Test L1 Norm: 0.0125, Train Linf Norm: 1.4670, Test Linf Norm: 0.8856\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0153, Test L1 Norm: 0.0122, Train Linf Norm: 1.4302, Test Linf Norm: 0.8597\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0152, Test L1 Norm: 0.0121, Train Linf Norm: 1.3903, Test Linf Norm: 0.8446\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0150, Test L1 Norm: 0.0119, Train Linf Norm: 1.4092, Test Linf Norm: 0.8452\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0148, Test L1 Norm: 0.0119, Train Linf Norm: 1.3705, Test Linf Norm: 0.8571\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0148, Test L1 Norm: 0.0117, Train Linf Norm: 1.3578, Test Linf Norm: 0.8351\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0145, Test L1 Norm: 0.0117, Train Linf Norm: 1.3881, Test Linf Norm: 0.8258\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0143, Test L1 Norm: 0.0115, Train Linf Norm: 1.3380, Test Linf Norm: 0.8272\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0142, Test L1 Norm: 0.0115, Train Linf Norm: 1.3295, Test Linf Norm: 0.8259\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0140, Test L1 Norm: 0.0114, Train Linf Norm: 1.2894, Test Linf Norm: 0.8317\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0141, Test L1 Norm: 0.0114, Train Linf Norm: 1.3026, Test Linf Norm: 0.8260\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0139, Test L1 Norm: 0.0113, Train Linf Norm: 1.3278, Test Linf Norm: 0.8291\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0139, Test L1 Norm: 0.0112, Train Linf Norm: 1.2854, Test Linf Norm: 0.8244\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0138, Test L1 Norm: 0.0113, Train Linf Norm: 1.3140, Test Linf Norm: 0.8232\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0138, Test L1 Norm: 0.0113, Train Linf Norm: 1.2983, Test Linf Norm: 0.8280\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0138, Test L1 Norm: 0.0113, Train Linf Norm: 1.3123, Test Linf Norm: 0.8278\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0138, Test L1 Norm: 0.0113, Train Linf Norm: 1.2861, Test Linf Norm: 0.8278\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0138, Test L1 Norm: 0.0113, Train Linf Norm: 1.2913, Test Linf Norm: 0.8281\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0138, Test L1 Norm: 0.0113, Train Linf Norm: 1.2692, Test Linf Norm: 0.8296\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0138, Test L1 Norm: 0.0113, Train Linf Norm: 1.2774, Test Linf Norm: 0.8285\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0138, Test L1 Norm: 0.0112, Train Linf Norm: 1.2676, Test Linf Norm: 0.8171\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0138, Test L1 Norm: 0.0112, Train Linf Norm: 1.3106, Test Linf Norm: 0.8249\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0137, Test L1 Norm: 0.0112, Train Linf Norm: 1.2777, Test Linf Norm: 0.8225\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0136, Test L1 Norm: 0.0111, Train Linf Norm: 1.2562, Test Linf Norm: 0.8146\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0135, Test L1 Norm: 0.0111, Train Linf Norm: 1.2076, Test Linf Norm: 0.8226\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0135, Test L1 Norm: 0.0111, Train Linf Norm: 1.2829, Test Linf Norm: 0.8236\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0134, Test L1 Norm: 0.0111, Train Linf Norm: 1.2474, Test Linf Norm: 0.8046\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0133, Test L1 Norm: 0.0109, Train Linf Norm: 1.2570, Test Linf Norm: 0.8079\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0132, Test L1 Norm: 0.0109, Train Linf Norm: 1.2596, Test Linf Norm: 0.8051\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0131, Test L1 Norm: 0.0108, Train Linf Norm: 1.2210, Test Linf Norm: 0.8256\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0129, Test L1 Norm: 0.0107, Train Linf Norm: 1.2097, Test Linf Norm: 0.8103\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0129, Test L1 Norm: 0.0106, Train Linf Norm: 1.2224, Test Linf Norm: 0.7999\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0128, Test L1 Norm: 0.0106, Train Linf Norm: 1.2096, Test Linf Norm: 0.8025\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0128, Test L1 Norm: 0.0105, Train Linf Norm: 1.2242, Test Linf Norm: 0.8079\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0127, Test L1 Norm: 0.0104, Train Linf Norm: 1.2304, Test Linf Norm: 0.7970\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0126, Test L1 Norm: 0.0105, Train Linf Norm: 1.1994, Test Linf Norm: 0.7968\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0125, Test L1 Norm: 0.0104, Train Linf Norm: 1.1969, Test Linf Norm: 0.7939\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0125, Test L1 Norm: 0.0105, Train Linf Norm: 1.1779, Test Linf Norm: 0.7967\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0126, Test L1 Norm: 0.0105, Train Linf Norm: 1.2040, Test Linf Norm: 0.7986\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0126, Test L1 Norm: 0.0105, Train Linf Norm: 1.1472, Test Linf Norm: 0.7984\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0126, Test L1 Norm: 0.0105, Train Linf Norm: 1.1908, Test Linf Norm: 0.7984\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0126, Test L1 Norm: 0.0105, Train Linf Norm: 1.1954, Test Linf Norm: 0.7983\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0126, Test L1 Norm: 0.0105, Train Linf Norm: 1.2100, Test Linf Norm: 0.7980\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0125, Test L1 Norm: 0.0104, Train Linf Norm: 1.1934, Test Linf Norm: 0.7956\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0125, Test L1 Norm: 0.0104, Train Linf Norm: 1.1745, Test Linf Norm: 0.7952\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0125, Test L1 Norm: 0.0104, Train Linf Norm: 1.2042, Test Linf Norm: 0.7982\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0124, Test L1 Norm: 0.0103, Train Linf Norm: 1.1884, Test Linf Norm: 0.7878\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0124, Test L1 Norm: 0.0104, Train Linf Norm: 1.2100, Test Linf Norm: 0.7893\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0124, Test L1 Norm: 0.0103, Train Linf Norm: 1.1695, Test Linf Norm: 0.7827\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0103, Train Linf Norm: 1.1518, Test Linf Norm: 0.7748\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0102, Train Linf Norm: 1.1325, Test Linf Norm: 0.7711\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0101, Train Linf Norm: 1.1538, Test Linf Norm: 0.7740\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0120, Test L1 Norm: 0.0100, Train Linf Norm: 1.1219, Test Linf Norm: 0.7630\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0119, Test L1 Norm: 0.0100, Train Linf Norm: 1.1078, Test Linf Norm: 0.7648\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0120, Test L1 Norm: 0.0100, Train Linf Norm: 1.1264, Test Linf Norm: 0.7622\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0120, Test L1 Norm: 0.0099, Train Linf Norm: 1.1318, Test Linf Norm: 0.7520\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0119, Test L1 Norm: 0.0099, Train Linf Norm: 1.1391, Test Linf Norm: 0.7543\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0117, Test L1 Norm: 0.0098, Train Linf Norm: 1.0820, Test Linf Norm: 0.7596\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0117, Test L1 Norm: 0.0098, Train Linf Norm: 1.1137, Test Linf Norm: 0.7552\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0116, Test L1 Norm: 0.0098, Train Linf Norm: 1.1177, Test Linf Norm: 0.7562\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0116, Test L1 Norm: 0.0098, Train Linf Norm: 1.0987, Test Linf Norm: 0.7565\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0116, Test L1 Norm: 0.0097, Train Linf Norm: 1.0803, Test Linf Norm: 0.7488\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0116, Test L1 Norm: 0.0098, Train Linf Norm: 1.1235, Test Linf Norm: 0.7562\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0116, Test L1 Norm: 0.0098, Train Linf Norm: 1.1080, Test Linf Norm: 0.7565\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0116, Test L1 Norm: 0.0098, Train Linf Norm: 1.1193, Test Linf Norm: 0.7565\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0116, Test L1 Norm: 0.0098, Train Linf Norm: 1.0649, Test Linf Norm: 0.7558\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0116, Test L1 Norm: 0.0097, Train Linf Norm: 1.1041, Test Linf Norm: 0.7571\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0116, Test L1 Norm: 0.0098, Train Linf Norm: 1.0727, Test Linf Norm: 0.7557\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0116, Test L1 Norm: 0.0097, Train Linf Norm: 1.0845, Test Linf Norm: 0.7542\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0116, Test L1 Norm: 0.0097, Train Linf Norm: 1.1082, Test Linf Norm: 0.7544\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0115, Test L1 Norm: 0.0097, Train Linf Norm: 1.1009, Test Linf Norm: 0.7514\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0115, Test L1 Norm: 0.0097, Train Linf Norm: 1.1013, Test Linf Norm: 0.7444\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0115, Test L1 Norm: 0.0096, Train Linf Norm: 1.1002, Test Linf Norm: 0.7408\n",
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0114, Test L1 Norm: 0.0096, Train Linf Norm: 1.0966, Test Linf Norm: 0.7456\n",
            "Epoch 119: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0113, Test L1 Norm: 0.0095, Train Linf Norm: 1.0584, Test Linf Norm: 0.7435\n",
            "Epoch 120: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0112, Test L1 Norm: 0.0094, Train Linf Norm: 1.0408, Test Linf Norm: 0.7369\n",
            "Epoch 121: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0113, Test L1 Norm: 0.0094, Train Linf Norm: 1.1034, Test Linf Norm: 0.7462\n",
            "Epoch 122: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0112, Test L1 Norm: 0.0095, Train Linf Norm: 1.0664, Test Linf Norm: 0.7321\n",
            "Epoch 123: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0111, Test L1 Norm: 0.0094, Train Linf Norm: 1.0735, Test Linf Norm: 0.7395\n",
            "Epoch 124: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0111, Test L1 Norm: 0.0093, Train Linf Norm: 1.0467, Test Linf Norm: 0.7356\n",
            "Epoch 125: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0110, Test L1 Norm: 0.0093, Train Linf Norm: 1.0426, Test Linf Norm: 0.7429\n",
            "Epoch 126: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0110, Test L1 Norm: 0.0093, Train Linf Norm: 1.0161, Test Linf Norm: 0.7326\n",
            "Epoch 127: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0110, Test L1 Norm: 0.0092, Train Linf Norm: 1.0458, Test Linf Norm: 0.7244\n",
            "Epoch 128: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0109, Test L1 Norm: 0.0092, Train Linf Norm: 1.0434, Test Linf Norm: 0.7282\n",
            "Epoch 129: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0110, Test L1 Norm: 0.0093, Train Linf Norm: 1.0383, Test Linf Norm: 0.7302\n",
            "Epoch 130: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0109, Test L1 Norm: 0.0092, Train Linf Norm: 1.0475, Test Linf Norm: 0.7330\n",
            "Epoch 131: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0109, Test L1 Norm: 0.0092, Train Linf Norm: 1.0451, Test Linf Norm: 0.7304\n",
            "Epoch 132: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0109, Test L1 Norm: 0.0092, Train Linf Norm: 1.0443, Test Linf Norm: 0.7303\n",
            "Epoch 133: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0109, Test L1 Norm: 0.0092, Train Linf Norm: 1.0489, Test Linf Norm: 0.7303\n",
            "Epoch 134: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0109, Test L1 Norm: 0.0092, Train Linf Norm: 1.0507, Test Linf Norm: 0.7301\n",
            "Epoch 135: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0109, Test L1 Norm: 0.0092, Train Linf Norm: 1.0526, Test Linf Norm: 0.7295\n",
            "Epoch 136: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0109, Test L1 Norm: 0.0092, Train Linf Norm: 1.0194, Test Linf Norm: 0.7298\n",
            "Epoch 137: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0109, Test L1 Norm: 0.0092, Train Linf Norm: 1.0476, Test Linf Norm: 0.7270\n",
            "Epoch 138: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0109, Test L1 Norm: 0.0092, Train Linf Norm: 1.0293, Test Linf Norm: 0.7269\n",
            "Epoch 139: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0108, Test L1 Norm: 0.0092, Train Linf Norm: 1.0318, Test Linf Norm: 0.7270\n",
            "Epoch 140: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0109, Test L1 Norm: 0.0091, Train Linf Norm: 1.0436, Test Linf Norm: 0.7294\n",
            "Epoch 141: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0109, Test L1 Norm: 0.0091, Train Linf Norm: 1.0245, Test Linf Norm: 0.7185\n",
            "Epoch 142: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0108, Test L1 Norm: 0.0092, Train Linf Norm: 0.9990, Test Linf Norm: 0.7095\n",
            "Epoch 143: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0108, Test L1 Norm: 0.0089, Train Linf Norm: 1.0381, Test Linf Norm: 0.7206\n",
            "Epoch 144: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0108, Test L1 Norm: 0.0091, Train Linf Norm: 1.0102, Test Linf Norm: 0.7138\n",
            "Epoch 145: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0106, Test L1 Norm: 0.0090, Train Linf Norm: 1.0084, Test Linf Norm: 0.7148\n",
            "Epoch 146: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0106, Test L1 Norm: 0.0089, Train Linf Norm: 0.9992, Test Linf Norm: 0.7047\n",
            "Epoch 147: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0106, Test L1 Norm: 0.0090, Train Linf Norm: 1.0193, Test Linf Norm: 0.7037\n",
            "Epoch 148: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0106, Test L1 Norm: 0.0090, Train Linf Norm: 1.0317, Test Linf Norm: 0.7087\n",
            "Epoch 149: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0105, Test L1 Norm: 0.0089, Train Linf Norm: 0.9998, Test Linf Norm: 0.7099\n",
            "Epoch 150: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0105, Test L1 Norm: 0.0089, Train Linf Norm: 1.0403, Test Linf Norm: 0.7130\n",
            "Epoch 151: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0088, Train Linf Norm: 1.0041, Test Linf Norm: 0.7100\n",
            "Epoch 152: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0088, Train Linf Norm: 1.0083, Test Linf Norm: 0.7112\n",
            "Epoch 153: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0088, Train Linf Norm: 1.0129, Test Linf Norm: 0.7084\n",
            "Epoch 154: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0088, Train Linf Norm: 1.0085, Test Linf Norm: 0.7042\n",
            "Epoch 155: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0088, Train Linf Norm: 1.0147, Test Linf Norm: 0.7040\n",
            "Epoch 156: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0088, Train Linf Norm: 1.0359, Test Linf Norm: 0.7045\n",
            "Epoch 157: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0088, Train Linf Norm: 1.0002, Test Linf Norm: 0.7045\n",
            "Epoch 158: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0088, Train Linf Norm: 0.9992, Test Linf Norm: 0.7046\n",
            "Epoch 159: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0088, Train Linf Norm: 0.9995, Test Linf Norm: 0.7037\n",
            "Epoch 160: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0088, Train Linf Norm: 0.9810, Test Linf Norm: 0.7027\n",
            "Epoch 161: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0088, Train Linf Norm: 1.0253, Test Linf Norm: 0.7046\n",
            "Epoch 162: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0088, Train Linf Norm: 1.0229, Test Linf Norm: 0.7043\n",
            "Epoch 163: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0087, Train Linf Norm: 0.9868, Test Linf Norm: 0.7024\n",
            "Epoch 164: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0087, Train Linf Norm: 1.0010, Test Linf Norm: 0.6942\n",
            "Epoch 165: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0103, Test L1 Norm: 0.0088, Train Linf Norm: 1.0038, Test Linf Norm: 0.6981\n",
            "Epoch 166: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0103, Test L1 Norm: 0.0087, Train Linf Norm: 0.9943, Test Linf Norm: 0.6914\n",
            "Epoch 167: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0103, Test L1 Norm: 0.0087, Train Linf Norm: 0.9892, Test Linf Norm: 0.6976\n",
            "Epoch 168: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0103, Test L1 Norm: 0.0085, Train Linf Norm: 1.0074, Test Linf Norm: 0.6865\n",
            "Epoch 169: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0102, Test L1 Norm: 0.0084, Train Linf Norm: 0.9834, Test Linf Norm: 0.6912\n",
            "Epoch 170: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0101, Test L1 Norm: 0.0086, Train Linf Norm: 0.9926, Test Linf Norm: 0.6884\n",
            "Epoch 171: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0101, Test L1 Norm: 0.0086, Train Linf Norm: 0.9953, Test Linf Norm: 0.6883\n",
            "Epoch 172: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0101, Test L1 Norm: 0.0085, Train Linf Norm: 1.0015, Test Linf Norm: 0.6846\n",
            "Epoch 173: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0085, Train Linf Norm: 0.9796, Test Linf Norm: 0.6923\n",
            "Epoch 174: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0085, Train Linf Norm: 0.9701, Test Linf Norm: 0.6881\n",
            "Epoch 175: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0101, Test L1 Norm: 0.0084, Train Linf Norm: 0.9459, Test Linf Norm: 0.6908\n",
            "Epoch 176: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0084, Train Linf Norm: 0.9757, Test Linf Norm: 0.6874\n",
            "Epoch 177: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0084, Train Linf Norm: 0.9634, Test Linf Norm: 0.6874\n",
            "Epoch 178: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0084, Train Linf Norm: 0.9649, Test Linf Norm: 0.6870\n",
            "Epoch 179: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0084, Train Linf Norm: 0.9636, Test Linf Norm: 0.6867\n",
            "Epoch 180: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0099, Test L1 Norm: 0.0084, Train Linf Norm: 0.9569, Test Linf Norm: 0.6867\n",
            "Epoch 181: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0084, Train Linf Norm: 0.9797, Test Linf Norm: 0.6867\n",
            "Epoch 182: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0084, Train Linf Norm: 0.9713, Test Linf Norm: 0.6866\n",
            "Epoch 183: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0099, Test L1 Norm: 0.0084, Train Linf Norm: 0.9811, Test Linf Norm: 0.6874\n",
            "Epoch 184: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0099, Test L1 Norm: 0.0084, Train Linf Norm: 0.9794, Test Linf Norm: 0.6861\n",
            "Epoch 185: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0099, Test L1 Norm: 0.0084, Train Linf Norm: 0.9679, Test Linf Norm: 0.6847\n",
            "Epoch 186: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0099, Test L1 Norm: 0.0084, Train Linf Norm: 0.9694, Test Linf Norm: 0.6875\n",
            "Epoch 187: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0099, Test L1 Norm: 0.0084, Train Linf Norm: 0.9546, Test Linf Norm: 0.6821\n",
            "Epoch 188: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0099, Test L1 Norm: 0.0083, Train Linf Norm: 0.9648, Test Linf Norm: 0.6786\n",
            "Epoch 189: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0099, Test L1 Norm: 0.0084, Train Linf Norm: 0.9210, Test Linf Norm: 0.6812\n",
            "Epoch 190: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0099, Test L1 Norm: 0.0083, Train Linf Norm: 0.9781, Test Linf Norm: 0.6753\n",
            "Epoch 191: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0098, Test L1 Norm: 0.0082, Train Linf Norm: 0.9198, Test Linf Norm: 0.6719\n",
            "Epoch 192: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0097, Test L1 Norm: 0.0081, Train Linf Norm: 0.9569, Test Linf Norm: 0.6616\n",
            "Epoch 193: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0096, Test L1 Norm: 0.0081, Train Linf Norm: 0.9562, Test Linf Norm: 0.6593\n",
            "Epoch 194: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0096, Test L1 Norm: 0.0082, Train Linf Norm: 0.9470, Test Linf Norm: 0.6646\n",
            "Epoch 195: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0095, Test L1 Norm: 0.0081, Train Linf Norm: 0.8894, Test Linf Norm: 0.6590\n",
            "Epoch 196: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0095, Test L1 Norm: 0.0080, Train Linf Norm: 0.9148, Test Linf Norm: 0.6495\n",
            "Epoch 197: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0095, Test L1 Norm: 0.0080, Train Linf Norm: 0.9291, Test Linf Norm: 0.6474\n",
            "Epoch 198: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0095, Test L1 Norm: 0.0079, Train Linf Norm: 0.9472, Test Linf Norm: 0.6443\n",
            "Epoch 199: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0079, Train Linf Norm: 0.9354, Test Linf Norm: 0.6443\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:21:58,285]\u001b[0m Trial 9 finished with value: 0.007877636802196502 and parameters: {'n_layers': 2, 'n_units_0': 743, 'n_units_1': 315, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'LogCosh', 'optimizer': 'Adagrad', 'lr': 0.00018789171477546116, 'batch_size': 244, 'n_epochs': 200, 'scheduler': 'CosineAnnealingLR', 'T_max': 12}. Best is trial 9 with value: 0.007877636802196502.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 200: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0079, Train Linf Norm: 0.9174, Test Linf Norm: 0.6425\n",
            "Epoch 1: Train Loss: 0.0763, Test Loss: 0.0021, Train L1 Norm: 0.3713, Test L1 Norm: 0.1864, Train Linf Norm: 59.0653, Test Linf Norm: 37.7502\n",
            "Epoch 2: Train Loss: 0.0011, Test Loss: 0.0007, Train L1 Norm: 0.0976, Test L1 Norm: 0.0809, Train Linf Norm: 15.8494, Test Linf Norm: 15.2128\n",
            "Epoch 3: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.0555, Test L1 Norm: 0.0274, Train Linf Norm: 8.9655, Test Linf Norm: 3.1028\n",
            "Epoch 4: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0430, Test L1 Norm: 0.0374, Train Linf Norm: 6.7654, Test Linf Norm: 5.7280\n",
            "Epoch 5: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0383, Test L1 Norm: 0.0223, Train Linf Norm: 6.1952, Test Linf Norm: 2.4981\n",
            "Epoch 6: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0343, Test L1 Norm: 0.0178, Train Linf Norm: 5.5150, Test Linf Norm: 1.5288\n",
            "Epoch 7: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0333, Test L1 Norm: 0.0167, Train Linf Norm: 5.5090, Test Linf Norm: 1.3877\n",
            "Epoch 8: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0331, Test L1 Norm: 0.0167, Train Linf Norm: 5.6144, Test Linf Norm: 1.4849\n",
            "Epoch 9: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0342, Test L1 Norm: 0.0154, Train Linf Norm: 5.8925, Test Linf Norm: 1.2417\n",
            "Epoch 10: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0325, Test L1 Norm: 0.0160, Train Linf Norm: 5.5474, Test Linf Norm: 1.4234\n",
            "Epoch 11: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0319, Test L1 Norm: 0.0158, Train Linf Norm: 5.4722, Test Linf Norm: 1.4334\n",
            "Epoch 12: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0319, Test L1 Norm: 0.0159, Train Linf Norm: 5.4298, Test Linf Norm: 1.4499\n",
            "Epoch 13: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0315, Test L1 Norm: 0.0158, Train Linf Norm: 5.2295, Test Linf Norm: 1.4518\n",
            "Epoch 14: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0317, Test L1 Norm: 0.0157, Train Linf Norm: 5.4308, Test Linf Norm: 1.4320\n",
            "Epoch 15: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0316, Test L1 Norm: 0.0157, Train Linf Norm: 5.3757, Test Linf Norm: 1.4293\n",
            "Epoch 16: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0317, Test L1 Norm: 0.0157, Train Linf Norm: 5.4133, Test Linf Norm: 1.4293\n",
            "Epoch 17: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0316, Test L1 Norm: 0.0156, Train Linf Norm: 5.3992, Test Linf Norm: 1.4217\n",
            "Epoch 18: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0312, Test L1 Norm: 0.0157, Train Linf Norm: 5.3120, Test Linf Norm: 1.4399\n",
            "Epoch 19: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0314, Test L1 Norm: 0.0157, Train Linf Norm: 5.2804, Test Linf Norm: 1.4368\n",
            "Epoch 20: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0314, Test L1 Norm: 0.0157, Train Linf Norm: 5.2531, Test Linf Norm: 1.4494\n",
            "Epoch 21: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0313, Test L1 Norm: 0.0149, Train Linf Norm: 5.3620, Test Linf Norm: 1.3138\n",
            "Epoch 22: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0296, Test L1 Norm: 0.0154, Train Linf Norm: 4.8924, Test Linf Norm: 1.4191\n",
            "Epoch 23: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0279, Test L1 Norm: 0.0147, Train Linf Norm: 4.6310, Test Linf Norm: 1.3279\n",
            "Epoch 24: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0275, Test L1 Norm: 0.0141, Train Linf Norm: 4.5659, Test Linf Norm: 1.2424\n",
            "Epoch 25: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0276, Test L1 Norm: 0.0186, Train Linf Norm: 4.6172, Test Linf Norm: 2.3750\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0267, Test L1 Norm: 0.0173, Train Linf Norm: 4.4357, Test Linf Norm: 2.1428\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0275, Test L1 Norm: 0.0157, Train Linf Norm: 4.6505, Test Linf Norm: 1.7904\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0246, Test L1 Norm: 0.0121, Train Linf Norm: 4.0573, Test Linf Norm: 0.9854\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0254, Test L1 Norm: 0.0154, Train Linf Norm: 4.1945, Test Linf Norm: 1.8480\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0236, Test L1 Norm: 0.0114, Train Linf Norm: 3.8522, Test Linf Norm: 0.8283\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0230, Test L1 Norm: 0.0133, Train Linf Norm: 3.7043, Test Linf Norm: 1.4181\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0227, Test L1 Norm: 0.0107, Train Linf Norm: 3.6664, Test Linf Norm: 0.8212\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0222, Test L1 Norm: 0.0134, Train Linf Norm: 3.6084, Test Linf Norm: 1.5180\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0208, Test L1 Norm: 0.0115, Train Linf Norm: 3.2813, Test Linf Norm: 1.0835\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0207, Test L1 Norm: 0.0167, Train Linf Norm: 3.3089, Test Linf Norm: 2.2640\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0206, Test L1 Norm: 0.0108, Train Linf Norm: 3.3278, Test Linf Norm: 0.9568\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0203, Test L1 Norm: 0.0106, Train Linf Norm: 3.2173, Test Linf Norm: 0.9453\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0200, Test L1 Norm: 0.0103, Train Linf Norm: 3.2623, Test Linf Norm: 0.9089\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0198, Test L1 Norm: 0.0105, Train Linf Norm: 3.1740, Test Linf Norm: 0.9372\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0196, Test L1 Norm: 0.0103, Train Linf Norm: 3.1642, Test Linf Norm: 0.9125\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0194, Test L1 Norm: 0.0102, Train Linf Norm: 3.1024, Test Linf Norm: 0.9016\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0193, Test L1 Norm: 0.0104, Train Linf Norm: 3.0571, Test Linf Norm: 0.9290\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0193, Test L1 Norm: 0.0101, Train Linf Norm: 3.0668, Test Linf Norm: 0.8896\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0192, Test L1 Norm: 0.0101, Train Linf Norm: 3.0579, Test Linf Norm: 0.8955\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0192, Test L1 Norm: 0.0102, Train Linf Norm: 3.1055, Test Linf Norm: 0.9029\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0192, Test L1 Norm: 0.0102, Train Linf Norm: 3.0697, Test Linf Norm: 0.9029\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0192, Test L1 Norm: 0.0102, Train Linf Norm: 3.0685, Test Linf Norm: 0.9057\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0192, Test L1 Norm: 0.0102, Train Linf Norm: 3.0876, Test Linf Norm: 0.8999\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0192, Test L1 Norm: 0.0101, Train Linf Norm: 3.0944, Test Linf Norm: 0.8954\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0192, Test L1 Norm: 0.0101, Train Linf Norm: 3.0631, Test Linf Norm: 0.8869\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0192, Test L1 Norm: 0.0100, Train Linf Norm: 3.0615, Test Linf Norm: 0.8751\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0191, Test L1 Norm: 0.0102, Train Linf Norm: 3.0804, Test Linf Norm: 0.9143\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0191, Test L1 Norm: 0.0098, Train Linf Norm: 3.0563, Test Linf Norm: 0.8561\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0186, Test L1 Norm: 0.0098, Train Linf Norm: 2.9830, Test Linf Norm: 0.8191\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0188, Test L1 Norm: 0.0099, Train Linf Norm: 3.0040, Test Linf Norm: 0.8800\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0188, Test L1 Norm: 0.0100, Train Linf Norm: 3.0124, Test Linf Norm: 0.9054\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0182, Test L1 Norm: 0.0114, Train Linf Norm: 2.8920, Test Linf Norm: 1.2506\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0187, Test L1 Norm: 0.0094, Train Linf Norm: 3.0198, Test Linf Norm: 0.7842\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0185, Test L1 Norm: 0.0091, Train Linf Norm: 2.9232, Test Linf Norm: 0.7670\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0186, Test L1 Norm: 0.0152, Train Linf Norm: 3.0066, Test Linf Norm: 2.1641\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0181, Test L1 Norm: 0.0092, Train Linf Norm: 2.8955, Test Linf Norm: 0.7884\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0184, Test L1 Norm: 0.0091, Train Linf Norm: 3.0315, Test Linf Norm: 0.7742\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0174, Test L1 Norm: 0.0113, Train Linf Norm: 2.8073, Test Linf Norm: 1.3044\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0174, Test L1 Norm: 0.0093, Train Linf Norm: 2.8078, Test Linf Norm: 0.8524\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0171, Test L1 Norm: 0.0091, Train Linf Norm: 2.7819, Test Linf Norm: 0.8471\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0168, Test L1 Norm: 0.0090, Train Linf Norm: 2.6823, Test Linf Norm: 0.7973\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0165, Test L1 Norm: 0.0089, Train Linf Norm: 2.6282, Test Linf Norm: 0.7800\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0167, Test L1 Norm: 0.0088, Train Linf Norm: 2.6571, Test Linf Norm: 0.7748\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0166, Test L1 Norm: 0.0095, Train Linf Norm: 2.6322, Test Linf Norm: 0.9463\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0164, Test L1 Norm: 0.0087, Train Linf Norm: 2.6237, Test Linf Norm: 0.7569\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0164, Test L1 Norm: 0.0087, Train Linf Norm: 2.6437, Test Linf Norm: 0.7992\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0164, Test L1 Norm: 0.0087, Train Linf Norm: 2.6038, Test Linf Norm: 0.7895\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0164, Test L1 Norm: 0.0087, Train Linf Norm: 2.6283, Test Linf Norm: 0.7793\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0163, Test L1 Norm: 0.0087, Train Linf Norm: 2.6181, Test Linf Norm: 0.7710\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0164, Test L1 Norm: 0.0087, Train Linf Norm: 2.6149, Test Linf Norm: 0.7743\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0164, Test L1 Norm: 0.0087, Train Linf Norm: 2.5957, Test Linf Norm: 0.7743\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0163, Test L1 Norm: 0.0087, Train Linf Norm: 2.6108, Test Linf Norm: 0.7730\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0164, Test L1 Norm: 0.0087, Train Linf Norm: 2.6001, Test Linf Norm: 0.7733\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0163, Test L1 Norm: 0.0087, Train Linf Norm: 2.5972, Test Linf Norm: 0.7630\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0163, Test L1 Norm: 0.0086, Train Linf Norm: 2.6287, Test Linf Norm: 0.7564\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0163, Test L1 Norm: 0.0090, Train Linf Norm: 2.5973, Test Linf Norm: 0.8566\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0162, Test L1 Norm: 0.0087, Train Linf Norm: 2.5017, Test Linf Norm: 0.7596\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0162, Test L1 Norm: 0.0088, Train Linf Norm: 2.5778, Test Linf Norm: 0.8188\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0161, Test L1 Norm: 0.0085, Train Linf Norm: 2.5896, Test Linf Norm: 0.7481\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0162, Test L1 Norm: 0.0084, Train Linf Norm: 2.5924, Test Linf Norm: 0.7211\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0161, Test L1 Norm: 0.0085, Train Linf Norm: 2.5875, Test Linf Norm: 0.7525\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0158, Test L1 Norm: 0.0083, Train Linf Norm: 2.5385, Test Linf Norm: 0.7037\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0157, Test L1 Norm: 0.0085, Train Linf Norm: 2.4603, Test Linf Norm: 0.8000\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0156, Test L1 Norm: 0.0080, Train Linf Norm: 2.4733, Test Linf Norm: 0.6612\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0156, Test L1 Norm: 0.0081, Train Linf Norm: 2.4825, Test Linf Norm: 0.6745\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0152, Test L1 Norm: 0.0090, Train Linf Norm: 2.3616, Test Linf Norm: 0.9674\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0148, Test L1 Norm: 0.0093, Train Linf Norm: 2.3184, Test Linf Norm: 1.0226\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0151, Test L1 Norm: 0.0104, Train Linf Norm: 2.3691, Test Linf Norm: 1.3092\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0147, Test L1 Norm: 0.0093, Train Linf Norm: 2.3130, Test Linf Norm: 1.0204\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0148, Test L1 Norm: 0.0077, Train Linf Norm: 2.3173, Test Linf Norm: 0.6149\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0149, Test L1 Norm: 0.0090, Train Linf Norm: 2.3798, Test Linf Norm: 1.0120\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0147, Test L1 Norm: 0.0104, Train Linf Norm: 2.3155, Test Linf Norm: 1.3090\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0148, Test L1 Norm: 0.0105, Train Linf Norm: 2.3337, Test Linf Norm: 1.3643\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0147, Test L1 Norm: 0.0083, Train Linf Norm: 2.3443, Test Linf Norm: 0.8207\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0146, Test L1 Norm: 0.0101, Train Linf Norm: 2.3062, Test Linf Norm: 1.2483\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0146, Test L1 Norm: 0.0091, Train Linf Norm: 2.3134, Test Linf Norm: 1.0297\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0145, Test L1 Norm: 0.0090, Train Linf Norm: 2.2941, Test Linf Norm: 0.9922\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0145, Test L1 Norm: 0.0090, Train Linf Norm: 2.3095, Test Linf Norm: 1.0034\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0145, Test L1 Norm: 0.0092, Train Linf Norm: 2.3089, Test Linf Norm: 1.0409\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0145, Test L1 Norm: 0.0091, Train Linf Norm: 2.3029, Test Linf Norm: 1.0231\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0144, Test L1 Norm: 0.0091, Train Linf Norm: 2.2794, Test Linf Norm: 1.0231\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0145, Test L1 Norm: 0.0092, Train Linf Norm: 2.3004, Test Linf Norm: 1.0462\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0145, Test L1 Norm: 0.0091, Train Linf Norm: 2.3082, Test Linf Norm: 1.0231\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0145, Test L1 Norm: 0.0088, Train Linf Norm: 2.2857, Test Linf Norm: 0.9425\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0145, Test L1 Norm: 0.0094, Train Linf Norm: 2.2814, Test Linf Norm: 1.1067\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0145, Test L1 Norm: 0.0081, Train Linf Norm: 2.3195, Test Linf Norm: 0.7722\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0144, Test L1 Norm: 0.0087, Train Linf Norm: 2.3028, Test Linf Norm: 0.9163\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0144, Test L1 Norm: 0.0088, Train Linf Norm: 2.2836, Test Linf Norm: 0.9406\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0142, Test L1 Norm: 0.0094, Train Linf Norm: 2.2184, Test Linf Norm: 1.1463\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0141, Test L1 Norm: 0.0097, Train Linf Norm: 2.1931, Test Linf Norm: 1.2091\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0141, Test L1 Norm: 0.0081, Train Linf Norm: 2.2195, Test Linf Norm: 0.7898\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0142, Test L1 Norm: 0.0084, Train Linf Norm: 2.2447, Test Linf Norm: 0.8870\n",
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0142, Test L1 Norm: 0.0075, Train Linf Norm: 2.2462, Test Linf Norm: 0.6812\n",
            "Epoch 119: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0141, Test L1 Norm: 0.0075, Train Linf Norm: 2.2167, Test Linf Norm: 0.6397\n",
            "Epoch 120: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0143, Test L1 Norm: 0.0075, Train Linf Norm: 2.2410, Test Linf Norm: 0.5087\n",
            "Epoch 121: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0140, Test L1 Norm: 0.0103, Train Linf Norm: 2.2113, Test Linf Norm: 1.3233\n",
            "Epoch 122: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0142, Test L1 Norm: 0.0074, Train Linf Norm: 2.2358, Test Linf Norm: 0.5828\n",
            "Epoch 123: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0137, Test L1 Norm: 0.0078, Train Linf Norm: 2.1335, Test Linf Norm: 0.7313\n",
            "Epoch 124: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0136, Test L1 Norm: 0.0073, Train Linf Norm: 2.1150, Test Linf Norm: 0.6056\n",
            "Epoch 125: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0138, Test L1 Norm: 0.0074, Train Linf Norm: 2.1640, Test Linf Norm: 0.6391\n",
            "Epoch 126: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0135, Test L1 Norm: 0.0078, Train Linf Norm: 2.1123, Test Linf Norm: 0.7306\n",
            "Epoch 127: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0133, Test L1 Norm: 0.0083, Train Linf Norm: 2.0321, Test Linf Norm: 0.9071\n",
            "Epoch 128: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0134, Test L1 Norm: 0.0073, Train Linf Norm: 2.1078, Test Linf Norm: 0.6351\n",
            "Epoch 129: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0133, Test L1 Norm: 0.0078, Train Linf Norm: 2.0690, Test Linf Norm: 0.7675\n",
            "Epoch 130: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0134, Test L1 Norm: 0.0073, Train Linf Norm: 2.0924, Test Linf Norm: 0.6407\n",
            "Epoch 131: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0133, Test L1 Norm: 0.0074, Train Linf Norm: 2.0956, Test Linf Norm: 0.6798\n",
            "Epoch 132: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0132, Test L1 Norm: 0.0073, Train Linf Norm: 2.0567, Test Linf Norm: 0.6363\n",
            "Epoch 133: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0133, Test L1 Norm: 0.0077, Train Linf Norm: 2.0712, Test Linf Norm: 0.7432\n",
            "Epoch 134: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0133, Test L1 Norm: 0.0073, Train Linf Norm: 2.0928, Test Linf Norm: 0.6505\n",
            "Epoch 135: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0133, Test L1 Norm: 0.0074, Train Linf Norm: 2.0770, Test Linf Norm: 0.6710\n",
            "Epoch 136: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0133, Test L1 Norm: 0.0074, Train Linf Norm: 2.0737, Test Linf Norm: 0.6710\n",
            "Epoch 137: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0133, Test L1 Norm: 0.0074, Train Linf Norm: 2.0776, Test Linf Norm: 0.6652\n",
            "Epoch 138: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0133, Test L1 Norm: 0.0074, Train Linf Norm: 2.0834, Test Linf Norm: 0.6687\n",
            "Epoch 139: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0133, Test L1 Norm: 0.0075, Train Linf Norm: 2.0814, Test Linf Norm: 0.7081\n",
            "Epoch 140: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0133, Test L1 Norm: 0.0074, Train Linf Norm: 2.0827, Test Linf Norm: 0.6779\n",
            "Epoch 141: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0132, Test L1 Norm: 0.0074, Train Linf Norm: 2.0718, Test Linf Norm: 0.6663\n",
            "Epoch 142: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0132, Test L1 Norm: 0.0072, Train Linf Norm: 2.0804, Test Linf Norm: 0.6362\n",
            "Epoch 143: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0132, Test L1 Norm: 0.0077, Train Linf Norm: 2.0561, Test Linf Norm: 0.7542\n",
            "Epoch 144: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0132, Test L1 Norm: 0.0079, Train Linf Norm: 2.0496, Test Linf Norm: 0.8313\n",
            "Epoch 145: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0131, Test L1 Norm: 0.0072, Train Linf Norm: 2.0586, Test Linf Norm: 0.6453\n",
            "Epoch 146: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0133, Test L1 Norm: 0.0072, Train Linf Norm: 2.1077, Test Linf Norm: 0.6194\n",
            "Epoch 147: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0132, Test L1 Norm: 0.0072, Train Linf Norm: 2.0967, Test Linf Norm: 0.6716\n",
            "Epoch 148: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0130, Test L1 Norm: 0.0073, Train Linf Norm: 2.0514, Test Linf Norm: 0.6727\n",
            "Epoch 149: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0130, Test L1 Norm: 0.0071, Train Linf Norm: 2.0421, Test Linf Norm: 0.6162\n",
            "Epoch 150: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0128, Test L1 Norm: 0.0072, Train Linf Norm: 1.9522, Test Linf Norm: 0.6545\n",
            "Epoch 151: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0127, Test L1 Norm: 0.0071, Train Linf Norm: 1.9291, Test Linf Norm: 0.6150\n",
            "Epoch 152: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0126, Test L1 Norm: 0.0071, Train Linf Norm: 1.9483, Test Linf Norm: 0.6558\n",
            "Epoch 153: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0127, Test L1 Norm: 0.0070, Train Linf Norm: 1.9937, Test Linf Norm: 0.6660\n",
            "Epoch 154: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0125, Test L1 Norm: 0.0069, Train Linf Norm: 1.9321, Test Linf Norm: 0.6319\n",
            "Epoch 155: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0085, Train Linf Norm: 1.8751, Test Linf Norm: 0.9866\n",
            "Epoch 156: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0126, Test L1 Norm: 0.0070, Train Linf Norm: 1.9869, Test Linf Norm: 0.6508\n",
            "Epoch 157: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0075, Train Linf Norm: 1.8829, Test Linf Norm: 0.7699\n",
            "Epoch 158: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0069, Train Linf Norm: 1.9260, Test Linf Norm: 0.5998\n",
            "Epoch 159: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0070, Train Linf Norm: 1.8943, Test Linf Norm: 0.6469\n",
            "Epoch 160: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0069, Train Linf Norm: 1.9057, Test Linf Norm: 0.6360\n",
            "Epoch 161: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0069, Train Linf Norm: 1.8908, Test Linf Norm: 0.6317\n",
            "Epoch 162: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0069, Train Linf Norm: 1.8870, Test Linf Norm: 0.6210\n",
            "Epoch 163: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0069, Train Linf Norm: 1.9128, Test Linf Norm: 0.6287\n",
            "Epoch 164: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0069, Train Linf Norm: 1.8190, Test Linf Norm: 0.6264\n",
            "Epoch 165: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0069, Train Linf Norm: 1.8767, Test Linf Norm: 0.6242\n",
            "Epoch 166: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0069, Train Linf Norm: 1.8832, Test Linf Norm: 0.6242\n",
            "Epoch 167: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0069, Train Linf Norm: 1.8943, Test Linf Norm: 0.6237\n",
            "Epoch 168: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0069, Train Linf Norm: 1.9028, Test Linf Norm: 0.6259\n",
            "Epoch 169: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0069, Train Linf Norm: 1.9149, Test Linf Norm: 0.6099\n",
            "Epoch 170: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0069, Train Linf Norm: 1.8722, Test Linf Norm: 0.6310\n",
            "Epoch 171: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0069, Train Linf Norm: 1.8178, Test Linf Norm: 0.6249\n",
            "Epoch 172: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0069, Train Linf Norm: 1.8914, Test Linf Norm: 0.6079\n",
            "Epoch 173: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0068, Train Linf Norm: 1.8766, Test Linf Norm: 0.6121\n",
            "Epoch 174: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0069, Train Linf Norm: 1.8876, Test Linf Norm: 0.6361\n",
            "Epoch 175: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0069, Train Linf Norm: 1.9175, Test Linf Norm: 0.6258\n",
            "Epoch 176: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0118, Test L1 Norm: 0.0069, Train Linf Norm: 1.7872, Test Linf Norm: 0.6434\n",
            "Epoch 177: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0118, Test L1 Norm: 0.0068, Train Linf Norm: 1.7854, Test Linf Norm: 0.5909\n",
            "Epoch 178: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0120, Test L1 Norm: 0.0067, Train Linf Norm: 1.8725, Test Linf Norm: 0.6039\n",
            "Epoch 179: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0068, Train Linf Norm: 1.9090, Test Linf Norm: 0.6511\n",
            "Epoch 180: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0120, Test L1 Norm: 0.0067, Train Linf Norm: 1.8625, Test Linf Norm: 0.5893\n",
            "Epoch 181: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0119, Test L1 Norm: 0.0073, Train Linf Norm: 1.8375, Test Linf Norm: 0.7263\n",
            "Epoch 182: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0119, Test L1 Norm: 0.0065, Train Linf Norm: 1.8379, Test Linf Norm: 0.5465\n",
            "Epoch 183: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0117, Test L1 Norm: 0.0067, Train Linf Norm: 1.8047, Test Linf Norm: 0.6082\n",
            "Epoch 184: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0116, Test L1 Norm: 0.0067, Train Linf Norm: 1.7869, Test Linf Norm: 0.6152\n",
            "Epoch 185: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0117, Test L1 Norm: 0.0066, Train Linf Norm: 1.7876, Test Linf Norm: 0.6262\n",
            "Epoch 186: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0115, Test L1 Norm: 0.0064, Train Linf Norm: 1.7909, Test Linf Norm: 0.5614\n",
            "Epoch 187: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0115, Test L1 Norm: 0.0066, Train Linf Norm: 1.7750, Test Linf Norm: 0.5940\n",
            "Epoch 188: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0115, Test L1 Norm: 0.0066, Train Linf Norm: 1.7695, Test Linf Norm: 0.6165\n",
            "Epoch 189: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0115, Test L1 Norm: 0.0065, Train Linf Norm: 1.7659, Test Linf Norm: 0.5761\n",
            "Epoch 190: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0114, Test L1 Norm: 0.0066, Train Linf Norm: 1.7430, Test Linf Norm: 0.5739\n",
            "Epoch 191: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0114, Test L1 Norm: 0.0065, Train Linf Norm: 1.7552, Test Linf Norm: 0.5858\n",
            "Epoch 192: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0114, Test L1 Norm: 0.0066, Train Linf Norm: 1.7204, Test Linf Norm: 0.5969\n",
            "Epoch 193: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0114, Test L1 Norm: 0.0066, Train Linf Norm: 1.7436, Test Linf Norm: 0.5926\n",
            "Epoch 194: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0114, Test L1 Norm: 0.0065, Train Linf Norm: 1.7505, Test Linf Norm: 0.5837\n",
            "Epoch 195: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0114, Test L1 Norm: 0.0065, Train Linf Norm: 1.7530, Test Linf Norm: 0.5858\n",
            "Epoch 196: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0114, Test L1 Norm: 0.0065, Train Linf Norm: 1.7371, Test Linf Norm: 0.5858\n",
            "Epoch 197: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0114, Test L1 Norm: 0.0065, Train Linf Norm: 1.7246, Test Linf Norm: 0.5856\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:26:58,122]\u001b[0m Trial 10 finished with value: 0.006550250181416049 and parameters: {'n_layers': 3, 'n_units_0': 1031, 'n_units_1': 316, 'n_units_2': 26, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'LogCosh', 'optimizer': 'Adagrad', 'lr': 0.00027497635554443126, 'batch_size': 254, 'n_epochs': 198, 'scheduler': 'CosineAnnealingLR', 'T_max': 15}. Best is trial 10 with value: 0.006550250181416049.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 198: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0114, Test L1 Norm: 0.0066, Train Linf Norm: 1.7642, Test Linf Norm: 0.5874\n",
            "Epoch 1: Train Loss: 0.1906, Test Loss: 0.0115, Train L1 Norm: 0.3643, Test L1 Norm: 0.0696, Train Linf Norm: 38.3396, Test Linf Norm: 1.0513\n",
            "Epoch 2: Train Loss: 0.0052, Test Loss: 0.0047, Train L1 Norm: 0.0706, Test L1 Norm: 0.1782, Train Linf Norm: 6.4909, Test Linf Norm: 34.9886\n",
            "Epoch 3: Train Loss: 0.0017, Test Loss: 0.0012, Train L1 Norm: 0.0602, Test L1 Norm: 0.1122, Train Linf Norm: 7.4495, Test Linf Norm: 21.5754\n",
            "Epoch 4: Train Loss: 0.0009, Test Loss: 0.0007, Train L1 Norm: 0.0549, Test L1 Norm: 0.1165, Train Linf Norm: 6.9216, Test Linf Norm: 23.4436\n",
            "Epoch 5: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0482, Test L1 Norm: 0.0894, Train Linf Norm: 6.0912, Test Linf Norm: 17.2202\n",
            "Epoch 6: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0441, Test L1 Norm: 0.0850, Train Linf Norm: 5.7342, Test Linf Norm: 16.5986\n",
            "Epoch 7: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0408, Test L1 Norm: 0.0672, Train Linf Norm: 5.3392, Test Linf Norm: 12.3758\n",
            "Epoch 8: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0377, Test L1 Norm: 0.0671, Train Linf Norm: 4.6327, Test Linf Norm: 12.6169\n",
            "Epoch 9: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0362, Test L1 Norm: 0.0670, Train Linf Norm: 4.7451, Test Linf Norm: 12.8024\n",
            "Epoch 10: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0353, Test L1 Norm: 0.0648, Train Linf Norm: 4.6623, Test Linf Norm: 12.3614\n",
            "Epoch 11: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0347, Test L1 Norm: 0.0588, Train Linf Norm: 4.5727, Test Linf Norm: 10.8862\n",
            "Epoch 12: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0337, Test L1 Norm: 0.0607, Train Linf Norm: 4.4284, Test Linf Norm: 11.4228\n",
            "Epoch 13: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0336, Test L1 Norm: 0.0621, Train Linf Norm: 4.4085, Test Linf Norm: 11.8355\n",
            "Epoch 14: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0332, Test L1 Norm: 0.0610, Train Linf Norm: 4.3108, Test Linf Norm: 11.5835\n",
            "Epoch 15: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0333, Test L1 Norm: 0.0613, Train Linf Norm: 4.3947, Test Linf Norm: 11.6535\n",
            "Epoch 16: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0333, Test L1 Norm: 0.0613, Train Linf Norm: 4.4427, Test Linf Norm: 11.6535\n",
            "Epoch 17: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0333, Test L1 Norm: 0.0610, Train Linf Norm: 4.3724, Test Linf Norm: 11.5938\n",
            "Epoch 18: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0332, Test L1 Norm: 0.0611, Train Linf Norm: 4.3453, Test Linf Norm: 11.6349\n",
            "Epoch 19: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0329, Test L1 Norm: 0.0611, Train Linf Norm: 4.3246, Test Linf Norm: 11.6311\n",
            "Epoch 20: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0327, Test L1 Norm: 0.0591, Train Linf Norm: 4.2327, Test Linf Norm: 11.2006\n",
            "Epoch 21: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0326, Test L1 Norm: 0.0581, Train Linf Norm: 4.3349, Test Linf Norm: 10.9836\n",
            "Epoch 22: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0325, Test L1 Norm: 0.0585, Train Linf Norm: 4.1391, Test Linf Norm: 11.1681\n",
            "Epoch 23: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0314, Test L1 Norm: 0.0550, Train Linf Norm: 4.0686, Test Linf Norm: 10.3576\n",
            "Epoch 24: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0309, Test L1 Norm: 0.0521, Train Linf Norm: 4.1377, Test Linf Norm: 9.7643\n",
            "Epoch 25: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0293, Test L1 Norm: 0.0547, Train Linf Norm: 3.8237, Test Linf Norm: 10.5214\n",
            "Epoch 26: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0287, Test L1 Norm: 0.0485, Train Linf Norm: 3.6696, Test Linf Norm: 9.0835\n",
            "Epoch 27: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0280, Test L1 Norm: 0.0422, Train Linf Norm: 3.6477, Test Linf Norm: 7.5681\n",
            "Epoch 28: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0272, Test L1 Norm: 0.0419, Train Linf Norm: 3.6388, Test Linf Norm: 7.6484\n",
            "Epoch 29: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0254, Test L1 Norm: 0.0389, Train Linf Norm: 3.2450, Test Linf Norm: 6.9775\n",
            "Epoch 30: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0245, Test L1 Norm: 0.0405, Train Linf Norm: 3.1476, Test Linf Norm: 7.4993\n",
            "Epoch 31: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0239, Test L1 Norm: 0.0430, Train Linf Norm: 3.0845, Test Linf Norm: 8.1735\n",
            "Epoch 32: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0230, Test L1 Norm: 0.0387, Train Linf Norm: 2.8733, Test Linf Norm: 7.2032\n",
            "Epoch 33: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0224, Test L1 Norm: 0.0226, Train Linf Norm: 2.7642, Test Linf Norm: 3.0420\n",
            "Epoch 34: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0218, Test L1 Norm: 0.0270, Train Linf Norm: 2.7503, Test Linf Norm: 4.3631\n",
            "Epoch 35: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0211, Test L1 Norm: 0.0271, Train Linf Norm: 2.6381, Test Linf Norm: 4.4186\n",
            "Epoch 36: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0209, Test L1 Norm: 0.0255, Train Linf Norm: 2.6810, Test Linf Norm: 4.0734\n",
            "Epoch 37: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0203, Test L1 Norm: 0.0268, Train Linf Norm: 2.4969, Test Linf Norm: 4.4438\n",
            "Epoch 38: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0200, Test L1 Norm: 0.0239, Train Linf Norm: 2.5122, Test Linf Norm: 3.7357\n",
            "Epoch 39: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0197, Test L1 Norm: 0.0216, Train Linf Norm: 2.4897, Test Linf Norm: 3.1601\n",
            "Epoch 40: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0196, Test L1 Norm: 0.0214, Train Linf Norm: 2.4776, Test Linf Norm: 3.1260\n",
            "Epoch 41: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0196, Test L1 Norm: 0.0232, Train Linf Norm: 2.4776, Test Linf Norm: 3.5874\n",
            "Epoch 42: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0193, Test L1 Norm: 0.0219, Train Linf Norm: 2.4270, Test Linf Norm: 3.2916\n",
            "Epoch 43: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0194, Test L1 Norm: 0.0220, Train Linf Norm: 2.4622, Test Linf Norm: 3.3062\n",
            "Epoch 44: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0193, Test L1 Norm: 0.0218, Train Linf Norm: 2.3927, Test Linf Norm: 3.2715\n",
            "Epoch 45: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0193, Test L1 Norm: 0.0217, Train Linf Norm: 2.4384, Test Linf Norm: 3.2390\n",
            "Epoch 46: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0193, Test L1 Norm: 0.0217, Train Linf Norm: 2.4181, Test Linf Norm: 3.2390\n",
            "Epoch 47: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0193, Test L1 Norm: 0.0217, Train Linf Norm: 2.4583, Test Linf Norm: 3.2380\n",
            "Epoch 48: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0193, Test L1 Norm: 0.0217, Train Linf Norm: 2.4361, Test Linf Norm: 3.2309\n",
            "Epoch 49: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0193, Test L1 Norm: 0.0212, Train Linf Norm: 2.4410, Test Linf Norm: 3.0985\n",
            "Epoch 50: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0193, Test L1 Norm: 0.0218, Train Linf Norm: 2.4301, Test Linf Norm: 3.2636\n",
            "Epoch 51: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0191, Test L1 Norm: 0.0208, Train Linf Norm: 2.4210, Test Linf Norm: 3.0274\n",
            "Epoch 52: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0191, Test L1 Norm: 0.0206, Train Linf Norm: 2.3849, Test Linf Norm: 3.0049\n",
            "Epoch 53: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0189, Test L1 Norm: 0.0184, Train Linf Norm: 2.4087, Test Linf Norm: 2.4434\n",
            "Epoch 54: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0187, Test L1 Norm: 0.0186, Train Linf Norm: 2.3065, Test Linf Norm: 2.5037\n",
            "Epoch 55: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0187, Test L1 Norm: 0.0169, Train Linf Norm: 2.3253, Test Linf Norm: 2.1071\n",
            "Epoch 56: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0183, Test L1 Norm: 0.0157, Train Linf Norm: 2.3017, Test Linf Norm: 1.8177\n",
            "Epoch 57: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0180, Test L1 Norm: 0.0195, Train Linf Norm: 2.2492, Test Linf Norm: 2.8230\n",
            "Epoch 58: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0179, Test L1 Norm: 0.0169, Train Linf Norm: 2.2827, Test Linf Norm: 2.2084\n",
            "Epoch 59: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0179, Test L1 Norm: 0.0195, Train Linf Norm: 2.2992, Test Linf Norm: 2.8384\n",
            "Epoch 60: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0172, Test L1 Norm: 0.0158, Train Linf Norm: 2.1515, Test Linf Norm: 1.9900\n",
            "Epoch 61: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0170, Test L1 Norm: 0.0202, Train Linf Norm: 2.0842, Test Linf Norm: 2.9689\n",
            "Epoch 62: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0170, Test L1 Norm: 0.0147, Train Linf Norm: 2.1141, Test Linf Norm: 1.7139\n",
            "Epoch 63: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0165, Test L1 Norm: 0.0116, Train Linf Norm: 2.0502, Test Linf Norm: 0.8748\n",
            "Epoch 64: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0163, Test L1 Norm: 0.0117, Train Linf Norm: 2.0324, Test Linf Norm: 0.9854\n",
            "Epoch 65: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0162, Test L1 Norm: 0.0115, Train Linf Norm: 2.0242, Test Linf Norm: 0.9108\n",
            "Epoch 66: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0160, Test L1 Norm: 0.0114, Train Linf Norm: 1.9601, Test Linf Norm: 0.8880\n",
            "Epoch 67: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0159, Test L1 Norm: 0.0113, Train Linf Norm: 1.9724, Test Linf Norm: 0.9005\n",
            "Epoch 68: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0158, Test L1 Norm: 0.0113, Train Linf Norm: 1.9592, Test Linf Norm: 0.8365\n",
            "Epoch 69: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0157, Test L1 Norm: 0.0113, Train Linf Norm: 1.9301, Test Linf Norm: 1.0305\n",
            "Epoch 70: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0157, Test L1 Norm: 0.0112, Train Linf Norm: 1.9444, Test Linf Norm: 0.9614\n",
            "Epoch 71: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0155, Test L1 Norm: 0.0111, Train Linf Norm: 1.9417, Test Linf Norm: 0.9529\n",
            "Epoch 72: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0155, Test L1 Norm: 0.0111, Train Linf Norm: 1.9438, Test Linf Norm: 0.9469\n",
            "Epoch 73: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0155, Test L1 Norm: 0.0111, Train Linf Norm: 1.9105, Test Linf Norm: 0.9620\n",
            "Epoch 74: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0155, Test L1 Norm: 0.0111, Train Linf Norm: 1.9042, Test Linf Norm: 0.9467\n",
            "Epoch 75: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0154, Test L1 Norm: 0.0111, Train Linf Norm: 1.9087, Test Linf Norm: 0.9441\n",
            "Epoch 76: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0154, Test L1 Norm: 0.0111, Train Linf Norm: 1.9141, Test Linf Norm: 0.9441\n",
            "Epoch 77: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0154, Test L1 Norm: 0.0111, Train Linf Norm: 1.9107, Test Linf Norm: 0.9478\n",
            "Epoch 78: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0154, Test L1 Norm: 0.0111, Train Linf Norm: 1.8998, Test Linf Norm: 0.9454\n",
            "Epoch 79: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0154, Test L1 Norm: 0.0111, Train Linf Norm: 1.9195, Test Linf Norm: 0.9614\n",
            "Epoch 80: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0154, Test L1 Norm: 0.0110, Train Linf Norm: 1.9024, Test Linf Norm: 0.9295\n",
            "Epoch 81: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0153, Test L1 Norm: 0.0110, Train Linf Norm: 1.8656, Test Linf Norm: 0.9382\n",
            "Epoch 82: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0154, Test L1 Norm: 0.0109, Train Linf Norm: 1.8799, Test Linf Norm: 0.9223\n",
            "Epoch 83: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0152, Test L1 Norm: 0.0109, Train Linf Norm: 1.8865, Test Linf Norm: 0.8917\n",
            "Epoch 84: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0152, Test L1 Norm: 0.0110, Train Linf Norm: 1.8371, Test Linf Norm: 0.9044\n",
            "Epoch 85: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0152, Test L1 Norm: 0.0108, Train Linf Norm: 1.8370, Test Linf Norm: 0.8610\n",
            "Epoch 86: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0150, Test L1 Norm: 0.0108, Train Linf Norm: 1.8353, Test Linf Norm: 0.9210\n",
            "Epoch 87: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0150, Test L1 Norm: 0.0106, Train Linf Norm: 1.8751, Test Linf Norm: 0.8815\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0148, Test L1 Norm: 0.0109, Train Linf Norm: 1.7559, Test Linf Norm: 0.9625\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0148, Test L1 Norm: 0.0107, Train Linf Norm: 1.8141, Test Linf Norm: 0.9468\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0148, Test L1 Norm: 0.0103, Train Linf Norm: 1.8550, Test Linf Norm: 0.9086\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0144, Test L1 Norm: 0.0102, Train Linf Norm: 1.7338, Test Linf Norm: 0.8453\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0145, Test L1 Norm: 0.0103, Train Linf Norm: 1.8124, Test Linf Norm: 0.7905\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0140, Test L1 Norm: 0.0132, Train Linf Norm: 1.6831, Test Linf Norm: 1.4375\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0141, Test L1 Norm: 0.0101, Train Linf Norm: 1.6929, Test Linf Norm: 0.8298\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0142, Test L1 Norm: 0.0099, Train Linf Norm: 1.7685, Test Linf Norm: 0.8046\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0141, Test L1 Norm: 0.0097, Train Linf Norm: 1.7487, Test Linf Norm: 0.7804\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0138, Test L1 Norm: 0.0098, Train Linf Norm: 1.6993, Test Linf Norm: 0.8127\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0137, Test L1 Norm: 0.0099, Train Linf Norm: 1.6823, Test Linf Norm: 0.8270\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0137, Test L1 Norm: 0.0097, Train Linf Norm: 1.6603, Test Linf Norm: 0.7997\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0136, Test L1 Norm: 0.0097, Train Linf Norm: 1.6655, Test Linf Norm: 0.8186\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0136, Test L1 Norm: 0.0097, Train Linf Norm: 1.6798, Test Linf Norm: 0.8290\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0136, Test L1 Norm: 0.0097, Train Linf Norm: 1.6820, Test Linf Norm: 0.7941\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0135, Test L1 Norm: 0.0096, Train Linf Norm: 1.6415, Test Linf Norm: 0.7904\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0136, Test L1 Norm: 0.0097, Train Linf Norm: 1.6326, Test Linf Norm: 0.8075\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0135, Test L1 Norm: 0.0097, Train Linf Norm: 1.6516, Test Linf Norm: 0.8080\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0136, Test L1 Norm: 0.0097, Train Linf Norm: 1.6876, Test Linf Norm: 0.8080\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0136, Test L1 Norm: 0.0097, Train Linf Norm: 1.6061, Test Linf Norm: 0.8021\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0136, Test L1 Norm: 0.0097, Train Linf Norm: 1.6635, Test Linf Norm: 0.8072\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0135, Test L1 Norm: 0.0097, Train Linf Norm: 1.6553, Test Linf Norm: 0.8115\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0135, Test L1 Norm: 0.0096, Train Linf Norm: 1.6323, Test Linf Norm: 0.7852\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0135, Test L1 Norm: 0.0097, Train Linf Norm: 1.6281, Test Linf Norm: 0.8061\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0135, Test L1 Norm: 0.0096, Train Linf Norm: 1.6299, Test Linf Norm: 0.8105\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0135, Test L1 Norm: 0.0098, Train Linf Norm: 1.6586, Test Linf Norm: 0.8532\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0134, Test L1 Norm: 0.0096, Train Linf Norm: 1.6664, Test Linf Norm: 0.8164\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0134, Test L1 Norm: 0.0095, Train Linf Norm: 1.6350, Test Linf Norm: 0.7789\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0134, Test L1 Norm: 0.0096, Train Linf Norm: 1.5489, Test Linf Norm: 0.7997\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0133, Test L1 Norm: 0.0096, Train Linf Norm: 1.6366, Test Linf Norm: 0.8273\n",
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0132, Test L1 Norm: 0.0094, Train Linf Norm: 1.6219, Test Linf Norm: 0.7809\n",
            "Epoch 119: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0132, Test L1 Norm: 0.0104, Train Linf Norm: 1.6480, Test Linf Norm: 0.9575\n",
            "Epoch 120: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0132, Test L1 Norm: 0.0095, Train Linf Norm: 1.5973, Test Linf Norm: 0.6872\n",
            "Epoch 121: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0132, Test L1 Norm: 0.0094, Train Linf Norm: 1.6111, Test Linf Norm: 0.7863\n",
            "Epoch 122: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0129, Test L1 Norm: 0.0094, Train Linf Norm: 1.5719, Test Linf Norm: 0.8041\n",
            "Epoch 123: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0128, Test L1 Norm: 0.0093, Train Linf Norm: 1.5277, Test Linf Norm: 0.7604\n",
            "Epoch 124: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0128, Test L1 Norm: 0.0094, Train Linf Norm: 1.5748, Test Linf Norm: 0.8146\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:30:07,617]\u001b[0m Trial 11 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 125: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0126, Test L1 Norm: 0.0095, Train Linf Norm: 1.4895, Test Linf Norm: 0.8484\n",
            "Epoch 1: Train Loss: 0.0504, Test Loss: 0.0005, Train L1 Norm: 0.2424, Test L1 Norm: 0.2087, Train Linf Norm: 29.0900, Test Linf Norm: 37.8057\n",
            "Epoch 2: Train Loss: 0.0006, Test Loss: 0.0002, Train L1 Norm: 0.0692, Test L1 Norm: 0.1515, Train Linf Norm: 9.6640, Test Linf Norm: 27.6704\n",
            "Epoch 3: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0485, Test L1 Norm: 0.1044, Train Linf Norm: 6.7487, Test Linf Norm: 18.8178\n",
            "Epoch 4: Train Loss: 0.0001, Test Loss: 0.0004, Train L1 Norm: 0.0426, Test L1 Norm: 0.1133, Train Linf Norm: 6.0393, Test Linf Norm: 20.3378\n",
            "Epoch 5: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0375, Test L1 Norm: 0.1052, Train Linf Norm: 5.3329, Test Linf Norm: 19.1012\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0336, Test L1 Norm: 0.0868, Train Linf Norm: 4.8256, Test Linf Norm: 15.8103\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0318, Test L1 Norm: 0.0849, Train Linf Norm: 4.5412, Test Linf Norm: 15.4912\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0304, Test L1 Norm: 0.0829, Train Linf Norm: 4.3538, Test Linf Norm: 15.1483\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0303, Test L1 Norm: 0.0807, Train Linf Norm: 4.2899, Test Linf Norm: 14.7125\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0293, Test L1 Norm: 0.0752, Train Linf Norm: 4.1418, Test Linf Norm: 13.5598\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0288, Test L1 Norm: 0.0778, Train Linf Norm: 4.1774, Test Linf Norm: 14.2047\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0286, Test L1 Norm: 0.0780, Train Linf Norm: 4.0773, Test Linf Norm: 14.2292\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0290, Test L1 Norm: 0.0771, Train Linf Norm: 4.2458, Test Linf Norm: 14.0966\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0287, Test L1 Norm: 0.0773, Train Linf Norm: 4.1619, Test Linf Norm: 14.1297\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0285, Test L1 Norm: 0.0773, Train Linf Norm: 4.0866, Test Linf Norm: 14.1297\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0285, Test L1 Norm: 0.0771, Train Linf Norm: 4.1268, Test Linf Norm: 14.0910\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0284, Test L1 Norm: 0.0770, Train Linf Norm: 4.0850, Test Linf Norm: 14.0724\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0283, Test L1 Norm: 0.0763, Train Linf Norm: 4.0585, Test Linf Norm: 13.9422\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0281, Test L1 Norm: 0.0737, Train Linf Norm: 4.0904, Test Linf Norm: 13.4358\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0277, Test L1 Norm: 0.0736, Train Linf Norm: 4.0169, Test Linf Norm: 13.3930\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0274, Test L1 Norm: 0.0741, Train Linf Norm: 3.9531, Test Linf Norm: 13.5408\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0273, Test L1 Norm: 0.0717, Train Linf Norm: 3.9719, Test Linf Norm: 13.0820\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0262, Test L1 Norm: 0.0655, Train Linf Norm: 3.7829, Test Linf Norm: 11.8020\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0258, Test L1 Norm: 0.0716, Train Linf Norm: 3.7376, Test Linf Norm: 13.0179\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0253, Test L1 Norm: 0.0617, Train Linf Norm: 3.4946, Test Linf Norm: 11.1475\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0248, Test L1 Norm: 0.0640, Train Linf Norm: 3.5361, Test Linf Norm: 11.6300\n",
            "Epoch 27: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0276, Test L1 Norm: 0.0479, Train Linf Norm: 3.9430, Test Linf Norm: 8.1678\n",
            "Epoch 28: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0241, Test L1 Norm: 0.0566, Train Linf Norm: 3.0882, Test Linf Norm: 10.1011\n",
            "Epoch 29: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0230, Test L1 Norm: 0.0564, Train Linf Norm: 3.1906, Test Linf Norm: 10.1700\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0235, Test L1 Norm: 0.0439, Train Linf Norm: 3.3317, Test Linf Norm: 7.5458\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0231, Test L1 Norm: 0.0474, Train Linf Norm: 3.3515, Test Linf Norm: 8.4394\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0222, Test L1 Norm: 0.0419, Train Linf Norm: 3.1744, Test Linf Norm: 7.3270\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0204, Test L1 Norm: 0.0417, Train Linf Norm: 2.9584, Test Linf Norm: 7.2544\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0205, Test L1 Norm: 0.0367, Train Linf Norm: 2.9907, Test Linf Norm: 6.2467\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0194, Test L1 Norm: 0.0407, Train Linf Norm: 2.6372, Test Linf Norm: 7.1492\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0198, Test L1 Norm: 0.0396, Train Linf Norm: 2.8386, Test Linf Norm: 6.9559\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0189, Test L1 Norm: 0.0386, Train Linf Norm: 2.7105, Test Linf Norm: 6.7722\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0190, Test L1 Norm: 0.0367, Train Linf Norm: 2.7300, Test Linf Norm: 6.3590\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0189, Test L1 Norm: 0.0369, Train Linf Norm: 2.7445, Test Linf Norm: 6.4151\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0186, Test L1 Norm: 0.0374, Train Linf Norm: 2.6777, Test Linf Norm: 6.5257\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0185, Test L1 Norm: 0.0374, Train Linf Norm: 2.5624, Test Linf Norm: 6.5219\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0184, Test L1 Norm: 0.0372, Train Linf Norm: 2.6344, Test Linf Norm: 6.4840\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0185, Test L1 Norm: 0.0372, Train Linf Norm: 2.6564, Test Linf Norm: 6.4840\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0184, Test L1 Norm: 0.0372, Train Linf Norm: 2.6467, Test Linf Norm: 6.4809\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0184, Test L1 Norm: 0.0373, Train Linf Norm: 2.5510, Test Linf Norm: 6.5051\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0183, Test L1 Norm: 0.0368, Train Linf Norm: 2.5515, Test Linf Norm: 6.4163\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0185, Test L1 Norm: 0.0373, Train Linf Norm: 2.6403, Test Linf Norm: 6.5076\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0181, Test L1 Norm: 0.0366, Train Linf Norm: 2.5464, Test Linf Norm: 6.3947\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0185, Test L1 Norm: 0.0359, Train Linf Norm: 2.6471, Test Linf Norm: 6.2525\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0178, Test L1 Norm: 0.0333, Train Linf Norm: 2.5075, Test Linf Norm: 5.6994\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0177, Test L1 Norm: 0.0341, Train Linf Norm: 2.4768, Test Linf Norm: 5.8890\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0176, Test L1 Norm: 0.0328, Train Linf Norm: 2.4641, Test Linf Norm: 5.6384\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0181, Test L1 Norm: 0.0358, Train Linf Norm: 2.5748, Test Linf Norm: 6.1871\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0170, Test L1 Norm: 0.0313, Train Linf Norm: 2.4002, Test Linf Norm: 5.3129\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0178, Test L1 Norm: 0.0421, Train Linf Norm: 2.5398, Test Linf Norm: 7.3157\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0183, Test L1 Norm: 0.0242, Train Linf Norm: 2.5250, Test Linf Norm: 3.7449\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0171, Test L1 Norm: 0.0267, Train Linf Norm: 2.1840, Test Linf Norm: 4.4413\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0167, Test L1 Norm: 0.0283, Train Linf Norm: 2.2597, Test Linf Norm: 4.7865\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0159, Test L1 Norm: 0.0279, Train Linf Norm: 2.2123, Test Linf Norm: 4.7175\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0160, Test L1 Norm: 0.0241, Train Linf Norm: 2.2472, Test Linf Norm: 3.9509\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0153, Test L1 Norm: 0.0300, Train Linf Norm: 2.0433, Test Linf Norm: 5.1029\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0157, Test L1 Norm: 0.0256, Train Linf Norm: 2.1579, Test Linf Norm: 4.2571\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0156, Test L1 Norm: 0.0284, Train Linf Norm: 2.2123, Test Linf Norm: 4.8366\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0149, Test L1 Norm: 0.0253, Train Linf Norm: 2.0756, Test Linf Norm: 4.2321\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0150, Test L1 Norm: 0.0248, Train Linf Norm: 2.0916, Test Linf Norm: 4.1487\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0147, Test L1 Norm: 0.0255, Train Linf Norm: 2.0297, Test Linf Norm: 4.3072\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0148, Test L1 Norm: 0.0247, Train Linf Norm: 2.0732, Test Linf Norm: 4.1316\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0147, Test L1 Norm: 0.0244, Train Linf Norm: 2.0403, Test Linf Norm: 4.0848\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0147, Test L1 Norm: 0.0247, Train Linf Norm: 1.9267, Test Linf Norm: 4.1341\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0147, Test L1 Norm: 0.0247, Train Linf Norm: 1.9193, Test Linf Norm: 4.1409\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0147, Test L1 Norm: 0.0247, Train Linf Norm: 2.0199, Test Linf Norm: 4.1409\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0147, Test L1 Norm: 0.0246, Train Linf Norm: 2.0472, Test Linf Norm: 4.1124\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0146, Test L1 Norm: 0.0243, Train Linf Norm: 2.0328, Test Linf Norm: 4.0571\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0147, Test L1 Norm: 0.0245, Train Linf Norm: 2.0426, Test Linf Norm: 4.0845\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0145, Test L1 Norm: 0.0252, Train Linf Norm: 2.0039, Test Linf Norm: 4.2257\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0145, Test L1 Norm: 0.0260, Train Linf Norm: 2.0005, Test Linf Norm: 4.3781\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0147, Test L1 Norm: 0.0259, Train Linf Norm: 2.0597, Test Linf Norm: 4.3691\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0144, Test L1 Norm: 0.0240, Train Linf Norm: 2.0054, Test Linf Norm: 3.9816\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0144, Test L1 Norm: 0.0256, Train Linf Norm: 2.0176, Test Linf Norm: 4.3172\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0143, Test L1 Norm: 0.0279, Train Linf Norm: 1.9781, Test Linf Norm: 4.7593\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0146, Test L1 Norm: 0.0220, Train Linf Norm: 2.0510, Test Linf Norm: 3.5957\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0142, Test L1 Norm: 0.0222, Train Linf Norm: 1.9642, Test Linf Norm: 3.6528\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0139, Test L1 Norm: 0.0232, Train Linf Norm: 1.8786, Test Linf Norm: 3.8327\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0146, Test L1 Norm: 0.0203, Train Linf Norm: 1.9963, Test Linf Norm: 3.2855\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0136, Test L1 Norm: 0.0211, Train Linf Norm: 1.7960, Test Linf Norm: 3.4531\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0147, Test L1 Norm: 0.0234, Train Linf Norm: 1.9909, Test Linf Norm: 3.9176\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0135, Test L1 Norm: 0.0211, Train Linf Norm: 1.8505, Test Linf Norm: 3.4517\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0137, Test L1 Norm: 0.0184, Train Linf Norm: 1.9110, Test Linf Norm: 2.9274\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0131, Test L1 Norm: 0.0199, Train Linf Norm: 1.8015, Test Linf Norm: 3.2370\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0131, Test L1 Norm: 0.0161, Train Linf Norm: 1.8104, Test Linf Norm: 2.4048\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0131, Test L1 Norm: 0.0180, Train Linf Norm: 1.8004, Test Linf Norm: 2.8518\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0130, Test L1 Norm: 0.0189, Train Linf Norm: 1.7930, Test Linf Norm: 3.0554\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0129, Test L1 Norm: 0.0189, Train Linf Norm: 1.7843, Test Linf Norm: 3.0378\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0131, Test L1 Norm: 0.0181, Train Linf Norm: 1.8340, Test Linf Norm: 2.8799\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0129, Test L1 Norm: 0.0197, Train Linf Norm: 1.8014, Test Linf Norm: 3.2161\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0128, Test L1 Norm: 0.0183, Train Linf Norm: 1.6619, Test Linf Norm: 2.9310\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0128, Test L1 Norm: 0.0191, Train Linf Norm: 1.7797, Test Linf Norm: 3.0814\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0128, Test L1 Norm: 0.0190, Train Linf Norm: 1.7384, Test Linf Norm: 3.0618\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0128, Test L1 Norm: 0.0190, Train Linf Norm: 1.7847, Test Linf Norm: 3.0618\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0128, Test L1 Norm: 0.0188, Train Linf Norm: 1.7751, Test Linf Norm: 3.0237\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0128, Test L1 Norm: 0.0189, Train Linf Norm: 1.7341, Test Linf Norm: 3.0477\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0128, Test L1 Norm: 0.0188, Train Linf Norm: 1.7396, Test Linf Norm: 3.0358\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0129, Test L1 Norm: 0.0188, Train Linf Norm: 1.7943, Test Linf Norm: 3.0276\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0129, Test L1 Norm: 0.0172, Train Linf Norm: 1.8014, Test Linf Norm: 2.6958\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0128, Test L1 Norm: 0.0182, Train Linf Norm: 1.7588, Test Linf Norm: 2.9221\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0136, Test L1 Norm: 0.0190, Train Linf Norm: 1.9307, Test Linf Norm: 3.0494\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0126, Test L1 Norm: 0.0190, Train Linf Norm: 1.7298, Test Linf Norm: 3.0883\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0128, Test L1 Norm: 0.0169, Train Linf Norm: 1.7798, Test Linf Norm: 2.6571\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0135, Test L1 Norm: 0.0234, Train Linf Norm: 1.9023, Test Linf Norm: 3.8651\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0126, Test L1 Norm: 0.0179, Train Linf Norm: 1.6948, Test Linf Norm: 2.8673\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0130, Test L1 Norm: 0.0159, Train Linf Norm: 1.7967, Test Linf Norm: 2.4644\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0125, Test L1 Norm: 0.0221, Train Linf Norm: 1.7195, Test Linf Norm: 3.6429\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0138, Test L1 Norm: 0.0101, Train Linf Norm: 1.8921, Test Linf Norm: 1.1019\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0118, Test L1 Norm: 0.0175, Train Linf Norm: 1.5487, Test Linf Norm: 2.7892\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0159, Train Linf Norm: 1.7141, Test Linf Norm: 2.4651\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0158, Train Linf Norm: 1.7231, Test Linf Norm: 2.4680\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0166, Train Linf Norm: 1.7340, Test Linf Norm: 2.6280\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:33:28,182]\u001b[0m Trial 12 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0120, Test L1 Norm: 0.0189, Train Linf Norm: 1.5867, Test Linf Norm: 3.0659\n",
            "Epoch 1: Train Loss: 0.1945, Test Loss: 0.0093, Train L1 Norm: 0.6260, Test L1 Norm: 0.2807, Train Linf Norm: 74.8092, Test Linf Norm: 45.7296\n",
            "Epoch 2: Train Loss: 0.0047, Test Loss: 0.0027, Train L1 Norm: 0.1057, Test L1 Norm: 0.2131, Train Linf Norm: 11.8880, Test Linf Norm: 37.6838\n",
            "Epoch 3: Train Loss: 0.0017, Test Loss: 0.0012, Train L1 Norm: 0.1106, Test L1 Norm: 0.1751, Train Linf Norm: 15.5175, Test Linf Norm: 30.8910\n",
            "Epoch 4: Train Loss: 0.0009, Test Loss: 0.0007, Train L1 Norm: 0.0879, Test L1 Norm: 0.1243, Train Linf Norm: 12.0411, Test Linf Norm: 21.1720\n",
            "Epoch 5: Train Loss: 0.0006, Test Loss: 0.0005, Train L1 Norm: 0.0683, Test L1 Norm: 0.1034, Train Linf Norm: 8.8715, Test Linf Norm: 17.2569\n",
            "Epoch 6: Train Loss: 0.0004, Test Loss: 0.0004, Train L1 Norm: 0.0479, Test L1 Norm: 0.0973, Train Linf Norm: 5.3081, Test Linf Norm: 16.0962\n",
            "Epoch 7: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0422, Test L1 Norm: 0.0860, Train Linf Norm: 3.8591, Test Linf Norm: 13.9508\n",
            "Epoch 8: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0404, Test L1 Norm: 0.0795, Train Linf Norm: 4.1455, Test Linf Norm: 12.7434\n",
            "Epoch 9: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0363, Test L1 Norm: 0.0795, Train Linf Norm: 3.4265, Test Linf Norm: 12.9429\n",
            "Epoch 10: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0338, Test L1 Norm: 0.0774, Train Linf Norm: 3.1468, Test Linf Norm: 12.6530\n",
            "Epoch 11: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0316, Test L1 Norm: 0.0739, Train Linf Norm: 2.9382, Test Linf Norm: 12.0942\n",
            "Epoch 12: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0296, Test L1 Norm: 0.0672, Train Linf Norm: 2.6966, Test Linf Norm: 10.8324\n",
            "Epoch 13: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0285, Test L1 Norm: 0.0711, Train Linf Norm: 2.6373, Test Linf Norm: 11.7842\n",
            "Epoch 14: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0285, Test L1 Norm: 0.0640, Train Linf Norm: 2.7622, Test Linf Norm: 10.4419\n",
            "Epoch 15: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0281, Test L1 Norm: 0.0637, Train Linf Norm: 2.7931, Test Linf Norm: 10.4636\n",
            "Epoch 16: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0283, Test L1 Norm: 0.0609, Train Linf Norm: 2.8312, Test Linf Norm: 9.9913\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0280, Test L1 Norm: 0.0576, Train Linf Norm: 2.9606, Test Linf Norm: 9.3770\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0264, Test L1 Norm: 0.0588, Train Linf Norm: 2.7596, Test Linf Norm: 9.7144\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0254, Test L1 Norm: 0.0584, Train Linf Norm: 2.6003, Test Linf Norm: 9.7228\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0265, Test L1 Norm: 0.0562, Train Linf Norm: 2.9314, Test Linf Norm: 9.3445\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0279, Test L1 Norm: 0.0559, Train Linf Norm: 3.3099, Test Linf Norm: 9.3533\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0275, Test L1 Norm: 0.0562, Train Linf Norm: 3.2549, Test Linf Norm: 9.5319\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0285, Test L1 Norm: 0.0570, Train Linf Norm: 3.5414, Test Linf Norm: 9.7458\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0281, Test L1 Norm: 0.0565, Train Linf Norm: 3.5619, Test Linf Norm: 9.7086\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0279, Test L1 Norm: 0.0548, Train Linf Norm: 3.5286, Test Linf Norm: 9.4121\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0287, Test L1 Norm: 0.0552, Train Linf Norm: 3.6959, Test Linf Norm: 9.5516\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0276, Test L1 Norm: 0.0549, Train Linf Norm: 3.1951, Test Linf Norm: 9.5123\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0276, Test L1 Norm: 0.0579, Train Linf Norm: 3.5960, Test Linf Norm: 10.1387\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0270, Test L1 Norm: 0.0573, Train Linf Norm: 3.4899, Test Linf Norm: 10.0516\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0268, Test L1 Norm: 0.0540, Train Linf Norm: 3.4846, Test Linf Norm: 9.4254\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0260, Test L1 Norm: 0.0531, Train Linf Norm: 3.3542, Test Linf Norm: 9.2690\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0257, Test L1 Norm: 0.0510, Train Linf Norm: 3.3217, Test Linf Norm: 8.8593\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0250, Test L1 Norm: 0.0512, Train Linf Norm: 3.1644, Test Linf Norm: 8.9286\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0235, Test L1 Norm: 0.0498, Train Linf Norm: 2.9101, Test Linf Norm: 8.6661\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0231, Test L1 Norm: 0.0503, Train Linf Norm: 2.8736, Test Linf Norm: 8.7989\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0237, Test L1 Norm: 0.0491, Train Linf Norm: 3.0390, Test Linf Norm: 8.5705\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0227, Test L1 Norm: 0.0466, Train Linf Norm: 2.8382, Test Linf Norm: 8.0663\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0221, Test L1 Norm: 0.0467, Train Linf Norm: 2.6855, Test Linf Norm: 8.1119\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0215, Test L1 Norm: 0.0467, Train Linf Norm: 2.4059, Test Linf Norm: 8.1430\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0217, Test L1 Norm: 0.0456, Train Linf Norm: 2.7093, Test Linf Norm: 7.9111\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0204, Test L1 Norm: 0.0445, Train Linf Norm: 2.2994, Test Linf Norm: 7.7167\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0206, Test L1 Norm: 0.0438, Train Linf Norm: 2.4692, Test Linf Norm: 7.5854\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0195, Test L1 Norm: 0.0439, Train Linf Norm: 2.2954, Test Linf Norm: 7.6329\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0193, Test L1 Norm: 0.0424, Train Linf Norm: 2.2845, Test Linf Norm: 7.3426\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0195, Test L1 Norm: 0.0408, Train Linf Norm: 2.3429, Test Linf Norm: 7.0116\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0188, Test L1 Norm: 0.0407, Train Linf Norm: 2.2566, Test Linf Norm: 7.0191\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0191, Test L1 Norm: 0.0404, Train Linf Norm: 2.2996, Test Linf Norm: 6.9785\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0183, Test L1 Norm: 0.0419, Train Linf Norm: 2.1256, Test Linf Norm: 7.3065\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0179, Test L1 Norm: 0.0423, Train Linf Norm: 2.1038, Test Linf Norm: 7.3969\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0183, Test L1 Norm: 0.0400, Train Linf Norm: 2.2022, Test Linf Norm: 6.9253\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0174, Test L1 Norm: 0.0394, Train Linf Norm: 2.0117, Test Linf Norm: 6.8282\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0170, Test L1 Norm: 0.0397, Train Linf Norm: 1.9239, Test Linf Norm: 6.8937\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0166, Test L1 Norm: 0.0383, Train Linf Norm: 1.8678, Test Linf Norm: 6.6251\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0167, Test L1 Norm: 0.0381, Train Linf Norm: 1.9433, Test Linf Norm: 6.5868\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0163, Test L1 Norm: 0.0384, Train Linf Norm: 1.8329, Test Linf Norm: 6.6531\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0156, Test L1 Norm: 0.0393, Train Linf Norm: 1.6905, Test Linf Norm: 6.8484\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0157, Test L1 Norm: 0.0366, Train Linf Norm: 1.5938, Test Linf Norm: 6.2963\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0147, Test L1 Norm: 0.0370, Train Linf Norm: 1.5856, Test Linf Norm: 6.4047\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0149, Test L1 Norm: 0.0369, Train Linf Norm: 1.5749, Test Linf Norm: 6.3850\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0148, Test L1 Norm: 0.0353, Train Linf Norm: 1.6141, Test Linf Norm: 6.0656\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0147, Test L1 Norm: 0.0371, Train Linf Norm: 1.5400, Test Linf Norm: 6.4415\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0146, Test L1 Norm: 0.0346, Train Linf Norm: 1.5838, Test Linf Norm: 5.9375\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0138, Test L1 Norm: 0.0368, Train Linf Norm: 1.4150, Test Linf Norm: 6.4059\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0139, Test L1 Norm: 0.0335, Train Linf Norm: 1.4305, Test Linf Norm: 5.7229\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0143, Test L1 Norm: 0.0355, Train Linf Norm: 1.5309, Test Linf Norm: 6.1292\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0139, Test L1 Norm: 0.0340, Train Linf Norm: 1.4677, Test Linf Norm: 5.8462\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0138, Test L1 Norm: 0.0337, Train Linf Norm: 1.4718, Test Linf Norm: 5.8013\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0141, Test L1 Norm: 0.0332, Train Linf Norm: 1.5238, Test Linf Norm: 5.7011\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0135, Test L1 Norm: 0.0330, Train Linf Norm: 1.4326, Test Linf Norm: 5.6518\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0133, Test L1 Norm: 0.0331, Train Linf Norm: 1.3747, Test Linf Norm: 5.7016\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0130, Test L1 Norm: 0.0328, Train Linf Norm: 1.2641, Test Linf Norm: 5.6426\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0130, Test L1 Norm: 0.0327, Train Linf Norm: 1.3035, Test Linf Norm: 5.6170\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0130, Test L1 Norm: 0.0327, Train Linf Norm: 1.3250, Test Linf Norm: 5.6342\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0129, Test L1 Norm: 0.0317, Train Linf Norm: 1.3325, Test Linf Norm: 5.4277\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0312, Train Linf Norm: 1.1802, Test Linf Norm: 5.3376\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0129, Test L1 Norm: 0.0327, Train Linf Norm: 1.3516, Test Linf Norm: 5.6448\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0319, Train Linf Norm: 1.2017, Test Linf Norm: 5.4992\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0125, Test L1 Norm: 0.0312, Train Linf Norm: 1.2492, Test Linf Norm: 5.3628\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0316, Train Linf Norm: 1.2667, Test Linf Norm: 5.4336\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0310, Train Linf Norm: 1.2413, Test Linf Norm: 5.3199\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0305, Train Linf Norm: 1.2248, Test Linf Norm: 5.2297\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0120, Test L1 Norm: 0.0298, Train Linf Norm: 1.2134, Test Linf Norm: 5.0880\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0119, Test L1 Norm: 0.0299, Train Linf Norm: 1.1812, Test Linf Norm: 5.1170\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0117, Test L1 Norm: 0.0298, Train Linf Norm: 1.1532, Test Linf Norm: 5.0950\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0118, Test L1 Norm: 0.0309, Train Linf Norm: 1.1820, Test Linf Norm: 5.3111\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0116, Test L1 Norm: 0.0291, Train Linf Norm: 1.1319, Test Linf Norm: 4.9670\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0113, Test L1 Norm: 0.0288, Train Linf Norm: 1.1191, Test Linf Norm: 4.8862\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0112, Test L1 Norm: 0.0300, Train Linf Norm: 1.0860, Test Linf Norm: 5.1520\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0115, Test L1 Norm: 0.0285, Train Linf Norm: 1.1571, Test Linf Norm: 4.8384\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0111, Test L1 Norm: 0.0290, Train Linf Norm: 1.0921, Test Linf Norm: 4.9567\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0113, Test L1 Norm: 0.0276, Train Linf Norm: 1.1096, Test Linf Norm: 4.6751\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0112, Test L1 Norm: 0.0282, Train Linf Norm: 1.0951, Test Linf Norm: 4.7854\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0109, Test L1 Norm: 0.0289, Train Linf Norm: 1.0651, Test Linf Norm: 4.9436\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0110, Test L1 Norm: 0.0280, Train Linf Norm: 1.0957, Test Linf Norm: 4.7659\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0111, Test L1 Norm: 0.0276, Train Linf Norm: 1.0955, Test Linf Norm: 4.6819\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0107, Test L1 Norm: 0.0274, Train Linf Norm: 1.0110, Test Linf Norm: 4.6513\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0106, Test L1 Norm: 0.0276, Train Linf Norm: 1.0105, Test Linf Norm: 4.6849\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0106, Test L1 Norm: 0.0272, Train Linf Norm: 1.0076, Test Linf Norm: 4.6196\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0105, Test L1 Norm: 0.0268, Train Linf Norm: 0.9899, Test Linf Norm: 4.5366\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0103, Test L1 Norm: 0.0269, Train Linf Norm: 0.9592, Test Linf Norm: 4.5656\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0275, Train Linf Norm: 0.9929, Test Linf Norm: 4.6874\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0102, Test L1 Norm: 0.0253, Train Linf Norm: 0.9489, Test Linf Norm: 4.2220\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0102, Test L1 Norm: 0.0263, Train Linf Norm: 0.9708, Test Linf Norm: 4.4392\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0103, Test L1 Norm: 0.0258, Train Linf Norm: 0.9611, Test Linf Norm: 4.3471\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0102, Test L1 Norm: 0.0256, Train Linf Norm: 0.9786, Test Linf Norm: 4.3155\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0101, Test L1 Norm: 0.0257, Train Linf Norm: 0.9322, Test Linf Norm: 4.3271\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0254, Train Linf Norm: 1.0358, Test Linf Norm: 4.2672\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0253, Train Linf Norm: 0.9220, Test Linf Norm: 4.2572\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0101, Test L1 Norm: 0.0255, Train Linf Norm: 0.9749, Test Linf Norm: 4.3077\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0105, Test L1 Norm: 0.0254, Train Linf Norm: 0.9751, Test Linf Norm: 4.2813\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0098, Test L1 Norm: 0.0252, Train Linf Norm: 0.8937, Test Linf Norm: 4.2330\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0099, Test L1 Norm: 0.0248, Train Linf Norm: 0.9327, Test Linf Norm: 4.1764\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0098, Test L1 Norm: 0.0250, Train Linf Norm: 0.9237, Test Linf Norm: 4.2106\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0099, Test L1 Norm: 0.0254, Train Linf Norm: 0.8845, Test Linf Norm: 4.2824\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0098, Test L1 Norm: 0.0251, Train Linf Norm: 0.8973, Test Linf Norm: 4.2316\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0097, Test L1 Norm: 0.0246, Train Linf Norm: 0.8516, Test Linf Norm: 4.1454\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0096, Test L1 Norm: 0.0244, Train Linf Norm: 0.8905, Test Linf Norm: 4.1019\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:36:44,342]\u001b[0m Trial 13 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0096, Test L1 Norm: 0.0251, Train Linf Norm: 0.8767, Test Linf Norm: 4.2424\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:36:45,614]\u001b[0m Trial 14 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 5.6853, Test Loss: 3.8828, Train L1 Norm: 3.5607, Test L1 Norm: 7.2997, Train Linf Norm: 419.5216, Test Linf Norm: 1228.9056\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:36:47,507]\u001b[0m Trial 15 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:36:48,986]\u001b[0m Trial 16 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.2242, Test Loss: 0.0018, Train L1 Norm: 0.5267, Test L1 Norm: 0.0603, Train Linf Norm: 46.4385, Test Linf Norm: 1.8655\n",
            "Epoch 2: Train Loss: 0.0026, Test Loss: 0.0005, Train L1 Norm: 0.0775, Test L1 Norm: 0.0416, Train Linf Norm: 5.4818, Test Linf Norm: 2.0844\n",
            "Epoch 3: Train Loss: 0.0016, Test Loss: 0.0004, Train L1 Norm: 0.0506, Test L1 Norm: 0.0303, Train Linf Norm: 2.9374, Test Linf Norm: 1.2580\n",
            "Epoch 4: Train Loss: 0.0017, Test Loss: 0.0004, Train L1 Norm: 0.0553, Test L1 Norm: 0.0298, Train Linf Norm: 4.7547, Test Linf Norm: 1.0642\n",
            "Epoch 5: Train Loss: 0.0021, Test Loss: 0.0001, Train L1 Norm: 0.0376, Test L1 Norm: 0.0233, Train Linf Norm: 1.7267, Test Linf Norm: 1.2050\n",
            "Epoch 6: Train Loss: 0.0015, Test Loss: 0.0002, Train L1 Norm: 0.0382, Test L1 Norm: 0.0254, Train Linf Norm: 2.0260, Test Linf Norm: 1.2028\n",
            "Epoch 7: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0284, Test L1 Norm: 0.0231, Train Linf Norm: 1.8730, Test Linf Norm: 1.2458\n",
            "Epoch 8: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0289, Test L1 Norm: 0.0231, Train Linf Norm: 1.9948, Test Linf Norm: 1.2303\n",
            "Epoch 9: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0284, Test L1 Norm: 0.0229, Train Linf Norm: 1.9153, Test Linf Norm: 1.2362\n",
            "Epoch 10: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0287, Test L1 Norm: 0.0231, Train Linf Norm: 1.9900, Test Linf Norm: 1.2451\n",
            "Epoch 11: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0306, Test L1 Norm: 0.0252, Train Linf Norm: 2.2657, Test Linf Norm: 1.2207\n",
            "Epoch 12: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0285, Test L1 Norm: 0.0225, Train Linf Norm: 1.9239, Test Linf Norm: 1.2725\n",
            "Epoch 13: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0274, Test L1 Norm: 0.0225, Train Linf Norm: 1.7959, Test Linf Norm: 1.2548\n",
            "Epoch 14: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0274, Test L1 Norm: 0.0224, Train Linf Norm: 1.8317, Test Linf Norm: 1.2600\n",
            "Epoch 15: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0274, Test L1 Norm: 0.0226, Train Linf Norm: 1.8360, Test Linf Norm: 1.2519\n",
            "Epoch 16: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0274, Test L1 Norm: 0.0223, Train Linf Norm: 1.8377, Test Linf Norm: 1.2596\n",
            "Epoch 17: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0273, Test L1 Norm: 0.0225, Train Linf Norm: 1.8005, Test Linf Norm: 1.2522\n",
            "Epoch 18: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0274, Test L1 Norm: 0.0222, Train Linf Norm: 1.8390, Test Linf Norm: 1.2736\n",
            "Epoch 19: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0272, Test L1 Norm: 0.0222, Train Linf Norm: 1.8262, Test Linf Norm: 1.2652\n",
            "Epoch 20: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7865, Test Linf Norm: 1.2621\n",
            "Epoch 21: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0272, Test L1 Norm: 0.0222, Train Linf Norm: 1.8363, Test Linf Norm: 1.2577\n",
            "Epoch 22: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0272, Test L1 Norm: 0.0222, Train Linf Norm: 1.8295, Test Linf Norm: 1.2575\n",
            "Epoch 23: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7974, Test Linf Norm: 1.2584\n",
            "Epoch 24: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8158, Test Linf Norm: 1.2591\n",
            "Epoch 25: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7934, Test Linf Norm: 1.2577\n",
            "Epoch 26: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7943, Test Linf Norm: 1.2579\n",
            "Epoch 27: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8182, Test Linf Norm: 1.2580\n",
            "Epoch 28: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7908, Test Linf Norm: 1.2583\n",
            "Epoch 29: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7360, Test Linf Norm: 1.2576\n",
            "Epoch 30: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7989, Test Linf Norm: 1.2581\n",
            "Epoch 31: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8114, Test Linf Norm: 1.2580\n",
            "Epoch 32: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8180, Test Linf Norm: 1.2580\n",
            "Epoch 33: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8255, Test Linf Norm: 1.2580\n",
            "Epoch 34: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7486, Test Linf Norm: 1.2579\n",
            "Epoch 35: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8032, Test Linf Norm: 1.2579\n",
            "Epoch 36: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7948, Test Linf Norm: 1.2579\n",
            "Epoch 37: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7818, Test Linf Norm: 1.2579\n",
            "Epoch 38: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8156, Test Linf Norm: 1.2579\n",
            "Epoch 39: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7941, Test Linf Norm: 1.2579\n",
            "Epoch 40: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8278, Test Linf Norm: 1.2579\n",
            "Epoch 41: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8221, Test Linf Norm: 1.2579\n",
            "Epoch 42: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8011, Test Linf Norm: 1.2579\n",
            "Epoch 43: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7926, Test Linf Norm: 1.2579\n",
            "Epoch 44: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8161, Test Linf Norm: 1.2579\n",
            "Epoch 45: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8127, Test Linf Norm: 1.2579\n",
            "Epoch 46: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8196, Test Linf Norm: 1.2579\n",
            "Epoch 47: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8098, Test Linf Norm: 1.2579\n",
            "Epoch 48: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7923, Test Linf Norm: 1.2579\n",
            "Epoch 49: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7952, Test Linf Norm: 1.2579\n",
            "Epoch 50: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8079, Test Linf Norm: 1.2579\n",
            "Epoch 51: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7948, Test Linf Norm: 1.2579\n",
            "Epoch 52: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7734, Test Linf Norm: 1.2579\n",
            "Epoch 53: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8212, Test Linf Norm: 1.2579\n",
            "Epoch 54: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8118, Test Linf Norm: 1.2579\n",
            "Epoch 55: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8011, Test Linf Norm: 1.2579\n",
            "Epoch 56: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8112, Test Linf Norm: 1.2579\n",
            "Epoch 57: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8196, Test Linf Norm: 1.2579\n",
            "Epoch 58: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7874, Test Linf Norm: 1.2579\n",
            "Epoch 59: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8110, Test Linf Norm: 1.2579\n",
            "Epoch 60: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8239, Test Linf Norm: 1.2579\n",
            "Epoch 61: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7888, Test Linf Norm: 1.2579\n",
            "Epoch 62: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8258, Test Linf Norm: 1.2579\n",
            "Epoch 63: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8077, Test Linf Norm: 1.2579\n",
            "Epoch 64: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7983, Test Linf Norm: 1.2579\n",
            "Epoch 65: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8130, Test Linf Norm: 1.2579\n",
            "Epoch 66: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8300, Test Linf Norm: 1.2579\n",
            "Epoch 67: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7911, Test Linf Norm: 1.2579\n",
            "Epoch 68: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8197, Test Linf Norm: 1.2579\n",
            "Epoch 69: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7625, Test Linf Norm: 1.2579\n",
            "Epoch 70: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8161, Test Linf Norm: 1.2579\n",
            "Epoch 71: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8052, Test Linf Norm: 1.2579\n",
            "Epoch 72: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7876, Test Linf Norm: 1.2579\n",
            "Epoch 73: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7813, Test Linf Norm: 1.2579\n",
            "Epoch 74: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8173, Test Linf Norm: 1.2579\n",
            "Epoch 75: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7994, Test Linf Norm: 1.2579\n",
            "Epoch 76: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7860, Test Linf Norm: 1.2579\n",
            "Epoch 77: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7978, Test Linf Norm: 1.2579\n",
            "Epoch 78: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7672, Test Linf Norm: 1.2579\n",
            "Epoch 79: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7957, Test Linf Norm: 1.2579\n",
            "Epoch 80: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8315, Test Linf Norm: 1.2579\n",
            "Epoch 81: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8148, Test Linf Norm: 1.2579\n",
            "Epoch 82: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7850, Test Linf Norm: 1.2579\n",
            "Epoch 83: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8349, Test Linf Norm: 1.2579\n",
            "Epoch 84: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8095, Test Linf Norm: 1.2579\n",
            "Epoch 85: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7983, Test Linf Norm: 1.2579\n",
            "Epoch 86: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7886, Test Linf Norm: 1.2579\n",
            "Epoch 87: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8212, Test Linf Norm: 1.2579\n",
            "Epoch 88: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8099, Test Linf Norm: 1.2579\n",
            "Epoch 89: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8199, Test Linf Norm: 1.2579\n",
            "Epoch 90: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8157, Test Linf Norm: 1.2579\n",
            "Epoch 91: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7819, Test Linf Norm: 1.2579\n",
            "Epoch 92: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7972, Test Linf Norm: 1.2579\n",
            "Epoch 93: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8081, Test Linf Norm: 1.2579\n",
            "Epoch 94: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7474, Test Linf Norm: 1.2579\n",
            "Epoch 95: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7713, Test Linf Norm: 1.2579\n",
            "Epoch 96: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8043, Test Linf Norm: 1.2579\n",
            "Epoch 97: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8245, Test Linf Norm: 1.2579\n",
            "Epoch 98: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8038, Test Linf Norm: 1.2579\n",
            "Epoch 99: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8090, Test Linf Norm: 1.2579\n",
            "Epoch 100: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7885, Test Linf Norm: 1.2579\n",
            "Epoch 101: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8108, Test Linf Norm: 1.2579\n",
            "Epoch 102: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7521, Test Linf Norm: 1.2579\n",
            "Epoch 103: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8032, Test Linf Norm: 1.2579\n",
            "Epoch 104: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8247, Test Linf Norm: 1.2579\n",
            "Epoch 105: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8000, Test Linf Norm: 1.2579\n",
            "Epoch 106: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8000, Test Linf Norm: 1.2579\n",
            "Epoch 107: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8306, Test Linf Norm: 1.2579\n",
            "Epoch 108: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7753, Test Linf Norm: 1.2579\n",
            "Epoch 109: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8122, Test Linf Norm: 1.2579\n",
            "Epoch 110: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7972, Test Linf Norm: 1.2579\n",
            "Epoch 111: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7803, Test Linf Norm: 1.2579\n",
            "Epoch 112: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8069, Test Linf Norm: 1.2579\n",
            "Epoch 113: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8256, Test Linf Norm: 1.2579\n",
            "Epoch 114: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7947, Test Linf Norm: 1.2579\n",
            "Epoch 115: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8089, Test Linf Norm: 1.2579\n",
            "Epoch 116: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.7792, Test Linf Norm: 1.2579\n",
            "Epoch 117: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8086, Test Linf Norm: 1.2579\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:39:44,639]\u001b[0m Trial 17 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 118: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0222, Train Linf Norm: 1.8034, Test Linf Norm: 1.2579\n",
            "Epoch 1: Train Loss: 0.3514, Test Loss: 0.1428, Train L1 Norm: 0.2590, Test L1 Norm: 0.2946, Train Linf Norm: 10.4122, Test Linf Norm: 25.6568\n",
            "Epoch 2: Train Loss: 0.1629, Test Loss: 0.2334, Train L1 Norm: 0.1413, Test L1 Norm: 0.0967, Train Linf Norm: 7.2821, Test Linf Norm: 0.8247\n",
            "Epoch 3: Train Loss: 0.1180, Test Loss: 0.1264, Train L1 Norm: 0.1122, Test L1 Norm: 0.2884, Train Linf Norm: 6.0359, Test Linf Norm: 25.9087\n",
            "Epoch 4: Train Loss: 0.0883, Test Loss: 0.1244, Train L1 Norm: 0.0699, Test L1 Norm: 0.0513, Train Linf Norm: 3.0044, Test Linf Norm: 0.6167\n",
            "Epoch 5: Train Loss: 0.0654, Test Loss: 0.0511, Train L1 Norm: 0.0566, Test L1 Norm: 0.1194, Train Linf Norm: 2.7554, Test Linf Norm: 11.2112\n",
            "Epoch 6: Train Loss: 0.0446, Test Loss: 0.0547, Train L1 Norm: 0.0381, Test L1 Norm: 0.1498, Train Linf Norm: 1.7921, Test Linf Norm: 13.9224\n",
            "Epoch 7: Train Loss: 0.0274, Test Loss: 0.0422, Train L1 Norm: 0.0208, Test L1 Norm: 0.1062, Train Linf Norm: 0.7360, Test Linf Norm: 9.7254\n",
            "Epoch 8: Train Loss: 0.0138, Test Loss: 0.0155, Train L1 Norm: 0.0145, Test L1 Norm: 0.0166, Train Linf Norm: 0.7364, Test Linf Norm: 0.9349\n",
            "Epoch 9: Train Loss: 0.0073, Test Loss: 0.0065, Train L1 Norm: 0.0102, Test L1 Norm: 0.0307, Train Linf Norm: 0.5431, Test Linf Norm: 2.9664\n",
            "Epoch 10: Train Loss: 0.0065, Test Loss: 0.0065, Train L1 Norm: 0.0095, Test L1 Norm: 0.0307, Train Linf Norm: 0.4853, Test Linf Norm: 2.9664\n",
            "Epoch 11: Train Loss: 0.0069, Test Loss: 0.0058, Train L1 Norm: 0.0091, Test L1 Norm: 0.0238, Train Linf Norm: 0.4247, Test Linf Norm: 2.2073\n",
            "Epoch 12: Train Loss: 0.0141, Test Loss: 0.0143, Train L1 Norm: 0.0124, Test L1 Norm: 0.0543, Train Linf Norm: 0.5005, Test Linf Norm: 5.3223\n",
            "Epoch 13: Train Loss: 0.0271, Test Loss: 0.0310, Train L1 Norm: 0.0210, Test L1 Norm: 0.0909, Train Linf Norm: 0.8026, Test Linf Norm: 8.7458\n",
            "Epoch 14: Train Loss: 0.0432, Test Loss: 0.0237, Train L1 Norm: 0.0345, Test L1 Norm: 0.0233, Train Linf Norm: 1.4576, Test Linf Norm: 1.0403\n",
            "Epoch 15: Train Loss: 0.0592, Test Loss: 0.0657, Train L1 Norm: 0.0473, Test L1 Norm: 0.0695, Train Linf Norm: 2.0565, Test Linf Norm: 4.9828\n",
            "Epoch 16: Train Loss: 0.0712, Test Loss: 0.0460, Train L1 Norm: 0.0651, Test L1 Norm: 0.0722, Train Linf Norm: 3.4802, Test Linf Norm: 5.9019\n",
            "Epoch 17: Train Loss: 0.0781, Test Loss: 0.0532, Train L1 Norm: 0.0611, Test L1 Norm: 0.0360, Train Linf Norm: 2.6277, Test Linf Norm: 1.1569\n",
            "Epoch 18: Train Loss: 0.0793, Test Loss: 0.0239, Train L1 Norm: 0.0716, Test L1 Norm: 0.1441, Train Linf Norm: 3.8247, Test Linf Norm: 14.2886\n",
            "Epoch 19: Train Loss: 0.0766, Test Loss: 0.0673, Train L1 Norm: 0.0700, Test L1 Norm: 0.0434, Train Linf Norm: 3.8365, Test Linf Norm: 0.7556\n",
            "Epoch 20: Train Loss: 0.0704, Test Loss: 0.0365, Train L1 Norm: 0.0599, Test L1 Norm: 0.1297, Train Linf Norm: 2.9701, Test Linf Norm: 12.8856\n",
            "Epoch 21: Train Loss: 0.0614, Test Loss: 0.0600, Train L1 Norm: 0.0599, Test L1 Norm: 0.1324, Train Linf Norm: 3.4109, Test Linf Norm: 12.3882\n",
            "Epoch 22: Train Loss: 0.0506, Test Loss: 0.0318, Train L1 Norm: 0.0434, Test L1 Norm: 0.1420, Train Linf Norm: 2.1631, Test Linf Norm: 14.1842\n",
            "Epoch 23: Train Loss: 0.0393, Test Loss: 0.0291, Train L1 Norm: 0.0353, Test L1 Norm: 0.1097, Train Linf Norm: 1.8168, Test Linf Norm: 10.7765\n",
            "Epoch 24: Train Loss: 0.0275, Test Loss: 0.0337, Train L1 Norm: 0.0263, Test L1 Norm: 0.0522, Train Linf Norm: 1.4379, Test Linf Norm: 4.4747\n",
            "Epoch 25: Train Loss: 0.0170, Test Loss: 0.0143, Train L1 Norm: 0.0155, Test L1 Norm: 0.0466, Train Linf Norm: 0.7424, Test Linf Norm: 4.5307\n",
            "Epoch 26: Train Loss: 0.0095, Test Loss: 0.0107, Train L1 Norm: 0.0098, Test L1 Norm: 0.0337, Train Linf Norm: 0.4755, Test Linf Norm: 3.2699\n",
            "Epoch 27: Train Loss: 0.0063, Test Loss: 0.0069, Train L1 Norm: 0.0081, Test L1 Norm: 0.0273, Train Linf Norm: 0.4472, Test Linf Norm: 2.6682\n",
            "Epoch 28: Train Loss: 0.0068, Test Loss: 0.0069, Train L1 Norm: 0.0085, Test L1 Norm: 0.0273, Train Linf Norm: 0.4888, Test Linf Norm: 2.6682\n",
            "Epoch 29: Train Loss: 0.0059, Test Loss: 0.0057, Train L1 Norm: 0.0078, Test L1 Norm: 0.0170, Train Linf Norm: 0.4288, Test Linf Norm: 1.5059\n",
            "Epoch 30: Train Loss: 0.0093, Test Loss: 0.0085, Train L1 Norm: 0.0106, Test L1 Norm: 0.0095, Train Linf Norm: 0.5733, Test Linf Norm: 0.4823\n",
            "Epoch 31: Train Loss: 0.0173, Test Loss: 0.0295, Train L1 Norm: 0.0169, Test L1 Norm: 0.0144, Train Linf Norm: 0.9383, Test Linf Norm: 0.3767\n",
            "Epoch 32: Train Loss: 0.0273, Test Loss: 0.0244, Train L1 Norm: 0.0224, Test L1 Norm: 0.0167, Train Linf Norm: 1.0046, Test Linf Norm: 0.4716\n",
            "Epoch 33: Train Loss: 0.0379, Test Loss: 0.0156, Train L1 Norm: 0.0324, Test L1 Norm: 0.0440, Train Linf Norm: 1.5979, Test Linf Norm: 4.0084\n",
            "Epoch 34: Train Loss: 0.0471, Test Loss: 0.0556, Train L1 Norm: 0.0449, Test L1 Norm: 0.0367, Train Linf Norm: 2.5312, Test Linf Norm: 0.7373\n",
            "Epoch 35: Train Loss: 0.0544, Test Loss: 0.0469, Train L1 Norm: 0.0455, Test L1 Norm: 0.0910, Train Linf Norm: 2.1910, Test Linf Norm: 7.8724\n",
            "Epoch 36: Train Loss: 0.0574, Test Loss: 0.0427, Train L1 Norm: 0.0475, Test L1 Norm: 0.1669, Train Linf Norm: 2.2695, Test Linf Norm: 15.9638\n",
            "Epoch 37: Train Loss: 0.0577, Test Loss: 0.0338, Train L1 Norm: 0.0460, Test L1 Norm: 0.1350, Train Linf Norm: 2.1566, Test Linf Norm: 13.2979\n",
            "Epoch 38: Train Loss: 0.0547, Test Loss: 0.0408, Train L1 Norm: 0.0520, Test L1 Norm: 0.1085, Train Linf Norm: 2.9741, Test Linf Norm: 10.4488\n",
            "Epoch 39: Train Loss: 0.0485, Test Loss: 0.0477, Train L1 Norm: 0.0408, Test L1 Norm: 0.0330, Train Linf Norm: 2.0100, Test Linf Norm: 0.6383\n",
            "Epoch 40: Train Loss: 0.0408, Test Loss: 0.0264, Train L1 Norm: 0.0336, Test L1 Norm: 0.0300, Train Linf Norm: 1.6159, Test Linf Norm: 1.2685\n",
            "Epoch 41: Train Loss: 0.0316, Test Loss: 0.0382, Train L1 Norm: 0.0294, Test L1 Norm: 0.0189, Train Linf Norm: 1.6094, Test Linf Norm: 0.5282\n",
            "Epoch 42: Train Loss: 0.0222, Test Loss: 0.0317, Train L1 Norm: 0.0216, Test L1 Norm: 0.0307, Train Linf Norm: 1.2419, Test Linf Norm: 2.0295\n",
            "Epoch 43: Train Loss: 0.0137, Test Loss: 0.0190, Train L1 Norm: 0.0145, Test L1 Norm: 0.0227, Train Linf Norm: 0.8621, Test Linf Norm: 1.6648\n",
            "Epoch 44: Train Loss: 0.0077, Test Loss: 0.0072, Train L1 Norm: 0.0094, Test L1 Norm: 0.0084, Train Linf Norm: 0.5700, Test Linf Norm: 0.4362\n",
            "Epoch 45: Train Loss: 0.0051, Test Loss: 0.0061, Train L1 Norm: 0.0078, Test L1 Norm: 0.0223, Train Linf Norm: 0.4920, Test Linf Norm: 2.1499\n",
            "Epoch 46: Train Loss: 0.0061, Test Loss: 0.0061, Train L1 Norm: 0.0084, Test L1 Norm: 0.0223, Train Linf Norm: 0.5249, Test Linf Norm: 2.1499\n",
            "Epoch 47: Train Loss: 0.0048, Test Loss: 0.0046, Train L1 Norm: 0.0077, Test L1 Norm: 0.0205, Train Linf Norm: 0.4901, Test Linf Norm: 1.9959\n",
            "Epoch 48: Train Loss: 0.0077, Test Loss: 0.0070, Train L1 Norm: 0.0088, Test L1 Norm: 0.0163, Train Linf Norm: 0.5073, Test Linf Norm: 1.4205\n",
            "Epoch 49: Train Loss: 0.0141, Test Loss: 0.0167, Train L1 Norm: 0.0138, Test L1 Norm: 0.0192, Train Linf Norm: 0.7733, Test Linf Norm: 1.4284\n",
            "Epoch 50: Train Loss: 0.0222, Test Loss: 0.0238, Train L1 Norm: 0.0190, Test L1 Norm: 0.0122, Train Linf Norm: 0.9314, Test Linf Norm: 0.2736\n",
            "Epoch 51: Train Loss: 0.0312, Test Loss: 0.0429, Train L1 Norm: 0.0279, Test L1 Norm: 0.0461, Train Linf Norm: 1.4969, Test Linf Norm: 3.5332\n",
            "Epoch 52: Train Loss: 0.0389, Test Loss: 0.0293, Train L1 Norm: 0.0325, Test L1 Norm: 0.1282, Train Linf Norm: 1.6118, Test Linf Norm: 12.7164\n",
            "Epoch 53: Train Loss: 0.0456, Test Loss: 0.0371, Train L1 Norm: 0.0384, Test L1 Norm: 0.0265, Train Linf Norm: 1.9230, Test Linf Norm: 0.5667\n",
            "Epoch 54: Train Loss: 0.0487, Test Loss: 0.0512, Train L1 Norm: 0.0385, Test L1 Norm: 0.1404, Train Linf Norm: 1.7972, Test Linf Norm: 12.9812\n",
            "Epoch 55: Train Loss: 0.0490, Test Loss: 0.0601, Train L1 Norm: 0.0372, Test L1 Norm: 0.1080, Train Linf Norm: 1.5704, Test Linf Norm: 9.4696\n",
            "Epoch 56: Train Loss: 0.0470, Test Loss: 0.0561, Train L1 Norm: 0.0443, Test L1 Norm: 0.0267, Train Linf Norm: 2.5018, Test Linf Norm: 0.5224\n",
            "Epoch 57: Train Loss: 0.0423, Test Loss: 0.0417, Train L1 Norm: 0.0343, Test L1 Norm: 0.0900, Train Linf Norm: 1.6464, Test Linf Norm: 8.3878\n",
            "Epoch 58: Train Loss: 0.0353, Test Loss: 0.0298, Train L1 Norm: 0.0344, Test L1 Norm: 0.0319, Train Linf Norm: 2.0366, Test Linf Norm: 1.9802\n",
            "Epoch 59: Train Loss: 0.0277, Test Loss: 0.0163, Train L1 Norm: 0.0268, Test L1 Norm: 0.0329, Train Linf Norm: 1.5303, Test Linf Norm: 2.7754\n",
            "Epoch 60: Train Loss: 0.0194, Test Loss: 0.0271, Train L1 Norm: 0.0179, Test L1 Norm: 0.0207, Train Linf Norm: 0.9323, Test Linf Norm: 1.1142\n",
            "Epoch 61: Train Loss: 0.0120, Test Loss: 0.0134, Train L1 Norm: 0.0124, Test L1 Norm: 0.0296, Train Linf Norm: 0.7169, Test Linf Norm: 2.7788\n",
            "Epoch 62: Train Loss: 0.0065, Test Loss: 0.0065, Train L1 Norm: 0.0083, Test L1 Norm: 0.0236, Train Linf Norm: 0.5110, Test Linf Norm: 2.3151\n",
            "Epoch 63: Train Loss: 0.0043, Test Loss: 0.0041, Train L1 Norm: 0.0077, Test L1 Norm: 0.0270, Train Linf Norm: 0.5170, Test Linf Norm: 2.7890\n",
            "Epoch 64: Train Loss: 0.0040, Test Loss: 0.0041, Train L1 Norm: 0.0074, Test L1 Norm: 0.0270, Train Linf Norm: 0.5026, Test Linf Norm: 2.7890\n",
            "Epoch 65: Train Loss: 0.0041, Test Loss: 0.0038, Train L1 Norm: 0.0077, Test L1 Norm: 0.0263, Train Linf Norm: 0.5202, Test Linf Norm: 2.7188\n",
            "Epoch 66: Train Loss: 0.0067, Test Loss: 0.0042, Train L1 Norm: 0.0083, Test L1 Norm: 0.0307, Train Linf Norm: 0.5032, Test Linf Norm: 3.1390\n",
            "Epoch 67: Train Loss: 0.0123, Test Loss: 0.0148, Train L1 Norm: 0.0123, Test L1 Norm: 0.0208, Train Linf Norm: 0.7031, Test Linf Norm: 1.6774\n",
            "Epoch 68: Train Loss: 0.0194, Test Loss: 0.0195, Train L1 Norm: 0.0190, Test L1 Norm: 0.0395, Train Linf Norm: 1.0792, Test Linf Norm: 3.6575\n",
            "Epoch 69: Train Loss: 0.0273, Test Loss: 0.0313, Train L1 Norm: 0.0230, Test L1 Norm: 0.0363, Train Linf Norm: 1.1258, Test Linf Norm: 2.7150\n",
            "Epoch 70: Train Loss: 0.0345, Test Loss: 0.0396, Train L1 Norm: 0.0380, Test L1 Norm: 0.0227, Train Linf Norm: 2.3950, Test Linf Norm: 0.6461\n",
            "Epoch 71: Train Loss: 0.0403, Test Loss: 0.0378, Train L1 Norm: 0.0427, Test L1 Norm: 0.1153, Train Linf Norm: 2.3265, Test Linf Norm: 11.2160\n",
            "Epoch 72: Train Loss: 0.0431, Test Loss: 0.0486, Train L1 Norm: 0.0442, Test L1 Norm: 0.0254, Train Linf Norm: 2.7309, Test Linf Norm: 0.4757\n",
            "Epoch 73: Train Loss: 0.0441, Test Loss: 0.0565, Train L1 Norm: 0.0447, Test L1 Norm: 0.0784, Train Linf Norm: 2.7190, Test Linf Norm: 6.5286\n",
            "Epoch 74: Train Loss: 0.0422, Test Loss: 0.0455, Train L1 Norm: 0.0456, Test L1 Norm: 0.0356, Train Linf Norm: 2.9223, Test Linf Norm: 2.0138\n",
            "Epoch 75: Train Loss: 0.0380, Test Loss: 0.0298, Train L1 Norm: 0.0305, Test L1 Norm: 0.0186, Train Linf Norm: 1.4325, Test Linf Norm: 0.4295\n",
            "Epoch 76: Train Loss: 0.0321, Test Loss: 0.0445, Train L1 Norm: 0.0319, Test L1 Norm: 0.0265, Train Linf Norm: 1.9256, Test Linf Norm: 0.5403\n",
            "Epoch 77: Train Loss: 0.0250, Test Loss: 0.0082, Train L1 Norm: 0.0248, Test L1 Norm: 0.0233, Train Linf Norm: 1.4576, Test Linf Norm: 1.5499\n",
            "Epoch 78: Train Loss: 0.0178, Test Loss: 0.0251, Train L1 Norm: 0.0189, Test L1 Norm: 0.0141, Train Linf Norm: 1.1881, Test Linf Norm: 0.3450\n",
            "Epoch 79: Train Loss: 0.0110, Test Loss: 0.0101, Train L1 Norm: 0.0115, Test L1 Norm: 0.0199, Train Linf Norm: 0.6812, Test Linf Norm: 1.7875\n",
            "Epoch 80: Train Loss: 0.0060, Test Loss: 0.0041, Train L1 Norm: 0.0086, Test L1 Norm: 0.0231, Train Linf Norm: 0.5821, Test Linf Norm: 2.3335\n",
            "Epoch 81: Train Loss: 0.0040, Test Loss: 0.0040, Train L1 Norm: 0.0077, Test L1 Norm: 0.0184, Train Linf Norm: 0.5538, Test Linf Norm: 1.8093\n",
            "Epoch 82: Train Loss: 0.0039, Test Loss: 0.0040, Train L1 Norm: 0.0075, Test L1 Norm: 0.0184, Train Linf Norm: 0.5334, Test Linf Norm: 1.8093\n",
            "Epoch 83: Train Loss: 0.0037, Test Loss: 0.0035, Train L1 Norm: 0.0073, Test L1 Norm: 0.0199, Train Linf Norm: 0.5074, Test Linf Norm: 1.9972\n",
            "Epoch 84: Train Loss: 0.0061, Test Loss: 0.0071, Train L1 Norm: 0.0076, Test L1 Norm: 0.0208, Train Linf Norm: 0.4669, Test Linf Norm: 1.9792\n",
            "Epoch 85: Train Loss: 0.0112, Test Loss: 0.0086, Train L1 Norm: 0.0130, Test L1 Norm: 0.0255, Train Linf Norm: 0.8230, Test Linf Norm: 2.4257\n",
            "Epoch 86: Train Loss: 0.0177, Test Loss: 0.0247, Train L1 Norm: 0.0152, Test L1 Norm: 0.0352, Train Linf Norm: 0.7574, Test Linf Norm: 2.9500\n",
            "Epoch 87: Train Loss: 0.0250, Test Loss: 0.0111, Train L1 Norm: 0.0235, Test L1 Norm: 0.0400, Train Linf Norm: 1.3340, Test Linf Norm: 3.7250\n",
            "Epoch 88: Train Loss: 0.0314, Test Loss: 0.0307, Train L1 Norm: 0.0276, Test L1 Norm: 0.0483, Train Linf Norm: 1.4589, Test Linf Norm: 4.2956\n",
            "Epoch 89: Train Loss: 0.0367, Test Loss: 0.0582, Train L1 Norm: 0.0342, Test L1 Norm: 0.0332, Train Linf Norm: 1.9299, Test Linf Norm: 1.3175\n",
            "Epoch 90: Train Loss: 0.0397, Test Loss: 0.0584, Train L1 Norm: 0.0453, Test L1 Norm: 0.0333, Train Linf Norm: 3.0749, Test Linf Norm: 1.4846\n",
            "Epoch 91: Train Loss: 0.0405, Test Loss: 0.0453, Train L1 Norm: 0.0432, Test L1 Norm: 0.0283, Train Linf Norm: 2.7832, Test Linf Norm: 0.7576\n",
            "Epoch 92: Train Loss: 0.0389, Test Loss: 0.0454, Train L1 Norm: 0.0368, Test L1 Norm: 0.0208, Train Linf Norm: 2.1349, Test Linf Norm: 0.5297\n",
            "Epoch 93: Train Loss: 0.0349, Test Loss: 0.0401, Train L1 Norm: 0.0372, Test L1 Norm: 0.0198, Train Linf Norm: 2.3948, Test Linf Norm: 0.5199\n",
            "Epoch 94: Train Loss: 0.0296, Test Loss: 0.0261, Train L1 Norm: 0.0353, Test L1 Norm: 0.0552, Train Linf Norm: 2.4546, Test Linf Norm: 4.9566\n",
            "Epoch 95: Train Loss: 0.0230, Test Loss: 0.0293, Train L1 Norm: 0.0248, Test L1 Norm: 0.0679, Train Linf Norm: 1.5762, Test Linf Norm: 6.1614\n",
            "Epoch 96: Train Loss: 0.0162, Test Loss: 0.0196, Train L1 Norm: 0.0151, Test L1 Norm: 0.0504, Train Linf Norm: 0.8307, Test Linf Norm: 4.7965\n",
            "Epoch 97: Train Loss: 0.0099, Test Loss: 0.0095, Train L1 Norm: 0.0141, Test L1 Norm: 0.0451, Train Linf Norm: 1.0305, Test Linf Norm: 4.6394\n",
            "Epoch 98: Train Loss: 0.0053, Test Loss: 0.0074, Train L1 Norm: 0.0108, Test L1 Norm: 0.0344, Train Linf Norm: 0.8718, Test Linf Norm: 3.5448\n",
            "Epoch 99: Train Loss: 0.0035, Test Loss: 0.0034, Train L1 Norm: 0.0088, Test L1 Norm: 0.0268, Train Linf Norm: 0.7106, Test Linf Norm: 2.8147\n",
            "Epoch 100: Train Loss: 0.0033, Test Loss: 0.0034, Train L1 Norm: 0.0084, Test L1 Norm: 0.0268, Train Linf Norm: 0.6604, Test Linf Norm: 2.8147\n",
            "Epoch 101: Train Loss: 0.0033, Test Loss: 0.0031, Train L1 Norm: 0.0088, Test L1 Norm: 0.0253, Train Linf Norm: 0.7202, Test Linf Norm: 2.6642\n",
            "Epoch 102: Train Loss: 0.0055, Test Loss: 0.0045, Train L1 Norm: 0.0091, Test L1 Norm: 0.0195, Train Linf Norm: 0.6630, Test Linf Norm: 1.9274\n",
            "Epoch 103: Train Loss: 0.0101, Test Loss: 0.0120, Train L1 Norm: 0.0140, Test L1 Norm: 0.0191, Train Linf Norm: 1.0008, Test Linf Norm: 1.5499\n",
            "Epoch 104: Train Loss: 0.0162, Test Loss: 0.0178, Train L1 Norm: 0.0176, Test L1 Norm: 0.0338, Train Linf Norm: 1.1216, Test Linf Norm: 3.0736\n",
            "Epoch 105: Train Loss: 0.0226, Test Loss: 0.0074, Train L1 Norm: 0.0190, Test L1 Norm: 0.0452, Train Linf Norm: 0.9273, Test Linf Norm: 4.2353\n",
            "Epoch 106: Train Loss: 0.0288, Test Loss: 0.0240, Train L1 Norm: 0.0282, Test L1 Norm: 0.0418, Train Linf Norm: 1.6595, Test Linf Norm: 3.8283\n",
            "Epoch 107: Train Loss: 0.0334, Test Loss: 0.0481, Train L1 Norm: 0.0336, Test L1 Norm: 0.0405, Train Linf Norm: 2.0658, Test Linf Norm: 2.8371\n",
            "Epoch 108: Train Loss: 0.0364, Test Loss: 0.0296, Train L1 Norm: 0.0360, Test L1 Norm: 0.0409, Train Linf Norm: 2.1528, Test Linf Norm: 2.9824\n",
            "Epoch 109: Train Loss: 0.0372, Test Loss: 0.0341, Train L1 Norm: 0.0323, Test L1 Norm: 0.0207, Train Linf Norm: 1.7202, Test Linf Norm: 0.5363\n",
            "Epoch 110: Train Loss: 0.0356, Test Loss: 0.0387, Train L1 Norm: 0.0326, Test L1 Norm: 0.0217, Train Linf Norm: 1.8446, Test Linf Norm: 0.5696\n",
            "Epoch 111: Train Loss: 0.0320, Test Loss: 0.0173, Train L1 Norm: 0.0252, Test L1 Norm: 0.0186, Train Linf Norm: 1.1525, Test Linf Norm: 0.5200\n",
            "Epoch 112: Train Loss: 0.0270, Test Loss: 0.0414, Train L1 Norm: 0.0282, Test L1 Norm: 0.0169, Train Linf Norm: 1.7536, Test Linf Norm: 0.4573\n",
            "Epoch 113: Train Loss: 0.0212, Test Loss: 0.0301, Train L1 Norm: 0.0246, Test L1 Norm: 0.0158, Train Linf Norm: 1.6728, Test Linf Norm: 0.4168\n",
            "Epoch 114: Train Loss: 0.0148, Test Loss: 0.0114, Train L1 Norm: 0.0172, Test L1 Norm: 0.0285, Train Linf Norm: 1.1556, Test Linf Norm: 2.6749\n",
            "Epoch 115: Train Loss: 0.0091, Test Loss: 0.0120, Train L1 Norm: 0.0119, Test L1 Norm: 0.0228, Train Linf Norm: 0.8309, Test Linf Norm: 2.0988\n",
            "Epoch 116: Train Loss: 0.0049, Test Loss: 0.0063, Train L1 Norm: 0.0087, Test L1 Norm: 0.0185, Train Linf Norm: 0.6530, Test Linf Norm: 1.7673\n",
            "Epoch 117: Train Loss: 0.0032, Test Loss: 0.0034, Train L1 Norm: 0.0073, Test L1 Norm: 0.0148, Train Linf Norm: 0.5666, Test Linf Norm: 1.4385\n",
            "Epoch 118: Train Loss: 0.0034, Test Loss: 0.0034, Train L1 Norm: 0.0069, Test L1 Norm: 0.0148, Train Linf Norm: 0.4961, Test Linf Norm: 1.4385\n",
            "Epoch 119: Train Loss: 0.0030, Test Loss: 0.0029, Train L1 Norm: 0.0073, Test L1 Norm: 0.0185, Train Linf Norm: 0.5652, Test Linf Norm: 1.8673\n",
            "Epoch 120: Train Loss: 0.0051, Test Loss: 0.0037, Train L1 Norm: 0.0082, Test L1 Norm: 0.0190, Train Linf Norm: 0.5833, Test Linf Norm: 1.8794\n",
            "Epoch 121: Train Loss: 0.0094, Test Loss: 0.0086, Train L1 Norm: 0.0094, Test L1 Norm: 0.0259, Train Linf Norm: 0.5344, Test Linf Norm: 2.4539\n",
            "Epoch 122: Train Loss: 0.0149, Test Loss: 0.0147, Train L1 Norm: 0.0137, Test L1 Norm: 0.0296, Train Linf Norm: 0.6936, Test Linf Norm: 2.6812\n",
            "Epoch 123: Train Loss: 0.0209, Test Loss: 0.0226, Train L1 Norm: 0.0203, Test L1 Norm: 0.0464, Train Linf Norm: 1.1797, Test Linf Norm: 4.1338\n",
            "Epoch 124: Train Loss: 0.0265, Test Loss: 0.0283, Train L1 Norm: 0.0234, Test L1 Norm: 0.0394, Train Linf Norm: 1.2392, Test Linf Norm: 3.1728\n",
            "Epoch 125: Train Loss: 0.0311, Test Loss: 0.0310, Train L1 Norm: 0.0380, Test L1 Norm: 0.0736, Train Linf Norm: 2.6780, Test Linf Norm: 6.8373\n",
            "Epoch 126: Train Loss: 0.0338, Test Loss: 0.0331, Train L1 Norm: 0.0359, Test L1 Norm: 0.0938, Train Linf Norm: 2.1854, Test Linf Norm: 8.6897\n",
            "Epoch 127: Train Loss: 0.0343, Test Loss: 0.0258, Train L1 Norm: 0.0312, Test L1 Norm: 0.0658, Train Linf Norm: 1.7611, Test Linf Norm: 6.3112\n",
            "Epoch 128: Train Loss: 0.0329, Test Loss: 0.0441, Train L1 Norm: 0.0364, Test L1 Norm: 0.1091, Train Linf Norm: 2.3702, Test Linf Norm: 10.1772\n",
            "Epoch 129: Train Loss: 0.0300, Test Loss: 0.0293, Train L1 Norm: 0.0306, Test L1 Norm: 0.0408, Train Linf Norm: 1.8916, Test Linf Norm: 3.3521\n",
            "Epoch 130: Train Loss: 0.0253, Test Loss: 0.0296, Train L1 Norm: 0.0263, Test L1 Norm: 0.0479, Train Linf Norm: 1.6584, Test Linf Norm: 4.0748\n",
            "Epoch 131: Train Loss: 0.0198, Test Loss: 0.0205, Train L1 Norm: 0.0210, Test L1 Norm: 0.0530, Train Linf Norm: 1.3218, Test Linf Norm: 4.9707\n",
            "Epoch 132: Train Loss: 0.0139, Test Loss: 0.0159, Train L1 Norm: 0.0162, Test L1 Norm: 0.0264, Train Linf Norm: 1.0813, Test Linf Norm: 2.2282\n",
            "Epoch 133: Train Loss: 0.0086, Test Loss: 0.0086, Train L1 Norm: 0.0110, Test L1 Norm: 0.0241, Train Linf Norm: 0.7604, Test Linf Norm: 2.3259\n",
            "Epoch 134: Train Loss: 0.0046, Test Loss: 0.0037, Train L1 Norm: 0.0076, Test L1 Norm: 0.0234, Train Linf Norm: 0.5408, Test Linf Norm: 2.4145\n",
            "Epoch 135: Train Loss: 0.0029, Test Loss: 0.0041, Train L1 Norm: 0.0071, Test L1 Norm: 0.0188, Train Linf Norm: 0.5518, Test Linf Norm: 1.9043\n",
            "Epoch 136: Train Loss: 0.0040, Test Loss: 0.0041, Train L1 Norm: 0.0069, Test L1 Norm: 0.0188, Train Linf Norm: 0.5049, Test Linf Norm: 1.9043\n",
            "Epoch 137: Train Loss: 0.0028, Test Loss: 0.0032, Train L1 Norm: 0.0069, Test L1 Norm: 0.0211, Train Linf Norm: 0.5345, Test Linf Norm: 2.1863\n",
            "Epoch 138: Train Loss: 0.0047, Test Loss: 0.0053, Train L1 Norm: 0.0086, Test L1 Norm: 0.0183, Train Linf Norm: 0.6583, Test Linf Norm: 1.8016\n",
            "Epoch 139: Train Loss: 0.0088, Test Loss: 0.0087, Train L1 Norm: 0.0128, Test L1 Norm: 0.0139, Train Linf Norm: 0.9557, Test Linf Norm: 1.1708\n",
            "Epoch 140: Train Loss: 0.0140, Test Loss: 0.0183, Train L1 Norm: 0.0144, Test L1 Norm: 0.0479, Train Linf Norm: 0.8703, Test Linf Norm: 4.4843\n",
            "Epoch 141: Train Loss: 0.0195, Test Loss: 0.0168, Train L1 Norm: 0.0170, Test L1 Norm: 0.0377, Train Linf Norm: 0.8740, Test Linf Norm: 3.6298\n",
            "Epoch 142: Train Loss: 0.0249, Test Loss: 0.0237, Train L1 Norm: 0.0220, Test L1 Norm: 0.0322, Train Linf Norm: 1.1489, Test Linf Norm: 2.5313\n",
            "Epoch 143: Train Loss: 0.0289, Test Loss: 0.0286, Train L1 Norm: 0.0259, Test L1 Norm: 0.0176, Train Linf Norm: 1.3626, Test Linf Norm: 0.3576\n",
            "Epoch 144: Train Loss: 0.0316, Test Loss: 0.0195, Train L1 Norm: 0.0287, Test L1 Norm: 0.0201, Train Linf Norm: 1.5838, Test Linf Norm: 0.5943\n",
            "Epoch 145: Train Loss: 0.0325, Test Loss: 0.0321, Train L1 Norm: 0.0323, Test L1 Norm: 0.0173, Train Linf Norm: 1.9340, Test Linf Norm: 0.4197\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:44:33,166]\u001b[0m Trial 18 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 146: Train Loss: 0.0312, Test Loss: 0.0337, Train L1 Norm: 0.0344, Test L1 Norm: 0.0205, Train Linf Norm: 2.2373, Test Linf Norm: 0.4267\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:44:34,765]\u001b[0m Trial 19 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 4.3100, Test Loss: 3.0101, Train L1 Norm: 2.4529, Test L1 Norm: 5.0260, Train Linf Norm: 285.9984, Test Linf Norm: 786.4432\n",
            "Epoch 1: Train Loss: 0.5843, Test Loss: 0.3311, Train L1 Norm: 0.6912, Test L1 Norm: 0.4329, Train Linf Norm: 43.2891, Test Linf Norm: 16.0786\n",
            "Epoch 2: Train Loss: 0.2619, Test Loss: 0.2134, Train L1 Norm: 0.5146, Test L1 Norm: 0.7044, Train Linf Norm: 39.2180, Test Linf Norm: 90.1416\n",
            "Epoch 3: Train Loss: 0.1716, Test Loss: 0.1427, Train L1 Norm: 0.3851, Test L1 Norm: 0.9176, Train Linf Norm: 25.3808, Test Linf Norm: 154.6143\n",
            "Epoch 4: Train Loss: 0.1197, Test Loss: 0.1060, Train L1 Norm: 0.2895, Test L1 Norm: 0.6356, Train Linf Norm: 16.1844, Test Linf Norm: 102.1138\n",
            "Epoch 5: Train Loss: 0.0936, Test Loss: 0.0872, Train L1 Norm: 0.2358, Test L1 Norm: 0.4301, Train Linf Norm: 11.9953, Test Linf Norm: 60.3344\n",
            "Epoch 6: Train Loss: 0.0802, Test Loss: 0.0776, Train L1 Norm: 0.2057, Test L1 Norm: 0.2944, Train Linf Norm: 8.8129, Test Linf Norm: 31.4396\n",
            "Epoch 7: Train Loss: 0.0734, Test Loss: 0.0728, Train L1 Norm: 0.1944, Test L1 Norm: 0.2641, Train Linf Norm: 7.8400, Test Linf Norm: 25.2371\n",
            "Epoch 8: Train Loss: 0.0701, Test Loss: 0.0708, Train L1 Norm: 0.1906, Test L1 Norm: 0.2376, Train Linf Norm: 7.5096, Test Linf Norm: 19.5535\n",
            "Epoch 9: Train Loss: 0.0689, Test Loss: 0.0703, Train L1 Norm: 0.1895, Test L1 Norm: 0.2164, Train Linf Norm: 7.5783, Test Linf Norm: 14.8509\n",
            "Epoch 10: Train Loss: 0.0686, Test Loss: 0.0703, Train L1 Norm: 0.1880, Test L1 Norm: 0.2164, Train Linf Norm: 7.2764, Test Linf Norm: 14.8509\n",
            "Epoch 11: Train Loss: 0.0684, Test Loss: 0.0699, Train L1 Norm: 0.1883, Test L1 Norm: 0.2165, Train Linf Norm: 7.3816, Test Linf Norm: 14.8798\n",
            "Epoch 12: Train Loss: 0.0674, Test Loss: 0.0682, Train L1 Norm: 0.1869, Test L1 Norm: 0.1762, Train Linf Norm: 7.0996, Test Linf Norm: 6.0167\n",
            "Epoch 13: Train Loss: 0.0649, Test Loss: 0.0649, Train L1 Norm: 0.1846, Test L1 Norm: 0.1631, Train Linf Norm: 7.0833, Test Linf Norm: 3.4701\n",
            "Epoch 14: Train Loss: 0.0610, Test Loss: 0.0600, Train L1 Norm: 0.1808, Test L1 Norm: 0.1540, Train Linf Norm: 6.8612, Test Linf Norm: 2.1971\n",
            "Epoch 15: Train Loss: 0.0558, Test Loss: 0.0542, Train L1 Norm: 0.1780, Test L1 Norm: 0.1499, Train Linf Norm: 6.8813, Test Linf Norm: 1.9466\n",
            "Epoch 16: Train Loss: 0.0500, Test Loss: 0.0479, Train L1 Norm: 0.1775, Test L1 Norm: 0.1476, Train Linf Norm: 7.4866, Test Linf Norm: 1.9044\n",
            "Epoch 17: Train Loss: 0.0441, Test Loss: 0.0420, Train L1 Norm: 0.1781, Test L1 Norm: 0.1475, Train Linf Norm: 8.2367, Test Linf Norm: 2.4534\n",
            "Epoch 18: Train Loss: 0.0387, Test Loss: 0.0367, Train L1 Norm: 0.1761, Test L1 Norm: 0.1477, Train Linf Norm: 8.4555, Test Linf Norm: 3.1290\n",
            "Epoch 19: Train Loss: 0.0341, Test Loss: 0.0322, Train L1 Norm: 0.1762, Test L1 Norm: 0.1485, Train Linf Norm: 9.1744, Test Linf Norm: 4.0115\n",
            "Epoch 20: Train Loss: 0.0302, Test Loss: 0.0288, Train L1 Norm: 0.1824, Test L1 Norm: 0.1496, Train Linf Norm: 11.3086, Test Linf Norm: 4.7803\n",
            "Epoch 21: Train Loss: 0.0272, Test Loss: 0.0260, Train L1 Norm: 0.1805, Test L1 Norm: 0.1471, Train Linf Norm: 11.4611, Test Linf Norm: 4.7860\n",
            "Epoch 22: Train Loss: 0.0249, Test Loss: 0.0240, Train L1 Norm: 0.1808, Test L1 Norm: 0.1445, Train Linf Norm: 12.0581, Test Linf Norm: 4.7121\n",
            "Epoch 23: Train Loss: 0.0233, Test Loss: 0.0226, Train L1 Norm: 0.1787, Test L1 Norm: 0.1477, Train Linf Norm: 11.8927, Test Linf Norm: 5.8782\n",
            "Epoch 24: Train Loss: 0.0222, Test Loss: 0.0216, Train L1 Norm: 0.1788, Test L1 Norm: 0.1434, Train Linf Norm: 12.4145, Test Linf Norm: 5.1998\n",
            "Epoch 25: Train Loss: 0.0214, Test Loss: 0.0211, Train L1 Norm: 0.1791, Test L1 Norm: 0.1446, Train Linf Norm: 10.5744, Test Linf Norm: 5.7790\n",
            "Epoch 26: Train Loss: 0.0211, Test Loss: 0.0208, Train L1 Norm: 0.1772, Test L1 Norm: 0.1438, Train Linf Norm: 12.2839, Test Linf Norm: 5.6145\n",
            "Epoch 27: Train Loss: 0.0209, Test Loss: 0.0208, Train L1 Norm: 0.1771, Test L1 Norm: 0.1437, Train Linf Norm: 12.2624, Test Linf Norm: 5.6323\n",
            "Epoch 28: Train Loss: 0.0209, Test Loss: 0.0208, Train L1 Norm: 0.1771, Test L1 Norm: 0.1437, Train Linf Norm: 12.4621, Test Linf Norm: 5.6323\n",
            "Epoch 29: Train Loss: 0.0209, Test Loss: 0.0207, Train L1 Norm: 0.1772, Test L1 Norm: 0.1437, Train Linf Norm: 12.4425, Test Linf Norm: 5.6517\n",
            "Epoch 30: Train Loss: 0.0207, Test Loss: 0.0205, Train L1 Norm: 0.1775, Test L1 Norm: 0.1432, Train Linf Norm: 12.4081, Test Linf Norm: 5.5960\n",
            "Epoch 31: Train Loss: 0.0204, Test Loss: 0.0201, Train L1 Norm: 0.1775, Test L1 Norm: 0.1423, Train Linf Norm: 12.6595, Test Linf Norm: 5.5628\n",
            "Epoch 32: Train Loss: 0.0198, Test Loss: 0.0193, Train L1 Norm: 0.1772, Test L1 Norm: 0.1409, Train Linf Norm: 12.7290, Test Linf Norm: 5.4735\n",
            "Epoch 33: Train Loss: 0.0190, Test Loss: 0.0183, Train L1 Norm: 0.1758, Test L1 Norm: 0.1394, Train Linf Norm: 12.7279, Test Linf Norm: 5.5040\n",
            "Epoch 34: Train Loss: 0.0180, Test Loss: 0.0172, Train L1 Norm: 0.1714, Test L1 Norm: 0.1397, Train Linf Norm: 12.1221, Test Linf Norm: 5.9827\n",
            "Epoch 35: Train Loss: 0.0169, Test Loss: 0.0160, Train L1 Norm: 0.1732, Test L1 Norm: 0.1383, Train Linf Norm: 13.0006, Test Linf Norm: 6.1311\n",
            "Epoch 36: Train Loss: 0.0158, Test Loss: 0.0150, Train L1 Norm: 0.1704, Test L1 Norm: 0.1339, Train Linf Norm: 12.6732, Test Linf Norm: 5.6524\n",
            "Epoch 37: Train Loss: 0.0147, Test Loss: 0.0139, Train L1 Norm: 0.1689, Test L1 Norm: 0.1327, Train Linf Norm: 13.2040, Test Linf Norm: 6.0156\n",
            "Epoch 38: Train Loss: 0.0137, Test Loss: 0.0131, Train L1 Norm: 0.1643, Test L1 Norm: 0.1303, Train Linf Norm: 12.7360, Test Linf Norm: 5.9716\n",
            "Epoch 39: Train Loss: 0.0129, Test Loss: 0.0122, Train L1 Norm: 0.1617, Test L1 Norm: 0.1289, Train Linf Norm: 12.6567, Test Linf Norm: 6.0885\n",
            "Epoch 40: Train Loss: 0.0123, Test Loss: 0.0116, Train L1 Norm: 0.1577, Test L1 Norm: 0.1245, Train Linf Norm: 12.1303, Test Linf Norm: 5.6325\n",
            "Epoch 41: Train Loss: 0.0118, Test Loss: 0.0112, Train L1 Norm: 0.1558, Test L1 Norm: 0.1232, Train Linf Norm: 12.0269, Test Linf Norm: 5.5907\n",
            "Epoch 42: Train Loss: 0.0114, Test Loss: 0.0109, Train L1 Norm: 0.1568, Test L1 Norm: 0.1210, Train Linf Norm: 12.6519, Test Linf Norm: 5.3574\n",
            "Epoch 43: Train Loss: 0.0112, Test Loss: 0.0108, Train L1 Norm: 0.1543, Test L1 Norm: 0.1212, Train Linf Norm: 12.2944, Test Linf Norm: 5.4906\n",
            "Epoch 44: Train Loss: 0.0111, Test Loss: 0.0107, Train L1 Norm: 0.1557, Test L1 Norm: 0.1226, Train Linf Norm: 12.6263, Test Linf Norm: 5.7483\n",
            "Epoch 45: Train Loss: 0.0110, Test Loss: 0.0107, Train L1 Norm: 0.1553, Test L1 Norm: 0.1215, Train Linf Norm: 11.8478, Test Linf Norm: 5.5793\n",
            "Epoch 46: Train Loss: 0.0110, Test Loss: 0.0107, Train L1 Norm: 0.1555, Test L1 Norm: 0.1215, Train Linf Norm: 12.6551, Test Linf Norm: 5.5793\n",
            "Epoch 47: Train Loss: 0.0110, Test Loss: 0.0106, Train L1 Norm: 0.1552, Test L1 Norm: 0.1212, Train Linf Norm: 12.5451, Test Linf Norm: 5.5474\n",
            "Epoch 48: Train Loss: 0.0109, Test Loss: 0.0106, Train L1 Norm: 0.1545, Test L1 Norm: 0.1215, Train Linf Norm: 12.5793, Test Linf Norm: 5.6249\n",
            "Epoch 49: Train Loss: 0.0108, Test Loss: 0.0104, Train L1 Norm: 0.1547, Test L1 Norm: 0.1211, Train Linf Norm: 12.6978, Test Linf Norm: 5.6660\n",
            "Epoch 50: Train Loss: 0.0106, Test Loss: 0.0102, Train L1 Norm: 0.1544, Test L1 Norm: 0.1205, Train Linf Norm: 11.8567, Test Linf Norm: 5.7066\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:45:53,545]\u001b[0m Trial 20 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 51: Train Loss: 0.0104, Test Loss: 0.0099, Train L1 Norm: 0.1501, Test L1 Norm: 0.1158, Train Linf Norm: 12.0353, Test Linf Norm: 5.0615\n",
            "Epoch 1: Train Loss: 0.5524, Test Loss: 0.0964, Train L1 Norm: 0.6218, Test L1 Norm: 0.1451, Train Linf Norm: 28.2529, Test Linf Norm: 7.3560\n",
            "Epoch 2: Train Loss: 0.0697, Test Loss: 0.0685, Train L1 Norm: 0.1082, Test L1 Norm: 0.0755, Train Linf Norm: 4.5938, Test Linf Norm: 1.1791\n",
            "Epoch 3: Train Loss: 0.0534, Test Loss: 0.0549, Train L1 Norm: 0.0724, Test L1 Norm: 0.0505, Train Linf Norm: 2.4731, Test Linf Norm: 1.5344\n",
            "Epoch 4: Train Loss: 0.0522, Test Loss: 0.0465, Train L1 Norm: 0.0760, Test L1 Norm: 0.0494, Train Linf Norm: 3.2188, Test Linf Norm: 1.0111\n",
            "Epoch 5: Train Loss: 0.0471, Test Loss: 0.0543, Train L1 Norm: 0.0707, Test L1 Norm: 0.0773, Train Linf Norm: 2.9932, Test Linf Norm: 0.9961\n",
            "Epoch 6: Train Loss: 0.0453, Test Loss: 0.0350, Train L1 Norm: 0.0669, Test L1 Norm: 0.0401, Train Linf Norm: 2.6103, Test Linf Norm: 1.3111\n",
            "Epoch 7: Train Loss: 0.0483, Test Loss: 0.0366, Train L1 Norm: 0.0763, Test L1 Norm: 0.0481, Train Linf Norm: 3.3860, Test Linf Norm: 1.3419\n",
            "Epoch 8: Train Loss: 0.0467, Test Loss: 0.0400, Train L1 Norm: 0.0752, Test L1 Norm: 0.0597, Train Linf Norm: 3.3802, Test Linf Norm: 2.3702\n",
            "Epoch 9: Train Loss: 0.0506, Test Loss: 0.0516, Train L1 Norm: 0.0705, Test L1 Norm: 0.0572, Train Linf Norm: 2.6381, Test Linf Norm: 2.1428\n",
            "Epoch 10: Train Loss: 0.0410, Test Loss: 0.0377, Train L1 Norm: 0.0717, Test L1 Norm: 0.0381, Train Linf Norm: 3.6142, Test Linf Norm: 1.2908\n",
            "Epoch 11: Train Loss: 0.0408, Test Loss: 0.0394, Train L1 Norm: 0.0585, Test L1 Norm: 0.0512, Train Linf Norm: 2.1405, Test Linf Norm: 0.9882\n",
            "Epoch 12: Train Loss: 0.0458, Test Loss: 0.0524, Train L1 Norm: 0.0779, Test L1 Norm: 0.0764, Train Linf Norm: 3.7803, Test Linf Norm: 2.8585\n",
            "Epoch 13: Train Loss: 0.0309, Test Loss: 0.0292, Train L1 Norm: 0.0534, Test L1 Norm: 0.0366, Train Linf Norm: 2.4125, Test Linf Norm: 1.1035\n",
            "Epoch 14: Train Loss: 0.0279, Test Loss: 0.0274, Train L1 Norm: 0.0491, Test L1 Norm: 0.0378, Train Linf Norm: 2.1843, Test Linf Norm: 1.2091\n",
            "Epoch 15: Train Loss: 0.0284, Test Loss: 0.0271, Train L1 Norm: 0.0479, Test L1 Norm: 0.0425, Train Linf Norm: 2.0327, Test Linf Norm: 1.5625\n",
            "Epoch 16: Train Loss: 0.0269, Test Loss: 0.0255, Train L1 Norm: 0.0485, Test L1 Norm: 0.0350, Train Linf Norm: 2.2528, Test Linf Norm: 1.1067\n",
            "Epoch 17: Train Loss: 0.0268, Test Loss: 0.0280, Train L1 Norm: 0.0439, Test L1 Norm: 0.0412, Train Linf Norm: 1.7381, Test Linf Norm: 1.2569\n",
            "Epoch 18: Train Loss: 0.0263, Test Loss: 0.0278, Train L1 Norm: 0.0454, Test L1 Norm: 0.0411, Train Linf Norm: 1.8886, Test Linf Norm: 1.3202\n",
            "Epoch 19: Train Loss: 0.0260, Test Loss: 0.0297, Train L1 Norm: 0.0431, Test L1 Norm: 0.0343, Train Linf Norm: 1.6788, Test Linf Norm: 1.1797\n",
            "Epoch 20: Train Loss: 0.0262, Test Loss: 0.0240, Train L1 Norm: 0.0467, Test L1 Norm: 0.0366, Train Linf Norm: 2.0353, Test Linf Norm: 1.1869\n",
            "Epoch 21: Train Loss: 0.0257, Test Loss: 0.0276, Train L1 Norm: 0.0459, Test L1 Norm: 0.0400, Train Linf Norm: 2.0436, Test Linf Norm: 1.0186\n",
            "Epoch 22: Train Loss: 0.0253, Test Loss: 0.0236, Train L1 Norm: 0.0445, Test L1 Norm: 0.0346, Train Linf Norm: 1.8597, Test Linf Norm: 1.1399\n",
            "Epoch 23: Train Loss: 0.0249, Test Loss: 0.0235, Train L1 Norm: 0.0469, Test L1 Norm: 0.0363, Train Linf Norm: 2.1906, Test Linf Norm: 1.0508\n",
            "Epoch 24: Train Loss: 0.0251, Test Loss: 0.0250, Train L1 Norm: 0.0463, Test L1 Norm: 0.0350, Train Linf Norm: 2.1479, Test Linf Norm: 1.2043\n",
            "Epoch 25: Train Loss: 0.0251, Test Loss: 0.0230, Train L1 Norm: 0.0440, Test L1 Norm: 0.0340, Train Linf Norm: 1.8190, Test Linf Norm: 1.2114\n",
            "Epoch 26: Train Loss: 0.0251, Test Loss: 0.0247, Train L1 Norm: 0.0467, Test L1 Norm: 0.0362, Train Linf Norm: 2.1724, Test Linf Norm: 1.2169\n",
            "Epoch 27: Train Loss: 0.0244, Test Loss: 0.0263, Train L1 Norm: 0.0450, Test L1 Norm: 0.0374, Train Linf Norm: 1.9973, Test Linf Norm: 1.1864\n",
            "Epoch 28: Train Loss: 0.0243, Test Loss: 0.0233, Train L1 Norm: 0.0437, Test L1 Norm: 0.0326, Train Linf Norm: 1.8909, Test Linf Norm: 1.2332\n",
            "Epoch 29: Train Loss: 0.0240, Test Loss: 0.0227, Train L1 Norm: 0.0419, Test L1 Norm: 0.0350, Train Linf Norm: 1.7276, Test Linf Norm: 1.0607\n",
            "Epoch 30: Train Loss: 0.0245, Test Loss: 0.0234, Train L1 Norm: 0.0436, Test L1 Norm: 0.0390, Train Linf Norm: 1.8817, Test Linf Norm: 1.4001\n",
            "Epoch 31: Train Loss: 0.0253, Test Loss: 0.0309, Train L1 Norm: 0.0453, Test L1 Norm: 0.0365, Train Linf Norm: 1.9922, Test Linf Norm: 1.1967\n",
            "Epoch 32: Train Loss: 0.0243, Test Loss: 0.0248, Train L1 Norm: 0.0441, Test L1 Norm: 0.0359, Train Linf Norm: 1.9741, Test Linf Norm: 1.2997\n",
            "Epoch 33: Train Loss: 0.0247, Test Loss: 0.0250, Train L1 Norm: 0.0440, Test L1 Norm: 0.0355, Train Linf Norm: 1.9415, Test Linf Norm: 1.1365\n",
            "Epoch 34: Train Loss: 0.0245, Test Loss: 0.0251, Train L1 Norm: 0.0431, Test L1 Norm: 0.0338, Train Linf Norm: 1.8341, Test Linf Norm: 1.2016\n",
            "Epoch 35: Train Loss: 0.0247, Test Loss: 0.0224, Train L1 Norm: 0.0427, Test L1 Norm: 0.0325, Train Linf Norm: 1.7778, Test Linf Norm: 1.2638\n",
            "Epoch 36: Train Loss: 0.0236, Test Loss: 0.0215, Train L1 Norm: 0.0416, Test L1 Norm: 0.0332, Train Linf Norm: 1.7269, Test Linf Norm: 1.0771\n",
            "Epoch 37: Train Loss: 0.0245, Test Loss: 0.0279, Train L1 Norm: 0.0452, Test L1 Norm: 0.0349, Train Linf Norm: 2.0857, Test Linf Norm: 1.0983\n",
            "Epoch 38: Train Loss: 0.0241, Test Loss: 0.0246, Train L1 Norm: 0.0425, Test L1 Norm: 0.0321, Train Linf Norm: 1.8081, Test Linf Norm: 1.1434\n",
            "Epoch 39: Train Loss: 0.0241, Test Loss: 0.0267, Train L1 Norm: 0.0453, Test L1 Norm: 0.0373, Train Linf Norm: 2.1582, Test Linf Norm: 1.0402\n",
            "Epoch 40: Train Loss: 0.0238, Test Loss: 0.0275, Train L1 Norm: 0.0424, Test L1 Norm: 0.0377, Train Linf Norm: 1.8076, Test Linf Norm: 1.0211\n",
            "Epoch 41: Train Loss: 0.0240, Test Loss: 0.0254, Train L1 Norm: 0.0409, Test L1 Norm: 0.0389, Train Linf Norm: 1.6566, Test Linf Norm: 1.4154\n",
            "Epoch 42: Train Loss: 0.0240, Test Loss: 0.0253, Train L1 Norm: 0.0424, Test L1 Norm: 0.0421, Train Linf Norm: 1.8033, Test Linf Norm: 1.5896\n",
            "Epoch 43: Train Loss: 0.0215, Test Loss: 0.0216, Train L1 Norm: 0.0399, Test L1 Norm: 0.0321, Train Linf Norm: 1.7646, Test Linf Norm: 1.0763\n",
            "Epoch 44: Train Loss: 0.0212, Test Loss: 0.0208, Train L1 Norm: 0.0405, Test L1 Norm: 0.0319, Train Linf Norm: 1.8583, Test Linf Norm: 1.1000\n",
            "Epoch 45: Train Loss: 0.0213, Test Loss: 0.0214, Train L1 Norm: 0.0405, Test L1 Norm: 0.0328, Train Linf Norm: 1.8143, Test Linf Norm: 1.0892\n",
            "Epoch 46: Train Loss: 0.0212, Test Loss: 0.0211, Train L1 Norm: 0.0400, Test L1 Norm: 0.0308, Train Linf Norm: 1.7558, Test Linf Norm: 1.0837\n",
            "Epoch 47: Train Loss: 0.0211, Test Loss: 0.0215, Train L1 Norm: 0.0397, Test L1 Norm: 0.0311, Train Linf Norm: 1.7680, Test Linf Norm: 1.0723\n",
            "Epoch 48: Train Loss: 0.0210, Test Loss: 0.0221, Train L1 Norm: 0.0396, Test L1 Norm: 0.0311, Train Linf Norm: 1.7428, Test Linf Norm: 1.0551\n",
            "Epoch 49: Train Loss: 0.0210, Test Loss: 0.0211, Train L1 Norm: 0.0412, Test L1 Norm: 0.0324, Train Linf Norm: 1.9236, Test Linf Norm: 1.1465\n",
            "Epoch 50: Train Loss: 0.0209, Test Loss: 0.0205, Train L1 Norm: 0.0401, Test L1 Norm: 0.0308, Train Linf Norm: 1.8066, Test Linf Norm: 1.0935\n",
            "Epoch 51: Train Loss: 0.0209, Test Loss: 0.0208, Train L1 Norm: 0.0404, Test L1 Norm: 0.0332, Train Linf Norm: 1.8387, Test Linf Norm: 1.1622\n",
            "Epoch 52: Train Loss: 0.0208, Test Loss: 0.0204, Train L1 Norm: 0.0405, Test L1 Norm: 0.0311, Train Linf Norm: 1.8671, Test Linf Norm: 1.0782\n",
            "Epoch 53: Train Loss: 0.0206, Test Loss: 0.0208, Train L1 Norm: 0.0406, Test L1 Norm: 0.0315, Train Linf Norm: 1.8190, Test Linf Norm: 1.0926\n",
            "Epoch 54: Train Loss: 0.0206, Test Loss: 0.0205, Train L1 Norm: 0.0397, Test L1 Norm: 0.0323, Train Linf Norm: 1.7703, Test Linf Norm: 1.1319\n",
            "Epoch 55: Train Loss: 0.0207, Test Loss: 0.0209, Train L1 Norm: 0.0396, Test L1 Norm: 0.0346, Train Linf Norm: 1.7201, Test Linf Norm: 1.2182\n",
            "Epoch 56: Train Loss: 0.0207, Test Loss: 0.0220, Train L1 Norm: 0.0404, Test L1 Norm: 0.0323, Train Linf Norm: 1.8284, Test Linf Norm: 1.0706\n",
            "Epoch 57: Train Loss: 0.0207, Test Loss: 0.0202, Train L1 Norm: 0.0391, Test L1 Norm: 0.0314, Train Linf Norm: 1.7120, Test Linf Norm: 1.0792\n",
            "Epoch 58: Train Loss: 0.0205, Test Loss: 0.0204, Train L1 Norm: 0.0396, Test L1 Norm: 0.0323, Train Linf Norm: 1.7975, Test Linf Norm: 1.1585\n",
            "Epoch 59: Train Loss: 0.0207, Test Loss: 0.0205, Train L1 Norm: 0.0394, Test L1 Norm: 0.0317, Train Linf Norm: 1.7717, Test Linf Norm: 1.0742\n",
            "Epoch 60: Train Loss: 0.0206, Test Loss: 0.0208, Train L1 Norm: 0.0396, Test L1 Norm: 0.0312, Train Linf Norm: 1.7970, Test Linf Norm: 1.0570\n",
            "Epoch 61: Train Loss: 0.0207, Test Loss: 0.0206, Train L1 Norm: 0.0400, Test L1 Norm: 0.0316, Train Linf Norm: 1.8452, Test Linf Norm: 1.0664\n",
            "Epoch 62: Train Loss: 0.0206, Test Loss: 0.0216, Train L1 Norm: 0.0400, Test L1 Norm: 0.0318, Train Linf Norm: 1.8397, Test Linf Norm: 1.0691\n",
            "Epoch 63: Train Loss: 0.0204, Test Loss: 0.0205, Train L1 Norm: 0.0389, Test L1 Norm: 0.0315, Train Linf Norm: 1.7302, Test Linf Norm: 1.0716\n",
            "Epoch 64: Train Loss: 0.0199, Test Loss: 0.0200, Train L1 Norm: 0.0388, Test L1 Norm: 0.0311, Train Linf Norm: 1.7750, Test Linf Norm: 1.0826\n",
            "Epoch 65: Train Loss: 0.0199, Test Loss: 0.0201, Train L1 Norm: 0.0390, Test L1 Norm: 0.0310, Train Linf Norm: 1.7884, Test Linf Norm: 1.0721\n",
            "Epoch 66: Train Loss: 0.0198, Test Loss: 0.0200, Train L1 Norm: 0.0390, Test L1 Norm: 0.0309, Train Linf Norm: 1.7885, Test Linf Norm: 1.0722\n",
            "Epoch 67: Train Loss: 0.0198, Test Loss: 0.0201, Train L1 Norm: 0.0390, Test L1 Norm: 0.0314, Train Linf Norm: 1.8079, Test Linf Norm: 1.0804\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:48:01,537]\u001b[0m Trial 21 finished with value: 0.031244779742509127 and parameters: {'n_layers': 2, 'n_units_0': 51, 'n_units_1': 20, 'hidden_activation': 'Tanh', 'output_activation': 'ReLU', 'loss': 'MAE', 'optimizer': 'Adam', 'lr': 0.001959980482890388, 'batch_size': 117, 'n_epochs': 68, 'scheduler': 'ReduceLROnPlateau', 'weight_decay': 0.009170553149520737, 'beta1': 0.9568883762485666, 'beta2': 0.9034587642647415, 'factor': 0.19299168338002448, 'patience': 5, 'threshold': 0.00028928000223032383}. Best is trial 10 with value: 0.006550250181416049.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 68: Train Loss: 0.0198, Test Loss: 0.0201, Train L1 Norm: 0.0395, Test L1 Norm: 0.0312, Train Linf Norm: 1.8247, Test Linf Norm: 1.0775\n",
            "Epoch 1: Train Loss: 0.2777, Test Loss: 0.0958, Train L1 Norm: 0.2802, Test L1 Norm: 0.1234, Train Linf Norm: 16.3443, Test Linf Norm: 1.5397\n",
            "Epoch 2: Train Loss: 0.0767, Test Loss: 0.0451, Train L1 Norm: 0.1385, Test L1 Norm: 0.0751, Train Linf Norm: 10.3378, Test Linf Norm: 1.7824\n",
            "Epoch 3: Train Loss: 0.0770, Test Loss: 0.1130, Train L1 Norm: 0.1414, Test L1 Norm: 0.0931, Train Linf Norm: 12.2731, Test Linf Norm: 2.0248\n",
            "Epoch 4: Train Loss: 0.0712, Test Loss: 0.0527, Train L1 Norm: 0.1292, Test L1 Norm: 0.1438, Train Linf Norm: 9.9534, Test Linf Norm: 19.2134\n",
            "Epoch 5: Train Loss: 0.0671, Test Loss: 0.0624, Train L1 Norm: 0.1319, Test L1 Norm: 0.2468, Train Linf Norm: 11.8355, Test Linf Norm: 39.8137\n",
            "Epoch 6: Train Loss: 0.0730, Test Loss: 0.0570, Train L1 Norm: 0.1093, Test L1 Norm: 0.3048, Train Linf Norm: 5.7849, Test Linf Norm: 50.2221\n",
            "Epoch 7: Train Loss: 0.0736, Test Loss: 0.0593, Train L1 Norm: 0.1476, Test L1 Norm: 0.1019, Train Linf Norm: 12.8302, Test Linf Norm: 1.2124\n",
            "Epoch 8: Train Loss: 0.0687, Test Loss: 0.0365, Train L1 Norm: 0.1249, Test L1 Norm: 0.3600, Train Linf Norm: 9.6336, Test Linf Norm: 66.3012\n",
            "Epoch 9: Train Loss: 0.0730, Test Loss: 0.0844, Train L1 Norm: 0.1309, Test L1 Norm: 0.0947, Train Linf Norm: 10.8658, Test Linf Norm: 1.4900\n",
            "Epoch 10: Train Loss: 0.0684, Test Loss: 0.0367, Train L1 Norm: 0.1163, Test L1 Norm: 0.0622, Train Linf Norm: 8.5726, Test Linf Norm: 1.7849\n",
            "Epoch 11: Train Loss: 0.0645, Test Loss: 0.0905, Train L1 Norm: 0.1236, Test L1 Norm: 0.8360, Train Linf Norm: 11.8378, Test Linf Norm: 149.0140\n",
            "Epoch 12: Train Loss: 0.0630, Test Loss: 0.0595, Train L1 Norm: 0.1293, Test L1 Norm: 0.2791, Train Linf Norm: 10.9548, Test Linf Norm: 49.1047\n",
            "Epoch 13: Train Loss: 0.0657, Test Loss: 0.1433, Train L1 Norm: 0.1368, Test L1 Norm: 0.1711, Train Linf Norm: 12.6894, Test Linf Norm: 1.0000\n",
            "Epoch 14: Train Loss: 0.0714, Test Loss: 0.0453, Train L1 Norm: 0.1223, Test L1 Norm: 0.2447, Train Linf Norm: 9.5693, Test Linf Norm: 39.8867\n",
            "Epoch 15: Train Loss: 0.0799, Test Loss: 0.0514, Train L1 Norm: 0.1100, Test L1 Norm: 0.1874, Train Linf Norm: 6.5555, Test Linf Norm: 32.0218\n",
            "Epoch 16: Train Loss: 0.0660, Test Loss: 0.0729, Train L1 Norm: 0.1141, Test L1 Norm: 0.2162, Train Linf Norm: 7.1905, Test Linf Norm: 31.8028\n",
            "Epoch 17: Train Loss: 0.0566, Test Loss: 0.0729, Train L1 Norm: 0.1316, Test L1 Norm: 0.2915, Train Linf Norm: 12.9492, Test Linf Norm: 52.5051\n",
            "Epoch 18: Train Loss: 0.0633, Test Loss: 0.0417, Train L1 Norm: 0.1250, Test L1 Norm: 0.1304, Train Linf Norm: 10.2163, Test Linf Norm: 15.6264\n",
            "Epoch 19: Train Loss: 0.0604, Test Loss: 0.1154, Train L1 Norm: 0.1006, Test L1 Norm: 0.1294, Train Linf Norm: 5.2984, Test Linf Norm: 11.0897\n",
            "Epoch 20: Train Loss: 0.0730, Test Loss: 0.0529, Train L1 Norm: 0.1010, Test L1 Norm: 0.3074, Train Linf Norm: 5.3832, Test Linf Norm: 54.4873\n",
            "Epoch 21: Train Loss: 0.0666, Test Loss: 0.0661, Train L1 Norm: 0.1158, Test L1 Norm: 0.3234, Train Linf Norm: 8.1640, Test Linf Norm: 56.2094\n",
            "Epoch 22: Train Loss: 0.0592, Test Loss: 0.0705, Train L1 Norm: 0.1125, Test L1 Norm: 0.0708, Train Linf Norm: 8.9871, Test Linf Norm: 1.7697\n",
            "Epoch 23: Train Loss: 0.0576, Test Loss: 0.0592, Train L1 Norm: 0.1073, Test L1 Norm: 0.1653, Train Linf Norm: 7.2582, Test Linf Norm: 26.7020\n",
            "Epoch 24: Train Loss: 0.0505, Test Loss: 0.0735, Train L1 Norm: 0.0834, Test L1 Norm: 0.1328, Train Linf Norm: 4.1146, Test Linf Norm: 17.6067\n",
            "Epoch 25: Train Loss: 0.0601, Test Loss: 0.0307, Train L1 Norm: 0.0882, Test L1 Norm: 0.1632, Train Linf Norm: 3.8938, Test Linf Norm: 28.1085\n",
            "Epoch 26: Train Loss: 0.0591, Test Loss: 0.1272, Train L1 Norm: 0.1046, Test L1 Norm: 0.3779, Train Linf Norm: 7.9055, Test Linf Norm: 63.7142\n",
            "Epoch 27: Train Loss: 0.0597, Test Loss: 0.0731, Train L1 Norm: 0.1426, Test L1 Norm: 0.3193, Train Linf Norm: 15.8949, Test Linf Norm: 53.1589\n",
            "Epoch 28: Train Loss: 0.0592, Test Loss: 0.0918, Train L1 Norm: 0.1455, Test L1 Norm: 0.3087, Train Linf Norm: 14.5371, Test Linf Norm: 48.8487\n",
            "Epoch 29: Train Loss: 0.0555, Test Loss: 0.0978, Train L1 Norm: 0.1042, Test L1 Norm: 0.1492, Train Linf Norm: 7.1836, Test Linf Norm: 19.3295\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:48:43,183]\u001b[0m Trial 22 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Train Loss: 0.0549, Test Loss: 0.0531, Train L1 Norm: 0.1008, Test L1 Norm: 0.1815, Train Linf Norm: 7.1034, Test Linf Norm: 29.3073\n",
            "Epoch 1: Train Loss: 0.2205, Test Loss: 0.0696, Train L1 Norm: 0.1930, Test L1 Norm: 0.2156, Train Linf Norm: 3.3565, Test Linf Norm: 8.2049\n",
            "Epoch 2: Train Loss: 0.0580, Test Loss: 0.0811, Train L1 Norm: 0.0842, Test L1 Norm: 0.1073, Train Linf Norm: 1.7465, Test Linf Norm: 0.9546\n",
            "Epoch 3: Train Loss: 0.0514, Test Loss: 0.0480, Train L1 Norm: 0.0834, Test L1 Norm: 0.0634, Train Linf Norm: 1.9236, Test Linf Norm: 0.8280\n",
            "Epoch 4: Train Loss: 0.0503, Test Loss: 0.0308, Train L1 Norm: 0.0914, Test L1 Norm: 0.0520, Train Linf Norm: 2.2026, Test Linf Norm: 0.8913\n",
            "Epoch 5: Train Loss: 0.0475, Test Loss: 0.0414, Train L1 Norm: 0.0984, Test L1 Norm: 0.0602, Train Linf Norm: 2.7705, Test Linf Norm: 1.1572\n",
            "Epoch 6: Train Loss: 0.0461, Test Loss: 0.0756, Train L1 Norm: 0.0752, Test L1 Norm: 0.0720, Train Linf Norm: 1.6864, Test Linf Norm: 0.8949\n",
            "Epoch 7: Train Loss: 0.0465, Test Loss: 0.0355, Train L1 Norm: 0.0765, Test L1 Norm: 0.1518, Train Linf Norm: 1.7530, Test Linf Norm: 6.0704\n",
            "Epoch 8: Train Loss: 0.0449, Test Loss: 0.0577, Train L1 Norm: 0.0877, Test L1 Norm: 0.1726, Train Linf Norm: 2.3779, Test Linf Norm: 6.2791\n",
            "Epoch 9: Train Loss: 0.0439, Test Loss: 0.0454, Train L1 Norm: 0.0675, Test L1 Norm: 0.2293, Train Linf Norm: 1.4146, Test Linf Norm: 9.0330\n",
            "Epoch 10: Train Loss: 0.0437, Test Loss: 0.0693, Train L1 Norm: 0.0750, Test L1 Norm: 0.0862, Train Linf Norm: 1.8159, Test Linf Norm: 0.8861\n",
            "Epoch 11: Train Loss: 0.0432, Test Loss: 0.0537, Train L1 Norm: 0.0955, Test L1 Norm: 0.0477, Train Linf Norm: 2.8721, Test Linf Norm: 0.7053\n",
            "Epoch 12: Train Loss: 0.0422, Test Loss: 0.0861, Train L1 Norm: 0.0970, Test L1 Norm: 0.0516, Train Linf Norm: 2.9785, Test Linf Norm: 0.7365\n",
            "Epoch 13: Train Loss: 0.0422, Test Loss: 0.0699, Train L1 Norm: 0.0902, Test L1 Norm: 0.3125, Train Linf Norm: 2.6212, Test Linf Norm: 12.9833\n",
            "Epoch 14: Train Loss: 0.0418, Test Loss: 0.0307, Train L1 Norm: 0.0719, Test L1 Norm: 0.0432, Train Linf Norm: 1.7300, Test Linf Norm: 0.7996\n",
            "Epoch 15: Train Loss: 0.0106, Test Loss: 0.0105, Train L1 Norm: 0.0298, Test L1 Norm: 0.0245, Train Linf Norm: 0.8396, Test Linf Norm: 0.5636\n",
            "Epoch 16: Train Loss: 0.0100, Test Loss: 0.0098, Train L1 Norm: 0.0282, Test L1 Norm: 0.0226, Train Linf Norm: 0.8012, Test Linf Norm: 0.6271\n",
            "Epoch 17: Train Loss: 0.0097, Test Loss: 0.0076, Train L1 Norm: 0.0261, Test L1 Norm: 0.0202, Train Linf Norm: 0.7319, Test Linf Norm: 0.5746\n",
            "Epoch 18: Train Loss: 0.0092, Test Loss: 0.0089, Train L1 Norm: 0.0266, Test L1 Norm: 0.0214, Train Linf Norm: 0.7658, Test Linf Norm: 0.6021\n",
            "Epoch 19: Train Loss: 0.0091, Test Loss: 0.0076, Train L1 Norm: 0.0253, Test L1 Norm: 0.0190, Train Linf Norm: 0.7172, Test Linf Norm: 0.5424\n",
            "Epoch 20: Train Loss: 0.0089, Test Loss: 0.0107, Train L1 Norm: 0.0241, Test L1 Norm: 0.0203, Train Linf Norm: 0.6503, Test Linf Norm: 0.5655\n",
            "Epoch 21: Train Loss: 0.0089, Test Loss: 0.0105, Train L1 Norm: 0.0249, Test L1 Norm: 0.0242, Train Linf Norm: 0.7079, Test Linf Norm: 0.5835\n",
            "Epoch 22: Train Loss: 0.0087, Test Loss: 0.0068, Train L1 Norm: 0.0256, Test L1 Norm: 0.0197, Train Linf Norm: 0.7425, Test Linf Norm: 0.5870\n",
            "Epoch 23: Train Loss: 0.0086, Test Loss: 0.0103, Train L1 Norm: 0.0253, Test L1 Norm: 0.0313, Train Linf Norm: 0.7177, Test Linf Norm: 1.0146\n",
            "Epoch 24: Train Loss: 0.0086, Test Loss: 0.0071, Train L1 Norm: 0.0251, Test L1 Norm: 0.0216, Train Linf Norm: 0.7311, Test Linf Norm: 0.6260\n",
            "Epoch 25: Train Loss: 0.0084, Test Loss: 0.0074, Train L1 Norm: 0.0246, Test L1 Norm: 0.0223, Train Linf Norm: 0.7121, Test Linf Norm: 0.6457\n",
            "Epoch 26: Train Loss: 0.0083, Test Loss: 0.0112, Train L1 Norm: 0.0251, Test L1 Norm: 0.0228, Train Linf Norm: 0.7265, Test Linf Norm: 0.5451\n",
            "Epoch 27: Train Loss: 0.0084, Test Loss: 0.0069, Train L1 Norm: 0.0251, Test L1 Norm: 0.0191, Train Linf Norm: 0.7308, Test Linf Norm: 0.5600\n",
            "Epoch 28: Train Loss: 0.0083, Test Loss: 0.0075, Train L1 Norm: 0.0256, Test L1 Norm: 0.0209, Train Linf Norm: 0.7532, Test Linf Norm: 0.5514\n",
            "Epoch 29: Train Loss: 0.0084, Test Loss: 0.0093, Train L1 Norm: 0.0255, Test L1 Norm: 0.0200, Train Linf Norm: 0.7530, Test Linf Norm: 0.5493\n",
            "Epoch 30: Train Loss: 0.0085, Test Loss: 0.0080, Train L1 Norm: 0.0238, Test L1 Norm: 0.0203, Train Linf Norm: 0.6605, Test Linf Norm: 0.5447\n",
            "Epoch 31: Train Loss: 0.0082, Test Loss: 0.0084, Train L1 Norm: 0.0254, Test L1 Norm: 0.0215, Train Linf Norm: 0.7540, Test Linf Norm: 0.5921\n",
            "Epoch 32: Train Loss: 0.0084, Test Loss: 0.0095, Train L1 Norm: 0.0254, Test L1 Norm: 0.0204, Train Linf Norm: 0.7565, Test Linf Norm: 0.5773\n",
            "Epoch 33: Train Loss: 0.0053, Test Loss: 0.0052, Train L1 Norm: 0.0217, Test L1 Norm: 0.0189, Train Linf Norm: 0.6715, Test Linf Norm: 0.5596\n",
            "Epoch 34: Train Loss: 0.0052, Test Loss: 0.0051, Train L1 Norm: 0.0216, Test L1 Norm: 0.0188, Train Linf Norm: 0.6625, Test Linf Norm: 0.5539\n",
            "Epoch 35: Train Loss: 0.0052, Test Loss: 0.0053, Train L1 Norm: 0.0219, Test L1 Norm: 0.0187, Train Linf Norm: 0.6861, Test Linf Norm: 0.5737\n",
            "Epoch 36: Train Loss: 0.0052, Test Loss: 0.0053, Train L1 Norm: 0.0220, Test L1 Norm: 0.0189, Train Linf Norm: 0.6833, Test Linf Norm: 0.5660\n",
            "Epoch 37: Train Loss: 0.0051, Test Loss: 0.0054, Train L1 Norm: 0.0215, Test L1 Norm: 0.0196, Train Linf Norm: 0.6531, Test Linf Norm: 0.5987\n",
            "Epoch 38: Train Loss: 0.0051, Test Loss: 0.0048, Train L1 Norm: 0.0216, Test L1 Norm: 0.0190, Train Linf Norm: 0.6657, Test Linf Norm: 0.5776\n",
            "Epoch 39: Train Loss: 0.0051, Test Loss: 0.0055, Train L1 Norm: 0.0220, Test L1 Norm: 0.0190, Train Linf Norm: 0.6801, Test Linf Norm: 0.5746\n",
            "Epoch 40: Train Loss: 0.0051, Test Loss: 0.0050, Train L1 Norm: 0.0219, Test L1 Norm: 0.0189, Train Linf Norm: 0.6761, Test Linf Norm: 0.5813\n",
            "Epoch 41: Train Loss: 0.0051, Test Loss: 0.0051, Train L1 Norm: 0.0217, Test L1 Norm: 0.0194, Train Linf Norm: 0.6570, Test Linf Norm: 0.5898\n",
            "Epoch 42: Train Loss: 0.0050, Test Loss: 0.0048, Train L1 Norm: 0.0220, Test L1 Norm: 0.0187, Train Linf Norm: 0.6840, Test Linf Norm: 0.5763\n",
            "Epoch 43: Train Loss: 0.0050, Test Loss: 0.0056, Train L1 Norm: 0.0218, Test L1 Norm: 0.0190, Train Linf Norm: 0.6718, Test Linf Norm: 0.5638\n",
            "Epoch 44: Train Loss: 0.0050, Test Loss: 0.0057, Train L1 Norm: 0.0216, Test L1 Norm: 0.0189, Train Linf Norm: 0.6541, Test Linf Norm: 0.5660\n",
            "Epoch 45: Train Loss: 0.0050, Test Loss: 0.0052, Train L1 Norm: 0.0216, Test L1 Norm: 0.0196, Train Linf Norm: 0.6583, Test Linf Norm: 0.5946\n",
            "Epoch 46: Train Loss: 0.0050, Test Loss: 0.0050, Train L1 Norm: 0.0217, Test L1 Norm: 0.0189, Train Linf Norm: 0.6667, Test Linf Norm: 0.5619\n",
            "Epoch 47: Train Loss: 0.0049, Test Loss: 0.0050, Train L1 Norm: 0.0214, Test L1 Norm: 0.0189, Train Linf Norm: 0.6572, Test Linf Norm: 0.5806\n",
            "Epoch 48: Train Loss: 0.0049, Test Loss: 0.0048, Train L1 Norm: 0.0215, Test L1 Norm: 0.0187, Train Linf Norm: 0.6595, Test Linf Norm: 0.5733\n",
            "Epoch 49: Train Loss: 0.0049, Test Loss: 0.0049, Train L1 Norm: 0.0217, Test L1 Norm: 0.0189, Train Linf Norm: 0.6668, Test Linf Norm: 0.5691\n",
            "Epoch 50: Train Loss: 0.0049, Test Loss: 0.0047, Train L1 Norm: 0.0213, Test L1 Norm: 0.0184, Train Linf Norm: 0.6511, Test Linf Norm: 0.5583\n",
            "Epoch 51: Train Loss: 0.0049, Test Loss: 0.0048, Train L1 Norm: 0.0211, Test L1 Norm: 0.0185, Train Linf Norm: 0.6386, Test Linf Norm: 0.5637\n",
            "Epoch 52: Train Loss: 0.0049, Test Loss: 0.0048, Train L1 Norm: 0.0215, Test L1 Norm: 0.0184, Train Linf Norm: 0.6591, Test Linf Norm: 0.5611\n",
            "Epoch 53: Train Loss: 0.0049, Test Loss: 0.0052, Train L1 Norm: 0.0217, Test L1 Norm: 0.0184, Train Linf Norm: 0.6689, Test Linf Norm: 0.5589\n",
            "Epoch 54: Train Loss: 0.0048, Test Loss: 0.0049, Train L1 Norm: 0.0210, Test L1 Norm: 0.0185, Train Linf Norm: 0.6447, Test Linf Norm: 0.5561\n",
            "Epoch 55: Train Loss: 0.0048, Test Loss: 0.0052, Train L1 Norm: 0.0214, Test L1 Norm: 0.0182, Train Linf Norm: 0.6632, Test Linf Norm: 0.5510\n",
            "Epoch 56: Train Loss: 0.0048, Test Loss: 0.0051, Train L1 Norm: 0.0211, Test L1 Norm: 0.0182, Train Linf Norm: 0.6513, Test Linf Norm: 0.5524\n",
            "Epoch 57: Train Loss: 0.0048, Test Loss: 0.0049, Train L1 Norm: 0.0207, Test L1 Norm: 0.0186, Train Linf Norm: 0.6330, Test Linf Norm: 0.5651\n",
            "Epoch 58: Train Loss: 0.0048, Test Loss: 0.0045, Train L1 Norm: 0.0216, Test L1 Norm: 0.0184, Train Linf Norm: 0.6764, Test Linf Norm: 0.5605\n",
            "Epoch 59: Train Loss: 0.0048, Test Loss: 0.0049, Train L1 Norm: 0.0210, Test L1 Norm: 0.0185, Train Linf Norm: 0.6428, Test Linf Norm: 0.5654\n",
            "Epoch 60: Train Loss: 0.0048, Test Loss: 0.0045, Train L1 Norm: 0.0213, Test L1 Norm: 0.0185, Train Linf Norm: 0.6597, Test Linf Norm: 0.5672\n",
            "Epoch 61: Train Loss: 0.0047, Test Loss: 0.0050, Train L1 Norm: 0.0213, Test L1 Norm: 0.0185, Train Linf Norm: 0.6456, Test Linf Norm: 0.5575\n",
            "Epoch 62: Train Loss: 0.0047, Test Loss: 0.0046, Train L1 Norm: 0.0208, Test L1 Norm: 0.0181, Train Linf Norm: 0.6369, Test Linf Norm: 0.5524\n",
            "Epoch 63: Train Loss: 0.0047, Test Loss: 0.0054, Train L1 Norm: 0.0210, Test L1 Norm: 0.0184, Train Linf Norm: 0.6343, Test Linf Norm: 0.5522\n",
            "Epoch 64: Train Loss: 0.0047, Test Loss: 0.0046, Train L1 Norm: 0.0208, Test L1 Norm: 0.0181, Train Linf Norm: 0.6338, Test Linf Norm: 0.5534\n",
            "Epoch 65: Train Loss: 0.0047, Test Loss: 0.0046, Train L1 Norm: 0.0204, Test L1 Norm: 0.0180, Train Linf Norm: 0.6223, Test Linf Norm: 0.5478\n",
            "Epoch 66: Train Loss: 0.0047, Test Loss: 0.0052, Train L1 Norm: 0.0210, Test L1 Norm: 0.0183, Train Linf Norm: 0.6443, Test Linf Norm: 0.5460\n",
            "Epoch 67: Train Loss: 0.0046, Test Loss: 0.0049, Train L1 Norm: 0.0209, Test L1 Norm: 0.0183, Train Linf Norm: 0.6383, Test Linf Norm: 0.5385\n",
            "Epoch 68: Train Loss: 0.0047, Test Loss: 0.0047, Train L1 Norm: 0.0210, Test L1 Norm: 0.0178, Train Linf Norm: 0.6535, Test Linf Norm: 0.5374\n",
            "Epoch 69: Train Loss: 0.0044, Test Loss: 0.0043, Train L1 Norm: 0.0205, Test L1 Norm: 0.0178, Train Linf Norm: 0.6363, Test Linf Norm: 0.5453\n",
            "Epoch 70: Train Loss: 0.0044, Test Loss: 0.0044, Train L1 Norm: 0.0205, Test L1 Norm: 0.0180, Train Linf Norm: 0.6277, Test Linf Norm: 0.5532\n",
            "Epoch 71: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0204, Test L1 Norm: 0.0180, Train Linf Norm: 0.6276, Test Linf Norm: 0.5509\n",
            "Epoch 72: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0205, Test L1 Norm: 0.0179, Train Linf Norm: 0.6222, Test Linf Norm: 0.5462\n",
            "Epoch 73: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0205, Test L1 Norm: 0.0179, Train Linf Norm: 0.6368, Test Linf Norm: 0.5447\n",
            "Epoch 74: Train Loss: 0.0043, Test Loss: 0.0044, Train L1 Norm: 0.0205, Test L1 Norm: 0.0179, Train Linf Norm: 0.6228, Test Linf Norm: 0.5478\n",
            "Epoch 75: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0204, Test L1 Norm: 0.0179, Train Linf Norm: 0.6293, Test Linf Norm: 0.5466\n",
            "Epoch 76: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0204, Test L1 Norm: 0.0180, Train Linf Norm: 0.6361, Test Linf Norm: 0.5491\n",
            "Epoch 77: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0205, Test L1 Norm: 0.0179, Train Linf Norm: 0.6302, Test Linf Norm: 0.5467\n",
            "Epoch 78: Train Loss: 0.0043, Test Loss: 0.0045, Train L1 Norm: 0.0205, Test L1 Norm: 0.0180, Train Linf Norm: 0.6280, Test Linf Norm: 0.5487\n",
            "Epoch 79: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0206, Test L1 Norm: 0.0179, Train Linf Norm: 0.6417, Test Linf Norm: 0.5480\n",
            "Epoch 80: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0205, Test L1 Norm: 0.0179, Train Linf Norm: 0.6312, Test Linf Norm: 0.5476\n",
            "Epoch 81: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0205, Test L1 Norm: 0.0180, Train Linf Norm: 0.6292, Test Linf Norm: 0.5492\n",
            "Epoch 82: Train Loss: 0.0043, Test Loss: 0.0044, Train L1 Norm: 0.0205, Test L1 Norm: 0.0180, Train Linf Norm: 0.6327, Test Linf Norm: 0.5479\n",
            "Epoch 83: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0205, Test L1 Norm: 0.0180, Train Linf Norm: 0.6311, Test Linf Norm: 0.5520\n",
            "Epoch 84: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0205, Test L1 Norm: 0.0179, Train Linf Norm: 0.6254, Test Linf Norm: 0.5491\n",
            "Epoch 85: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0204, Test L1 Norm: 0.0179, Train Linf Norm: 0.6216, Test Linf Norm: 0.5478\n",
            "Epoch 86: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0204, Test L1 Norm: 0.0179, Train Linf Norm: 0.6302, Test Linf Norm: 0.5469\n",
            "Epoch 87: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0205, Test L1 Norm: 0.0179, Train Linf Norm: 0.6269, Test Linf Norm: 0.5484\n",
            "Epoch 88: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0205, Test L1 Norm: 0.0179, Train Linf Norm: 0.6386, Test Linf Norm: 0.5465\n",
            "Epoch 89: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0205, Test L1 Norm: 0.0180, Train Linf Norm: 0.6314, Test Linf Norm: 0.5488\n",
            "Epoch 90: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0205, Test L1 Norm: 0.0180, Train Linf Norm: 0.6355, Test Linf Norm: 0.5499\n",
            "Epoch 91: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0204, Test L1 Norm: 0.0179, Train Linf Norm: 0.6313, Test Linf Norm: 0.5478\n",
            "Epoch 92: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0206, Test L1 Norm: 0.0180, Train Linf Norm: 0.6339, Test Linf Norm: 0.5466\n",
            "Epoch 93: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0206, Test L1 Norm: 0.0179, Train Linf Norm: 0.6362, Test Linf Norm: 0.5475\n",
            "Epoch 94: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0206, Test L1 Norm: 0.0178, Train Linf Norm: 0.6332, Test Linf Norm: 0.5441\n",
            "Epoch 95: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0205, Test L1 Norm: 0.0179, Train Linf Norm: 0.6285, Test Linf Norm: 0.5475\n",
            "Epoch 96: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0206, Test L1 Norm: 0.0178, Train Linf Norm: 0.6314, Test Linf Norm: 0.5456\n",
            "Epoch 97: Train Loss: 0.0043, Test Loss: 0.0045, Train L1 Norm: 0.0206, Test L1 Norm: 0.0182, Train Linf Norm: 0.6363, Test Linf Norm: 0.5547\n",
            "Epoch 98: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0205, Test L1 Norm: 0.0179, Train Linf Norm: 0.6369, Test Linf Norm: 0.5461\n",
            "Epoch 99: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0204, Test L1 Norm: 0.0180, Train Linf Norm: 0.6290, Test Linf Norm: 0.5512\n",
            "Epoch 100: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0205, Test L1 Norm: 0.0180, Train Linf Norm: 0.6248, Test Linf Norm: 0.5479\n",
            "Epoch 101: Train Loss: 0.0043, Test Loss: 0.0043, Train L1 Norm: 0.0205, Test L1 Norm: 0.0179, Train Linf Norm: 0.6347, Test Linf Norm: 0.5448\n",
            "Epoch 102: Train Loss: 0.0042, Test Loss: 0.0042, Train L1 Norm: 0.0204, Test L1 Norm: 0.0178, Train Linf Norm: 0.6275, Test Linf Norm: 0.5452\n",
            "Epoch 103: Train Loss: 0.0042, Test Loss: 0.0042, Train L1 Norm: 0.0204, Test L1 Norm: 0.0179, Train Linf Norm: 0.6325, Test Linf Norm: 0.5455\n",
            "Epoch 104: Train Loss: 0.0042, Test Loss: 0.0042, Train L1 Norm: 0.0205, Test L1 Norm: 0.0179, Train Linf Norm: 0.6382, Test Linf Norm: 0.5472\n",
            "Epoch 105: Train Loss: 0.0042, Test Loss: 0.0042, Train L1 Norm: 0.0204, Test L1 Norm: 0.0179, Train Linf Norm: 0.6297, Test Linf Norm: 0.5466\n",
            "Epoch 106: Train Loss: 0.0042, Test Loss: 0.0042, Train L1 Norm: 0.0204, Test L1 Norm: 0.0179, Train Linf Norm: 0.6336, Test Linf Norm: 0.5474\n",
            "Epoch 107: Train Loss: 0.0042, Test Loss: 0.0042, Train L1 Norm: 0.0204, Test L1 Norm: 0.0179, Train Linf Norm: 0.6323, Test Linf Norm: 0.5466\n",
            "Epoch 108: Train Loss: 0.0042, Test Loss: 0.0042, Train L1 Norm: 0.0205, Test L1 Norm: 0.0179, Train Linf Norm: 0.6265, Test Linf Norm: 0.5461\n",
            "Epoch 109: Train Loss: 0.0042, Test Loss: 0.0042, Train L1 Norm: 0.0205, Test L1 Norm: 0.0179, Train Linf Norm: 0.6294, Test Linf Norm: 0.5460\n",
            "Epoch 110: Train Loss: 0.0042, Test Loss: 0.0042, Train L1 Norm: 0.0205, Test L1 Norm: 0.0179, Train Linf Norm: 0.6305, Test Linf Norm: 0.5472\n",
            "Epoch 111: Train Loss: 0.0042, Test Loss: 0.0042, Train L1 Norm: 0.0205, Test L1 Norm: 0.0179, Train Linf Norm: 0.6304, Test Linf Norm: 0.5465\n",
            "Epoch 112: Train Loss: 0.0042, Test Loss: 0.0042, Train L1 Norm: 0.0205, Test L1 Norm: 0.0179, Train Linf Norm: 0.6336, Test Linf Norm: 0.5465\n",
            "Epoch 113: Train Loss: 0.0042, Test Loss: 0.0042, Train L1 Norm: 0.0205, Test L1 Norm: 0.0179, Train Linf Norm: 0.6320, Test Linf Norm: 0.5466\n",
            "Epoch 114: Train Loss: 0.0042, Test Loss: 0.0042, Train L1 Norm: 0.0205, Test L1 Norm: 0.0179, Train Linf Norm: 0.6321, Test Linf Norm: 0.5463\n",
            "Epoch 115: Train Loss: 0.0042, Test Loss: 0.0042, Train L1 Norm: 0.0205, Test L1 Norm: 0.0179, Train Linf Norm: 0.6314, Test Linf Norm: 0.5465\n",
            "Epoch 116: Train Loss: 0.0042, Test Loss: 0.0042, Train L1 Norm: 0.0205, Test L1 Norm: 0.0179, Train Linf Norm: 0.6257, Test Linf Norm: 0.5464\n",
            "Epoch 117: Train Loss: 0.0042, Test Loss: 0.0042, Train L1 Norm: 0.0205, Test L1 Norm: 0.0179, Train Linf Norm: 0.6334, Test Linf Norm: 0.5466\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:55:41,636]\u001b[0m Trial 23 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 118: Train Loss: 0.0042, Test Loss: 0.0042, Train L1 Norm: 0.0205, Test L1 Norm: 0.0179, Train Linf Norm: 0.6395, Test Linf Norm: 0.5469\n",
            "Epoch 1: Train Loss: 1.2686, Test Loss: 0.6525, Train L1 Norm: 2.1046, Test L1 Norm: 0.3339, Train Linf Norm: 145.8601, Test Linf Norm: 1.6098\n",
            "Epoch 2: Train Loss: 0.5103, Test Loss: 0.4130, Train L1 Norm: 0.3777, Test L1 Norm: 0.2790, Train Linf Norm: 10.4379, Test Linf Norm: 4.4661\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:55:46,379]\u001b[0m Trial 24 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Train Loss: 0.3347, Test Loss: 0.2875, Train L1 Norm: 0.7298, Test L1 Norm: 0.3690, Train Linf Norm: 65.8043, Test Linf Norm: 24.8466\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:55:48,049]\u001b[0m Trial 25 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.8307, Test Loss: 0.3780, Train L1 Norm: 1.1637, Test L1 Norm: 0.4554, Train Linf Norm: 59.0250, Test Linf Norm: 1.1106\n",
            "Epoch 1: Train Loss: 0.2025, Test Loss: 0.1001, Train L1 Norm: 0.3298, Test L1 Norm: 0.4053, Train Linf Norm: 21.1150, Test Linf Norm: 33.7782\n",
            "Epoch 2: Train Loss: 0.1038, Test Loss: 0.0920, Train L1 Norm: 0.1702, Test L1 Norm: 0.0548, Train Linf Norm: 11.5069, Test Linf Norm: 1.0592\n",
            "Epoch 3: Train Loss: 0.1001, Test Loss: 0.0944, Train L1 Norm: 0.1114, Test L1 Norm: 0.0735, Train Linf Norm: 6.1447, Test Linf Norm: 4.0733\n",
            "Epoch 4: Train Loss: 0.0780, Test Loss: 0.0458, Train L1 Norm: 0.0926, Test L1 Norm: 0.0385, Train Linf Norm: 5.4908, Test Linf Norm: 0.7493\n",
            "Epoch 5: Train Loss: 0.0838, Test Loss: 0.1474, Train L1 Norm: 0.0944, Test L1 Norm: 0.2825, Train Linf Norm: 5.5062, Test Linf Norm: 21.9636\n",
            "Epoch 6: Train Loss: 0.0812, Test Loss: 0.1884, Train L1 Norm: 0.0815, Test L1 Norm: 0.0698, Train Linf Norm: 4.3064, Test Linf Norm: 0.5620\n",
            "Epoch 7: Train Loss: 0.0766, Test Loss: 0.0169, Train L1 Norm: 0.0918, Test L1 Norm: 0.0936, Train Linf Norm: 5.6273, Test Linf Norm: 7.8722\n",
            "Epoch 8: Train Loss: 0.0758, Test Loss: 0.0199, Train L1 Norm: 0.0769, Test L1 Norm: 0.0310, Train Linf Norm: 4.1871, Test Linf Norm: 1.7494\n",
            "Epoch 9: Train Loss: 0.0884, Test Loss: 0.0849, Train L1 Norm: 0.0965, Test L1 Norm: 0.1315, Train Linf Norm: 5.7006, Test Linf Norm: 9.2450\n",
            "Epoch 10: Train Loss: 0.0784, Test Loss: 0.1054, Train L1 Norm: 0.0831, Test L1 Norm: 0.0487, Train Linf Norm: 4.8228, Test Linf Norm: 0.7047\n",
            "Epoch 11: Train Loss: 0.0752, Test Loss: 0.0278, Train L1 Norm: 0.0829, Test L1 Norm: 0.0763, Train Linf Norm: 4.5541, Test Linf Norm: 5.9528\n",
            "Epoch 12: Train Loss: 0.0701, Test Loss: 0.0902, Train L1 Norm: 0.0691, Test L1 Norm: 0.1940, Train Linf Norm: 3.6308, Test Linf Norm: 15.2946\n",
            "Epoch 13: Train Loss: 0.0647, Test Loss: 0.0622, Train L1 Norm: 0.0711, Test L1 Norm: 0.0388, Train Linf Norm: 4.1237, Test Linf Norm: 0.8740\n",
            "Epoch 14: Train Loss: 0.0607, Test Loss: 0.1179, Train L1 Norm: 0.0691, Test L1 Norm: 0.2173, Train Linf Norm: 4.1755, Test Linf Norm: 16.4015\n",
            "Epoch 15: Train Loss: 0.0671, Test Loss: 0.1727, Train L1 Norm: 0.0722, Test L1 Norm: 0.2087, Train Linf Norm: 4.1732, Test Linf Norm: 14.2785\n",
            "Epoch 16: Train Loss: 0.0598, Test Loss: 0.0453, Train L1 Norm: 0.0714, Test L1 Norm: 0.0974, Train Linf Norm: 4.4378, Test Linf Norm: 7.7325\n",
            "Epoch 17: Train Loss: 0.0695, Test Loss: 0.0287, Train L1 Norm: 0.0683, Test L1 Norm: 0.0564, Train Linf Norm: 3.8285, Test Linf Norm: 4.2577\n",
            "Epoch 18: Train Loss: 0.0559, Test Loss: 0.0365, Train L1 Norm: 0.0605, Test L1 Norm: 0.0344, Train Linf Norm: 3.4081, Test Linf Norm: 1.1378\n",
            "Epoch 19: Train Loss: 0.0726, Test Loss: 0.0613, Train L1 Norm: 0.0809, Test L1 Norm: 0.0628, Train Linf Norm: 4.8272, Test Linf Norm: 4.0453\n",
            "Epoch 20: Train Loss: 0.0601, Test Loss: 0.0788, Train L1 Norm: 0.0667, Test L1 Norm: 0.0365, Train Linf Norm: 3.9833, Test Linf Norm: 0.7242\n",
            "Epoch 21: Train Loss: 0.0576, Test Loss: 0.0121, Train L1 Norm: 0.0646, Test L1 Norm: 0.0530, Train Linf Norm: 3.8742, Test Linf Norm: 4.4465\n",
            "Epoch 22: Train Loss: 0.0688, Test Loss: 0.1584, Train L1 Norm: 0.0626, Test L1 Norm: 0.0638, Train Linf Norm: 3.2863, Test Linf Norm: 0.7865\n",
            "Epoch 23: Train Loss: 0.0650, Test Loss: 0.0388, Train L1 Norm: 0.0584, Test L1 Norm: 0.0633, Train Linf Norm: 2.9135, Test Linf Norm: 4.8976\n",
            "Epoch 24: Train Loss: 0.0597, Test Loss: 0.0168, Train L1 Norm: 0.0667, Test L1 Norm: 0.0644, Train Linf Norm: 3.9944, Test Linf Norm: 5.4922\n",
            "Epoch 25: Train Loss: 0.0624, Test Loss: 0.0573, Train L1 Norm: 0.0610, Test L1 Norm: 0.0448, Train Linf Norm: 3.3124, Test Linf Norm: 2.4258\n",
            "Epoch 26: Train Loss: 0.0612, Test Loss: 0.0309, Train L1 Norm: 0.0620, Test L1 Norm: 0.0256, Train Linf Norm: 3.4555, Test Linf Norm: 0.7404\n",
            "Epoch 27: Train Loss: 0.0613, Test Loss: 0.1315, Train L1 Norm: 0.0634, Test L1 Norm: 0.2104, Train Linf Norm: 3.6488, Test Linf Norm: 15.8246\n",
            "Epoch 28: Train Loss: 0.0537, Test Loss: 0.0330, Train L1 Norm: 0.0524, Test L1 Norm: 0.0281, Train Linf Norm: 2.8312, Test Linf Norm: 0.8454\n",
            "Epoch 29: Train Loss: 0.0612, Test Loss: 0.0697, Train L1 Norm: 0.0671, Test L1 Norm: 0.1289, Train Linf Norm: 3.9921, Test Linf Norm: 9.9828\n",
            "Epoch 30: Train Loss: 0.0613, Test Loss: 0.0879, Train L1 Norm: 0.0546, Test L1 Norm: 0.0641, Train Linf Norm: 2.6926, Test Linf Norm: 3.6855\n",
            "Epoch 31: Train Loss: 0.0526, Test Loss: 0.0176, Train L1 Norm: 0.0494, Test L1 Norm: 0.0799, Train Linf Norm: 2.5998, Test Linf Norm: 7.0068\n",
            "Epoch 32: Train Loss: 0.0468, Test Loss: 0.0697, Train L1 Norm: 0.0545, Test L1 Norm: 0.0389, Train Linf Norm: 3.3045, Test Linf Norm: 0.7283\n",
            "Epoch 33: Train Loss: 0.0569, Test Loss: 0.1113, Train L1 Norm: 0.0553, Test L1 Norm: 0.0511, Train Linf Norm: 3.0557, Test Linf Norm: 0.7712\n",
            "Epoch 34: Train Loss: 0.0572, Test Loss: 0.0882, Train L1 Norm: 0.0575, Test L1 Norm: 0.1475, Train Linf Norm: 3.2296, Test Linf Norm: 11.4476\n",
            "Epoch 35: Train Loss: 0.0594, Test Loss: 0.0412, Train L1 Norm: 0.0657, Test L1 Norm: 0.1135, Train Linf Norm: 3.9383, Test Linf Norm: 9.5360\n",
            "Epoch 36: Train Loss: 0.0547, Test Loss: 0.0416, Train L1 Norm: 0.0556, Test L1 Norm: 0.0366, Train Linf Norm: 3.1687, Test Linf Norm: 2.1216\n",
            "Epoch 37: Train Loss: 0.0531, Test Loss: 0.1198, Train L1 Norm: 0.0518, Test L1 Norm: 0.1344, Train Linf Norm: 2.8585, Test Linf Norm: 8.9275\n",
            "Epoch 38: Train Loss: 0.0543, Test Loss: 0.0158, Train L1 Norm: 0.0482, Test L1 Norm: 0.0217, Train Linf Norm: 2.3982, Test Linf Norm: 1.0257\n",
            "Epoch 39: Train Loss: 0.0531, Test Loss: 0.0685, Train L1 Norm: 0.0629, Test L1 Norm: 0.1541, Train Linf Norm: 3.8643, Test Linf Norm: 12.5987\n",
            "Epoch 40: Train Loss: 0.0473, Test Loss: 0.0419, Train L1 Norm: 0.0533, Test L1 Norm: 0.0646, Train Linf Norm: 3.2008, Test Linf Norm: 4.8256\n",
            "Epoch 41: Train Loss: 0.0527, Test Loss: 0.0118, Train L1 Norm: 0.0537, Test L1 Norm: 0.0912, Train Linf Norm: 3.0477, Test Linf Norm: 8.1691\n",
            "Epoch 42: Train Loss: 0.0491, Test Loss: 0.0569, Train L1 Norm: 0.0496, Test L1 Norm: 0.0343, Train Linf Norm: 2.7041, Test Linf Norm: 0.7475\n",
            "Epoch 43: Train Loss: 0.0462, Test Loss: 0.0373, Train L1 Norm: 0.0543, Test L1 Norm: 0.0334, Train Linf Norm: 3.3473, Test Linf Norm: 1.9343\n",
            "Epoch 44: Train Loss: 0.0449, Test Loss: 0.0184, Train L1 Norm: 0.0485, Test L1 Norm: 0.0261, Train Linf Norm: 2.8393, Test Linf Norm: 1.5887\n",
            "Epoch 45: Train Loss: 0.0474, Test Loss: 0.1102, Train L1 Norm: 0.0508, Test L1 Norm: 0.0468, Train Linf Norm: 2.9576, Test Linf Norm: 0.7395\n",
            "Epoch 46: Train Loss: 0.0427, Test Loss: 0.0506, Train L1 Norm: 0.0428, Test L1 Norm: 0.0519, Train Linf Norm: 2.3148, Test Linf Norm: 3.4272\n",
            "Epoch 47: Train Loss: 0.0493, Test Loss: 0.0586, Train L1 Norm: 0.0450, Test L1 Norm: 0.1894, Train Linf Norm: 2.3391, Test Linf Norm: 15.5420\n",
            "Epoch 48: Train Loss: 0.0394, Test Loss: 0.1043, Train L1 Norm: 0.0496, Test L1 Norm: 0.2065, Train Linf Norm: 2.7661, Test Linf Norm: 16.0245\n",
            "Epoch 49: Train Loss: 0.0459, Test Loss: 0.2684, Train L1 Norm: 0.0493, Test L1 Norm: 0.0956, Train Linf Norm: 2.8993, Test Linf Norm: 0.8446\n",
            "Epoch 50: Train Loss: 0.0426, Test Loss: 0.0762, Train L1 Norm: 0.0433, Test L1 Norm: 0.1656, Train Linf Norm: 2.3535, Test Linf Norm: 12.9054\n",
            "Epoch 51: Train Loss: 0.0421, Test Loss: 0.0260, Train L1 Norm: 0.0483, Test L1 Norm: 0.0189, Train Linf Norm: 2.9263, Test Linf Norm: 0.7212\n",
            "Epoch 52: Train Loss: 0.0500, Test Loss: 0.0234, Train L1 Norm: 0.0475, Test L1 Norm: 0.0748, Train Linf Norm: 2.5436, Test Linf Norm: 6.3472\n",
            "Epoch 53: Train Loss: 0.0439, Test Loss: 0.1507, Train L1 Norm: 0.0526, Test L1 Norm: 0.0952, Train Linf Norm: 3.2480, Test Linf Norm: 4.9986\n",
            "Epoch 54: Train Loss: 0.0442, Test Loss: 0.0338, Train L1 Norm: 0.0403, Test L1 Norm: 0.0895, Train Linf Norm: 1.9582, Test Linf Norm: 7.1718\n",
            "Epoch 55: Train Loss: 0.0426, Test Loss: 0.1104, Train L1 Norm: 0.0453, Test L1 Norm: 0.2181, Train Linf Norm: 2.5093, Test Linf Norm: 16.7187\n",
            "Epoch 56: Train Loss: 0.0397, Test Loss: 0.0115, Train L1 Norm: 0.0465, Test L1 Norm: 0.0199, Train Linf Norm: 2.7992, Test Linf Norm: 1.2332\n",
            "Epoch 57: Train Loss: 0.0331, Test Loss: 0.0231, Train L1 Norm: 0.0435, Test L1 Norm: 0.0344, Train Linf Norm: 2.7485, Test Linf Norm: 2.4355\n",
            "Epoch 58: Train Loss: 0.0341, Test Loss: 0.0254, Train L1 Norm: 0.0437, Test L1 Norm: 0.0270, Train Linf Norm: 2.7042, Test Linf Norm: 1.6076\n",
            "Epoch 59: Train Loss: 0.0404, Test Loss: 0.0390, Train L1 Norm: 0.0423, Test L1 Norm: 0.0219, Train Linf Norm: 2.3384, Test Linf Norm: 0.6375\n",
            "Epoch 60: Train Loss: 0.0383, Test Loss: 0.0187, Train L1 Norm: 0.0506, Test L1 Norm: 0.0334, Train Linf Norm: 3.2424, Test Linf Norm: 2.4147\n",
            "Epoch 61: Train Loss: 0.0363, Test Loss: 0.0368, Train L1 Norm: 0.0404, Test L1 Norm: 0.0618, Train Linf Norm: 2.3611, Test Linf Norm: 4.7479\n",
            "Epoch 62: Train Loss: 0.0381, Test Loss: 0.0048, Train L1 Norm: 0.0385, Test L1 Norm: 0.0238, Train Linf Norm: 2.0356, Test Linf Norm: 1.8119\n",
            "Epoch 63: Train Loss: 0.0377, Test Loss: 0.0076, Train L1 Norm: 0.0385, Test L1 Norm: 0.0135, Train Linf Norm: 2.0518, Test Linf Norm: 0.5902\n",
            "Epoch 64: Train Loss: 0.0375, Test Loss: 0.0960, Train L1 Norm: 0.0410, Test L1 Norm: 0.0496, Train Linf Norm: 2.2292, Test Linf Norm: 0.7433\n",
            "Epoch 65: Train Loss: 0.0318, Test Loss: 0.0646, Train L1 Norm: 0.0399, Test L1 Norm: 0.0327, Train Linf Norm: 2.4383, Test Linf Norm: 0.6704\n",
            "Epoch 66: Train Loss: 0.0353, Test Loss: 0.0107, Train L1 Norm: 0.0347, Test L1 Norm: 0.0225, Train Linf Norm: 1.8027, Test Linf Norm: 1.5760\n",
            "Epoch 67: Train Loss: 0.0356, Test Loss: 0.0376, Train L1 Norm: 0.0370, Test L1 Norm: 0.0266, Train Linf Norm: 1.9411, Test Linf Norm: 0.6245\n",
            "Epoch 68: Train Loss: 0.0336, Test Loss: 0.0081, Train L1 Norm: 0.0386, Test L1 Norm: 0.0146, Train Linf Norm: 2.2521, Test Linf Norm: 0.6662\n",
            "Epoch 69: Train Loss: 0.0371, Test Loss: 0.0286, Train L1 Norm: 0.0373, Test L1 Norm: 0.0416, Train Linf Norm: 2.0208, Test Linf Norm: 2.8543\n",
            "Epoch 70: Train Loss: 0.0323, Test Loss: 0.0397, Train L1 Norm: 0.0336, Test L1 Norm: 0.0314, Train Linf Norm: 1.7933, Test Linf Norm: 1.5633\n",
            "Epoch 71: Train Loss: 0.0356, Test Loss: 0.0488, Train L1 Norm: 0.0382, Test L1 Norm: 0.0285, Train Linf Norm: 2.1368, Test Linf Norm: 0.6224\n",
            "Epoch 72: Train Loss: 0.0314, Test Loss: 0.0145, Train L1 Norm: 0.0367, Test L1 Norm: 0.0561, Train Linf Norm: 2.0900, Test Linf Norm: 4.7414\n",
            "Epoch 73: Train Loss: 0.0325, Test Loss: 0.0077, Train L1 Norm: 0.0388, Test L1 Norm: 0.0587, Train Linf Norm: 2.3329, Test Linf Norm: 4.9955\n",
            "Epoch 74: Train Loss: 0.0321, Test Loss: 0.0501, Train L1 Norm: 0.0389, Test L1 Norm: 0.1126, Train Linf Norm: 2.3360, Test Linf Norm: 8.7045\n",
            "Epoch 75: Train Loss: 0.0312, Test Loss: 0.0920, Train L1 Norm: 0.0344, Test L1 Norm: 0.0427, Train Linf Norm: 1.9415, Test Linf Norm: 0.6584\n",
            "Epoch 76: Train Loss: 0.0362, Test Loss: 0.0088, Train L1 Norm: 0.0396, Test L1 Norm: 0.0321, Train Linf Norm: 2.2525, Test Linf Norm: 2.4546\n",
            "Epoch 77: Train Loss: 0.0311, Test Loss: 0.0629, Train L1 Norm: 0.0382, Test L1 Norm: 0.1199, Train Linf Norm: 2.3194, Test Linf Norm: 9.5345\n",
            "Epoch 78: Train Loss: 0.0333, Test Loss: 0.0279, Train L1 Norm: 0.0398, Test L1 Norm: 0.0207, Train Linf Norm: 2.3745, Test Linf Norm: 0.8763\n",
            "Epoch 79: Train Loss: 0.0295, Test Loss: 0.0106, Train L1 Norm: 0.0345, Test L1 Norm: 0.0292, Train Linf Norm: 1.9649, Test Linf Norm: 2.2676\n",
            "Epoch 80: Train Loss: 0.0273, Test Loss: 0.0059, Train L1 Norm: 0.0341, Test L1 Norm: 0.0617, Train Linf Norm: 1.9920, Test Linf Norm: 5.4244\n",
            "Epoch 81: Train Loss: 0.0268, Test Loss: 0.0101, Train L1 Norm: 0.0327, Test L1 Norm: 0.0211, Train Linf Norm: 1.9027, Test Linf Norm: 1.3877\n",
            "Epoch 82: Train Loss: 0.0309, Test Loss: 0.0265, Train L1 Norm: 0.0371, Test L1 Norm: 0.0518, Train Linf Norm: 2.2158, Test Linf Norm: 4.2316\n",
            "Epoch 83: Train Loss: 0.0285, Test Loss: 0.0191, Train L1 Norm: 0.0411, Test L1 Norm: 0.0374, Train Linf Norm: 2.6841, Test Linf Norm: 2.9488\n",
            "Epoch 84: Train Loss: 0.0286, Test Loss: 0.0400, Train L1 Norm: 0.0324, Test L1 Norm: 0.0949, Train Linf Norm: 1.8397, Test Linf Norm: 7.6246\n",
            "Epoch 85: Train Loss: 0.0248, Test Loss: 0.0244, Train L1 Norm: 0.0279, Test L1 Norm: 0.0178, Train Linf Norm: 1.5326, Test Linf Norm: 0.8162\n",
            "Epoch 86: Train Loss: 0.0326, Test Loss: 0.0183, Train L1 Norm: 0.0378, Test L1 Norm: 0.0191, Train Linf Norm: 2.2489, Test Linf Norm: 0.5718\n",
            "Epoch 87: Train Loss: 0.0268, Test Loss: 0.0223, Train L1 Norm: 0.0327, Test L1 Norm: 0.0553, Train Linf Norm: 1.9046, Test Linf Norm: 4.5572\n",
            "Epoch 88: Train Loss: 0.0269, Test Loss: 0.0190, Train L1 Norm: 0.0349, Test L1 Norm: 0.0200, Train Linf Norm: 2.1244, Test Linf Norm: 0.5637\n",
            "Epoch 89: Train Loss: 0.0275, Test Loss: 0.0284, Train L1 Norm: 0.0337, Test L1 Norm: 0.0253, Train Linf Norm: 1.9911, Test Linf Norm: 1.3801\n",
            "Epoch 90: Train Loss: 0.0265, Test Loss: 0.0234, Train L1 Norm: 0.0358, Test L1 Norm: 0.0181, Train Linf Norm: 2.1343, Test Linf Norm: 0.7692\n",
            "Epoch 91: Train Loss: 0.0263, Test Loss: 0.0088, Train L1 Norm: 0.0310, Test L1 Norm: 0.0786, Train Linf Norm: 1.7598, Test Linf Norm: 6.8367\n",
            "Epoch 92: Train Loss: 0.0275, Test Loss: 0.0646, Train L1 Norm: 0.0362, Test L1 Norm: 0.0725, Train Linf Norm: 2.2712, Test Linf Norm: 4.8574\n",
            "Epoch 93: Train Loss: 0.0271, Test Loss: 0.0107, Train L1 Norm: 0.0394, Test L1 Norm: 0.0243, Train Linf Norm: 2.5224, Test Linf Norm: 1.7538\n",
            "Epoch 94: Train Loss: 0.0240, Test Loss: 0.0395, Train L1 Norm: 0.0313, Test L1 Norm: 0.0610, Train Linf Norm: 1.8735, Test Linf Norm: 4.7011\n",
            "Epoch 95: Train Loss: 0.0229, Test Loss: 0.0210, Train L1 Norm: 0.0259, Test L1 Norm: 0.0209, Train Linf Norm: 1.4096, Test Linf Norm: 1.1462\n",
            "Epoch 96: Train Loss: 0.0251, Test Loss: 0.0685, Train L1 Norm: 0.0288, Test L1 Norm: 0.0356, Train Linf Norm: 1.6115, Test Linf Norm: 1.4320\n",
            "Epoch 97: Train Loss: 0.0274, Test Loss: 0.0378, Train L1 Norm: 0.0336, Test L1 Norm: 0.0431, Train Linf Norm: 1.9878, Test Linf Norm: 2.9973\n",
            "Epoch 98: Train Loss: 0.0243, Test Loss: 0.0073, Train L1 Norm: 0.0347, Test L1 Norm: 0.0353, Train Linf Norm: 2.2211, Test Linf Norm: 2.9293\n",
            "Epoch 99: Train Loss: 0.0226, Test Loss: 0.0152, Train L1 Norm: 0.0354, Test L1 Norm: 0.0161, Train Linf Norm: 2.3209, Test Linf Norm: 0.8425\n",
            "Epoch 100: Train Loss: 0.0249, Test Loss: 0.0049, Train L1 Norm: 0.0323, Test L1 Norm: 0.0171, Train Linf Norm: 1.9215, Test Linf Norm: 1.2160\n",
            "Epoch 101: Train Loss: 0.0270, Test Loss: 0.0327, Train L1 Norm: 0.0361, Test L1 Norm: 0.0228, Train Linf Norm: 2.2556, Test Linf Norm: 0.7845\n",
            "Epoch 102: Train Loss: 0.0243, Test Loss: 0.0126, Train L1 Norm: 0.0290, Test L1 Norm: 0.0345, Train Linf Norm: 1.6072, Test Linf Norm: 2.7008\n",
            "Epoch 103: Train Loss: 0.0271, Test Loss: 0.0120, Train L1 Norm: 0.0321, Test L1 Norm: 0.0160, Train Linf Norm: 1.8193, Test Linf Norm: 0.7523\n",
            "Epoch 104: Train Loss: 0.0228, Test Loss: 0.0270, Train L1 Norm: 0.0319, Test L1 Norm: 0.0551, Train Linf Norm: 1.9918, Test Linf Norm: 4.3148\n",
            "Epoch 105: Train Loss: 0.0220, Test Loss: 0.0338, Train L1 Norm: 0.0306, Test L1 Norm: 0.0213, Train Linf Norm: 1.8853, Test Linf Norm: 0.5956\n",
            "Epoch 106: Train Loss: 0.0217, Test Loss: 0.0069, Train L1 Norm: 0.0283, Test L1 Norm: 0.0220, Train Linf Norm: 1.6647, Test Linf Norm: 1.6573\n",
            "Epoch 107: Train Loss: 0.0230, Test Loss: 0.0399, Train L1 Norm: 0.0330, Test L1 Norm: 0.0591, Train Linf Norm: 2.0957, Test Linf Norm: 4.2731\n",
            "Epoch 108: Train Loss: 0.0260, Test Loss: 0.0086, Train L1 Norm: 0.0313, Test L1 Norm: 0.0464, Train Linf Norm: 1.8068, Test Linf Norm: 4.0031\n",
            "Epoch 109: Train Loss: 0.0219, Test Loss: 0.0445, Train L1 Norm: 0.0267, Test L1 Norm: 0.0773, Train Linf Norm: 1.4818, Test Linf Norm: 5.8884\n",
            "Epoch 110: Train Loss: 0.0255, Test Loss: 0.0536, Train L1 Norm: 0.0291, Test L1 Norm: 0.0826, Train Linf Norm: 1.5112, Test Linf Norm: 5.9611\n",
            "Epoch 111: Train Loss: 0.0266, Test Loss: 0.0086, Train L1 Norm: 0.0368, Test L1 Norm: 0.0289, Train Linf Norm: 2.3405, Test Linf Norm: 2.3580\n",
            "Epoch 112: Train Loss: 0.0213, Test Loss: 0.0039, Train L1 Norm: 0.0313, Test L1 Norm: 0.0126, Train Linf Norm: 1.9629, Test Linf Norm: 0.7882\n",
            "Epoch 113: Train Loss: 0.0246, Test Loss: 0.0180, Train L1 Norm: 0.0329, Test L1 Norm: 0.0215, Train Linf Norm: 2.0060, Test Linf Norm: 1.4118\n",
            "Epoch 114: Train Loss: 0.0211, Test Loss: 0.0348, Train L1 Norm: 0.0303, Test L1 Norm: 0.0876, Train Linf Norm: 1.8926, Test Linf Norm: 7.2167\n",
            "Epoch 115: Train Loss: 0.0238, Test Loss: 0.0503, Train L1 Norm: 0.0318, Test L1 Norm: 0.0269, Train Linf Norm: 1.9324, Test Linf Norm: 0.6050\n",
            "Epoch 116: Train Loss: 0.0219, Test Loss: 0.0103, Train L1 Norm: 0.0293, Test L1 Norm: 0.0173, Train Linf Norm: 1.7440, Test Linf Norm: 1.1056\n",
            "Epoch 117: Train Loss: 0.0212, Test Loss: 0.0107, Train L1 Norm: 0.0295, Test L1 Norm: 0.0157, Train Linf Norm: 1.8066, Test Linf Norm: 0.9685\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:59:47,719]\u001b[0m Trial 26 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 118: Train Loss: 0.0208, Test Loss: 0.0172, Train L1 Norm: 0.0305, Test L1 Norm: 0.0140, Train Linf Norm: 1.9016, Test Linf Norm: 0.4649\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:59:49,451]\u001b[0m Trial 27 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 20.5584, Test Loss: 20.7300, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:59:50,815]\u001b[0m Trial 28 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.3492, Test Loss: 0.1346, Train L1 Norm: 0.7668, Test L1 Norm: 0.4239, Train Linf Norm: 103.6391, Test Linf Norm: 46.3630\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 12:59:53,703]\u001b[0m Trial 29 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0652, Test Loss: 0.0024, Train L1 Norm: 2.7392, Test L1 Norm: 1.7224, Train Linf Norm: 159.3453, Test Linf Norm: 102.4369\n",
            "Epoch 1: Train Loss: 0.2672, Test Loss: 0.0423, Train L1 Norm: 0.4049, Test L1 Norm: 0.1284, Train Linf Norm: 51.2220, Test Linf Norm: 20.1435\n",
            "Epoch 2: Train Loss: 0.0473, Test Loss: 0.0541, Train L1 Norm: 0.0665, Test L1 Norm: 0.0484, Train Linf Norm: 5.3971, Test Linf Norm: 1.2067\n",
            "Epoch 3: Train Loss: 0.0419, Test Loss: 0.0326, Train L1 Norm: 0.0539, Test L1 Norm: 0.0322, Train Linf Norm: 3.9531, Test Linf Norm: 1.1317\n",
            "Epoch 4: Train Loss: 0.0392, Test Loss: 0.0503, Train L1 Norm: 0.0476, Test L1 Norm: 0.0438, Train Linf Norm: 3.1348, Test Linf Norm: 1.0735\n",
            "Epoch 5: Train Loss: 0.0389, Test Loss: 0.0191, Train L1 Norm: 0.0472, Test L1 Norm: 0.0822, Train Linf Norm: 3.3406, Test Linf Norm: 13.5577\n",
            "Epoch 6: Train Loss: 0.0349, Test Loss: 0.0249, Train L1 Norm: 0.0458, Test L1 Norm: 0.0816, Train Linf Norm: 3.5120, Test Linf Norm: 13.2192\n",
            "Epoch 7: Train Loss: 0.0300, Test Loss: 0.0621, Train L1 Norm: 0.0377, Test L1 Norm: 0.2015, Train Linf Norm: 2.4285, Test Linf Norm: 34.3893\n",
            "Epoch 8: Train Loss: 0.0237, Test Loss: 0.0258, Train L1 Norm: 0.0347, Test L1 Norm: 0.0307, Train Linf Norm: 2.6511, Test Linf Norm: 0.9115\n",
            "Epoch 9: Train Loss: 0.0174, Test Loss: 0.0131, Train L1 Norm: 0.0261, Test L1 Norm: 0.0554, Train Linf Norm: 1.3943, Test Linf Norm: 8.7052\n",
            "Epoch 10: Train Loss: 0.0136, Test Loss: 0.0080, Train L1 Norm: 0.0265, Test L1 Norm: 0.0562, Train Linf Norm: 1.9555, Test Linf Norm: 9.0497\n",
            "Epoch 11: Train Loss: 0.0108, Test Loss: 0.0076, Train L1 Norm: 0.0234, Test L1 Norm: 0.0339, Train Linf Norm: 1.5949, Test Linf Norm: 4.4281\n",
            "Epoch 12: Train Loss: 0.0084, Test Loss: 0.0069, Train L1 Norm: 0.0229, Test L1 Norm: 0.0372, Train Linf Norm: 1.7107, Test Linf Norm: 5.1620\n",
            "Epoch 13: Train Loss: 0.0074, Test Loss: 0.0102, Train L1 Norm: 0.0215, Test L1 Norm: 0.0487, Train Linf Norm: 1.5498, Test Linf Norm: 7.5667\n",
            "Epoch 14: Train Loss: 0.0067, Test Loss: 0.0066, Train L1 Norm: 0.0219, Test L1 Norm: 0.0340, Train Linf Norm: 1.6886, Test Linf Norm: 4.5734\n",
            "Epoch 15: Train Loss: 0.0064, Test Loss: 0.0063, Train L1 Norm: 0.0211, Test L1 Norm: 0.0371, Train Linf Norm: 1.5655, Test Linf Norm: 5.2560\n",
            "Epoch 16: Train Loss: 0.0063, Test Loss: 0.0063, Train L1 Norm: 0.0214, Test L1 Norm: 0.0371, Train Linf Norm: 1.6351, Test Linf Norm: 5.2560\n",
            "Epoch 17: Train Loss: 0.0063, Test Loss: 0.0062, Train L1 Norm: 0.0213, Test L1 Norm: 0.0329, Train Linf Norm: 1.6087, Test Linf Norm: 4.3407\n",
            "Epoch 18: Train Loss: 0.0065, Test Loss: 0.0062, Train L1 Norm: 0.0210, Test L1 Norm: 0.0302, Train Linf Norm: 1.4926, Test Linf Norm: 3.7586\n",
            "Epoch 19: Train Loss: 0.0070, Test Loss: 0.0065, Train L1 Norm: 0.0207, Test L1 Norm: 0.0345, Train Linf Norm: 1.4517, Test Linf Norm: 4.7161\n",
            "Epoch 20: Train Loss: 0.0077, Test Loss: 0.0059, Train L1 Norm: 0.0209, Test L1 Norm: 0.0395, Train Linf Norm: 1.4682, Test Linf Norm: 5.8850\n",
            "Epoch 21: Train Loss: 0.0111, Test Loss: 0.0150, Train L1 Norm: 0.0224, Test L1 Norm: 0.0207, Train Linf Norm: 1.5122, Test Linf Norm: 0.8378\n",
            "Epoch 22: Train Loss: 0.0152, Test Loss: 0.0060, Train L1 Norm: 0.0256, Test L1 Norm: 0.0325, Train Linf Norm: 1.8629, Test Linf Norm: 4.3135\n",
            "Epoch 23: Train Loss: 0.0208, Test Loss: 0.0306, Train L1 Norm: 0.0301, Test L1 Norm: 0.0929, Train Linf Norm: 2.2455, Test Linf Norm: 15.5453\n",
            "Epoch 24: Train Loss: 0.0273, Test Loss: 0.0107, Train L1 Norm: 0.0317, Test L1 Norm: 0.0266, Train Linf Norm: 1.9854, Test Linf Norm: 2.9115\n",
            "Epoch 25: Train Loss: 0.0286, Test Loss: 0.0140, Train L1 Norm: 0.0351, Test L1 Norm: 0.0516, Train Linf Norm: 2.6424, Test Linf Norm: 8.1687\n",
            "Epoch 26: Train Loss: 0.0320, Test Loss: 0.0174, Train L1 Norm: 0.0336, Test L1 Norm: 0.0183, Train Linf Norm: 1.9584, Test Linf Norm: 0.7813\n",
            "Epoch 27: Train Loss: 0.0400, Test Loss: 0.0257, Train L1 Norm: 0.0488, Test L1 Norm: 0.0233, Train Linf Norm: 4.3920, Test Linf Norm: 0.7975\n",
            "Epoch 28: Train Loss: 0.0365, Test Loss: 0.0448, Train L1 Norm: 0.0450, Test L1 Norm: 0.0319, Train Linf Norm: 4.0573, Test Linf Norm: 0.8300\n",
            "Epoch 29: Train Loss: 0.0459, Test Loss: 0.1039, Train L1 Norm: 0.0383, Test L1 Norm: 0.0621, Train Linf Norm: 1.7452, Test Linf Norm: 0.9514\n",
            "Epoch 30: Train Loss: 0.0493, Test Loss: 0.0659, Train L1 Norm: 0.0438, Test L1 Norm: 0.0390, Train Linf Norm: 2.4416, Test Linf Norm: 0.8376\n",
            "Epoch 31: Train Loss: 0.0556, Test Loss: 0.0116, Train L1 Norm: 0.0532, Test L1 Norm: 0.0211, Train Linf Norm: 3.8244, Test Linf Norm: 0.8110\n",
            "Epoch 32: Train Loss: 0.0466, Test Loss: 0.1159, Train L1 Norm: 0.0409, Test L1 Norm: 0.0646, Train Linf Norm: 2.3080, Test Linf Norm: 0.9539\n",
            "Epoch 33: Train Loss: 0.0414, Test Loss: 0.0134, Train L1 Norm: 0.0416, Test L1 Norm: 0.0153, Train Linf Norm: 3.0771, Test Linf Norm: 0.7154\n",
            "Epoch 34: Train Loss: 0.0376, Test Loss: 0.0103, Train L1 Norm: 0.0432, Test L1 Norm: 0.0174, Train Linf Norm: 3.7817, Test Linf Norm: 0.7304\n",
            "Epoch 35: Train Loss: 0.0360, Test Loss: 0.0200, Train L1 Norm: 0.0324, Test L1 Norm: 0.0595, Train Linf Norm: 1.7352, Test Linf Norm: 9.5741\n",
            "Epoch 36: Train Loss: 0.0367, Test Loss: 0.0177, Train L1 Norm: 0.0474, Test L1 Norm: 0.0167, Train Linf Norm: 4.9193, Test Linf Norm: 0.7205\n",
            "Epoch 37: Train Loss: 0.0273, Test Loss: 0.0281, Train L1 Norm: 0.0301, Test L1 Norm: 0.0277, Train Linf Norm: 2.1984, Test Linf Norm: 0.8001\n",
            "Epoch 38: Train Loss: 0.0242, Test Loss: 0.0071, Train L1 Norm: 0.0243, Test L1 Norm: 0.0239, Train Linf Norm: 1.3621, Test Linf Norm: 3.0626\n",
            "Epoch 39: Train Loss: 0.0187, Test Loss: 0.0098, Train L1 Norm: 0.0216, Test L1 Norm: 0.0131, Train Linf Norm: 1.3975, Test Linf Norm: 0.6941\n",
            "Epoch 40: Train Loss: 0.0126, Test Loss: 0.0260, Train L1 Norm: 0.0206, Test L1 Norm: 0.0603, Train Linf Norm: 1.8082, Test Linf Norm: 9.4040\n",
            "Epoch 41: Train Loss: 0.0087, Test Loss: 0.0066, Train L1 Norm: 0.0174, Test L1 Norm: 0.0198, Train Linf Norm: 1.4678, Test Linf Norm: 2.3613\n",
            "Epoch 42: Train Loss: 0.0068, Test Loss: 0.0080, Train L1 Norm: 0.0157, Test L1 Norm: 0.0132, Train Linf Norm: 1.3009, Test Linf Norm: 0.6971\n",
            "Epoch 43: Train Loss: 0.0055, Test Loss: 0.0047, Train L1 Norm: 0.0145, Test L1 Norm: 0.0148, Train Linf Norm: 1.1542, Test Linf Norm: 1.3999\n",
            "Epoch 44: Train Loss: 0.0049, Test Loss: 0.0049, Train L1 Norm: 0.0149, Test L1 Norm: 0.0139, Train Linf Norm: 1.3154, Test Linf Norm: 1.1971\n",
            "Epoch 45: Train Loss: 0.0046, Test Loss: 0.0046, Train L1 Norm: 0.0144, Test L1 Norm: 0.0115, Train Linf Norm: 1.2122, Test Linf Norm: 0.6801\n",
            "Epoch 46: Train Loss: 0.0046, Test Loss: 0.0046, Train L1 Norm: 0.0143, Test L1 Norm: 0.0115, Train Linf Norm: 1.1806, Test Linf Norm: 0.6801\n",
            "Epoch 47: Train Loss: 0.0046, Test Loss: 0.0046, Train L1 Norm: 0.0143, Test L1 Norm: 0.0115, Train Linf Norm: 1.1926, Test Linf Norm: 0.6807\n",
            "Epoch 48: Train Loss: 0.0048, Test Loss: 0.0046, Train L1 Norm: 0.0145, Test L1 Norm: 0.0135, Train Linf Norm: 1.2245, Test Linf Norm: 1.1351\n",
            "Epoch 49: Train Loss: 0.0053, Test Loss: 0.0055, Train L1 Norm: 0.0145, Test L1 Norm: 0.0119, Train Linf Norm: 1.2242, Test Linf Norm: 0.6780\n",
            "Epoch 50: Train Loss: 0.0068, Test Loss: 0.0076, Train L1 Norm: 0.0151, Test L1 Norm: 0.0166, Train Linf Norm: 1.2243, Test Linf Norm: 1.7016\n",
            "Epoch 51: Train Loss: 0.0098, Test Loss: 0.0206, Train L1 Norm: 0.0163, Test L1 Norm: 0.0196, Train Linf Norm: 1.2228, Test Linf Norm: 0.7164\n",
            "Epoch 52: Train Loss: 0.0146, Test Loss: 0.0265, Train L1 Norm: 0.0207, Test L1 Norm: 0.0197, Train Linf Norm: 1.7028, Test Linf Norm: 0.7220\n",
            "Epoch 53: Train Loss: 0.0207, Test Loss: 0.0262, Train L1 Norm: 0.0236, Test L1 Norm: 0.0563, Train Linf Norm: 1.7797, Test Linf Norm: 8.9216\n",
            "Epoch 54: Train Loss: 0.0255, Test Loss: 0.0239, Train L1 Norm: 0.0245, Test L1 Norm: 0.0471, Train Linf Norm: 1.4688, Test Linf Norm: 7.2407\n",
            "Epoch 55: Train Loss: 0.0317, Test Loss: 0.0419, Train L1 Norm: 0.0297, Test L1 Norm: 0.0308, Train Linf Norm: 1.9029, Test Linf Norm: 0.7856\n",
            "Epoch 56: Train Loss: 0.0322, Test Loss: 0.0060, Train L1 Norm: 0.0323, Test L1 Norm: 0.0126, Train Linf Norm: 2.3279, Test Linf Norm: 0.6893\n",
            "Epoch 57: Train Loss: 0.0365, Test Loss: 0.0337, Train L1 Norm: 0.0315, Test L1 Norm: 0.0523, Train Linf Norm: 1.7691, Test Linf Norm: 7.4992\n",
            "Epoch 58: Train Loss: 0.0430, Test Loss: 0.1010, Train L1 Norm: 0.0399, Test L1 Norm: 0.1922, Train Linf Norm: 2.8383, Test Linf Norm: 31.0353\n",
            "Epoch 59: Train Loss: 0.0436, Test Loss: 0.0131, Train L1 Norm: 0.0429, Test L1 Norm: 0.0241, Train Linf Norm: 3.3851, Test Linf Norm: 2.7997\n",
            "Epoch 60: Train Loss: 0.0399, Test Loss: 0.0427, Train L1 Norm: 0.0420, Test L1 Norm: 0.0216, Train Linf Norm: 3.5507, Test Linf Norm: 0.7366\n",
            "Epoch 61: Train Loss: 0.0444, Test Loss: 0.0621, Train L1 Norm: 0.0372, Test L1 Norm: 0.0883, Train Linf Norm: 2.1048, Test Linf Norm: 13.2704\n",
            "Epoch 62: Train Loss: 0.0398, Test Loss: 0.0594, Train L1 Norm: 0.0375, Test L1 Norm: 0.1026, Train Linf Norm: 2.5937, Test Linf Norm: 16.0666\n",
            "Epoch 63: Train Loss: 0.0396, Test Loss: 0.0736, Train L1 Norm: 0.0345, Test L1 Norm: 0.1145, Train Linf Norm: 2.1843, Test Linf Norm: 17.8506\n",
            "Epoch 64: Train Loss: 0.0417, Test Loss: 0.0426, Train L1 Norm: 0.0363, Test L1 Norm: 0.0303, Train Linf Norm: 2.3791, Test Linf Norm: 0.7919\n",
            "Epoch 65: Train Loss: 0.0320, Test Loss: 0.0362, Train L1 Norm: 0.0329, Test L1 Norm: 0.0913, Train Linf Norm: 2.6196, Test Linf Norm: 14.3629\n",
            "Epoch 66: Train Loss: 0.0293, Test Loss: 0.0206, Train L1 Norm: 0.0312, Test L1 Norm: 0.0171, Train Linf Norm: 2.6155, Test Linf Norm: 0.9435\n",
            "Epoch 67: Train Loss: 0.0236, Test Loss: 0.0239, Train L1 Norm: 0.0245, Test L1 Norm: 0.0233, Train Linf Norm: 1.7066, Test Linf Norm: 0.7481\n",
            "Epoch 68: Train Loss: 0.0203, Test Loss: 0.0253, Train L1 Norm: 0.0215, Test L1 Norm: 0.0512, Train Linf Norm: 1.4612, Test Linf Norm: 7.7697\n",
            "Epoch 69: Train Loss: 0.0138, Test Loss: 0.0105, Train L1 Norm: 0.0168, Test L1 Norm: 0.0121, Train Linf Norm: 1.1722, Test Linf Norm: 0.6318\n",
            "Epoch 70: Train Loss: 0.0114, Test Loss: 0.0089, Train L1 Norm: 0.0165, Test L1 Norm: 0.0121, Train Linf Norm: 1.3488, Test Linf Norm: 0.6470\n",
            "Epoch 71: Train Loss: 0.0075, Test Loss: 0.0145, Train L1 Norm: 0.0132, Test L1 Norm: 0.0243, Train Linf Norm: 1.0260, Test Linf Norm: 3.0392\n",
            "Epoch 72: Train Loss: 0.0060, Test Loss: 0.0069, Train L1 Norm: 0.0132, Test L1 Norm: 0.0103, Train Linf Norm: 1.1573, Test Linf Norm: 0.6464\n",
            "Epoch 73: Train Loss: 0.0046, Test Loss: 0.0049, Train L1 Norm: 0.0117, Test L1 Norm: 0.0104, Train Linf Norm: 0.9290, Test Linf Norm: 0.6319\n",
            "Epoch 74: Train Loss: 0.0041, Test Loss: 0.0038, Train L1 Norm: 0.0115, Test L1 Norm: 0.0096, Train Linf Norm: 0.9229, Test Linf Norm: 0.6193\n",
            "Epoch 75: Train Loss: 0.0039, Test Loss: 0.0039, Train L1 Norm: 0.0115, Test L1 Norm: 0.0096, Train Linf Norm: 0.9580, Test Linf Norm: 0.6278\n",
            "Epoch 76: Train Loss: 0.0039, Test Loss: 0.0039, Train L1 Norm: 0.0114, Test L1 Norm: 0.0096, Train Linf Norm: 0.9414, Test Linf Norm: 0.6278\n",
            "Epoch 77: Train Loss: 0.0038, Test Loss: 0.0039, Train L1 Norm: 0.0114, Test L1 Norm: 0.0097, Train Linf Norm: 0.9467, Test Linf Norm: 0.6216\n",
            "Epoch 78: Train Loss: 0.0040, Test Loss: 0.0038, Train L1 Norm: 0.0117, Test L1 Norm: 0.0098, Train Linf Norm: 1.0167, Test Linf Norm: 0.6243\n",
            "Epoch 79: Train Loss: 0.0044, Test Loss: 0.0038, Train L1 Norm: 0.0114, Test L1 Norm: 0.0096, Train Linf Norm: 0.9277, Test Linf Norm: 0.6221\n",
            "Epoch 80: Train Loss: 0.0065, Test Loss: 0.0129, Train L1 Norm: 0.0122, Test L1 Norm: 0.0140, Train Linf Norm: 0.9358, Test Linf Norm: 0.6699\n",
            "Epoch 81: Train Loss: 0.0091, Test Loss: 0.0057, Train L1 Norm: 0.0144, Test L1 Norm: 0.0107, Train Linf Norm: 1.2096, Test Linf Norm: 0.6215\n",
            "Epoch 82: Train Loss: 0.0127, Test Loss: 0.0053, Train L1 Norm: 0.0147, Test L1 Norm: 0.0099, Train Linf Norm: 0.9398, Test Linf Norm: 0.6183\n",
            "Epoch 83: Train Loss: 0.0178, Test Loss: 0.0281, Train L1 Norm: 0.0229, Test L1 Norm: 0.0578, Train Linf Norm: 1.9123, Test Linf Norm: 9.1827\n",
            "Epoch 84: Train Loss: 0.0200, Test Loss: 0.0090, Train L1 Norm: 0.0234, Test L1 Norm: 0.0112, Train Linf Norm: 1.9795, Test Linf Norm: 0.6260\n",
            "Epoch 85: Train Loss: 0.0251, Test Loss: 0.0093, Train L1 Norm: 0.0245, Test L1 Norm: 0.0119, Train Linf Norm: 1.7224, Test Linf Norm: 0.7486\n",
            "Epoch 86: Train Loss: 0.0299, Test Loss: 0.0052, Train L1 Norm: 0.0288, Test L1 Norm: 0.0123, Train Linf Norm: 2.1337, Test Linf Norm: 1.1837\n",
            "Epoch 87: Train Loss: 0.0311, Test Loss: 0.0179, Train L1 Norm: 0.0284, Test L1 Norm: 0.0418, Train Linf Norm: 1.9625, Test Linf Norm: 6.3788\n",
            "Epoch 88: Train Loss: 0.0346, Test Loss: 0.0173, Train L1 Norm: 0.0311, Test L1 Norm: 0.0169, Train Linf Norm: 2.0959, Test Linf Norm: 1.3839\n",
            "Epoch 89: Train Loss: 0.0383, Test Loss: 0.0783, Train L1 Norm: 0.0342, Test L1 Norm: 0.1801, Train Linf Norm: 2.3877, Test Linf Norm: 28.9191\n",
            "Epoch 90: Train Loss: 0.0372, Test Loss: 0.0245, Train L1 Norm: 0.0329, Test L1 Norm: 0.0218, Train Linf Norm: 2.2499, Test Linf Norm: 0.7773\n",
            "Epoch 91: Train Loss: 0.0384, Test Loss: 0.0130, Train L1 Norm: 0.0399, Test L1 Norm: 0.0178, Train Linf Norm: 3.6220, Test Linf Norm: 1.0309\n",
            "Epoch 92: Train Loss: 0.0392, Test Loss: 0.0612, Train L1 Norm: 0.0399, Test L1 Norm: 0.0365, Train Linf Norm: 3.5308, Test Linf Norm: 0.8393\n",
            "Epoch 93: Train Loss: 0.0466, Test Loss: 0.0150, Train L1 Norm: 0.0509, Test L1 Norm: 0.0176, Train Linf Norm: 5.1625, Test Linf Norm: 0.7243\n",
            "Epoch 94: Train Loss: 0.0352, Test Loss: 0.0400, Train L1 Norm: 0.0304, Test L1 Norm: 0.0456, Train Linf Norm: 1.8959, Test Linf Norm: 5.8549\n",
            "Epoch 95: Train Loss: 0.0305, Test Loss: 0.0246, Train L1 Norm: 0.0334, Test L1 Norm: 0.0189, Train Linf Norm: 3.1014, Test Linf Norm: 0.8031\n",
            "Epoch 96: Train Loss: 0.0309, Test Loss: 0.0363, Train L1 Norm: 0.0306, Test L1 Norm: 0.0275, Train Linf Norm: 2.4674, Test Linf Norm: 0.7864\n",
            "Epoch 97: Train Loss: 0.0281, Test Loss: 0.0379, Train L1 Norm: 0.0274, Test L1 Norm: 0.0235, Train Linf Norm: 2.1051, Test Linf Norm: 0.7246\n",
            "Epoch 98: Train Loss: 0.0216, Test Loss: 0.0335, Train L1 Norm: 0.0231, Test L1 Norm: 0.0500, Train Linf Norm: 1.8706, Test Linf Norm: 6.9272\n",
            "Epoch 99: Train Loss: 0.0145, Test Loss: 0.0179, Train L1 Norm: 0.0196, Test L1 Norm: 0.0159, Train Linf Norm: 1.8463, Test Linf Norm: 1.3985\n",
            "Epoch 100: Train Loss: 0.0106, Test Loss: 0.0206, Train L1 Norm: 0.0146, Test L1 Norm: 0.0302, Train Linf Norm: 1.2092, Test Linf Norm: 3.8773\n",
            "Epoch 101: Train Loss: 0.0080, Test Loss: 0.0135, Train L1 Norm: 0.0140, Test L1 Norm: 0.0138, Train Linf Norm: 1.3073, Test Linf Norm: 0.6433\n",
            "Epoch 102: Train Loss: 0.0052, Test Loss: 0.0052, Train L1 Norm: 0.0117, Test L1 Norm: 0.0090, Train Linf Norm: 1.0771, Test Linf Norm: 0.5697\n",
            "Epoch 103: Train Loss: 0.0043, Test Loss: 0.0038, Train L1 Norm: 0.0108, Test L1 Norm: 0.0084, Train Linf Norm: 0.9819, Test Linf Norm: 0.5919\n",
            "Epoch 104: Train Loss: 0.0038, Test Loss: 0.0040, Train L1 Norm: 0.0105, Test L1 Norm: 0.0082, Train Linf Norm: 0.9454, Test Linf Norm: 0.5597\n",
            "Epoch 105: Train Loss: 0.0036, Test Loss: 0.0039, Train L1 Norm: 0.0101, Test L1 Norm: 0.0085, Train Linf Norm: 0.8975, Test Linf Norm: 0.5713\n",
            "Epoch 106: Train Loss: 0.0039, Test Loss: 0.0039, Train L1 Norm: 0.0104, Test L1 Norm: 0.0085, Train Linf Norm: 0.9106, Test Linf Norm: 0.5713\n",
            "Epoch 107: Train Loss: 0.0035, Test Loss: 0.0035, Train L1 Norm: 0.0103, Test L1 Norm: 0.0084, Train Linf Norm: 0.9196, Test Linf Norm: 0.5736\n",
            "Epoch 108: Train Loss: 0.0037, Test Loss: 0.0042, Train L1 Norm: 0.0101, Test L1 Norm: 0.0087, Train Linf Norm: 0.8780, Test Linf Norm: 0.5722\n",
            "Epoch 109: Train Loss: 0.0043, Test Loss: 0.0037, Train L1 Norm: 0.0105, Test L1 Norm: 0.0082, Train Linf Norm: 0.8891, Test Linf Norm: 0.5617\n",
            "Epoch 110: Train Loss: 0.0055, Test Loss: 0.0040, Train L1 Norm: 0.0109, Test L1 Norm: 0.0080, Train Linf Norm: 0.9176, Test Linf Norm: 0.5311\n",
            "Epoch 111: Train Loss: 0.0083, Test Loss: 0.0058, Train L1 Norm: 0.0119, Test L1 Norm: 0.0085, Train Linf Norm: 0.9001, Test Linf Norm: 0.5417\n",
            "Epoch 112: Train Loss: 0.0126, Test Loss: 0.0111, Train L1 Norm: 0.0144, Test L1 Norm: 0.0137, Train Linf Norm: 1.0289, Test Linf Norm: 0.6352\n",
            "Epoch 113: Train Loss: 0.0144, Test Loss: 0.0065, Train L1 Norm: 0.0169, Test L1 Norm: 0.0090, Train Linf Norm: 1.3755, Test Linf Norm: 0.6104\n",
            "Epoch 114: Train Loss: 0.0224, Test Loss: 0.0117, Train L1 Norm: 0.0192, Test L1 Norm: 0.0116, Train Linf Norm: 0.9874, Test Linf Norm: 0.8425\n",
            "Epoch 115: Train Loss: 0.0245, Test Loss: 0.0717, Train L1 Norm: 0.0260, Test L1 Norm: 0.0413, Train Linf Norm: 2.2501, Test Linf Norm: 0.8075\n",
            "Epoch 116: Train Loss: 0.0303, Test Loss: 0.0382, Train L1 Norm: 0.0278, Test L1 Norm: 0.0409, Train Linf Norm: 1.9976, Test Linf Norm: 5.4602\n",
            "Epoch 117: Train Loss: 0.0295, Test Loss: 0.0177, Train L1 Norm: 0.0273, Test L1 Norm: 0.0381, Train Linf Norm: 1.9587, Test Linf Norm: 5.4293\n",
            "Epoch 118: Train Loss: 0.0320, Test Loss: 0.0374, Train L1 Norm: 0.0298, Test L1 Norm: 0.0546, Train Linf Norm: 2.2431, Test Linf Norm: 8.0416\n",
            "Epoch 119: Train Loss: 0.0348, Test Loss: 0.0073, Train L1 Norm: 0.0317, Test L1 Norm: 0.0099, Train Linf Norm: 2.3328, Test Linf Norm: 0.5583\n",
            "Epoch 120: Train Loss: 0.0424, Test Loss: 0.1170, Train L1 Norm: 0.0386, Test L1 Norm: 0.2739, Train Linf Norm: 2.9700, Test Linf Norm: 45.0878\n",
            "Epoch 121: Train Loss: 0.0369, Test Loss: 0.0081, Train L1 Norm: 0.0386, Test L1 Norm: 0.0102, Train Linf Norm: 3.5939, Test Linf Norm: 0.5672\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:02:35,916]\u001b[0m Trial 30 finished with value: 0.08595354712754488 and parameters: {'n_layers': 1, 'n_units_0': 609, 'hidden_activation': 'LeakyReLU', 'output_activation': 'ReLU', 'loss': 'MAE', 'optimizer': 'Adam', 'lr': 0.0002936149259065989, 'batch_size': 218, 'n_epochs': 122, 'scheduler': 'CosineAnnealingLR', 'weight_decay': 0.0042495089515245076, 'beta1': 0.7348495411544427, 'beta2': 0.9662193293025739, 'T_max': 15}. Best is trial 10 with value: 0.006550250181416049.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 122: Train Loss: 0.0379, Test Loss: 0.0380, Train L1 Norm: 0.0416, Test L1 Norm: 0.0860, Train Linf Norm: 4.0939, Test Linf Norm: 13.3441\n",
            "Epoch 1: Train Loss: 0.5156, Test Loss: 0.1117, Train L1 Norm: 0.5472, Test L1 Norm: 0.0873, Train Linf Norm: 38.2467, Test Linf Norm: 1.1911\n",
            "Epoch 2: Train Loss: 0.0853, Test Loss: 0.0598, Train L1 Norm: 0.0965, Test L1 Norm: 0.0698, Train Linf Norm: 3.0233, Test Linf Norm: 1.2144\n",
            "Epoch 3: Train Loss: 0.0677, Test Loss: 0.0580, Train L1 Norm: 0.0892, Test L1 Norm: 0.0713, Train Linf Norm: 2.6967, Test Linf Norm: 1.4561\n",
            "Epoch 4: Train Loss: 0.0632, Test Loss: 0.0516, Train L1 Norm: 0.0903, Test L1 Norm: 0.0649, Train Linf Norm: 3.6492, Test Linf Norm: 1.3418\n",
            "Epoch 5: Train Loss: 0.0588, Test Loss: 0.0700, Train L1 Norm: 0.0752, Test L1 Norm: 0.0782, Train Linf Norm: 2.2884, Test Linf Norm: 1.0127\n",
            "Epoch 6: Train Loss: 0.0586, Test Loss: 0.0520, Train L1 Norm: 0.0702, Test L1 Norm: 0.0573, Train Linf Norm: 2.0280, Test Linf Norm: 1.9846\n",
            "Epoch 7: Train Loss: 0.0549, Test Loss: 0.0540, Train L1 Norm: 0.0688, Test L1 Norm: 0.0516, Train Linf Norm: 2.2452, Test Linf Norm: 1.8737\n",
            "Epoch 8: Train Loss: 0.0544, Test Loss: 0.0549, Train L1 Norm: 0.0625, Test L1 Norm: 0.0457, Train Linf Norm: 1.8420, Test Linf Norm: 1.3376\n",
            "Epoch 9: Train Loss: 0.0528, Test Loss: 0.0667, Train L1 Norm: 0.0623, Test L1 Norm: 0.0707, Train Linf Norm: 1.9472, Test Linf Norm: 1.1675\n",
            "Epoch 10: Train Loss: 0.0550, Test Loss: 0.0544, Train L1 Norm: 0.0720, Test L1 Norm: 0.0711, Train Linf Norm: 3.0688, Test Linf Norm: 1.1081\n",
            "Epoch 11: Train Loss: 0.0307, Test Loss: 0.0315, Train L1 Norm: 0.0423, Test L1 Norm: 0.0385, Train Linf Norm: 1.3863, Test Linf Norm: 1.4128\n",
            "Epoch 12: Train Loss: 0.0293, Test Loss: 0.0282, Train L1 Norm: 0.0426, Test L1 Norm: 0.0378, Train Linf Norm: 1.5584, Test Linf Norm: 1.3157\n",
            "Epoch 13: Train Loss: 0.0287, Test Loss: 0.0274, Train L1 Norm: 0.0433, Test L1 Norm: 0.0371, Train Linf Norm: 1.6718, Test Linf Norm: 1.3336\n",
            "Epoch 14: Train Loss: 0.0282, Test Loss: 0.0319, Train L1 Norm: 0.0412, Test L1 Norm: 0.0378, Train Linf Norm: 1.4101, Test Linf Norm: 1.2700\n",
            "Epoch 15: Train Loss: 0.0278, Test Loss: 0.0296, Train L1 Norm: 0.0419, Test L1 Norm: 0.0375, Train Linf Norm: 1.5540, Test Linf Norm: 1.3292\n",
            "Epoch 16: Train Loss: 0.0276, Test Loss: 0.0266, Train L1 Norm: 0.0427, Test L1 Norm: 0.0358, Train Linf Norm: 1.6442, Test Linf Norm: 1.3192\n",
            "Epoch 17: Train Loss: 0.0268, Test Loss: 0.0263, Train L1 Norm: 0.0407, Test L1 Norm: 0.0371, Train Linf Norm: 1.4896, Test Linf Norm: 1.2382\n",
            "Epoch 18: Train Loss: 0.0269, Test Loss: 0.0269, Train L1 Norm: 0.0413, Test L1 Norm: 0.0364, Train Linf Norm: 1.5668, Test Linf Norm: 1.3008\n",
            "Epoch 19: Train Loss: 0.0262, Test Loss: 0.0256, Train L1 Norm: 0.0428, Test L1 Norm: 0.0353, Train Linf Norm: 1.7540, Test Linf Norm: 1.3147\n",
            "Epoch 20: Train Loss: 0.0260, Test Loss: 0.0258, Train L1 Norm: 0.0415, Test L1 Norm: 0.0365, Train Linf Norm: 1.6139, Test Linf Norm: 1.2989\n",
            "Epoch 21: Train Loss: 0.0257, Test Loss: 0.0273, Train L1 Norm: 0.0419, Test L1 Norm: 0.0374, Train Linf Norm: 1.7032, Test Linf Norm: 1.4395\n",
            "Epoch 22: Train Loss: 0.0256, Test Loss: 0.0272, Train L1 Norm: 0.0414, Test L1 Norm: 0.0353, Train Linf Norm: 1.6563, Test Linf Norm: 1.2066\n",
            "Epoch 23: Train Loss: 0.0251, Test Loss: 0.0247, Train L1 Norm: 0.0402, Test L1 Norm: 0.0346, Train Linf Norm: 1.5408, Test Linf Norm: 1.2399\n",
            "Epoch 24: Train Loss: 0.0250, Test Loss: 0.0239, Train L1 Norm: 0.0403, Test L1 Norm: 0.0362, Train Linf Norm: 1.5506, Test Linf Norm: 1.1384\n",
            "Epoch 25: Train Loss: 0.0248, Test Loss: 0.0237, Train L1 Norm: 0.0408, Test L1 Norm: 0.0346, Train Linf Norm: 1.6599, Test Linf Norm: 1.2885\n",
            "Epoch 26: Train Loss: 0.0245, Test Loss: 0.0244, Train L1 Norm: 0.0417, Test L1 Norm: 0.0372, Train Linf Norm: 1.7586, Test Linf Norm: 1.1197\n",
            "Epoch 27: Train Loss: 0.0244, Test Loss: 0.0237, Train L1 Norm: 0.0390, Test L1 Norm: 0.0355, Train Linf Norm: 1.4043, Test Linf Norm: 1.1315\n",
            "Epoch 28: Train Loss: 0.0239, Test Loss: 0.0235, Train L1 Norm: 0.0393, Test L1 Norm: 0.0353, Train Linf Norm: 1.5142, Test Linf Norm: 1.2971\n",
            "Epoch 29: Train Loss: 0.0239, Test Loss: 0.0253, Train L1 Norm: 0.0388, Test L1 Norm: 0.0353, Train Linf Norm: 1.4740, Test Linf Norm: 1.1304\n",
            "Epoch 30: Train Loss: 0.0237, Test Loss: 0.0236, Train L1 Norm: 0.0400, Test L1 Norm: 0.0334, Train Linf Norm: 1.5479, Test Linf Norm: 1.1895\n",
            "Epoch 31: Train Loss: 0.0238, Test Loss: 0.0250, Train L1 Norm: 0.0402, Test L1 Norm: 0.0338, Train Linf Norm: 1.6582, Test Linf Norm: 1.2311\n",
            "Epoch 32: Train Loss: 0.0235, Test Loss: 0.0225, Train L1 Norm: 0.0396, Test L1 Norm: 0.0338, Train Linf Norm: 1.5998, Test Linf Norm: 1.1427\n",
            "Epoch 33: Train Loss: 0.0236, Test Loss: 0.0256, Train L1 Norm: 0.0389, Test L1 Norm: 0.0371, Train Linf Norm: 1.5346, Test Linf Norm: 1.4009\n",
            "Epoch 34: Train Loss: 0.0232, Test Loss: 0.0221, Train L1 Norm: 0.0408, Test L1 Norm: 0.0328, Train Linf Norm: 1.7420, Test Linf Norm: 1.1395\n",
            "Epoch 35: Train Loss: 0.0233, Test Loss: 0.0228, Train L1 Norm: 0.0392, Test L1 Norm: 0.0338, Train Linf Norm: 1.5740, Test Linf Norm: 1.0748\n",
            "Epoch 36: Train Loss: 0.0229, Test Loss: 0.0233, Train L1 Norm: 0.0383, Test L1 Norm: 0.0348, Train Linf Norm: 1.4978, Test Linf Norm: 1.0974\n",
            "Epoch 37: Train Loss: 0.0227, Test Loss: 0.0218, Train L1 Norm: 0.0388, Test L1 Norm: 0.0319, Train Linf Norm: 1.5721, Test Linf Norm: 1.2282\n",
            "Epoch 38: Train Loss: 0.0227, Test Loss: 0.0216, Train L1 Norm: 0.0401, Test L1 Norm: 0.0327, Train Linf Norm: 1.7127, Test Linf Norm: 1.1691\n",
            "Epoch 39: Train Loss: 0.0225, Test Loss: 0.0229, Train L1 Norm: 0.0374, Test L1 Norm: 0.0329, Train Linf Norm: 1.3783, Test Linf Norm: 1.2273\n",
            "Epoch 40: Train Loss: 0.0230, Test Loss: 0.0246, Train L1 Norm: 0.0384, Test L1 Norm: 0.0330, Train Linf Norm: 1.5049, Test Linf Norm: 1.1190\n",
            "Epoch 41: Train Loss: 0.0226, Test Loss: 0.0218, Train L1 Norm: 0.0400, Test L1 Norm: 0.0317, Train Linf Norm: 1.7534, Test Linf Norm: 1.1083\n",
            "Epoch 42: Train Loss: 0.0225, Test Loss: 0.0216, Train L1 Norm: 0.0388, Test L1 Norm: 0.0321, Train Linf Norm: 1.6299, Test Linf Norm: 1.0730\n",
            "Epoch 43: Train Loss: 0.0224, Test Loss: 0.0222, Train L1 Norm: 0.0392, Test L1 Norm: 0.0321, Train Linf Norm: 1.6656, Test Linf Norm: 1.1003\n",
            "Epoch 44: Train Loss: 0.0223, Test Loss: 0.0226, Train L1 Norm: 0.0384, Test L1 Norm: 0.0321, Train Linf Norm: 1.6036, Test Linf Norm: 1.0935\n",
            "Epoch 45: Train Loss: 0.0205, Test Loss: 0.0206, Train L1 Norm: 0.0372, Test L1 Norm: 0.0314, Train Linf Norm: 1.5097, Test Linf Norm: 1.0809\n",
            "Epoch 46: Train Loss: 0.0205, Test Loss: 0.0205, Train L1 Norm: 0.0377, Test L1 Norm: 0.0311, Train Linf Norm: 1.6327, Test Linf Norm: 1.1235\n",
            "Epoch 47: Train Loss: 0.0205, Test Loss: 0.0206, Train L1 Norm: 0.0375, Test L1 Norm: 0.0316, Train Linf Norm: 1.6073, Test Linf Norm: 1.1099\n",
            "Epoch 48: Train Loss: 0.0205, Test Loss: 0.0208, Train L1 Norm: 0.0376, Test L1 Norm: 0.0318, Train Linf Norm: 1.5937, Test Linf Norm: 1.0781\n",
            "Epoch 49: Train Loss: 0.0204, Test Loss: 0.0206, Train L1 Norm: 0.0381, Test L1 Norm: 0.0317, Train Linf Norm: 1.6781, Test Linf Norm: 1.0784\n",
            "Epoch 50: Train Loss: 0.0204, Test Loss: 0.0205, Train L1 Norm: 0.0375, Test L1 Norm: 0.0315, Train Linf Norm: 1.6022, Test Linf Norm: 1.1219\n",
            "Epoch 51: Train Loss: 0.0204, Test Loss: 0.0205, Train L1 Norm: 0.0379, Test L1 Norm: 0.0313, Train Linf Norm: 1.6622, Test Linf Norm: 1.1196\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:04:09,621]\u001b[0m Trial 31 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 52: Train Loss: 0.0203, Test Loss: 0.0204, Train L1 Norm: 0.0376, Test L1 Norm: 0.0315, Train Linf Norm: 1.6239, Test Linf Norm: 1.1074\n",
            "Epoch 1: Train Loss: 0.2357, Test Loss: 0.1122, Train L1 Norm: 0.2758, Test L1 Norm: 0.1213, Train Linf Norm: 8.1442, Test Linf Norm: 1.5952\n",
            "Epoch 2: Train Loss: 0.1013, Test Loss: 0.1111, Train L1 Norm: 0.1707, Test L1 Norm: 0.4487, Train Linf Norm: 7.1406, Test Linf Norm: 31.3656\n",
            "Epoch 3: Train Loss: 0.0997, Test Loss: 0.1983, Train L1 Norm: 0.1601, Test L1 Norm: 0.2936, Train Linf Norm: 6.5317, Test Linf Norm: 13.1040\n",
            "Epoch 4: Train Loss: 0.1010, Test Loss: 0.0553, Train L1 Norm: 0.1587, Test L1 Norm: 0.0714, Train Linf Norm: 6.3093, Test Linf Norm: 1.4275\n",
            "Epoch 5: Train Loss: 0.0921, Test Loss: 0.0813, Train L1 Norm: 0.2002, Test L1 Norm: 0.1776, Train Linf Norm: 10.9935, Test Linf Norm: 10.0204\n",
            "Epoch 6: Train Loss: 0.0921, Test Loss: 0.0975, Train L1 Norm: 0.1509, Test L1 Norm: 0.6948, Train Linf Norm: 6.2086, Test Linf Norm: 50.1979\n",
            "Epoch 7: Train Loss: 0.0996, Test Loss: 0.1490, Train L1 Norm: 0.2418, Test L1 Norm: 0.1646, Train Linf Norm: 14.5845, Test Linf Norm: 1.0000\n",
            "Epoch 8: Train Loss: 0.0999, Test Loss: 0.1577, Train L1 Norm: 0.1916, Test L1 Norm: 0.2746, Train Linf Norm: 9.5700, Test Linf Norm: 13.9424\n",
            "Epoch 9: Train Loss: 0.0939, Test Loss: 0.1417, Train L1 Norm: 0.1693, Test L1 Norm: 0.1426, Train Linf Norm: 8.0655, Test Linf Norm: 0.9996\n",
            "Epoch 10: Train Loss: 0.0996, Test Loss: 0.0500, Train L1 Norm: 0.1994, Test L1 Norm: 0.0804, Train Linf Norm: 10.5262, Test Linf Norm: 0.9883\n",
            "Epoch 11: Train Loss: 0.0926, Test Loss: 0.0723, Train L1 Norm: 0.1430, Test L1 Norm: 0.0751, Train Linf Norm: 5.6453, Test Linf Norm: 2.4557\n",
            "Epoch 12: Train Loss: 0.0947, Test Loss: 0.0544, Train L1 Norm: 0.1348, Test L1 Norm: 0.2226, Train Linf Norm: 4.9430, Test Linf Norm: 16.0805\n",
            "Epoch 13: Train Loss: 0.0949, Test Loss: 0.1086, Train L1 Norm: 0.1357, Test L1 Norm: 0.1125, Train Linf Norm: 5.2260, Test Linf Norm: 0.9865\n",
            "Epoch 14: Train Loss: 0.0956, Test Loss: 0.1161, Train L1 Norm: 0.1926, Test L1 Norm: 0.2659, Train Linf Norm: 10.4834, Test Linf Norm: 18.8397\n",
            "Epoch 15: Train Loss: 0.0963, Test Loss: 0.1089, Train L1 Norm: 0.1365, Test L1 Norm: 0.0843, Train Linf Norm: 5.1533, Test Linf Norm: 0.9655\n",
            "Epoch 16: Train Loss: 0.0971, Test Loss: 0.1125, Train L1 Norm: 0.2429, Test L1 Norm: 0.1861, Train Linf Norm: 15.2278, Test Linf Norm: 10.7244\n",
            "Epoch 17: Train Loss: 0.0393, Test Loss: 0.0318, Train L1 Norm: 0.0565, Test L1 Norm: 0.0414, Train Linf Norm: 1.9146, Test Linf Norm: 1.0525\n",
            "Epoch 18: Train Loss: 0.0372, Test Loss: 0.0541, Train L1 Norm: 0.0655, Test L1 Norm: 0.0813, Train Linf Norm: 2.7965, Test Linf Norm: 3.7387\n",
            "Epoch 19: Train Loss: 0.0361, Test Loss: 0.0291, Train L1 Norm: 0.0566, Test L1 Norm: 0.0503, Train Linf Norm: 1.8499, Test Linf Norm: 1.8614\n",
            "Epoch 20: Train Loss: 0.0342, Test Loss: 0.0878, Train L1 Norm: 0.0589, Test L1 Norm: 0.0706, Train Linf Norm: 2.1530, Test Linf Norm: 0.9521\n",
            "Epoch 21: Train Loss: 0.0326, Test Loss: 0.0443, Train L1 Norm: 0.0574, Test L1 Norm: 0.0445, Train Linf Norm: 2.1743, Test Linf Norm: 1.2823\n",
            "Epoch 22: Train Loss: 0.0330, Test Loss: 0.0334, Train L1 Norm: 0.0583, Test L1 Norm: 0.0387, Train Linf Norm: 2.1941, Test Linf Norm: 1.2737\n",
            "Epoch 23: Train Loss: 0.0351, Test Loss: 0.0274, Train L1 Norm: 0.0666, Test L1 Norm: 0.0586, Train Linf Norm: 2.8116, Test Linf Norm: 2.5547\n",
            "Epoch 24: Train Loss: 0.0339, Test Loss: 0.0274, Train L1 Norm: 0.0702, Test L1 Norm: 0.0485, Train Linf Norm: 3.2382, Test Linf Norm: 1.4584\n",
            "Epoch 25: Train Loss: 0.0346, Test Loss: 0.0510, Train L1 Norm: 0.0625, Test L1 Norm: 0.0449, Train Linf Norm: 2.5298, Test Linf Norm: 0.9170\n",
            "Epoch 26: Train Loss: 0.0341, Test Loss: 0.0354, Train L1 Norm: 0.0607, Test L1 Norm: 0.0444, Train Linf Norm: 2.3803, Test Linf Norm: 0.9595\n",
            "Epoch 27: Train Loss: 0.0329, Test Loss: 0.0586, Train L1 Norm: 0.0602, Test L1 Norm: 0.1194, Train Linf Norm: 2.2977, Test Linf Norm: 7.2059\n",
            "Epoch 28: Train Loss: 0.0334, Test Loss: 0.0517, Train L1 Norm: 0.0605, Test L1 Norm: 0.1873, Train Linf Norm: 2.4007, Test Linf Norm: 12.7879\n",
            "Epoch 29: Train Loss: 0.0339, Test Loss: 0.0257, Train L1 Norm: 0.0639, Test L1 Norm: 0.0357, Train Linf Norm: 2.6080, Test Linf Norm: 0.9887\n",
            "Epoch 30: Train Loss: 0.0342, Test Loss: 0.0272, Train L1 Norm: 0.0698, Test L1 Norm: 0.0494, Train Linf Norm: 3.2335, Test Linf Norm: 1.2702\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:05:16,541]\u001b[0m Trial 32 finished with value: 0.046471478710882363 and parameters: {'n_layers': 2, 'n_units_0': 113, 'n_units_1': 87, 'hidden_activation': 'Tanh', 'output_activation': 'ReLU', 'loss': 'MAE', 'optimizer': 'Adam', 'lr': 0.0023610584180782206, 'batch_size': 95, 'n_epochs': 31, 'scheduler': 'ReduceLROnPlateau', 'weight_decay': 0.00954404830289775, 'beta1': 0.922025197472693, 'beta2': 0.9211348633881934, 'factor': 0.23527037388883149, 'patience': 5, 'threshold': 0.0003633956117901666}. Best is trial 10 with value: 0.006550250181416049.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31: Train Loss: 0.0335, Test Loss: 0.0252, Train L1 Norm: 0.0652, Test L1 Norm: 0.0465, Train Linf Norm: 2.8972, Test Linf Norm: 1.5371\n",
            "Epoch 1: Train Loss: 0.2688, Test Loss: 0.0738, Train L1 Norm: 0.2899, Test L1 Norm: 0.1069, Train Linf Norm: 9.5078, Test Linf Norm: 1.9373\n",
            "Epoch 2: Train Loss: 0.0564, Test Loss: 0.0501, Train L1 Norm: 0.1314, Test L1 Norm: 0.0856, Train Linf Norm: 5.1391, Test Linf Norm: 1.0073\n",
            "Epoch 3: Train Loss: 0.0680, Test Loss: 0.0739, Train L1 Norm: 0.1543, Test L1 Norm: 0.0821, Train Linf Norm: 7.7435, Test Linf Norm: 2.2146\n",
            "Epoch 4: Train Loss: 0.0618, Test Loss: 0.0587, Train L1 Norm: 0.1031, Test L1 Norm: 0.0883, Train Linf Norm: 3.5111, Test Linf Norm: 1.1421\n",
            "Epoch 5: Train Loss: 0.0526, Test Loss: 0.0606, Train L1 Norm: 0.0995, Test L1 Norm: 0.0813, Train Linf Norm: 4.3598, Test Linf Norm: 2.5462\n",
            "Epoch 6: Train Loss: 0.0623, Test Loss: 0.0627, Train L1 Norm: 0.1310, Test L1 Norm: 0.0630, Train Linf Norm: 6.2301, Test Linf Norm: 1.0296\n",
            "Epoch 7: Train Loss: 0.0550, Test Loss: 0.0432, Train L1 Norm: 0.1203, Test L1 Norm: 0.0772, Train Linf Norm: 5.4897, Test Linf Norm: 1.9782\n",
            "Epoch 8: Train Loss: 0.0669, Test Loss: 0.0855, Train L1 Norm: 0.1406, Test L1 Norm: 0.2072, Train Linf Norm: 6.7192, Test Linf Norm: 12.8429\n",
            "Epoch 9: Train Loss: 0.0590, Test Loss: 0.0726, Train L1 Norm: 0.1339, Test L1 Norm: 0.3172, Train Linf Norm: 7.0464, Test Linf Norm: 22.7436\n",
            "Epoch 10: Train Loss: 0.0433, Test Loss: 0.0438, Train L1 Norm: 0.0858, Test L1 Norm: 0.0562, Train Linf Norm: 3.4804, Test Linf Norm: 1.7009\n",
            "Epoch 11: Train Loss: 0.0428, Test Loss: 0.0285, Train L1 Norm: 0.0989, Test L1 Norm: 0.0606, Train Linf Norm: 4.9543, Test Linf Norm: 1.0351\n",
            "Epoch 12: Train Loss: 0.0470, Test Loss: 0.0327, Train L1 Norm: 0.1807, Test L1 Norm: 0.0604, Train Linf Norm: 13.1371, Test Linf Norm: 1.3765\n",
            "Epoch 13: Train Loss: 0.0495, Test Loss: 0.0429, Train L1 Norm: 0.2682, Test L1 Norm: 0.0525, Train Linf Norm: 22.2023, Test Linf Norm: 1.0467\n",
            "Epoch 14: Train Loss: 0.0493, Test Loss: 0.0563, Train L1 Norm: 0.1043, Test L1 Norm: 0.0603, Train Linf Norm: 5.1201, Test Linf Norm: 1.1447\n",
            "Epoch 15: Train Loss: 0.0433, Test Loss: 0.0462, Train L1 Norm: 0.2142, Test L1 Norm: 0.1624, Train Linf Norm: 17.0626, Test Linf Norm: 12.0204\n",
            "Epoch 16: Train Loss: 0.0473, Test Loss: 0.0495, Train L1 Norm: 0.1000, Test L1 Norm: 0.0680, Train Linf Norm: 4.2466, Test Linf Norm: 0.9872\n",
            "Epoch 17: Train Loss: 0.0458, Test Loss: 0.0424, Train L1 Norm: 0.1299, Test L1 Norm: 0.0861, Train Linf Norm: 7.9896, Test Linf Norm: 2.1601\n",
            "Epoch 18: Train Loss: 0.0468, Test Loss: 0.0538, Train L1 Norm: 0.1068, Test L1 Norm: 0.0554, Train Linf Norm: 5.1750, Test Linf Norm: 1.2432\n",
            "Epoch 19: Train Loss: 0.0584, Test Loss: 0.0571, Train L1 Norm: 0.1310, Test L1 Norm: 0.0946, Train Linf Norm: 6.5009, Test Linf Norm: 0.9969\n",
            "Epoch 20: Train Loss: 0.0223, Test Loss: 0.0180, Train L1 Norm: 0.0591, Test L1 Norm: 0.0397, Train Linf Norm: 3.2555, Test Linf Norm: 1.0447\n",
            "Epoch 21: Train Loss: 0.0200, Test Loss: 0.0218, Train L1 Norm: 0.0485, Test L1 Norm: 0.0327, Train Linf Norm: 2.2755, Test Linf Norm: 1.1592\n",
            "Epoch 22: Train Loss: 0.0217, Test Loss: 0.0163, Train L1 Norm: 0.0522, Test L1 Norm: 0.0338, Train Linf Norm: 2.6078, Test Linf Norm: 1.1584\n",
            "Epoch 23: Train Loss: 0.0223, Test Loss: 0.0406, Train L1 Norm: 0.0547, Test L1 Norm: 0.0630, Train Linf Norm: 2.8995, Test Linf Norm: 2.8953\n",
            "Epoch 24: Train Loss: 0.0236, Test Loss: 0.0264, Train L1 Norm: 0.0534, Test L1 Norm: 0.0393, Train Linf Norm: 2.4288, Test Linf Norm: 1.4428\n",
            "Epoch 25: Train Loss: 0.0188, Test Loss: 0.0216, Train L1 Norm: 0.0643, Test L1 Norm: 0.0335, Train Linf Norm: 4.0666, Test Linf Norm: 1.0327\n",
            "Epoch 26: Train Loss: 0.0213, Test Loss: 0.0188, Train L1 Norm: 0.0486, Test L1 Norm: 0.0337, Train Linf Norm: 2.3009, Test Linf Norm: 1.0203\n",
            "Epoch 27: Train Loss: 0.0187, Test Loss: 0.0225, Train L1 Norm: 0.0492, Test L1 Norm: 0.0457, Train Linf Norm: 2.3813, Test Linf Norm: 1.0002\n",
            "Epoch 28: Train Loss: 0.0180, Test Loss: 0.0194, Train L1 Norm: 0.0404, Test L1 Norm: 0.0346, Train Linf Norm: 1.5919, Test Linf Norm: 0.9707\n",
            "Epoch 29: Train Loss: 0.0193, Test Loss: 0.0282, Train L1 Norm: 0.0426, Test L1 Norm: 0.0461, Train Linf Norm: 1.7886, Test Linf Norm: 1.4955\n",
            "Epoch 30: Train Loss: 0.0219, Test Loss: 0.0177, Train L1 Norm: 0.0472, Test L1 Norm: 0.0364, Train Linf Norm: 1.8989, Test Linf Norm: 1.0003\n",
            "Epoch 31: Train Loss: 0.0129, Test Loss: 0.0125, Train L1 Norm: 0.0374, Test L1 Norm: 0.0305, Train Linf Norm: 1.6402, Test Linf Norm: 1.0070\n",
            "Epoch 32: Train Loss: 0.0122, Test Loss: 0.0111, Train L1 Norm: 0.0364, Test L1 Norm: 0.0278, Train Linf Norm: 1.6181, Test Linf Norm: 1.0328\n",
            "Epoch 33: Train Loss: 0.0127, Test Loss: 0.0110, Train L1 Norm: 0.0365, Test L1 Norm: 0.0298, Train Linf Norm: 1.5954, Test Linf Norm: 1.0169\n",
            "Epoch 34: Train Loss: 0.0116, Test Loss: 0.0121, Train L1 Norm: 0.0364, Test L1 Norm: 0.0320, Train Linf Norm: 1.6554, Test Linf Norm: 1.2396\n",
            "Epoch 35: Train Loss: 0.0117, Test Loss: 0.0105, Train L1 Norm: 0.0362, Test L1 Norm: 0.0273, Train Linf Norm: 1.6221, Test Linf Norm: 0.9805\n",
            "Epoch 36: Train Loss: 0.0122, Test Loss: 0.0111, Train L1 Norm: 0.0365, Test L1 Norm: 0.0291, Train Linf Norm: 1.6581, Test Linf Norm: 0.9634\n",
            "Epoch 37: Train Loss: 0.0116, Test Loss: 0.0097, Train L1 Norm: 0.0369, Test L1 Norm: 0.0277, Train Linf Norm: 1.7561, Test Linf Norm: 0.9844\n",
            "Epoch 38: Train Loss: 0.0121, Test Loss: 0.0109, Train L1 Norm: 0.0361, Test L1 Norm: 0.0292, Train Linf Norm: 1.5763, Test Linf Norm: 1.0481\n",
            "Epoch 39: Train Loss: 0.0124, Test Loss: 0.0110, Train L1 Norm: 0.0359, Test L1 Norm: 0.0263, Train Linf Norm: 1.5558, Test Linf Norm: 0.9854\n",
            "Epoch 40: Train Loss: 0.0115, Test Loss: 0.0135, Train L1 Norm: 0.0351, Test L1 Norm: 0.0271, Train Linf Norm: 1.4957, Test Linf Norm: 1.0325\n",
            "Epoch 41: Train Loss: 0.0118, Test Loss: 0.0114, Train L1 Norm: 0.0376, Test L1 Norm: 0.0296, Train Linf Norm: 1.7757, Test Linf Norm: 0.9541\n",
            "Epoch 42: Train Loss: 0.0122, Test Loss: 0.0116, Train L1 Norm: 0.0349, Test L1 Norm: 0.0270, Train Linf Norm: 1.4870, Test Linf Norm: 1.0292\n",
            "Epoch 43: Train Loss: 0.0120, Test Loss: 0.0107, Train L1 Norm: 0.0402, Test L1 Norm: 0.0282, Train Linf Norm: 2.0559, Test Linf Norm: 0.9610\n",
            "Epoch 44: Train Loss: 0.0115, Test Loss: 0.0095, Train L1 Norm: 0.0342, Test L1 Norm: 0.0272, Train Linf Norm: 1.4241, Test Linf Norm: 0.9734\n",
            "Epoch 45: Train Loss: 0.0115, Test Loss: 0.0117, Train L1 Norm: 0.0343, Test L1 Norm: 0.0265, Train Linf Norm: 1.4855, Test Linf Norm: 0.9762\n",
            "Epoch 46: Train Loss: 0.0103, Test Loss: 0.0092, Train L1 Norm: 0.0325, Test L1 Norm: 0.0262, Train Linf Norm: 1.3245, Test Linf Norm: 1.0249\n",
            "Epoch 47: Train Loss: 0.0114, Test Loss: 0.0100, Train L1 Norm: 0.0342, Test L1 Norm: 0.0267, Train Linf Norm: 1.4492, Test Linf Norm: 1.0005\n",
            "Epoch 48: Train Loss: 0.0116, Test Loss: 0.0127, Train L1 Norm: 0.0358, Test L1 Norm: 0.0276, Train Linf Norm: 1.6193, Test Linf Norm: 1.0136\n",
            "Epoch 49: Train Loss: 0.0107, Test Loss: 0.0119, Train L1 Norm: 0.0337, Test L1 Norm: 0.0270, Train Linf Norm: 1.4538, Test Linf Norm: 0.9489\n",
            "Epoch 50: Train Loss: 0.0109, Test Loss: 0.0109, Train L1 Norm: 0.0332, Test L1 Norm: 0.0277, Train Linf Norm: 1.3729, Test Linf Norm: 0.9497\n",
            "Epoch 51: Train Loss: 0.0117, Test Loss: 0.0100, Train L1 Norm: 0.0362, Test L1 Norm: 0.0271, Train Linf Norm: 1.6629, Test Linf Norm: 0.9609\n",
            "Epoch 52: Train Loss: 0.0112, Test Loss: 0.0126, Train L1 Norm: 0.0364, Test L1 Norm: 0.0308, Train Linf Norm: 1.6634, Test Linf Norm: 0.9727\n",
            "Epoch 53: Train Loss: 0.0103, Test Loss: 0.0099, Train L1 Norm: 0.0336, Test L1 Norm: 0.0270, Train Linf Norm: 1.4535, Test Linf Norm: 0.9664\n",
            "Epoch 54: Train Loss: 0.0109, Test Loss: 0.0112, Train L1 Norm: 0.0350, Test L1 Norm: 0.0260, Train Linf Norm: 1.5743, Test Linf Norm: 0.9451\n",
            "Epoch 55: Train Loss: 0.0093, Test Loss: 0.0098, Train L1 Norm: 0.0327, Test L1 Norm: 0.0276, Train Linf Norm: 1.4529, Test Linf Norm: 0.9914\n",
            "Epoch 56: Train Loss: 0.0093, Test Loss: 0.0092, Train L1 Norm: 0.0338, Test L1 Norm: 0.0274, Train Linf Norm: 1.5432, Test Linf Norm: 1.0032\n",
            "Epoch 57: Train Loss: 0.0089, Test Loss: 0.0088, Train L1 Norm: 0.0328, Test L1 Norm: 0.0269, Train Linf Norm: 1.4507, Test Linf Norm: 0.9678\n",
            "Epoch 58: Train Loss: 0.0089, Test Loss: 0.0086, Train L1 Norm: 0.0330, Test L1 Norm: 0.0266, Train Linf Norm: 1.4631, Test Linf Norm: 0.9704\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:07:19,827]\u001b[0m Trial 33 finished with value: 0.026782296072132887 and parameters: {'n_layers': 2, 'n_units_0': 221, 'n_units_1': 230, 'hidden_activation': 'Tanh', 'output_activation': 'ReLU', 'loss': 'MAE', 'optimizer': 'Adam', 'lr': 0.001303886404061728, 'batch_size': 109, 'n_epochs': 59, 'scheduler': 'ReduceLROnPlateau', 'weight_decay': 0.003774047784245367, 'beta1': 0.9806531154177887, 'beta2': 0.9180233945666931, 'factor': 0.2617047400571077, 'patience': 7, 'threshold': 0.002309384613813296}. Best is trial 10 with value: 0.006550250181416049.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 59: Train Loss: 0.0091, Test Loss: 0.0095, Train L1 Norm: 0.0320, Test L1 Norm: 0.0268, Train Linf Norm: 1.3575, Test Linf Norm: 0.9479\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:07:23,070]\u001b[0m Trial 34 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.2644, Test Loss: 0.1380, Train L1 Norm: 0.3021, Test L1 Norm: 0.3135, Train Linf Norm: 10.2088, Test Linf Norm: 15.2456\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:07:25,519]\u001b[0m Trial 35 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0231, Test Loss: 0.0012, Train L1 Norm: 1.2842, Test L1 Norm: 0.7560, Train Linf Norm: 89.9410, Test Linf Norm: 54.3168\n",
            "Epoch 1: Train Loss: 0.1759, Test Loss: 0.0284, Train L1 Norm: 0.3566, Test L1 Norm: 0.1693, Train Linf Norm: 5.0222, Test Linf Norm: 1.9214\n",
            "Epoch 2: Train Loss: 0.0622, Test Loss: 0.0527, Train L1 Norm: 0.2593, Test L1 Norm: 0.2118, Train Linf Norm: 5.9309, Test Linf Norm: 2.4952\n",
            "Epoch 3: Train Loss: 0.0497, Test Loss: 0.0740, Train L1 Norm: 0.2262, Test L1 Norm: 0.1967, Train Linf Norm: 4.5173, Test Linf Norm: 1.0000\n",
            "Epoch 4: Train Loss: 0.0433, Test Loss: 0.0706, Train L1 Norm: 0.2223, Test L1 Norm: 0.1593, Train Linf Norm: 4.9590, Test Linf Norm: 1.0313\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:07:35,610]\u001b[0m Trial 36 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Train Loss: 0.0388, Test Loss: 0.0091, Train L1 Norm: 0.2197, Test L1 Norm: 0.2525, Train Linf Norm: 5.2794, Test Linf Norm: 15.1156\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:07:36,935]\u001b[0m Trial 37 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.2656, Test Loss: 0.1407, Train L1 Norm: 3.7976, Test L1 Norm: 2.6330, Train Linf Norm: 706.3324, Test Linf Norm: 497.4038\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:07:38,543]\u001b[0m Trial 38 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:07:43,990]\u001b[0m Trial 39 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0313, Test Loss: 0.0013, Train L1 Norm: 1.5969, Test L1 Norm: 1.0434, Train Linf Norm: 40.0337, Test Linf Norm: 27.0405\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:07:48,323]\u001b[0m Trial 40 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 3.1200, Test Loss: 1.9848, Train L1 Norm: 0.9231, Test L1 Norm: 0.4851, Train Linf Norm: 12.8444, Test Linf Norm: 1.5657\n",
            "Epoch 1: Train Loss: 0.4560, Test Loss: 0.0608, Train L1 Norm: 0.5737, Test L1 Norm: 0.1109, Train Linf Norm: 28.5762, Test Linf Norm: 3.5122\n",
            "Epoch 2: Train Loss: 0.0547, Test Loss: 0.0422, Train L1 Norm: 0.1235, Test L1 Norm: 0.0830, Train Linf Norm: 4.8463, Test Linf Norm: 2.5058\n",
            "Epoch 3: Train Loss: 0.0467, Test Loss: 0.0484, Train L1 Norm: 0.0934, Test L1 Norm: 0.1564, Train Linf Norm: 3.5200, Test Linf Norm: 11.9114\n",
            "Epoch 4: Train Loss: 0.0481, Test Loss: 0.0483, Train L1 Norm: 0.1024, Test L1 Norm: 0.0546, Train Linf Norm: 4.8648, Test Linf Norm: 1.8442\n",
            "Epoch 5: Train Loss: 0.0416, Test Loss: 0.0456, Train L1 Norm: 0.0698, Test L1 Norm: 0.0703, Train Linf Norm: 2.5254, Test Linf Norm: 4.5265\n",
            "Epoch 6: Train Loss: 0.0444, Test Loss: 0.0336, Train L1 Norm: 0.0800, Test L1 Norm: 0.0617, Train Linf Norm: 3.5459, Test Linf Norm: 2.6579\n",
            "Epoch 7: Train Loss: 0.0420, Test Loss: 0.0344, Train L1 Norm: 0.0976, Test L1 Norm: 0.0510, Train Linf Norm: 5.3889, Test Linf Norm: 2.3706\n",
            "Epoch 8: Train Loss: 0.0476, Test Loss: 0.0429, Train L1 Norm: 0.0919, Test L1 Norm: 0.0529, Train Linf Norm: 4.3722, Test Linf Norm: 1.9142\n",
            "Epoch 9: Train Loss: 0.0444, Test Loss: 0.0751, Train L1 Norm: 0.0881, Test L1 Norm: 0.0653, Train Linf Norm: 4.4818, Test Linf Norm: 1.8532\n",
            "Epoch 10: Train Loss: 0.0457, Test Loss: 0.0620, Train L1 Norm: 0.0867, Test L1 Norm: 0.0958, Train Linf Norm: 4.0920, Test Linf Norm: 3.9731\n",
            "Epoch 11: Train Loss: 0.0485, Test Loss: 0.0466, Train L1 Norm: 0.1380, Test L1 Norm: 0.0987, Train Linf Norm: 9.6855, Test Linf Norm: 5.7929\n",
            "Epoch 12: Train Loss: 0.0438, Test Loss: 0.0330, Train L1 Norm: 0.1188, Test L1 Norm: 0.0463, Train Linf Norm: 7.5330, Test Linf Norm: 1.6982\n",
            "Epoch 13: Train Loss: 0.0331, Test Loss: 0.0512, Train L1 Norm: 0.0800, Test L1 Norm: 0.1118, Train Linf Norm: 4.4368, Test Linf Norm: 5.8803\n",
            "Epoch 14: Train Loss: 0.0489, Test Loss: 0.0349, Train L1 Norm: 0.0939, Test L1 Norm: 0.0887, Train Linf Norm: 3.9000, Test Linf Norm: 6.1138\n",
            "Epoch 15: Train Loss: 0.0437, Test Loss: 0.0289, Train L1 Norm: 0.0960, Test L1 Norm: 0.0681, Train Linf Norm: 5.0310, Test Linf Norm: 3.2204\n",
            "Epoch 16: Train Loss: 0.0410, Test Loss: 0.0343, Train L1 Norm: 0.0814, Test L1 Norm: 0.1198, Train Linf Norm: 3.8849, Test Linf Norm: 8.6823\n",
            "Epoch 17: Train Loss: 0.0381, Test Loss: 0.0478, Train L1 Norm: 0.0844, Test L1 Norm: 0.0577, Train Linf Norm: 4.1054, Test Linf Norm: 0.9688\n",
            "Epoch 18: Train Loss: 0.0499, Test Loss: 0.0720, Train L1 Norm: 0.1277, Test L1 Norm: 0.1064, Train Linf Norm: 6.7617, Test Linf Norm: 6.6073\n",
            "Epoch 19: Train Loss: 0.0405, Test Loss: 0.0566, Train L1 Norm: 0.0848, Test L1 Norm: 0.2153, Train Linf Norm: 4.1001, Test Linf Norm: 16.8380\n",
            "Epoch 20: Train Loss: 0.0456, Test Loss: 0.0375, Train L1 Norm: 0.1284, Test L1 Norm: 0.0640, Train Linf Norm: 8.2290, Test Linf Norm: 1.1297\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:08:28,004]\u001b[0m Trial 41 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: Train Loss: 0.0486, Test Loss: 0.0695, Train L1 Norm: 0.1218, Test L1 Norm: 0.0726, Train Linf Norm: 7.1280, Test Linf Norm: 1.6382\n",
            "Epoch 1: Train Loss: 0.4556, Test Loss: 0.0938, Train L1 Norm: 0.5439, Test L1 Norm: 0.0834, Train Linf Norm: 23.4478, Test Linf Norm: 1.0584\n",
            "Epoch 2: Train Loss: 0.0639, Test Loss: 0.0663, Train L1 Norm: 0.0853, Test L1 Norm: 0.0732, Train Linf Norm: 2.3532, Test Linf Norm: 1.2171\n",
            "Epoch 3: Train Loss: 0.0502, Test Loss: 0.0393, Train L1 Norm: 0.0711, Test L1 Norm: 0.0538, Train Linf Norm: 2.0126, Test Linf Norm: 1.0662\n",
            "Epoch 4: Train Loss: 0.0423, Test Loss: 0.0349, Train L1 Norm: 0.0603, Test L1 Norm: 0.0455, Train Linf Norm: 1.7334, Test Linf Norm: 1.0421\n",
            "Epoch 5: Train Loss: 0.0415, Test Loss: 0.0456, Train L1 Norm: 0.0685, Test L1 Norm: 0.2153, Train Linf Norm: 2.5436, Test Linf Norm: 14.2026\n",
            "Epoch 6: Train Loss: 0.0425, Test Loss: 0.0511, Train L1 Norm: 0.0631, Test L1 Norm: 0.0524, Train Linf Norm: 1.8124, Test Linf Norm: 0.9272\n",
            "Epoch 7: Train Loss: 0.0438, Test Loss: 0.0271, Train L1 Norm: 0.0709, Test L1 Norm: 0.0359, Train Linf Norm: 2.4342, Test Linf Norm: 1.1053\n",
            "Epoch 8: Train Loss: 0.0402, Test Loss: 0.0226, Train L1 Norm: 0.0658, Test L1 Norm: 0.0786, Train Linf Norm: 2.2883, Test Linf Norm: 4.3942\n",
            "Epoch 9: Train Loss: 0.0393, Test Loss: 0.0482, Train L1 Norm: 0.0585, Test L1 Norm: 0.0419, Train Linf Norm: 1.8136, Test Linf Norm: 0.8864\n",
            "Epoch 10: Train Loss: 0.0394, Test Loss: 0.0347, Train L1 Norm: 0.0722, Test L1 Norm: 0.0597, Train Linf Norm: 2.8516, Test Linf Norm: 1.8552\n",
            "Epoch 11: Train Loss: 0.0375, Test Loss: 0.0443, Train L1 Norm: 0.0564, Test L1 Norm: 0.0904, Train Linf Norm: 1.7038, Test Linf Norm: 3.0700\n",
            "Epoch 12: Train Loss: 0.0406, Test Loss: 0.0346, Train L1 Norm: 0.0864, Test L1 Norm: 0.0503, Train Linf Norm: 4.2089, Test Linf Norm: 1.2340\n",
            "Epoch 13: Train Loss: 0.0438, Test Loss: 0.0320, Train L1 Norm: 0.0629, Test L1 Norm: 0.0516, Train Linf Norm: 1.9058, Test Linf Norm: 1.2062\n",
            "Epoch 14: Train Loss: 0.0395, Test Loss: 0.0451, Train L1 Norm: 0.0685, Test L1 Norm: 0.0513, Train Linf Norm: 2.7152, Test Linf Norm: 1.3164\n",
            "Epoch 15: Train Loss: 0.0196, Test Loss: 0.0164, Train L1 Norm: 0.0367, Test L1 Norm: 0.0256, Train Linf Norm: 1.3530, Test Linf Norm: 0.8391\n",
            "Epoch 16: Train Loss: 0.0182, Test Loss: 0.0188, Train L1 Norm: 0.0337, Test L1 Norm: 0.0288, Train Linf Norm: 1.1492, Test Linf Norm: 0.8909\n",
            "Epoch 17: Train Loss: 0.0178, Test Loss: 0.0162, Train L1 Norm: 0.0342, Test L1 Norm: 0.0300, Train Linf Norm: 1.2266, Test Linf Norm: 1.0768\n",
            "Epoch 18: Train Loss: 0.0176, Test Loss: 0.0173, Train L1 Norm: 0.0358, Test L1 Norm: 0.0264, Train Linf Norm: 1.3726, Test Linf Norm: 0.8788\n",
            "Epoch 19: Train Loss: 0.0178, Test Loss: 0.0192, Train L1 Norm: 0.0343, Test L1 Norm: 0.0281, Train Linf Norm: 1.2013, Test Linf Norm: 0.7753\n",
            "Epoch 20: Train Loss: 0.0169, Test Loss: 0.0163, Train L1 Norm: 0.0343, Test L1 Norm: 0.0277, Train Linf Norm: 1.2845, Test Linf Norm: 1.0224\n",
            "Epoch 21: Train Loss: 0.0168, Test Loss: 0.0157, Train L1 Norm: 0.0362, Test L1 Norm: 0.0292, Train Linf Norm: 1.4503, Test Linf Norm: 0.8334\n",
            "Epoch 22: Train Loss: 0.0168, Test Loss: 0.0233, Train L1 Norm: 0.0365, Test L1 Norm: 0.0384, Train Linf Norm: 1.4199, Test Linf Norm: 1.2888\n",
            "Epoch 23: Train Loss: 0.0167, Test Loss: 0.0182, Train L1 Norm: 0.0327, Test L1 Norm: 0.0306, Train Linf Norm: 1.1522, Test Linf Norm: 1.0007\n",
            "Epoch 24: Train Loss: 0.0170, Test Loss: 0.0183, Train L1 Norm: 0.0385, Test L1 Norm: 0.0276, Train Linf Norm: 1.6222, Test Linf Norm: 0.7741\n",
            "Epoch 25: Train Loss: 0.0163, Test Loss: 0.0199, Train L1 Norm: 0.0319, Test L1 Norm: 0.0289, Train Linf Norm: 1.1187, Test Linf Norm: 0.8482\n",
            "Epoch 26: Train Loss: 0.0176, Test Loss: 0.0143, Train L1 Norm: 0.0347, Test L1 Norm: 0.0257, Train Linf Norm: 1.2774, Test Linf Norm: 0.9226\n",
            "Epoch 27: Train Loss: 0.0157, Test Loss: 0.0184, Train L1 Norm: 0.0325, Test L1 Norm: 0.0272, Train Linf Norm: 1.1601, Test Linf Norm: 0.9090\n",
            "Epoch 28: Train Loss: 0.0160, Test Loss: 0.0141, Train L1 Norm: 0.0344, Test L1 Norm: 0.0229, Train Linf Norm: 1.3088, Test Linf Norm: 0.7466\n",
            "Epoch 29: Train Loss: 0.0165, Test Loss: 0.0150, Train L1 Norm: 0.0329, Test L1 Norm: 0.0258, Train Linf Norm: 1.1993, Test Linf Norm: 0.8812\n",
            "Epoch 30: Train Loss: 0.0159, Test Loss: 0.0144, Train L1 Norm: 0.0323, Test L1 Norm: 0.0291, Train Linf Norm: 1.1758, Test Linf Norm: 1.0203\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:09:38,762]\u001b[0m Trial 42 finished with value: 0.025211193546792493 and parameters: {'n_layers': 2, 'n_units_0': 130, 'n_units_1': 22, 'hidden_activation': 'Tanh', 'output_activation': 'ReLU', 'loss': 'MAE', 'optimizer': 'Adam', 'lr': 0.0013226075178407201, 'batch_size': 87, 'n_epochs': 31, 'scheduler': 'ReduceLROnPlateau', 'weight_decay': 0.005345116577395413, 'beta1': 0.93645296589026, 'beta2': 0.9160397452153295, 'factor': 0.19479042635756635, 'patience': 5, 'threshold': 0.004474078478387027}. Best is trial 10 with value: 0.006550250181416049.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31: Train Loss: 0.0161, Test Loss: 0.0168, Train L1 Norm: 0.0321, Test L1 Norm: 0.0252, Train Linf Norm: 1.1115, Test Linf Norm: 0.9032\n",
            "Epoch 1: Train Loss: 0.3064, Test Loss: 0.0899, Train L1 Norm: 0.2990, Test L1 Norm: 0.0976, Train Linf Norm: 7.4250, Test Linf Norm: 1.9519\n",
            "Epoch 2: Train Loss: 0.0706, Test Loss: 0.0565, Train L1 Norm: 0.1194, Test L1 Norm: 0.0640, Train Linf Norm: 4.2979, Test Linf Norm: 1.3678\n",
            "Epoch 3: Train Loss: 0.0526, Test Loss: 0.0633, Train L1 Norm: 0.0870, Test L1 Norm: 0.0566, Train Linf Norm: 2.9646, Test Linf Norm: 1.1604\n",
            "Epoch 4: Train Loss: 0.0492, Test Loss: 0.0369, Train L1 Norm: 0.0799, Test L1 Norm: 0.0638, Train Linf Norm: 2.5545, Test Linf Norm: 1.0250\n",
            "Epoch 5: Train Loss: 0.0459, Test Loss: 0.0378, Train L1 Norm: 0.0737, Test L1 Norm: 0.0631, Train Linf Norm: 2.1409, Test Linf Norm: 1.3182\n",
            "Epoch 6: Train Loss: 0.0435, Test Loss: 0.0455, Train L1 Norm: 0.0708, Test L1 Norm: 0.0698, Train Linf Norm: 2.0251, Test Linf Norm: 1.4888\n",
            "Epoch 7: Train Loss: 0.0451, Test Loss: 0.0497, Train L1 Norm: 0.0904, Test L1 Norm: 0.1351, Train Linf Norm: 3.5136, Test Linf Norm: 6.3205\n",
            "Epoch 8: Train Loss: 0.0415, Test Loss: 0.0327, Train L1 Norm: 0.0716, Test L1 Norm: 0.0445, Train Linf Norm: 2.3434, Test Linf Norm: 1.2757\n",
            "Epoch 9: Train Loss: 0.0457, Test Loss: 0.0375, Train L1 Norm: 0.0901, Test L1 Norm: 0.0543, Train Linf Norm: 3.4539, Test Linf Norm: 1.9793\n",
            "Epoch 10: Train Loss: 0.0447, Test Loss: 0.0445, Train L1 Norm: 0.0934, Test L1 Norm: 0.0683, Train Linf Norm: 3.9070, Test Linf Norm: 1.0454\n",
            "Epoch 11: Train Loss: 0.0434, Test Loss: 0.0388, Train L1 Norm: 0.0844, Test L1 Norm: 0.0719, Train Linf Norm: 3.0791, Test Linf Norm: 2.0690\n",
            "Epoch 12: Train Loss: 0.0428, Test Loss: 0.0354, Train L1 Norm: 0.0737, Test L1 Norm: 0.0509, Train Linf Norm: 2.2632, Test Linf Norm: 1.1633\n",
            "Epoch 13: Train Loss: 0.0419, Test Loss: 0.0426, Train L1 Norm: 0.1168, Test L1 Norm: 0.0627, Train Linf Norm: 5.9062, Test Linf Norm: 1.0595\n",
            "Epoch 14: Train Loss: 0.0427, Test Loss: 0.0393, Train L1 Norm: 0.0792, Test L1 Norm: 0.1433, Train Linf Norm: 2.7971, Test Linf Norm: 7.9379\n",
            "Epoch 15: Train Loss: 0.0403, Test Loss: 0.0317, Train L1 Norm: 0.0676, Test L1 Norm: 0.0400, Train Linf Norm: 2.0751, Test Linf Norm: 0.9734\n",
            "Epoch 16: Train Loss: 0.0415, Test Loss: 0.0700, Train L1 Norm: 0.1030, Test L1 Norm: 0.0893, Train Linf Norm: 4.7864, Test Linf Norm: 3.1223\n",
            "Epoch 17: Train Loss: 0.0413, Test Loss: 0.0247, Train L1 Norm: 0.0823, Test L1 Norm: 0.0431, Train Linf Norm: 2.9839, Test Linf Norm: 1.2746\n",
            "Epoch 18: Train Loss: 0.0417, Test Loss: 0.0346, Train L1 Norm: 0.0865, Test L1 Norm: 0.0527, Train Linf Norm: 3.3483, Test Linf Norm: 1.8688\n",
            "Epoch 19: Train Loss: 0.0421, Test Loss: 0.0279, Train L1 Norm: 0.0803, Test L1 Norm: 0.0402, Train Linf Norm: 2.8652, Test Linf Norm: 1.1083\n",
            "Epoch 20: Train Loss: 0.0436, Test Loss: 0.0337, Train L1 Norm: 0.0986, Test L1 Norm: 0.0562, Train Linf Norm: 4.3614, Test Linf Norm: 0.9434\n",
            "Epoch 21: Train Loss: 0.0385, Test Loss: 0.0266, Train L1 Norm: 0.0684, Test L1 Norm: 0.0373, Train Linf Norm: 2.1428, Test Linf Norm: 1.2395\n",
            "Epoch 22: Train Loss: 0.0427, Test Loss: 0.0321, Train L1 Norm: 0.1130, Test L1 Norm: 0.1053, Train Linf Norm: 5.6212, Test Linf Norm: 4.0358\n",
            "Epoch 23: Train Loss: 0.0438, Test Loss: 0.0316, Train L1 Norm: 0.0907, Test L1 Norm: 0.0396, Train Linf Norm: 3.5718, Test Linf Norm: 0.7966\n",
            "Epoch 24: Train Loss: 0.0399, Test Loss: 0.0275, Train L1 Norm: 0.0909, Test L1 Norm: 0.0496, Train Linf Norm: 3.8584, Test Linf Norm: 0.8658\n",
            "Epoch 25: Train Loss: 0.0411, Test Loss: 0.0351, Train L1 Norm: 0.1095, Test L1 Norm: 0.0500, Train Linf Norm: 5.1352, Test Linf Norm: 1.5103\n",
            "Epoch 26: Train Loss: 0.0174, Test Loss: 0.0161, Train L1 Norm: 0.0412, Test L1 Norm: 0.0347, Train Linf Norm: 1.5350, Test Linf Norm: 1.2120\n",
            "Epoch 27: Train Loss: 0.0169, Test Loss: 0.0171, Train L1 Norm: 0.0414, Test L1 Norm: 0.0374, Train Linf Norm: 1.5576, Test Linf Norm: 0.8764\n",
            "Epoch 28: Train Loss: 0.0174, Test Loss: 0.0194, Train L1 Norm: 0.0439, Test L1 Norm: 0.0347, Train Linf Norm: 1.7513, Test Linf Norm: 1.2193\n",
            "Epoch 29: Train Loss: 0.0170, Test Loss: 0.0175, Train L1 Norm: 0.0419, Test L1 Norm: 0.0361, Train Linf Norm: 1.6385, Test Linf Norm: 0.8894\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:10:49,621]\u001b[0m Trial 43 finished with value: 0.029252002319321038 and parameters: {'n_layers': 2, 'n_units_0': 152, 'n_units_1': 124, 'hidden_activation': 'Tanh', 'output_activation': 'ReLU', 'loss': 'MAE', 'optimizer': 'Adam', 'lr': 0.0008486626283065677, 'batch_size': 84, 'n_epochs': 30, 'scheduler': 'ReduceLROnPlateau', 'weight_decay': 0.0022364186533141268, 'beta1': 0.9142873081214863, 'beta2': 0.9164115617105812, 'factor': 0.30160592465697933, 'patience': 7, 'threshold': 0.005689486152867347}. Best is trial 10 with value: 0.006550250181416049.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Train Loss: 0.0162, Test Loss: 0.0130, Train L1 Norm: 0.0440, Test L1 Norm: 0.0293, Train Linf Norm: 1.8274, Test Linf Norm: 0.8190\n",
            "Epoch 1: Train Loss: 0.1073, Test Loss: 0.0677, Train L1 Norm: 0.2290, Test L1 Norm: 0.0910, Train Linf Norm: 9.1261, Test Linf Norm: 3.2269\n",
            "Epoch 2: Train Loss: 0.0484, Test Loss: 0.0590, Train L1 Norm: 0.1159, Test L1 Norm: 0.0847, Train Linf Norm: 4.8489, Test Linf Norm: 3.2077\n",
            "Epoch 3: Train Loss: 0.0407, Test Loss: 0.0296, Train L1 Norm: 0.0997, Test L1 Norm: 0.0933, Train Linf Norm: 4.1328, Test Linf Norm: 4.2820\n",
            "Epoch 4: Train Loss: 0.0350, Test Loss: 0.0373, Train L1 Norm: 0.0912, Test L1 Norm: 0.0943, Train Linf Norm: 3.9204, Test Linf Norm: 4.2927\n",
            "Epoch 5: Train Loss: 0.0321, Test Loss: 0.0458, Train L1 Norm: 0.0813, Test L1 Norm: 0.1799, Train Linf Norm: 3.4153, Test Linf Norm: 8.1850\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:11:10,748]\u001b[0m Trial 44 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Train Loss: 0.0303, Test Loss: 0.0096, Train L1 Norm: 0.0806, Test L1 Norm: 0.0962, Train Linf Norm: 3.4718, Test Linf Norm: 4.7948\n",
            "Epoch 1: Train Loss: 0.0419, Test Loss: 0.0113, Train L1 Norm: 0.1217, Test L1 Norm: 0.1298, Train Linf Norm: 5.1042, Test Linf Norm: 7.0782\n",
            "Epoch 2: Train Loss: 0.0004, Test Loss: 0.0001, Train L1 Norm: 0.0243, Test L1 Norm: 0.0123, Train Linf Norm: 1.0140, Test Linf Norm: 0.3506\n",
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0172, Test L1 Norm: 0.0110, Train Linf Norm: 0.6778, Test Linf Norm: 0.3247\n",
            "Epoch 4: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0155, Test L1 Norm: 0.0099, Train Linf Norm: 0.6660, Test Linf Norm: 0.2937\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0143, Test L1 Norm: 0.0097, Train Linf Norm: 0.6427, Test Linf Norm: 0.2883\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0129, Test L1 Norm: 0.0092, Train Linf Norm: 0.5715, Test Linf Norm: 0.2869\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0120, Test L1 Norm: 0.0085, Train Linf Norm: 0.5176, Test Linf Norm: 0.2577\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0112, Test L1 Norm: 0.0083, Train Linf Norm: 0.4837, Test Linf Norm: 0.2696\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0081, Train Linf Norm: 0.4288, Test Linf Norm: 0.2680\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0107, Test L1 Norm: 0.0078, Train Linf Norm: 0.4673, Test Linf Norm: 0.2469\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0077, Train Linf Norm: 0.4390, Test Linf Norm: 0.2458\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0105, Test L1 Norm: 0.0077, Train Linf Norm: 0.4495, Test Linf Norm: 0.2469\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0077, Train Linf Norm: 0.4430, Test Linf Norm: 0.2477\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0077, Train Linf Norm: 0.4476, Test Linf Norm: 0.2477\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0103, Test L1 Norm: 0.0077, Train Linf Norm: 0.4368, Test Linf Norm: 0.2477\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0105, Test L1 Norm: 0.0076, Train Linf Norm: 0.4517, Test Linf Norm: 0.2452\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0105, Test L1 Norm: 0.0078, Train Linf Norm: 0.4572, Test Linf Norm: 0.2489\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0106, Test L1 Norm: 0.0078, Train Linf Norm: 0.4661, Test Linf Norm: 0.2493\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0108, Test L1 Norm: 0.0076, Train Linf Norm: 0.4886, Test Linf Norm: 0.2480\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0107, Test L1 Norm: 0.0075, Train Linf Norm: 0.4896, Test Linf Norm: 0.2412\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0103, Test L1 Norm: 0.0075, Train Linf Norm: 0.4548, Test Linf Norm: 0.2520\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0110, Test L1 Norm: 0.0073, Train Linf Norm: 0.5198, Test Linf Norm: 0.2473\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0110, Test L1 Norm: 0.0073, Train Linf Norm: 0.5166, Test Linf Norm: 0.2315\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0101, Test L1 Norm: 0.0066, Train Linf Norm: 0.4555, Test Linf Norm: 0.2110\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0077, Train Linf Norm: 0.4300, Test Linf Norm: 0.2737\n",
            "Epoch 26: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0107, Test L1 Norm: 0.0064, Train Linf Norm: 0.4180, Test Linf Norm: 0.2076\n",
            "Epoch 27: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0111, Test L1 Norm: 0.0062, Train Linf Norm: 0.4807, Test Linf Norm: 0.2046\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0096, Test L1 Norm: 0.0060, Train Linf Norm: 0.4268, Test Linf Norm: 0.1955\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0062, Train Linf Norm: 0.3718, Test Linf Norm: 0.2284\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0079, Test L1 Norm: 0.0054, Train Linf Norm: 0.3408, Test Linf Norm: 0.1719\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0057, Train Linf Norm: 0.3228, Test Linf Norm: 0.1805\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0054, Train Linf Norm: 0.3519, Test Linf Norm: 0.1751\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0058, Train Linf Norm: 0.3313, Test Linf Norm: 0.1923\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0079, Test L1 Norm: 0.0053, Train Linf Norm: 0.3577, Test Linf Norm: 0.1695\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0053, Train Linf Norm: 0.3368, Test Linf Norm: 0.1708\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0054, Train Linf Norm: 0.3292, Test Linf Norm: 0.1717\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0051, Train Linf Norm: 0.3218, Test Linf Norm: 0.1650\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0053, Train Linf Norm: 0.3272, Test Linf Norm: 0.1693\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:12:57,315]\u001b[0m Trial 45 finished with value: 0.0052774796700803565 and parameters: {'n_layers': 2, 'n_units_0': 967, 'n_units_1': 474, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.0011056016481593817, 'batch_size': 83, 'n_epochs': 39, 'scheduler': 'CosineAnnealingLR', 'T_max': 13}. Best is trial 45 with value: 0.0052774796700803565.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0053, Train Linf Norm: 0.3255, Test Linf Norm: 0.1687\n",
            "Epoch 1: Train Loss: 0.0495, Test Loss: 0.0042, Train L1 Norm: 0.3538, Test L1 Norm: 0.0891, Train Linf Norm: 15.6917, Test Linf Norm: 1.1967\n",
            "Epoch 2: Train Loss: 0.0028, Test Loss: 0.0020, Train L1 Norm: 0.1167, Test L1 Norm: 0.0757, Train Linf Norm: 4.5376, Test Linf Norm: 2.6445\n",
            "Epoch 3: Train Loss: 0.0016, Test Loss: 0.0013, Train L1 Norm: 0.1064, Test L1 Norm: 0.0541, Train Linf Norm: 5.2138, Test Linf Norm: 1.5223\n",
            "Epoch 4: Train Loss: 0.0012, Test Loss: 0.0011, Train L1 Norm: 0.0582, Test L1 Norm: 0.0417, Train Linf Norm: 1.9110, Test Linf Norm: 0.8571\n",
            "Epoch 5: Train Loss: 0.0010, Test Loss: 0.0010, Train L1 Norm: 0.0531, Test L1 Norm: 0.0405, Train Linf Norm: 1.6249, Test Linf Norm: 0.8523\n",
            "Epoch 6: Train Loss: 0.0008, Test Loss: 0.0008, Train L1 Norm: 0.0528, Test L1 Norm: 0.0393, Train Linf Norm: 1.6938, Test Linf Norm: 0.8625\n",
            "Epoch 7: Train Loss: 0.0008, Test Loss: 0.0007, Train L1 Norm: 0.0532, Test L1 Norm: 0.0382, Train Linf Norm: 1.8084, Test Linf Norm: 0.8606\n",
            "Epoch 8: Train Loss: 0.0007, Test Loss: 0.0007, Train L1 Norm: 0.0520, Test L1 Norm: 0.0375, Train Linf Norm: 1.7788, Test Linf Norm: 0.8576\n",
            "Epoch 9: Train Loss: 0.0007, Test Loss: 0.0007, Train L1 Norm: 0.0526, Test L1 Norm: 0.0369, Train Linf Norm: 1.8706, Test Linf Norm: 0.8500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:13:19,587]\u001b[0m Trial 46 finished with value: 0.03663151956349611 and parameters: {'n_layers': 1, 'n_units_0': 1037, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.0005938428827348165, 'batch_size': 80, 'n_epochs': 10, 'scheduler': 'CosineAnnealingLR', 'T_max': 13}. Best is trial 45 with value: 0.0052774796700803565.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Train Loss: 0.0006, Test Loss: 0.0006, Train L1 Norm: 0.0524, Test L1 Norm: 0.0366, Train Linf Norm: 1.8720, Test Linf Norm: 0.8488\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:13:25,156]\u001b[0m Trial 47 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0227, Test Loss: 0.0052, Train L1 Norm: 2.5423, Test L1 Norm: 1.8282, Train Linf Norm: 83.5919, Test Linf Norm: 61.2226\n",
            "Epoch 1: Train Loss: 0.0079, Test Loss: 0.0000, Train L1 Norm: 0.0677, Test L1 Norm: 0.0167, Train Linf Norm: 1.0187, Test Linf Norm: 0.2470\n",
            "Epoch 2: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0168, Test L1 Norm: 0.0220, Train Linf Norm: 0.2572, Test Linf Norm: 0.3651\n",
            "Epoch 3: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0111, Test L1 Norm: 0.0357, Train Linf Norm: 0.1642, Test Linf Norm: 0.6204\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0182, Train Linf Norm: 0.1242, Test Linf Norm: 0.3112\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0158, Train Linf Norm: 0.1082, Test Linf Norm: 0.2671\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0180, Train Linf Norm: 0.0984, Test Linf Norm: 0.3121\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0188, Train Linf Norm: 0.0935, Test Linf Norm: 0.3279\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0191, Train Linf Norm: 0.0861, Test Linf Norm: 0.3344\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0150, Train Linf Norm: 0.0847, Test Linf Norm: 0.2563\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0181, Train Linf Norm: 0.0813, Test Linf Norm: 0.3182\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0176, Train Linf Norm: 0.0807, Test Linf Norm: 0.3094\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0180, Train Linf Norm: 0.0779, Test Linf Norm: 0.3167\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0176, Train Linf Norm: 0.0776, Test Linf Norm: 0.3097\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0171, Train Linf Norm: 0.0769, Test Linf Norm: 0.3004\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0171, Train Linf Norm: 0.0765, Test Linf Norm: 0.3009\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0171, Train Linf Norm: 0.0752, Test Linf Norm: 0.3009\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0173, Train Linf Norm: 0.0762, Test Linf Norm: 0.3041\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0169, Train Linf Norm: 0.0761, Test Linf Norm: 0.2958\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0176, Train Linf Norm: 0.0751, Test Linf Norm: 0.3108\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0164, Train Linf Norm: 0.0757, Test Linf Norm: 0.2875\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0199, Train Linf Norm: 0.0773, Test Linf Norm: 0.3533\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0168, Train Linf Norm: 0.0752, Test Linf Norm: 0.2939\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0194, Train Linf Norm: 0.0757, Test Linf Norm: 0.3455\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0192, Train Linf Norm: 0.0703, Test Linf Norm: 0.3407\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0193, Train Linf Norm: 0.0727, Test Linf Norm: 0.3420\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0143, Train Linf Norm: 0.0729, Test Linf Norm: 0.2285\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0176, Train Linf Norm: 0.0751, Test Linf Norm: 0.3122\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0178, Train Linf Norm: 0.0783, Test Linf Norm: 0.3126\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0194, Train Linf Norm: 0.0712, Test Linf Norm: 0.3467\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0163, Train Linf Norm: 0.0725, Test Linf Norm: 0.2866\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0055, Test L1 Norm: 0.0091, Train Linf Norm: 0.0801, Test Linf Norm: 0.0861\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0156, Train Linf Norm: 0.0765, Test Linf Norm: 0.2756\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0160, Train Linf Norm: 0.0803, Test Linf Norm: 0.2829\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0152, Train Linf Norm: 0.0702, Test Linf Norm: 0.2685\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0127, Train Linf Norm: 0.0622, Test Linf Norm: 0.2174\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0159, Train Linf Norm: 0.0680, Test Linf Norm: 0.2836\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0138, Train Linf Norm: 0.0627, Test Linf Norm: 0.2421\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0158, Train Linf Norm: 0.0634, Test Linf Norm: 0.2807\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0136, Train Linf Norm: 0.0618, Test Linf Norm: 0.2397\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0133, Train Linf Norm: 0.0621, Test Linf Norm: 0.2329\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0150, Train Linf Norm: 0.0628, Test Linf Norm: 0.2672\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0143, Train Linf Norm: 0.0614, Test Linf Norm: 0.2534\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0148, Train Linf Norm: 0.0611, Test Linf Norm: 0.2619\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:21:28,373]\u001b[0m Trial 48 finished with value: 0.01469709746291337 and parameters: {'n_layers': 3, 'n_units_0': 877, 'n_units_1': 462, 'n_units_2': 635, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.0010302043428068186, 'batch_size': 19, 'n_epochs': 44, 'scheduler': 'CosineAnnealingLR', 'T_max': 15}. Best is trial 45 with value: 0.0052774796700803565.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0147, Train Linf Norm: 0.0606, Test Linf Norm: 0.2609\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:21:37,800]\u001b[0m Trial 49 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.8117, Test Loss: 0.0004, Train L1 Norm: 0.4207, Test L1 Norm: 0.0974, Train Linf Norm: 13.9830, Test Linf Norm: 13.0813\n",
            "Epoch 2: Train Loss: 0.0010, Test Loss: 0.0001, Train L1 Norm: 0.0500, Test L1 Norm: 0.0425, Train Linf Norm: 4.7956, Test Linf Norm: 5.3122\n",
            "Epoch 3: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0272, Test L1 Norm: 0.0135, Train Linf Norm: 2.4860, Test Linf Norm: 0.9807\n",
            "Epoch 4: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0221, Test L1 Norm: 0.0118, Train Linf Norm: 2.2011, Test Linf Norm: 0.6757\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0160, Test L1 Norm: 0.0092, Train Linf Norm: 1.6017, Test Linf Norm: 0.6342\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0140, Test L1 Norm: 0.0156, Train Linf Norm: 1.4194, Test Linf Norm: 1.7825\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0131, Test L1 Norm: 0.0131, Train Linf Norm: 1.3570, Test Linf Norm: 1.4307\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0116, Train Linf Norm: 1.2596, Test Linf Norm: 1.2013\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0117, Test L1 Norm: 0.0114, Train Linf Norm: 1.1919, Test Linf Norm: 1.2063\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0111, Test L1 Norm: 0.0110, Train Linf Norm: 1.1255, Test Linf Norm: 1.1638\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0107, Test L1 Norm: 0.0120, Train Linf Norm: 1.0740, Test Linf Norm: 1.3419\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0106, Test L1 Norm: 0.0113, Train Linf Norm: 1.0725, Test Linf Norm: 1.2302\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0105, Test L1 Norm: 0.0113, Train Linf Norm: 1.0550, Test Linf Norm: 1.2431\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0106, Test L1 Norm: 0.0112, Train Linf Norm: 1.0717, Test Linf Norm: 1.2264\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0105, Test L1 Norm: 0.0112, Train Linf Norm: 1.0561, Test Linf Norm: 1.2264\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0105, Test L1 Norm: 0.0112, Train Linf Norm: 1.0467, Test Linf Norm: 1.2307\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0105, Test L1 Norm: 0.0109, Train Linf Norm: 1.0595, Test Linf Norm: 1.1792\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0105, Train Linf Norm: 1.0372, Test Linf Norm: 1.1274\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0103, Train Linf Norm: 1.0520, Test Linf Norm: 1.0857\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0102, Test L1 Norm: 0.0113, Train Linf Norm: 1.0304, Test Linf Norm: 1.2675\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0111, Train Linf Norm: 0.9938, Test Linf Norm: 1.2540\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0103, Test L1 Norm: 0.0103, Train Linf Norm: 1.0583, Test Linf Norm: 1.1229\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0092, Test L1 Norm: 0.0097, Train Linf Norm: 0.8999, Test Linf Norm: 1.0699\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0089, Test L1 Norm: 0.0078, Train Linf Norm: 0.8650, Test Linf Norm: 0.7695\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0113, Train Linf Norm: 0.9799, Test Linf Norm: 1.3469\n",
            "Epoch 26: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0120, Test L1 Norm: 0.0173, Train Linf Norm: 1.0542, Test Linf Norm: 2.1239\n",
            "Epoch 27: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0146, Test L1 Norm: 0.0064, Train Linf Norm: 1.3247, Test Linf Norm: 0.3595\n",
            "Epoch 28: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0142, Test L1 Norm: 0.0282, Train Linf Norm: 1.1627, Test Linf Norm: 3.7136\n",
            "Epoch 29: Train Loss: 0.0003, Test Loss: 0.0000, Train L1 Norm: 0.0129, Test L1 Norm: 0.0088, Train Linf Norm: 1.1965, Test Linf Norm: 0.8439\n",
            "Epoch 30: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0113, Test L1 Norm: 0.0064, Train Linf Norm: 1.0749, Test Linf Norm: 0.5938\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0057, Train Linf Norm: 0.6559, Test Linf Norm: 0.5264\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0057, Train Linf Norm: 0.6256, Test Linf Norm: 0.4949\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0074, Train Linf Norm: 0.6527, Test Linf Norm: 0.8140\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0059, Train Linf Norm: 0.5898, Test Linf Norm: 0.5821\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0068, Train Linf Norm: 0.6047, Test Linf Norm: 0.7269\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:22:45,200]\u001b[0m Trial 50 finished with value: 0.006401583661185577 and parameters: {'n_layers': 3, 'n_units_0': 935, 'n_units_1': 625, 'n_units_2': 554, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.0010771614166728628, 'batch_size': 164, 'n_epochs': 36, 'scheduler': 'CosineAnnealingLR', 'T_max': 14}. Best is trial 45 with value: 0.0052774796700803565.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0064, Train Linf Norm: 0.5678, Test Linf Norm: 0.6673\n",
            "Epoch 1: Train Loss: 0.0605, Test Loss: 0.0002, Train L1 Norm: 0.2332, Test L1 Norm: 0.0639, Train Linf Norm: 23.3872, Test Linf Norm: 8.6501\n",
            "Epoch 2: Train Loss: 0.0004, Test Loss: 0.0001, Train L1 Norm: 0.0286, Test L1 Norm: 0.0422, Train Linf Norm: 2.5582, Test Linf Norm: 5.8238\n",
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0170, Test L1 Norm: 0.0137, Train Linf Norm: 1.5875, Test Linf Norm: 1.2239\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0136, Test L1 Norm: 0.0105, Train Linf Norm: 1.2773, Test Linf Norm: 0.9448\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0107, Test L1 Norm: 0.0125, Train Linf Norm: 0.9452, Test Linf Norm: 1.3114\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0100, Train Linf Norm: 0.9045, Test Linf Norm: 0.9717\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0093, Test L1 Norm: 0.0106, Train Linf Norm: 0.8316, Test Linf Norm: 1.1301\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0091, Train Linf Norm: 0.7887, Test Linf Norm: 0.9109\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0094, Train Linf Norm: 0.6959, Test Linf Norm: 0.9721\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0100, Train Linf Norm: 0.6994, Test Linf Norm: 1.0751\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0105, Train Linf Norm: 0.6563, Test Linf Norm: 1.1768\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0092, Train Linf Norm: 0.6772, Test Linf Norm: 0.9635\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0096, Train Linf Norm: 0.6586, Test Linf Norm: 1.0279\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0094, Train Linf Norm: 0.6478, Test Linf Norm: 0.9945\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0094, Train Linf Norm: 0.6451, Test Linf Norm: 0.9945\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0092, Train Linf Norm: 0.6472, Test Linf Norm: 0.9722\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0089, Train Linf Norm: 0.6617, Test Linf Norm: 0.9242\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0096, Train Linf Norm: 0.6296, Test Linf Norm: 1.0343\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0099, Train Linf Norm: 0.6270, Test Linf Norm: 1.0882\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0081, Train Linf Norm: 0.6400, Test Linf Norm: 0.7884\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0104, Train Linf Norm: 0.6753, Test Linf Norm: 1.1912\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0073, Test L1 Norm: 0.0090, Train Linf Norm: 0.6239, Test Linf Norm: 0.9712\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0109, Train Linf Norm: 0.6139, Test Linf Norm: 1.2914\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0100, Train Linf Norm: 0.5776, Test Linf Norm: 1.1515\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0109, Train Linf Norm: 0.5694, Test Linf Norm: 1.3260\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0091, Test L1 Norm: 0.0104, Train Linf Norm: 0.8112, Test Linf Norm: 1.2351\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0088, Test L1 Norm: 0.0093, Train Linf Norm: 0.7320, Test Linf Norm: 1.0681\n",
            "Epoch 28: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0118, Test L1 Norm: 0.0098, Train Linf Norm: 0.9256, Test Linf Norm: 1.1297\n",
            "Epoch 29: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0108, Test L1 Norm: 0.0093, Train Linf Norm: 0.7313, Test Linf Norm: 1.0650\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0204, Train Linf Norm: 0.5764, Test Linf Norm: 2.6649\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0135, Train Linf Norm: 0.4863, Test Linf Norm: 1.7498\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0082, Train Linf Norm: 0.4985, Test Linf Norm: 0.9004\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0109, Train Linf Norm: 0.4474, Test Linf Norm: 1.3937\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0102, Train Linf Norm: 0.4728, Test Linf Norm: 1.2894\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0118, Train Linf Norm: 0.4301, Test Linf Norm: 1.5599\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0094, Train Linf Norm: 0.4382, Test Linf Norm: 1.1722\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0101, Train Linf Norm: 0.4428, Test Linf Norm: 1.2917\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0107, Train Linf Norm: 0.4141, Test Linf Norm: 1.3865\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0104, Train Linf Norm: 0.4102, Test Linf Norm: 1.3545\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0105, Train Linf Norm: 0.4207, Test Linf Norm: 1.3590\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0105, Train Linf Norm: 0.4116, Test Linf Norm: 1.3745\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0105, Train Linf Norm: 0.4080, Test Linf Norm: 1.3641\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0105, Train Linf Norm: 0.3998, Test Linf Norm: 1.3641\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0105, Train Linf Norm: 0.4087, Test Linf Norm: 1.3753\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0107, Train Linf Norm: 0.3979, Test Linf Norm: 1.3934\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0109, Train Linf Norm: 0.4144, Test Linf Norm: 1.4325\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0105, Train Linf Norm: 0.4169, Test Linf Norm: 1.3639\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0107, Train Linf Norm: 0.4050, Test Linf Norm: 1.4109\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0107, Train Linf Norm: 0.4107, Test Linf Norm: 1.4040\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0108, Train Linf Norm: 0.3974, Test Linf Norm: 1.4218\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:24:20,394]\u001b[0m Trial 51 finished with value: 0.010646515850606374 and parameters: {'n_layers': 3, 'n_units_0': 942, 'n_units_1': 721, 'n_units_2': 563, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.0010404958168812087, 'batch_size': 166, 'n_epochs': 51, 'scheduler': 'CosineAnnealingLR', 'T_max': 14}. Best is trial 45 with value: 0.0052774796700803565.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0106, Train Linf Norm: 0.4395, Test Linf Norm: 1.3972\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:24:22,172]\u001b[0m Trial 52 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0623, Test Loss: 0.0005, Train L1 Norm: 0.2533, Test L1 Norm: 0.1348, Train Linf Norm: 26.0134, Test Linf Norm: 20.3988\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:24:24,088]\u001b[0m Trial 53 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0566, Test Loss: 0.0007, Train L1 Norm: 0.2205, Test L1 Norm: 0.1410, Train Linf Norm: 25.1584, Test Linf Norm: 23.2343\n",
            "Epoch 1: Train Loss: 0.0529, Test Loss: 0.0002, Train L1 Norm: 0.1913, Test L1 Norm: 0.0313, Train Linf Norm: 17.3522, Test Linf Norm: 2.8182\n",
            "Epoch 2: Train Loss: 0.0008, Test Loss: 0.0001, Train L1 Norm: 0.0358, Test L1 Norm: 0.0306, Train Linf Norm: 3.2032, Test Linf Norm: 3.4582\n",
            "Epoch 3: Train Loss: 0.0002, Test Loss: 0.0006, Train L1 Norm: 0.0225, Test L1 Norm: 0.0171, Train Linf Norm: 2.0458, Test Linf Norm: 0.5152\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0157, Test L1 Norm: 0.0123, Train Linf Norm: 1.4540, Test Linf Norm: 0.9424\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0135, Test L1 Norm: 0.0100, Train Linf Norm: 1.2582, Test Linf Norm: 0.7951\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0082, Train Linf Norm: 1.1280, Test Linf Norm: 0.5736\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0107, Test L1 Norm: 0.0075, Train Linf Norm: 0.9824, Test Linf Norm: 0.5242\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0103, Test L1 Norm: 0.0072, Train Linf Norm: 0.9416, Test Linf Norm: 0.5315\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0102, Test L1 Norm: 0.0070, Train Linf Norm: 0.9613, Test Linf Norm: 0.5144\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0095, Test L1 Norm: 0.0067, Train Linf Norm: 0.8708, Test Linf Norm: 0.4768\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0095, Test L1 Norm: 0.0071, Train Linf Norm: 0.8679, Test Linf Norm: 0.5360\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0066, Train Linf Norm: 0.8715, Test Linf Norm: 0.4612\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0093, Test L1 Norm: 0.0066, Train Linf Norm: 0.8501, Test Linf Norm: 0.4705\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0093, Test L1 Norm: 0.0066, Train Linf Norm: 0.8570, Test Linf Norm: 0.4716\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0093, Test L1 Norm: 0.0066, Train Linf Norm: 0.8470, Test Linf Norm: 0.4716\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0093, Test L1 Norm: 0.0067, Train Linf Norm: 0.8657, Test Linf Norm: 0.4714\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0092, Test L1 Norm: 0.0066, Train Linf Norm: 0.8509, Test Linf Norm: 0.4718\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0092, Test L1 Norm: 0.0068, Train Linf Norm: 0.8576, Test Linf Norm: 0.4943\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0092, Test L1 Norm: 0.0070, Train Linf Norm: 0.8578, Test Linf Norm: 0.5344\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0092, Test L1 Norm: 0.0065, Train Linf Norm: 0.8590, Test Linf Norm: 0.4719\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0089, Test L1 Norm: 0.0065, Train Linf Norm: 0.8358, Test Linf Norm: 0.4867\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0090, Test L1 Norm: 0.0063, Train Linf Norm: 0.7832, Test Linf Norm: 0.4655\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0088, Test L1 Norm: 0.0061, Train Linf Norm: 0.8157, Test Linf Norm: 0.4287\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0084, Test L1 Norm: 0.0057, Train Linf Norm: 0.7724, Test Linf Norm: 0.3954\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0060, Train Linf Norm: 0.7165, Test Linf Norm: 0.4505\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0092, Test L1 Norm: 0.0058, Train Linf Norm: 0.7920, Test Linf Norm: 0.4232\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0003, Train L1 Norm: 0.0088, Test L1 Norm: 0.0401, Train Linf Norm: 0.7328, Test Linf Norm: 4.6364\n",
            "Epoch 28: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0142, Test L1 Norm: 0.0062, Train Linf Norm: 1.2060, Test Linf Norm: 0.4930\n",
            "Epoch 29: Train Loss: 0.0003, Test Loss: 0.0000, Train L1 Norm: 0.0136, Test L1 Norm: 0.0052, Train Linf Norm: 1.1435, Test Linf Norm: 0.3323\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0066, Train Linf Norm: 0.7179, Test Linf Norm: 0.4976\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0048, Train Linf Norm: 0.6230, Test Linf Norm: 0.3056\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0049, Train Linf Norm: 0.5548, Test Linf Norm: 0.3007\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0044, Train Linf Norm: 0.5800, Test Linf Norm: 0.2884\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0044, Train Linf Norm: 0.5927, Test Linf Norm: 0.2901\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0041, Train Linf Norm: 0.5075, Test Linf Norm: 0.2529\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0041, Train Linf Norm: 0.5165, Test Linf Norm: 0.2545\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:25:36,799]\u001b[0m Trial 54 finished with value: 0.003927992188790813 and parameters: {'n_layers': 3, 'n_units_0': 883, 'n_units_1': 672, 'n_units_2': 600, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.0010753909568587674, 'batch_size': 153, 'n_epochs': 37, 'scheduler': 'CosineAnnealingLR', 'T_max': 14}. Best is trial 54 with value: 0.003927992188790813.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0039, Train Linf Norm: 0.4969, Test Linf Norm: 0.2352\n",
            "Epoch 1: Train Loss: 0.0544, Test Loss: 0.0003, Train L1 Norm: 0.2118, Test L1 Norm: 0.1130, Train Linf Norm: 19.4225, Test Linf Norm: 15.8625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:25:40,409]\u001b[0m Trial 55 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Train Loss: 0.0005, Test Loss: 0.0002, Train L1 Norm: 0.0468, Test L1 Norm: 0.0969, Train Linf Norm: 5.0214, Test Linf Norm: 13.9903\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:25:42,416]\u001b[0m Trial 56 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.0616, Test Loss: 0.0005, Train L1 Norm: 0.2331, Test L1 Norm: 0.0770, Train Linf Norm: 25.2861, Test Linf Norm: 11.0905\n",
            "Epoch 2: Train Loss: 0.0007, Test Loss: 0.0005, Train L1 Norm: 0.0437, Test L1 Norm: 0.0633, Train Linf Norm: 4.8155, Test Linf Norm: 9.3699\n",
            "Epoch 3: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0308, Test L1 Norm: 0.0302, Train Linf Norm: 3.7550, Test Linf Norm: 4.1876\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0226, Test L1 Norm: 0.0168, Train Linf Norm: 2.8632, Test Linf Norm: 2.0142\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0186, Test L1 Norm: 0.0096, Train Linf Norm: 2.2703, Test Linf Norm: 0.7859\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0138, Test L1 Norm: 0.0082, Train Linf Norm: 1.6040, Test Linf Norm: 0.6514\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0119, Test L1 Norm: 0.0085, Train Linf Norm: 1.3127, Test Linf Norm: 0.7178\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0111, Test L1 Norm: 0.0075, Train Linf Norm: 1.1977, Test Linf Norm: 0.5761\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0107, Test L1 Norm: 0.0075, Train Linf Norm: 1.1608, Test Linf Norm: 0.5731\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0075, Train Linf Norm: 1.0876, Test Linf Norm: 0.6033\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0102, Test L1 Norm: 0.0072, Train Linf Norm: 1.0634, Test Linf Norm: 0.5552\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0072, Train Linf Norm: 1.0305, Test Linf Norm: 0.5645\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0101, Test L1 Norm: 0.0072, Train Linf Norm: 1.0982, Test Linf Norm: 0.5645\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0101, Test L1 Norm: 0.0072, Train Linf Norm: 1.0578, Test Linf Norm: 0.5648\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0072, Train Linf Norm: 1.0617, Test Linf Norm: 0.5605\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0071, Train Linf Norm: 1.0051, Test Linf Norm: 0.5479\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0098, Test L1 Norm: 0.0070, Train Linf Norm: 1.0462, Test Linf Norm: 0.5378\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0070, Train Linf Norm: 1.0864, Test Linf Norm: 0.5981\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0097, Test L1 Norm: 0.0065, Train Linf Norm: 1.0545, Test Linf Norm: 0.4584\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0089, Test L1 Norm: 0.0071, Train Linf Norm: 0.9235, Test Linf Norm: 0.6183\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0090, Test L1 Norm: 0.0056, Train Linf Norm: 0.9467, Test Linf Norm: 0.4032\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0103, Test L1 Norm: 0.0067, Train Linf Norm: 1.0422, Test Linf Norm: 0.5798\n",
            "Epoch 23: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0172, Test L1 Norm: 0.0054, Train Linf Norm: 1.8705, Test Linf Norm: 0.3808\n",
            "Epoch 24: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0167, Test L1 Norm: 0.0078, Train Linf Norm: 1.6942, Test Linf Norm: 0.7754\n",
            "Epoch 25: Train Loss: 0.0002, Test Loss: 0.0006, Train L1 Norm: 0.0149, Test L1 Norm: 0.0493, Train Linf Norm: 1.5460, Test Linf Norm: 6.7905\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0131, Test L1 Norm: 0.0056, Train Linf Norm: 1.5377, Test Linf Norm: 0.3970\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0084, Test L1 Norm: 0.0059, Train Linf Norm: 0.9258, Test Linf Norm: 0.5332\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0047, Train Linf Norm: 0.7922, Test Linf Norm: 0.3282\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0051, Train Linf Norm: 0.6731, Test Linf Norm: 0.4131\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0051, Train Linf Norm: 0.6645, Test Linf Norm: 0.3856\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0045, Train Linf Norm: 0.6619, Test Linf Norm: 0.3334\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0043, Train Linf Norm: 0.6452, Test Linf Norm: 0.2997\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0045, Train Linf Norm: 0.6347, Test Linf Norm: 0.3411\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0043, Train Linf Norm: 0.6247, Test Linf Norm: 0.3092\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0042, Train Linf Norm: 0.6209, Test Linf Norm: 0.2961\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0042, Train Linf Norm: 0.6199, Test Linf Norm: 0.2902\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0042, Train Linf Norm: 0.6224, Test Linf Norm: 0.2902\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0042, Train Linf Norm: 0.6175, Test Linf Norm: 0.2907\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0042, Train Linf Norm: 0.6121, Test Linf Norm: 0.2959\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0042, Train Linf Norm: 0.6094, Test Linf Norm: 0.3006\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0042, Train Linf Norm: 0.6192, Test Linf Norm: 0.3005\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0041, Train Linf Norm: 0.6120, Test Linf Norm: 0.2890\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0043, Train Linf Norm: 0.5993, Test Linf Norm: 0.3151\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0042, Train Linf Norm: 0.6068, Test Linf Norm: 0.2912\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0040, Train Linf Norm: 0.6040, Test Linf Norm: 0.2746\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0042, Train Linf Norm: 0.5602, Test Linf Norm: 0.3139\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0043, Train Linf Norm: 0.5732, Test Linf Norm: 0.3196\n",
            "Epoch 48: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0108, Test L1 Norm: 0.0043, Train Linf Norm: 0.9044, Test Linf Norm: 0.2966\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0051, Train Linf Norm: 0.6555, Test Linf Norm: 0.3587\n",
            "Epoch 50: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0049, Train Linf Norm: 0.7247, Test Linf Norm: 0.3178\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0036, Train Linf Norm: 0.5530, Test Linf Norm: 0.2462\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0036, Train Linf Norm: 0.5069, Test Linf Norm: 0.2520\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:27:16,830]\u001b[0m Trial 57 finished with value: 0.0038909989546984432 and parameters: {'n_layers': 3, 'n_units_0': 755, 'n_units_1': 597, 'n_units_2': 489, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.0010640808820252497, 'batch_size': 194, 'n_epochs': 53, 'scheduler': 'CosineAnnealingLR', 'T_max': 12}. Best is trial 57 with value: 0.0038909989546984432.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0039, Train Linf Norm: 0.4635, Test Linf Norm: 0.2855\n",
            "Epoch 1: Train Loss: 0.0719, Test Loss: 0.0006, Train L1 Norm: 0.3843, Test L1 Norm: 0.0934, Train Linf Norm: 50.4285, Test Linf Norm: 13.7273\n",
            "Epoch 2: Train Loss: 0.0010, Test Loss: 0.0001, Train L1 Norm: 0.0549, Test L1 Norm: 0.0506, Train Linf Norm: 6.6409, Test Linf Norm: 7.3380\n",
            "Epoch 3: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0342, Test L1 Norm: 0.0727, Train Linf Norm: 4.2662, Test Linf Norm: 11.8979\n",
            "Epoch 4: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0235, Test L1 Norm: 0.0457, Train Linf Norm: 2.8160, Test Linf Norm: 7.2220\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0154, Test L1 Norm: 0.0464, Train Linf Norm: 1.4497, Test Linf Norm: 7.4757\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0151, Test L1 Norm: 0.0273, Train Linf Norm: 1.5786, Test Linf Norm: 3.9543\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0139, Test L1 Norm: 0.0285, Train Linf Norm: 1.3981, Test Linf Norm: 4.3023\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0131, Test L1 Norm: 0.0349, Train Linf Norm: 1.2986, Test Linf Norm: 5.5090\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0124, Test L1 Norm: 0.0282, Train Linf Norm: 1.2252, Test Linf Norm: 4.3080\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0119, Test L1 Norm: 0.0236, Train Linf Norm: 1.1407, Test Linf Norm: 3.4252\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0119, Test L1 Norm: 0.0263, Train Linf Norm: 1.1752, Test Linf Norm: 3.9329\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0117, Test L1 Norm: 0.0254, Train Linf Norm: 1.0993, Test Linf Norm: 3.7734\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0117, Test L1 Norm: 0.0254, Train Linf Norm: 1.1283, Test Linf Norm: 3.7734\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0117, Test L1 Norm: 0.0257, Train Linf Norm: 1.1270, Test Linf Norm: 3.8378\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0117, Test L1 Norm: 0.0239, Train Linf Norm: 1.1334, Test Linf Norm: 3.4999\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0117, Test L1 Norm: 0.0225, Train Linf Norm: 1.1318, Test Linf Norm: 3.2384\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0120, Test L1 Norm: 0.0234, Train Linf Norm: 1.2023, Test Linf Norm: 3.3869\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0115, Test L1 Norm: 0.0249, Train Linf Norm: 1.1294, Test Linf Norm: 3.6496\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0108, Test L1 Norm: 0.0259, Train Linf Norm: 1.0110, Test Linf Norm: 3.9360\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0108, Test L1 Norm: 0.0201, Train Linf Norm: 1.0274, Test Linf Norm: 2.8399\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0105, Test L1 Norm: 0.0213, Train Linf Norm: 0.9610, Test Linf Norm: 3.1071\n",
            "Epoch 22: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0130, Test L1 Norm: 0.0415, Train Linf Norm: 1.2701, Test Linf Norm: 6.6845\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:27:57,682]\u001b[0m Trial 58 finished with value: 0.014607222041347996 and parameters: {'n_layers': 3, 'n_units_0': 766, 'n_units_1': 602, 'n_units_2': 284, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.0015865099788077645, 'batch_size': 190, 'n_epochs': 23, 'scheduler': 'CosineAnnealingLR', 'T_max': 12}. Best is trial 57 with value: 0.0038909989546984432.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0133, Test L1 Norm: 0.0146, Train Linf Norm: 1.0449, Test Linf Norm: 1.8245\n",
            "Epoch 1: Train Loss: 0.0602, Test Loss: 0.0010, Train L1 Norm: 0.2152, Test L1 Norm: 0.1074, Train Linf Norm: 22.0938, Test Linf Norm: 17.2912\n",
            "Epoch 2: Train Loss: 0.0010, Test Loss: 0.0002, Train L1 Norm: 0.0520, Test L1 Norm: 0.0294, Train Linf Norm: 6.6235, Test Linf Norm: 3.8814\n",
            "Epoch 3: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0252, Test L1 Norm: 0.0216, Train Linf Norm: 2.8439, Test Linf Norm: 2.5428\n",
            "Epoch 4: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0188, Test L1 Norm: 0.0291, Train Linf Norm: 2.2169, Test Linf Norm: 4.7506\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0165, Test L1 Norm: 0.0304, Train Linf Norm: 1.9659, Test Linf Norm: 5.2110\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0149, Test L1 Norm: 0.0258, Train Linf Norm: 1.7719, Test Linf Norm: 4.3257\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0136, Test L1 Norm: 0.0238, Train Linf Norm: 1.6711, Test Linf Norm: 3.9323\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0131, Test L1 Norm: 0.0204, Train Linf Norm: 1.5302, Test Linf Norm: 3.2589\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0127, Test L1 Norm: 0.0195, Train Linf Norm: 1.5042, Test Linf Norm: 3.0717\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0197, Train Linf Norm: 1.3122, Test Linf Norm: 3.1465\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0124, Test L1 Norm: 0.0195, Train Linf Norm: 1.4936, Test Linf Norm: 3.0940\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0190, Train Linf Norm: 1.5137, Test Linf Norm: 2.9870\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0190, Train Linf Norm: 1.4840, Test Linf Norm: 2.9870\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0189, Train Linf Norm: 1.4644, Test Linf Norm: 2.9753\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0190, Train Linf Norm: 1.4939, Test Linf Norm: 2.9902\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0186, Train Linf Norm: 1.5054, Test Linf Norm: 2.9245\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0119, Test L1 Norm: 0.0174, Train Linf Norm: 1.4234, Test Linf Norm: 2.6686\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0118, Test L1 Norm: 0.0166, Train Linf Norm: 1.4493, Test Linf Norm: 2.5152\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0117, Test L1 Norm: 0.0171, Train Linf Norm: 1.4303, Test Linf Norm: 2.6359\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0112, Test L1 Norm: 0.0178, Train Linf Norm: 1.3268, Test Linf Norm: 2.8541\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0112, Test L1 Norm: 0.0119, Train Linf Norm: 1.2960, Test Linf Norm: 1.5534\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0118, Test L1 Norm: 0.0112, Train Linf Norm: 1.3177, Test Linf Norm: 1.4455\n",
            "Epoch 23: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0154, Test L1 Norm: 0.0164, Train Linf Norm: 1.6158, Test Linf Norm: 2.5827\n",
            "Epoch 24: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0180, Test L1 Norm: 0.0232, Train Linf Norm: 1.8593, Test Linf Norm: 3.9332\n",
            "Epoch 25: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0125, Test L1 Norm: 0.0230, Train Linf Norm: 1.3015, Test Linf Norm: 3.8763\n",
            "Epoch 26: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0134, Test L1 Norm: 0.0133, Train Linf Norm: 1.5675, Test Linf Norm: 1.9780\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0089, Test L1 Norm: 0.0118, Train Linf Norm: 1.0189, Test Linf Norm: 1.7019\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0089, Test L1 Norm: 0.0123, Train Linf Norm: 1.0671, Test Linf Norm: 1.8529\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0108, Train Linf Norm: 0.9454, Test Linf Norm: 1.5296\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0114, Train Linf Norm: 0.8879, Test Linf Norm: 1.6888\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0099, Train Linf Norm: 0.8897, Test Linf Norm: 1.3641\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0093, Train Linf Norm: 0.8557, Test Linf Norm: 1.2466\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0093, Train Linf Norm: 0.8516, Test Linf Norm: 1.2426\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0100, Train Linf Norm: 0.8212, Test Linf Norm: 1.4012\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0096, Train Linf Norm: 0.8550, Test Linf Norm: 1.3036\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0096, Train Linf Norm: 0.8344, Test Linf Norm: 1.3158\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0096, Train Linf Norm: 0.8424, Test Linf Norm: 1.3158\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0098, Train Linf Norm: 0.8557, Test Linf Norm: 1.3477\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0097, Train Linf Norm: 0.8303, Test Linf Norm: 1.3369\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0099, Train Linf Norm: 0.8111, Test Linf Norm: 1.3765\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0096, Train Linf Norm: 0.8366, Test Linf Norm: 1.3209\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0089, Train Linf Norm: 0.8077, Test Linf Norm: 1.1687\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0088, Train Linf Norm: 0.8498, Test Linf Norm: 1.1662\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0083, Train Linf Norm: 0.8360, Test Linf Norm: 1.0370\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0089, Train Linf Norm: 0.8365, Test Linf Norm: 1.1987\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0100, Train Linf Norm: 0.8281, Test Linf Norm: 1.4380\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0085, Train Linf Norm: 0.7502, Test Linf Norm: 1.1195\n",
            "Epoch 48: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0091, Test L1 Norm: 0.0059, Train Linf Norm: 0.7660, Test Linf Norm: 0.4884\n",
            "Epoch 49: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0110, Test L1 Norm: 0.0110, Train Linf Norm: 1.0156, Test Linf Norm: 1.6207\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0077, Train Linf Norm: 0.8237, Test Linf Norm: 0.9428\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0065, Train Linf Norm: 0.7632, Test Linf Norm: 0.6745\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0077, Train Linf Norm: 0.6985, Test Linf Norm: 0.9758\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0078, Train Linf Norm: 0.7485, Test Linf Norm: 1.0048\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0075, Train Linf Norm: 0.7651, Test Linf Norm: 0.9311\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0073, Train Linf Norm: 0.7633, Test Linf Norm: 0.8871\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0080, Train Linf Norm: 0.7468, Test Linf Norm: 1.0583\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0075, Train Linf Norm: 0.7555, Test Linf Norm: 0.9641\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0078, Train Linf Norm: 0.7342, Test Linf Norm: 1.0216\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0076, Train Linf Norm: 0.7407, Test Linf Norm: 0.9862\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0076, Train Linf Norm: 0.7372, Test Linf Norm: 0.9692\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0076, Train Linf Norm: 0.7393, Test Linf Norm: 0.9692\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0076, Train Linf Norm: 0.7355, Test Linf Norm: 0.9757\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0075, Train Linf Norm: 0.7176, Test Linf Norm: 0.9520\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0079, Train Linf Norm: 0.7391, Test Linf Norm: 1.0611\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0074, Train Linf Norm: 0.7471, Test Linf Norm: 0.9367\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0071, Train Linf Norm: 0.7145, Test Linf Norm: 0.8885\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0071, Train Linf Norm: 0.7258, Test Linf Norm: 0.8830\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0070, Train Linf Norm: 0.7150, Test Linf Norm: 0.8372\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0071, Train Linf Norm: 0.7504, Test Linf Norm: 0.9001\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0067, Train Linf Norm: 0.7752, Test Linf Norm: 0.8081\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0069, Train Linf Norm: 0.7539, Test Linf Norm: 0.8646\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0070, Train Linf Norm: 0.8065, Test Linf Norm: 0.9064\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0112, Train Linf Norm: 0.6993, Test Linf Norm: 1.6896\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0077, Test L1 Norm: 0.0094, Train Linf Norm: 0.8680, Test Linf Norm: 0.4075\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0054, Train Linf Norm: 0.7597, Test Linf Norm: 0.5307\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0069, Train Linf Norm: 0.6956, Test Linf Norm: 0.8604\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0067, Train Linf Norm: 0.6683, Test Linf Norm: 0.8451\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0054, Train Linf Norm: 0.6891, Test Linf Norm: 0.5248\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0077, Train Linf Norm: 0.6824, Test Linf Norm: 1.0542\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0071, Train Linf Norm: 0.6394, Test Linf Norm: 0.9343\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0067, Train Linf Norm: 0.6500, Test Linf Norm: 0.8425\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0067, Train Linf Norm: 0.6864, Test Linf Norm: 0.8467\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0062, Train Linf Norm: 0.6819, Test Linf Norm: 0.7421\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0065, Train Linf Norm: 0.6544, Test Linf Norm: 0.8067\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0065, Train Linf Norm: 0.6716, Test Linf Norm: 0.8067\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0064, Train Linf Norm: 0.6735, Test Linf Norm: 0.7756\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0066, Train Linf Norm: 0.6729, Test Linf Norm: 0.8231\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0065, Train Linf Norm: 0.6672, Test Linf Norm: 0.8059\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0061, Train Linf Norm: 0.6526, Test Linf Norm: 0.7095\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0059, Train Linf Norm: 0.6821, Test Linf Norm: 0.6666\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0069, Train Linf Norm: 0.6580, Test Linf Norm: 0.9128\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0059, Train Linf Norm: 0.6522, Test Linf Norm: 0.6782\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0083, Train Linf Norm: 0.6675, Test Linf Norm: 1.2290\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0060, Train Linf Norm: 0.6584, Test Linf Norm: 0.7175\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0091, Train Linf Norm: 0.6192, Test Linf Norm: 1.3544\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0049, Train Linf Norm: 0.9207, Test Linf Norm: 0.4690\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0053, Train Linf Norm: 0.7475, Test Linf Norm: 0.2812\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0042, Train Linf Norm: 0.6448, Test Linf Norm: 0.2752\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0054, Train Linf Norm: 0.6320, Test Linf Norm: 0.6061\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0052, Train Linf Norm: 0.6317, Test Linf Norm: 0.5241\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0064, Train Linf Norm: 0.5798, Test Linf Norm: 0.8264\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0062, Train Linf Norm: 0.5965, Test Linf Norm: 0.7990\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0066, Train Linf Norm: 0.6270, Test Linf Norm: 0.8783\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0061, Train Linf Norm: 0.6111, Test Linf Norm: 0.7706\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0063, Train Linf Norm: 0.6123, Test Linf Norm: 0.7992\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0061, Train Linf Norm: 0.6180, Test Linf Norm: 0.7807\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0061, Train Linf Norm: 0.6026, Test Linf Norm: 0.7755\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0062, Train Linf Norm: 0.5638, Test Linf Norm: 0.8026\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0062, Train Linf Norm: 0.5862, Test Linf Norm: 0.8026\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0062, Train Linf Norm: 0.6025, Test Linf Norm: 0.7895\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0060, Train Linf Norm: 0.5974, Test Linf Norm: 0.7433\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0062, Train Linf Norm: 0.6057, Test Linf Norm: 0.7980\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0062, Train Linf Norm: 0.5890, Test Linf Norm: 0.8053\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0068, Train Linf Norm: 0.5690, Test Linf Norm: 0.9473\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0064, Train Linf Norm: 0.5977, Test Linf Norm: 0.8413\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0062, Train Linf Norm: 0.6002, Test Linf Norm: 0.8047\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0063, Train Linf Norm: 0.5800, Test Linf Norm: 0.8358\n",
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0083, Train Linf Norm: 0.6098, Test Linf Norm: 1.2123\n",
            "Epoch 119: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0062, Train Linf Norm: 0.5609, Test Linf Norm: 0.8026\n",
            "Epoch 120: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0085, Train Linf Norm: 0.7051, Test Linf Norm: 1.2645\n",
            "Epoch 121: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0169, Train Linf Norm: 0.7560, Test Linf Norm: 2.7105\n",
            "Epoch 122: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0063, Train Linf Norm: 0.5442, Test Linf Norm: 0.8527\n",
            "Epoch 123: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0084, Train Linf Norm: 0.5547, Test Linf Norm: 1.2625\n",
            "Epoch 124: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0060, Train Linf Norm: 0.5766, Test Linf Norm: 0.7741\n",
            "Epoch 125: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0066, Train Linf Norm: 0.5572, Test Linf Norm: 0.9063\n",
            "Epoch 126: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0079, Train Linf Norm: 0.5672, Test Linf Norm: 1.2064\n",
            "Epoch 127: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0062, Train Linf Norm: 0.5535, Test Linf Norm: 0.8361\n",
            "Epoch 128: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0067, Train Linf Norm: 0.5592, Test Linf Norm: 0.9510\n",
            "Epoch 129: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0066, Train Linf Norm: 0.5560, Test Linf Norm: 0.9406\n",
            "Epoch 130: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0072, Train Linf Norm: 0.5472, Test Linf Norm: 1.0291\n",
            "Epoch 131: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0069, Train Linf Norm: 0.5484, Test Linf Norm: 0.9844\n",
            "Epoch 132: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0069, Train Linf Norm: 0.5545, Test Linf Norm: 0.9933\n",
            "Epoch 133: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0069, Train Linf Norm: 0.5576, Test Linf Norm: 0.9933\n",
            "Epoch 134: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0069, Train Linf Norm: 0.5311, Test Linf Norm: 0.9982\n",
            "Epoch 135: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0070, Train Linf Norm: 0.5509, Test Linf Norm: 1.0227\n",
            "Epoch 136: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0069, Train Linf Norm: 0.5674, Test Linf Norm: 0.9749\n",
            "Epoch 137: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0067, Train Linf Norm: 0.5521, Test Linf Norm: 0.9297\n",
            "Epoch 138: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0065, Train Linf Norm: 0.5420, Test Linf Norm: 0.9152\n",
            "Epoch 139: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0076, Train Linf Norm: 0.5454, Test Linf Norm: 1.1471\n",
            "Epoch 140: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0070, Train Linf Norm: 0.5651, Test Linf Norm: 1.0183\n",
            "Epoch 141: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0066, Train Linf Norm: 0.5503, Test Linf Norm: 0.9309\n",
            "Epoch 142: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0078, Train Linf Norm: 0.5863, Test Linf Norm: 1.1650\n",
            "Epoch 143: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0105, Train Linf Norm: 0.5710, Test Linf Norm: 1.7278\n",
            "Epoch 144: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0088, Train Linf Norm: 0.7324, Test Linf Norm: 1.3301\n",
            "Epoch 145: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0051, Train Linf Norm: 0.5217, Test Linf Norm: 0.4895\n",
            "Epoch 146: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0070, Train Linf Norm: 0.5239, Test Linf Norm: 0.9776\n",
            "Epoch 147: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0068, Train Linf Norm: 0.5605, Test Linf Norm: 0.9515\n",
            "Epoch 148: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0060, Train Linf Norm: 0.5178, Test Linf Norm: 0.7819\n",
            "Epoch 149: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0063, Train Linf Norm: 0.5213, Test Linf Norm: 0.8946\n",
            "Epoch 150: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0062, Train Linf Norm: 0.5009, Test Linf Norm: 0.8520\n",
            "Epoch 151: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0067, Train Linf Norm: 0.4983, Test Linf Norm: 0.9582\n",
            "Epoch 152: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0066, Train Linf Norm: 0.5155, Test Linf Norm: 0.9503\n",
            "Epoch 153: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0065, Train Linf Norm: 0.5197, Test Linf Norm: 0.9237\n",
            "Epoch 154: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0067, Train Linf Norm: 0.4938, Test Linf Norm: 0.9628\n",
            "Epoch 155: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0065, Train Linf Norm: 0.5140, Test Linf Norm: 0.9423\n",
            "Epoch 156: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0065, Train Linf Norm: 0.5159, Test Linf Norm: 0.9237\n",
            "Epoch 157: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0065, Train Linf Norm: 0.5162, Test Linf Norm: 0.9237\n",
            "Epoch 158: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0065, Train Linf Norm: 0.5125, Test Linf Norm: 0.9333\n",
            "Epoch 159: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0064, Train Linf Norm: 0.5151, Test Linf Norm: 0.9220\n",
            "Epoch 160: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0066, Train Linf Norm: 0.5023, Test Linf Norm: 0.9453\n",
            "Epoch 161: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0066, Train Linf Norm: 0.5050, Test Linf Norm: 0.9450\n",
            "Epoch 162: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0063, Train Linf Norm: 0.5177, Test Linf Norm: 0.9016\n",
            "Epoch 163: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0070, Train Linf Norm: 0.5049, Test Linf Norm: 1.0295\n",
            "Epoch 164: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0053, Train Linf Norm: 0.5146, Test Linf Norm: 0.6822\n",
            "Epoch 165: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0068, Train Linf Norm: 0.4955, Test Linf Norm: 1.0073\n",
            "Epoch 166: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0046, Train Linf Norm: 0.5373, Test Linf Norm: 0.4572\n",
            "Epoch 167: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0057, Train Linf Norm: 0.4975, Test Linf Norm: 0.7288\n",
            "Epoch 168: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0057, Train Linf Norm: 0.5304, Test Linf Norm: 0.7277\n",
            "Epoch 169: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0112, Train Linf Norm: 0.5763, Test Linf Norm: 1.8341\n",
            "Epoch 170: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0060, Train Linf Norm: 0.5427, Test Linf Norm: 0.8422\n",
            "Epoch 171: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0066, Train Linf Norm: 0.5274, Test Linf Norm: 0.9524\n",
            "Epoch 172: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0066, Train Linf Norm: 0.5036, Test Linf Norm: 0.9591\n",
            "Epoch 173: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0055, Train Linf Norm: 0.4989, Test Linf Norm: 0.7364\n",
            "Epoch 174: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0064, Train Linf Norm: 0.4900, Test Linf Norm: 0.9310\n",
            "Epoch 175: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0060, Train Linf Norm: 0.5102, Test Linf Norm: 0.8412\n",
            "Epoch 176: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0064, Train Linf Norm: 0.4867, Test Linf Norm: 0.9451\n",
            "Epoch 177: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0060, Train Linf Norm: 0.4840, Test Linf Norm: 0.8280\n",
            "Epoch 178: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0063, Train Linf Norm: 0.5001, Test Linf Norm: 0.8990\n",
            "Epoch 179: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0060, Train Linf Norm: 0.4969, Test Linf Norm: 0.8323\n",
            "Epoch 180: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0060, Train Linf Norm: 0.4833, Test Linf Norm: 0.8439\n",
            "Epoch 181: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0060, Train Linf Norm: 0.4853, Test Linf Norm: 0.8439\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:32:58,709]\u001b[0m Trial 59 finished with value: 0.005955404524016194 and parameters: {'n_layers': 3, 'n_units_0': 713, 'n_units_1': 610, 'n_units_2': 830, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.000620137664603181, 'batch_size': 218, 'n_epochs': 182, 'scheduler': 'CosineAnnealingLR', 'T_max': 12}. Best is trial 57 with value: 0.0038909989546984432.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 182: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0060, Train Linf Norm: 0.4857, Test Linf Norm: 0.8392\n",
            "Epoch 1: Train Loss: 0.0535, Test Loss: 0.0005, Train L1 Norm: 0.1949, Test L1 Norm: 0.0594, Train Linf Norm: 20.3969, Test Linf Norm: 8.7417\n",
            "Epoch 2: Train Loss: 0.0010, Test Loss: 0.0003, Train L1 Norm: 0.0343, Test L1 Norm: 0.0678, Train Linf Norm: 3.2293, Test Linf Norm: 10.2041\n",
            "Epoch 3: Train Loss: 0.0004, Test Loss: 0.0001, Train L1 Norm: 0.0258, Test L1 Norm: 0.0113, Train Linf Norm: 2.5778, Test Linf Norm: 0.6360\n",
            "Epoch 4: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0192, Test L1 Norm: 0.0105, Train Linf Norm: 1.8303, Test Linf Norm: 0.6045\n",
            "Epoch 5: Train Loss: 0.0004, Test Loss: 0.0001, Train L1 Norm: 0.0152, Test L1 Norm: 0.0098, Train Linf Norm: 1.3466, Test Linf Norm: 0.5494\n",
            "Epoch 6: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0134, Test L1 Norm: 0.0172, Train Linf Norm: 1.3845, Test Linf Norm: 2.1249\n",
            "Epoch 7: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0122, Test L1 Norm: 0.0102, Train Linf Norm: 1.1817, Test Linf Norm: 0.6400\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0002, Train L1 Norm: 0.0102, Test L1 Norm: 0.0104, Train Linf Norm: 0.9302, Test Linf Norm: 0.4636\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0101, Test L1 Norm: 0.0064, Train Linf Norm: 1.0139, Test Linf Norm: 0.5069\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0107, Test L1 Norm: 0.0068, Train Linf Norm: 1.1645, Test Linf Norm: 0.4800\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0105, Test L1 Norm: 0.0065, Train Linf Norm: 1.1098, Test Linf Norm: 0.4581\n",
            "Epoch 12: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0095, Test L1 Norm: 0.0084, Train Linf Norm: 0.8475, Test Linf Norm: 0.5460\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0084, Test L1 Norm: 0.0061, Train Linf Norm: 0.8172, Test Linf Norm: 0.3824\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0059, Train Linf Norm: 0.7539, Test Linf Norm: 0.4130\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0088, Test L1 Norm: 0.0058, Train Linf Norm: 0.8741, Test Linf Norm: 0.4429\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0079, Test L1 Norm: 0.0052, Train Linf Norm: 0.8290, Test Linf Norm: 0.3951\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0102, Train Linf Norm: 0.7808, Test Linf Norm: 1.0268\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0063, Train Linf Norm: 0.7714, Test Linf Norm: 0.4757\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0002, Train L1 Norm: 0.0080, Test L1 Norm: 0.0088, Train Linf Norm: 0.8767, Test Linf Norm: 0.3907\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0054, Train Linf Norm: 0.7738, Test Linf Norm: 0.4602\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0052, Train Linf Norm: 0.6838, Test Linf Norm: 0.3336\n",
            "Epoch 22: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0084, Test L1 Norm: 0.0052, Train Linf Norm: 0.7671, Test Linf Norm: 0.3445\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0052, Train Linf Norm: 0.6407, Test Linf Norm: 0.4257\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0002, Train L1 Norm: 0.0063, Test L1 Norm: 0.0098, Train Linf Norm: 0.6181, Test Linf Norm: 0.4228\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0051, Train Linf Norm: 0.6823, Test Linf Norm: 0.4015\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0043, Train Linf Norm: 0.6654, Test Linf Norm: 0.3228\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0053, Train Linf Norm: 0.6756, Test Linf Norm: 0.3266\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0051, Train Linf Norm: 0.7377, Test Linf Norm: 0.3373\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0043, Train Linf Norm: 0.7761, Test Linf Norm: 0.3093\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0048, Train Linf Norm: 0.6967, Test Linf Norm: 0.3256\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0054, Train Linf Norm: 0.6896, Test Linf Norm: 0.4387\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0046, Train Linf Norm: 0.8938, Test Linf Norm: 0.2945\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0046, Train Linf Norm: 0.7348, Test Linf Norm: 0.2823\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0041, Train Linf Norm: 0.6531, Test Linf Norm: 0.2634\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0030, Train L1 Norm: 0.0061, Test L1 Norm: 0.0476, Train Linf Norm: 0.6638, Test Linf Norm: 5.6710\n",
            "Epoch 36: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0051, Train Linf Norm: 0.9319, Test Linf Norm: 0.3899\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0046, Train Linf Norm: 0.7341, Test Linf Norm: 0.3833\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0045, Train Linf Norm: 0.5674, Test Linf Norm: 0.3139\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0011, Train L1 Norm: 0.0058, Test L1 Norm: 0.0334, Train Linf Norm: 0.6122, Test Linf Norm: 4.0224\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0055, Train Linf Norm: 0.6123, Test Linf Norm: 0.4305\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0089, Train Linf Norm: 0.5544, Test Linf Norm: 0.6662\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0039, Train Linf Norm: 0.6692, Test Linf Norm: 0.2458\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0037, Train Linf Norm: 0.6460, Test Linf Norm: 0.2631\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0036, Train Linf Norm: 0.7104, Test Linf Norm: 0.2432\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0044, Train Linf Norm: 0.6417, Test Linf Norm: 0.3139\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0038, Train Linf Norm: 0.6430, Test Linf Norm: 0.2821\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0034, Train Linf Norm: 0.5877, Test Linf Norm: 0.2401\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0058, Train Linf Norm: 0.5836, Test Linf Norm: 0.4314\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0049, Train Linf Norm: 0.6749, Test Linf Norm: 0.3681\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0037, Train Linf Norm: 0.6733, Test Linf Norm: 0.2650\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0053, Train Linf Norm: 0.6513, Test Linf Norm: 0.3982\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0049, Train Linf Norm: 0.5764, Test Linf Norm: 0.3146\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0046, Train Linf Norm: 0.5815, Test Linf Norm: 0.3064\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0034, Train Linf Norm: 0.5539, Test Linf Norm: 0.2534\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0038, Train Linf Norm: 0.6249, Test Linf Norm: 0.2609\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0044, Train Linf Norm: 0.6459, Test Linf Norm: 0.3100\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0036, Train Linf Norm: 0.5911, Test Linf Norm: 0.2638\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0033, Train Linf Norm: 0.5710, Test Linf Norm: 0.2585\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0040, Train Linf Norm: 0.6721, Test Linf Norm: 0.2635\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0031, Train Linf Norm: 0.6430, Test Linf Norm: 0.2344\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0033, Train Linf Norm: 0.6103, Test Linf Norm: 0.2219\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0038, Train Linf Norm: 0.6355, Test Linf Norm: 0.2793\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0033, Train Linf Norm: 0.5863, Test Linf Norm: 0.2498\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0034, Train Linf Norm: 0.6351, Test Linf Norm: 0.2357\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0034, Train Linf Norm: 0.5792, Test Linf Norm: 0.2584\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0031, Train Linf Norm: 0.6144, Test Linf Norm: 0.2323\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0030, Train Linf Norm: 0.5694, Test Linf Norm: 0.2144\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0038, Train Linf Norm: 0.5332, Test Linf Norm: 0.2823\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0033, Train Linf Norm: 0.5711, Test Linf Norm: 0.2538\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0033, Train Linf Norm: 0.5555, Test Linf Norm: 0.2667\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0031, Train Linf Norm: 0.6461, Test Linf Norm: 0.2544\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0031, Train Linf Norm: 0.6048, Test Linf Norm: 0.2372\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0034, Train Linf Norm: 0.5914, Test Linf Norm: 0.2677\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0029, Train Linf Norm: 0.5291, Test Linf Norm: 0.2170\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0031, Train Linf Norm: 0.5673, Test Linf Norm: 0.2466\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0030, Train Linf Norm: 0.5179, Test Linf Norm: 0.2281\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0043, Train Linf Norm: 0.5892, Test Linf Norm: 0.3927\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0032, Train Linf Norm: 0.6342, Test Linf Norm: 0.2500\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0028, Train Linf Norm: 0.5906, Test Linf Norm: 0.2127\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0028, Train Linf Norm: 0.6193, Test Linf Norm: 0.2124\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0031, Train Linf Norm: 0.5488, Test Linf Norm: 0.2222\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0050, Train Linf Norm: 0.5490, Test Linf Norm: 0.3202\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0027, Train Linf Norm: 0.5624, Test Linf Norm: 0.1987\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0046, Train Linf Norm: 0.5731, Test Linf Norm: 0.3077\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0033, Train Linf Norm: 0.6677, Test Linf Norm: 0.2463\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0031, Train Linf Norm: 0.5476, Test Linf Norm: 0.2433\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0029, Train Linf Norm: 0.5272, Test Linf Norm: 0.2092\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0030, Train Linf Norm: 0.5269, Test Linf Norm: 0.2227\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0038, Train Linf Norm: 0.5508, Test Linf Norm: 0.2760\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0029, Train Linf Norm: 0.4803, Test Linf Norm: 0.2145\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0028, Train Linf Norm: 0.5760, Test Linf Norm: 0.2124\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0033, Train Linf Norm: 0.5302, Test Linf Norm: 0.2531\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0031, Train Linf Norm: 0.5582, Test Linf Norm: 0.2381\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0048, Train Linf Norm: 0.5308, Test Linf Norm: 0.3495\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0024, Train Linf Norm: 0.5023, Test Linf Norm: 0.1670\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0046, Train Linf Norm: 0.4936, Test Linf Norm: 0.2588\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0029, Train Linf Norm: 0.5509, Test Linf Norm: 0.2049\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0032, Train Linf Norm: 0.5159, Test Linf Norm: 0.2362\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0037, Train Linf Norm: 0.4906, Test Linf Norm: 0.2686\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0024, Train Linf Norm: 0.5028, Test Linf Norm: 0.1715\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0028, Train Linf Norm: 0.4763, Test Linf Norm: 0.2168\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0030, Train Linf Norm: 0.5151, Test Linf Norm: 0.2349\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0031, Train Linf Norm: 0.4753, Test Linf Norm: 0.2204\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0026, Train Linf Norm: 0.4629, Test Linf Norm: 0.1981\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0028, Train Linf Norm: 0.5014, Test Linf Norm: 0.2020\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0043, Train Linf Norm: 0.4575, Test Linf Norm: 0.3040\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0030, Train Linf Norm: 0.5775, Test Linf Norm: 0.2175\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0031, Train Linf Norm: 0.4226, Test Linf Norm: 0.2145\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0024, Train Linf Norm: 0.5035, Test Linf Norm: 0.1604\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0030, Train Linf Norm: 0.5011, Test Linf Norm: 0.2155\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0042, Train Linf Norm: 0.4556, Test Linf Norm: 0.3069\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0025, Train Linf Norm: 0.5810, Test Linf Norm: 0.1771\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0028, Train Linf Norm: 0.5165, Test Linf Norm: 0.2087\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0087, Train Linf Norm: 0.4750, Test Linf Norm: 1.0533\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0026, Train Linf Norm: 0.5407, Test Linf Norm: 0.1928\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0025, Train Linf Norm: 0.5096, Test Linf Norm: 0.1973\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0022, Train Linf Norm: 0.4460, Test Linf Norm: 0.1528\n",
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0024, Train Linf Norm: 0.4684, Test Linf Norm: 0.1879\n",
            "Epoch 119: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0035, Train Linf Norm: 0.4993, Test Linf Norm: 0.2071\n",
            "Epoch 120: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0026, Train Linf Norm: 0.4495, Test Linf Norm: 0.1891\n",
            "Epoch 121: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0043, Train Linf Norm: 0.4695, Test Linf Norm: 0.3260\n",
            "Epoch 122: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0027, Train Linf Norm: 0.4381, Test Linf Norm: 0.2008\n",
            "Epoch 123: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0031, Train Linf Norm: 0.4181, Test Linf Norm: 0.2642\n",
            "Epoch 124: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0031, Train Linf Norm: 0.4306, Test Linf Norm: 0.2959\n",
            "Epoch 125: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0022, Train Linf Norm: 0.4953, Test Linf Norm: 0.1596\n",
            "Epoch 126: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0040, Train Linf Norm: 0.4549, Test Linf Norm: 0.2417\n",
            "Epoch 127: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0044, Train Linf Norm: 0.4903, Test Linf Norm: 0.3335\n",
            "Epoch 128: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0047, Train Linf Norm: 0.5046, Test Linf Norm: 0.3664\n",
            "Epoch 129: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0024, Train Linf Norm: 0.4416, Test Linf Norm: 0.1703\n",
            "Epoch 130: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0046, Train Linf Norm: 0.4493, Test Linf Norm: 0.2883\n",
            "Epoch 131: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0045, Train Linf Norm: 0.4741, Test Linf Norm: 0.4262\n",
            "Epoch 132: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0030, Train Linf Norm: 0.4502, Test Linf Norm: 0.2096\n",
            "Epoch 133: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0024, Train Linf Norm: 0.5278, Test Linf Norm: 0.1709\n",
            "Epoch 134: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0023, Train Linf Norm: 0.4816, Test Linf Norm: 0.1694\n",
            "Epoch 135: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0026, Train Linf Norm: 0.4459, Test Linf Norm: 0.2092\n",
            "Epoch 136: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0022, Train Linf Norm: 0.4519, Test Linf Norm: 0.1692\n",
            "Epoch 137: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0023, Train Linf Norm: 0.3842, Test Linf Norm: 0.1711\n",
            "Epoch 138: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0026, Train Linf Norm: 0.3981, Test Linf Norm: 0.2097\n",
            "Epoch 139: Train Loss: 0.0000, Test Loss: 0.0004, Train L1 Norm: 0.0040, Test L1 Norm: 0.0172, Train Linf Norm: 0.4783, Test Linf Norm: 2.1089\n",
            "Epoch 140: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0023, Train Linf Norm: 0.4867, Test Linf Norm: 0.1738\n",
            "Epoch 141: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0029, Train Linf Norm: 0.4552, Test Linf Norm: 0.2207\n",
            "Epoch 142: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0024, Train Linf Norm: 0.4398, Test Linf Norm: 0.1829\n",
            "Epoch 143: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0031, Train Linf Norm: 0.4306, Test Linf Norm: 0.2305\n",
            "Epoch 144: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0023, Train Linf Norm: 0.4161, Test Linf Norm: 0.1818\n",
            "Epoch 145: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0045, Train Linf Norm: 0.4210, Test Linf Norm: 0.2927\n",
            "Epoch 146: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0031, Train Linf Norm: 0.4421, Test Linf Norm: 0.2512\n",
            "Epoch 147: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0076, Train Linf Norm: 0.4340, Test Linf Norm: 1.0577\n",
            "Epoch 148: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0025, Train Linf Norm: 0.4548, Test Linf Norm: 0.1840\n",
            "Epoch 149: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0029, Train Linf Norm: 0.4842, Test Linf Norm: 0.2044\n",
            "Epoch 150: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0026, Train Linf Norm: 0.4334, Test Linf Norm: 0.1975\n",
            "Epoch 151: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0029, Train Linf Norm: 0.3815, Test Linf Norm: 0.2415\n",
            "Epoch 152: Train Loss: 0.0000, Test Loss: 0.0003, Train L1 Norm: 0.0039, Test L1 Norm: 0.0087, Train Linf Norm: 0.4664, Test Linf Norm: 0.3266\n",
            "Epoch 153: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0030, Train Linf Norm: 0.4386, Test Linf Norm: 0.1871\n",
            "Epoch 154: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0026, Train Linf Norm: 0.4385, Test Linf Norm: 0.1899\n",
            "Epoch 155: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0022, Train Linf Norm: 0.4137, Test Linf Norm: 0.1652\n",
            "Epoch 156: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0023, Train Linf Norm: 0.4166, Test Linf Norm: 0.1899\n",
            "Epoch 157: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0031, Train Linf Norm: 0.4322, Test Linf Norm: 0.2372\n",
            "Epoch 158: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0023, Train Linf Norm: 0.5075, Test Linf Norm: 0.1765\n",
            "Epoch 159: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0033, Train Linf Norm: 0.4420, Test Linf Norm: 0.2799\n",
            "Epoch 160: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0024, Train Linf Norm: 0.4721, Test Linf Norm: 0.1893\n",
            "Epoch 161: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0034, Train Linf Norm: 0.4265, Test Linf Norm: 0.3191\n",
            "Epoch 162: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0026, Train Linf Norm: 0.4136, Test Linf Norm: 0.1692\n",
            "Epoch 163: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0023, Train Linf Norm: 0.3676, Test Linf Norm: 0.1720\n",
            "Epoch 164: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0032, Train Linf Norm: 0.4161, Test Linf Norm: 0.3127\n",
            "Epoch 165: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0023, Train Linf Norm: 0.4028, Test Linf Norm: 0.1938\n",
            "Epoch 166: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0029, Train Linf Norm: 0.3910, Test Linf Norm: 0.2302\n",
            "Epoch 167: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0022, Train Linf Norm: 0.4135, Test Linf Norm: 0.1649\n",
            "Epoch 168: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0023, Train Linf Norm: 0.4046, Test Linf Norm: 0.1741\n",
            "Epoch 169: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0026, Train Linf Norm: 0.4205, Test Linf Norm: 0.1973\n",
            "Epoch 170: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0044, Train Linf Norm: 0.4123, Test Linf Norm: 0.2573\n",
            "Epoch 171: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0037, Test L1 Norm: 0.0055, Train Linf Norm: 0.4325, Test Linf Norm: 0.3391\n",
            "Epoch 172: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0037, Train Linf Norm: 0.3849, Test Linf Norm: 0.3022\n",
            "Epoch 173: Train Loss: 0.0000, Test Loss: 0.0002, Train L1 Norm: 0.0035, Test L1 Norm: 0.0083, Train Linf Norm: 0.3897, Test Linf Norm: 0.3583\n",
            "Epoch 174: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0025, Train Linf Norm: 0.4657, Test Linf Norm: 0.2168\n",
            "Epoch 175: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0024, Train Linf Norm: 0.3952, Test Linf Norm: 0.1967\n",
            "Epoch 176: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0033, Train Linf Norm: 0.3891, Test Linf Norm: 0.2603\n",
            "Epoch 177: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0022, Train Linf Norm: 0.4010, Test Linf Norm: 0.1670\n",
            "Epoch 178: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0027, Train Linf Norm: 0.4146, Test Linf Norm: 0.1934\n",
            "Epoch 179: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0050, Train Linf Norm: 0.3719, Test Linf Norm: 0.2643\n",
            "Epoch 180: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0031, Train Linf Norm: 0.3738, Test Linf Norm: 0.2190\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:38:15,181]\u001b[0m Trial 60 finished with value: 0.003238059613585938 and parameters: {'n_layers': 3, 'n_units_0': 830, 'n_units_1': 599, 'n_units_2': 849, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.0005586476608910378, 'batch_size': 201, 'n_epochs': 181, 'scheduler': 'None'}. Best is trial 60 with value: 0.003238059613585938.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 181: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0032, Train Linf Norm: 0.4273, Test Linf Norm: 0.2845\n",
            "Epoch 1: Train Loss: 0.0612, Test Loss: 0.0024, Train L1 Norm: 0.2348, Test L1 Norm: 0.0371, Train Linf Norm: 25.3141, Test Linf Norm: 1.1820\n",
            "Epoch 2: Train Loss: 0.0012, Test Loss: 0.0002, Train L1 Norm: 0.0465, Test L1 Norm: 0.0187, Train Linf Norm: 5.3879, Test Linf Norm: 1.2850\n",
            "Epoch 3: Train Loss: 0.0004, Test Loss: 0.0010, Train L1 Norm: 0.0379, Test L1 Norm: 0.0211, Train Linf Norm: 5.1107, Test Linf Norm: 0.8964\n",
            "Epoch 4: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0245, Test L1 Norm: 0.0124, Train Linf Norm: 3.0258, Test Linf Norm: 0.9082\n",
            "Epoch 5: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0234, Test L1 Norm: 0.0193, Train Linf Norm: 3.1076, Test Linf Norm: 2.0598\n",
            "Epoch 6: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0225, Test L1 Norm: 0.0107, Train Linf Norm: 2.8109, Test Linf Norm: 0.8943\n",
            "Epoch 7: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0195, Test L1 Norm: 0.0094, Train Linf Norm: 2.5666, Test Linf Norm: 0.4554\n",
            "Epoch 8: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0176, Test L1 Norm: 0.0091, Train Linf Norm: 2.1916, Test Linf Norm: 0.6406\n",
            "Epoch 9: Train Loss: 0.0001, Test Loss: 0.0003, Train L1 Norm: 0.0176, Test L1 Norm: 0.0186, Train Linf Norm: 2.3993, Test Linf Norm: 1.9560\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0145, Test L1 Norm: 0.0070, Train Linf Norm: 1.9328, Test Linf Norm: 0.4022\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0152, Test L1 Norm: 0.0061, Train Linf Norm: 2.0827, Test Linf Norm: 0.4249\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0131, Test L1 Norm: 0.0064, Train Linf Norm: 1.7214, Test Linf Norm: 0.4036\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0129, Test L1 Norm: 0.0056, Train Linf Norm: 1.7325, Test Linf Norm: 0.3686\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0003, Train L1 Norm: 0.0115, Test L1 Norm: 0.0243, Train Linf Norm: 1.5497, Test Linf Norm: 3.2354\n",
            "Epoch 15: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0117, Test L1 Norm: 0.0053, Train Linf Norm: 1.4407, Test Linf Norm: 0.3861\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0119, Test L1 Norm: 0.0056, Train Linf Norm: 1.6374, Test Linf Norm: 0.4173\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0117, Test L1 Norm: 0.0053, Train Linf Norm: 1.5675, Test Linf Norm: 0.3892\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0105, Test L1 Norm: 0.0077, Train Linf Norm: 1.4306, Test Linf Norm: 0.7884\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0097, Test L1 Norm: 0.0052, Train Linf Norm: 1.2730, Test Linf Norm: 0.3507\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0101, Test L1 Norm: 0.0051, Train Linf Norm: 1.3457, Test Linf Norm: 0.3575\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0098, Test L1 Norm: 0.0052, Train Linf Norm: 1.3172, Test Linf Norm: 0.4200\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0093, Test L1 Norm: 0.0049, Train Linf Norm: 1.1547, Test Linf Norm: 0.3295\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0093, Test L1 Norm: 0.0048, Train Linf Norm: 1.2450, Test Linf Norm: 0.3547\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0055, Train Linf Norm: 1.1325, Test Linf Norm: 0.4769\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0091, Test L1 Norm: 0.0048, Train Linf Norm: 1.2439, Test Linf Norm: 0.3410\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0090, Test L1 Norm: 0.0045, Train Linf Norm: 1.1491, Test Linf Norm: 0.3305\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0112, Test L1 Norm: 0.0060, Train Linf Norm: 1.5966, Test Linf Norm: 0.5757\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0083, Test L1 Norm: 0.0045, Train Linf Norm: 1.0804, Test Linf Norm: 0.3477\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0044, Train Linf Norm: 0.9936, Test Linf Norm: 0.3153\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0057, Train Linf Norm: 0.9565, Test Linf Norm: 0.3333\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0063, Train Linf Norm: 0.9148, Test Linf Norm: 0.6515\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0002, Train L1 Norm: 0.0079, Test L1 Norm: 0.0256, Train Linf Norm: 1.0114, Test Linf Norm: 3.7983\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0060, Train Linf Norm: 1.0559, Test Linf Norm: 0.6733\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0045, Train Linf Norm: 1.0166, Test Linf Norm: 0.3923\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0045, Train Linf Norm: 0.9530, Test Linf Norm: 0.4188\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0049, Train Linf Norm: 0.8816, Test Linf Norm: 0.3014\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0045, Train Linf Norm: 0.9485, Test Linf Norm: 0.3821\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0049, Train Linf Norm: 0.9037, Test Linf Norm: 0.3206\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0045, Train Linf Norm: 0.9555, Test Linf Norm: 0.4279\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0050, Train Linf Norm: 0.9343, Test Linf Norm: 0.4998\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0047, Train Linf Norm: 0.8456, Test Linf Norm: 0.4868\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0045, Train Linf Norm: 0.9148, Test Linf Norm: 0.3722\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0043, Train Linf Norm: 0.8718, Test Linf Norm: 0.3781\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0070, Train Linf Norm: 0.9026, Test Linf Norm: 0.9354\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0057, Train Linf Norm: 0.9354, Test Linf Norm: 0.7114\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0045, Train Linf Norm: 0.8016, Test Linf Norm: 0.4331\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0047, Train Linf Norm: 0.7689, Test Linf Norm: 0.4989\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0040, Train Linf Norm: 0.8222, Test Linf Norm: 0.3576\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0040, Train Linf Norm: 0.9077, Test Linf Norm: 0.3329\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0040, Train Linf Norm: 0.8082, Test Linf Norm: 0.3723\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0051, Train Linf Norm: 0.8389, Test Linf Norm: 0.6221\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0055, Train Linf Norm: 0.8930, Test Linf Norm: 0.6813\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0041, Train Linf Norm: 0.7985, Test Linf Norm: 0.4001\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0042, Train Linf Norm: 0.7274, Test Linf Norm: 0.4342\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0048, Train Linf Norm: 0.8130, Test Linf Norm: 0.5634\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0057, Train Linf Norm: 0.7490, Test Linf Norm: 0.7633\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0040, Train Linf Norm: 0.7500, Test Linf Norm: 0.3846\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0037, Train Linf Norm: 0.7829, Test Linf Norm: 0.3472\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0106, Train Linf Norm: 0.7572, Test Linf Norm: 1.6275\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0003, Train L1 Norm: 0.0062, Test L1 Norm: 0.0423, Train Linf Norm: 0.7789, Test Linf Norm: 6.9357\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0051, Train Linf Norm: 0.7788, Test Linf Norm: 0.5567\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0056, Train Linf Norm: 0.7461, Test Linf Norm: 0.7638\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0056, Train Linf Norm: 0.8490, Test Linf Norm: 0.7604\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0066, Train Linf Norm: 0.8132, Test Linf Norm: 0.9695\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0036, Train Linf Norm: 0.7160, Test Linf Norm: 0.3237\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0044, Train Linf Norm: 0.7081, Test Linf Norm: 0.4800\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0038, Train Linf Norm: 0.6959, Test Linf Norm: 0.3711\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0055, Train Linf Norm: 0.7126, Test Linf Norm: 0.7415\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0037, Train Linf Norm: 0.9718, Test Linf Norm: 0.3182\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0035, Train Linf Norm: 0.6624, Test Linf Norm: 0.3121\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0057, Train Linf Norm: 0.7325, Test Linf Norm: 0.7977\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0142, Train Linf Norm: 0.7396, Test Linf Norm: 2.1946\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0048, Train Linf Norm: 0.6599, Test Linf Norm: 0.5964\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0038, Train Linf Norm: 0.6297, Test Linf Norm: 0.3899\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0087, Train Linf Norm: 0.6438, Test Linf Norm: 1.3717\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0037, Train Linf Norm: 0.7187, Test Linf Norm: 0.3767\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0061, Train Linf Norm: 0.7038, Test Linf Norm: 0.8766\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0036, Train Linf Norm: 0.6791, Test Linf Norm: 0.3369\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0035, Train Linf Norm: 0.6939, Test Linf Norm: 0.3111\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0058, Train Linf Norm: 0.6443, Test Linf Norm: 0.8380\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0050, Train Linf Norm: 0.6501, Test Linf Norm: 0.6707\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0044, Train Linf Norm: 0.6032, Test Linf Norm: 0.5215\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0048, Train Linf Norm: 0.6639, Test Linf Norm: 0.6122\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0045, Train Linf Norm: 0.6568, Test Linf Norm: 0.5318\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0053, Train Linf Norm: 0.6943, Test Linf Norm: 0.7150\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0039, Train Linf Norm: 0.6304, Test Linf Norm: 0.3864\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0037, Train Linf Norm: 0.5485, Test Linf Norm: 0.3969\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0048, Train Linf Norm: 0.5976, Test Linf Norm: 0.6365\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0044, Train Linf Norm: 0.5758, Test Linf Norm: 0.5518\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0047, Train Linf Norm: 0.6482, Test Linf Norm: 0.6029\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0038, Train Linf Norm: 0.5954, Test Linf Norm: 0.4261\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0043, Train Linf Norm: 0.6884, Test Linf Norm: 0.5287\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0040, Train Linf Norm: 0.6552, Test Linf Norm: 0.4695\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0064, Train Linf Norm: 0.5630, Test Linf Norm: 0.9585\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0073, Train Linf Norm: 0.6054, Test Linf Norm: 1.1065\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0035, Train Linf Norm: 0.6487, Test Linf Norm: 0.3257\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0044, Train Linf Norm: 0.5925, Test Linf Norm: 0.5649\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0035, Train Linf Norm: 0.7441, Test Linf Norm: 0.3043\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0032, Train Linf Norm: 0.5936, Test Linf Norm: 0.2995\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0043, Train Linf Norm: 0.5813, Test Linf Norm: 0.5252\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0049, Train Linf Norm: 0.5583, Test Linf Norm: 0.6971\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0039, Train Linf Norm: 0.5574, Test Linf Norm: 0.4565\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0070, Train Linf Norm: 0.6141, Test Linf Norm: 1.0997\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0054, Train Linf Norm: 0.5789, Test Linf Norm: 0.7803\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0051, Train Linf Norm: 0.5162, Test Linf Norm: 0.7257\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0043, Train Linf Norm: 0.5695, Test Linf Norm: 0.5415\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0045, Train Linf Norm: 0.5356, Test Linf Norm: 0.5910\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0057, Train Linf Norm: 0.5341, Test Linf Norm: 0.8462\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0061, Train Linf Norm: 0.5987, Test Linf Norm: 0.9521\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0055, Train Linf Norm: 0.5720, Test Linf Norm: 0.8077\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0063, Train Linf Norm: 0.5597, Test Linf Norm: 0.9768\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0043, Train Linf Norm: 0.5435, Test Linf Norm: 0.5630\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0059, Train Linf Norm: 0.5660, Test Linf Norm: 0.9051\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0033, Train Linf Norm: 0.5181, Test Linf Norm: 0.3482\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0033, Train Linf Norm: 0.6107, Test Linf Norm: 0.3503\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0033, Train Linf Norm: 0.6473, Test Linf Norm: 0.3282\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0033, Train Linf Norm: 0.5764, Test Linf Norm: 0.2914\n",
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0035, Train Linf Norm: 0.5106, Test Linf Norm: 0.2483\n",
            "Epoch 119: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0043, Train Linf Norm: 0.5266, Test Linf Norm: 0.5394\n",
            "Epoch 120: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0055, Train Linf Norm: 0.5233, Test Linf Norm: 0.8105\n",
            "Epoch 121: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0038, Train Linf Norm: 0.4960, Test Linf Norm: 0.4619\n",
            "Epoch 122: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0038, Train Linf Norm: 0.5172, Test Linf Norm: 0.4239\n",
            "Epoch 123: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0040, Train Linf Norm: 0.5051, Test Linf Norm: 0.4918\n",
            "Epoch 124: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0030, Train Linf Norm: 0.6003, Test Linf Norm: 0.2861\n",
            "Epoch 125: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0045, Train Linf Norm: 0.5606, Test Linf Norm: 0.2724\n",
            "Epoch 126: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0052, Test L1 Norm: 0.0073, Train Linf Norm: 0.6651, Test Linf Norm: 0.3567\n",
            "Epoch 127: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0042, Train Linf Norm: 0.4366, Test Linf Norm: 0.5749\n",
            "Epoch 128: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0035, Train Linf Norm: 0.5340, Test Linf Norm: 0.3154\n",
            "Epoch 129: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0067, Train Linf Norm: 0.4739, Test Linf Norm: 1.0672\n",
            "Epoch 130: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0032, Train Linf Norm: 0.4976, Test Linf Norm: 0.3182\n",
            "Epoch 131: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0031, Train Linf Norm: 0.6274, Test Linf Norm: 0.3216\n",
            "Epoch 132: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0040, Train Linf Norm: 0.5271, Test Linf Norm: 0.5226\n",
            "Epoch 133: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0035, Train Linf Norm: 0.5492, Test Linf Norm: 0.2714\n",
            "Epoch 134: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0034, Train Linf Norm: 0.5149, Test Linf Norm: 0.3911\n",
            "Epoch 135: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0064, Train Linf Norm: 0.4458, Test Linf Norm: 0.9756\n",
            "Epoch 136: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0030, Train Linf Norm: 0.5306, Test Linf Norm: 0.3099\n",
            "Epoch 137: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0031, Train Linf Norm: 0.4965, Test Linf Norm: 0.3171\n",
            "Epoch 138: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0037, Train Linf Norm: 0.5023, Test Linf Norm: 0.4625\n",
            "Epoch 139: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0035, Train Linf Norm: 0.5532, Test Linf Norm: 0.4037\n",
            "Epoch 140: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0032, Train Linf Norm: 0.5039, Test Linf Norm: 0.3060\n",
            "Epoch 141: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0030, Train Linf Norm: 0.4794, Test Linf Norm: 0.2976\n",
            "Epoch 142: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0036, Train Linf Norm: 0.4666, Test Linf Norm: 0.4543\n",
            "Epoch 143: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0031, Train Linf Norm: 0.4835, Test Linf Norm: 0.3349\n",
            "Epoch 144: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0031, Train Linf Norm: 0.4676, Test Linf Norm: 0.3093\n",
            "Epoch 145: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0047, Train Linf Norm: 0.4848, Test Linf Norm: 0.6530\n",
            "Epoch 146: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0034, Train Linf Norm: 0.5231, Test Linf Norm: 0.3920\n",
            "Epoch 147: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0033, Train Linf Norm: 0.5081, Test Linf Norm: 0.3638\n",
            "Epoch 148: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0031, Train Linf Norm: 0.5061, Test Linf Norm: 0.3077\n",
            "Epoch 149: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0032, Train Linf Norm: 0.5186, Test Linf Norm: 0.2727\n",
            "Epoch 150: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0031, Train Linf Norm: 0.4947, Test Linf Norm: 0.3235\n",
            "Epoch 151: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0029, Train Linf Norm: 0.5289, Test Linf Norm: 0.3012\n",
            "Epoch 152: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0038, Train Linf Norm: 0.5441, Test Linf Norm: 0.4875\n",
            "Epoch 153: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0030, Train Linf Norm: 0.4863, Test Linf Norm: 0.3062\n",
            "Epoch 154: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0067, Train Linf Norm: 0.4651, Test Linf Norm: 1.0635\n",
            "Epoch 155: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0028, Train Linf Norm: 0.4699, Test Linf Norm: 0.2675\n",
            "Epoch 156: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0030, Train Linf Norm: 0.5145, Test Linf Norm: 0.3323\n",
            "Epoch 157: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0032, Train Linf Norm: 0.5476, Test Linf Norm: 0.3617\n",
            "Epoch 158: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0029, Train Linf Norm: 0.4631, Test Linf Norm: 0.2806\n",
            "Epoch 159: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0029, Train Linf Norm: 0.5008, Test Linf Norm: 0.2894\n",
            "Epoch 160: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0033, Train Linf Norm: 0.4825, Test Linf Norm: 0.3787\n",
            "Epoch 161: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0029, Train Linf Norm: 0.4797, Test Linf Norm: 0.3239\n",
            "Epoch 162: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0030, Train Linf Norm: 0.4716, Test Linf Norm: 0.3166\n",
            "Epoch 163: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0028, Train Linf Norm: 0.4986, Test Linf Norm: 0.2661\n",
            "Epoch 164: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0082, Train Linf Norm: 0.4270, Test Linf Norm: 1.2539\n",
            "Epoch 165: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0030, Train Linf Norm: 0.4924, Test Linf Norm: 0.2971\n",
            "Epoch 166: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0028, Train Linf Norm: 0.3975, Test Linf Norm: 0.2700\n",
            "Epoch 167: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0030, Train Linf Norm: 0.4970, Test Linf Norm: 0.3332\n",
            "Epoch 168: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0027, Train Linf Norm: 0.4715, Test Linf Norm: 0.2406\n",
            "Epoch 169: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0028, Train Linf Norm: 0.4758, Test Linf Norm: 0.2877\n",
            "Epoch 170: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0026, Train Linf Norm: 0.5164, Test Linf Norm: 0.2353\n",
            "Epoch 171: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0047, Train Linf Norm: 0.4853, Test Linf Norm: 0.6451\n",
            "Epoch 172: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0025, Train Linf Norm: 0.5089, Test Linf Norm: 0.2231\n",
            "Epoch 173: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0027, Train Linf Norm: 0.4858, Test Linf Norm: 0.2334\n",
            "Epoch 174: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0030, Train Linf Norm: 0.4469, Test Linf Norm: 0.3230\n",
            "Epoch 175: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0038, Train Linf Norm: 0.5271, Test Linf Norm: 0.4887\n",
            "Epoch 176: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0023, Train Linf Norm: 0.4995, Test Linf Norm: 0.1736\n",
            "Epoch 177: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0026, Train Linf Norm: 0.4754, Test Linf Norm: 0.2311\n",
            "Epoch 178: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0030, Train Linf Norm: 0.4882, Test Linf Norm: 0.3244\n",
            "Epoch 179: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0029, Train Linf Norm: 0.5140, Test Linf Norm: 0.3004\n",
            "Epoch 180: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0029, Train Linf Norm: 0.4811, Test Linf Norm: 0.3036\n",
            "Epoch 181: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0029, Train Linf Norm: 0.4829, Test Linf Norm: 0.2975\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:43:16,341]\u001b[0m Trial 61 finished with value: 0.002592882362008095 and parameters: {'n_layers': 3, 'n_units_0': 840, 'n_units_1': 616, 'n_units_2': 829, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.0006386244239209805, 'batch_size': 216, 'n_epochs': 182, 'scheduler': 'None'}. Best is trial 61 with value: 0.002592882362008095.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 182: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0026, Train Linf Norm: 0.4809, Test Linf Norm: 0.2405\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:43:17,902]\u001b[0m Trial 62 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.0594, Test Loss: 0.0005, Train L1 Norm: 0.4978, Test L1 Norm: 0.0562, Train Linf Norm: 76.5326, Test Linf Norm: 7.7197\n",
            "Epoch 2: Train Loss: 0.0015, Test Loss: 0.0002, Train L1 Norm: 0.0460, Test L1 Norm: 0.0206, Train Linf Norm: 4.8984, Test Linf Norm: 1.5016\n",
            "Epoch 3: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0271, Test L1 Norm: 0.0305, Train Linf Norm: 2.7955, Test Linf Norm: 4.2501\n",
            "Epoch 4: Train Loss: 0.0002, Test Loss: 0.0058, Train L1 Norm: 0.0243, Test L1 Norm: 0.1254, Train Linf Norm: 2.6775, Test Linf Norm: 18.5591\n",
            "Epoch 5: Train Loss: 0.0003, Test Loss: 0.0000, Train L1 Norm: 0.0218, Test L1 Norm: 0.0142, Train Linf Norm: 2.4118, Test Linf Norm: 1.6083\n",
            "Epoch 6: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0192, Test L1 Norm: 0.0172, Train Linf Norm: 2.3671, Test Linf Norm: 2.3280\n",
            "Epoch 7: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0169, Test L1 Norm: 0.0122, Train Linf Norm: 1.9563, Test Linf Norm: 0.5312\n",
            "Epoch 8: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0197, Test L1 Norm: 0.0079, Train Linf Norm: 2.5097, Test Linf Norm: 0.4854\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0141, Test L1 Norm: 0.0381, Train Linf Norm: 1.6435, Test Linf Norm: 5.9434\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0118, Test L1 Norm: 0.0079, Train Linf Norm: 1.3353, Test Linf Norm: 0.4433\n",
            "Epoch 11: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0114, Test L1 Norm: 0.0201, Train Linf Norm: 1.1151, Test Linf Norm: 3.0613\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0106, Test L1 Norm: 0.0100, Train Linf Norm: 1.1584, Test Linf Norm: 1.1629\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0014, Train L1 Norm: 0.0111, Test L1 Norm: 0.0699, Train Linf Norm: 1.2222, Test Linf Norm: 10.4731\n",
            "Epoch 14: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0114, Test L1 Norm: 0.0097, Train Linf Norm: 1.2325, Test Linf Norm: 1.1264\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0114, Test L1 Norm: 0.0202, Train Linf Norm: 1.3781, Test Linf Norm: 3.0669\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0106, Test L1 Norm: 0.0118, Train Linf Norm: 1.2029, Test Linf Norm: 1.6339\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0092, Test L1 Norm: 0.0150, Train Linf Norm: 1.0331, Test Linf Norm: 2.2757\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0063, Train Linf Norm: 1.4767, Test Linf Norm: 0.3380\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0105, Test L1 Norm: 0.0163, Train Linf Norm: 1.2836, Test Linf Norm: 2.4593\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0090, Test L1 Norm: 0.0122, Train Linf Norm: 1.0046, Test Linf Norm: 1.7403\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0103, Test L1 Norm: 0.0140, Train Linf Norm: 1.2599, Test Linf Norm: 2.1240\n",
            "Epoch 22: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0102, Test L1 Norm: 0.0112, Train Linf Norm: 1.1280, Test Linf Norm: 1.3338\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0129, Train Linf Norm: 0.9744, Test Linf Norm: 1.8928\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0084, Test L1 Norm: 0.0047, Train Linf Norm: 0.9522, Test Linf Norm: 0.3256\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0089, Train Linf Norm: 0.8492, Test Linf Norm: 1.1643\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0076, Train Linf Norm: 0.8179, Test Linf Norm: 0.9148\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0043, Train Linf Norm: 0.7392, Test Linf Norm: 0.3136\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0083, Test L1 Norm: 0.0047, Train Linf Norm: 0.9946, Test Linf Norm: 0.2997\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0073, Test L1 Norm: 0.0063, Train Linf Norm: 0.8106, Test Linf Norm: 0.3193\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0091, Test L1 Norm: 0.0041, Train Linf Norm: 1.0938, Test Linf Norm: 0.3245\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0053, Train Linf Norm: 0.8224, Test Linf Norm: 0.5288\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0102, Train Linf Norm: 0.8661, Test Linf Norm: 1.3021\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0065, Test L1 Norm: 0.0061, Train Linf Norm: 0.6845, Test Linf Norm: 0.3085\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0062, Train Linf Norm: 0.7949, Test Linf Norm: 0.7029\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0049, Train Linf Norm: 0.6916, Test Linf Norm: 0.4301\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0100, Train Linf Norm: 0.7791, Test Linf Norm: 1.1739\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0054, Train Linf Norm: 0.6888, Test Linf Norm: 0.6044\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0039, Train Linf Norm: 0.6945, Test Linf Norm: 0.2928\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0044, Train Linf Norm: 0.5932, Test Linf Norm: 0.2735\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0023, Train L1 Norm: 0.0072, Test L1 Norm: 0.1020, Train Linf Norm: 0.8119, Test Linf Norm: 15.7381\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0058, Train Linf Norm: 0.6309, Test Linf Norm: 0.5117\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0053, Train Linf Norm: 0.6861, Test Linf Norm: 0.3558\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0039, Train Linf Norm: 0.6177, Test Linf Norm: 0.3043\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0002, Train L1 Norm: 0.0057, Test L1 Norm: 0.0289, Train Linf Norm: 0.5934, Test Linf Norm: 4.3032\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0062, Train Linf Norm: 0.6054, Test Linf Norm: 0.3252\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0042, Train Linf Norm: 0.6038, Test Linf Norm: 0.3518\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0045, Train Linf Norm: 0.5707, Test Linf Norm: 0.4135\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0049, Train Linf Norm: 0.5676, Test Linf Norm: 0.5350\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0035, Train Linf Norm: 0.5915, Test Linf Norm: 0.2742\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0050, Train Linf Norm: 0.5580, Test Linf Norm: 0.5020\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0036, Train Linf Norm: 0.5646, Test Linf Norm: 0.2502\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0040, Train Linf Norm: 0.5371, Test Linf Norm: 0.3572\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0044, Train Linf Norm: 0.6783, Test Linf Norm: 0.2523\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0035, Train Linf Norm: 0.5077, Test Linf Norm: 0.2320\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0061, Train Linf Norm: 0.5468, Test Linf Norm: 0.7302\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0052, Train Linf Norm: 0.5829, Test Linf Norm: 0.2766\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0034, Train Linf Norm: 0.5373, Test Linf Norm: 0.2644\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0036, Train Linf Norm: 0.5469, Test Linf Norm: 0.3263\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0074, Train Linf Norm: 0.5690, Test Linf Norm: 0.9097\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0116, Train Linf Norm: 0.5375, Test Linf Norm: 1.5342\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0034, Train Linf Norm: 0.5424, Test Linf Norm: 0.2544\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0047, Train Linf Norm: 0.5705, Test Linf Norm: 0.4934\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0034, Train Linf Norm: 0.5656, Test Linf Norm: 0.2441\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0061, Train Linf Norm: 0.5664, Test Linf Norm: 0.3037\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0036, Train Linf Norm: 0.5062, Test Linf Norm: 0.2621\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0039, Train Linf Norm: 0.5929, Test Linf Norm: 0.3894\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0060, Train Linf Norm: 0.4940, Test Linf Norm: 0.7569\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0043, Train Linf Norm: 0.4058, Test Linf Norm: 0.4616\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0033, Train Linf Norm: 0.4158, Test Linf Norm: 0.2443\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0063, Train Linf Norm: 0.4965, Test Linf Norm: 0.7572\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0077, Train Linf Norm: 0.5222, Test Linf Norm: 0.5794\n",
            "Epoch 72: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0093, Test L1 Norm: 0.0047, Train Linf Norm: 1.1793, Test Linf Norm: 0.5516\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0048, Train Linf Norm: 0.4948, Test Linf Norm: 0.3285\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0047, Train Linf Norm: 0.5242, Test Linf Norm: 0.5503\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0038, Train Linf Norm: 0.4946, Test Linf Norm: 0.4010\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0038, Train Linf Norm: 0.4681, Test Linf Norm: 0.3902\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0066, Train Linf Norm: 0.4787, Test Linf Norm: 0.8959\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0132, Train Linf Norm: 0.4475, Test Linf Norm: 1.9809\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0037, Train Linf Norm: 0.4457, Test Linf Norm: 0.3562\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0031, Train Linf Norm: 0.4790, Test Linf Norm: 0.2284\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0056, Train Linf Norm: 0.4744, Test Linf Norm: 0.7456\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0043, Train Linf Norm: 0.4729, Test Linf Norm: 0.4854\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0057, Train Linf Norm: 0.4032, Test Linf Norm: 0.7739\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0037, Train Linf Norm: 0.4564, Test Linf Norm: 0.3212\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0048, Train Linf Norm: 0.4358, Test Linf Norm: 0.5633\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0059, Train Linf Norm: 0.4861, Test Linf Norm: 0.7852\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0047, Train Linf Norm: 0.3885, Test Linf Norm: 0.5590\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0152, Train Linf Norm: 0.4678, Test Linf Norm: 2.3188\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0052, Train Linf Norm: 0.4026, Test Linf Norm: 0.6834\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0062, Train Linf Norm: 0.4082, Test Linf Norm: 0.8529\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0135, Train Linf Norm: 0.4488, Test Linf Norm: 2.0808\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0037, Train Linf Norm: 0.4321, Test Linf Norm: 0.3273\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0054, Train Linf Norm: 0.4223, Test Linf Norm: 0.7038\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0059, Train Linf Norm: 0.4802, Test Linf Norm: 0.8122\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0051, Train Linf Norm: 0.4083, Test Linf Norm: 0.6431\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0053, Train Linf Norm: 0.4755, Test Linf Norm: 0.6977\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0074, Train Linf Norm: 0.3851, Test Linf Norm: 1.0996\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0080, Train Linf Norm: 0.4284, Test Linf Norm: 1.1962\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0042, Train Linf Norm: 0.3994, Test Linf Norm: 0.4220\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0074, Train Linf Norm: 0.4576, Test Linf Norm: 1.1212\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0080, Train Linf Norm: 0.4145, Test Linf Norm: 1.1895\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0066, Train Linf Norm: 0.3711, Test Linf Norm: 0.9828\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0057, Train Linf Norm: 0.4217, Test Linf Norm: 0.7853\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0071, Train Linf Norm: 0.4540, Test Linf Norm: 1.0547\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0035, Train Linf Norm: 0.4039, Test Linf Norm: 0.2266\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0057, Train Linf Norm: 0.4087, Test Linf Norm: 0.7834\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0067, Train Linf Norm: 0.4262, Test Linf Norm: 0.9905\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0068, Train Linf Norm: 0.3956, Test Linf Norm: 1.0042\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0041, Train Linf Norm: 0.4290, Test Linf Norm: 0.4565\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0159, Train Linf Norm: 0.3996, Test Linf Norm: 2.6401\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0040, Test L1 Norm: 0.0055, Train Linf Norm: 0.4053, Test Linf Norm: 0.2600\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0061, Train Linf Norm: 0.3958, Test Linf Norm: 0.7984\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0094, Train Linf Norm: 0.4193, Test Linf Norm: 1.4781\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0054, Train Linf Norm: 0.4099, Test Linf Norm: 0.7162\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0079, Train Linf Norm: 0.3938, Test Linf Norm: 1.2087\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0054, Train Linf Norm: 0.3615, Test Linf Norm: 0.7420\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0116, Train Linf Norm: 0.3658, Test Linf Norm: 1.8670\n",
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0070, Train Linf Norm: 0.3962, Test Linf Norm: 1.0458\n",
            "Epoch 119: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0095, Train Linf Norm: 0.3995, Test Linf Norm: 1.5264\n",
            "Epoch 120: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0057, Train Linf Norm: 0.3879, Test Linf Norm: 0.7230\n",
            "Epoch 121: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0070, Train Linf Norm: 0.4286, Test Linf Norm: 1.0454\n",
            "Epoch 122: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0085, Train Linf Norm: 0.3945, Test Linf Norm: 1.3711\n",
            "Epoch 123: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0060, Train Linf Norm: 0.4019, Test Linf Norm: 0.8599\n",
            "Epoch 124: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0072, Train Linf Norm: 0.3740, Test Linf Norm: 1.0897\n",
            "Epoch 125: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0064, Train Linf Norm: 0.4131, Test Linf Norm: 0.9127\n",
            "Epoch 126: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0076, Train Linf Norm: 0.3859, Test Linf Norm: 1.1500\n",
            "Epoch 127: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0172, Train Linf Norm: 0.4945, Test Linf Norm: 2.6321\n",
            "Epoch 128: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0061, Train Linf Norm: 0.4089, Test Linf Norm: 0.8814\n",
            "Epoch 129: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0097, Train Linf Norm: 0.3535, Test Linf Norm: 1.5023\n",
            "Epoch 130: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0097, Train Linf Norm: 0.4402, Test Linf Norm: 1.5607\n",
            "Epoch 131: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0092, Train Linf Norm: 0.3783, Test Linf Norm: 1.4617\n",
            "Epoch 132: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0065, Train Linf Norm: 0.3333, Test Linf Norm: 0.9382\n",
            "Epoch 133: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0060, Train Linf Norm: 0.3933, Test Linf Norm: 0.8767\n",
            "Epoch 134: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0075, Train Linf Norm: 0.3721, Test Linf Norm: 1.1296\n",
            "Epoch 135: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0071, Train Linf Norm: 0.3794, Test Linf Norm: 1.0840\n",
            "Epoch 136: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0065, Train Linf Norm: 0.3421, Test Linf Norm: 0.9668\n",
            "Epoch 137: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0034, Train Linf Norm: 0.3418, Test Linf Norm: 0.2137\n",
            "Epoch 138: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0070, Train Linf Norm: 0.3987, Test Linf Norm: 1.0366\n",
            "Epoch 139: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0117, Train Linf Norm: 0.4280, Test Linf Norm: 1.8776\n",
            "Epoch 140: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0083, Train Linf Norm: 0.3900, Test Linf Norm: 1.3334\n",
            "Epoch 141: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0060, Train Linf Norm: 0.3498, Test Linf Norm: 0.8844\n",
            "Epoch 142: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0063, Train Linf Norm: 0.3946, Test Linf Norm: 0.9271\n",
            "Epoch 143: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0063, Train Linf Norm: 0.3593, Test Linf Norm: 0.9422\n",
            "Epoch 144: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0053, Train Linf Norm: 0.3785, Test Linf Norm: 0.6852\n",
            "Epoch 145: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0070, Train Linf Norm: 0.3544, Test Linf Norm: 1.0874\n",
            "Epoch 146: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0089, Train Linf Norm: 0.3284, Test Linf Norm: 1.4356\n",
            "Epoch 147: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0063, Train Linf Norm: 0.3848, Test Linf Norm: 0.7605\n",
            "Epoch 148: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0046, Train Linf Norm: 0.3356, Test Linf Norm: 0.5006\n",
            "Epoch 149: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0064, Train Linf Norm: 0.3211, Test Linf Norm: 0.9687\n",
            "Epoch 150: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0082, Train Linf Norm: 0.3753, Test Linf Norm: 1.3246\n",
            "Epoch 151: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0084, Train Linf Norm: 0.3675, Test Linf Norm: 1.3679\n",
            "Epoch 152: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0212, Train Linf Norm: 0.3162, Test Linf Norm: 3.4625\n",
            "Epoch 153: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0061, Train Linf Norm: 0.3364, Test Linf Norm: 0.8216\n",
            "Epoch 154: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0043, Train Linf Norm: 0.4179, Test Linf Norm: 0.2842\n",
            "Epoch 155: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0068, Train Linf Norm: 0.3040, Test Linf Norm: 1.0403\n",
            "Epoch 156: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0073, Train Linf Norm: 0.4207, Test Linf Norm: 1.1043\n",
            "Epoch 157: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0100, Train Linf Norm: 0.3411, Test Linf Norm: 1.6403\n",
            "Epoch 158: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0079, Train Linf Norm: 0.3884, Test Linf Norm: 1.2522\n",
            "Epoch 159: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0082, Train Linf Norm: 0.3429, Test Linf Norm: 1.3164\n",
            "Epoch 160: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0082, Train Linf Norm: 0.3583, Test Linf Norm: 1.3036\n",
            "Epoch 161: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0059, Train Linf Norm: 0.3833, Test Linf Norm: 0.8124\n",
            "Epoch 162: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0081, Train Linf Norm: 0.3655, Test Linf Norm: 1.2958\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:48:04,563]\u001b[0m Trial 63 finished with value: 0.00720262367730029 and parameters: {'n_layers': 3, 'n_units_0': 709, 'n_units_1': 551, 'n_units_2': 831, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.0007507269667877237, 'batch_size': 198, 'n_epochs': 163, 'scheduler': 'None'}. Best is trial 61 with value: 0.002592882362008095.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 163: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0072, Train Linf Norm: 0.3583, Test Linf Norm: 1.0733\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:48:06,470]\u001b[0m Trial 64 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0535, Test Loss: 0.0006, Train L1 Norm: 0.2296, Test L1 Norm: 0.1305, Train Linf Norm: 23.1238, Test Linf Norm: 20.0400\n",
            "Epoch 1: Train Loss: 0.0527, Test Loss: 0.0003, Train L1 Norm: 0.2142, Test L1 Norm: 0.0871, Train Linf Norm: 27.3459, Test Linf Norm: 15.2990\n",
            "Epoch 2: Train Loss: 0.0012, Test Loss: 0.0002, Train L1 Norm: 0.0469, Test L1 Norm: 0.0502, Train Linf Norm: 5.6456, Test Linf Norm: 8.2276\n",
            "Epoch 3: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0272, Test L1 Norm: 0.0349, Train Linf Norm: 3.4918, Test Linf Norm: 5.5223\n",
            "Epoch 4: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0232, Test L1 Norm: 0.0099, Train Linf Norm: 2.8850, Test Linf Norm: 0.5607\n",
            "Epoch 5: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0188, Test L1 Norm: 0.0094, Train Linf Norm: 2.3462, Test Linf Norm: 0.8010\n",
            "Epoch 6: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0164, Test L1 Norm: 0.0198, Train Linf Norm: 1.9114, Test Linf Norm: 3.0195\n",
            "Epoch 7: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0143, Test L1 Norm: 0.0204, Train Linf Norm: 1.6077, Test Linf Norm: 3.1588\n",
            "Epoch 8: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0133, Test L1 Norm: 0.0066, Train Linf Norm: 1.5079, Test Linf Norm: 0.4291\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0118, Test L1 Norm: 0.0166, Train Linf Norm: 1.3698, Test Linf Norm: 2.5878\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0108, Test L1 Norm: 0.0103, Train Linf Norm: 1.2190, Test Linf Norm: 1.3977\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0090, Test L1 Norm: 0.0069, Train Linf Norm: 0.9080, Test Linf Norm: 0.4510\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0095, Test L1 Norm: 0.0064, Train Linf Norm: 1.0222, Test Linf Norm: 0.5977\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0088, Test L1 Norm: 0.0069, Train Linf Norm: 0.8609, Test Linf Norm: 0.6659\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0084, Test L1 Norm: 0.0145, Train Linf Norm: 0.9377, Test Linf Norm: 2.3373\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0062, Train Linf Norm: 0.8731, Test Linf Norm: 0.5785\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0070, Train Linf Norm: 0.8654, Test Linf Norm: 0.7485\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0143, Train Linf Norm: 0.7260, Test Linf Norm: 2.4229\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0095, Test L1 Norm: 0.0060, Train Linf Norm: 1.1369, Test Linf Norm: 0.3910\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0112, Train Linf Norm: 0.6226, Test Linf Norm: 1.8058\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0090, Train Linf Norm: 0.7079, Test Linf Norm: 1.3380\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0090, Train Linf Norm: 0.6973, Test Linf Norm: 1.3134\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0148, Train Linf Norm: 0.6479, Test Linf Norm: 2.5979\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0064, Test L1 Norm: 0.0219, Train Linf Norm: 0.6288, Test Linf Norm: 3.8665\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0093, Train Linf Norm: 0.6554, Test Linf Norm: 1.3547\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0085, Train Linf Norm: 0.8763, Test Linf Norm: 1.1551\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0139, Train Linf Norm: 0.6124, Test Linf Norm: 2.4683\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0096, Train Linf Norm: 1.0670, Test Linf Norm: 1.5158\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0103, Train Linf Norm: 0.5333, Test Linf Norm: 1.6587\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0074, Train Linf Norm: 0.6124, Test Linf Norm: 0.9906\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0127, Train Linf Norm: 0.5273, Test Linf Norm: 2.2175\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0107, Train Linf Norm: 0.5904, Test Linf Norm: 1.7998\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0075, Train Linf Norm: 0.5164, Test Linf Norm: 1.0479\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0002, Train L1 Norm: 0.0053, Test L1 Norm: 0.0405, Train Linf Norm: 0.5511, Test Linf Norm: 7.0076\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0122, Train Linf Norm: 0.7111, Test Linf Norm: 2.1483\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0113, Train Linf Norm: 0.5013, Test Linf Norm: 1.9403\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0142, Train Linf Norm: 0.4930, Test Linf Norm: 2.5650\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0123, Train Linf Norm: 0.5649, Test Linf Norm: 2.1584\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0103, Train Linf Norm: 0.5203, Test Linf Norm: 1.7495\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0111, Train Linf Norm: 0.4965, Test Linf Norm: 1.9248\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0123, Train Linf Norm: 0.4671, Test Linf Norm: 2.1992\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0129, Train Linf Norm: 0.5005, Test Linf Norm: 2.3215\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0141, Train Linf Norm: 0.4973, Test Linf Norm: 2.5775\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0110, Train Linf Norm: 0.5460, Test Linf Norm: 1.8922\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0126, Train Linf Norm: 0.4873, Test Linf Norm: 2.2813\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0105, Train Linf Norm: 0.5112, Test Linf Norm: 1.7711\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0107, Train Linf Norm: 0.4724, Test Linf Norm: 1.8443\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0165, Train Linf Norm: 0.4525, Test Linf Norm: 3.0318\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0132, Train Linf Norm: 0.5802, Test Linf Norm: 2.4123\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0126, Train Linf Norm: 0.4582, Test Linf Norm: 2.2900\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0124, Train Linf Norm: 0.4186, Test Linf Norm: 2.2366\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0146, Train Linf Norm: 0.4458, Test Linf Norm: 2.6814\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0129, Train Linf Norm: 0.5280, Test Linf Norm: 2.3661\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0145, Train Linf Norm: 0.4933, Test Linf Norm: 2.6847\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0128, Train Linf Norm: 0.4438, Test Linf Norm: 2.3453\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0127, Train Linf Norm: 0.4165, Test Linf Norm: 2.3220\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0121, Train Linf Norm: 0.5237, Test Linf Norm: 2.2039\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0167, Train Linf Norm: 0.4508, Test Linf Norm: 3.0984\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0120, Train Linf Norm: 0.4632, Test Linf Norm: 2.1899\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0130, Train Linf Norm: 0.6239, Test Linf Norm: 2.4024\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0130, Train Linf Norm: 0.3918, Test Linf Norm: 2.3926\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0134, Train Linf Norm: 0.3856, Test Linf Norm: 2.4931\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0122, Train Linf Norm: 0.4213, Test Linf Norm: 2.2441\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0147, Train Linf Norm: 0.3812, Test Linf Norm: 2.7069\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0123, Train Linf Norm: 0.4904, Test Linf Norm: 2.2603\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0124, Train Linf Norm: 0.4141, Test Linf Norm: 2.2800\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0129, Train Linf Norm: 0.4782, Test Linf Norm: 2.3942\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0125, Train Linf Norm: 0.3761, Test Linf Norm: 2.3115\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0121, Train Linf Norm: 0.3841, Test Linf Norm: 2.2267\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0120, Train Linf Norm: 0.4189, Test Linf Norm: 2.2149\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0143, Train Linf Norm: 0.3820, Test Linf Norm: 2.6971\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0121, Train Linf Norm: 0.4096, Test Linf Norm: 2.2368\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0127, Train Linf Norm: 0.4300, Test Linf Norm: 2.3806\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0206, Train Linf Norm: 0.4421, Test Linf Norm: 3.7409\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0130, Train Linf Norm: 0.5514, Test Linf Norm: 2.4317\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0138, Train Linf Norm: 0.4199, Test Linf Norm: 2.6143\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0127, Train Linf Norm: 0.4498, Test Linf Norm: 2.3793\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0131, Train Linf Norm: 0.4196, Test Linf Norm: 2.4771\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0120, Train Linf Norm: 0.4581, Test Linf Norm: 2.2383\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0116, Train Linf Norm: 0.4349, Test Linf Norm: 2.1008\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0176, Train Linf Norm: 0.4445, Test Linf Norm: 3.3430\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0132, Train Linf Norm: 0.3957, Test Linf Norm: 2.4989\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0113, Train Linf Norm: 0.4138, Test Linf Norm: 2.0622\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0117, Train Linf Norm: 0.3831, Test Linf Norm: 2.1848\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0130, Train Linf Norm: 0.4039, Test Linf Norm: 2.4731\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0119, Train Linf Norm: 0.3865, Test Linf Norm: 2.2248\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0118, Train Linf Norm: 0.3634, Test Linf Norm: 2.1938\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0129, Train Linf Norm: 0.3433, Test Linf Norm: 2.4540\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0145, Train Linf Norm: 0.4277, Test Linf Norm: 2.7518\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0133, Train Linf Norm: 0.3725, Test Linf Norm: 2.5188\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0136, Train Linf Norm: 0.3583, Test Linf Norm: 2.6046\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0115, Train Linf Norm: 0.3849, Test Linf Norm: 2.1497\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0089, Train Linf Norm: 0.3847, Test Linf Norm: 1.5161\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0138, Train Linf Norm: 0.4125, Test Linf Norm: 2.6401\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0134, Train Linf Norm: 0.4619, Test Linf Norm: 2.5755\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0122, Train Linf Norm: 0.3938, Test Linf Norm: 2.3018\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0148, Train Linf Norm: 0.3174, Test Linf Norm: 2.8525\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0140, Train Linf Norm: 0.3913, Test Linf Norm: 2.6746\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0128, Train Linf Norm: 0.4111, Test Linf Norm: 2.4145\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0166, Train Linf Norm: 0.3593, Test Linf Norm: 3.1718\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0103, Train Linf Norm: 0.3611, Test Linf Norm: 1.7988\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0176, Train Linf Norm: 0.3642, Test Linf Norm: 3.3347\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0123, Train Linf Norm: 0.3987, Test Linf Norm: 2.3311\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0144, Train Linf Norm: 0.3817, Test Linf Norm: 2.7643\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0110, Train Linf Norm: 0.3718, Test Linf Norm: 2.0368\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0149, Train Linf Norm: 0.3907, Test Linf Norm: 2.8598\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0152, Train Linf Norm: 0.4100, Test Linf Norm: 2.8739\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0120, Train Linf Norm: 0.3683, Test Linf Norm: 2.2879\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0139, Train Linf Norm: 0.4595, Test Linf Norm: 2.6906\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0132, Train Linf Norm: 0.4009, Test Linf Norm: 2.5345\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0116, Train Linf Norm: 0.3851, Test Linf Norm: 2.1965\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0125, Train Linf Norm: 0.3499, Test Linf Norm: 2.3836\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0127, Train Linf Norm: 0.3784, Test Linf Norm: 2.4263\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0132, Train Linf Norm: 0.3385, Test Linf Norm: 2.5432\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0131, Train Linf Norm: 0.3821, Test Linf Norm: 2.5175\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0141, Train Linf Norm: 0.3608, Test Linf Norm: 2.7446\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0102, Train Linf Norm: 0.3583, Test Linf Norm: 1.7133\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0116, Train Linf Norm: 0.3687, Test Linf Norm: 2.1706\n",
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0113, Train Linf Norm: 0.3906, Test Linf Norm: 2.1082\n",
            "Epoch 119: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0094, Train Linf Norm: 0.4096, Test Linf Norm: 1.6147\n",
            "Epoch 120: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0124, Train Linf Norm: 0.3303, Test Linf Norm: 2.3697\n",
            "Epoch 121: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0118, Train Linf Norm: 0.3804, Test Linf Norm: 2.2479\n",
            "Epoch 122: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0144, Train Linf Norm: 0.3604, Test Linf Norm: 2.7630\n",
            "Epoch 123: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0119, Train Linf Norm: 0.3403, Test Linf Norm: 2.2480\n",
            "Epoch 124: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0122, Train Linf Norm: 0.3455, Test Linf Norm: 2.3389\n",
            "Epoch 125: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0106, Train Linf Norm: 0.3170, Test Linf Norm: 1.9087\n",
            "Epoch 126: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0118, Train Linf Norm: 0.7653, Test Linf Norm: 2.2471\n",
            "Epoch 127: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0122, Train Linf Norm: 0.3796, Test Linf Norm: 2.3325\n",
            "Epoch 128: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0122, Train Linf Norm: 0.3589, Test Linf Norm: 2.3332\n",
            "Epoch 129: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0097, Train Linf Norm: 0.3974, Test Linf Norm: 1.7236\n",
            "Epoch 130: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0120, Train Linf Norm: 0.3299, Test Linf Norm: 2.3015\n",
            "Epoch 131: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0130, Train Linf Norm: 0.3447, Test Linf Norm: 2.5008\n",
            "Epoch 132: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0123, Train Linf Norm: 0.3019, Test Linf Norm: 2.3506\n",
            "Epoch 133: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0111, Train Linf Norm: 0.3558, Test Linf Norm: 2.0928\n",
            "Epoch 134: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0178, Train Linf Norm: 0.3739, Test Linf Norm: 3.4152\n",
            "Epoch 135: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0126, Train Linf Norm: 0.3606, Test Linf Norm: 2.4306\n",
            "Epoch 136: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0127, Train Linf Norm: 0.3691, Test Linf Norm: 2.4249\n",
            "Epoch 137: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0113, Train Linf Norm: 0.3609, Test Linf Norm: 2.1566\n",
            "Epoch 138: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0105, Train Linf Norm: 0.3138, Test Linf Norm: 1.9784\n",
            "Epoch 139: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0113, Train Linf Norm: 0.3786, Test Linf Norm: 2.1414\n",
            "Epoch 140: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0104, Train Linf Norm: 0.3493, Test Linf Norm: 1.9392\n",
            "Epoch 141: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0101, Train Linf Norm: 0.3195, Test Linf Norm: 1.8900\n",
            "Epoch 142: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0096, Train Linf Norm: 0.3280, Test Linf Norm: 1.7512\n",
            "Epoch 143: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0108, Train Linf Norm: 0.3179, Test Linf Norm: 2.0362\n",
            "Epoch 144: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0109, Train Linf Norm: 0.3334, Test Linf Norm: 2.0576\n",
            "Epoch 145: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0107, Train Linf Norm: 0.3522, Test Linf Norm: 2.0193\n",
            "Epoch 146: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0108, Train Linf Norm: 0.3243, Test Linf Norm: 2.0332\n",
            "Epoch 147: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0087, Train Linf Norm: 0.3232, Test Linf Norm: 1.4906\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:52:11,227]\u001b[0m Trial 65 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 148: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0089, Train Linf Norm: 0.3143, Test Linf Norm: 1.6254\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:52:12,995]\u001b[0m Trial 66 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0632, Test Loss: 0.0005, Train L1 Norm: 0.3840, Test L1 Norm: 0.1121, Train Linf Norm: 53.2154, Test Linf Norm: 17.9735\n",
            "Epoch 1: Train Loss: 0.0566, Test Loss: 0.0005, Train L1 Norm: 0.2021, Test L1 Norm: 0.0767, Train Linf Norm: 21.5445, Test Linf Norm: 12.1400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:52:16,588]\u001b[0m Trial 67 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Train Loss: 0.0010, Test Loss: 0.0003, Train L1 Norm: 0.0529, Test L1 Norm: 0.0895, Train Linf Norm: 6.4822, Test Linf Norm: 15.6442\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:52:18,563]\u001b[0m Trial 68 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.2655, Test Loss: 0.0390, Train L1 Norm: 3.3385, Test L1 Norm: 1.6764, Train Linf Norm: 524.7658, Test Linf Norm: 303.3863\n",
            "Epoch 1: Train Loss: 0.0318, Test Loss: 0.0006, Train L1 Norm: 0.1651, Test L1 Norm: 0.0554, Train Linf Norm: 13.2081, Test Linf Norm: 5.9670\n",
            "Epoch 2: Train Loss: 0.0003, Test Loss: 0.0002, Train L1 Norm: 0.0295, Test L1 Norm: 0.0421, Train Linf Norm: 2.5456, Test Linf Norm: 4.6330\n",
            "Epoch 3: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0210, Test L1 Norm: 0.0122, Train Linf Norm: 1.7387, Test Linf Norm: 0.9303\n",
            "Epoch 4: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0164, Test L1 Norm: 0.0143, Train Linf Norm: 1.4538, Test Linf Norm: 1.3979\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0141, Test L1 Norm: 0.0095, Train Linf Norm: 1.2763, Test Linf Norm: 0.3336\n",
            "Epoch 6: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0138, Test L1 Norm: 0.0065, Train Linf Norm: 1.0982, Test Linf Norm: 0.3966\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0101, Test L1 Norm: 0.0061, Train Linf Norm: 0.8169, Test Linf Norm: 0.3693\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0101, Test L1 Norm: 0.0076, Train Linf Norm: 0.8417, Test Linf Norm: 0.5549\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0088, Test L1 Norm: 0.0054, Train Linf Norm: 0.6998, Test Linf Norm: 0.3172\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0089, Test L1 Norm: 0.0092, Train Linf Norm: 0.7372, Test Linf Norm: 0.6874\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0083, Test L1 Norm: 0.0052, Train Linf Norm: 0.7007, Test Linf Norm: 0.3159\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0052, Train Linf Norm: 0.6641, Test Linf Norm: 0.3058\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0051, Train Linf Norm: 0.6738, Test Linf Norm: 0.2999\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0052, Train Linf Norm: 0.6537, Test Linf Norm: 0.3092\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0052, Train Linf Norm: 0.6532, Test Linf Norm: 0.3138\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0052, Train Linf Norm: 0.6421, Test Linf Norm: 0.3089\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6258, Test Linf Norm: 0.3029\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0052, Train Linf Norm: 0.6432, Test Linf Norm: 0.3064\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6225, Test Linf Norm: 0.3006\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6498, Test Linf Norm: 0.3026\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6337, Test Linf Norm: 0.3027\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6393, Test Linf Norm: 0.3031\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6480, Test Linf Norm: 0.3034\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6343, Test Linf Norm: 0.3036\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6470, Test Linf Norm: 0.3038\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6393, Test Linf Norm: 0.3038\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6419, Test Linf Norm: 0.3040\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6185, Test Linf Norm: 0.3040\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6359, Test Linf Norm: 0.3039\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6033, Test Linf Norm: 0.3042\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6406, Test Linf Norm: 0.3043\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6389, Test Linf Norm: 0.3043\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6289, Test Linf Norm: 0.3044\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6278, Test Linf Norm: 0.3045\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6347, Test Linf Norm: 0.3045\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6367, Test Linf Norm: 0.3046\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6396, Test Linf Norm: 0.3047\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6392, Test Linf Norm: 0.3047\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6414, Test Linf Norm: 0.3048\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6355, Test Linf Norm: 0.3049\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6335, Test Linf Norm: 0.3049\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6357, Test Linf Norm: 0.3049\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6365, Test Linf Norm: 0.3049\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6327, Test Linf Norm: 0.3049\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6186, Test Linf Norm: 0.3049\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6291, Test Linf Norm: 0.3049\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6414, Test Linf Norm: 0.3049\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6332, Test Linf Norm: 0.3049\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6425, Test Linf Norm: 0.3049\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6344, Test Linf Norm: 0.3049\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6307, Test Linf Norm: 0.3049\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6207, Test Linf Norm: 0.3049\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6449, Test Linf Norm: 0.3049\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6343, Test Linf Norm: 0.3049\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6365, Test Linf Norm: 0.3049\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6376, Test Linf Norm: 0.3049\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6436, Test Linf Norm: 0.3049\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6451, Test Linf Norm: 0.3049\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6383, Test Linf Norm: 0.3049\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6359, Test Linf Norm: 0.3049\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6386, Test Linf Norm: 0.3049\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6429, Test Linf Norm: 0.3049\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6347, Test Linf Norm: 0.3049\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6307, Test Linf Norm: 0.3049\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6259, Test Linf Norm: 0.3049\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6343, Test Linf Norm: 0.3049\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6495, Test Linf Norm: 0.3049\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6321, Test Linf Norm: 0.3049\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6351, Test Linf Norm: 0.3049\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6307, Test Linf Norm: 0.3049\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6319, Test Linf Norm: 0.3049\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6298, Test Linf Norm: 0.3049\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6355, Test Linf Norm: 0.3049\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6410, Test Linf Norm: 0.3049\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6296, Test Linf Norm: 0.3049\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6303, Test Linf Norm: 0.3049\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6349, Test Linf Norm: 0.3049\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6401, Test Linf Norm: 0.3049\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6300, Test Linf Norm: 0.3049\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6311, Test Linf Norm: 0.3049\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6299, Test Linf Norm: 0.3049\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6426, Test Linf Norm: 0.3049\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6356, Test Linf Norm: 0.3049\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6399, Test Linf Norm: 0.3049\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6399, Test Linf Norm: 0.3049\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6396, Test Linf Norm: 0.3049\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6255, Test Linf Norm: 0.3049\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6242, Test Linf Norm: 0.3049\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6411, Test Linf Norm: 0.3049\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6400, Test Linf Norm: 0.3049\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6253, Test Linf Norm: 0.3049\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6365, Test Linf Norm: 0.3049\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6296, Test Linf Norm: 0.3049\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6351, Test Linf Norm: 0.3049\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6332, Test Linf Norm: 0.3049\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6315, Test Linf Norm: 0.3049\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6314, Test Linf Norm: 0.3049\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6462, Test Linf Norm: 0.3049\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6393, Test Linf Norm: 0.3049\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6379, Test Linf Norm: 0.3049\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6381, Test Linf Norm: 0.3049\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6326, Test Linf Norm: 0.3049\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6385, Test Linf Norm: 0.3049\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6316, Test Linf Norm: 0.3049\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6303, Test Linf Norm: 0.3049\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6323, Test Linf Norm: 0.3049\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6353, Test Linf Norm: 0.3049\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6300, Test Linf Norm: 0.3049\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6381, Test Linf Norm: 0.3049\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6427, Test Linf Norm: 0.3049\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6266, Test Linf Norm: 0.3049\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6304, Test Linf Norm: 0.3049\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6423, Test Linf Norm: 0.3049\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6384, Test Linf Norm: 0.3049\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6329, Test Linf Norm: 0.3049\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6406, Test Linf Norm: 0.3049\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6306, Test Linf Norm: 0.3049\n",
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6398, Test Linf Norm: 0.3049\n",
            "Epoch 119: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6322, Test Linf Norm: 0.3049\n",
            "Epoch 120: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6439, Test Linf Norm: 0.3049\n",
            "Epoch 121: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6351, Test Linf Norm: 0.3049\n",
            "Epoch 122: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6312, Test Linf Norm: 0.3049\n",
            "Epoch 123: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6356, Test Linf Norm: 0.3049\n",
            "Epoch 124: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6351, Test Linf Norm: 0.3049\n",
            "Epoch 125: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6415, Test Linf Norm: 0.3049\n",
            "Epoch 126: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6186, Test Linf Norm: 0.3049\n",
            "Epoch 127: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6395, Test Linf Norm: 0.3049\n",
            "Epoch 128: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6399, Test Linf Norm: 0.3049\n",
            "Epoch 129: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6314, Test Linf Norm: 0.3049\n",
            "Epoch 130: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6232, Test Linf Norm: 0.3049\n",
            "Epoch 131: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6303, Test Linf Norm: 0.3049\n",
            "Epoch 132: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6349, Test Linf Norm: 0.3049\n",
            "Epoch 133: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6380, Test Linf Norm: 0.3049\n",
            "Epoch 134: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6317, Test Linf Norm: 0.3049\n",
            "Epoch 135: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6338, Test Linf Norm: 0.3049\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:56:54,613]\u001b[0m Trial 69 finished with value: 0.00512345370145049 and parameters: {'n_layers': 3, 'n_units_0': 798, 'n_units_1': 713, 'n_units_2': 936, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.0004159805159066751, 'batch_size': 149, 'n_epochs': 136, 'scheduler': 'StepLR', 'step_size': 10, 'gamma': 0.058891204263889524}. Best is trial 61 with value: 0.002592882362008095.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 136: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0051, Train Linf Norm: 0.6328, Test Linf Norm: 0.3049\n",
            "Epoch 1: Train Loss: 0.0490, Test Loss: 0.0010, Train L1 Norm: 0.2807, Test L1 Norm: 0.0792, Train Linf Norm: 35.8777, Test Linf Norm: 11.8416\n",
            "Epoch 2: Train Loss: 0.0008, Test Loss: 0.0007, Train L1 Norm: 0.0376, Test L1 Norm: 0.0447, Train Linf Norm: 3.5408, Test Linf Norm: 5.6972\n",
            "Epoch 3: Train Loss: 0.0004, Test Loss: 0.0001, Train L1 Norm: 0.0297, Test L1 Norm: 0.0165, Train Linf Norm: 2.9207, Test Linf Norm: 1.3518\n",
            "Epoch 4: Train Loss: 0.0009, Test Loss: 0.0001, Train L1 Norm: 0.0286, Test L1 Norm: 0.0148, Train Linf Norm: 2.7226, Test Linf Norm: 1.1363\n",
            "Epoch 5: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0175, Test L1 Norm: 0.0200, Train Linf Norm: 1.6981, Test Linf Norm: 2.5884\n",
            "Epoch 6: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0154, Test L1 Norm: 0.0310, Train Linf Norm: 1.4096, Test Linf Norm: 4.8647\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0137, Test L1 Norm: 0.0201, Train Linf Norm: 1.3472, Test Linf Norm: 2.8587\n",
            "Epoch 8: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0130, Test L1 Norm: 0.0247, Train Linf Norm: 1.1705, Test Linf Norm: 3.9012\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0115, Test L1 Norm: 0.0280, Train Linf Norm: 1.0362, Test Linf Norm: 4.4650\n",
            "Epoch 10: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0113, Test L1 Norm: 0.0222, Train Linf Norm: 0.9605, Test Linf Norm: 3.4963\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0093, Test L1 Norm: 0.0218, Train Linf Norm: 0.7776, Test Linf Norm: 3.4237\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0092, Test L1 Norm: 0.0223, Train Linf Norm: 0.7769, Test Linf Norm: 3.5281\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0091, Test L1 Norm: 0.0217, Train Linf Norm: 0.7743, Test Linf Norm: 3.4188\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0091, Test L1 Norm: 0.0212, Train Linf Norm: 0.7589, Test Linf Norm: 3.3199\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0089, Test L1 Norm: 0.0212, Train Linf Norm: 0.7393, Test Linf Norm: 3.3124\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0089, Test L1 Norm: 0.0215, Train Linf Norm: 0.7375, Test Linf Norm: 3.3925\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0088, Test L1 Norm: 0.0209, Train Linf Norm: 0.7291, Test Linf Norm: 3.2590\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0088, Test L1 Norm: 0.0207, Train Linf Norm: 0.7384, Test Linf Norm: 3.2392\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0088, Test L1 Norm: 0.0212, Train Linf Norm: 0.7385, Test Linf Norm: 3.3433\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0212, Train Linf Norm: 0.7423, Test Linf Norm: 3.3345\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0088, Test L1 Norm: 0.0210, Train Linf Norm: 0.7385, Test Linf Norm: 3.3079\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0209, Train Linf Norm: 0.7245, Test Linf Norm: 3.2846\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0208, Train Linf Norm: 0.7227, Test Linf Norm: 3.2665\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0208, Train Linf Norm: 0.7117, Test Linf Norm: 3.2496\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0208, Train Linf Norm: 0.7207, Test Linf Norm: 3.2485\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0207, Train Linf Norm: 0.7044, Test Linf Norm: 3.2329\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0207, Train Linf Norm: 0.7228, Test Linf Norm: 3.2350\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0207, Train Linf Norm: 0.7097, Test Linf Norm: 3.2360\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0207, Train Linf Norm: 0.7253, Test Linf Norm: 3.2333\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0206, Train Linf Norm: 0.7194, Test Linf Norm: 3.2209\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:57:47,980]\u001b[0m Trial 70 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0206, Train Linf Norm: 0.7222, Test Linf Norm: 3.2205\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 13:57:49,933]\u001b[0m Trial 71 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.0402, Test Loss: 0.0002, Train L1 Norm: 0.2817, Test L1 Norm: 0.0403, Train Linf Norm: 28.6047, Test Linf Norm: 3.6452\n",
            "Epoch 2: Train Loss: 0.0004, Test Loss: 0.0001, Train L1 Norm: 0.0268, Test L1 Norm: 0.0144, Train Linf Norm: 1.9321, Test Linf Norm: 0.7953\n",
            "Epoch 3: Train Loss: 0.0003, Test Loss: 0.0000, Train L1 Norm: 0.0201, Test L1 Norm: 0.0173, Train Linf Norm: 1.3801, Test Linf Norm: 1.6766\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0129, Test L1 Norm: 0.0140, Train Linf Norm: 0.9577, Test Linf Norm: 1.3059\n",
            "Epoch 5: Train Loss: 0.0003, Test Loss: 0.0000, Train L1 Norm: 0.0143, Test L1 Norm: 0.0077, Train Linf Norm: 0.9930, Test Linf Norm: 0.4819\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0114, Test L1 Norm: 0.0081, Train Linf Norm: 0.9037, Test Linf Norm: 0.6066\n",
            "Epoch 7: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0112, Test L1 Norm: 0.0084, Train Linf Norm: 0.8681, Test Linf Norm: 0.6388\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0091, Test L1 Norm: 0.0055, Train Linf Norm: 0.6893, Test Linf Norm: 0.2844\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0091, Test L1 Norm: 0.0063, Train Linf Norm: 0.7298, Test Linf Norm: 0.3063\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0054, Train Linf Norm: 0.6671, Test Linf Norm: 0.3024\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0048, Train Linf Norm: 0.6769, Test Linf Norm: 0.2742\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0048, Train Linf Norm: 0.6253, Test Linf Norm: 0.2576\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0046, Train Linf Norm: 0.5942, Test Linf Norm: 0.2614\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0045, Train Linf Norm: 0.5865, Test Linf Norm: 0.2530\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0044, Train Linf Norm: 0.5755, Test Linf Norm: 0.2515\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0044, Train Linf Norm: 0.5644, Test Linf Norm: 0.2534\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0046, Train Linf Norm: 0.5841, Test Linf Norm: 0.2631\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0045, Train Linf Norm: 0.5773, Test Linf Norm: 0.2564\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0044, Train Linf Norm: 0.5706, Test Linf Norm: 0.2445\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0046, Train Linf Norm: 0.5741, Test Linf Norm: 0.2536\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0044, Train Linf Norm: 0.5677, Test Linf Norm: 0.2462\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0044, Train Linf Norm: 0.5749, Test Linf Norm: 0.2440\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0045, Train Linf Norm: 0.5640, Test Linf Norm: 0.2467\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0044, Train Linf Norm: 0.5594, Test Linf Norm: 0.2428\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0044, Train Linf Norm: 0.5644, Test Linf Norm: 0.2445\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0044, Train Linf Norm: 0.5655, Test Linf Norm: 0.2439\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0044, Train Linf Norm: 0.5501, Test Linf Norm: 0.2445\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0044, Train Linf Norm: 0.5608, Test Linf Norm: 0.2446\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5609, Test Linf Norm: 0.2453\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5530, Test Linf Norm: 0.2456\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5662, Test Linf Norm: 0.2459\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5776, Test Linf Norm: 0.2458\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5708, Test Linf Norm: 0.2460\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5654, Test Linf Norm: 0.2460\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5623, Test Linf Norm: 0.2466\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5743, Test Linf Norm: 0.2469\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5681, Test Linf Norm: 0.2468\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5592, Test Linf Norm: 0.2467\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5687, Test Linf Norm: 0.2466\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5603, Test Linf Norm: 0.2465\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5641, Test Linf Norm: 0.2464\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5673, Test Linf Norm: 0.2463\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5768, Test Linf Norm: 0.2462\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5629, Test Linf Norm: 0.2461\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5627, Test Linf Norm: 0.2460\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5702, Test Linf Norm: 0.2459\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5409, Test Linf Norm: 0.2458\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5663, Test Linf Norm: 0.2457\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5585, Test Linf Norm: 0.2457\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5704, Test Linf Norm: 0.2457\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5655, Test Linf Norm: 0.2457\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5620, Test Linf Norm: 0.2457\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5672, Test Linf Norm: 0.2457\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5594, Test Linf Norm: 0.2457\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5643, Test Linf Norm: 0.2457\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5613, Test Linf Norm: 0.2457\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5690, Test Linf Norm: 0.2457\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5697, Test Linf Norm: 0.2457\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5681, Test Linf Norm: 0.2457\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5657, Test Linf Norm: 0.2457\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5586, Test Linf Norm: 0.2457\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5675, Test Linf Norm: 0.2457\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5597, Test Linf Norm: 0.2457\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5655, Test Linf Norm: 0.2457\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5697, Test Linf Norm: 0.2457\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5650, Test Linf Norm: 0.2457\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5684, Test Linf Norm: 0.2457\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5760, Test Linf Norm: 0.2457\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5661, Test Linf Norm: 0.2457\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5507, Test Linf Norm: 0.2457\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5576, Test Linf Norm: 0.2457\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5605, Test Linf Norm: 0.2457\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5608, Test Linf Norm: 0.2457\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5628, Test Linf Norm: 0.2457\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5724, Test Linf Norm: 0.2457\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5685, Test Linf Norm: 0.2457\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5612, Test Linf Norm: 0.2457\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5668, Test Linf Norm: 0.2457\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5670, Test Linf Norm: 0.2457\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5112, Test Linf Norm: 0.2457\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5629, Test Linf Norm: 0.2457\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5621, Test Linf Norm: 0.2457\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5626, Test Linf Norm: 0.2457\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5757, Test Linf Norm: 0.2457\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5714, Test Linf Norm: 0.2457\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5640, Test Linf Norm: 0.2457\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5625, Test Linf Norm: 0.2457\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5706, Test Linf Norm: 0.2457\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5588, Test Linf Norm: 0.2457\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5702, Test Linf Norm: 0.2457\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5668, Test Linf Norm: 0.2457\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5651, Test Linf Norm: 0.2457\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5521, Test Linf Norm: 0.2457\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5727, Test Linf Norm: 0.2457\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5573, Test Linf Norm: 0.2457\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5666, Test Linf Norm: 0.2457\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5610, Test Linf Norm: 0.2457\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5665, Test Linf Norm: 0.2457\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5676, Test Linf Norm: 0.2457\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5563, Test Linf Norm: 0.2457\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5650, Test Linf Norm: 0.2457\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5565, Test Linf Norm: 0.2457\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5623, Test Linf Norm: 0.2457\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5723, Test Linf Norm: 0.2457\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5683, Test Linf Norm: 0.2457\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5672, Test Linf Norm: 0.2457\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5635, Test Linf Norm: 0.2457\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5625, Test Linf Norm: 0.2457\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5691, Test Linf Norm: 0.2457\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5748, Test Linf Norm: 0.2457\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5632, Test Linf Norm: 0.2457\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5727, Test Linf Norm: 0.2457\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5719, Test Linf Norm: 0.2457\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5614, Test Linf Norm: 0.2457\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5330, Test Linf Norm: 0.2457\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5587, Test Linf Norm: 0.2457\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5670, Test Linf Norm: 0.2457\n",
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5647, Test Linf Norm: 0.2457\n",
            "Epoch 119: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5611, Test Linf Norm: 0.2457\n",
            "Epoch 120: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5693, Test Linf Norm: 0.2457\n",
            "Epoch 121: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5642, Test Linf Norm: 0.2457\n",
            "Epoch 122: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5523, Test Linf Norm: 0.2457\n",
            "Epoch 123: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5606, Test Linf Norm: 0.2457\n",
            "Epoch 124: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5688, Test Linf Norm: 0.2457\n",
            "Epoch 125: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5721, Test Linf Norm: 0.2457\n",
            "Epoch 126: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5692, Test Linf Norm: 0.2457\n",
            "Epoch 127: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5598, Test Linf Norm: 0.2457\n",
            "Epoch 128: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5657, Test Linf Norm: 0.2457\n",
            "Epoch 129: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5694, Test Linf Norm: 0.2457\n",
            "Epoch 130: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5695, Test Linf Norm: 0.2457\n",
            "Epoch 131: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5704, Test Linf Norm: 0.2457\n",
            "Epoch 132: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5691, Test Linf Norm: 0.2457\n",
            "Epoch 133: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5702, Test Linf Norm: 0.2457\n",
            "Epoch 134: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5704, Test Linf Norm: 0.2457\n",
            "Epoch 135: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5709, Test Linf Norm: 0.2457\n",
            "Epoch 136: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5608, Test Linf Norm: 0.2457\n",
            "Epoch 137: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5678, Test Linf Norm: 0.2457\n",
            "Epoch 138: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5540, Test Linf Norm: 0.2457\n",
            "Epoch 139: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5543, Test Linf Norm: 0.2457\n",
            "Epoch 140: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5623, Test Linf Norm: 0.2457\n",
            "Epoch 141: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5666, Test Linf Norm: 0.2457\n",
            "Epoch 142: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5690, Test Linf Norm: 0.2457\n",
            "Epoch 143: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5657, Test Linf Norm: 0.2457\n",
            "Epoch 144: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5726, Test Linf Norm: 0.2457\n",
            "Epoch 145: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5684, Test Linf Norm: 0.2457\n",
            "Epoch 146: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5605, Test Linf Norm: 0.2457\n",
            "Epoch 147: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5646, Test Linf Norm: 0.2457\n",
            "Epoch 148: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5676, Test Linf Norm: 0.2457\n",
            "Epoch 149: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5657, Test Linf Norm: 0.2457\n",
            "Epoch 150: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5625, Test Linf Norm: 0.2457\n",
            "Epoch 151: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5699, Test Linf Norm: 0.2457\n",
            "Epoch 152: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5655, Test Linf Norm: 0.2457\n",
            "Epoch 153: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5732, Test Linf Norm: 0.2457\n",
            "Epoch 154: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5689, Test Linf Norm: 0.2457\n",
            "Epoch 155: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5654, Test Linf Norm: 0.2457\n",
            "Epoch 156: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5366, Test Linf Norm: 0.2457\n",
            "Epoch 157: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5641, Test Linf Norm: 0.2457\n",
            "Epoch 158: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5688, Test Linf Norm: 0.2457\n",
            "Epoch 159: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5649, Test Linf Norm: 0.2457\n",
            "Epoch 160: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5641, Test Linf Norm: 0.2457\n",
            "Epoch 161: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5754, Test Linf Norm: 0.2457\n",
            "Epoch 162: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5643, Test Linf Norm: 0.2457\n",
            "Epoch 163: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5747, Test Linf Norm: 0.2457\n",
            "Epoch 164: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5720, Test Linf Norm: 0.2457\n",
            "Epoch 165: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5539, Test Linf Norm: 0.2457\n",
            "Epoch 166: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5701, Test Linf Norm: 0.2457\n",
            "Epoch 167: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5663, Test Linf Norm: 0.2457\n",
            "Epoch 168: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5579, Test Linf Norm: 0.2457\n",
            "Epoch 169: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5647, Test Linf Norm: 0.2457\n",
            "Epoch 170: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5672, Test Linf Norm: 0.2457\n",
            "Epoch 171: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5724, Test Linf Norm: 0.2457\n",
            "Epoch 172: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5647, Test Linf Norm: 0.2457\n",
            "Epoch 173: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5657, Test Linf Norm: 0.2457\n",
            "Epoch 174: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5625, Test Linf Norm: 0.2457\n",
            "Epoch 175: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5685, Test Linf Norm: 0.2457\n",
            "Epoch 176: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5655, Test Linf Norm: 0.2457\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:04:00,335]\u001b[0m Trial 72 finished with value: 0.004347703158855438 and parameters: {'n_layers': 3, 'n_units_0': 858, 'n_units_1': 762, 'n_units_2': 774, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.0007294498231179089, 'batch_size': 140, 'n_epochs': 177, 'scheduler': 'StepLR', 'step_size': 12, 'gamma': 0.05962393317753145}. Best is trial 61 with value: 0.002592882362008095.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 177: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.5641, Test Linf Norm: 0.2457\n",
            "Epoch 1: Train Loss: 0.0413, Test Loss: 0.0003, Train L1 Norm: 0.1959, Test L1 Norm: 0.0259, Train Linf Norm: 16.5646, Test Linf Norm: 1.6018\n",
            "Epoch 2: Train Loss: 0.0005, Test Loss: 0.0001, Train L1 Norm: 0.0325, Test L1 Norm: 0.0215, Train Linf Norm: 2.3833, Test Linf Norm: 1.6820\n",
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0265, Test L1 Norm: 0.0117, Train Linf Norm: 2.3568, Test Linf Norm: 0.5494\n",
            "Epoch 4: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0207, Test L1 Norm: 0.0157, Train Linf Norm: 1.7321, Test Linf Norm: 1.1728\n",
            "Epoch 5: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0148, Test L1 Norm: 0.0085, Train Linf Norm: 1.1751, Test Linf Norm: 0.4766\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0128, Test L1 Norm: 0.0077, Train Linf Norm: 0.9535, Test Linf Norm: 0.4180\n",
            "Epoch 7: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0110, Test L1 Norm: 0.0069, Train Linf Norm: 0.7370, Test Linf Norm: 0.3867\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0068, Train Linf Norm: 0.7535, Test Linf Norm: 0.3869\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0102, Test L1 Norm: 0.0126, Train Linf Norm: 0.7359, Test Linf Norm: 1.1303\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0002, Train L1 Norm: 0.0082, Test L1 Norm: 0.0129, Train Linf Norm: 0.5838, Test Linf Norm: 0.4346\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0095, Test L1 Norm: 0.0055, Train Linf Norm: 0.7191, Test Linf Norm: 0.3255\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0086, Test L1 Norm: 0.0084, Train Linf Norm: 0.6326, Test Linf Norm: 0.6690\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0053, Train Linf Norm: 0.5868, Test Linf Norm: 0.3130\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0052, Train Linf Norm: 0.6005, Test Linf Norm: 0.3058\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0052, Train Linf Norm: 0.5917, Test Linf Norm: 0.3086\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0052, Train Linf Norm: 0.5854, Test Linf Norm: 0.3054\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0052, Train Linf Norm: 0.5972, Test Linf Norm: 0.3067\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0052, Train Linf Norm: 0.5960, Test Linf Norm: 0.3040\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5720, Test Linf Norm: 0.3048\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5972, Test Linf Norm: 0.3066\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0052, Train Linf Norm: 0.5910, Test Linf Norm: 0.3078\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5852, Test Linf Norm: 0.3064\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0053, Train Linf Norm: 0.5864, Test Linf Norm: 0.3149\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5785, Test Linf Norm: 0.3068\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5749, Test Linf Norm: 0.3077\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5810, Test Linf Norm: 0.3081\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5815, Test Linf Norm: 0.3083\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5832, Test Linf Norm: 0.3086\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5856, Test Linf Norm: 0.3083\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5906, Test Linf Norm: 0.3083\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5845, Test Linf Norm: 0.3081\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5748, Test Linf Norm: 0.3081\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5771, Test Linf Norm: 0.3081\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5733, Test Linf Norm: 0.3078\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5864, Test Linf Norm: 0.3077\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5820, Test Linf Norm: 0.3076\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5815, Test Linf Norm: 0.3076\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5690, Test Linf Norm: 0.3077\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5666, Test Linf Norm: 0.3077\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5809, Test Linf Norm: 0.3078\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5773, Test Linf Norm: 0.3079\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5837, Test Linf Norm: 0.3079\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5808, Test Linf Norm: 0.3080\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5862, Test Linf Norm: 0.3081\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5840, Test Linf Norm: 0.3081\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5881, Test Linf Norm: 0.3082\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5772, Test Linf Norm: 0.3082\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5883, Test Linf Norm: 0.3083\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5725, Test Linf Norm: 0.3083\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5801, Test Linf Norm: 0.3083\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5862, Test Linf Norm: 0.3083\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5857, Test Linf Norm: 0.3083\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5802, Test Linf Norm: 0.3083\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5777, Test Linf Norm: 0.3083\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5832, Test Linf Norm: 0.3083\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5778, Test Linf Norm: 0.3083\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5823, Test Linf Norm: 0.3083\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5717, Test Linf Norm: 0.3083\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5840, Test Linf Norm: 0.3083\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5795, Test Linf Norm: 0.3083\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5772, Test Linf Norm: 0.3083\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5849, Test Linf Norm: 0.3083\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5672, Test Linf Norm: 0.3083\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5778, Test Linf Norm: 0.3083\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5801, Test Linf Norm: 0.3083\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5742, Test Linf Norm: 0.3083\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5695, Test Linf Norm: 0.3083\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5788, Test Linf Norm: 0.3083\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5780, Test Linf Norm: 0.3083\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5723, Test Linf Norm: 0.3083\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5789, Test Linf Norm: 0.3083\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5745, Test Linf Norm: 0.3083\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5775, Test Linf Norm: 0.3083\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5824, Test Linf Norm: 0.3083\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5780, Test Linf Norm: 0.3083\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5678, Test Linf Norm: 0.3083\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5734, Test Linf Norm: 0.3083\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5752, Test Linf Norm: 0.3083\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5766, Test Linf Norm: 0.3083\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5836, Test Linf Norm: 0.3083\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5872, Test Linf Norm: 0.3083\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5763, Test Linf Norm: 0.3083\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5820, Test Linf Norm: 0.3083\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5666, Test Linf Norm: 0.3083\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5763, Test Linf Norm: 0.3083\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5771, Test Linf Norm: 0.3083\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5728, Test Linf Norm: 0.3083\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5825, Test Linf Norm: 0.3083\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5718, Test Linf Norm: 0.3083\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5880, Test Linf Norm: 0.3083\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5854, Test Linf Norm: 0.3083\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5832, Test Linf Norm: 0.3083\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5871, Test Linf Norm: 0.3083\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5846, Test Linf Norm: 0.3083\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5855, Test Linf Norm: 0.3083\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5925, Test Linf Norm: 0.3083\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5756, Test Linf Norm: 0.3083\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5855, Test Linf Norm: 0.3083\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5769, Test Linf Norm: 0.3083\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5870, Test Linf Norm: 0.3083\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5500, Test Linf Norm: 0.3083\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5619, Test Linf Norm: 0.3083\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5740, Test Linf Norm: 0.3083\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5791, Test Linf Norm: 0.3083\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5810, Test Linf Norm: 0.3083\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5861, Test Linf Norm: 0.3083\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5735, Test Linf Norm: 0.3083\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5835, Test Linf Norm: 0.3083\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5832, Test Linf Norm: 0.3083\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5737, Test Linf Norm: 0.3083\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5794, Test Linf Norm: 0.3083\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5857, Test Linf Norm: 0.3083\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5872, Test Linf Norm: 0.3083\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5843, Test Linf Norm: 0.3083\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5818, Test Linf Norm: 0.3083\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5860, Test Linf Norm: 0.3083\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5857, Test Linf Norm: 0.3083\n",
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5828, Test Linf Norm: 0.3083\n",
            "Epoch 119: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5833, Test Linf Norm: 0.3083\n",
            "Epoch 120: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5815, Test Linf Norm: 0.3083\n",
            "Epoch 121: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5775, Test Linf Norm: 0.3083\n",
            "Epoch 122: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5806, Test Linf Norm: 0.3083\n",
            "Epoch 123: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5783, Test Linf Norm: 0.3083\n",
            "Epoch 124: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5839, Test Linf Norm: 0.3083\n",
            "Epoch 125: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5799, Test Linf Norm: 0.3083\n",
            "Epoch 126: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5638, Test Linf Norm: 0.3083\n",
            "Epoch 127: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5196, Test Linf Norm: 0.3083\n",
            "Epoch 128: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5843, Test Linf Norm: 0.3083\n",
            "Epoch 129: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5857, Test Linf Norm: 0.3083\n",
            "Epoch 130: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5732, Test Linf Norm: 0.3083\n",
            "Epoch 131: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5795, Test Linf Norm: 0.3083\n",
            "Epoch 132: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5829, Test Linf Norm: 0.3083\n",
            "Epoch 133: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5703, Test Linf Norm: 0.3083\n",
            "Epoch 134: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5776, Test Linf Norm: 0.3083\n",
            "Epoch 135: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5640, Test Linf Norm: 0.3083\n",
            "Epoch 136: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5729, Test Linf Norm: 0.3083\n",
            "Epoch 137: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5829, Test Linf Norm: 0.3083\n",
            "Epoch 138: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5795, Test Linf Norm: 0.3083\n",
            "Epoch 139: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5833, Test Linf Norm: 0.3083\n",
            "Epoch 140: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5694, Test Linf Norm: 0.3083\n",
            "Epoch 141: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5793, Test Linf Norm: 0.3083\n",
            "Epoch 142: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5778, Test Linf Norm: 0.3083\n",
            "Epoch 143: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5797, Test Linf Norm: 0.3083\n",
            "Epoch 144: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5817, Test Linf Norm: 0.3083\n",
            "Epoch 145: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5808, Test Linf Norm: 0.3083\n",
            "Epoch 146: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5671, Test Linf Norm: 0.3083\n",
            "Epoch 147: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5769, Test Linf Norm: 0.3083\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:09:08,242]\u001b[0m Trial 73 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 148: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.5807, Test Linf Norm: 0.3083\n",
            "Epoch 1: Train Loss: 0.0365, Test Loss: 0.0002, Train L1 Norm: 0.1784, Test L1 Norm: 0.0405, Train Linf Norm: 13.6018, Test Linf Norm: 3.2469\n",
            "Epoch 2: Train Loss: 0.0005, Test Loss: 0.0003, Train L1 Norm: 0.0415, Test L1 Norm: 0.0334, Train Linf Norm: 3.2263, Test Linf Norm: 2.4849\n",
            "Epoch 3: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0351, Test L1 Norm: 0.0256, Train Linf Norm: 2.9148, Test Linf Norm: 2.0656\n",
            "Epoch 4: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0303, Test L1 Norm: 0.0133, Train Linf Norm: 2.5469, Test Linf Norm: 0.6539\n",
            "Epoch 5: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0239, Test L1 Norm: 0.0143, Train Linf Norm: 1.8180, Test Linf Norm: 0.8826\n",
            "Epoch 6: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0198, Test L1 Norm: 0.0144, Train Linf Norm: 1.4681, Test Linf Norm: 0.9863\n",
            "Epoch 7: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0166, Test L1 Norm: 0.0120, Train Linf Norm: 1.1421, Test Linf Norm: 0.7846\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0168, Test L1 Norm: 0.0101, Train Linf Norm: 1.3059, Test Linf Norm: 0.5753\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0138, Test L1 Norm: 0.0097, Train Linf Norm: 1.0526, Test Linf Norm: 0.5702\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0136, Test L1 Norm: 0.0098, Train Linf Norm: 1.0364, Test Linf Norm: 0.5873\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0134, Test L1 Norm: 0.0092, Train Linf Norm: 1.0218, Test Linf Norm: 0.5349\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0131, Test L1 Norm: 0.0089, Train Linf Norm: 0.9798, Test Linf Norm: 0.5132\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0128, Test L1 Norm: 0.0089, Train Linf Norm: 0.9497, Test Linf Norm: 0.5100\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0127, Test L1 Norm: 0.0093, Train Linf Norm: 0.9592, Test Linf Norm: 0.5509\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0126, Test L1 Norm: 0.0089, Train Linf Norm: 0.9571, Test Linf Norm: 0.5169\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0124, Test L1 Norm: 0.0087, Train Linf Norm: 0.9000, Test Linf Norm: 0.4916\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0087, Train Linf Norm: 0.9035, Test Linf Norm: 0.4941\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0087, Train Linf Norm: 0.9099, Test Linf Norm: 0.4995\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0087, Train Linf Norm: 0.8885, Test Linf Norm: 0.4966\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0086, Train Linf Norm: 0.9148, Test Linf Norm: 0.4914\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0086, Train Linf Norm: 0.9139, Test Linf Norm: 0.4911\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0086, Train Linf Norm: 0.9140, Test Linf Norm: 0.4880\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0086, Train Linf Norm: 0.9111, Test Linf Norm: 0.4903\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0086, Train Linf Norm: 0.9097, Test Linf Norm: 0.4902\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0086, Train Linf Norm: 0.9095, Test Linf Norm: 0.4907\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0086, Train Linf Norm: 0.9150, Test Linf Norm: 0.4895\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0086, Train Linf Norm: 0.9020, Test Linf Norm: 0.4901\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0086, Train Linf Norm: 0.9082, Test Linf Norm: 0.4894\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0086, Train Linf Norm: 0.8940, Test Linf Norm: 0.4888\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9017, Test Linf Norm: 0.4888\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9077, Test Linf Norm: 0.4888\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8927, Test Linf Norm: 0.4879\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8739, Test Linf Norm: 0.4878\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8892, Test Linf Norm: 0.4877\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8901, Test Linf Norm: 0.4875\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9065, Test Linf Norm: 0.4874\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8944, Test Linf Norm: 0.4874\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8865, Test Linf Norm: 0.4873\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8953, Test Linf Norm: 0.4871\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8957, Test Linf Norm: 0.4870\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8934, Test Linf Norm: 0.4870\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9022, Test Linf Norm: 0.4870\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9024, Test Linf Norm: 0.4870\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8878, Test Linf Norm: 0.4870\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9046, Test Linf Norm: 0.4869\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8981, Test Linf Norm: 0.4869\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9063, Test Linf Norm: 0.4869\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8726, Test Linf Norm: 0.4869\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8960, Test Linf Norm: 0.4869\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9009, Test Linf Norm: 0.4869\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9016, Test Linf Norm: 0.4869\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8988, Test Linf Norm: 0.4869\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8937, Test Linf Norm: 0.4869\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8997, Test Linf Norm: 0.4869\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9063, Test Linf Norm: 0.4869\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8962, Test Linf Norm: 0.4869\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8954, Test Linf Norm: 0.4869\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9009, Test Linf Norm: 0.4869\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9119, Test Linf Norm: 0.4869\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9010, Test Linf Norm: 0.4869\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8968, Test Linf Norm: 0.4869\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8999, Test Linf Norm: 0.4869\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8939, Test Linf Norm: 0.4869\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8841, Test Linf Norm: 0.4869\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9008, Test Linf Norm: 0.4869\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9046, Test Linf Norm: 0.4869\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9004, Test Linf Norm: 0.4869\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8946, Test Linf Norm: 0.4869\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9048, Test Linf Norm: 0.4869\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9044, Test Linf Norm: 0.4869\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8970, Test Linf Norm: 0.4869\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8912, Test Linf Norm: 0.4869\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9005, Test Linf Norm: 0.4869\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8950, Test Linf Norm: 0.4869\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9022, Test Linf Norm: 0.4869\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8842, Test Linf Norm: 0.4869\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9041, Test Linf Norm: 0.4869\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8976, Test Linf Norm: 0.4869\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9059, Test Linf Norm: 0.4869\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8988, Test Linf Norm: 0.4869\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8962, Test Linf Norm: 0.4869\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8962, Test Linf Norm: 0.4869\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8961, Test Linf Norm: 0.4869\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8932, Test Linf Norm: 0.4869\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8768, Test Linf Norm: 0.4869\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9040, Test Linf Norm: 0.4869\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8941, Test Linf Norm: 0.4869\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8742, Test Linf Norm: 0.4869\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8952, Test Linf Norm: 0.4869\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8899, Test Linf Norm: 0.4869\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8896, Test Linf Norm: 0.4869\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.9023, Test Linf Norm: 0.4869\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8888, Test Linf Norm: 0.4869\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8956, Test Linf Norm: 0.4869\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8976, Test Linf Norm: 0.4869\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8963, Test Linf Norm: 0.4869\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8998, Test Linf Norm: 0.4869\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8820, Test Linf Norm: 0.4869\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:12:42,502]\u001b[0m Trial 74 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.8913, Test Linf Norm: 0.4869\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:12:44,931]\u001b[0m Trial 75 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0363, Test Loss: 0.0008, Train L1 Norm: 0.2165, Test L1 Norm: 0.1072, Train Linf Norm: 19.2506, Test Linf Norm: 11.3289\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:12:46,599]\u001b[0m Trial 76 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.3260, Test Loss: 0.0106, Train L1 Norm: 0.6273, Test L1 Norm: 0.4837, Train Linf Norm: 97.2357, Test Linf Norm: 89.4679\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:12:48,120]\u001b[0m Trial 77 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:12:49,654]\u001b[0m Trial 78 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:12:51,475]\u001b[0m Trial 79 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.5681, Test Loss: 0.2395, Train L1 Norm: 6.2826, Test L1 Norm: 2.3976, Train Linf Norm: 823.4080, Test Linf Norm: 303.7495\n",
            "Epoch 1: Train Loss: 0.0602, Test Loss: 0.0022, Train L1 Norm: 0.2054, Test L1 Norm: 0.0393, Train Linf Norm: 22.6117, Test Linf Norm: 1.4129\n",
            "Epoch 2: Train Loss: 0.0012, Test Loss: 0.0033, Train L1 Norm: 0.0491, Test L1 Norm: 0.0304, Train Linf Norm: 6.0229, Test Linf Norm: 0.6938\n",
            "Epoch 3: Train Loss: 0.0008, Test Loss: 0.0012, Train L1 Norm: 0.0349, Test L1 Norm: 0.0208, Train Linf Norm: 4.1411, Test Linf Norm: 0.5981\n",
            "Epoch 4: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0213, Test L1 Norm: 0.0172, Train Linf Norm: 2.4121, Test Linf Norm: 1.6764\n",
            "Epoch 5: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0175, Test L1 Norm: 0.0121, Train Linf Norm: 1.8173, Test Linf Norm: 0.5067\n",
            "Epoch 6: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0169, Test L1 Norm: 0.0086, Train Linf Norm: 1.7319, Test Linf Norm: 0.4769\n",
            "Epoch 7: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0140, Test L1 Norm: 0.0089, Train Linf Norm: 1.3423, Test Linf Norm: 0.5662\n",
            "Epoch 8: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0132, Test L1 Norm: 0.0088, Train Linf Norm: 1.5027, Test Linf Norm: 0.8077\n",
            "Epoch 9: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0124, Test L1 Norm: 0.0119, Train Linf Norm: 1.3444, Test Linf Norm: 1.4919\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0114, Test L1 Norm: 0.0071, Train Linf Norm: 1.2004, Test Linf Norm: 0.5831\n",
            "Epoch 11: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0116, Test L1 Norm: 0.0111, Train Linf Norm: 1.2476, Test Linf Norm: 1.4992\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0102, Test L1 Norm: 0.0079, Train Linf Norm: 1.2358, Test Linf Norm: 0.8778\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0088, Test L1 Norm: 0.0071, Train Linf Norm: 0.8890, Test Linf Norm: 0.6947\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0085, Train Linf Norm: 0.8845, Test Linf Norm: 1.0101\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0084, Train Linf Norm: 0.8888, Test Linf Norm: 1.0364\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0175, Train Linf Norm: 0.9223, Test Linf Norm: 2.8888\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0089, Test L1 Norm: 0.0124, Train Linf Norm: 0.9465, Test Linf Norm: 1.9659\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0088, Test L1 Norm: 0.0072, Train Linf Norm: 1.0214, Test Linf Norm: 0.7549\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0081, Train Linf Norm: 0.7707, Test Linf Norm: 0.9503\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0128, Train Linf Norm: 0.7662, Test Linf Norm: 2.0616\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0119, Train Linf Norm: 0.9170, Test Linf Norm: 1.9126\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0113, Train Linf Norm: 0.7164, Test Linf Norm: 1.8190\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0090, Train Linf Norm: 0.8822, Test Linf Norm: 1.2661\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0157, Train Linf Norm: 0.7432, Test Linf Norm: 2.7461\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0153, Train Linf Norm: 0.8340, Test Linf Norm: 2.6784\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0137, Train Linf Norm: 0.7317, Test Linf Norm: 2.3716\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0102, Train Linf Norm: 0.8263, Test Linf Norm: 1.6165\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0132, Train Linf Norm: 0.7543, Test Linf Norm: 2.2730\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0103, Train Linf Norm: 0.7644, Test Linf Norm: 1.6555\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0114, Train Linf Norm: 0.7251, Test Linf Norm: 1.9162\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0080, Train Linf Norm: 0.6992, Test Linf Norm: 0.9890\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0009, Train L1 Norm: 0.0075, Test L1 Norm: 0.0791, Train Linf Norm: 0.8349, Test Linf Norm: 13.7118\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0073, Test L1 Norm: 0.0154, Train Linf Norm: 0.6915, Test Linf Norm: 2.7888\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0103, Train Linf Norm: 0.6944, Test Linf Norm: 1.6184\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0100, Train Linf Norm: 0.6812, Test Linf Norm: 1.6355\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0104, Train Linf Norm: 0.7285, Test Linf Norm: 1.3570\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0089, Train Linf Norm: 0.7222, Test Linf Norm: 1.3506\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0113, Train Linf Norm: 0.7850, Test Linf Norm: 1.9243\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0064, Test L1 Norm: 0.0060, Train Linf Norm: 0.6817, Test Linf Norm: 0.3763\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0102, Train Linf Norm: 0.6430, Test Linf Norm: 1.6563\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0107, Train Linf Norm: 0.6358, Test Linf Norm: 1.8076\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0115, Train Linf Norm: 0.6005, Test Linf Norm: 1.9853\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0083, Train Linf Norm: 0.5814, Test Linf Norm: 1.2028\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0104, Train Linf Norm: 0.7011, Test Linf Norm: 1.7484\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0098, Train Linf Norm: 0.5497, Test Linf Norm: 1.6288\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0086, Train Linf Norm: 0.5581, Test Linf Norm: 1.3564\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0097, Train Linf Norm: 0.6847, Test Linf Norm: 1.5840\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0080, Train Linf Norm: 0.5728, Test Linf Norm: 1.2348\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0070, Train Linf Norm: 0.5348, Test Linf Norm: 0.9785\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0096, Train Linf Norm: 0.5617, Test Linf Norm: 1.5739\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0110, Train Linf Norm: 0.5509, Test Linf Norm: 1.8617\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0087, Train Linf Norm: 0.5671, Test Linf Norm: 1.4149\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0095, Train Linf Norm: 0.5447, Test Linf Norm: 1.5766\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0070, Train Linf Norm: 0.5220, Test Linf Norm: 1.0255\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0068, Train Linf Norm: 0.5682, Test Linf Norm: 0.9931\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0074, Train Linf Norm: 0.5617, Test Linf Norm: 1.1324\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0058, Train Linf Norm: 0.5315, Test Linf Norm: 0.7538\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0076, Train Linf Norm: 0.5565, Test Linf Norm: 1.1357\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0058, Train Linf Norm: 0.5534, Test Linf Norm: 0.7716\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0063, Train Linf Norm: 0.5219, Test Linf Norm: 0.9081\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0051, Train Linf Norm: 0.5586, Test Linf Norm: 0.6098\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0065, Train Linf Norm: 0.5104, Test Linf Norm: 0.9632\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0068, Train Linf Norm: 0.4977, Test Linf Norm: 0.9982\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0040, Train Linf Norm: 0.6958, Test Linf Norm: 0.3293\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0038, Train Linf Norm: 0.5129, Test Linf Norm: 0.2979\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0075, Train Linf Norm: 0.4890, Test Linf Norm: 1.1888\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0065, Train Linf Norm: 0.5292, Test Linf Norm: 0.9319\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0045, Train Linf Norm: 0.5145, Test Linf Norm: 0.4554\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0071, Train Linf Norm: 0.4805, Test Linf Norm: 1.0783\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0055, Train Linf Norm: 0.5262, Test Linf Norm: 0.3885\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0061, Train Linf Norm: 0.5349, Test Linf Norm: 0.8900\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0050, Train Linf Norm: 0.4779, Test Linf Norm: 0.6165\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0067, Train Linf Norm: 0.4815, Test Linf Norm: 0.9805\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0068, Train Linf Norm: 0.4925, Test Linf Norm: 1.0377\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0071, Train Linf Norm: 0.5341, Test Linf Norm: 1.0876\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0044, Train Linf Norm: 0.4800, Test Linf Norm: 0.4807\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0093, Train Linf Norm: 0.4596, Test Linf Norm: 1.5280\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0083, Train Linf Norm: 0.4497, Test Linf Norm: 1.3266\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0058, Train Linf Norm: 0.4697, Test Linf Norm: 0.8125\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0092, Train Linf Norm: 0.4540, Test Linf Norm: 1.5412\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0081, Train Linf Norm: 0.4789, Test Linf Norm: 1.3042\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0088, Train Linf Norm: 0.4621, Test Linf Norm: 1.4030\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0095, Train Linf Norm: 0.4598, Test Linf Norm: 1.5943\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0048, Train Linf Norm: 0.4392, Test Linf Norm: 0.5818\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0054, Train Linf Norm: 0.4774, Test Linf Norm: 0.7117\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0061, Train Linf Norm: 0.4487, Test Linf Norm: 0.8927\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0074, Train Linf Norm: 0.4574, Test Linf Norm: 1.1750\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0067, Train Linf Norm: 0.4240, Test Linf Norm: 1.0346\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0058, Train Linf Norm: 0.4899, Test Linf Norm: 0.8261\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0062, Train Linf Norm: 0.4435, Test Linf Norm: 0.9241\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0066, Train Linf Norm: 0.4218, Test Linf Norm: 1.0161\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0065, Train Linf Norm: 0.4025, Test Linf Norm: 1.0045\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0056, Train Linf Norm: 0.4833, Test Linf Norm: 0.7927\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0038, Train Linf Norm: 0.4551, Test Linf Norm: 0.3857\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0084, Train Linf Norm: 0.4450, Test Linf Norm: 1.3626\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0052, Train Linf Norm: 0.4321, Test Linf Norm: 0.7089\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0070, Train Linf Norm: 0.4382, Test Linf Norm: 1.0909\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0077, Train Linf Norm: 0.5048, Test Linf Norm: 1.1782\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0048, Train Linf Norm: 0.4234, Test Linf Norm: 0.3227\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0058, Train Linf Norm: 0.4630, Test Linf Norm: 0.8354\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0051, Train Linf Norm: 0.4154, Test Linf Norm: 0.6993\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0069, Train Linf Norm: 0.4093, Test Linf Norm: 1.0891\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0049, Train Linf Norm: 0.4303, Test Linf Norm: 0.6518\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0059, Train Linf Norm: 0.3852, Test Linf Norm: 0.8759\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0046, Train Linf Norm: 0.4542, Test Linf Norm: 0.5207\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0044, Train Linf Norm: 0.4278, Test Linf Norm: 0.5432\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0067, Train Linf Norm: 0.4256, Test Linf Norm: 1.0669\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0052, Train Linf Norm: 0.4608, Test Linf Norm: 0.7015\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0035, Train Linf Norm: 0.4109, Test Linf Norm: 0.3232\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0049, Train Linf Norm: 0.3761, Test Linf Norm: 0.6676\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0038, Train Linf Norm: 0.4311, Test Linf Norm: 0.3709\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0048, Train Linf Norm: 0.4001, Test Linf Norm: 0.6296\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0052, Train Linf Norm: 0.4053, Test Linf Norm: 0.7242\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0060, Train Linf Norm: 0.3860, Test Linf Norm: 0.9020\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0043, Train Linf Norm: 0.4465, Test Linf Norm: 0.5281\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0071, Train Linf Norm: 0.3912, Test Linf Norm: 1.1479\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0045, Train Linf Norm: 0.4492, Test Linf Norm: 0.5610\n",
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0045, Train Linf Norm: 0.3788, Test Linf Norm: 0.6020\n",
            "Epoch 119: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0047, Train Linf Norm: 0.4163, Test Linf Norm: 0.6426\n",
            "Epoch 120: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0042, Train Linf Norm: 0.4262, Test Linf Norm: 0.5092\n",
            "Epoch 121: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0121, Train Linf Norm: 0.3769, Test Linf Norm: 2.0416\n",
            "Epoch 122: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0038, Train Linf Norm: 0.4037, Test Linf Norm: 0.4286\n",
            "Epoch 123: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0050, Train Linf Norm: 0.4333, Test Linf Norm: 0.7047\n",
            "Epoch 124: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0041, Train Linf Norm: 0.4012, Test Linf Norm: 0.5094\n",
            "Epoch 125: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0062, Train Linf Norm: 0.3790, Test Linf Norm: 0.9520\n",
            "Epoch 126: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0041, Train Linf Norm: 0.3950, Test Linf Norm: 0.4887\n",
            "Epoch 127: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0041, Train Linf Norm: 0.4042, Test Linf Norm: 0.5036\n",
            "Epoch 128: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0067, Train Linf Norm: 0.3863, Test Linf Norm: 1.0432\n",
            "Epoch 129: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0064, Train Linf Norm: 0.4075, Test Linf Norm: 0.9944\n",
            "Epoch 130: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0046, Train Linf Norm: 0.4063, Test Linf Norm: 0.6277\n",
            "Epoch 131: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0080, Train Linf Norm: 0.3972, Test Linf Norm: 1.2843\n",
            "Epoch 132: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0048, Train Linf Norm: 0.3753, Test Linf Norm: 0.6790\n",
            "Epoch 133: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0043, Train Linf Norm: 0.4134, Test Linf Norm: 0.5669\n",
            "Epoch 134: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0046, Train Linf Norm: 0.3870, Test Linf Norm: 0.6136\n",
            "Epoch 135: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0041, Train Linf Norm: 0.3646, Test Linf Norm: 0.4797\n",
            "Epoch 136: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0048, Train Linf Norm: 0.4122, Test Linf Norm: 0.6731\n",
            "Epoch 137: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0040, Train Linf Norm: 0.4434, Test Linf Norm: 0.4850\n",
            "Epoch 138: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0037, Train Linf Norm: 0.3906, Test Linf Norm: 0.4082\n",
            "Epoch 139: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0060, Train Linf Norm: 0.4117, Test Linf Norm: 0.8973\n",
            "Epoch 140: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0049, Train Linf Norm: 0.3749, Test Linf Norm: 0.6980\n",
            "Epoch 141: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0037, Train Linf Norm: 0.4137, Test Linf Norm: 0.4086\n",
            "Epoch 142: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0039, Train Linf Norm: 0.4283, Test Linf Norm: 0.4767\n",
            "Epoch 143: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0037, Train Linf Norm: 0.3874, Test Linf Norm: 0.4023\n",
            "Epoch 144: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0046, Train Linf Norm: 0.3924, Test Linf Norm: 0.6295\n",
            "Epoch 145: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0032, Train Linf Norm: 0.3764, Test Linf Norm: 0.2294\n",
            "Epoch 146: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0029, Train Linf Norm: 0.8263, Test Linf Norm: 0.2462\n",
            "Epoch 147: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0050, Train Linf Norm: 0.3480, Test Linf Norm: 0.7162\n",
            "Epoch 148: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0049, Train Linf Norm: 0.3949, Test Linf Norm: 0.7006\n",
            "Epoch 149: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0071, Train Linf Norm: 0.3650, Test Linf Norm: 1.1300\n",
            "Epoch 150: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0047, Train Linf Norm: 0.4070, Test Linf Norm: 0.6429\n",
            "Epoch 151: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0053, Train Linf Norm: 0.4233, Test Linf Norm: 0.7811\n",
            "Epoch 152: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0031, Train Linf Norm: 0.3914, Test Linf Norm: 0.2961\n",
            "Epoch 153: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0039, Train Linf Norm: 0.3978, Test Linf Norm: 0.4934\n",
            "Epoch 154: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0043, Train Linf Norm: 0.4117, Test Linf Norm: 0.5537\n",
            "Epoch 155: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0030, Train Linf Norm: 0.3738, Test Linf Norm: 0.2280\n",
            "Epoch 156: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0045, Train Linf Norm: 0.3833, Test Linf Norm: 0.6204\n",
            "Epoch 157: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0029, Train Linf Norm: 0.3988, Test Linf Norm: 0.2302\n",
            "Epoch 158: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0040, Train Linf Norm: 0.4008, Test Linf Norm: 0.4919\n",
            "Epoch 159: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0043, Train Linf Norm: 0.3990, Test Linf Norm: 0.5906\n",
            "Epoch 160: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0042, Train Linf Norm: 0.3867, Test Linf Norm: 0.5485\n",
            "Epoch 161: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0033, Train Linf Norm: 0.3721, Test Linf Norm: 0.3445\n",
            "Epoch 162: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0041, Train Linf Norm: 0.3738, Test Linf Norm: 0.5359\n",
            "Epoch 163: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0054, Train Linf Norm: 0.3821, Test Linf Norm: 0.8214\n",
            "Epoch 164: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0044, Train Linf Norm: 0.4022, Test Linf Norm: 0.6011\n",
            "Epoch 165: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0036, Test L1 Norm: 0.0228, Train Linf Norm: 0.3815, Test Linf Norm: 3.7554\n",
            "Epoch 166: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0041, Train Linf Norm: 0.4016, Test Linf Norm: 0.5241\n",
            "Epoch 167: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0058, Train Linf Norm: 0.3808, Test Linf Norm: 0.9059\n",
            "Epoch 168: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0052, Train Linf Norm: 0.3744, Test Linf Norm: 0.7513\n",
            "Epoch 169: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0029, Train Linf Norm: 0.3526, Test Linf Norm: 0.2317\n",
            "Epoch 170: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0029, Train Linf Norm: 0.3609, Test Linf Norm: 0.2224\n",
            "Epoch 171: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0056, Train Linf Norm: 0.3959, Test Linf Norm: 0.8493\n",
            "Epoch 172: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0032, Train Linf Norm: 0.3788, Test Linf Norm: 0.3409\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:17:35,453]\u001b[0m Trial 80 finished with value: 0.007233287389017642 and parameters: {'n_layers': 3, 'n_units_0': 543, 'n_units_1': 741, 'n_units_2': 715, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.000597729947300063, 'batch_size': 224, 'n_epochs': 173, 'scheduler': 'None'}. Best is trial 61 with value: 0.002592882362008095.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 173: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0072, Train Linf Norm: 0.3968, Test Linf Norm: 1.1629\n",
            "Epoch 1: Train Loss: 0.0656, Test Loss: 0.0012, Train L1 Norm: 0.2618, Test L1 Norm: 0.0709, Train Linf Norm: 28.3849, Test Linf Norm: 8.3954\n",
            "Epoch 2: Train Loss: 0.0007, Test Loss: 0.0001, Train L1 Norm: 0.0381, Test L1 Norm: 0.0251, Train Linf Norm: 3.8549, Test Linf Norm: 2.4491\n",
            "Epoch 3: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0205, Test L1 Norm: 0.0113, Train Linf Norm: 1.9806, Test Linf Norm: 0.7554\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0149, Test L1 Norm: 0.0095, Train Linf Norm: 1.3814, Test Linf Norm: 0.5957\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0003, Train L1 Norm: 0.0126, Test L1 Norm: 0.0175, Train Linf Norm: 1.1067, Test Linf Norm: 0.7476\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0084, Train Linf Norm: 1.1232, Test Linf Norm: 0.5999\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0116, Test L1 Norm: 0.0072, Train Linf Norm: 1.0973, Test Linf Norm: 0.4246\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0103, Test L1 Norm: 0.0067, Train Linf Norm: 0.9229, Test Linf Norm: 0.3584\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0102, Test L1 Norm: 0.0068, Train Linf Norm: 0.8910, Test Linf Norm: 0.3905\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0066, Train Linf Norm: 0.8977, Test Linf Norm: 0.3447\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0097, Test L1 Norm: 0.0066, Train Linf Norm: 0.8505, Test Linf Norm: 0.3582\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0097, Test L1 Norm: 0.0066, Train Linf Norm: 0.8593, Test Linf Norm: 0.3582\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0098, Test L1 Norm: 0.0066, Train Linf Norm: 0.8700, Test Linf Norm: 0.3709\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0068, Train Linf Norm: 0.9107, Test Linf Norm: 0.3893\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0097, Test L1 Norm: 0.0068, Train Linf Norm: 0.8520, Test Linf Norm: 0.3983\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0098, Test L1 Norm: 0.0064, Train Linf Norm: 0.8875, Test Linf Norm: 0.3119\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0096, Test L1 Norm: 0.0070, Train Linf Norm: 0.8719, Test Linf Norm: 0.4146\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0098, Test L1 Norm: 0.0064, Train Linf Norm: 0.9248, Test Linf Norm: 0.3475\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0088, Test L1 Norm: 0.0099, Train Linf Norm: 0.7648, Test Linf Norm: 0.8934\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0056, Train Linf Norm: 0.7267, Test Linf Norm: 0.3051\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0088, Test L1 Norm: 0.0075, Train Linf Norm: 0.7693, Test Linf Norm: 0.5557\n",
            "Epoch 22: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0108, Test L1 Norm: 0.0058, Train Linf Norm: 0.8574, Test Linf Norm: 0.3079\n",
            "Epoch 23: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0115, Test L1 Norm: 0.0054, Train Linf Norm: 0.9841, Test Linf Norm: 0.2745\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0077, Test L1 Norm: 0.0081, Train Linf Norm: 0.6089, Test Linf Norm: 0.4659\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0084, Train Linf Norm: 0.7631, Test Linf Norm: 0.6655\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0055, Train Linf Norm: 0.6954, Test Linf Norm: 0.3774\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0060, Train Linf Norm: 0.6118, Test Linf Norm: 0.3985\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0051, Train Linf Norm: 0.6123, Test Linf Norm: 0.3488\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0054, Train Linf Norm: 0.5874, Test Linf Norm: 0.3829\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0050, Train Linf Norm: 0.5448, Test Linf Norm: 0.3499\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0050, Train Linf Norm: 0.5122, Test Linf Norm: 0.3065\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0052, Train Linf Norm: 0.5404, Test Linf Norm: 0.3736\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0053, Train Linf Norm: 0.5345, Test Linf Norm: 0.3938\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0053, Train Linf Norm: 0.5378, Test Linf Norm: 0.3938\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0053, Train Linf Norm: 0.5440, Test Linf Norm: 0.3960\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0054, Train Linf Norm: 0.5389, Test Linf Norm: 0.3990\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:18:44,370]\u001b[0m Trial 81 finished with value: 0.005060697455960326 and parameters: {'n_layers': 3, 'n_units_0': 962, 'n_units_1': 617, 'n_units_2': 498, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.0011438583806091362, 'batch_size': 167, 'n_epochs': 37, 'scheduler': 'CosineAnnealingLR', 'T_max': 11}. Best is trial 61 with value: 0.002592882362008095.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0051, Train Linf Norm: 0.5287, Test Linf Norm: 0.3497\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:18:46,453]\u001b[0m Trial 82 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:18:49,138]\u001b[0m Trial 83 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0599, Test Loss: 0.0004, Train L1 Norm: 0.1532, Test L1 Norm: 0.1133, Train Linf Norm: 10.5303, Test Linf Norm: 14.9351\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:18:51,261]\u001b[0m Trial 84 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0470, Test Loss: 0.0002, Train L1 Norm: 0.3716, Test L1 Norm: 0.1269, Train Linf Norm: 40.2520, Test Linf Norm: 15.6861\n",
            "Epoch 1: Train Loss: 0.5007, Test Loss: 0.0005, Train L1 Norm: 0.3203, Test L1 Norm: 0.0263, Train Linf Norm: 16.0927, Test Linf Norm: 2.3755\n",
            "Epoch 2: Train Loss: 0.0014, Test Loss: 0.0001, Train L1 Norm: 0.0281, Test L1 Norm: 0.0207, Train Linf Norm: 1.8817, Test Linf Norm: 2.1691\n",
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0149, Test L1 Norm: 0.0095, Train Linf Norm: 1.1821, Test Linf Norm: 0.5170\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0116, Test L1 Norm: 0.0120, Train Linf Norm: 0.9090, Test Linf Norm: 1.1218\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0102, Test L1 Norm: 0.0083, Train Linf Norm: 0.7887, Test Linf Norm: 0.5686\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0095, Train Linf Norm: 0.7284, Test Linf Norm: 0.7932\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0091, Test L1 Norm: 0.0075, Train Linf Norm: 0.6811, Test Linf Norm: 0.4556\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0086, Test L1 Norm: 0.0081, Train Linf Norm: 0.6344, Test Linf Norm: 0.5726\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0077, Train Linf Norm: 0.6148, Test Linf Norm: 0.5246\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0078, Train Linf Norm: 0.6314, Test Linf Norm: 0.5371\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0078, Train Linf Norm: 0.6257, Test Linf Norm: 0.5371\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0077, Train Linf Norm: 0.6224, Test Linf Norm: 0.5363\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0075, Train Linf Norm: 0.6468, Test Linf Norm: 0.4969\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0083, Test L1 Norm: 0.0073, Train Linf Norm: 0.6099, Test Linf Norm: 0.4826\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0072, Train Linf Norm: 0.6054, Test Linf Norm: 0.4658\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0079, Test L1 Norm: 0.0062, Train Linf Norm: 0.5620, Test Linf Norm: 0.3133\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0059, Train Linf Norm: 0.5440, Test Linf Norm: 0.3061\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0061, Train Linf Norm: 0.4756, Test Linf Norm: 0.3256\n",
            "Epoch 19: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0093, Test L1 Norm: 0.0103, Train Linf Norm: 0.6154, Test Linf Norm: 0.4605\n",
            "Epoch 20: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0117, Test L1 Norm: 0.0056, Train Linf Norm: 0.9669, Test Linf Norm: 0.3010\n",
            "Epoch 21: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0060, Train Linf Norm: 0.5366, Test Linf Norm: 0.3542\n",
            "Epoch 22: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0084, Test L1 Norm: 0.0060, Train Linf Norm: 0.5657, Test Linf Norm: 0.3278\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0054, Train Linf Norm: 0.5025, Test Linf Norm: 0.3090\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0050, Train Linf Norm: 0.4356, Test Linf Norm: 0.2868\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0050, Train Linf Norm: 0.4526, Test Linf Norm: 0.2869\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0049, Train Linf Norm: 0.4373, Test Linf Norm: 0.2774\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0049, Train Linf Norm: 0.3998, Test Linf Norm: 0.2902\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0048, Train Linf Norm: 0.4311, Test Linf Norm: 0.2793\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0048, Train Linf Norm: 0.4361, Test Linf Norm: 0.2807\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0048, Train Linf Norm: 0.4145, Test Linf Norm: 0.2690\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0048, Train Linf Norm: 0.4169, Test Linf Norm: 0.2690\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0048, Train Linf Norm: 0.4226, Test Linf Norm: 0.2696\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0048, Train Linf Norm: 0.4268, Test Linf Norm: 0.2693\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0048, Train Linf Norm: 0.4261, Test Linf Norm: 0.2675\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0048, Train Linf Norm: 0.4236, Test Linf Norm: 0.2997\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0047, Train Linf Norm: 0.4329, Test Linf Norm: 0.2801\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0048, Train Linf Norm: 0.4418, Test Linf Norm: 0.2828\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0047, Train Linf Norm: 0.4166, Test Linf Norm: 0.2649\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0046, Train Linf Norm: 0.4316, Test Linf Norm: 0.2671\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0049, Train Linf Norm: 0.5404, Test Linf Norm: 0.2846\n",
            "Epoch 41: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0055, Train Linf Norm: 0.6221, Test Linf Norm: 0.3259\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0047, Train Linf Norm: 0.4976, Test Linf Norm: 0.2957\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0046, Train Linf Norm: 0.4314, Test Linf Norm: 0.2696\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0043, Train Linf Norm: 0.4352, Test Linf Norm: 0.2535\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0045, Train Linf Norm: 0.4013, Test Linf Norm: 0.2644\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0043, Train Linf Norm: 0.3971, Test Linf Norm: 0.2505\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0043, Train Linf Norm: 0.4001, Test Linf Norm: 0.2584\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0042, Train Linf Norm: 0.4039, Test Linf Norm: 0.2448\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0041, Train Linf Norm: 0.4115, Test Linf Norm: 0.2442\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0042, Train Linf Norm: 0.4054, Test Linf Norm: 0.2494\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0042, Train Linf Norm: 0.4002, Test Linf Norm: 0.2494\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0042, Train Linf Norm: 0.4039, Test Linf Norm: 0.2459\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0041, Train Linf Norm: 0.3974, Test Linf Norm: 0.2429\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0041, Train Linf Norm: 0.4053, Test Linf Norm: 0.2405\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0043, Train Linf Norm: 0.4225, Test Linf Norm: 0.2624\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0040, Train Linf Norm: 0.3936, Test Linf Norm: 0.2442\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0040, Train Linf Norm: 0.3907, Test Linf Norm: 0.2508\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0041, Train Linf Norm: 0.4014, Test Linf Norm: 0.2500\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0039, Train Linf Norm: 0.4003, Test Linf Norm: 0.2412\n",
            "Epoch 60: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0042, Train Linf Norm: 0.4559, Test Linf Norm: 0.2690\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0045, Train Linf Norm: 0.5762, Test Linf Norm: 0.2749\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0043, Train Linf Norm: 0.4570, Test Linf Norm: 0.3000\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0040, Train Linf Norm: 0.3494, Test Linf Norm: 0.2410\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0043, Train Linf Norm: 0.3808, Test Linf Norm: 0.2910\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0039, Train Linf Norm: 0.4033, Test Linf Norm: 0.2505\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0039, Train Linf Norm: 0.3660, Test Linf Norm: 0.2500\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0039, Train Linf Norm: 0.3598, Test Linf Norm: 0.2404\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0039, Train Linf Norm: 0.3521, Test Linf Norm: 0.2514\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0038, Train Linf Norm: 0.3542, Test Linf Norm: 0.2586\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0038, Train Linf Norm: 0.3613, Test Linf Norm: 0.2537\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0038, Train Linf Norm: 0.3635, Test Linf Norm: 0.2537\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0038, Train Linf Norm: 0.3567, Test Linf Norm: 0.2549\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0038, Train Linf Norm: 0.3553, Test Linf Norm: 0.2487\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0038, Train Linf Norm: 0.3702, Test Linf Norm: 0.2495\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0039, Train Linf Norm: 0.3614, Test Linf Norm: 0.2560\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0037, Train Linf Norm: 0.3656, Test Linf Norm: 0.2454\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0038, Train Linf Norm: 0.3701, Test Linf Norm: 0.2522\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0036, Train Linf Norm: 0.3617, Test Linf Norm: 0.2446\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0043, Train Linf Norm: 0.3609, Test Linf Norm: 0.3114\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0062, Train Linf Norm: 0.5790, Test Linf Norm: 0.5317\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0047, Train Linf Norm: 0.5389, Test Linf Norm: 0.3735\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0070, Train Linf Norm: 0.4191, Test Linf Norm: 0.6316\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0040, Train Linf Norm: 0.3401, Test Linf Norm: 0.2920\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0036, Train Linf Norm: 0.3586, Test Linf Norm: 0.2460\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0038, Train Linf Norm: 0.3417, Test Linf Norm: 0.2881\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0038, Train Linf Norm: 0.3442, Test Linf Norm: 0.2956\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0038, Train Linf Norm: 0.3418, Test Linf Norm: 0.3021\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0040, Train Linf Norm: 0.3521, Test Linf Norm: 0.3072\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0039, Train Linf Norm: 0.3492, Test Linf Norm: 0.2969\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0039, Train Linf Norm: 0.3344, Test Linf Norm: 0.2986\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0039, Train Linf Norm: 0.3453, Test Linf Norm: 0.2986\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0038, Train Linf Norm: 0.3480, Test Linf Norm: 0.2949\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0038, Train Linf Norm: 0.3506, Test Linf Norm: 0.2939\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0038, Train Linf Norm: 0.3515, Test Linf Norm: 0.3028\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0039, Train Linf Norm: 0.3491, Test Linf Norm: 0.2988\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0040, Train Linf Norm: 0.3491, Test Linf Norm: 0.3157\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0038, Train Linf Norm: 0.3695, Test Linf Norm: 0.2993\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0039, Train Linf Norm: 0.3473, Test Linf Norm: 0.3049\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0037, Train Linf Norm: 0.3997, Test Linf Norm: 0.2836\n",
            "Epoch 100: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0132, Test L1 Norm: 0.0045, Train Linf Norm: 1.7507, Test Linf Norm: 0.3225\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0049, Train Linf Norm: 0.3899, Test Linf Norm: 0.2790\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0038, Train Linf Norm: 0.4060, Test Linf Norm: 0.2956\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0038, Train Linf Norm: 0.3461, Test Linf Norm: 0.2659\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0039, Train Linf Norm: 0.3513, Test Linf Norm: 0.3165\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0037, Train Linf Norm: 0.3426, Test Linf Norm: 0.2961\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0038, Train Linf Norm: 0.3503, Test Linf Norm: 0.3231\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0038, Train Linf Norm: 0.3298, Test Linf Norm: 0.3194\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0038, Train Linf Norm: 0.3288, Test Linf Norm: 0.3168\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0037, Train Linf Norm: 0.3336, Test Linf Norm: 0.3051\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0037, Train Linf Norm: 0.3367, Test Linf Norm: 0.3088\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0037, Train Linf Norm: 0.3314, Test Linf Norm: 0.3088\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0037, Train Linf Norm: 0.3351, Test Linf Norm: 0.3115\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0037, Train Linf Norm: 0.3380, Test Linf Norm: 0.3093\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0037, Train Linf Norm: 0.3268, Test Linf Norm: 0.3024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:22:22,902]\u001b[0m Trial 85 finished with value: 0.003738072231109254 and parameters: {'n_layers': 3, 'n_units_0': 797, 'n_units_1': 792, 'n_units_2': 345, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.000796029463442166, 'batch_size': 181, 'n_epochs': 115, 'scheduler': 'CosineAnnealingLR', 'T_max': 10}. Best is trial 61 with value: 0.002592882362008095.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0037, Train Linf Norm: 0.3389, Test Linf Norm: 0.3118\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:22:24,767]\u001b[0m Trial 86 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.2025, Test Loss: 0.0005, Train L1 Norm: 0.2109, Test L1 Norm: 0.1373, Train Linf Norm: 20.6541, Test Linf Norm: 21.0414\n",
            "Epoch 1: Train Loss: 0.0455, Test Loss: 0.0008, Train L1 Norm: 0.2110, Test L1 Norm: 0.0769, Train Linf Norm: 15.8941, Test Linf Norm: 6.9185\n",
            "Epoch 2: Train Loss: 0.0005, Test Loss: 0.0001, Train L1 Norm: 0.0579, Test L1 Norm: 0.0457, Train Linf Norm: 4.7092, Test Linf Norm: 4.1508\n",
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0333, Test L1 Norm: 0.0208, Train Linf Norm: 2.5867, Test Linf Norm: 1.4552\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0287, Test L1 Norm: 0.0166, Train Linf Norm: 2.3642, Test Linf Norm: 1.1130\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0229, Test L1 Norm: 0.0131, Train Linf Norm: 1.8244, Test Linf Norm: 0.7773\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0217, Test L1 Norm: 0.0137, Train Linf Norm: 1.7531, Test Linf Norm: 0.8582\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0197, Test L1 Norm: 0.0118, Train Linf Norm: 1.5541, Test Linf Norm: 0.6919\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0190, Test L1 Norm: 0.0118, Train Linf Norm: 1.5071, Test Linf Norm: 0.7161\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0185, Test L1 Norm: 0.0116, Train Linf Norm: 1.4671, Test Linf Norm: 0.7020\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0184, Test L1 Norm: 0.0114, Train Linf Norm: 1.4613, Test Linf Norm: 0.6904\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0183, Test L1 Norm: 0.0114, Train Linf Norm: 1.4351, Test Linf Norm: 0.6904\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0183, Test L1 Norm: 0.0114, Train Linf Norm: 1.4365, Test Linf Norm: 0.6775\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0183, Test L1 Norm: 0.0117, Train Linf Norm: 1.4574, Test Linf Norm: 0.7138\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0179, Test L1 Norm: 0.0112, Train Linf Norm: 1.4087, Test Linf Norm: 0.6735\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0177, Test L1 Norm: 0.0105, Train Linf Norm: 1.4053, Test Linf Norm: 0.5791\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0175, Test L1 Norm: 0.0103, Train Linf Norm: 1.4065, Test Linf Norm: 0.5794\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0166, Test L1 Norm: 0.0092, Train Linf Norm: 1.3142, Test Linf Norm: 0.4965\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0160, Test L1 Norm: 0.0089, Train Linf Norm: 1.2432, Test Linf Norm: 0.4958\n",
            "Epoch 19: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0184, Test L1 Norm: 0.0090, Train Linf Norm: 1.4124, Test Linf Norm: 0.4746\n",
            "Epoch 20: Train Loss: 0.0003, Test Loss: 0.0000, Train L1 Norm: 0.0186, Test L1 Norm: 0.0106, Train Linf Norm: 1.2465, Test Linf Norm: 0.5942\n",
            "Epoch 21: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0154, Test L1 Norm: 0.0107, Train Linf Norm: 1.0639, Test Linf Norm: 0.6006\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0141, Test L1 Norm: 0.0076, Train Linf Norm: 1.0520, Test Linf Norm: 0.3866\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0130, Test L1 Norm: 0.0078, Train Linf Norm: 0.9935, Test Linf Norm: 0.4542\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0126, Test L1 Norm: 0.0076, Train Linf Norm: 0.9929, Test Linf Norm: 0.4640\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0118, Test L1 Norm: 0.0067, Train Linf Norm: 0.9229, Test Linf Norm: 0.3465\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0116, Test L1 Norm: 0.0065, Train Linf Norm: 0.8733, Test Linf Norm: 0.3427\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0110, Test L1 Norm: 0.0064, Train Linf Norm: 0.8553, Test Linf Norm: 0.3481\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0110, Test L1 Norm: 0.0064, Train Linf Norm: 0.8589, Test Linf Norm: 0.3340\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0108, Test L1 Norm: 0.0064, Train Linf Norm: 0.8410, Test Linf Norm: 0.3367\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0109, Test L1 Norm: 0.0064, Train Linf Norm: 0.8539, Test Linf Norm: 0.3399\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0109, Test L1 Norm: 0.0064, Train Linf Norm: 0.8461, Test Linf Norm: 0.3399\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0108, Test L1 Norm: 0.0064, Train Linf Norm: 0.8356, Test Linf Norm: 0.3367\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0108, Test L1 Norm: 0.0063, Train Linf Norm: 0.8365, Test Linf Norm: 0.3291\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0108, Test L1 Norm: 0.0062, Train Linf Norm: 0.8432, Test Linf Norm: 0.3225\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0107, Test L1 Norm: 0.0064, Train Linf Norm: 0.8267, Test Linf Norm: 0.3513\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0107, Test L1 Norm: 0.0062, Train Linf Norm: 0.8283, Test Linf Norm: 0.3323\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0103, Test L1 Norm: 0.0079, Train Linf Norm: 0.7843, Test Linf Norm: 0.5217\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0106, Test L1 Norm: 0.0058, Train Linf Norm: 0.8340, Test Linf Norm: 0.3078\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0108, Test L1 Norm: 0.0057, Train Linf Norm: 0.8319, Test Linf Norm: 0.3015\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0109, Test L1 Norm: 0.0061, Train Linf Norm: 0.7993, Test Linf Norm: 0.3011\n",
            "Epoch 41: Train Loss: 0.0005, Test Loss: 0.0000, Train L1 Norm: 0.0164, Test L1 Norm: 0.0082, Train Linf Norm: 1.1499, Test Linf Norm: 0.3732\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0125, Test L1 Norm: 0.0063, Train Linf Norm: 0.9601, Test Linf Norm: 0.3279\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0112, Test L1 Norm: 0.0063, Train Linf Norm: 0.8678, Test Linf Norm: 0.3573\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0105, Test L1 Norm: 0.0063, Train Linf Norm: 0.8391, Test Linf Norm: 0.3688\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0103, Test L1 Norm: 0.0062, Train Linf Norm: 0.7878, Test Linf Norm: 0.3726\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0056, Train Linf Norm: 0.8023, Test Linf Norm: 0.3080\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0098, Test L1 Norm: 0.0055, Train Linf Norm: 0.7794, Test Linf Norm: 0.3031\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0095, Test L1 Norm: 0.0056, Train Linf Norm: 0.7534, Test Linf Norm: 0.3060\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0055, Train Linf Norm: 0.7385, Test Linf Norm: 0.3048\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0055, Train Linf Norm: 0.7389, Test Linf Norm: 0.3052\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0055, Train Linf Norm: 0.7245, Test Linf Norm: 0.3052\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0056, Train Linf Norm: 0.7498, Test Linf Norm: 0.3143\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0055, Train Linf Norm: 0.7526, Test Linf Norm: 0.2996\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0093, Test L1 Norm: 0.0054, Train Linf Norm: 0.7212, Test Linf Norm: 0.3005\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0053, Train Linf Norm: 0.7371, Test Linf Norm: 0.2829\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0093, Test L1 Norm: 0.0052, Train Linf Norm: 0.7427, Test Linf Norm: 0.2815\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0091, Test L1 Norm: 0.0053, Train Linf Norm: 0.6877, Test Linf Norm: 0.2873\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0053, Train Linf Norm: 0.7608, Test Linf Norm: 0.2967\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0091, Test L1 Norm: 0.0055, Train Linf Norm: 0.7146, Test Linf Norm: 0.2892\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0098, Test L1 Norm: 0.0065, Train Linf Norm: 0.7411, Test Linf Norm: 0.3894\n",
            "Epoch 61: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0102, Test L1 Norm: 0.0058, Train Linf Norm: 0.7389, Test Linf Norm: 0.3396\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0090, Test L1 Norm: 0.0053, Train Linf Norm: 0.7007, Test Linf Norm: 0.3028\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0065, Train Linf Norm: 0.6475, Test Linf Norm: 0.4300\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0083, Test L1 Norm: 0.0052, Train Linf Norm: 0.6541, Test Linf Norm: 0.3099\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0058, Train Linf Norm: 0.6410, Test Linf Norm: 0.3768\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0079, Test L1 Norm: 0.0054, Train Linf Norm: 0.6196, Test Linf Norm: 0.3399\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0051, Train Linf Norm: 0.6226, Test Linf Norm: 0.2984\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0050, Train Linf Norm: 0.5930, Test Linf Norm: 0.3001\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0050, Train Linf Norm: 0.6064, Test Linf Norm: 0.2932\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0050, Train Linf Norm: 0.6019, Test Linf Norm: 0.2899\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0050, Train Linf Norm: 0.5891, Test Linf Norm: 0.2899\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0049, Train Linf Norm: 0.6047, Test Linf Norm: 0.2892\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0049, Train Linf Norm: 0.6063, Test Linf Norm: 0.2877\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0048, Train Linf Norm: 0.5997, Test Linf Norm: 0.2688\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0049, Train Linf Norm: 0.5909, Test Linf Norm: 0.2888\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0051, Train Linf Norm: 0.5825, Test Linf Norm: 0.3054\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0048, Train Linf Norm: 0.5802, Test Linf Norm: 0.2774\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0046, Train Linf Norm: 0.5844, Test Linf Norm: 0.2419\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0049, Train Linf Norm: 0.5911, Test Linf Norm: 0.2888\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0047, Train Linf Norm: 0.5377, Test Linf Norm: 0.2579\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0048, Train Linf Norm: 0.6702, Test Linf Norm: 0.2640\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0047, Train Linf Norm: 0.6029, Test Linf Norm: 0.2450\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0047, Train Linf Norm: 0.5908, Test Linf Norm: 0.2770\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0044, Train Linf Norm: 0.5803, Test Linf Norm: 0.2408\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0047, Train Linf Norm: 0.5519, Test Linf Norm: 0.2763\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0044, Train Linf Norm: 0.5835, Test Linf Norm: 0.2489\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0073, Test L1 Norm: 0.0046, Train Linf Norm: 0.5672, Test Linf Norm: 0.2681\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0044, Train Linf Norm: 0.5631, Test Linf Norm: 0.2504\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0045, Train Linf Norm: 0.5651, Test Linf Norm: 0.2555\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0073, Test L1 Norm: 0.0045, Train Linf Norm: 0.5643, Test Linf Norm: 0.2588\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0045, Train Linf Norm: 0.5548, Test Linf Norm: 0.2588\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0045, Train Linf Norm: 0.5525, Test Linf Norm: 0.2570\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0045, Train Linf Norm: 0.5683, Test Linf Norm: 0.2560\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0073, Test L1 Norm: 0.0046, Train Linf Norm: 0.5705, Test Linf Norm: 0.2712\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0073, Test L1 Norm: 0.0044, Train Linf Norm: 0.5693, Test Linf Norm: 0.2515\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0046, Train Linf Norm: 0.5444, Test Linf Norm: 0.2785\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0045, Train Linf Norm: 0.5805, Test Linf Norm: 0.2588\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0044, Train Linf Norm: 0.5468, Test Linf Norm: 0.2500\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0045, Train Linf Norm: 0.5392, Test Linf Norm: 0.2645\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0044, Train Linf Norm: 0.5714, Test Linf Norm: 0.2380\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0045, Train Linf Norm: 0.5352, Test Linf Norm: 0.2527\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0047, Train Linf Norm: 0.5269, Test Linf Norm: 0.2813\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0042, Train Linf Norm: 0.5106, Test Linf Norm: 0.2218\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0044, Train Linf Norm: 0.5441, Test Linf Norm: 0.2644\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0042, Train Linf Norm: 0.5185, Test Linf Norm: 0.2310\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0043, Train Linf Norm: 0.5163, Test Linf Norm: 0.2477\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0042, Train Linf Norm: 0.5216, Test Linf Norm: 0.2342\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0042, Train Linf Norm: 0.5127, Test Linf Norm: 0.2346\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0042, Train Linf Norm: 0.5127, Test Linf Norm: 0.2382\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0041, Train Linf Norm: 0.5071, Test Linf Norm: 0.2290\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0041, Train Linf Norm: 0.5044, Test Linf Norm: 0.2290\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0041, Train Linf Norm: 0.5101, Test Linf Norm: 0.2289\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0041, Train Linf Norm: 0.5097, Test Linf Norm: 0.2280\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0041, Train Linf Norm: 0.5067, Test Linf Norm: 0.2349\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0040, Train Linf Norm: 0.5083, Test Linf Norm: 0.2155\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0042, Train Linf Norm: 0.5102, Test Linf Norm: 0.2414\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0040, Train Linf Norm: 0.4954, Test Linf Norm: 0.2214\n",
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0041, Train Linf Norm: 0.5129, Test Linf Norm: 0.2301\n",
            "Epoch 119: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0041, Train Linf Norm: 0.5101, Test Linf Norm: 0.2260\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:27:01,602]\u001b[0m Trial 87 finished with value: 0.005286718016024679 and parameters: {'n_layers': 3, 'n_units_0': 911, 'n_units_1': 810, 'n_units_2': 102, 'hidden_activation': 'LeakyReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.0011224273046049294, 'batch_size': 120, 'n_epochs': 120, 'scheduler': 'CosineAnnealingLR', 'T_max': 10}. Best is trial 61 with value: 0.002592882362008095.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 120: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0053, Train Linf Norm: 0.5483, Test Linf Norm: 0.2787\n",
            "Epoch 1: Train Loss: 0.0557, Test Loss: 0.0064, Train L1 Norm: 0.1556, Test L1 Norm: 0.0434, Train Linf Norm: 12.7192, Test Linf Norm: 0.7221\n",
            "Epoch 2: Train Loss: 0.0004, Test Loss: 0.0001, Train L1 Norm: 0.0321, Test L1 Norm: 0.0161, Train Linf Norm: 3.3353, Test Linf Norm: 1.4047\n",
            "Epoch 3: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0223, Test L1 Norm: 0.0109, Train Linf Norm: 2.5889, Test Linf Norm: 0.8338\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0192, Test L1 Norm: 0.0097, Train Linf Norm: 2.1851, Test Linf Norm: 0.7461\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0172, Test L1 Norm: 0.0093, Train Linf Norm: 1.9283, Test Linf Norm: 0.7260\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0152, Test L1 Norm: 0.0090, Train Linf Norm: 1.7139, Test Linf Norm: 0.7064\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0153, Test L1 Norm: 0.0088, Train Linf Norm: 1.7246, Test Linf Norm: 0.6760\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0144, Test L1 Norm: 0.0089, Train Linf Norm: 1.5964, Test Linf Norm: 0.6853\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0149, Test L1 Norm: 0.0089, Train Linf Norm: 1.6654, Test Linf Norm: 0.6853\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0145, Test L1 Norm: 0.0088, Train Linf Norm: 1.6101, Test Linf Norm: 0.6821\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0148, Test L1 Norm: 0.0087, Train Linf Norm: 1.6439, Test Linf Norm: 0.6764\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0141, Test L1 Norm: 0.0087, Train Linf Norm: 1.5262, Test Linf Norm: 0.6794\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0140, Test L1 Norm: 0.0081, Train Linf Norm: 1.5571, Test Linf Norm: 0.6299\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0131, Test L1 Norm: 0.0077, Train Linf Norm: 1.3777, Test Linf Norm: 0.5555\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0124, Test L1 Norm: 0.0080, Train Linf Norm: 1.2790, Test Linf Norm: 0.5794\n",
            "Epoch 16: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0142, Test L1 Norm: 0.0111, Train Linf Norm: 1.2320, Test Linf Norm: 0.7359\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:27:35,010]\u001b[0m Trial 88 finished with value: 0.00703787620505318 and parameters: {'n_layers': 3, 'n_units_0': 817, 'n_units_1': 886, 'n_units_2': 368, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.0007941885431587574, 'batch_size': 170, 'n_epochs': 17, 'scheduler': 'CosineAnnealingLR', 'T_max': 8}. Best is trial 61 with value: 0.002592882362008095.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Train Loss: 0.0003, Test Loss: 0.0000, Train L1 Norm: 0.0182, Test L1 Norm: 0.0070, Train Linf Norm: 1.7068, Test Linf Norm: 0.4826\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:27:36,790]\u001b[0m Trial 89 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0905, Test Loss: 0.0111, Train L1 Norm: 0.3215, Test L1 Norm: 0.1301, Train Linf Norm: 19.9314, Test Linf Norm: 6.3763\n",
            "Epoch 1: Train Loss: 0.0725, Test Loss: 0.0005, Train L1 Norm: 0.2579, Test L1 Norm: 0.0745, Train Linf Norm: 26.0459, Test Linf Norm: 10.8351\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:27:40,323]\u001b[0m Trial 90 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Train Loss: 0.0013, Test Loss: 0.0001, Train L1 Norm: 0.0458, Test L1 Norm: 0.0624, Train Linf Norm: 4.7776, Test Linf Norm: 9.3130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:27:43,400]\u001b[0m Trial 91 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0438, Test Loss: 0.0002, Train L1 Norm: 0.1555, Test L1 Norm: 0.1227, Train Linf Norm: 10.2690, Test Linf Norm: 12.9518\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:27:45,237]\u001b[0m Trial 92 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0538, Test Loss: 0.0004, Train L1 Norm: 0.2614, Test L1 Norm: 0.1712, Train Linf Norm: 26.2163, Test Linf Norm: 23.5263\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:27:47,798]\u001b[0m Trial 93 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0274, Test Loss: 0.0003, Train L1 Norm: 0.3397, Test L1 Norm: 0.2112, Train Linf Norm: 24.0780, Test Linf Norm: 17.2595\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:27:49,772]\u001b[0m Trial 94 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0498, Test Loss: 0.0010, Train L1 Norm: 0.2541, Test L1 Norm: 0.1228, Train Linf Norm: 24.2506, Test Linf Norm: 14.7539\n",
            "Epoch 1: Train Loss: 0.0723, Test Loss: 0.0004, Train L1 Norm: 0.2289, Test L1 Norm: 0.0824, Train Linf Norm: 20.7173, Test Linf Norm: 10.9811\n",
            "Epoch 2: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0411, Test L1 Norm: 0.0184, Train Linf Norm: 4.4901, Test Linf Norm: 1.0485\n",
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0307, Test L1 Norm: 0.0301, Train Linf Norm: 3.4148, Test Linf Norm: 3.5136\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0259, Test L1 Norm: 0.0152, Train Linf Norm: 2.8836, Test Linf Norm: 1.2485\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0220, Test L1 Norm: 0.0141, Train Linf Norm: 2.4428, Test Linf Norm: 1.2052\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0201, Test L1 Norm: 0.0198, Train Linf Norm: 2.1993, Test Linf Norm: 2.2507\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0204, Test L1 Norm: 0.0214, Train Linf Norm: 2.3667, Test Linf Norm: 2.6382\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0188, Test L1 Norm: 0.0194, Train Linf Norm: 2.1404, Test Linf Norm: 2.3373\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0182, Test L1 Norm: 0.0206, Train Linf Norm: 2.0689, Test Linf Norm: 2.5870\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0177, Test L1 Norm: 0.0190, Train Linf Norm: 1.9838, Test Linf Norm: 2.3112\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0168, Test L1 Norm: 0.0196, Train Linf Norm: 1.8738, Test Linf Norm: 2.4536\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0166, Test L1 Norm: 0.0206, Train Linf Norm: 1.8056, Test Linf Norm: 2.6432\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0165, Test L1 Norm: 0.0199, Train Linf Norm: 1.8293, Test Linf Norm: 2.4962\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0161, Test L1 Norm: 0.0198, Train Linf Norm: 1.7713, Test Linf Norm: 2.5073\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0160, Test L1 Norm: 0.0214, Train Linf Norm: 1.7475, Test Linf Norm: 2.7966\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0159, Test L1 Norm: 0.0210, Train Linf Norm: 1.7536, Test Linf Norm: 2.7317\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0157, Test L1 Norm: 0.0208, Train Linf Norm: 1.6653, Test Linf Norm: 2.6937\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0157, Test L1 Norm: 0.0207, Train Linf Norm: 1.7089, Test Linf Norm: 2.6946\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0157, Test L1 Norm: 0.0205, Train Linf Norm: 1.7203, Test Linf Norm: 2.6676\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0157, Test L1 Norm: 0.0208, Train Linf Norm: 1.7451, Test Linf Norm: 2.7177\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0157, Test L1 Norm: 0.0206, Train Linf Norm: 1.7499, Test Linf Norm: 2.6746\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0156, Test L1 Norm: 0.0206, Train Linf Norm: 1.6526, Test Linf Norm: 2.6797\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0155, Test L1 Norm: 0.0206, Train Linf Norm: 1.6990, Test Linf Norm: 2.6875\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0155, Test L1 Norm: 0.0205, Train Linf Norm: 1.6879, Test Linf Norm: 2.6599\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0155, Test L1 Norm: 0.0207, Train Linf Norm: 1.6760, Test Linf Norm: 2.6905\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0155, Test L1 Norm: 0.0208, Train Linf Norm: 1.7103, Test Linf Norm: 2.7177\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0156, Test L1 Norm: 0.0206, Train Linf Norm: 1.7048, Test Linf Norm: 2.6881\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:28:42,922]\u001b[0m Trial 95 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0155, Test L1 Norm: 0.0206, Train Linf Norm: 1.7087, Test Linf Norm: 2.6843\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:28:44,667]\u001b[0m Trial 96 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 3.7994, Test Loss: 2.9774, Train L1 Norm: 1.5293, Test L1 Norm: 1.0000, Train Linf Norm: 22.2764, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:28:47,256]\u001b[0m Trial 97 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0570, Test Loss: 0.0004, Train L1 Norm: 0.1925, Test L1 Norm: 0.1647, Train Linf Norm: 14.4723, Test Linf Norm: 19.4138\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:28:50,009]\u001b[0m Trial 98 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:28:51,666]\u001b[0m Trial 99 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.2045, Test Loss: 0.6333, Train L1 Norm: 3.1454, Test L1 Norm: 1.4874, Train Linf Norm: 320.0641, Test Linf Norm: 230.6377\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:28:53,285]\u001b[0m Trial 100 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:28:55,023]\u001b[0m Trial 101 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0559, Test Loss: 0.0003, Train L1 Norm: 0.2128, Test L1 Norm: 0.0989, Train Linf Norm: 23.5332, Test Linf Norm: 16.5807\n",
            "Epoch 1: Train Loss: 0.0561, Test Loss: 0.0010, Train L1 Norm: 0.2012, Test L1 Norm: 0.0278, Train Linf Norm: 21.8604, Test Linf Norm: 1.1144\n",
            "Epoch 2: Train Loss: 0.0011, Test Loss: 0.0002, Train L1 Norm: 0.0388, Test L1 Norm: 0.0232, Train Linf Norm: 4.1715, Test Linf Norm: 2.2917\n",
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0003, Train L1 Norm: 0.0235, Test L1 Norm: 0.0210, Train Linf Norm: 2.9166, Test Linf Norm: 1.8288\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0206, Test L1 Norm: 0.0102, Train Linf Norm: 2.7899, Test Linf Norm: 0.6990\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0178, Test L1 Norm: 0.0097, Train Linf Norm: 2.3644, Test Linf Norm: 0.7034\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0158, Test L1 Norm: 0.0091, Train Linf Norm: 2.0617, Test Linf Norm: 0.6434\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0150, Test L1 Norm: 0.0086, Train Linf Norm: 1.9721, Test Linf Norm: 0.6249\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0140, Test L1 Norm: 0.0085, Train Linf Norm: 1.8203, Test Linf Norm: 0.6622\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0132, Test L1 Norm: 0.0082, Train Linf Norm: 1.6873, Test Linf Norm: 0.6278\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0129, Test L1 Norm: 0.0082, Train Linf Norm: 1.6130, Test Linf Norm: 0.6159\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0128, Test L1 Norm: 0.0081, Train Linf Norm: 1.5937, Test Linf Norm: 0.6376\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0126, Test L1 Norm: 0.0081, Train Linf Norm: 1.5547, Test Linf Norm: 0.6313\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0126, Test L1 Norm: 0.0081, Train Linf Norm: 1.5993, Test Linf Norm: 0.6313\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0126, Test L1 Norm: 0.0081, Train Linf Norm: 1.5740, Test Linf Norm: 0.6313\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0126, Test L1 Norm: 0.0081, Train Linf Norm: 1.5748, Test Linf Norm: 0.6269\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0124, Test L1 Norm: 0.0081, Train Linf Norm: 1.5470, Test Linf Norm: 0.6203\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0080, Train Linf Norm: 1.5361, Test Linf Norm: 0.6018\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0119, Test L1 Norm: 0.0077, Train Linf Norm: 1.4850, Test Linf Norm: 0.5905\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0115, Test L1 Norm: 0.0075, Train Linf Norm: 1.3385, Test Linf Norm: 0.5684\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0111, Test L1 Norm: 0.0072, Train Linf Norm: 1.3210, Test Linf Norm: 0.5544\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0109, Test L1 Norm: 0.0070, Train Linf Norm: 1.2962, Test Linf Norm: 0.5184\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0118, Test L1 Norm: 0.0076, Train Linf Norm: 1.4413, Test Linf Norm: 0.4796\n",
            "Epoch 23: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0138, Test L1 Norm: 0.0068, Train Linf Norm: 1.5317, Test Linf Norm: 0.5105\n",
            "Epoch 24: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0141, Test L1 Norm: 0.0069, Train Linf Norm: 1.3142, Test Linf Norm: 0.5047\n",
            "Epoch 25: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0161, Test L1 Norm: 0.0067, Train Linf Norm: 2.1246, Test Linf Norm: 0.5509\n",
            "Epoch 26: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0108, Test L1 Norm: 0.0074, Train Linf Norm: 1.2253, Test Linf Norm: 0.4492\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0093, Test L1 Norm: 0.0064, Train Linf Norm: 1.1046, Test Linf Norm: 0.4858\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0088, Test L1 Norm: 0.0061, Train Linf Norm: 1.0383, Test Linf Norm: 0.4817\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0060, Train Linf Norm: 1.0533, Test Linf Norm: 0.4945\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0084, Test L1 Norm: 0.0058, Train Linf Norm: 1.0140, Test Linf Norm: 0.4856\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0058, Train Linf Norm: 1.0521, Test Linf Norm: 0.4991\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0083, Test L1 Norm: 0.0056, Train Linf Norm: 1.0186, Test Linf Norm: 0.4645\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0056, Train Linf Norm: 0.9863, Test Linf Norm: 0.4718\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0056, Train Linf Norm: 0.9883, Test Linf Norm: 0.4654\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0056, Train Linf Norm: 0.9972, Test Linf Norm: 0.4811\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0056, Train Linf Norm: 1.0051, Test Linf Norm: 0.4732\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0056, Train Linf Norm: 0.9710, Test Linf Norm: 0.4732\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0056, Train Linf Norm: 0.9791, Test Linf Norm: 0.4739\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0056, Train Linf Norm: 1.0066, Test Linf Norm: 0.4738\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0055, Train Linf Norm: 0.9941, Test Linf Norm: 0.4611\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0055, Train Linf Norm: 1.0006, Test Linf Norm: 0.4649\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0055, Train Linf Norm: 0.9821, Test Linf Norm: 0.4767\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0056, Train Linf Norm: 0.9878, Test Linf Norm: 0.4864\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0079, Test L1 Norm: 0.0054, Train Linf Norm: 0.9531, Test Linf Norm: 0.4695\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0054, Train Linf Norm: 0.9468, Test Linf Norm: 0.4631\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0051, Train Linf Norm: 0.9253, Test Linf Norm: 0.4202\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0063, Train Linf Norm: 0.8203, Test Linf Norm: 0.5621\n",
            "Epoch 48: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0114, Test L1 Norm: 0.0054, Train Linf Norm: 1.2908, Test Linf Norm: 0.4519\n",
            "Epoch 49: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0058, Train Linf Norm: 1.2044, Test Linf Norm: 0.4404\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0050, Train Linf Norm: 0.8987, Test Linf Norm: 0.4302\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0049, Train Linf Norm: 0.8536, Test Linf Norm: 0.4264\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0049, Train Linf Norm: 0.8192, Test Linf Norm: 0.4272\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0049, Train Linf Norm: 0.8209, Test Linf Norm: 0.4404\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0048, Train Linf Norm: 0.8377, Test Linf Norm: 0.4274\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0048, Train Linf Norm: 0.8085, Test Linf Norm: 0.4190\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0046, Train Linf Norm: 0.7927, Test Linf Norm: 0.4048\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0045, Train Linf Norm: 0.8034, Test Linf Norm: 0.3919\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0046, Train Linf Norm: 0.8031, Test Linf Norm: 0.3929\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0046, Train Linf Norm: 0.8021, Test Linf Norm: 0.4055\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0046, Train Linf Norm: 0.7955, Test Linf Norm: 0.4038\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0046, Train Linf Norm: 0.8062, Test Linf Norm: 0.4038\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0046, Train Linf Norm: 0.7902, Test Linf Norm: 0.4014\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0046, Train Linf Norm: 0.8033, Test Linf Norm: 0.3972\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0045, Train Linf Norm: 0.7953, Test Linf Norm: 0.3858\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0045, Train Linf Norm: 0.7933, Test Linf Norm: 0.3860\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0046, Train Linf Norm: 0.7707, Test Linf Norm: 0.4067\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0044, Train Linf Norm: 0.8101, Test Linf Norm: 0.3787\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0045, Train Linf Norm: 0.8063, Test Linf Norm: 0.4090\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0046, Train Linf Norm: 0.7628, Test Linf Norm: 0.4122\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0043, Train Linf Norm: 0.7690, Test Linf Norm: 0.3789\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0045, Train Linf Norm: 0.7466, Test Linf Norm: 0.3677\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0044, Train Linf Norm: 0.8992, Test Linf Norm: 0.3729\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0079, Test L1 Norm: 0.0045, Train Linf Norm: 0.8695, Test Linf Norm: 0.4067\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0045, Train Linf Norm: 0.7283, Test Linf Norm: 0.3995\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0041, Train Linf Norm: 0.6516, Test Linf Norm: 0.3712\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0041, Train Linf Norm: 0.6571, Test Linf Norm: 0.3598\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0039, Train Linf Norm: 0.6777, Test Linf Norm: 0.3508\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0039, Train Linf Norm: 0.6698, Test Linf Norm: 0.3384\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0040, Train Linf Norm: 0.6833, Test Linf Norm: 0.3550\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0042, Train Linf Norm: 0.6551, Test Linf Norm: 0.3720\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0041, Train Linf Norm: 0.6559, Test Linf Norm: 0.3625\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0041, Train Linf Norm: 0.6709, Test Linf Norm: 0.3630\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0041, Train Linf Norm: 0.6638, Test Linf Norm: 0.3630\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0041, Train Linf Norm: 0.6557, Test Linf Norm: 0.3616\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0041, Train Linf Norm: 0.6630, Test Linf Norm: 0.3616\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0040, Train Linf Norm: 0.6759, Test Linf Norm: 0.3605\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0040, Train Linf Norm: 0.6563, Test Linf Norm: 0.3597\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0041, Train Linf Norm: 0.6574, Test Linf Norm: 0.3595\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0040, Train Linf Norm: 0.6758, Test Linf Norm: 0.3560\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0039, Train Linf Norm: 0.6764, Test Linf Norm: 0.3386\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0037, Train Linf Norm: 0.6519, Test Linf Norm: 0.3229\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0039, Train Linf Norm: 0.7090, Test Linf Norm: 0.3486\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0040, Train Linf Norm: 0.6664, Test Linf Norm: 0.3557\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0041, Train Linf Norm: 0.6974, Test Linf Norm: 0.3669\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0040, Train Linf Norm: 0.6936, Test Linf Norm: 0.3595\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0040, Train Linf Norm: 0.7014, Test Linf Norm: 0.3637\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0047, Train Linf Norm: 0.7371, Test Linf Norm: 0.4514\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0041, Train Linf Norm: 0.8005, Test Linf Norm: 0.3653\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0040, Train Linf Norm: 0.6140, Test Linf Norm: 0.3720\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0039, Train Linf Norm: 0.6559, Test Linf Norm: 0.3445\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0038, Train Linf Norm: 0.6035, Test Linf Norm: 0.3322\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0036, Train Linf Norm: 0.5994, Test Linf Norm: 0.3214\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0038, Train Linf Norm: 0.5997, Test Linf Norm: 0.3494\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0037, Train Linf Norm: 0.5835, Test Linf Norm: 0.3402\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0037, Train Linf Norm: 0.5876, Test Linf Norm: 0.3303\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:31:44,606]\u001b[0m Trial 102 finished with value: 0.0037282930773682894 and parameters: {'n_layers': 3, 'n_units_0': 803, 'n_units_1': 656, 'n_units_2': 834, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.000508806547897463, 'batch_size': 232, 'n_epochs': 106, 'scheduler': 'CosineAnnealingLR', 'T_max': 12}. Best is trial 61 with value: 0.002592882362008095.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0037, Train Linf Norm: 0.5869, Test Linf Norm: 0.3401\n",
            "Epoch 1: Train Loss: 0.0596, Test Loss: 0.0010, Train L1 Norm: 0.2532, Test L1 Norm: 0.0361, Train Linf Norm: 33.2247, Test Linf Norm: 3.5685\n",
            "Epoch 2: Train Loss: 0.0010, Test Loss: 0.0003, Train L1 Norm: 0.0422, Test L1 Norm: 0.0188, Train Linf Norm: 4.3866, Test Linf Norm: 1.2746\n",
            "Epoch 3: Train Loss: 0.0004, Test Loss: 0.0001, Train L1 Norm: 0.0297, Test L1 Norm: 0.0175, Train Linf Norm: 3.4902, Test Linf Norm: 1.6750\n",
            "Epoch 4: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0188, Test L1 Norm: 0.0129, Train Linf Norm: 2.1848, Test Linf Norm: 0.9646\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0002, Train L1 Norm: 0.0182, Test L1 Norm: 0.0165, Train Linf Norm: 2.3792, Test Linf Norm: 1.4537\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0164, Test L1 Norm: 0.0092, Train Linf Norm: 2.1663, Test Linf Norm: 0.6611\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0154, Test L1 Norm: 0.0088, Train Linf Norm: 2.0300, Test Linf Norm: 0.6307\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0142, Test L1 Norm: 0.0085, Train Linf Norm: 1.9017, Test Linf Norm: 0.6142\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0138, Test L1 Norm: 0.0078, Train Linf Norm: 1.8302, Test Linf Norm: 0.5457\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0134, Test L1 Norm: 0.0079, Train Linf Norm: 1.7664, Test Linf Norm: 0.5851\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0133, Test L1 Norm: 0.0077, Train Linf Norm: 1.7660, Test Linf Norm: 0.5542\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0131, Test L1 Norm: 0.0077, Train Linf Norm: 1.7640, Test Linf Norm: 0.5546\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0131, Test L1 Norm: 0.0076, Train Linf Norm: 1.7427, Test Linf Norm: 0.5544\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0131, Test L1 Norm: 0.0076, Train Linf Norm: 1.5967, Test Linf Norm: 0.5544\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0131, Test L1 Norm: 0.0076, Train Linf Norm: 1.7561, Test Linf Norm: 0.5584\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0131, Test L1 Norm: 0.0075, Train Linf Norm: 1.7408, Test Linf Norm: 0.5561\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0130, Test L1 Norm: 0.0075, Train Linf Norm: 1.7545, Test Linf Norm: 0.5523\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0128, Test L1 Norm: 0.0073, Train Linf Norm: 1.6859, Test Linf Norm: 0.5412\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0128, Test L1 Norm: 0.0071, Train Linf Norm: 1.7442, Test Linf Norm: 0.5118\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0080, Train Linf Norm: 1.5457, Test Linf Norm: 0.7252\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0069, Train Linf Norm: 1.6716, Test Linf Norm: 0.5107\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0115, Test L1 Norm: 0.0066, Train Linf Norm: 1.4972, Test Linf Norm: 0.4977\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0066, Train Linf Norm: 1.5591, Test Linf Norm: 0.5028\n",
            "Epoch 24: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0134, Test L1 Norm: 0.0066, Train Linf Norm: 1.5723, Test Linf Norm: 0.5239\n",
            "Epoch 25: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0146, Test L1 Norm: 0.0093, Train Linf Norm: 1.5460, Test Linf Norm: 0.6786\n",
            "Epoch 26: Train Loss: 0.0004, Test Loss: 0.0000, Train L1 Norm: 0.0158, Test L1 Norm: 0.0063, Train Linf Norm: 1.5019, Test Linf Norm: 0.4457\n",
            "Epoch 27: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0144, Test L1 Norm: 0.0155, Train Linf Norm: 1.6881, Test Linf Norm: 1.3272\n",
            "Epoch 28: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0125, Test L1 Norm: 0.0062, Train Linf Norm: 1.5204, Test Linf Norm: 0.4872\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0080, Train Linf Norm: 1.3151, Test Linf Norm: 0.7112\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0097, Test L1 Norm: 0.0055, Train Linf Norm: 1.2918, Test Linf Norm: 0.4218\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0095, Test L1 Norm: 0.0054, Train Linf Norm: 1.2685, Test Linf Norm: 0.4174\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0093, Test L1 Norm: 0.0054, Train Linf Norm: 1.2384, Test Linf Norm: 0.4280\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0090, Test L1 Norm: 0.0057, Train Linf Norm: 1.1884, Test Linf Norm: 0.4446\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0090, Test L1 Norm: 0.0052, Train Linf Norm: 1.2344, Test Linf Norm: 0.4085\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0051, Train Linf Norm: 1.1601, Test Linf Norm: 0.4003\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0088, Test L1 Norm: 0.0052, Train Linf Norm: 1.1627, Test Linf Norm: 0.4088\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0052, Train Linf Norm: 1.1734, Test Linf Norm: 0.4080\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0051, Train Linf Norm: 1.1734, Test Linf Norm: 0.4052\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0051, Train Linf Norm: 1.1751, Test Linf Norm: 0.4086\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0051, Train Linf Norm: 1.1762, Test Linf Norm: 0.4086\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0051, Train Linf Norm: 1.1648, Test Linf Norm: 0.4087\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0052, Train Linf Norm: 1.1782, Test Linf Norm: 0.4087\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0051, Train Linf Norm: 1.1745, Test Linf Norm: 0.4064\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0086, Test L1 Norm: 0.0052, Train Linf Norm: 1.1540, Test Linf Norm: 0.4138\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0050, Train Linf Norm: 1.1719, Test Linf Norm: 0.4085\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0051, Train Linf Norm: 1.0695, Test Linf Norm: 0.4110\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0086, Test L1 Norm: 0.0049, Train Linf Norm: 1.1383, Test Linf Norm: 0.3854\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0049, Train Linf Norm: 1.1291, Test Linf Norm: 0.3868\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0049, Train Linf Norm: 1.0767, Test Linf Norm: 0.4078\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0089, Test L1 Norm: 0.0050, Train Linf Norm: 1.2172, Test Linf Norm: 0.4187\n",
            "Epoch 51: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0103, Test L1 Norm: 0.0104, Train Linf Norm: 1.1999, Test Linf Norm: 0.5504\n",
            "Epoch 52: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0140, Test L1 Norm: 0.0068, Train Linf Norm: 1.7975, Test Linf Norm: 0.4557\n",
            "Epoch 53: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0102, Test L1 Norm: 0.0049, Train Linf Norm: 1.1479, Test Linf Norm: 0.4017\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0050, Train Linf Norm: 1.2838, Test Linf Norm: 0.4094\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0082, Test L1 Norm: 0.0102, Train Linf Norm: 1.0906, Test Linf Norm: 0.8097\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0047, Train Linf Norm: 1.0071, Test Linf Norm: 0.3838\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0046, Train Linf Norm: 0.9682, Test Linf Norm: 0.3794\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0043, Train Linf Norm: 0.9738, Test Linf Norm: 0.3467\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0073, Test L1 Norm: 0.0045, Train Linf Norm: 0.9712, Test Linf Norm: 0.3693\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0073, Test L1 Norm: 0.0046, Train Linf Norm: 0.9513, Test Linf Norm: 0.3847\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0045, Train Linf Norm: 0.9391, Test Linf Norm: 0.3635\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0044, Train Linf Norm: 0.9104, Test Linf Norm: 0.3572\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0044, Train Linf Norm: 0.9328, Test Linf Norm: 0.3602\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0044, Train Linf Norm: 0.9306, Test Linf Norm: 0.3622\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0044, Train Linf Norm: 0.9374, Test Linf Norm: 0.3619\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0044, Train Linf Norm: 0.9356, Test Linf Norm: 0.3619\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0044, Train Linf Norm: 0.8503, Test Linf Norm: 0.3627\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0044, Train Linf Norm: 0.9352, Test Linf Norm: 0.3609\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0044, Train Linf Norm: 0.9397, Test Linf Norm: 0.3629\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0043, Train Linf Norm: 0.9268, Test Linf Norm: 0.3575\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0044, Train Linf Norm: 0.9463, Test Linf Norm: 0.3597\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0044, Train Linf Norm: 0.8670, Test Linf Norm: 0.3670\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0045, Train Linf Norm: 0.9250, Test Linf Norm: 0.3797\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.8957, Test Linf Norm: 0.3519\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.8954, Test Linf Norm: 0.3539\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0045, Train Linf Norm: 0.8610, Test Linf Norm: 0.3802\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0041, Train Linf Norm: 0.9791, Test Linf Norm: 0.3528\n",
            "Epoch 78: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0095, Test L1 Norm: 0.0043, Train Linf Norm: 1.0547, Test Linf Norm: 0.3516\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0042, Train Linf Norm: 0.9777, Test Linf Norm: 0.3495\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.8589, Test Linf Norm: 0.3670\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0045, Train Linf Norm: 0.8730, Test Linf Norm: 0.4219\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0041, Train Linf Norm: 0.8347, Test Linf Norm: 0.3517\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0041, Train Linf Norm: 0.8001, Test Linf Norm: 0.3476\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0039, Train Linf Norm: 0.7943, Test Linf Norm: 0.3164\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0042, Train Linf Norm: 0.8063, Test Linf Norm: 0.3585\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0040, Train Linf Norm: 0.7747, Test Linf Norm: 0.3260\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0041, Train Linf Norm: 0.7814, Test Linf Norm: 0.3477\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0040, Train Linf Norm: 0.7738, Test Linf Norm: 0.3366\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0040, Train Linf Norm: 0.7786, Test Linf Norm: 0.3406\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0040, Train Linf Norm: 0.7704, Test Linf Norm: 0.3328\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0040, Train Linf Norm: 0.7764, Test Linf Norm: 0.3383\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0040, Train Linf Norm: 0.7576, Test Linf Norm: 0.3383\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0040, Train Linf Norm: 0.7802, Test Linf Norm: 0.3384\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0039, Train Linf Norm: 0.7764, Test Linf Norm: 0.3270\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0040, Train Linf Norm: 0.7639, Test Linf Norm: 0.3385\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0040, Train Linf Norm: 0.7685, Test Linf Norm: 0.3322\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0039, Train Linf Norm: 0.7516, Test Linf Norm: 0.3322\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0038, Train Linf Norm: 0.7628, Test Linf Norm: 0.3099\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0040, Train Linf Norm: 0.7601, Test Linf Norm: 0.3439\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0040, Train Linf Norm: 0.7747, Test Linf Norm: 0.3433\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0039, Train Linf Norm: 0.7477, Test Linf Norm: 0.3221\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0042, Train Linf Norm: 0.7347, Test Linf Norm: 0.4065\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0040, Train Linf Norm: 0.7994, Test Linf Norm: 0.3426\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:34:32,060]\u001b[0m Trial 103 finished with value: 0.0038999582937918604 and parameters: {'n_layers': 3, 'n_units_0': 808, 'n_units_1': 656, 'n_units_2': 814, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.00048190989240888847, 'batch_size': 231, 'n_epochs': 104, 'scheduler': 'CosineAnnealingLR', 'T_max': 13}. Best is trial 61 with value: 0.002592882362008095.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 104: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0039, Train Linf Norm: 0.8893, Test Linf Norm: 0.3290\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:34:33,590]\u001b[0m Trial 104 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0543, Test Loss: 0.0039, Train L1 Norm: 0.1768, Test L1 Norm: 0.1360, Train Linf Norm: 20.6735, Test Linf Norm: 23.7347\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:34:35,120]\u001b[0m Trial 105 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0520, Test Loss: 0.0005, Train L1 Norm: 0.2059, Test L1 Norm: 0.1840, Train Linf Norm: 23.4327, Test Linf Norm: 36.9090\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:34:36,742]\u001b[0m Trial 106 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0611, Test Loss: 0.0010, Train L1 Norm: 0.2578, Test L1 Norm: 0.1045, Train Linf Norm: 35.2958, Test Linf Norm: 20.9638\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:34:38,198]\u001b[0m Trial 107 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0573, Test Loss: 0.0029, Train L1 Norm: 0.1602, Test L1 Norm: 0.2230, Train Linf Norm: 16.2088, Test Linf Norm: 46.4470\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:34:40,371]\u001b[0m Trial 108 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.1159, Test Loss: 0.0043, Train L1 Norm: 0.7966, Test L1 Norm: 0.5945, Train Linf Norm: 91.1168, Test Linf Norm: 83.3321\n",
            "Epoch 1: Train Loss: 0.0573, Test Loss: 0.0007, Train L1 Norm: 0.2337, Test L1 Norm: 0.0638, Train Linf Norm: 30.4874, Test Linf Norm: 11.2112\n",
            "Epoch 2: Train Loss: 0.0009, Test Loss: 0.0001, Train L1 Norm: 0.0291, Test L1 Norm: 0.0304, Train Linf Norm: 2.4784, Test Linf Norm: 4.7577\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:34:45,195]\u001b[0m Trial 109 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0224, Test L1 Norm: 0.0580, Train Linf Norm: 2.3187, Test Linf Norm: 10.6433\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:34:47,050]\u001b[0m Trial 110 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0364, Test Loss: 0.0013, Train L1 Norm: 1.9453, Test L1 Norm: 1.1729, Train Linf Norm: 254.9550, Test Linf Norm: 171.8080\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:34:48,954]\u001b[0m Trial 111 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:34:50,671]\u001b[0m Trial 112 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:34:52,616]\u001b[0m Trial 113 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9577, Test Loss: 2.9774, Train L1 Norm: 1.3568, Test L1 Norm: 1.0072, Train Linf Norm: 86.7969, Test Linf Norm: 2.8624\n",
            "Epoch 1: Train Loss: 0.0416, Test Loss: 0.0014, Train L1 Norm: 0.1735, Test L1 Norm: 0.0280, Train Linf Norm: 15.4821, Test Linf Norm: 0.9311\n",
            "Epoch 2: Train Loss: 0.0005, Test Loss: 0.0002, Train L1 Norm: 0.0375, Test L1 Norm: 0.0394, Train Linf Norm: 3.5046, Test Linf Norm: 4.6738\n",
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0254, Test L1 Norm: 0.0107, Train Linf Norm: 2.6456, Test Linf Norm: 0.6303\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0202, Test L1 Norm: 0.0097, Train Linf Norm: 2.1529, Test Linf Norm: 0.6245\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0179, Test L1 Norm: 0.0096, Train Linf Norm: 1.8407, Test Linf Norm: 0.6601\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0170, Test L1 Norm: 0.0085, Train Linf Norm: 1.7808, Test Linf Norm: 0.5675\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0164, Test L1 Norm: 0.0082, Train Linf Norm: 1.7151, Test Linf Norm: 0.5432\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0159, Test L1 Norm: 0.0080, Train Linf Norm: 1.6688, Test Linf Norm: 0.5282\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0157, Test L1 Norm: 0.0081, Train Linf Norm: 1.6459, Test Linf Norm: 0.5351\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0156, Test L1 Norm: 0.0081, Train Linf Norm: 1.6232, Test Linf Norm: 0.5351\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0156, Test L1 Norm: 0.0081, Train Linf Norm: 1.6341, Test Linf Norm: 0.5389\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0156, Test L1 Norm: 0.0079, Train Linf Norm: 1.6430, Test Linf Norm: 0.5247\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0153, Test L1 Norm: 0.0079, Train Linf Norm: 1.6016, Test Linf Norm: 0.5313\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0150, Test L1 Norm: 0.0077, Train Linf Norm: 1.5491, Test Linf Norm: 0.5015\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0145, Test L1 Norm: 0.0074, Train Linf Norm: 1.5167, Test Linf Norm: 0.5002\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0135, Test L1 Norm: 0.0078, Train Linf Norm: 1.2736, Test Linf Norm: 0.5251\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0146, Test L1 Norm: 0.0075, Train Linf Norm: 1.4474, Test Linf Norm: 0.4050\n",
            "Epoch 18: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0166, Test L1 Norm: 0.0074, Train Linf Norm: 1.4999, Test Linf Norm: 0.3828\n",
            "Epoch 19: Train Loss: 0.0003, Test Loss: 0.0000, Train L1 Norm: 0.0160, Test L1 Norm: 0.0069, Train Linf Norm: 1.4080, Test Linf Norm: 0.4610\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0137, Test L1 Norm: 0.0067, Train Linf Norm: 1.4100, Test Linf Norm: 0.3702\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0113, Test L1 Norm: 0.0061, Train Linf Norm: 1.1402, Test Linf Norm: 0.3864\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0060, Train Linf Norm: 1.0414, Test Linf Norm: 0.4064\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0103, Test L1 Norm: 0.0061, Train Linf Norm: 1.0378, Test Linf Norm: 0.4264\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0099, Test L1 Norm: 0.0059, Train Linf Norm: 0.9835, Test Linf Norm: 0.4084\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0097, Test L1 Norm: 0.0059, Train Linf Norm: 0.9702, Test Linf Norm: 0.4115\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0096, Test L1 Norm: 0.0058, Train Linf Norm: 0.9794, Test Linf Norm: 0.4092\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0096, Test L1 Norm: 0.0058, Train Linf Norm: 0.9650, Test Linf Norm: 0.4090\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0096, Test L1 Norm: 0.0058, Train Linf Norm: 0.9678, Test Linf Norm: 0.4090\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0096, Test L1 Norm: 0.0059, Train Linf Norm: 0.9800, Test Linf Norm: 0.4106\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0096, Test L1 Norm: 0.0058, Train Linf Norm: 0.9561, Test Linf Norm: 0.4082\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0095, Test L1 Norm: 0.0058, Train Linf Norm: 0.9623, Test Linf Norm: 0.4031\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0055, Train Linf Norm: 0.9511, Test Linf Norm: 0.3826\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0093, Test L1 Norm: 0.0056, Train Linf Norm: 0.9026, Test Linf Norm: 0.3838\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0090, Test L1 Norm: 0.0056, Train Linf Norm: 0.8924, Test Linf Norm: 0.4005\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0090, Test L1 Norm: 0.0055, Train Linf Norm: 0.8878, Test Linf Norm: 0.3796\n",
            "Epoch 36: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0055, Train Linf Norm: 1.1382, Test Linf Norm: 0.4085\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0099, Test L1 Norm: 0.0061, Train Linf Norm: 0.8758, Test Linf Norm: 0.4485\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0092, Test L1 Norm: 0.0064, Train Linf Norm: 0.9255, Test Linf Norm: 0.3420\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0084, Test L1 Norm: 0.0050, Train Linf Norm: 0.8172, Test Linf Norm: 0.3527\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0083, Test L1 Norm: 0.0051, Train Linf Norm: 0.8526, Test Linf Norm: 0.3656\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0052, Train Linf Norm: 0.7755, Test Linf Norm: 0.3767\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0049, Train Linf Norm: 0.8159, Test Linf Norm: 0.3488\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0050, Train Linf Norm: 0.8014, Test Linf Norm: 0.3561\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0049, Train Linf Norm: 0.8006, Test Linf Norm: 0.3436\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0049, Train Linf Norm: 0.8157, Test Linf Norm: 0.3432\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0079, Test L1 Norm: 0.0049, Train Linf Norm: 0.8095, Test Linf Norm: 0.3432\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0079, Test L1 Norm: 0.0049, Train Linf Norm: 0.8075, Test Linf Norm: 0.3449\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0049, Train Linf Norm: 0.8057, Test Linf Norm: 0.3525\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0049, Train Linf Norm: 0.8197, Test Linf Norm: 0.3486\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0048, Train Linf Norm: 0.8107, Test Linf Norm: 0.3482\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0050, Train Linf Norm: 0.7866, Test Linf Norm: 0.3713\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0079, Test L1 Norm: 0.0049, Train Linf Norm: 0.8012, Test Linf Norm: 0.3527\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0047, Train Linf Norm: 0.8169, Test Linf Norm: 0.3034\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0084, Test L1 Norm: 0.0051, Train Linf Norm: 0.8436, Test Linf Norm: 0.3621\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0046, Train Linf Norm: 0.7430, Test Linf Norm: 0.3272\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0046, Train Linf Norm: 0.6814, Test Linf Norm: 0.3289\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0073, Test L1 Norm: 0.0047, Train Linf Norm: 0.7252, Test Linf Norm: 0.3576\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0073, Test L1 Norm: 0.0048, Train Linf Norm: 0.7290, Test Linf Norm: 0.3593\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0044, Train Linf Norm: 0.7309, Test Linf Norm: 0.3148\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0044, Train Linf Norm: 0.7079, Test Linf Norm: 0.3204\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0043, Train Linf Norm: 0.7175, Test Linf Norm: 0.3060\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0043, Train Linf Norm: 0.7085, Test Linf Norm: 0.3039\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0043, Train Linf Norm: 0.6988, Test Linf Norm: 0.3070\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0043, Train Linf Norm: 0.7109, Test Linf Norm: 0.3070\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0043, Train Linf Norm: 0.7058, Test Linf Norm: 0.3081\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0044, Train Linf Norm: 0.7203, Test Linf Norm: 0.3094\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0043, Train Linf Norm: 0.7075, Test Linf Norm: 0.3044\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.6977, Test Linf Norm: 0.3197\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0044, Train Linf Norm: 0.6887, Test Linf Norm: 0.3191\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0043, Train Linf Norm: 0.7027, Test Linf Norm: 0.3027\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0042, Train Linf Norm: 0.6905, Test Linf Norm: 0.3100\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0046, Train Linf Norm: 0.7392, Test Linf Norm: 0.3356\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0047, Train Linf Norm: 0.7610, Test Linf Norm: 0.3581\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0073, Test L1 Norm: 0.0043, Train Linf Norm: 0.6918, Test Linf Norm: 0.2983\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0041, Train Linf Norm: 0.6747, Test Linf Norm: 0.2914\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0039, Train Linf Norm: 0.6525, Test Linf Norm: 0.2731\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0041, Train Linf Norm: 0.6599, Test Linf Norm: 0.2846\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0040, Train Linf Norm: 0.6497, Test Linf Norm: 0.2847\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0039, Train Linf Norm: 0.6612, Test Linf Norm: 0.2854\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0039, Train Linf Norm: 0.6581, Test Linf Norm: 0.2875\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0040, Train Linf Norm: 0.6568, Test Linf Norm: 0.2799\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0040, Train Linf Norm: 0.6513, Test Linf Norm: 0.2799\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0039, Train Linf Norm: 0.6556, Test Linf Norm: 0.2792\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0039, Train Linf Norm: 0.6547, Test Linf Norm: 0.2840\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0039, Train Linf Norm: 0.6582, Test Linf Norm: 0.2801\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0039, Train Linf Norm: 0.6622, Test Linf Norm: 0.2808\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0038, Train Linf Norm: 0.6562, Test Linf Norm: 0.2622\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0038, Train Linf Norm: 0.6074, Test Linf Norm: 0.2682\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0041, Train Linf Norm: 0.6495, Test Linf Norm: 0.2917\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0039, Train Linf Norm: 0.6648, Test Linf Norm: 0.2787\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0045, Train Linf Norm: 0.6555, Test Linf Norm: 0.3298\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0037, Train Linf Norm: 0.6774, Test Linf Norm: 0.2676\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0044, Train Linf Norm: 0.6341, Test Linf Norm: 0.3166\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0037, Train Linf Norm: 0.6195, Test Linf Norm: 0.2705\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0037, Train Linf Norm: 0.6248, Test Linf Norm: 0.2681\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0038, Train Linf Norm: 0.6106, Test Linf Norm: 0.2661\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0037, Train Linf Norm: 0.6251, Test Linf Norm: 0.2650\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0037, Train Linf Norm: 0.6170, Test Linf Norm: 0.2691\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0037, Train Linf Norm: 0.6170, Test Linf Norm: 0.2624\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0037, Train Linf Norm: 0.6240, Test Linf Norm: 0.2624\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0037, Train Linf Norm: 0.6134, Test Linf Norm: 0.2620\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0037, Train Linf Norm: 0.6138, Test Linf Norm: 0.2662\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0037, Train Linf Norm: 0.6202, Test Linf Norm: 0.2690\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0036, Train Linf Norm: 0.6116, Test Linf Norm: 0.2585\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0036, Train Linf Norm: 0.6227, Test Linf Norm: 0.2574\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0036, Train Linf Norm: 0.6249, Test Linf Norm: 0.2569\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0036, Train Linf Norm: 0.6388, Test Linf Norm: 0.2455\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0037, Train Linf Norm: 0.5773, Test Linf Norm: 0.2627\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0036, Train Linf Norm: 0.6313, Test Linf Norm: 0.2383\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0036, Train Linf Norm: 0.5729, Test Linf Norm: 0.2503\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0037, Train Linf Norm: 0.6161, Test Linf Norm: 0.2605\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0037, Train Linf Norm: 0.6121, Test Linf Norm: 0.2503\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0035, Train Linf Norm: 0.5923, Test Linf Norm: 0.2439\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0037, Train Linf Norm: 0.6066, Test Linf Norm: 0.2561\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0035, Train Linf Norm: 0.6071, Test Linf Norm: 0.2451\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0035, Train Linf Norm: 0.5955, Test Linf Norm: 0.2465\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0035, Train Linf Norm: 0.5884, Test Linf Norm: 0.2441\n",
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0035, Train Linf Norm: 0.5924, Test Linf Norm: 0.2441\n",
            "Epoch 119: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0035, Train Linf Norm: 0.5893, Test Linf Norm: 0.2441\n",
            "Epoch 120: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0035, Train Linf Norm: 0.5977, Test Linf Norm: 0.2420\n",
            "Epoch 121: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0035, Train Linf Norm: 0.5868, Test Linf Norm: 0.2456\n",
            "Epoch 122: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0034, Train Linf Norm: 0.6112, Test Linf Norm: 0.2389\n",
            "Epoch 123: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0035, Train Linf Norm: 0.5899, Test Linf Norm: 0.2431\n",
            "Epoch 124: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0036, Train Linf Norm: 0.5719, Test Linf Norm: 0.2497\n",
            "Epoch 125: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0034, Train Linf Norm: 0.6361, Test Linf Norm: 0.2369\n",
            "Epoch 126: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0037, Train Linf Norm: 0.6077, Test Linf Norm: 0.2611\n",
            "Epoch 127: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0036, Train Linf Norm: 0.6475, Test Linf Norm: 0.2481\n",
            "Epoch 128: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0034, Train Linf Norm: 0.6747, Test Linf Norm: 0.2438\n",
            "Epoch 129: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0037, Train Linf Norm: 0.6151, Test Linf Norm: 0.2539\n",
            "Epoch 130: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0035, Train Linf Norm: 0.5808, Test Linf Norm: 0.2411\n",
            "Epoch 131: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0033, Train Linf Norm: 0.5740, Test Linf Norm: 0.2293\n",
            "Epoch 132: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0034, Train Linf Norm: 0.5676, Test Linf Norm: 0.2371\n",
            "Epoch 133: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0033, Train Linf Norm: 0.5738, Test Linf Norm: 0.2315\n",
            "Epoch 134: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0033, Train Linf Norm: 0.5742, Test Linf Norm: 0.2311\n",
            "Epoch 135: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0032, Train Linf Norm: 0.5747, Test Linf Norm: 0.2268\n",
            "Epoch 136: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0032, Train Linf Norm: 0.5762, Test Linf Norm: 0.2268\n",
            "Epoch 137: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0033, Train Linf Norm: 0.5754, Test Linf Norm: 0.2271\n",
            "Epoch 138: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0033, Train Linf Norm: 0.5757, Test Linf Norm: 0.2313\n",
            "Epoch 139: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0033, Train Linf Norm: 0.5797, Test Linf Norm: 0.2280\n",
            "Epoch 140: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0033, Train Linf Norm: 0.5666, Test Linf Norm: 0.2269\n",
            "Epoch 141: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0032, Train Linf Norm: 0.5817, Test Linf Norm: 0.2201\n",
            "Epoch 142: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0033, Train Linf Norm: 0.5945, Test Linf Norm: 0.2210\n",
            "Epoch 143: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0037, Train Linf Norm: 0.6283, Test Linf Norm: 0.2560\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:39:37,830]\u001b[0m Trial 114 finished with value: 0.00320622049132362 and parameters: {'n_layers': 3, 'n_units_0': 957, 'n_units_1': 922, 'n_units_2': 274, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.0004988292507683398, 'batch_size': 154, 'n_epochs': 144, 'scheduler': 'CosineAnnealingLR', 'T_max': 9}. Best is trial 61 with value: 0.002592882362008095.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 144: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0032, Train Linf Norm: 0.6060, Test Linf Norm: 0.2167\n",
            "Epoch 1: Train Loss: 0.0413, Test Loss: 0.0003, Train L1 Norm: 0.1705, Test L1 Norm: 0.0546, Train Linf Norm: 13.6856, Test Linf Norm: 6.0741\n",
            "Epoch 2: Train Loss: 0.0007, Test Loss: 0.0001, Train L1 Norm: 0.0413, Test L1 Norm: 0.0413, Train Linf Norm: 3.7037, Test Linf Norm: 4.7942\n",
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0284, Test L1 Norm: 0.0227, Train Linf Norm: 2.9124, Test Linf Norm: 2.3515\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0193, Test L1 Norm: 0.0184, Train Linf Norm: 1.8778, Test Linf Norm: 1.8989\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0160, Test L1 Norm: 0.0167, Train Linf Norm: 1.5267, Test Linf Norm: 1.7543\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0150, Test L1 Norm: 0.0130, Train Linf Norm: 1.4427, Test Linf Norm: 1.2335\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0136, Test L1 Norm: 0.0120, Train Linf Norm: 1.2920, Test Linf Norm: 1.1321\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0129, Test L1 Norm: 0.0123, Train Linf Norm: 1.2094, Test Linf Norm: 1.1696\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0128, Test L1 Norm: 0.0109, Train Linf Norm: 1.1930, Test Linf Norm: 0.9720\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0125, Test L1 Norm: 0.0109, Train Linf Norm: 1.1436, Test Linf Norm: 0.9720\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0127, Test L1 Norm: 0.0114, Train Linf Norm: 1.1876, Test Linf Norm: 1.0512\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0126, Test L1 Norm: 0.0115, Train Linf Norm: 1.1735, Test Linf Norm: 1.0859\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0101, Train Linf Norm: 1.1564, Test Linf Norm: 0.8809\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0116, Test L1 Norm: 0.0084, Train Linf Norm: 1.0861, Test Linf Norm: 0.6534\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0110, Test L1 Norm: 0.0097, Train Linf Norm: 1.0031, Test Linf Norm: 0.8694\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0067, Train Linf Norm: 0.9342, Test Linf Norm: 0.4604\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0109, Test L1 Norm: 0.0065, Train Linf Norm: 0.9703, Test Linf Norm: 0.4465\n",
            "Epoch 18: Train Loss: 0.0004, Test Loss: 0.0000, Train L1 Norm: 0.0136, Test L1 Norm: 0.0139, Train Linf Norm: 1.0701, Test Linf Norm: 1.5668\n",
            "Epoch 19: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0109, Test L1 Norm: 0.0054, Train Linf Norm: 0.7239, Test Linf Norm: 0.3085\n",
            "Epoch 20: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0107, Test L1 Norm: 0.0056, Train Linf Norm: 0.9165, Test Linf Norm: 0.2906\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0083, Test L1 Norm: 0.0055, Train Linf Norm: 0.7450, Test Linf Norm: 0.3678\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0084, Test L1 Norm: 0.0103, Train Linf Norm: 0.7893, Test Linf Norm: 1.0631\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0044, Train Linf Norm: 0.7460, Test Linf Norm: 0.2409\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0044, Train Linf Norm: 0.7059, Test Linf Norm: 0.2424\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0045, Train Linf Norm: 0.7194, Test Linf Norm: 0.2628\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0045, Train Linf Norm: 0.7120, Test Linf Norm: 0.2568\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0045, Train Linf Norm: 0.7029, Test Linf Norm: 0.2665\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0045, Train Linf Norm: 0.7086, Test Linf Norm: 0.2665\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0045, Train Linf Norm: 0.7098, Test Linf Norm: 0.2562\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0044, Train Linf Norm: 0.7015, Test Linf Norm: 0.2586\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0043, Train Linf Norm: 0.7129, Test Linf Norm: 0.2365\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0042, Train Linf Norm: 0.6961, Test Linf Norm: 0.2317\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0043, Train Linf Norm: 0.6779, Test Linf Norm: 0.2577\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0041, Train Linf Norm: 0.7089, Test Linf Norm: 0.2254\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0038, Train Linf Norm: 0.7127, Test Linf Norm: 0.2052\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0047, Train Linf Norm: 0.6978, Test Linf Norm: 0.2260\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0040, Train Linf Norm: 0.9309, Test Linf Norm: 0.2107\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0041, Train Linf Norm: 0.7321, Test Linf Norm: 0.2216\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0089, Train Linf Norm: 0.6238, Test Linf Norm: 0.9588\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0038, Train Linf Norm: 0.6407, Test Linf Norm: 0.2237\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0037, Train Linf Norm: 0.6343, Test Linf Norm: 0.2278\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0037, Train Linf Norm: 0.5961, Test Linf Norm: 0.2244\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0042, Train Linf Norm: 0.5952, Test Linf Norm: 0.3105\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0037, Train Linf Norm: 0.6027, Test Linf Norm: 0.2312\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0038, Train Linf Norm: 0.6048, Test Linf Norm: 0.2354\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0038, Train Linf Norm: 0.5991, Test Linf Norm: 0.2354\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0037, Train Linf Norm: 0.6091, Test Linf Norm: 0.2258\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0040, Train Linf Norm: 0.5899, Test Linf Norm: 0.2769\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0037, Train Linf Norm: 0.6078, Test Linf Norm: 0.2311\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0039, Train Linf Norm: 0.6041, Test Linf Norm: 0.2579\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0040, Train Linf Norm: 0.5947, Test Linf Norm: 0.2741\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0037, Train Linf Norm: 0.5823, Test Linf Norm: 0.2304\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0036, Train Linf Norm: 0.5966, Test Linf Norm: 0.2296\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0073, Train Linf Norm: 0.6093, Test Linf Norm: 0.7461\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0002, Train L1 Norm: 0.0082, Test L1 Norm: 0.0094, Train Linf Norm: 0.7187, Test Linf Norm: 0.3537\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0052, Train Linf Norm: 0.7463, Test Linf Norm: 0.4678\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0035, Train Linf Norm: 0.6026, Test Linf Norm: 0.2243\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0056, Train Linf Norm: 0.5682, Test Linf Norm: 0.5359\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0034, Train Linf Norm: 0.5598, Test Linf Norm: 0.2202\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0042, Train Linf Norm: 0.5517, Test Linf Norm: 0.3409\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0034, Train Linf Norm: 0.5501, Test Linf Norm: 0.2221\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0039, Train Linf Norm: 0.5535, Test Linf Norm: 0.3022\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0035, Train Linf Norm: 0.5436, Test Linf Norm: 0.2402\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0035, Train Linf Norm: 0.5404, Test Linf Norm: 0.2402\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0035, Train Linf Norm: 0.5511, Test Linf Norm: 0.2326\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0038, Train Linf Norm: 0.5458, Test Linf Norm: 0.2844\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0034, Train Linf Norm: 0.5588, Test Linf Norm: 0.2145\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0034, Train Linf Norm: 0.5270, Test Linf Norm: 0.2184\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0034, Train Linf Norm: 0.5482, Test Linf Norm: 0.2301\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0039, Train Linf Norm: 0.5199, Test Linf Norm: 0.3087\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0033, Train Linf Norm: 0.5113, Test Linf Norm: 0.2005\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0040, Train Linf Norm: 0.5080, Test Linf Norm: 0.3134\n",
            "Epoch 73: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0063, Train Linf Norm: 0.7009, Test Linf Norm: 0.6491\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0039, Train Linf Norm: 0.5809, Test Linf Norm: 0.3111\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0039, Train Linf Norm: 0.5407, Test Linf Norm: 0.2886\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0046, Train Linf Norm: 0.5478, Test Linf Norm: 0.4190\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0047, Train Linf Norm: 0.5386, Test Linf Norm: 0.4328\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0052, Train Linf Norm: 0.5468, Test Linf Norm: 0.5160\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0047, Train Linf Norm: 0.5266, Test Linf Norm: 0.4466\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0045, Train Linf Norm: 0.5387, Test Linf Norm: 0.4208\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0046, Train Linf Norm: 0.5364, Test Linf Norm: 0.4284\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0046, Train Linf Norm: 0.5405, Test Linf Norm: 0.4284\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0045, Train Linf Norm: 0.5331, Test Linf Norm: 0.4219\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0046, Train Linf Norm: 0.5283, Test Linf Norm: 0.4326\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0041, Train Linf Norm: 0.5360, Test Linf Norm: 0.3611\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0044, Train Linf Norm: 0.5290, Test Linf Norm: 0.3993\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0050, Train Linf Norm: 0.5221, Test Linf Norm: 0.4987\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0037, Train Linf Norm: 0.5301, Test Linf Norm: 0.2954\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0067, Train Linf Norm: 0.5281, Test Linf Norm: 0.7358\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0070, Train Linf Norm: 0.5415, Test Linf Norm: 0.7922\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0030, Train Linf Norm: 0.5580, Test Linf Norm: 0.1748\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0052, Train Linf Norm: 0.5635, Test Linf Norm: 0.5052\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0047, Train Linf Norm: 0.5435, Test Linf Norm: 0.4643\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0046, Train Linf Norm: 0.5053, Test Linf Norm: 0.4488\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0050, Train Linf Norm: 0.4921, Test Linf Norm: 0.5092\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0042, Train Linf Norm: 0.5036, Test Linf Norm: 0.3871\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0049, Train Linf Norm: 0.5029, Test Linf Norm: 0.4898\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0049, Train Linf Norm: 0.4846, Test Linf Norm: 0.4893\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0046, Train Linf Norm: 0.4930, Test Linf Norm: 0.4549\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0046, Train Linf Norm: 0.4947, Test Linf Norm: 0.4549\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0044, Train Linf Norm: 0.5009, Test Linf Norm: 0.4282\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0044, Train Linf Norm: 0.4979, Test Linf Norm: 0.4171\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0048, Train Linf Norm: 0.5103, Test Linf Norm: 0.4768\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0045, Train Linf Norm: 0.4884, Test Linf Norm: 0.4371\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0050, Train Linf Norm: 0.5012, Test Linf Norm: 0.5062\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0039, Train Linf Norm: 0.4920, Test Linf Norm: 0.3495\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0053, Train Linf Norm: 0.4879, Test Linf Norm: 0.5562\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0045, Train Linf Norm: 0.5056, Test Linf Norm: 0.4319\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0055, Train Linf Norm: 0.6308, Test Linf Norm: 0.5802\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0053, Train Linf Norm: 0.4719, Test Linf Norm: 0.5531\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0054, Train Linf Norm: 0.4654, Test Linf Norm: 0.5682\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0037, Train Linf Norm: 0.4867, Test Linf Norm: 0.3276\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0041, Train Linf Norm: 0.4963, Test Linf Norm: 0.3878\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0039, Train Linf Norm: 0.4773, Test Linf Norm: 0.3577\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0039, Train Linf Norm: 0.4648, Test Linf Norm: 0.3488\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0037, Train Linf Norm: 0.4722, Test Linf Norm: 0.3236\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0039, Train Linf Norm: 0.4634, Test Linf Norm: 0.3548\n",
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0039, Train Linf Norm: 0.4570, Test Linf Norm: 0.3548\n",
            "Epoch 119: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0039, Train Linf Norm: 0.4632, Test Linf Norm: 0.3613\n",
            "Epoch 120: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0043, Train Linf Norm: 0.4672, Test Linf Norm: 0.4218\n",
            "Epoch 121: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0040, Train Linf Norm: 0.4604, Test Linf Norm: 0.3769\n",
            "Epoch 122: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0044, Train Linf Norm: 0.4593, Test Linf Norm: 0.4283\n",
            "Epoch 123: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0035, Train Linf Norm: 0.4456, Test Linf Norm: 0.3041\n",
            "Epoch 124: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0043, Train Linf Norm: 0.4704, Test Linf Norm: 0.4261\n",
            "Epoch 125: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0031, Train Linf Norm: 0.4601, Test Linf Norm: 0.2201\n",
            "Epoch 126: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0073, Train Linf Norm: 0.3937, Test Linf Norm: 0.8464\n",
            "Epoch 127: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0106, Train Linf Norm: 0.4339, Test Linf Norm: 1.2053\n",
            "Epoch 128: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0101, Train Linf Norm: 0.5183, Test Linf Norm: 1.1935\n",
            "Epoch 129: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0043, Train Linf Norm: 0.4262, Test Linf Norm: 0.4305\n",
            "Epoch 130: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0052, Train Linf Norm: 0.4223, Test Linf Norm: 0.5673\n",
            "Epoch 131: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0043, Train Linf Norm: 0.4313, Test Linf Norm: 0.4354\n",
            "Epoch 132: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0044, Train Linf Norm: 0.4240, Test Linf Norm: 0.4436\n",
            "Epoch 133: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0041, Train Linf Norm: 0.4367, Test Linf Norm: 0.4038\n",
            "Epoch 134: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0043, Train Linf Norm: 0.4174, Test Linf Norm: 0.4248\n",
            "Epoch 135: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0043, Train Linf Norm: 0.4177, Test Linf Norm: 0.4262\n",
            "Epoch 136: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0043, Train Linf Norm: 0.4208, Test Linf Norm: 0.4262\n",
            "Epoch 137: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0043, Train Linf Norm: 0.4162, Test Linf Norm: 0.4235\n",
            "Epoch 138: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0043, Train Linf Norm: 0.4194, Test Linf Norm: 0.4288\n",
            "Epoch 139: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0041, Train Linf Norm: 0.4133, Test Linf Norm: 0.3970\n",
            "Epoch 140: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0040, Train Linf Norm: 0.4055, Test Linf Norm: 0.3779\n",
            "Epoch 141: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0037, Train Linf Norm: 0.4067, Test Linf Norm: 0.3359\n",
            "Epoch 142: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0050, Train Linf Norm: 0.4044, Test Linf Norm: 0.5322\n",
            "Epoch 143: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0053, Train Linf Norm: 0.4150, Test Linf Norm: 0.5788\n",
            "Epoch 144: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0044, Test L1 Norm: 0.0243, Train Linf Norm: 0.4009, Test Linf Norm: 2.8013\n",
            "Epoch 145: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0026, Train Linf Norm: 0.4782, Test Linf Norm: 0.1445\n",
            "Epoch 146: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0081, Train Linf Norm: 0.4167, Test Linf Norm: 0.9578\n",
            "Epoch 147: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0041, Train Linf Norm: 0.3978, Test Linf Norm: 0.4048\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:44:40,947]\u001b[0m Trial 115 finished with value: 0.00412633757898584 and parameters: {'n_layers': 3, 'n_units_0': 964, 'n_units_1': 931, 'n_units_2': 879, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.0005007081420484392, 'batch_size': 151, 'n_epochs': 148, 'scheduler': 'CosineAnnealingLR', 'T_max': 9}. Best is trial 61 with value: 0.002592882362008095.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 148: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0041, Train Linf Norm: 0.3748, Test Linf Norm: 0.4134\n",
            "Epoch 1: Train Loss: 0.0382, Test Loss: 0.0005, Train L1 Norm: 0.1609, Test L1 Norm: 0.0351, Train Linf Norm: 12.8281, Test Linf Norm: 3.0693\n",
            "Epoch 2: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0417, Test L1 Norm: 0.0418, Train Linf Norm: 4.1563, Test Linf Norm: 4.8201\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:44:46,655]\u001b[0m Trial 116 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0262, Test L1 Norm: 0.0298, Train Linf Norm: 2.4488, Test Linf Norm: 3.4378\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:44:48,549]\u001b[0m Trial 117 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.0404, Test Loss: 0.0002, Train L1 Norm: 0.1345, Test L1 Norm: 0.0572, Train Linf Norm: 9.9280, Test Linf Norm: 6.8866\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:44:52,900]\u001b[0m Trial 118 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Train Loss: 0.0004, Test Loss: 0.0002, Train L1 Norm: 0.0266, Test L1 Norm: 0.0465, Train Linf Norm: 2.0643, Test Linf Norm: 5.5384\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:44:55,371]\u001b[0m Trial 119 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0382, Test Loss: 0.0033, Train L1 Norm: 0.1355, Test L1 Norm: 0.1981, Train Linf Norm: 8.4767, Test Linf Norm: 22.2109\n",
            "Epoch 1: Train Loss: 0.0494, Test Loss: 0.0003, Train L1 Norm: 0.1996, Test L1 Norm: 0.0629, Train Linf Norm: 18.6846, Test Linf Norm: 8.0692\n",
            "Epoch 2: Train Loss: 0.0003, Test Loss: 0.0007, Train L1 Norm: 0.0305, Test L1 Norm: 0.0241, Train Linf Norm: 2.8596, Test Linf Norm: 0.6808\n",
            "Epoch 3: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0176, Test L1 Norm: 0.0152, Train Linf Norm: 1.6695, Test Linf Norm: 1.5283\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0135, Test L1 Norm: 0.0159, Train Linf Norm: 1.2560, Test Linf Norm: 1.7624\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0114, Test L1 Norm: 0.0080, Train Linf Norm: 1.0262, Test Linf Norm: 0.5818\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0103, Test L1 Norm: 0.0122, Train Linf Norm: 0.9078, Test Linf Norm: 1.2906\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0102, Test L1 Norm: 0.0099, Train Linf Norm: 0.9236, Test Linf Norm: 0.9201\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0098, Test L1 Norm: 0.0101, Train Linf Norm: 0.8375, Test Linf Norm: 0.9558\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0098, Test L1 Norm: 0.0101, Train Linf Norm: 0.8634, Test Linf Norm: 0.9558\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0098, Test L1 Norm: 0.0104, Train Linf Norm: 0.8591, Test Linf Norm: 0.9997\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0097, Test L1 Norm: 0.0102, Train Linf Norm: 0.8480, Test Linf Norm: 0.9729\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0096, Train Linf Norm: 0.8247, Test Linf Norm: 0.9113\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0093, Test L1 Norm: 0.0106, Train Linf Norm: 0.7988, Test Linf Norm: 1.0962\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0090, Test L1 Norm: 0.0079, Train Linf Norm: 0.8106, Test Linf Norm: 0.7011\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0101, Test L1 Norm: 0.0095, Train Linf Norm: 0.9567, Test Linf Norm: 0.9996\n",
            "Epoch 16: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0086, Train Linf Norm: 0.7951, Test Linf Norm: 0.8364\n",
            "Epoch 17: Train Loss: 0.0003, Test Loss: 0.0000, Train L1 Norm: 0.0137, Test L1 Norm: 0.0106, Train Linf Norm: 1.1034, Test Linf Norm: 1.2212\n",
            "Epoch 18: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0102, Train Linf Norm: 0.7811, Test Linf Norm: 1.1291\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0083, Train Linf Norm: 0.5757, Test Linf Norm: 0.8738\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0063, Train Linf Norm: 0.4935, Test Linf Norm: 0.6066\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0047, Train Linf Norm: 0.5007, Test Linf Norm: 0.3161\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0069, Train Linf Norm: 0.4462, Test Linf Norm: 0.7230\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0061, Train Linf Norm: 0.4805, Test Linf Norm: 0.5686\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0060, Train Linf Norm: 0.4695, Test Linf Norm: 0.5685\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0060, Train Linf Norm: 0.4646, Test Linf Norm: 0.5685\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0060, Train Linf Norm: 0.4674, Test Linf Norm: 0.5551\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0055, Train Linf Norm: 0.4564, Test Linf Norm: 0.4911\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0058, Train Linf Norm: 0.4594, Test Linf Norm: 0.5300\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0056, Train Linf Norm: 0.4348, Test Linf Norm: 0.5093\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0045, Train Linf Norm: 0.4491, Test Linf Norm: 0.2814\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0060, Train Linf Norm: 0.4375, Test Linf Norm: 0.5870\n",
            "Epoch 32: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0071, Test L1 Norm: 0.0067, Train Linf Norm: 0.4963, Test Linf Norm: 0.3124\n",
            "Epoch 33: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0083, Train Linf Norm: 0.6036, Test Linf Norm: 0.9512\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0038, Train Linf Norm: 0.4942, Test Linf Norm: 0.2563\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0040, Train Linf Norm: 0.3974, Test Linf Norm: 0.2991\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0036, Train Linf Norm: 0.3624, Test Linf Norm: 0.2407\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0036, Train Linf Norm: 0.3702, Test Linf Norm: 0.2379\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0042, Train Linf Norm: 0.3499, Test Linf Norm: 0.3422\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0036, Train Linf Norm: 0.3457, Test Linf Norm: 0.2419\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0036, Train Linf Norm: 0.3402, Test Linf Norm: 0.2429\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0036, Train Linf Norm: 0.3537, Test Linf Norm: 0.2429\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0036, Train Linf Norm: 0.3480, Test Linf Norm: 0.2431\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0035, Train Linf Norm: 0.3424, Test Linf Norm: 0.2362\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0036, Train Linf Norm: 0.3358, Test Linf Norm: 0.2608\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0035, Train Linf Norm: 0.3446, Test Linf Norm: 0.2410\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0048, Train Linf Norm: 0.3500, Test Linf Norm: 0.4486\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0042, Train Linf Norm: 0.3624, Test Linf Norm: 0.2330\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0038, Train Linf Norm: 0.4413, Test Linf Norm: 0.2863\n",
            "Epoch 49: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0034, Train Linf Norm: 0.5360, Test Linf Norm: 0.2095\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0045, Train Linf Norm: 0.3707, Test Linf Norm: 0.3676\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0033, Train Linf Norm: 0.3456, Test Linf Norm: 0.2058\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0033, Train Linf Norm: 0.3290, Test Linf Norm: 0.2097\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0032, Train Linf Norm: 0.3255, Test Linf Norm: 0.2128\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0031, Train Linf Norm: 0.3027, Test Linf Norm: 0.1902\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0032, Train Linf Norm: 0.3059, Test Linf Norm: 0.1959\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0032, Train Linf Norm: 0.3197, Test Linf Norm: 0.2049\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0032, Train Linf Norm: 0.3204, Test Linf Norm: 0.2049\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0032, Train Linf Norm: 0.3129, Test Linf Norm: 0.2058\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0032, Train Linf Norm: 0.3224, Test Linf Norm: 0.2068\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0031, Train Linf Norm: 0.3209, Test Linf Norm: 0.1990\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0031, Train Linf Norm: 0.3196, Test Linf Norm: 0.2041\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0031, Train Linf Norm: 0.3118, Test Linf Norm: 0.2013\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0031, Train Linf Norm: 0.2894, Test Linf Norm: 0.1903\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0039, Train Linf Norm: 0.6469, Test Linf Norm: 0.3230\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0042, Train Linf Norm: 0.4604, Test Linf Norm: 0.3817\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0095, Train Linf Norm: 0.3013, Test Linf Norm: 1.0394\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0030, Train Linf Norm: 0.3101, Test Linf Norm: 0.2017\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0029, Train Linf Norm: 0.2942, Test Linf Norm: 0.1846\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0029, Train Linf Norm: 0.3047, Test Linf Norm: 0.1916\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0029, Train Linf Norm: 0.2995, Test Linf Norm: 0.1924\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0028, Train Linf Norm: 0.3066, Test Linf Norm: 0.1820\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0029, Train Linf Norm: 0.2956, Test Linf Norm: 0.1887\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0029, Train Linf Norm: 0.3030, Test Linf Norm: 0.1887\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0029, Train Linf Norm: 0.2986, Test Linf Norm: 0.1863\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0028, Train Linf Norm: 0.2930, Test Linf Norm: 0.1846\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0029, Train Linf Norm: 0.3019, Test Linf Norm: 0.1876\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0028, Train Linf Norm: 0.3106, Test Linf Norm: 0.1724\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0029, Train Linf Norm: 0.3038, Test Linf Norm: 0.1937\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0028, Train Linf Norm: 0.3003, Test Linf Norm: 0.1831\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0004, Train L1 Norm: 0.0046, Test L1 Norm: 0.0415, Train Linf Norm: 0.3916, Test Linf Norm: 5.0192\n",
            "Epoch 81: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0039, Train Linf Norm: 0.5154, Test Linf Norm: 0.3474\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0070, Train Linf Norm: 0.3399, Test Linf Norm: 0.7662\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0029, Train Linf Norm: 0.2971, Test Linf Norm: 0.1916\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0029, Train Linf Norm: 0.2926, Test Linf Norm: 0.2102\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0027, Train Linf Norm: 0.2842, Test Linf Norm: 0.1814\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0027, Train Linf Norm: 0.2912, Test Linf Norm: 0.1814\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0027, Train Linf Norm: 0.2848, Test Linf Norm: 0.1774\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0027, Train Linf Norm: 0.2838, Test Linf Norm: 0.1806\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0027, Train Linf Norm: 0.2828, Test Linf Norm: 0.1806\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0027, Train Linf Norm: 0.2821, Test Linf Norm: 0.1751\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0027, Train Linf Norm: 0.2814, Test Linf Norm: 0.1822\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0029, Train Linf Norm: 0.2879, Test Linf Norm: 0.2088\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0028, Train Linf Norm: 0.2677, Test Linf Norm: 0.1930\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0027, Train Linf Norm: 0.2865, Test Linf Norm: 0.1675\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0027, Train Linf Norm: 0.2951, Test Linf Norm: 0.1793\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0026, Train Linf Norm: 0.2861, Test Linf Norm: 0.1721\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0084, Train Linf Norm: 0.3493, Test Linf Norm: 0.8923\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0027, Train Linf Norm: 0.2686, Test Linf Norm: 0.1824\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0027, Train Linf Norm: 0.2855, Test Linf Norm: 0.1949\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0026, Train Linf Norm: 0.2881, Test Linf Norm: 0.1727\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0026, Train Linf Norm: 0.2803, Test Linf Norm: 0.1647\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0026, Train Linf Norm: 0.2732, Test Linf Norm: 0.1683\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0026, Train Linf Norm: 0.2801, Test Linf Norm: 0.1734\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0026, Train Linf Norm: 0.2773, Test Linf Norm: 0.1670\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0026, Train Linf Norm: 0.2791, Test Linf Norm: 0.1670\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0026, Train Linf Norm: 0.2747, Test Linf Norm: 0.1694\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0025, Train Linf Norm: 0.2670, Test Linf Norm: 0.1641\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0026, Train Linf Norm: 0.2732, Test Linf Norm: 0.1756\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0026, Train Linf Norm: 0.2836, Test Linf Norm: 0.1698\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0026, Train Linf Norm: 0.2825, Test Linf Norm: 0.1710\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0027, Train Linf Norm: 0.3025, Test Linf Norm: 0.1800\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0026, Train Linf Norm: 0.3084, Test Linf Norm: 0.1731\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0056, Train Linf Norm: 0.2875, Test Linf Norm: 0.5699\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0025, Train Linf Norm: 0.2654, Test Linf Norm: 0.1627\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0026, Train Linf Norm: 0.2611, Test Linf Norm: 0.1829\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0024, Train Linf Norm: 0.2625, Test Linf Norm: 0.1596\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0024, Train Linf Norm: 0.2492, Test Linf Norm: 0.1556\n",
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0025, Train Linf Norm: 0.2606, Test Linf Norm: 0.1688\n",
            "Epoch 119: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0025, Train Linf Norm: 0.2572, Test Linf Norm: 0.1693\n",
            "Epoch 120: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0025, Train Linf Norm: 0.2599, Test Linf Norm: 0.1623\n",
            "Epoch 121: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0025, Train Linf Norm: 0.2589, Test Linf Norm: 0.1623\n",
            "Epoch 122: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0024, Train Linf Norm: 0.2615, Test Linf Norm: 0.1611\n",
            "Epoch 123: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0024, Train Linf Norm: 0.2565, Test Linf Norm: 0.1582\n",
            "Epoch 124: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0025, Train Linf Norm: 0.2526, Test Linf Norm: 0.1623\n",
            "Epoch 125: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0025, Train Linf Norm: 0.2582, Test Linf Norm: 0.1718\n",
            "Epoch 126: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0024, Train Linf Norm: 0.2538, Test Linf Norm: 0.1544\n",
            "Epoch 127: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0024, Train Linf Norm: 0.2770, Test Linf Norm: 0.1599\n",
            "Epoch 128: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0028, Train Linf Norm: 0.2794, Test Linf Norm: 0.2134\n",
            "Epoch 129: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0026, Train Linf Norm: 0.5057, Test Linf Norm: 0.1748\n",
            "Epoch 130: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0025, Train Linf Norm: 0.2645, Test Linf Norm: 0.1628\n",
            "Epoch 131: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0024, Train Linf Norm: 0.2660, Test Linf Norm: 0.1618\n",
            "Epoch 132: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0025, Train Linf Norm: 0.2521, Test Linf Norm: 0.1701\n",
            "Epoch 133: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0024, Train Linf Norm: 0.2543, Test Linf Norm: 0.1659\n",
            "Epoch 134: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0024, Train Linf Norm: 0.2447, Test Linf Norm: 0.1588\n",
            "Epoch 135: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0023, Train Linf Norm: 0.2525, Test Linf Norm: 0.1560\n",
            "Epoch 136: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0024, Train Linf Norm: 0.2512, Test Linf Norm: 0.1626\n",
            "Epoch 137: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0024, Train Linf Norm: 0.2530, Test Linf Norm: 0.1626\n",
            "Epoch 138: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0024, Train Linf Norm: 0.2490, Test Linf Norm: 0.1594\n",
            "Epoch 139: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0024, Train Linf Norm: 0.2455, Test Linf Norm: 0.1659\n",
            "Epoch 140: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0023, Train Linf Norm: 0.2429, Test Linf Norm: 0.1548\n",
            "Epoch 141: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0025, Train Linf Norm: 0.2429, Test Linf Norm: 0.1684\n",
            "Epoch 142: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0025, Train Linf Norm: 0.2485, Test Linf Norm: 0.1670\n",
            "Epoch 143: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0024, Train Linf Norm: 0.2500, Test Linf Norm: 0.1708\n",
            "Epoch 144: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0024, Train Linf Norm: 0.2644, Test Linf Norm: 0.1593\n",
            "Epoch 145: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0025, Train Linf Norm: 0.3034, Test Linf Norm: 0.1689\n",
            "Epoch 146: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0026, Train Linf Norm: 0.2626, Test Linf Norm: 0.1900\n",
            "Epoch 147: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0023, Train Linf Norm: 0.2530, Test Linf Norm: 0.1569\n",
            "Epoch 148: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0023, Train Linf Norm: 0.2520, Test Linf Norm: 0.1613\n",
            "Epoch 149: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0029, Test L1 Norm: 0.0023, Train Linf Norm: 0.2405, Test Linf Norm: 0.1538\n",
            "Epoch 150: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0029, Test L1 Norm: 0.0024, Train Linf Norm: 0.2418, Test Linf Norm: 0.1735\n",
            "Epoch 151: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0029, Test L1 Norm: 0.0023, Train Linf Norm: 0.2336, Test Linf Norm: 0.1644\n",
            "Epoch 152: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0029, Test L1 Norm: 0.0023, Train Linf Norm: 0.2380, Test Linf Norm: 0.1637\n",
            "Epoch 153: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0029, Test L1 Norm: 0.0023, Train Linf Norm: 0.2353, Test Linf Norm: 0.1637\n",
            "Epoch 154: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0029, Test L1 Norm: 0.0023, Train Linf Norm: 0.2344, Test Linf Norm: 0.1636\n",
            "Epoch 155: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0029, Test L1 Norm: 0.0024, Train Linf Norm: 0.2380, Test Linf Norm: 0.1698\n",
            "Epoch 156: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0024, Train Linf Norm: 0.2450, Test Linf Norm: 0.1686\n",
            "Epoch 157: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0029, Test L1 Norm: 0.0023, Train Linf Norm: 0.2421, Test Linf Norm: 0.1528\n",
            "Epoch 158: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0022, Train Linf Norm: 0.2445, Test Linf Norm: 0.1471\n",
            "Epoch 159: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0023, Train Linf Norm: 0.2451, Test Linf Norm: 0.1609\n",
            "Epoch 160: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0024, Train Linf Norm: 0.2663, Test Linf Norm: 0.1527\n",
            "Epoch 161: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0039, Train Linf Norm: 0.2488, Test Linf Norm: 0.2151\n",
            "Epoch 162: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0024, Train Linf Norm: 0.2558, Test Linf Norm: 0.1651\n",
            "Epoch 163: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0023, Train Linf Norm: 0.2464, Test Linf Norm: 0.1546\n",
            "Epoch 164: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0029, Test L1 Norm: 0.0026, Train Linf Norm: 0.2410, Test Linf Norm: 0.1825\n",
            "Epoch 165: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0029, Test L1 Norm: 0.0023, Train Linf Norm: 0.2410, Test Linf Norm: 0.1578\n",
            "Epoch 166: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0029, Test L1 Norm: 0.0022, Train Linf Norm: 0.2378, Test Linf Norm: 0.1561\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:50:18,193]\u001b[0m Trial 120 finished with value: 0.0022177403999259697 and parameters: {'n_layers': 3, 'n_units_0': 1044, 'n_units_1': 914, 'n_units_2': 840, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.0004971113311461239, 'batch_size': 164, 'n_epochs': 167, 'scheduler': 'CosineAnnealingLR', 'T_max': 8}. Best is trial 120 with value: 0.0022177403999259697.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 167: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0028, Test L1 Norm: 0.0022, Train Linf Norm: 0.2327, Test Linf Norm: 0.1519\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:50:20,697]\u001b[0m Trial 121 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0460, Test Loss: 0.0008, Train L1 Norm: 0.1579, Test L1 Norm: 0.1157, Train Linf Norm: 13.2427, Test Linf Norm: 15.6190\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:50:22,576]\u001b[0m Trial 122 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0468, Test Loss: 0.0003, Train L1 Norm: 0.1868, Test L1 Norm: 0.1057, Train Linf Norm: 17.6090, Test Linf Norm: 15.1528\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:50:24,468]\u001b[0m Trial 123 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.0394, Test Loss: 0.0002, Train L1 Norm: 0.1244, Test L1 Norm: 0.0181, Train Linf Norm: 8.6971, Test Linf Norm: 0.8560\n",
            "Epoch 2: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0252, Test L1 Norm: 0.0127, Train Linf Norm: 2.1365, Test Linf Norm: 0.7731\n",
            "Epoch 3: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0197, Test L1 Norm: 0.0104, Train Linf Norm: 1.8564, Test Linf Norm: 0.6456\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0162, Test L1 Norm: 0.0093, Train Linf Norm: 1.5150, Test Linf Norm: 0.5879\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0149, Test L1 Norm: 0.0084, Train Linf Norm: 1.3768, Test Linf Norm: 0.4820\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0143, Test L1 Norm: 0.0083, Train Linf Norm: 1.3351, Test Linf Norm: 0.4976\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0140, Test L1 Norm: 0.0083, Train Linf Norm: 1.3089, Test Linf Norm: 0.4952\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0139, Test L1 Norm: 0.0083, Train Linf Norm: 1.3015, Test Linf Norm: 0.4952\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0139, Test L1 Norm: 0.0083, Train Linf Norm: 1.3137, Test Linf Norm: 0.5082\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0136, Test L1 Norm: 0.0082, Train Linf Norm: 1.2645, Test Linf Norm: 0.5212\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0131, Test L1 Norm: 0.0083, Train Linf Norm: 1.2203, Test Linf Norm: 0.5321\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0126, Test L1 Norm: 0.0082, Train Linf Norm: 1.1863, Test Linf Norm: 0.5473\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0126, Test L1 Norm: 0.0072, Train Linf Norm: 1.1962, Test Linf Norm: 0.4423\n",
            "Epoch 14: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0146, Test L1 Norm: 0.0068, Train Linf Norm: 1.3641, Test Linf Norm: 0.4282\n",
            "Epoch 15: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0144, Test L1 Norm: 0.0078, Train Linf Norm: 1.1806, Test Linf Norm: 0.5152\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0111, Test L1 Norm: 0.0183, Train Linf Norm: 1.0500, Test Linf Norm: 1.8824\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0102, Test L1 Norm: 0.0062, Train Linf Norm: 0.9693, Test Linf Norm: 0.3979\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0095, Test L1 Norm: 0.0061, Train Linf Norm: 0.8963, Test Linf Norm: 0.3918\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0091, Test L1 Norm: 0.0058, Train Linf Norm: 0.8431, Test Linf Norm: 0.3673\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0090, Test L1 Norm: 0.0057, Train Linf Norm: 0.8487, Test Linf Norm: 0.3642\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0090, Test L1 Norm: 0.0057, Train Linf Norm: 0.8539, Test Linf Norm: 0.3647\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0089, Test L1 Norm: 0.0057, Train Linf Norm: 0.8417, Test Linf Norm: 0.3647\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0090, Test L1 Norm: 0.0057, Train Linf Norm: 0.8252, Test Linf Norm: 0.3621\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0089, Test L1 Norm: 0.0058, Train Linf Norm: 0.8422, Test Linf Norm: 0.3692\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0089, Test L1 Norm: 0.0056, Train Linf Norm: 0.8467, Test Linf Norm: 0.3481\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0088, Test L1 Norm: 0.0054, Train Linf Norm: 0.8241, Test Linf Norm: 0.3321\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0084, Test L1 Norm: 0.0063, Train Linf Norm: 0.7787, Test Linf Norm: 0.4726\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0101, Test L1 Norm: 0.0051, Train Linf Norm: 0.9611, Test Linf Norm: 0.3156\n",
            "Epoch 29: Train Loss: 0.0001, Test Loss: 0.0011, Train L1 Norm: 0.0139, Test L1 Norm: 0.0202, Train Linf Norm: 1.3899, Test Linf Norm: 0.4973\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0092, Test L1 Norm: 0.0257, Train Linf Norm: 0.8483, Test Linf Norm: 2.8273\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0049, Train Linf Norm: 0.7321, Test Linf Norm: 0.3106\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0054, Train Linf Norm: 0.6364, Test Linf Norm: 0.3833\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0068, Train Linf Norm: 0.6516, Test Linf Norm: 0.5945\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0053, Train Linf Norm: 0.6458, Test Linf Norm: 0.3870\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0060, Train Linf Norm: 0.6335, Test Linf Norm: 0.4850\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0060, Train Linf Norm: 0.6347, Test Linf Norm: 0.4850\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0060, Train Linf Norm: 0.6331, Test Linf Norm: 0.4858\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0061, Train Linf Norm: 0.6431, Test Linf Norm: 0.5005\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0062, Train Linf Norm: 0.6356, Test Linf Norm: 0.5234\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0046, Train Linf Norm: 0.6236, Test Linf Norm: 0.2755\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0074, Train Linf Norm: 0.6523, Test Linf Norm: 0.6903\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0091, Train Linf Norm: 0.6199, Test Linf Norm: 0.9525\n",
            "Epoch 43: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0086, Test L1 Norm: 0.0052, Train Linf Norm: 0.6636, Test Linf Norm: 0.2575\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0071, Train Linf Norm: 0.4950, Test Linf Norm: 0.6856\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0077, Train Linf Norm: 0.5875, Test Linf Norm: 0.7677\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0053, Train Linf Norm: 0.5531, Test Linf Norm: 0.4350\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0070, Train Linf Norm: 0.5308, Test Linf Norm: 0.6914\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0066, Train Linf Norm: 0.5222, Test Linf Norm: 0.6204\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0066, Train Linf Norm: 0.5029, Test Linf Norm: 0.6341\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0066, Train Linf Norm: 0.5285, Test Linf Norm: 0.6341\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0064, Train Linf Norm: 0.5107, Test Linf Norm: 0.6019\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0057, Train Linf Norm: 0.5098, Test Linf Norm: 0.5045\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0058, Train Linf Norm: 0.5262, Test Linf Norm: 0.5151\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0055, Train Linf Norm: 0.5059, Test Linf Norm: 0.4712\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0084, Train Linf Norm: 0.4885, Test Linf Norm: 0.8836\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0065, Train Linf Norm: 0.5290, Test Linf Norm: 0.6278\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0073, Test L1 Norm: 0.0048, Train Linf Norm: 0.5838, Test Linf Norm: 0.3725\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0071, Train Linf Norm: 0.4717, Test Linf Norm: 0.7290\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0062, Train Linf Norm: 0.4889, Test Linf Norm: 0.6026\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0069, Train Linf Norm: 0.4349, Test Linf Norm: 0.6975\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0053, Train Linf Norm: 0.4580, Test Linf Norm: 0.4824\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0062, Train Linf Norm: 0.4680, Test Linf Norm: 0.6196\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0061, Train Linf Norm: 0.4610, Test Linf Norm: 0.5939\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0061, Train Linf Norm: 0.4489, Test Linf Norm: 0.5939\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0061, Train Linf Norm: 0.4531, Test Linf Norm: 0.5935\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0057, Train Linf Norm: 0.4420, Test Linf Norm: 0.5397\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0063, Train Linf Norm: 0.4757, Test Linf Norm: 0.6393\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0051, Train Linf Norm: 0.4793, Test Linf Norm: 0.4429\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0061, Train Linf Norm: 0.4648, Test Linf Norm: 0.5957\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0059, Train Linf Norm: 0.4888, Test Linf Norm: 0.5707\n",
            "Epoch 71: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0079, Test L1 Norm: 0.0074, Train Linf Norm: 0.6101, Test Linf Norm: 0.3644\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0057, Train Linf Norm: 0.5186, Test Linf Norm: 0.5616\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0060, Train Linf Norm: 0.4479, Test Linf Norm: 0.5841\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0053, Train Linf Norm: 0.4387, Test Linf Norm: 0.4965\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0056, Train Linf Norm: 0.4430, Test Linf Norm: 0.5437\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0058, Train Linf Norm: 0.4162, Test Linf Norm: 0.5609\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0058, Train Linf Norm: 0.4263, Test Linf Norm: 0.5861\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0058, Train Linf Norm: 0.4354, Test Linf Norm: 0.5861\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0059, Train Linf Norm: 0.3830, Test Linf Norm: 0.5894\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0058, Train Linf Norm: 0.4346, Test Linf Norm: 0.5637\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0054, Train Linf Norm: 0.4404, Test Linf Norm: 0.5180\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0049, Train Linf Norm: 0.4435, Test Linf Norm: 0.4123\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0053, Train Linf Norm: 0.4389, Test Linf Norm: 0.5089\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0040, Train Linf Norm: 0.5044, Test Linf Norm: 0.2642\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0041, Train Linf Norm: 0.5358, Test Linf Norm: 0.2024\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0040, Train Linf Norm: 0.4272, Test Linf Norm: 0.2726\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0054, Train Linf Norm: 0.4683, Test Linf Norm: 0.5303\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0039, Train Linf Norm: 0.4289, Test Linf Norm: 0.3112\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0057, Train Linf Norm: 0.4129, Test Linf Norm: 0.5664\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0054, Train Linf Norm: 0.4215, Test Linf Norm: 0.5169\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0052, Train Linf Norm: 0.4089, Test Linf Norm: 0.5000\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0052, Train Linf Norm: 0.4082, Test Linf Norm: 0.5000\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0053, Train Linf Norm: 0.4124, Test Linf Norm: 0.5122\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0055, Train Linf Norm: 0.4196, Test Linf Norm: 0.5374\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0056, Train Linf Norm: 0.4070, Test Linf Norm: 0.5431\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0044, Train Linf Norm: 0.4070, Test Linf Norm: 0.3638\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0048, Train Linf Norm: 0.4199, Test Linf Norm: 0.4595\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0067, Train Linf Norm: 0.4203, Test Linf Norm: 0.6968\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0047, Train Linf Norm: 0.3869, Test Linf Norm: 0.4249\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0058, Train Linf Norm: 0.4547, Test Linf Norm: 0.5844\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0052, Train Linf Norm: 0.4294, Test Linf Norm: 0.4928\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0058, Train Linf Norm: 0.3814, Test Linf Norm: 0.5908\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0055, Train Linf Norm: 0.3957, Test Linf Norm: 0.5486\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0050, Train Linf Norm: 0.4001, Test Linf Norm: 0.4902\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0050, Train Linf Norm: 0.3968, Test Linf Norm: 0.4806\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0050, Train Linf Norm: 0.3924, Test Linf Norm: 0.4806\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0052, Train Linf Norm: 0.3958, Test Linf Norm: 0.5065\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0052, Train Linf Norm: 0.3969, Test Linf Norm: 0.5037\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0041, Train Linf Norm: 0.3911, Test Linf Norm: 0.3556\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0055, Train Linf Norm: 0.3981, Test Linf Norm: 0.5566\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0071, Train Linf Norm: 0.4313, Test Linf Norm: 0.7428\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0055, Train Linf Norm: 0.3980, Test Linf Norm: 0.5692\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0058, Train Linf Norm: 0.3930, Test Linf Norm: 0.5881\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0036, Train Linf Norm: 0.3947, Test Linf Norm: 0.2746\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0038, Train Linf Norm: 0.3599, Test Linf Norm: 0.2995\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0051, Train Linf Norm: 0.3885, Test Linf Norm: 0.4982\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0042, Train Linf Norm: 0.3668, Test Linf Norm: 0.3772\n",
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0041, Train Linf Norm: 0.3754, Test Linf Norm: 0.3605\n",
            "Epoch 119: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0046, Train Linf Norm: 0.3757, Test Linf Norm: 0.4336\n",
            "Epoch 120: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0046, Train Linf Norm: 0.3733, Test Linf Norm: 0.4336\n",
            "Epoch 121: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0046, Train Linf Norm: 0.3661, Test Linf Norm: 0.4430\n",
            "Epoch 122: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0044, Train Linf Norm: 0.3791, Test Linf Norm: 0.4100\n",
            "Epoch 123: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0047, Train Linf Norm: 0.3745, Test Linf Norm: 0.4489\n",
            "Epoch 124: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0037, Train Linf Norm: 0.3772, Test Linf Norm: 0.2934\n",
            "Epoch 125: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0040, Train Linf Norm: 0.3973, Test Linf Norm: 0.3401\n",
            "Epoch 126: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0037, Train Linf Norm: 0.4003, Test Linf Norm: 0.2753\n",
            "Epoch 127: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0108, Train Linf Norm: 0.6644, Test Linf Norm: 1.1754\n",
            "Epoch 128: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0047, Train Linf Norm: 0.3873, Test Linf Norm: 0.4366\n",
            "Epoch 129: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0049, Train Linf Norm: 0.3566, Test Linf Norm: 0.4730\n",
            "Epoch 130: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0042, Train Linf Norm: 0.3357, Test Linf Norm: 0.3781\n",
            "Epoch 131: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0053, Train Linf Norm: 0.3321, Test Linf Norm: 0.5223\n",
            "Epoch 132: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0046, Train Linf Norm: 0.3532, Test Linf Norm: 0.4430\n",
            "Epoch 133: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0043, Train Linf Norm: 0.3540, Test Linf Norm: 0.4029\n",
            "Epoch 134: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0043, Train Linf Norm: 0.3517, Test Linf Norm: 0.4029\n",
            "Epoch 135: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0043, Train Linf Norm: 0.3466, Test Linf Norm: 0.4004\n",
            "Epoch 136: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0040, Train Linf Norm: 0.3566, Test Linf Norm: 0.3682\n",
            "Epoch 137: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0045, Train Linf Norm: 0.3530, Test Linf Norm: 0.4115\n",
            "Epoch 138: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0053, Train Linf Norm: 0.3411, Test Linf Norm: 0.5473\n",
            "Epoch 139: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0055, Train Linf Norm: 0.3478, Test Linf Norm: 0.5783\n",
            "Epoch 140: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0040, Train Linf Norm: 0.3808, Test Linf Norm: 0.2760\n",
            "Epoch 141: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0037, Train Linf Norm: 0.3738, Test Linf Norm: 0.2802\n",
            "Epoch 142: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0053, Train Linf Norm: 0.3637, Test Linf Norm: 0.5353\n",
            "Epoch 143: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0043, Train Linf Norm: 0.3602, Test Linf Norm: 0.4116\n",
            "Epoch 144: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0042, Train Linf Norm: 0.3479, Test Linf Norm: 0.3870\n",
            "Epoch 145: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0041, Train Linf Norm: 0.3188, Test Linf Norm: 0.3752\n",
            "Epoch 146: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0044, Train Linf Norm: 0.3341, Test Linf Norm: 0.4258\n",
            "Epoch 147: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0044, Train Linf Norm: 0.3360, Test Linf Norm: 0.4235\n",
            "Epoch 148: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0044, Train Linf Norm: 0.3403, Test Linf Norm: 0.4235\n",
            "Epoch 149: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0043, Train Linf Norm: 0.3379, Test Linf Norm: 0.4112\n",
            "Epoch 150: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0042, Train Linf Norm: 0.3383, Test Linf Norm: 0.3939\n",
            "Epoch 151: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0047, Train Linf Norm: 0.3515, Test Linf Norm: 0.4661\n",
            "Epoch 152: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0042, Train Linf Norm: 0.3471, Test Linf Norm: 0.4023\n",
            "Epoch 153: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0059, Train Linf Norm: 0.3393, Test Linf Norm: 0.6358\n",
            "Epoch 154: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0039, Train Linf Norm: 0.3515, Test Linf Norm: 0.3595\n",
            "Epoch 155: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0049, Train Linf Norm: 0.3711, Test Linf Norm: 0.4898\n",
            "Epoch 156: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0047, Train Linf Norm: 0.3333, Test Linf Norm: 0.4698\n",
            "Epoch 157: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0039, Train Linf Norm: 0.3418, Test Linf Norm: 0.3390\n",
            "Epoch 158: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0037, Train Linf Norm: 0.3369, Test Linf Norm: 0.3284\n",
            "Epoch 159: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0041, Train Linf Norm: 0.3316, Test Linf Norm: 0.3847\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:55:57,013]\u001b[0m Trial 124 finished with value: 0.004318130042590201 and parameters: {'n_layers': 3, 'n_units_0': 885, 'n_units_1': 1046, 'n_units_2': 766, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.00036050786529351765, 'batch_size': 148, 'n_epochs': 160, 'scheduler': 'CosineAnnealingLR', 'T_max': 7}. Best is trial 120 with value: 0.0022177403999259697.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 160: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0043, Train Linf Norm: 0.3226, Test Linf Norm: 0.4139\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 14:55:59,672]\u001b[0m Trial 125 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0462, Test Loss: 0.0006, Train L1 Norm: 0.1741, Test L1 Norm: 0.1014, Train Linf Norm: 15.1308, Test Linf Norm: 13.9434\n",
            "Epoch 1: Train Loss: 0.1614, Test Loss: 0.0005, Train L1 Norm: 0.1454, Test L1 Norm: 0.0217, Train Linf Norm: 10.9483, Test Linf Norm: 1.5957\n",
            "Epoch 2: Train Loss: 0.0008, Test Loss: 0.0001, Train L1 Norm: 0.0251, Test L1 Norm: 0.0114, Train Linf Norm: 1.8472, Test Linf Norm: 0.6604\n",
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0137, Test L1 Norm: 0.0082, Train Linf Norm: 1.0103, Test Linf Norm: 0.4137\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0101, Test L1 Norm: 0.0078, Train Linf Norm: 0.6761, Test Linf Norm: 0.4581\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0093, Test L1 Norm: 0.0070, Train Linf Norm: 0.6509, Test Linf Norm: 0.3648\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0088, Test L1 Norm: 0.0065, Train Linf Norm: 0.6013, Test Linf Norm: 0.3402\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0088, Test L1 Norm: 0.0062, Train Linf Norm: 0.6148, Test Linf Norm: 0.3177\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0062, Train Linf Norm: 0.6168, Test Linf Norm: 0.3177\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0062, Train Linf Norm: 0.6157, Test Linf Norm: 0.3176\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0062, Train Linf Norm: 0.6141, Test Linf Norm: 0.3167\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0061, Train Linf Norm: 0.5998, Test Linf Norm: 0.3041\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0084, Test L1 Norm: 0.0058, Train Linf Norm: 0.5963, Test Linf Norm: 0.2939\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0053, Train Linf Norm: 0.5957, Test Linf Norm: 0.2697\n",
            "Epoch 14: Train Loss: 0.0006, Test Loss: 0.0000, Train L1 Norm: 0.0143, Test L1 Norm: 0.0094, Train Linf Norm: 1.0347, Test Linf Norm: 0.8010\n",
            "Epoch 15: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0107, Test L1 Norm: 0.0066, Train Linf Norm: 0.6819, Test Linf Norm: 0.4470\n",
            "Epoch 16: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0152, Train Linf Norm: 0.5566, Test Linf Norm: 1.6262\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0133, Train Linf Norm: 0.4186, Test Linf Norm: 1.4041\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0115, Train Linf Norm: 0.4351, Test Linf Norm: 1.2001\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0124, Train Linf Norm: 0.4056, Test Linf Norm: 1.3079\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0102, Train Linf Norm: 0.3882, Test Linf Norm: 1.0325\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0104, Train Linf Norm: 0.3859, Test Linf Norm: 1.0443\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0104, Train Linf Norm: 0.3846, Test Linf Norm: 1.0443\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0100, Train Linf Norm: 0.3828, Test Linf Norm: 0.9955\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0099, Train Linf Norm: 0.3725, Test Linf Norm: 0.9912\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0093, Train Linf Norm: 0.3772, Test Linf Norm: 0.9070\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0074, Train Linf Norm: 0.3677, Test Linf Norm: 0.6598\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0089, Train Linf Norm: 0.3790, Test Linf Norm: 0.8480\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0116, Train Linf Norm: 0.4019, Test Linf Norm: 1.2098\n",
            "Epoch 29: Train Loss: 0.0003, Test Loss: 0.0000, Train L1 Norm: 0.0125, Test L1 Norm: 0.0125, Train Linf Norm: 0.9437, Test Linf Norm: 1.3098\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0077, Train Linf Norm: 0.3139, Test Linf Norm: 0.7417\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0078, Train Linf Norm: 0.3222, Test Linf Norm: 0.7546\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0053, Train Linf Norm: 0.3372, Test Linf Norm: 0.4314\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0054, Train Linf Norm: 0.3046, Test Linf Norm: 0.4581\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0048, Train Linf Norm: 0.3180, Test Linf Norm: 0.3765\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0049, Train Linf Norm: 0.3052, Test Linf Norm: 0.3886\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0049, Train Linf Norm: 0.2933, Test Linf Norm: 0.3886\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0048, Train Linf Norm: 0.2953, Test Linf Norm: 0.3699\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0048, Train Linf Norm: 0.3002, Test Linf Norm: 0.3770\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0045, Train Linf Norm: 0.3087, Test Linf Norm: 0.3391\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0039, Train Linf Norm: 0.3134, Test Linf Norm: 0.2522\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0074, Train Linf Norm: 0.3209, Test Linf Norm: 0.6912\n",
            "Epoch 42: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0043, Train Linf Norm: 0.3447, Test Linf Norm: 0.2235\n",
            "Epoch 43: Train Loss: 0.0009, Test Loss: 0.0001, Train L1 Norm: 0.0097, Test L1 Norm: 0.0074, Train Linf Norm: 0.6178, Test Linf Norm: 0.5175\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0095, Train Linf Norm: 0.3209, Test Linf Norm: 0.9936\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0095, Train Linf Norm: 0.3378, Test Linf Norm: 0.9981\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0078, Train Linf Norm: 0.3637, Test Linf Norm: 0.7743\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0071, Train Linf Norm: 0.3499, Test Linf Norm: 0.6802\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0072, Train Linf Norm: 0.3221, Test Linf Norm: 0.7071\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0075, Train Linf Norm: 0.3303, Test Linf Norm: 0.7569\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0075, Train Linf Norm: 0.3292, Test Linf Norm: 0.7569\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0073, Train Linf Norm: 0.3302, Test Linf Norm: 0.7214\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0069, Train Linf Norm: 0.3411, Test Linf Norm: 0.6721\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0072, Train Linf Norm: 0.3233, Test Linf Norm: 0.7221\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0087, Train Linf Norm: 0.3256, Test Linf Norm: 0.9193\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0056, Train Linf Norm: 0.3252, Test Linf Norm: 0.5109\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0049, Test L1 Norm: 0.0049, Train Linf Norm: 0.3833, Test Linf Norm: 0.2522\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0057, Train Linf Norm: 0.3898, Test Linf Norm: 0.5294\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0055, Train Linf Norm: 0.2876, Test Linf Norm: 0.5003\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0044, Train Linf Norm: 0.3146, Test Linf Norm: 0.3579\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0042, Train Linf Norm: 0.2885, Test Linf Norm: 0.3382\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0036, Train Linf Norm: 0.3136, Test Linf Norm: 0.2605\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0037, Train Linf Norm: 0.2917, Test Linf Norm: 0.2704\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0037, Train Linf Norm: 0.3012, Test Linf Norm: 0.2785\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0037, Train Linf Norm: 0.3069, Test Linf Norm: 0.2785\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0037, Train Linf Norm: 0.3042, Test Linf Norm: 0.2729\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0037, Train Linf Norm: 0.3072, Test Linf Norm: 0.2759\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0036, Train Linf Norm: 0.2994, Test Linf Norm: 0.2705\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0035, Train Linf Norm: 0.3062, Test Linf Norm: 0.2526\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0037, Train Linf Norm: 0.2916, Test Linf Norm: 0.2784\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0039, Train Linf Norm: 0.3065, Test Linf Norm: 0.2971\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0043, Train Linf Norm: 0.4804, Test Linf Norm: 0.3095\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0035, Train Linf Norm: 0.3068, Test Linf Norm: 0.2568\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0035, Train Linf Norm: 0.3114, Test Linf Norm: 0.2602\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0033, Train Linf Norm: 0.3126, Test Linf Norm: 0.2240\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0035, Train Linf Norm: 0.3106, Test Linf Norm: 0.2636\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0034, Train Linf Norm: 0.3011, Test Linf Norm: 0.2464\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0034, Train Linf Norm: 0.2942, Test Linf Norm: 0.2438\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0034, Train Linf Norm: 0.2965, Test Linf Norm: 0.2438\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0034, Train Linf Norm: 0.2996, Test Linf Norm: 0.2460\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0033, Train Linf Norm: 0.3016, Test Linf Norm: 0.2388\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0033, Train Linf Norm: 0.3007, Test Linf Norm: 0.2419\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0034, Train Linf Norm: 0.2870, Test Linf Norm: 0.2544\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0035, Train Linf Norm: 0.3287, Test Linf Norm: 0.2598\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0036, Train Linf Norm: 0.2882, Test Linf Norm: 0.2789\n",
            "Epoch 85: Train Loss: 0.0003, Test Loss: 0.0000, Train L1 Norm: 0.0086, Test L1 Norm: 0.0033, Train Linf Norm: 0.7129, Test Linf Norm: 0.2258\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0032, Train Linf Norm: 0.3762, Test Linf Norm: 0.2154\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0032, Train Linf Norm: 0.3331, Test Linf Norm: 0.2258\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0031, Train Linf Norm: 0.3182, Test Linf Norm: 0.2091\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0032, Train Linf Norm: 0.3125, Test Linf Norm: 0.2261\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0032, Train Linf Norm: 0.3051, Test Linf Norm: 0.2225\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0032, Train Linf Norm: 0.3125, Test Linf Norm: 0.2243\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0032, Train Linf Norm: 0.3117, Test Linf Norm: 0.2243\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0031, Train Linf Norm: 0.3072, Test Linf Norm: 0.2223\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0031, Train Linf Norm: 0.3045, Test Linf Norm: 0.2184\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0031, Train Linf Norm: 0.3121, Test Linf Norm: 0.2150\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0030, Train Linf Norm: 0.3186, Test Linf Norm: 0.2101\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0032, Train Linf Norm: 0.2777, Test Linf Norm: 0.2249\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0034, Train Linf Norm: 0.2876, Test Linf Norm: 0.2497\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0032, Train Linf Norm: 0.3034, Test Linf Norm: 0.2286\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0034, Train Linf Norm: 0.3143, Test Linf Norm: 0.2217\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0030, Train Linf Norm: 0.2948, Test Linf Norm: 0.2112\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0032, Train Linf Norm: 0.2914, Test Linf Norm: 0.2343\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0032, Train Linf Norm: 0.2988, Test Linf Norm: 0.2266\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0030, Train Linf Norm: 0.2981, Test Linf Norm: 0.2137\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0031, Train Linf Norm: 0.2893, Test Linf Norm: 0.2247\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0031, Train Linf Norm: 0.2928, Test Linf Norm: 0.2247\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0031, Train Linf Norm: 0.2929, Test Linf Norm: 0.2206\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0030, Train Linf Norm: 0.2958, Test Linf Norm: 0.2068\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0030, Train Linf Norm: 0.2983, Test Linf Norm: 0.2126\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0031, Train Linf Norm: 0.2991, Test Linf Norm: 0.2250\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0031, Train Linf Norm: 0.3189, Test Linf Norm: 0.2311\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0035, Train Linf Norm: 0.2835, Test Linf Norm: 0.2716\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0033, Train Linf Norm: 0.3161, Test Linf Norm: 0.2475\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0033, Train Linf Norm: 0.3376, Test Linf Norm: 0.2578\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0030, Train Linf Norm: 0.2779, Test Linf Norm: 0.2108\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0030, Train Linf Norm: 0.2998, Test Linf Norm: 0.2130\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0029, Train Linf Norm: 0.2819, Test Linf Norm: 0.1998\n",
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0030, Train Linf Norm: 0.2876, Test Linf Norm: 0.2192\n",
            "Epoch 119: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0030, Train Linf Norm: 0.2858, Test Linf Norm: 0.2140\n",
            "Epoch 120: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0030, Train Linf Norm: 0.2820, Test Linf Norm: 0.2140\n",
            "Epoch 121: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0030, Train Linf Norm: 0.2838, Test Linf Norm: 0.2185\n",
            "Epoch 122: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0029, Train Linf Norm: 0.2879, Test Linf Norm: 0.2061\n",
            "Epoch 123: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0029, Train Linf Norm: 0.2900, Test Linf Norm: 0.1993\n",
            "Epoch 124: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0031, Train Linf Norm: 0.2931, Test Linf Norm: 0.2379\n",
            "Epoch 125: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0028, Train Linf Norm: 0.2964, Test Linf Norm: 0.1926\n",
            "Epoch 126: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0029, Train Linf Norm: 0.3033, Test Linf Norm: 0.1958\n",
            "Epoch 127: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0027, Train Linf Norm: 0.3125, Test Linf Norm: 0.1841\n",
            "Epoch 128: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0031, Train Linf Norm: 0.2918, Test Linf Norm: 0.1724\n",
            "Epoch 129: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0029, Train Linf Norm: 0.2926, Test Linf Norm: 0.1970\n",
            "Epoch 130: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0028, Train Linf Norm: 0.2863, Test Linf Norm: 0.2016\n",
            "Epoch 131: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0029, Train Linf Norm: 0.2843, Test Linf Norm: 0.2017\n",
            "Epoch 132: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0028, Train Linf Norm: 0.2853, Test Linf Norm: 0.2014\n",
            "Epoch 133: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0028, Train Linf Norm: 0.2810, Test Linf Norm: 0.1998\n",
            "Epoch 134: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0028, Train Linf Norm: 0.2804, Test Linf Norm: 0.1998\n",
            "Epoch 135: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0028, Train Linf Norm: 0.2760, Test Linf Norm: 0.1988\n",
            "Epoch 136: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0027, Train Linf Norm: 0.2847, Test Linf Norm: 0.1907\n",
            "Epoch 137: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0029, Train Linf Norm: 0.2784, Test Linf Norm: 0.2063\n",
            "Epoch 138: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0029, Train Linf Norm: 0.2843, Test Linf Norm: 0.2082\n",
            "Epoch 139: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0029, Train Linf Norm: 0.2858, Test Linf Norm: 0.2048\n",
            "Epoch 140: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0029, Train Linf Norm: 0.3077, Test Linf Norm: 0.1893\n",
            "Epoch 141: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0027, Train Linf Norm: 0.2897, Test Linf Norm: 0.1967\n",
            "Epoch 142: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0030, Train Linf Norm: 0.3177, Test Linf Norm: 0.2056\n",
            "Epoch 143: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0029, Train Linf Norm: 0.3037, Test Linf Norm: 0.1958\n",
            "Epoch 144: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0026, Train Linf Norm: 0.2702, Test Linf Norm: 0.1774\n",
            "Epoch 145: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0028, Train Linf Norm: 0.2867, Test Linf Norm: 0.1975\n",
            "Epoch 146: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0028, Train Linf Norm: 0.2825, Test Linf Norm: 0.2068\n",
            "Epoch 147: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0027, Train Linf Norm: 0.2737, Test Linf Norm: 0.1925\n",
            "Epoch 148: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0027, Train Linf Norm: 0.2761, Test Linf Norm: 0.1925\n",
            "Epoch 149: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0027, Train Linf Norm: 0.2791, Test Linf Norm: 0.1950\n",
            "Epoch 150: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0027, Train Linf Norm: 0.2771, Test Linf Norm: 0.1913\n",
            "Epoch 151: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0028, Train Linf Norm: 0.2848, Test Linf Norm: 0.2026\n",
            "Epoch 152: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0029, Train Linf Norm: 0.2850, Test Linf Norm: 0.2084\n",
            "Epoch 153: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0027, Train Linf Norm: 0.2691, Test Linf Norm: 0.1896\n",
            "Epoch 154: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0026, Train Linf Norm: 0.2915, Test Linf Norm: 0.1816\n",
            "Epoch 155: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0027, Train Linf Norm: 0.2956, Test Linf Norm: 0.1748\n",
            "Epoch 156: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0029, Train Linf Norm: 0.3008, Test Linf Norm: 0.2143\n",
            "Epoch 157: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0024, Train Linf Norm: 0.2832, Test Linf Norm: 0.1539\n",
            "Epoch 158: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0025, Train Linf Norm: 0.2730, Test Linf Norm: 0.1690\n",
            "Epoch 159: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0027, Train Linf Norm: 0.2803, Test Linf Norm: 0.1879\n",
            "Epoch 160: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0025, Train Linf Norm: 0.2773, Test Linf Norm: 0.1697\n",
            "Epoch 161: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0026, Train Linf Norm: 0.2713, Test Linf Norm: 0.1814\n",
            "Epoch 162: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0026, Train Linf Norm: 0.2691, Test Linf Norm: 0.1814\n",
            "Epoch 163: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0026, Train Linf Norm: 0.2702, Test Linf Norm: 0.1835\n",
            "Epoch 164: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0027, Train Linf Norm: 0.2749, Test Linf Norm: 0.1989\n",
            "Epoch 165: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0025, Train Linf Norm: 0.2699, Test Linf Norm: 0.1779\n",
            "Epoch 166: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0025, Train Linf Norm: 0.2713, Test Linf Norm: 0.1566\n",
            "Epoch 167: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0025, Train Linf Norm: 0.2801, Test Linf Norm: 0.1634\n",
            "Epoch 168: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0027, Train Linf Norm: 0.2855, Test Linf Norm: 0.1953\n",
            "Epoch 169: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0033, Train Linf Norm: 0.2888, Test Linf Norm: 0.2580\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:02:16,693]\u001b[0m Trial 126 finished with value: 0.0026709617742919365 and parameters: {'n_layers': 3, 'n_units_0': 991, 'n_units_1': 1004, 'n_units_2': 724, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'MSE', 'optimizer': 'Adagrad', 'lr': 0.0005039130941816819, 'batch_size': 133, 'n_epochs': 170, 'scheduler': 'CosineAnnealingLR', 'T_max': 7}. Best is trial 120 with value: 0.0022177403999259697.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 170: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0027, Train Linf Norm: 0.2724, Test Linf Norm: 0.1795\n",
            "Epoch 1: Train Loss: 0.0482, Test Loss: 0.0009, Train L1 Norm: 0.1523, Test L1 Norm: 0.0557, Train Linf Norm: 12.5565, Test Linf Norm: 7.8228\n",
            "Epoch 2: Train Loss: 0.0008, Test Loss: 0.0001, Train L1 Norm: 0.0404, Test L1 Norm: 0.0164, Train Linf Norm: 4.6123, Test Linf Norm: 1.0601\n",
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0229, Test L1 Norm: 0.0113, Train Linf Norm: 2.8060, Test Linf Norm: 0.7088\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0172, Test L1 Norm: 0.0093, Train Linf Norm: 2.0942, Test Linf Norm: 0.6019\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0149, Test L1 Norm: 0.0087, Train Linf Norm: 1.7118, Test Linf Norm: 0.5817\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0139, Test L1 Norm: 0.0085, Train Linf Norm: 1.5712, Test Linf Norm: 0.5328\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0135, Test L1 Norm: 0.0083, Train Linf Norm: 1.5418, Test Linf Norm: 0.5299\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0133, Test L1 Norm: 0.0083, Train Linf Norm: 1.5041, Test Linf Norm: 0.5299\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0131, Test L1 Norm: 0.0082, Train Linf Norm: 1.4818, Test Linf Norm: 0.5245\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0131, Test L1 Norm: 0.0081, Train Linf Norm: 1.5145, Test Linf Norm: 0.5414\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0128, Test L1 Norm: 0.0077, Train Linf Norm: 1.4354, Test Linf Norm: 0.5255\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0117, Test L1 Norm: 0.0073, Train Linf Norm: 1.2838, Test Linf Norm: 0.4819\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0114, Test L1 Norm: 0.0072, Train Linf Norm: 1.2317, Test Linf Norm: 0.5306\n",
            "Epoch 14: Train Loss: 0.0003, Test Loss: 0.0000, Train L1 Norm: 0.0200, Test L1 Norm: 0.0074, Train Linf Norm: 2.1767, Test Linf Norm: 0.4772\n",
            "Epoch 15: Train Loss: 0.0003, Test Loss: 0.0000, Train L1 Norm: 0.0169, Test L1 Norm: 0.0068, Train Linf Norm: 1.5564, Test Linf Norm: 0.4947\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0096, Test L1 Norm: 0.0070, Train Linf Norm: 1.0068, Test Linf Norm: 0.4981\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0064, Train Linf Norm: 0.9066, Test Linf Norm: 0.5072\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0057, Train Linf Norm: 0.8567, Test Linf Norm: 0.4053\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0055, Train Linf Norm: 0.8147, Test Linf Norm: 0.3950\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0055, Train Linf Norm: 0.7854, Test Linf Norm: 0.3959\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0054, Train Linf Norm: 0.8254, Test Linf Norm: 0.3911\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0054, Train Linf Norm: 0.8338, Test Linf Norm: 0.3911\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0054, Train Linf Norm: 0.8278, Test Linf Norm: 0.3851\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0054, Train Linf Norm: 0.8492, Test Linf Norm: 0.3901\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0053, Train Linf Norm: 0.8036, Test Linf Norm: 0.3851\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0052, Train Linf Norm: 0.7929, Test Linf Norm: 0.3795\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0073, Test L1 Norm: 0.0050, Train Linf Norm: 0.7749, Test Linf Norm: 0.3560\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0084, Test L1 Norm: 0.0051, Train Linf Norm: 0.9131, Test Linf Norm: 0.4164\n",
            "Epoch 29: Train Loss: 0.0003, Test Loss: 0.0011, Train L1 Norm: 0.0156, Test L1 Norm: 0.0201, Train Linf Norm: 1.6392, Test Linf Norm: 0.6477\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0046, Train Linf Norm: 1.1295, Test Linf Norm: 0.3340\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0045, Train Linf Norm: 0.7065, Test Linf Norm: 0.3228\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0043, Train Linf Norm: 0.7174, Test Linf Norm: 0.3174\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0042, Train Linf Norm: 0.6540, Test Linf Norm: 0.2981\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0043, Train Linf Norm: 0.6687, Test Linf Norm: 0.3095\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0042, Train Linf Norm: 0.6619, Test Linf Norm: 0.2970\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0042, Train Linf Norm: 0.6626, Test Linf Norm: 0.2970\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0043, Train Linf Norm: 0.6751, Test Linf Norm: 0.3115\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0042, Train Linf Norm: 0.6892, Test Linf Norm: 0.3007\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0041, Train Linf Norm: 0.6857, Test Linf Norm: 0.3021\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0042, Train Linf Norm: 0.6688, Test Linf Norm: 0.3288\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0040, Train Linf Norm: 0.6309, Test Linf Norm: 0.3051\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0044, Train Linf Norm: 0.6405, Test Linf Norm: 0.3274\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0089, Test L1 Norm: 0.0040, Train Linf Norm: 1.0585, Test Linf Norm: 0.3021\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0040, Train Linf Norm: 0.5878, Test Linf Norm: 0.2960\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0039, Train Linf Norm: 0.5948, Test Linf Norm: 0.2989\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0038, Train Linf Norm: 0.5865, Test Linf Norm: 0.2931\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0037, Train Linf Norm: 0.5886, Test Linf Norm: 0.2823\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0037, Train Linf Norm: 0.5703, Test Linf Norm: 0.2783\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0037, Train Linf Norm: 0.5543, Test Linf Norm: 0.2767\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0037, Train Linf Norm: 0.5572, Test Linf Norm: 0.2767\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0037, Train Linf Norm: 0.5549, Test Linf Norm: 0.2774\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0037, Train Linf Norm: 0.5468, Test Linf Norm: 0.2760\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0036, Train Linf Norm: 0.5514, Test Linf Norm: 0.2775\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0036, Train Linf Norm: 0.5488, Test Linf Norm: 0.2753\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0040, Train Linf Norm: 0.5307, Test Linf Norm: 0.3210\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0035, Train Linf Norm: 0.5749, Test Linf Norm: 0.2531\n",
            "Epoch 57: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0091, Test L1 Norm: 0.0046, Train Linf Norm: 0.8835, Test Linf Norm: 0.3145\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0034, Train Linf Norm: 0.5328, Test Linf Norm: 0.2581\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0033, Train Linf Norm: 0.5329, Test Linf Norm: 0.2416\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0033, Train Linf Norm: 0.5241, Test Linf Norm: 0.2435\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0033, Train Linf Norm: 0.5149, Test Linf Norm: 0.2342\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0033, Train Linf Norm: 0.5020, Test Linf Norm: 0.2409\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0033, Train Linf Norm: 0.4966, Test Linf Norm: 0.2425\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0033, Train Linf Norm: 0.4892, Test Linf Norm: 0.2425\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0032, Train Linf Norm: 0.4954, Test Linf Norm: 0.2423\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0032, Train Linf Norm: 0.4909, Test Linf Norm: 0.2414\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0032, Train Linf Norm: 0.5034, Test Linf Norm: 0.2399\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0032, Train Linf Norm: 0.4681, Test Linf Norm: 0.2384\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0035, Train Linf Norm: 0.4750, Test Linf Norm: 0.2799\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0031, Train Linf Norm: 0.4812, Test Linf Norm: 0.2285\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0031, Train Linf Norm: 0.5846, Test Linf Norm: 0.2296\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0036, Train Linf Norm: 0.4588, Test Linf Norm: 0.3107\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0033, Train Linf Norm: 0.4589, Test Linf Norm: 0.2449\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0032, Train Linf Norm: 0.4754, Test Linf Norm: 0.2357\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0030, Train Linf Norm: 0.4671, Test Linf Norm: 0.2313\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0029, Train Linf Norm: 0.4590, Test Linf Norm: 0.2213\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0030, Train Linf Norm: 0.4504, Test Linf Norm: 0.2213\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0030, Train Linf Norm: 0.4596, Test Linf Norm: 0.2213\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0030, Train Linf Norm: 0.4505, Test Linf Norm: 0.2198\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0030, Train Linf Norm: 0.4712, Test Linf Norm: 0.2183\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0028, Train Linf Norm: 0.4612, Test Linf Norm: 0.2152\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0030, Train Linf Norm: 0.4567, Test Linf Norm: 0.2192\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0032, Train Linf Norm: 0.4810, Test Linf Norm: 0.2383\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0031, Train Linf Norm: 0.5146, Test Linf Norm: 0.2256\n",
            "Epoch 85: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0037, Train Linf Norm: 0.5299, Test Linf Norm: 0.3016\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0030, Train Linf Norm: 0.5041, Test Linf Norm: 0.2210\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0028, Train Linf Norm: 0.4423, Test Linf Norm: 0.2043\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0028, Train Linf Norm: 0.4315, Test Linf Norm: 0.2053\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0029, Train Linf Norm: 0.4235, Test Linf Norm: 0.2115\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0028, Train Linf Norm: 0.4377, Test Linf Norm: 0.2016\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0029, Train Linf Norm: 0.4349, Test Linf Norm: 0.2101\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0029, Train Linf Norm: 0.4383, Test Linf Norm: 0.2101\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0028, Train Linf Norm: 0.4348, Test Linf Norm: 0.2063\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0029, Train Linf Norm: 0.4444, Test Linf Norm: 0.2107\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0029, Train Linf Norm: 0.4263, Test Linf Norm: 0.2133\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0029, Train Linf Norm: 0.4312, Test Linf Norm: 0.2201\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0029, Train Linf Norm: 0.4355, Test Linf Norm: 0.2142\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0027, Train Linf Norm: 0.4383, Test Linf Norm: 0.1994\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0029, Train Linf Norm: 0.5882, Test Linf Norm: 0.2277\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0031, Train Linf Norm: 0.4169, Test Linf Norm: 0.2395\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0029, Train Linf Norm: 0.4005, Test Linf Norm: 0.2190\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0029, Train Linf Norm: 0.4268, Test Linf Norm: 0.2242\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0027, Train Linf Norm: 0.4026, Test Linf Norm: 0.2068\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0027, Train Linf Norm: 0.4081, Test Linf Norm: 0.2100\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0028, Train Linf Norm: 0.4141, Test Linf Norm: 0.2068\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0028, Train Linf Norm: 0.4037, Test Linf Norm: 0.2068\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0028, Train Linf Norm: 0.4036, Test Linf Norm: 0.2066\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0027, Train Linf Norm: 0.4070, Test Linf Norm: 0.2066\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0028, Train Linf Norm: 0.4021, Test Linf Norm: 0.2048\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0027, Train Linf Norm: 0.3967, Test Linf Norm: 0.2048\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0026, Train Linf Norm: 0.4217, Test Linf Norm: 0.2014\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0035, Train Linf Norm: 0.4398, Test Linf Norm: 0.2498\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0041, Train Linf Norm: 0.5216, Test Linf Norm: 0.2940\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0026, Train Linf Norm: 0.3436, Test Linf Norm: 0.2108\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0027, Train Linf Norm: 0.4004, Test Linf Norm: 0.2090\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0027, Train Linf Norm: 0.3693, Test Linf Norm: 0.2137\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0027, Train Linf Norm: 0.3906, Test Linf Norm: 0.2122\n",
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0027, Train Linf Norm: 0.3864, Test Linf Norm: 0.2127\n",
            "Epoch 119: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0027, Train Linf Norm: 0.3643, Test Linf Norm: 0.2138\n",
            "Epoch 120: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0027, Train Linf Norm: 0.3822, Test Linf Norm: 0.2138\n",
            "Epoch 121: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0027, Train Linf Norm: 0.3658, Test Linf Norm: 0.2137\n",
            "Epoch 122: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0026, Train Linf Norm: 0.3729, Test Linf Norm: 0.2123\n",
            "Epoch 123: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0027, Train Linf Norm: 0.3849, Test Linf Norm: 0.2166\n",
            "Epoch 124: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0027, Train Linf Norm: 0.3824, Test Linf Norm: 0.2177\n",
            "Epoch 125: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0026, Train Linf Norm: 0.3902, Test Linf Norm: 0.2179\n",
            "Epoch 126: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0026, Train Linf Norm: 0.4484, Test Linf Norm: 0.2177\n",
            "Epoch 127: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0102, Test L1 Norm: 0.0028, Train Linf Norm: 1.3667, Test Linf Norm: 0.2205\n",
            "Epoch 128: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0027, Train Linf Norm: 0.4371, Test Linf Norm: 0.2263\n",
            "Epoch 129: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0027, Train Linf Norm: 0.3675, Test Linf Norm: 0.2173\n",
            "Epoch 130: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0025, Train Linf Norm: 0.3421, Test Linf Norm: 0.2072\n",
            "Epoch 131: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0026, Train Linf Norm: 0.3728, Test Linf Norm: 0.2131\n",
            "Epoch 132: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0026, Train Linf Norm: 0.3689, Test Linf Norm: 0.2118\n",
            "Epoch 133: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0026, Train Linf Norm: 0.3563, Test Linf Norm: 0.2118\n",
            "Epoch 134: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0026, Train Linf Norm: 0.3571, Test Linf Norm: 0.2118\n",
            "Epoch 135: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0026, Train Linf Norm: 0.3598, Test Linf Norm: 0.2124\n",
            "Epoch 136: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0026, Train Linf Norm: 0.3573, Test Linf Norm: 0.2113\n",
            "Epoch 137: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0025, Train Linf Norm: 0.3706, Test Linf Norm: 0.2088\n",
            "Epoch 138: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0026, Train Linf Norm: 0.3545, Test Linf Norm: 0.2138\n",
            "Epoch 139: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0026, Train Linf Norm: 0.3646, Test Linf Norm: 0.2128\n",
            "Epoch 140: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0027, Train Linf Norm: 0.3525, Test Linf Norm: 0.2142\n",
            "Epoch 141: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0027, Train Linf Norm: 0.3995, Test Linf Norm: 0.2132\n",
            "Epoch 142: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0028, Train Linf Norm: 0.3532, Test Linf Norm: 0.2283\n",
            "Epoch 143: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0027, Train Linf Norm: 0.3468, Test Linf Norm: 0.2189\n",
            "Epoch 144: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0026, Train Linf Norm: 0.3442, Test Linf Norm: 0.2141\n",
            "Epoch 145: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0026, Train Linf Norm: 0.3612, Test Linf Norm: 0.2145\n",
            "Epoch 146: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0025, Train Linf Norm: 0.3698, Test Linf Norm: 0.2134\n",
            "Epoch 147: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0026, Train Linf Norm: 0.3657, Test Linf Norm: 0.2138\n",
            "Epoch 148: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0026, Train Linf Norm: 0.3610, Test Linf Norm: 0.2138\n",
            "Epoch 149: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0025, Train Linf Norm: 0.3470, Test Linf Norm: 0.2137\n",
            "Epoch 150: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0026, Train Linf Norm: 0.3578, Test Linf Norm: 0.2117\n",
            "Epoch 151: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0025, Train Linf Norm: 0.3673, Test Linf Norm: 0.2143\n",
            "Epoch 152: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0026, Train Linf Norm: 0.3682, Test Linf Norm: 0.2130\n",
            "Epoch 153: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0025, Train Linf Norm: 0.3529, Test Linf Norm: 0.2118\n",
            "Epoch 154: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0026, Train Linf Norm: 0.3705, Test Linf Norm: 0.2225\n",
            "Epoch 155: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0026, Train Linf Norm: 0.6387, Test Linf Norm: 0.2119\n",
            "Epoch 156: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0029, Train Linf Norm: 0.3474, Test Linf Norm: 0.2530\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:06:44,954]\u001b[0m Trial 127 finished with value: 0.002502769040642306 and parameters: {'n_layers': 3, 'n_units_0': 991, 'n_units_1': 1025, 'n_units_2': 730, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'LogCosh', 'optimizer': 'Adagrad', 'lr': 0.00033487497640022064, 'batch_size': 222, 'n_epochs': 157, 'scheduler': 'CosineAnnealingLR', 'T_max': 7}. Best is trial 120 with value: 0.0022177403999259697.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 157: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0025, Train Linf Norm: 0.3548, Test Linf Norm: 0.2079\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:06:46,592]\u001b[0m Trial 128 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.1555, Test Loss: 0.0047, Train L1 Norm: 0.1667, Test L1 Norm: 0.1556, Train Linf Norm: 18.2045, Test Linf Norm: 27.6583\n",
            "Epoch 1: Train Loss: 0.0573, Test Loss: 0.0066, Train L1 Norm: 0.1914, Test L1 Norm: 0.0423, Train Linf Norm: 17.7033, Test Linf Norm: 0.8614\n",
            "Epoch 2: Train Loss: 0.0010, Test Loss: 0.0004, Train L1 Norm: 0.0400, Test L1 Norm: 0.0498, Train Linf Norm: 4.4441, Test Linf Norm: 7.6899\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:06:52,537]\u001b[0m Trial 129 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0183, Test L1 Norm: 0.0250, Train Linf Norm: 1.9732, Test Linf Norm: 3.6777\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:06:54,434]\u001b[0m Trial 130 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 4.3578, Test Loss: 1.9582, Train L1 Norm: 8.2052, Test L1 Norm: 2.1605, Train Linf Norm: 1134.8359, Test Linf Norm: 337.1221\n",
            "Epoch 1: Train Loss: 0.0382, Test Loss: 0.0003, Train L1 Norm: 0.1635, Test L1 Norm: 0.0563, Train Linf Norm: 11.7313, Test Linf Norm: 5.8583\n",
            "Epoch 2: Train Loss: 0.0004, Test Loss: 0.0001, Train L1 Norm: 0.0341, Test L1 Norm: 0.0260, Train Linf Norm: 2.9076, Test Linf Norm: 2.5180\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:07:00,924]\u001b[0m Trial 131 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0163, Test L1 Norm: 0.0329, Train Linf Norm: 1.2599, Test Linf Norm: 3.6150\n",
            "Epoch 1: Train Loss: 0.1796, Test Loss: 0.0018, Train L1 Norm: 0.1380, Test L1 Norm: 0.0217, Train Linf Norm: 15.4757, Test Linf Norm: 0.7474\n",
            "Epoch 2: Train Loss: 0.0014, Test Loss: 0.0002, Train L1 Norm: 0.0233, Test L1 Norm: 0.0122, Train Linf Norm: 1.7114, Test Linf Norm: 0.8133\n",
            "Epoch 3: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0137, Test L1 Norm: 0.0112, Train Linf Norm: 1.0974, Test Linf Norm: 0.6234\n",
            "Epoch 4: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0114, Test L1 Norm: 0.0156, Train Linf Norm: 1.0258, Test Linf Norm: 2.1471\n",
            "Epoch 5: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0104, Test L1 Norm: 0.0110, Train Linf Norm: 0.9347, Test Linf Norm: 1.1747\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0098, Test L1 Norm: 0.0139, Train Linf Norm: 0.8870, Test Linf Norm: 1.8703\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0096, Test L1 Norm: 0.0121, Train Linf Norm: 0.8861, Test Linf Norm: 1.4896\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0095, Test L1 Norm: 0.0123, Train Linf Norm: 0.8827, Test Linf Norm: 1.5440\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0123, Train Linf Norm: 0.8415, Test Linf Norm: 1.5440\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:07:18,363]\u001b[0m Trial 132 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0095, Test L1 Norm: 0.0121, Train Linf Norm: 0.8867, Test Linf Norm: 1.5085\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:07:20,024]\u001b[0m Trial 133 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.1505, Test Loss: 0.0020, Train L1 Norm: 0.1689, Test L1 Norm: 0.0750, Train Linf Norm: 17.4233, Test Linf Norm: 10.4902\n",
            "Epoch 2: Train Loss: 0.0018, Test Loss: 0.0001, Train L1 Norm: 0.0446, Test L1 Norm: 0.0191, Train Linf Norm: 5.0515, Test Linf Norm: 1.7274\n",
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0213, Test L1 Norm: 0.0131, Train Linf Norm: 2.4010, Test Linf Norm: 1.1039\n",
            "Epoch 4: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0171, Test L1 Norm: 0.0108, Train Linf Norm: 1.8880, Test Linf Norm: 0.8551\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0138, Test L1 Norm: 0.0091, Train Linf Norm: 1.4681, Test Linf Norm: 0.6553\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0130, Test L1 Norm: 0.0087, Train Linf Norm: 1.3028, Test Linf Norm: 0.6346\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0127, Test L1 Norm: 0.0087, Train Linf Norm: 1.3415, Test Linf Norm: 0.6156\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0124, Test L1 Norm: 0.0085, Train Linf Norm: 1.2123, Test Linf Norm: 0.6212\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0124, Test L1 Norm: 0.0085, Train Linf Norm: 1.2862, Test Linf Norm: 0.6212\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0124, Test L1 Norm: 0.0085, Train Linf Norm: 1.3130, Test Linf Norm: 0.6351\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0126, Test L1 Norm: 0.0083, Train Linf Norm: 1.3370, Test Linf Norm: 0.5867\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0122, Test L1 Norm: 0.0082, Train Linf Norm: 1.2776, Test Linf Norm: 0.6022\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0126, Test L1 Norm: 0.0082, Train Linf Norm: 1.3845, Test Linf Norm: 0.6190\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0117, Test L1 Norm: 0.0078, Train Linf Norm: 1.2376, Test Linf Norm: 0.6093\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0111, Test L1 Norm: 0.0073, Train Linf Norm: 1.1422, Test Linf Norm: 0.4949\n",
            "Epoch 16: Train Loss: 0.0004, Test Loss: 0.0000, Train L1 Norm: 0.0179, Test L1 Norm: 0.0070, Train Linf Norm: 1.7617, Test Linf Norm: 0.5015\n",
            "Epoch 17: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0133, Test L1 Norm: 0.0075, Train Linf Norm: 1.3831, Test Linf Norm: 0.6249\n",
            "Epoch 18: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0114, Test L1 Norm: 0.0065, Train Linf Norm: 1.1133, Test Linf Norm: 0.4923\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0065, Train Linf Norm: 0.8670, Test Linf Norm: 0.4844\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0091, Test L1 Norm: 0.0061, Train Linf Norm: 1.0024, Test Linf Norm: 0.4418\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0086, Test L1 Norm: 0.0058, Train Linf Norm: 0.8841, Test Linf Norm: 0.4445\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0057, Train Linf Norm: 0.8282, Test Linf Norm: 0.4173\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0056, Train Linf Norm: 0.8302, Test Linf Norm: 0.3998\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0056, Train Linf Norm: 0.8173, Test Linf Norm: 0.4030\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0056, Train Linf Norm: 0.8214, Test Linf Norm: 0.4030\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0055, Train Linf Norm: 0.8077, Test Linf Norm: 0.4086\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0056, Train Linf Norm: 0.8011, Test Linf Norm: 0.4036\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0079, Test L1 Norm: 0.0056, Train Linf Norm: 0.8024, Test Linf Norm: 0.4021\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0055, Train Linf Norm: 0.8491, Test Linf Norm: 0.4104\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0054, Train Linf Norm: 0.7742, Test Linf Norm: 0.4250\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0053, Train Linf Norm: 0.8462, Test Linf Norm: 0.4179\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0095, Test L1 Norm: 0.0051, Train Linf Norm: 1.0332, Test Linf Norm: 0.3664\n",
            "Epoch 33: Train Loss: 0.0003, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0049, Train Linf Norm: 1.2210, Test Linf Norm: 0.3546\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0056, Train Linf Norm: 0.6920, Test Linf Norm: 0.5135\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0049, Train Linf Norm: 0.7240, Test Linf Norm: 0.3852\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0056, Train Linf Norm: 0.7284, Test Linf Norm: 0.5203\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0047, Train Linf Norm: 0.6916, Test Linf Norm: 0.3535\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0044, Train Linf Norm: 0.6855, Test Linf Norm: 0.3303\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0044, Train Linf Norm: 0.6978, Test Linf Norm: 0.3285\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0044, Train Linf Norm: 0.6346, Test Linf Norm: 0.3330\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0044, Train Linf Norm: 0.7023, Test Linf Norm: 0.3330\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0044, Train Linf Norm: 0.6873, Test Linf Norm: 0.3322\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0044, Train Linf Norm: 0.6843, Test Linf Norm: 0.3325\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0045, Train Linf Norm: 0.7157, Test Linf Norm: 0.3433\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0043, Train Linf Norm: 0.6662, Test Linf Norm: 0.3191\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0045, Train Linf Norm: 0.6964, Test Linf Norm: 0.3480\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0045, Train Linf Norm: 0.6945, Test Linf Norm: 0.3713\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0044, Train Linf Norm: 0.6325, Test Linf Norm: 0.3134\n",
            "Epoch 49: Train Loss: 0.0003, Test Loss: 0.0000, Train L1 Norm: 0.0137, Test L1 Norm: 0.0043, Train Linf Norm: 1.3718, Test Linf Norm: 0.3085\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0063, Test L1 Norm: 0.0099, Train Linf Norm: 0.6356, Test Linf Norm: 1.1051\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0040, Train Linf Norm: 0.6677, Test Linf Norm: 0.3218\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0040, Train Linf Norm: 0.6211, Test Linf Norm: 0.3088\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0038, Train Linf Norm: 0.6583, Test Linf Norm: 0.2896\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0039, Train Linf Norm: 0.6252, Test Linf Norm: 0.2992\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0040, Train Linf Norm: 0.6292, Test Linf Norm: 0.3241\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0038, Train Linf Norm: 0.6298, Test Linf Norm: 0.2854\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0038, Train Linf Norm: 0.6450, Test Linf Norm: 0.2854\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0037, Train Linf Norm: 0.6208, Test Linf Norm: 0.2813\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0038, Train Linf Norm: 0.6286, Test Linf Norm: 0.2841\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0039, Train Linf Norm: 0.6399, Test Linf Norm: 0.3014\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0039, Train Linf Norm: 0.6428, Test Linf Norm: 0.3001\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0036, Train Linf Norm: 0.6204, Test Linf Norm: 0.2530\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0037, Train Linf Norm: 0.6360, Test Linf Norm: 0.2737\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0040, Train Linf Norm: 0.7214, Test Linf Norm: 0.3348\n",
            "Epoch 65: Train Loss: 0.0001, Test Loss: 0.0003, Train L1 Norm: 0.0105, Test L1 Norm: 0.0281, Train Linf Norm: 1.1229, Test Linf Norm: 3.9079\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0037, Train Linf Norm: 0.6544, Test Linf Norm: 0.3028\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0036, Train Linf Norm: 0.6677, Test Linf Norm: 0.2821\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0038, Train Linf Norm: 0.6167, Test Linf Norm: 0.3248\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0035, Train Linf Norm: 0.5940, Test Linf Norm: 0.2787\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0034, Train Linf Norm: 0.5610, Test Linf Norm: 0.2597\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0034, Train Linf Norm: 0.5639, Test Linf Norm: 0.2456\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0034, Train Linf Norm: 0.5540, Test Linf Norm: 0.2525\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0034, Train Linf Norm: 0.5594, Test Linf Norm: 0.2525\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0034, Train Linf Norm: 0.5837, Test Linf Norm: 0.2527\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0034, Train Linf Norm: 0.5768, Test Linf Norm: 0.2563\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0033, Train Linf Norm: 0.5707, Test Linf Norm: 0.2542\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0034, Train Linf Norm: 0.5807, Test Linf Norm: 0.2595\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0033, Train Linf Norm: 0.5684, Test Linf Norm: 0.2489\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0034, Train Linf Norm: 0.5761, Test Linf Norm: 0.2629\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0040, Train Linf Norm: 0.6901, Test Linf Norm: 0.3506\n",
            "Epoch 81: Train Loss: 0.0001, Test Loss: 0.0020, Train L1 Norm: 0.0080, Test L1 Norm: 0.0833, Train Linf Norm: 0.8187, Test Linf Norm: 11.7221\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0033, Train Linf Norm: 0.5063, Test Linf Norm: 0.2540\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0031, Train Linf Norm: 0.5536, Test Linf Norm: 0.2317\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0032, Train Linf Norm: 0.5768, Test Linf Norm: 0.2492\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0034, Train Linf Norm: 0.5486, Test Linf Norm: 0.2761\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0033, Train Linf Norm: 0.5423, Test Linf Norm: 0.2657\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0031, Train Linf Norm: 0.5272, Test Linf Norm: 0.2430\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0031, Train Linf Norm: 0.5209, Test Linf Norm: 0.2377\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0031, Train Linf Norm: 0.5348, Test Linf Norm: 0.2377\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0031, Train Linf Norm: 0.5278, Test Linf Norm: 0.2409\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0031, Train Linf Norm: 0.5212, Test Linf Norm: 0.2375\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0032, Train Linf Norm: 0.4991, Test Linf Norm: 0.2478\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0030, Train Linf Norm: 0.5162, Test Linf Norm: 0.2185\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0036, Train Linf Norm: 0.5162, Test Linf Norm: 0.3355\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0032, Train Linf Norm: 0.5938, Test Linf Norm: 0.2525\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0033, Train Linf Norm: 0.5546, Test Linf Norm: 0.2604\n",
            "Epoch 97: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0089, Test L1 Norm: 0.0032, Train Linf Norm: 0.9049, Test Linf Norm: 0.2473\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0029, Train Linf Norm: 0.5610, Test Linf Norm: 0.2191\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0037, Train Linf Norm: 0.5512, Test Linf Norm: 0.2935\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0031, Train Linf Norm: 0.5378, Test Linf Norm: 0.2559\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0030, Train Linf Norm: 0.5265, Test Linf Norm: 0.2409\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0029, Train Linf Norm: 0.5168, Test Linf Norm: 0.2275\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0029, Train Linf Norm: 0.5029, Test Linf Norm: 0.2256\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0030, Train Linf Norm: 0.5107, Test Linf Norm: 0.2295\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0030, Train Linf Norm: 0.5065, Test Linf Norm: 0.2295\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0030, Train Linf Norm: 0.5111, Test Linf Norm: 0.2286\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0030, Train Linf Norm: 0.5090, Test Linf Norm: 0.2301\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0031, Train Linf Norm: 0.5035, Test Linf Norm: 0.2533\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0030, Train Linf Norm: 0.5081, Test Linf Norm: 0.2420\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0033, Train Linf Norm: 0.4904, Test Linf Norm: 0.2733\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0032, Train Linf Norm: 0.4855, Test Linf Norm: 0.2559\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0030, Train Linf Norm: 0.7015, Test Linf Norm: 0.2441\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0074, Test L1 Norm: 0.0055, Train Linf Norm: 0.8431, Test Linf Norm: 0.2778\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0035, Train Linf Norm: 0.6034, Test Linf Norm: 0.2587\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0032, Train Linf Norm: 0.5094, Test Linf Norm: 0.2862\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0031, Train Linf Norm: 0.5178, Test Linf Norm: 0.2387\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0028, Train Linf Norm: 0.4904, Test Linf Norm: 0.2222\n",
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0029, Train Linf Norm: 0.5083, Test Linf Norm: 0.2291\n",
            "Epoch 119: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0028, Train Linf Norm: 0.4938, Test Linf Norm: 0.2204\n",
            "Epoch 120: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0028, Train Linf Norm: 0.4932, Test Linf Norm: 0.2202\n",
            "Epoch 121: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0028, Train Linf Norm: 0.4938, Test Linf Norm: 0.2202\n",
            "Epoch 122: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0028, Train Linf Norm: 0.5018, Test Linf Norm: 0.2235\n",
            "Epoch 123: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0029, Train Linf Norm: 0.4788, Test Linf Norm: 0.2294\n",
            "Epoch 124: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0029, Train Linf Norm: 0.4985, Test Linf Norm: 0.2335\n",
            "Epoch 125: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0028, Train Linf Norm: 0.5024, Test Linf Norm: 0.2217\n",
            "Epoch 126: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0028, Train Linf Norm: 0.4852, Test Linf Norm: 0.2217\n",
            "Epoch 127: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0028, Train Linf Norm: 0.5518, Test Linf Norm: 0.2225\n",
            "Epoch 128: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0028, Train Linf Norm: 0.5398, Test Linf Norm: 0.2231\n",
            "Epoch 129: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0096, Test L1 Norm: 0.0030, Train Linf Norm: 1.1240, Test Linf Norm: 0.2475\n",
            "Epoch 130: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0029, Train Linf Norm: 0.5423, Test Linf Norm: 0.2402\n",
            "Epoch 131: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0028, Train Linf Norm: 0.5144, Test Linf Norm: 0.2284\n",
            "Epoch 132: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0028, Train Linf Norm: 0.4909, Test Linf Norm: 0.2303\n",
            "Epoch 133: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0028, Train Linf Norm: 0.4954, Test Linf Norm: 0.2237\n",
            "Epoch 134: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0027, Train Linf Norm: 0.4677, Test Linf Norm: 0.2172\n",
            "Epoch 135: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0027, Train Linf Norm: 0.4792, Test Linf Norm: 0.2162\n",
            "Epoch 136: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0027, Train Linf Norm: 0.4639, Test Linf Norm: 0.2113\n",
            "Epoch 137: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0027, Train Linf Norm: 0.4839, Test Linf Norm: 0.2113\n",
            "Epoch 138: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0027, Train Linf Norm: 0.4676, Test Linf Norm: 0.2149\n",
            "Epoch 139: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0027, Train Linf Norm: 0.4494, Test Linf Norm: 0.2136\n",
            "Epoch 140: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0029, Train Linf Norm: 0.4673, Test Linf Norm: 0.2404\n",
            "Epoch 141: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0027, Train Linf Norm: 0.4734, Test Linf Norm: 0.2139\n",
            "Epoch 142: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0028, Train Linf Norm: 0.4495, Test Linf Norm: 0.2322\n",
            "Epoch 143: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0027, Train Linf Norm: 0.5030, Test Linf Norm: 0.2202\n",
            "Epoch 144: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0027, Train Linf Norm: 0.4567, Test Linf Norm: 0.2223\n",
            "Epoch 145: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0031, Train Linf Norm: 0.5088, Test Linf Norm: 0.2429\n",
            "Epoch 146: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0031, Train Linf Norm: 0.5483, Test Linf Norm: 0.2374\n",
            "Epoch 147: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0028, Train Linf Norm: 0.4538, Test Linf Norm: 0.2300\n",
            "Epoch 148: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0026, Train Linf Norm: 0.4697, Test Linf Norm: 0.2142\n",
            "Epoch 149: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0027, Train Linf Norm: 0.4736, Test Linf Norm: 0.2250\n",
            "Epoch 150: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0026, Train Linf Norm: 0.4606, Test Linf Norm: 0.2098\n",
            "Epoch 151: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0027, Train Linf Norm: 0.4625, Test Linf Norm: 0.2209\n",
            "Epoch 152: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0026, Train Linf Norm: 0.4522, Test Linf Norm: 0.2147\n",
            "Epoch 153: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0026, Train Linf Norm: 0.4489, Test Linf Norm: 0.2147\n",
            "Epoch 154: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0026, Train Linf Norm: 0.4467, Test Linf Norm: 0.2103\n",
            "Epoch 155: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0026, Train Linf Norm: 0.4529, Test Linf Norm: 0.2105\n",
            "Epoch 156: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0026, Train Linf Norm: 0.4500, Test Linf Norm: 0.2078\n",
            "Epoch 157: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0027, Train Linf Norm: 0.4497, Test Linf Norm: 0.2248\n",
            "Epoch 158: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0026, Train Linf Norm: 0.4581, Test Linf Norm: 0.2010\n",
            "Epoch 159: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0027, Train Linf Norm: 0.4503, Test Linf Norm: 0.2193\n",
            "Epoch 160: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0027, Train Linf Norm: 0.4983, Test Linf Norm: 0.2156\n",
            "Epoch 161: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0056, Test L1 Norm: 0.0127, Train Linf Norm: 0.5741, Test Linf Norm: 1.5999\n",
            "Epoch 162: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0027, Train Linf Norm: 0.5376, Test Linf Norm: 0.2379\n",
            "Epoch 163: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0027, Train Linf Norm: 0.4742, Test Linf Norm: 0.2243\n",
            "Epoch 164: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0026, Train Linf Norm: 0.4838, Test Linf Norm: 0.2159\n",
            "Epoch 165: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0026, Train Linf Norm: 0.4531, Test Linf Norm: 0.2125\n",
            "Epoch 166: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0026, Train Linf Norm: 0.4753, Test Linf Norm: 0.2182\n",
            "Epoch 167: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0026, Train Linf Norm: 0.4500, Test Linf Norm: 0.2122\n",
            "Epoch 168: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0025, Train Linf Norm: 0.4580, Test Linf Norm: 0.2077\n",
            "Epoch 169: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0025, Train Linf Norm: 0.4572, Test Linf Norm: 0.2077\n",
            "Epoch 170: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0025, Train Linf Norm: 0.4508, Test Linf Norm: 0.2077\n",
            "Epoch 171: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0026, Train Linf Norm: 0.4628, Test Linf Norm: 0.2096\n",
            "Epoch 172: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0026, Train Linf Norm: 0.4463, Test Linf Norm: 0.2191\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:12:39,977]\u001b[0m Trial 134 finished with value: 0.0026123005998320878 and parameters: {'n_layers': 3, 'n_units_0': 981, 'n_units_1': 1045, 'n_units_2': 786, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'MSE', 'optimizer': 'Adagrad', 'lr': 0.0005059332901766789, 'batch_size': 193, 'n_epochs': 173, 'scheduler': 'CosineAnnealingLR', 'T_max': 8}. Best is trial 120 with value: 0.0022177403999259697.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 173: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0026, Train Linf Norm: 0.4534, Test Linf Norm: 0.2162\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:12:41,921]\u001b[0m Trial 135 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.1768, Test Loss: 0.0021, Train L1 Norm: 0.1517, Test L1 Norm: 0.1163, Train Linf Norm: 15.1853, Test Linf Norm: 19.1457\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:12:44,530]\u001b[0m Trial 136 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 20.5584, Test Loss: 20.7300, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:12:46,257]\u001b[0m Trial 137 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.2896, Test Loss: 0.0513, Train L1 Norm: 6.5205, Test L1 Norm: 4.1380, Train Linf Norm: 907.3083, Test Linf Norm: 565.2561\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:12:48,148]\u001b[0m Trial 138 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.1597, Test Loss: 0.0015, Train L1 Norm: 0.1690, Test L1 Norm: 0.1344, Train Linf Norm: 15.4082, Test Linf Norm: 22.1592\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:12:49,703]\u001b[0m Trial 139 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0624, Test Loss: 0.0031, Train L1 Norm: 0.2383, Test L1 Norm: 0.2824, Train Linf Norm: 27.4032, Test Linf Norm: 53.6143\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:12:50,952]\u001b[0m Trial 140 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 1.3638, Test Loss: 0.3238, Train L1 Norm: 3.0329, Test L1 Norm: 1.6259, Train Linf Norm: 476.7697, Test Linf Norm: 282.0596\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:12:53,018]\u001b[0m Trial 141 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 20.5584, Test Loss: 20.7300, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:12:55,571]\u001b[0m Trial 142 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0359, Test Loss: 0.0105, Train L1 Norm: 0.1101, Test L1 Norm: 0.3167, Train Linf Norm: 5.9944, Test Linf Norm: 30.6630\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:12:58,278]\u001b[0m Trial 143 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.0390, Test Loss: 0.0009, Train L1 Norm: 0.1743, Test L1 Norm: 0.0191, Train Linf Norm: 15.8969, Test Linf Norm: 0.6450\n",
            "Epoch 2: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0264, Test L1 Norm: 0.0106, Train Linf Norm: 2.3588, Test Linf Norm: 0.5870\n",
            "Epoch 3: Train Loss: 0.0004, Test Loss: 0.0000, Train L1 Norm: 0.0195, Test L1 Norm: 0.0090, Train Linf Norm: 1.7146, Test Linf Norm: 0.4873\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0138, Test L1 Norm: 0.0079, Train Linf Norm: 1.1985, Test Linf Norm: 0.5073\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0131, Test L1 Norm: 0.0072, Train Linf Norm: 1.2332, Test Linf Norm: 0.3784\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0103, Test L1 Norm: 0.0122, Train Linf Norm: 0.8758, Test Linf Norm: 1.1375\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0071, Train Linf Norm: 0.5747, Test Linf Norm: 0.3361\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0089, Test L1 Norm: 0.0057, Train Linf Norm: 0.7305, Test Linf Norm: 0.3295\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0083, Test L1 Norm: 0.0056, Train Linf Norm: 0.6507, Test Linf Norm: 0.2808\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0051, Train Linf Norm: 0.5379, Test Linf Norm: 0.3085\n",
            "Epoch 11: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0093, Test L1 Norm: 0.0046, Train Linf Norm: 0.7340, Test Linf Norm: 0.2576\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0044, Train Linf Norm: 0.5105, Test Linf Norm: 0.2492\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0045, Train Linf Norm: 0.4897, Test Linf Norm: 0.2498\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0042, Train Linf Norm: 0.5010, Test Linf Norm: 0.2417\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0041, Train Linf Norm: 0.5833, Test Linf Norm: 0.2252\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0069, Train Linf Norm: 0.5069, Test Linf Norm: 0.6568\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0037, Train Linf Norm: 0.4449, Test Linf Norm: 0.2018\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0038, Train Linf Norm: 0.4424, Test Linf Norm: 0.2148\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0052, Train Linf Norm: 0.4413, Test Linf Norm: 0.4348\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0045, Train Linf Norm: 0.3784, Test Linf Norm: 0.3354\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0047, Train Linf Norm: 0.3852, Test Linf Norm: 0.4000\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0041, Train Linf Norm: 0.4368, Test Linf Norm: 0.2917\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0035, Train Linf Norm: 0.4099, Test Linf Norm: 0.2240\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0151, Train Linf Norm: 0.3901, Test Linf Norm: 1.7800\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0035, Train Linf Norm: 0.3704, Test Linf Norm: 0.2042\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0063, Train Linf Norm: 0.4107, Test Linf Norm: 0.6565\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0041, Train Linf Norm: 0.3661, Test Linf Norm: 0.3060\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0101, Train Linf Norm: 0.3600, Test Linf Norm: 1.1623\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0075, Train Linf Norm: 0.3706, Test Linf Norm: 0.8552\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0058, Train Linf Norm: 0.3630, Test Linf Norm: 0.6020\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0063, Train Linf Norm: 0.3427, Test Linf Norm: 0.6723\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0086, Train Linf Norm: 0.3616, Test Linf Norm: 0.9854\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0048, Train Linf Norm: 0.3585, Test Linf Norm: 0.3642\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0046, Train Linf Norm: 0.2998, Test Linf Norm: 0.4187\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0046, Train Linf Norm: 0.3941, Test Linf Norm: 0.4239\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0069, Train Linf Norm: 0.3538, Test Linf Norm: 0.7871\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0034, Train Linf Norm: 0.3269, Test Linf Norm: 0.2061\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0066, Train Linf Norm: 0.3402, Test Linf Norm: 0.7500\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0049, Train Linf Norm: 0.3111, Test Linf Norm: 0.4656\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0089, Train Linf Norm: 0.3315, Test Linf Norm: 1.0619\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0061, Train Linf Norm: 0.3142, Test Linf Norm: 0.6772\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0064, Train Linf Norm: 0.3374, Test Linf Norm: 0.7099\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0069, Train Linf Norm: 0.3310, Test Linf Norm: 0.8015\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0066, Train Linf Norm: 0.3290, Test Linf Norm: 0.7557\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0054, Train Linf Norm: 0.3181, Test Linf Norm: 0.5741\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0064, Train Linf Norm: 0.6140, Test Linf Norm: 0.7465\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0085, Train Linf Norm: 0.3126, Test Linf Norm: 1.0369\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0060, Train Linf Norm: 0.2920, Test Linf Norm: 0.6739\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0067, Train Linf Norm: 0.2709, Test Linf Norm: 0.7703\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0108, Train Linf Norm: 0.2955, Test Linf Norm: 1.3807\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0059, Train Linf Norm: 0.2895, Test Linf Norm: 0.6535\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0065, Train Linf Norm: 0.2944, Test Linf Norm: 0.7645\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0078, Train Linf Norm: 0.2971, Test Linf Norm: 0.9680\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0038, Train Linf Norm: 0.2981, Test Linf Norm: 0.2003\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0071, Train Linf Norm: 0.2738, Test Linf Norm: 0.8683\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0057, Train Linf Norm: 0.2996, Test Linf Norm: 0.6313\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0058, Train Linf Norm: 0.2960, Test Linf Norm: 0.6583\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0066, Train Linf Norm: 0.3318, Test Linf Norm: 0.7783\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0075, Train Linf Norm: 0.2712, Test Linf Norm: 0.9132\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0057, Train Linf Norm: 0.2922, Test Linf Norm: 0.6398\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0064, Train Linf Norm: 0.2677, Test Linf Norm: 0.7632\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0084, Train Linf Norm: 0.2852, Test Linf Norm: 1.0623\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0064, Train Linf Norm: 0.2444, Test Linf Norm: 0.7617\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0058, Train Linf Norm: 0.2474, Test Linf Norm: 0.5764\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0064, Train Linf Norm: 0.2709, Test Linf Norm: 0.7638\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0065, Train Linf Norm: 0.2660, Test Linf Norm: 0.7880\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0053, Train Linf Norm: 0.2591, Test Linf Norm: 0.5704\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0071, Train Linf Norm: 0.2636, Test Linf Norm: 0.8801\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0079, Train Linf Norm: 0.2172, Test Linf Norm: 0.9987\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0064, Train Linf Norm: 0.2696, Test Linf Norm: 0.7702\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0045, Train Linf Norm: 0.2631, Test Linf Norm: 0.3659\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0061, Train Linf Norm: 0.2565, Test Linf Norm: 0.7384\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0077, Train Linf Norm: 0.2540, Test Linf Norm: 0.9723\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0063, Train Linf Norm: 0.2255, Test Linf Norm: 0.7684\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0052, Train Linf Norm: 0.2496, Test Linf Norm: 0.5811\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0058, Train Linf Norm: 0.2399, Test Linf Norm: 0.6812\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0068, Train Linf Norm: 0.2398, Test Linf Norm: 0.8439\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0065, Train Linf Norm: 0.2377, Test Linf Norm: 0.7954\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0063, Train Linf Norm: 0.2556, Test Linf Norm: 0.7739\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0068, Train Linf Norm: 0.2463, Test Linf Norm: 0.8564\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0054, Train Linf Norm: 0.2351, Test Linf Norm: 0.6201\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0029, Test L1 Norm: 0.0057, Train Linf Norm: 0.2162, Test Linf Norm: 0.6736\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0028, Test L1 Norm: 0.0048, Train Linf Norm: 0.2131, Test Linf Norm: 0.4976\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0058, Train Linf Norm: 0.2242, Test Linf Norm: 0.6709\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0062, Train Linf Norm: 0.2282, Test Linf Norm: 0.7569\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0063, Train Linf Norm: 0.2227, Test Linf Norm: 0.7754\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0061, Train Linf Norm: 0.2449, Test Linf Norm: 0.7421\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0029, Test L1 Norm: 0.0066, Train Linf Norm: 0.2200, Test Linf Norm: 0.8186\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0029, Test L1 Norm: 0.0064, Train Linf Norm: 0.2210, Test Linf Norm: 0.7896\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0028, Test L1 Norm: 0.0066, Train Linf Norm: 0.2115, Test Linf Norm: 0.8149\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0028, Test L1 Norm: 0.0056, Train Linf Norm: 0.2105, Test Linf Norm: 0.6712\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0028, Test L1 Norm: 0.0083, Train Linf Norm: 0.2093, Test Linf Norm: 1.0308\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0029, Test L1 Norm: 0.0046, Train Linf Norm: 0.2069, Test Linf Norm: 0.4682\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0029, Test L1 Norm: 0.0048, Train Linf Norm: 0.2088, Test Linf Norm: 0.4973\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0028, Test L1 Norm: 0.0069, Train Linf Norm: 0.2172, Test Linf Norm: 0.8703\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0066, Train Linf Norm: 0.2370, Test Linf Norm: 0.8241\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0050, Train Linf Norm: 0.3223, Test Linf Norm: 0.5676\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0029, Test L1 Norm: 0.0069, Train Linf Norm: 0.2120, Test Linf Norm: 0.8747\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0029, Test L1 Norm: 0.0046, Train Linf Norm: 0.2242, Test Linf Norm: 0.4914\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0028, Test L1 Norm: 0.0059, Train Linf Norm: 0.2130, Test Linf Norm: 0.7264\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0027, Test L1 Norm: 0.0046, Train Linf Norm: 0.1993, Test Linf Norm: 0.4977\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0027, Test L1 Norm: 0.0068, Train Linf Norm: 0.2015, Test Linf Norm: 0.8552\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0028, Test L1 Norm: 0.0065, Train Linf Norm: 0.2155, Test Linf Norm: 0.8006\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0027, Test L1 Norm: 0.0055, Train Linf Norm: 0.1982, Test Linf Norm: 0.6541\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0027, Test L1 Norm: 0.0114, Train Linf Norm: 0.2057, Test Linf Norm: 1.4355\n",
            "Epoch 106: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0057, Train Linf Norm: 0.2291, Test Linf Norm: 0.6914\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0028, Test L1 Norm: 0.0054, Train Linf Norm: 0.2224, Test Linf Norm: 0.6311\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0026, Test L1 Norm: 0.0052, Train Linf Norm: 0.1979, Test Linf Norm: 0.6130\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0043, Train Linf Norm: 0.2443, Test Linf Norm: 0.4746\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0024, Test L1 Norm: 0.0054, Train Linf Norm: 0.1749, Test Linf Norm: 0.6482\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0026, Test L1 Norm: 0.0043, Train Linf Norm: 0.1913, Test Linf Norm: 0.4515\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0027, Test L1 Norm: 0.0062, Train Linf Norm: 0.1911, Test Linf Norm: 0.7693\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0026, Test L1 Norm: 0.0050, Train Linf Norm: 0.1863, Test Linf Norm: 0.5832\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0029, Test L1 Norm: 0.0062, Train Linf Norm: 0.2158, Test Linf Norm: 0.7676\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0026, Test L1 Norm: 0.0058, Train Linf Norm: 0.1944, Test Linf Norm: 0.7180\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0025, Test L1 Norm: 0.0062, Train Linf Norm: 0.1857, Test Linf Norm: 0.7720\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0025, Test L1 Norm: 0.0059, Train Linf Norm: 0.1828, Test Linf Norm: 0.7338\n",
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0025, Test L1 Norm: 0.0078, Train Linf Norm: 0.1896, Test Linf Norm: 0.9890\n",
            "Epoch 119: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0027, Test L1 Norm: 0.0057, Train Linf Norm: 0.2144, Test Linf Norm: 0.6638\n",
            "Epoch 120: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0025, Test L1 Norm: 0.0053, Train Linf Norm: 0.1836, Test Linf Norm: 0.6217\n",
            "Epoch 121: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0024, Test L1 Norm: 0.0050, Train Linf Norm: 0.1752, Test Linf Norm: 0.5768\n",
            "Epoch 122: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0025, Test L1 Norm: 0.0065, Train Linf Norm: 0.1835, Test Linf Norm: 0.8255\n",
            "Epoch 123: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0026, Test L1 Norm: 0.0070, Train Linf Norm: 0.1960, Test Linf Norm: 0.8809\n",
            "Epoch 124: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0025, Test L1 Norm: 0.0059, Train Linf Norm: 0.1741, Test Linf Norm: 0.7274\n",
            "Epoch 125: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0027, Test L1 Norm: 0.0153, Train Linf Norm: 0.2098, Test Linf Norm: 1.8880\n",
            "Epoch 126: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0027, Test L1 Norm: 0.0056, Train Linf Norm: 0.1869, Test Linf Norm: 0.6794\n",
            "Epoch 127: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0026, Test L1 Norm: 0.0063, Train Linf Norm: 0.2034, Test Linf Norm: 0.7936\n",
            "Epoch 128: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0025, Test L1 Norm: 0.0080, Train Linf Norm: 0.1983, Test Linf Norm: 1.0297\n",
            "Epoch 129: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0024, Test L1 Norm: 0.0054, Train Linf Norm: 0.1828, Test Linf Norm: 0.6594\n",
            "Epoch 130: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0027, Test L1 Norm: 0.0073, Train Linf Norm: 0.1924, Test Linf Norm: 0.9595\n",
            "Epoch 131: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0025, Test L1 Norm: 0.0056, Train Linf Norm: 0.1938, Test Linf Norm: 0.6826\n",
            "Epoch 132: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0025, Test L1 Norm: 0.0057, Train Linf Norm: 0.1962, Test Linf Norm: 0.7000\n",
            "Epoch 133: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0028, Test L1 Norm: 0.0053, Train Linf Norm: 0.1881, Test Linf Norm: 0.6416\n",
            "Epoch 134: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0026, Test L1 Norm: 0.0053, Train Linf Norm: 0.2056, Test Linf Norm: 0.6316\n",
            "Epoch 135: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0024, Test L1 Norm: 0.0059, Train Linf Norm: 0.1839, Test Linf Norm: 0.7384\n",
            "Epoch 136: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0023, Test L1 Norm: 0.0058, Train Linf Norm: 0.1640, Test Linf Norm: 0.7215\n",
            "Epoch 137: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0025, Test L1 Norm: 0.0056, Train Linf Norm: 0.1914, Test Linf Norm: 0.6874\n",
            "Epoch 138: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0024, Test L1 Norm: 0.0060, Train Linf Norm: 0.1838, Test Linf Norm: 0.7617\n",
            "Epoch 139: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0025, Test L1 Norm: 0.0065, Train Linf Norm: 0.1910, Test Linf Norm: 0.8292\n",
            "Epoch 140: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0025, Test L1 Norm: 0.0061, Train Linf Norm: 0.1867, Test Linf Norm: 0.7581\n",
            "Epoch 141: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0025, Test L1 Norm: 0.0053, Train Linf Norm: 0.1822, Test Linf Norm: 0.6401\n",
            "Epoch 142: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0024, Test L1 Norm: 0.0053, Train Linf Norm: 0.1816, Test Linf Norm: 0.6458\n",
            "Epoch 143: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0025, Test L1 Norm: 0.0055, Train Linf Norm: 0.1946, Test Linf Norm: 0.6846\n",
            "Epoch 144: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0023, Test L1 Norm: 0.0064, Train Linf Norm: 0.1732, Test Linf Norm: 0.8071\n",
            "Epoch 145: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0023, Test L1 Norm: 0.0061, Train Linf Norm: 0.1680, Test Linf Norm: 0.7785\n",
            "Epoch 146: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0024, Test L1 Norm: 0.0063, Train Linf Norm: 0.1721, Test Linf Norm: 0.7911\n",
            "Epoch 147: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0024, Test L1 Norm: 0.0058, Train Linf Norm: 0.1760, Test Linf Norm: 0.7345\n",
            "Epoch 148: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0024, Test L1 Norm: 0.0063, Train Linf Norm: 0.1812, Test Linf Norm: 0.7916\n",
            "Epoch 149: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0024, Test L1 Norm: 0.0060, Train Linf Norm: 0.1840, Test Linf Norm: 0.7486\n",
            "Epoch 150: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0023, Test L1 Norm: 0.0135, Train Linf Norm: 0.1556, Test Linf Norm: 1.7376\n",
            "Epoch 151: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0023, Test L1 Norm: 0.0064, Train Linf Norm: 0.1707, Test Linf Norm: 0.8071\n",
            "Epoch 152: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0023, Test L1 Norm: 0.0076, Train Linf Norm: 0.1836, Test Linf Norm: 0.9843\n",
            "Epoch 153: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0023, Test L1 Norm: 0.0070, Train Linf Norm: 0.1676, Test Linf Norm: 0.9014\n",
            "Epoch 154: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0023, Test L1 Norm: 0.0072, Train Linf Norm: 0.1664, Test Linf Norm: 0.9180\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:18:12,672]\u001b[0m Trial 144 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 155: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0025, Test L1 Norm: 0.0065, Train Linf Norm: 0.1925, Test Linf Norm: 0.8215\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:18:14,516]\u001b[0m Trial 145 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9894, Test Loss: 2.9774, Train L1 Norm: 1.0669, Test L1 Norm: 1.0000, Train Linf Norm: 5.4972, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.0636, Test Loss: 0.0010, Train L1 Norm: 0.2489, Test L1 Norm: 0.0618, Train Linf Norm: 30.8002, Test Linf Norm: 9.6924\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:18:17,883]\u001b[0m Trial 146 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Train Loss: 0.0009, Test Loss: 0.0001, Train L1 Norm: 0.0544, Test L1 Norm: 0.0646, Train Linf Norm: 7.8222, Test Linf Norm: 11.4911\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:18:19,513]\u001b[0m Trial 147 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 20.5584, Test Loss: 20.7300, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:18:21,330]\u001b[0m Trial 148 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0559, Test Loss: 0.0007, Train L1 Norm: 0.2439, Test L1 Norm: 0.1792, Train Linf Norm: 30.8523, Test Linf Norm: 37.6430\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:18:23,696]\u001b[0m Trial 149 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.0522, Test Loss: 0.0004, Train L1 Norm: 0.2182, Test L1 Norm: 0.0362, Train Linf Norm: 24.7979, Test Linf Norm: 4.1189\n",
            "Epoch 2: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0355, Test L1 Norm: 0.0403, Train Linf Norm: 4.1442, Test Linf Norm: 6.0217\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:18:28,883]\u001b[0m Trial 150 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0233, Test L1 Norm: 0.0415, Train Linf Norm: 2.7913, Test Linf Norm: 6.5599\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:18:30,755]\u001b[0m Trial 151 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0523, Test Loss: 0.0001, Train L1 Norm: 0.1945, Test L1 Norm: 0.0932, Train Linf Norm: 17.1917, Test Linf Norm: 12.7990\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:18:32,521]\u001b[0m Trial 152 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0454, Test Loss: 0.0005, Train L1 Norm: 0.1727, Test L1 Norm: 0.1226, Train Linf Norm: 16.9070, Test Linf Norm: 18.9360\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:18:34,827]\u001b[0m Trial 153 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:18:37,276]\u001b[0m Trial 154 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:18:38,971]\u001b[0m Trial 155 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0616, Test Loss: 0.0006, Train L1 Norm: 0.2184, Test L1 Norm: 0.1044, Train Linf Norm: 20.5609, Test Linf Norm: 16.6939\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:18:41,020]\u001b[0m Trial 156 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0369, Test Loss: 0.0003, Train L1 Norm: 0.1697, Test L1 Norm: 0.1103, Train Linf Norm: 14.2175, Test Linf Norm: 14.5684\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:18:43,107]\u001b[0m Trial 157 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:18:45,010]\u001b[0m Trial 158 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0500, Test Loss: 0.0009, Train L1 Norm: 1.1350, Test L1 Norm: 0.7938, Train Linf Norm: 142.3796, Test Linf Norm: 114.1818\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:18:47,125]\u001b[0m Trial 159 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.1028, Test Loss: 0.0063, Train L1 Norm: 1.4312, Test L1 Norm: 0.5310, Train Linf Norm: 147.0639, Test Linf Norm: 55.0517\n",
            "Epoch 1: Train Loss: 0.0412, Test Loss: 0.0003, Train L1 Norm: 0.1230, Test L1 Norm: 0.0271, Train Linf Norm: 8.4317, Test Linf Norm: 2.3120\n",
            "Epoch 2: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0297, Test L1 Norm: 0.0217, Train Linf Norm: 2.7437, Test Linf Norm: 2.2025\n",
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0219, Test L1 Norm: 0.0140, Train Linf Norm: 2.1748, Test Linf Norm: 1.1706\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0165, Test L1 Norm: 0.0135, Train Linf Norm: 1.6216, Test Linf Norm: 1.2849\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0142, Test L1 Norm: 0.0161, Train Linf Norm: 1.3897, Test Linf Norm: 1.8118\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0129, Test L1 Norm: 0.0185, Train Linf Norm: 1.2557, Test Linf Norm: 2.2319\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:19:01,537]\u001b[0m Trial 160 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0185, Train Linf Norm: 1.1945, Test Linf Norm: 2.2815\n",
            "Epoch 1: Train Loss: 0.0339, Test Loss: 0.0004, Train L1 Norm: 0.1362, Test L1 Norm: 0.0285, Train Linf Norm: 10.9782, Test Linf Norm: 1.9326\n",
            "Epoch 2: Train Loss: 0.0003, Test Loss: 0.0001, Train L1 Norm: 0.0259, Test L1 Norm: 0.0131, Train Linf Norm: 1.8254, Test Linf Norm: 0.6940\n",
            "Epoch 3: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0198, Test L1 Norm: 0.0133, Train Linf Norm: 1.4119, Test Linf Norm: 0.8462\n",
            "Epoch 4: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0171, Test L1 Norm: 0.0094, Train Linf Norm: 1.2655, Test Linf Norm: 0.4937\n",
            "Epoch 5: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0143, Test L1 Norm: 0.0103, Train Linf Norm: 1.1003, Test Linf Norm: 0.6145\n",
            "Epoch 6: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0113, Test L1 Norm: 0.0088, Train Linf Norm: 0.7364, Test Linf Norm: 0.5197\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0101, Test L1 Norm: 0.0078, Train Linf Norm: 0.6973, Test Linf Norm: 0.4460\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0093, Test L1 Norm: 0.0082, Train Linf Norm: 0.6337, Test Linf Norm: 0.3562\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0089, Test L1 Norm: 0.0060, Train Linf Norm: 0.6301, Test Linf Norm: 0.3366\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0083, Test L1 Norm: 0.0071, Train Linf Norm: 0.5813, Test Linf Norm: 0.3026\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0092, Test L1 Norm: 0.0097, Train Linf Norm: 0.6705, Test Linf Norm: 0.5150\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0056, Train Linf Norm: 0.4925, Test Linf Norm: 0.3119\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0055, Train Linf Norm: 0.5118, Test Linf Norm: 0.3071\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0055, Train Linf Norm: 0.5121, Test Linf Norm: 0.3055\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0056, Train Linf Norm: 0.5099, Test Linf Norm: 0.3147\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0056, Train Linf Norm: 0.4988, Test Linf Norm: 0.3183\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0055, Train Linf Norm: 0.4988, Test Linf Norm: 0.3111\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0055, Train Linf Norm: 0.4907, Test Linf Norm: 0.3098\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0054, Train Linf Norm: 0.5013, Test Linf Norm: 0.3045\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0055, Train Linf Norm: 0.5000, Test Linf Norm: 0.3095\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0054, Train Linf Norm: 0.4970, Test Linf Norm: 0.3074\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4999, Test Linf Norm: 0.3054\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0054, Train Linf Norm: 0.5064, Test Linf Norm: 0.3049\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4986, Test Linf Norm: 0.3054\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0054, Train Linf Norm: 0.4904, Test Linf Norm: 0.3058\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4998, Test Linf Norm: 0.3057\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4990, Test Linf Norm: 0.3060\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4992, Test Linf Norm: 0.3059\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.5009, Test Linf Norm: 0.3059\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4963, Test Linf Norm: 0.3053\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4885, Test Linf Norm: 0.3055\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.5029, Test Linf Norm: 0.3053\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4845, Test Linf Norm: 0.3058\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4951, Test Linf Norm: 0.3060\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4953, Test Linf Norm: 0.3061\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.5003, Test Linf Norm: 0.3062\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4922, Test Linf Norm: 0.3063\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4947, Test Linf Norm: 0.3065\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4852, Test Linf Norm: 0.3066\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4911, Test Linf Norm: 0.3067\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4920, Test Linf Norm: 0.3069\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4890, Test Linf Norm: 0.3070\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4880, Test Linf Norm: 0.3071\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4888, Test Linf Norm: 0.3072\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.5038, Test Linf Norm: 0.3072\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.5002, Test Linf Norm: 0.3072\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4933, Test Linf Norm: 0.3072\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4986, Test Linf Norm: 0.3072\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4929, Test Linf Norm: 0.3072\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4926, Test Linf Norm: 0.3072\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4907, Test Linf Norm: 0.3072\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4987, Test Linf Norm: 0.3072\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4908, Test Linf Norm: 0.3072\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4922, Test Linf Norm: 0.3072\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4906, Test Linf Norm: 0.3072\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4947, Test Linf Norm: 0.3072\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:20:59,588]\u001b[0m Trial 161 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0054, Train Linf Norm: 0.4953, Test Linf Norm: 0.3072\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:21:02,121]\u001b[0m Trial 162 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0368, Test Loss: 0.0004, Train L1 Norm: 0.2045, Test L1 Norm: 0.0828, Train Linf Norm: 18.9381, Test Linf Norm: 9.4030\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:21:03,738]\u001b[0m Trial 163 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0475, Test Loss: 0.0009, Train L1 Norm: 0.4116, Test L1 Norm: 0.1342, Train Linf Norm: 69.7482, Test Linf Norm: 24.8678\n",
            "Epoch 1: Train Loss: 0.0471, Test Loss: 0.0002, Train L1 Norm: 0.1727, Test L1 Norm: 0.0580, Train Linf Norm: 14.7228, Test Linf Norm: 7.1858\n",
            "Epoch 2: Train Loss: 0.0006, Test Loss: 0.0000, Train L1 Norm: 0.0442, Test L1 Norm: 0.0255, Train Linf Norm: 4.6734, Test Linf Norm: 2.7027\n",
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0237, Test L1 Norm: 0.0130, Train Linf Norm: 2.3544, Test Linf Norm: 0.9862\n",
            "Epoch 4: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0212, Test L1 Norm: 0.0108, Train Linf Norm: 2.0727, Test Linf Norm: 0.8103\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0156, Test L1 Norm: 0.0092, Train Linf Norm: 1.5174, Test Linf Norm: 0.7121\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0137, Test L1 Norm: 0.0119, Train Linf Norm: 1.3152, Test Linf Norm: 1.0460\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0124, Test L1 Norm: 0.0089, Train Linf Norm: 1.1482, Test Linf Norm: 0.7668\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0112, Test L1 Norm: 0.0075, Train Linf Norm: 1.0910, Test Linf Norm: 0.6230\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0105, Test L1 Norm: 0.0070, Train Linf Norm: 0.9875, Test Linf Norm: 0.5395\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0066, Train Linf Norm: 0.9192, Test Linf Norm: 0.5378\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0062, Train Linf Norm: 1.0145, Test Linf Norm: 0.4762\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0079, Test L1 Norm: 0.0061, Train Linf Norm: 0.7152, Test Linf Norm: 0.4845\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0063, Train Linf Norm: 0.7342, Test Linf Norm: 0.5030\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0062, Train Linf Norm: 0.7506, Test Linf Norm: 0.4898\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0061, Train Linf Norm: 0.7589, Test Linf Norm: 0.4958\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0061, Train Linf Norm: 0.7670, Test Linf Norm: 0.4856\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0061, Train Linf Norm: 0.7639, Test Linf Norm: 0.4953\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0079, Test L1 Norm: 0.0060, Train Linf Norm: 0.7459, Test Linf Norm: 0.4835\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0079, Test L1 Norm: 0.0060, Train Linf Norm: 0.7376, Test Linf Norm: 0.4926\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0060, Train Linf Norm: 0.7352, Test Linf Norm: 0.4884\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0059, Train Linf Norm: 0.7286, Test Linf Norm: 0.4761\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0059, Train Linf Norm: 0.7349, Test Linf Norm: 0.4846\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0059, Train Linf Norm: 0.7202, Test Linf Norm: 0.4820\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0059, Train Linf Norm: 0.7382, Test Linf Norm: 0.4795\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0059, Train Linf Norm: 0.7294, Test Linf Norm: 0.4785\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0059, Train Linf Norm: 0.7260, Test Linf Norm: 0.4749\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0059, Train Linf Norm: 0.7269, Test Linf Norm: 0.4732\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0059, Train Linf Norm: 0.7222, Test Linf Norm: 0.4759\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0059, Train Linf Norm: 0.7228, Test Linf Norm: 0.4731\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0059, Train Linf Norm: 0.7089, Test Linf Norm: 0.4735\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0059, Train Linf Norm: 0.7182, Test Linf Norm: 0.4743\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0059, Train Linf Norm: 0.7337, Test Linf Norm: 0.4763\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0059, Train Linf Norm: 0.7280, Test Linf Norm: 0.4717\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:22:09,101]\u001b[0m Trial 164 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0059, Train Linf Norm: 0.7212, Test Linf Norm: 0.4725\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:22:11,062]\u001b[0m Trial 165 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:22:13,039]\u001b[0m Trial 166 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.3193, Test Loss: 0.1717, Train L1 Norm: 0.5144, Test L1 Norm: 0.3543, Train Linf Norm: 18.0259, Test Linf Norm: 7.8688\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:22:14,918]\u001b[0m Trial 167 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.2852, Test Loss: 0.1883, Train L1 Norm: 0.2895, Test L1 Norm: 0.1001, Train Linf Norm: 24.0506, Test Linf Norm: 0.9718\n",
            "Epoch 1: Train Loss: 0.1292, Test Loss: 0.0006, Train L1 Norm: 0.1544, Test L1 Norm: 0.0660, Train Linf Norm: 14.1698, Test Linf Norm: 8.3851\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:22:19,562]\u001b[0m Trial 168 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Train Loss: 0.0007, Test Loss: 0.0018, Train L1 Norm: 0.0304, Test L1 Norm: 0.1180, Train Linf Norm: 2.4504, Test Linf Norm: 15.0856\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:22:21,497]\u001b[0m Trial 169 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0482, Test Loss: 0.0022, Train L1 Norm: 0.3381, Test L1 Norm: 0.2739, Train Linf Norm: 36.0683, Test Linf Norm: 39.3808\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:22:23,184]\u001b[0m Trial 170 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 3.3659, Test Loss: 2.9774, Train L1 Norm: 1.4347, Test L1 Norm: 1.0000, Train Linf Norm: 15.2874, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.0468, Test Loss: 0.0001, Train L1 Norm: 0.1015, Test L1 Norm: 0.0238, Train Linf Norm: 3.6636, Test Linf Norm: 1.0916\n",
            "Epoch 2: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0172, Test L1 Norm: 0.0072, Train Linf Norm: 0.6855, Test Linf Norm: 0.1926\n",
            "Epoch 3: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0106, Test L1 Norm: 0.0057, Train Linf Norm: 0.4245, Test Linf Norm: 0.1542\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0089, Test L1 Norm: 0.0051, Train Linf Norm: 0.3678, Test Linf Norm: 0.1374\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0059, Train Linf Norm: 0.3418, Test Linf Norm: 0.1818\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0073, Test L1 Norm: 0.0046, Train Linf Norm: 0.3087, Test Linf Norm: 0.1351\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0044, Train Linf Norm: 0.2792, Test Linf Norm: 0.1315\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0042, Train Linf Norm: 0.2594, Test Linf Norm: 0.1275\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0042, Train Linf Norm: 0.2429, Test Linf Norm: 0.1273\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0040, Train Linf Norm: 0.2407, Test Linf Norm: 0.1209\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0040, Train Linf Norm: 0.2304, Test Linf Norm: 0.1215\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0039, Train Linf Norm: 0.2372, Test Linf Norm: 0.1147\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0039, Train Linf Norm: 0.2350, Test Linf Norm: 0.1146\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0039, Train Linf Norm: 0.2337, Test Linf Norm: 0.1146\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0039, Train Linf Norm: 0.2344, Test Linf Norm: 0.1151\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0039, Train Linf Norm: 0.2406, Test Linf Norm: 0.1180\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0039, Train Linf Norm: 0.2308, Test Linf Norm: 0.1171\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0039, Train Linf Norm: 0.2292, Test Linf Norm: 0.1189\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0038, Train Linf Norm: 0.2200, Test Linf Norm: 0.1147\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0039, Train Linf Norm: 0.2157, Test Linf Norm: 0.1214\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0037, Train Linf Norm: 0.2199, Test Linf Norm: 0.1117\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0038, Train Linf Norm: 0.2223, Test Linf Norm: 0.1213\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0037, Train Linf Norm: 0.1821, Test Linf Norm: 0.1120\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0044, Train Linf Norm: 0.1977, Test Linf Norm: 0.1403\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0045, Train Linf Norm: 0.2080, Test Linf Norm: 0.1581\n",
            "Epoch 26: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0036, Train Linf Norm: 0.2331, Test Linf Norm: 0.1114\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0129, Train Linf Norm: 0.1972, Test Linf Norm: 0.6477\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0036, Train Linf Norm: 0.2036, Test Linf Norm: 0.1131\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0046, Train Linf Norm: 0.1575, Test Linf Norm: 0.1846\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0036, Train Linf Norm: 0.1538, Test Linf Norm: 0.1044\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0049, Train Linf Norm: 0.1536, Test Linf Norm: 0.1796\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0030, Train Linf Norm: 0.1505, Test Linf Norm: 0.0958\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0028, Train Linf Norm: 0.1495, Test Linf Norm: 0.0888\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0031, Train Linf Norm: 0.1357, Test Linf Norm: 0.1004\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0029, Train Linf Norm: 0.1421, Test Linf Norm: 0.0911\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0029, Train Linf Norm: 0.1359, Test Linf Norm: 0.0912\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0029, Train Linf Norm: 0.1306, Test Linf Norm: 0.0911\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0028, Train Linf Norm: 0.1333, Test Linf Norm: 0.0900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:24:45,670]\u001b[0m Trial 171 finished with value: 0.002873110597184859 and parameters: {'n_layers': 3, 'n_units_0': 1002, 'n_units_1': 614, 'n_units_2': 1014, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'Huber', 'optimizer': 'Adagrad', 'lr': 0.0010230044505180759, 'batch_size': 66, 'n_epochs': 39, 'scheduler': 'CosineAnnealingLR', 'T_max': 13}. Best is trial 120 with value: 0.0022177403999259697.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0029, Train Linf Norm: 0.1306, Test Linf Norm: 0.0915\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:24:52,496]\u001b[0m Trial 172 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0001, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:24:54,865]\u001b[0m Trial 173 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:24:56,925]\u001b[0m Trial 174 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0846, Test Loss: 0.0005, Train L1 Norm: 0.2743, Test L1 Norm: 0.1284, Train Linf Norm: 34.0013, Test Linf Norm: 24.8373\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:24:58,551]\u001b[0m Trial 175 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.9522, Test Loss: 2.9774, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.0460, Test Loss: 0.0014, Train L1 Norm: 0.1365, Test L1 Norm: 0.0278, Train Linf Norm: 9.0962, Test Linf Norm: 0.8531\n",
            "Epoch 2: Train Loss: 0.0006, Test Loss: 0.0001, Train L1 Norm: 0.0251, Test L1 Norm: 0.0110, Train Linf Norm: 1.7657, Test Linf Norm: 0.5938\n",
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0163, Test L1 Norm: 0.0094, Train Linf Norm: 1.2521, Test Linf Norm: 0.4544\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0111, Test L1 Norm: 0.0080, Train Linf Norm: 0.7977, Test Linf Norm: 0.4562\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0090, Test L1 Norm: 0.0076, Train Linf Norm: 0.6217, Test Linf Norm: 0.4945\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0084, Test L1 Norm: 0.0074, Train Linf Norm: 0.6092, Test Linf Norm: 0.4970\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0070, Train Linf Norm: 0.5124, Test Linf Norm: 0.4379\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0065, Train Linf Norm: 0.5120, Test Linf Norm: 0.4399\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0065, Train Linf Norm: 0.4887, Test Linf Norm: 0.4489\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0064, Train Linf Norm: 0.4857, Test Linf Norm: 0.4383\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0064, Train Linf Norm: 0.4797, Test Linf Norm: 0.4327\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0064, Train Linf Norm: 0.4624, Test Linf Norm: 0.4367\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0064, Train Linf Norm: 0.4841, Test Linf Norm: 0.4372\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0064, Train Linf Norm: 0.4671, Test Linf Norm: 0.4372\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0064, Train Linf Norm: 0.4651, Test Linf Norm: 0.4360\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0064, Train Linf Norm: 0.4670, Test Linf Norm: 0.4307\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0063, Train Linf Norm: 0.4744, Test Linf Norm: 0.4298\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0063, Train Linf Norm: 0.4600, Test Linf Norm: 0.4248\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0062, Train Linf Norm: 0.4666, Test Linf Norm: 0.4216\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0061, Train Linf Norm: 0.4668, Test Linf Norm: 0.4192\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0060, Train Linf Norm: 0.4548, Test Linf Norm: 0.4062\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0060, Train Linf Norm: 0.4456, Test Linf Norm: 0.3764\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0057, Train Linf Norm: 0.4463, Test Linf Norm: 0.4035\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0059, Train Linf Norm: 0.4966, Test Linf Norm: 0.4137\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0059, Train Linf Norm: 0.4653, Test Linf Norm: 0.4148\n",
            "Epoch 26: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0103, Test L1 Norm: 0.0354, Train Linf Norm: 0.6292, Test Linf Norm: 4.0376\n",
            "Epoch 27: Train Loss: 0.0006, Test Loss: 0.0000, Train L1 Norm: 0.0107, Test L1 Norm: 0.0069, Train Linf Norm: 0.5545, Test Linf Norm: 0.4411\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0057, Train Linf Norm: 0.5934, Test Linf Norm: 0.3757\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0053, Train Linf Norm: 0.4285, Test Linf Norm: 0.3480\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0051, Train Linf Norm: 0.3755, Test Linf Norm: 0.3088\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0050, Train Linf Norm: 0.3919, Test Linf Norm: 0.3184\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0050, Train Linf Norm: 0.3803, Test Linf Norm: 0.3054\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0047, Train Linf Norm: 0.3577, Test Linf Norm: 0.2960\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0048, Train Linf Norm: 0.3575, Test Linf Norm: 0.3127\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0048, Train Linf Norm: 0.3629, Test Linf Norm: 0.3013\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0048, Train Linf Norm: 0.3495, Test Linf Norm: 0.3012\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0047, Train Linf Norm: 0.3478, Test Linf Norm: 0.3098\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0047, Train Linf Norm: 0.3420, Test Linf Norm: 0.3053\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0047, Train Linf Norm: 0.3475, Test Linf Norm: 0.3078\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0047, Train Linf Norm: 0.3392, Test Linf Norm: 0.3078\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0047, Train Linf Norm: 0.3347, Test Linf Norm: 0.3081\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0048, Train Linf Norm: 0.3443, Test Linf Norm: 0.3126\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0047, Train Linf Norm: 0.3406, Test Linf Norm: 0.3094\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0046, Train Linf Norm: 0.3428, Test Linf Norm: 0.3000\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0047, Train Linf Norm: 0.3397, Test Linf Norm: 0.2962\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0061, Train Linf Norm: 0.3450, Test Linf Norm: 0.5361\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0046, Train Linf Norm: 0.3501, Test Linf Norm: 0.2926\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0046, Train Linf Norm: 0.3345, Test Linf Norm: 0.2894\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0071, Train Linf Norm: 0.3421, Test Linf Norm: 0.6702\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0052, Train Linf Norm: 0.3717, Test Linf Norm: 0.4164\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0056, Train Linf Norm: 0.3798, Test Linf Norm: 0.4684\n",
            "Epoch 52: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0058, Train Linf Norm: 0.4799, Test Linf Norm: 0.4849\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0051, Train Linf Norm: 0.3962, Test Linf Norm: 0.2643\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0052, Test L1 Norm: 0.0168, Train Linf Norm: 0.3509, Test Linf Norm: 1.9668\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0100, Train Linf Norm: 0.3190, Test Linf Norm: 1.1734\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0064, Train Linf Norm: 0.3507, Test Linf Norm: 0.6201\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0083, Train Linf Norm: 0.3123, Test Linf Norm: 0.9315\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0084, Train Linf Norm: 0.3148, Test Linf Norm: 0.9513\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0081, Train Linf Norm: 0.3167, Test Linf Norm: 0.9088\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0068, Train Linf Norm: 0.3024, Test Linf Norm: 0.6966\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0078, Train Linf Norm: 0.3012, Test Linf Norm: 0.8699\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0084, Train Linf Norm: 0.3050, Test Linf Norm: 0.9491\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0071, Train Linf Norm: 0.3030, Test Linf Norm: 0.7448\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0083, Train Linf Norm: 0.3130, Test Linf Norm: 0.9344\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0080, Train Linf Norm: 0.2979, Test Linf Norm: 0.8934\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0080, Train Linf Norm: 0.3012, Test Linf Norm: 0.8934\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0080, Train Linf Norm: 0.3033, Test Linf Norm: 0.9015\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0077, Train Linf Norm: 0.3100, Test Linf Norm: 0.8575\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0074, Train Linf Norm: 0.3022, Test Linf Norm: 0.8054\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0079, Train Linf Norm: 0.3046, Test Linf Norm: 0.8770\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0084, Train Linf Norm: 0.2952, Test Linf Norm: 0.9602\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0095, Train Linf Norm: 0.3011, Test Linf Norm: 1.1322\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0088, Train Linf Norm: 0.2946, Test Linf Norm: 1.0246\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0064, Train Linf Norm: 0.3231, Test Linf Norm: 0.6236\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0093, Train Linf Norm: 0.2970, Test Linf Norm: 1.1148\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0103, Train Linf Norm: 0.3108, Test Linf Norm: 1.2576\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0093, Train Linf Norm: 0.3070, Test Linf Norm: 1.1197\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0097, Train Linf Norm: 0.4972, Test Linf Norm: 1.1847\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0122, Train Linf Norm: 0.3813, Test Linf Norm: 1.5416\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0088, Train Linf Norm: 0.3233, Test Linf Norm: 1.0517\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0042, Test L1 Norm: 0.0096, Train Linf Norm: 0.2849, Test Linf Norm: 1.1831\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0088, Train Linf Norm: 0.2940, Test Linf Norm: 1.0417\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0080, Train Linf Norm: 0.2900, Test Linf Norm: 0.8971\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:27:43,474]\u001b[0m Trial 176 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0104, Train Linf Norm: 0.2973, Test Linf Norm: 1.3054\n",
            "Epoch 1: Train Loss: 0.0513, Test Loss: 0.0004, Train L1 Norm: 0.2002, Test L1 Norm: 0.0526, Train Linf Norm: 20.1788, Test Linf Norm: 6.7396\n",
            "Epoch 2: Train Loss: 0.0009, Test Loss: 0.0001, Train L1 Norm: 0.0314, Test L1 Norm: 0.0169, Train Linf Norm: 2.6953, Test Linf Norm: 1.4870\n",
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0214, Test L1 Norm: 0.0157, Train Linf Norm: 2.2165, Test Linf Norm: 1.2697\n",
            "Epoch 4: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0143, Test L1 Norm: 0.0166, Train Linf Norm: 1.3037, Test Linf Norm: 1.3125\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0120, Test L1 Norm: 0.0085, Train Linf Norm: 1.1525, Test Linf Norm: 0.6503\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0075, Train Linf Norm: 0.9509, Test Linf Norm: 0.5117\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0098, Test L1 Norm: 0.0071, Train Linf Norm: 0.9237, Test Linf Norm: 0.5350\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0089, Test L1 Norm: 0.0072, Train Linf Norm: 0.7793, Test Linf Norm: 0.5613\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0083, Test L1 Norm: 0.0067, Train Linf Norm: 0.7436, Test Linf Norm: 0.5018\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0068, Train Linf Norm: 0.7412, Test Linf Norm: 0.5343\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0065, Train Linf Norm: 0.7514, Test Linf Norm: 0.5003\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0079, Test L1 Norm: 0.0065, Train Linf Norm: 0.7099, Test Linf Norm: 0.5039\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0065, Train Linf Norm: 0.6986, Test Linf Norm: 0.5096\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0065, Train Linf Norm: 0.7066, Test Linf Norm: 0.5060\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0065, Train Linf Norm: 0.6896, Test Linf Norm: 0.5060\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0065, Train Linf Norm: 0.6934, Test Linf Norm: 0.5001\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0067, Train Linf Norm: 0.6878, Test Linf Norm: 0.5350\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0065, Train Linf Norm: 0.6991, Test Linf Norm: 0.5149\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0064, Train Linf Norm: 0.6525, Test Linf Norm: 0.4938\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0063, Train Linf Norm: 0.6778, Test Linf Norm: 0.4872\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0073, Test L1 Norm: 0.0063, Train Linf Norm: 0.6422, Test Linf Norm: 0.5003\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0062, Train Linf Norm: 0.6267, Test Linf Norm: 0.4749\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0059, Train Linf Norm: 0.5887, Test Linf Norm: 0.4602\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0061, Train Linf Norm: 0.5957, Test Linf Norm: 0.5157\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0077, Train Linf Norm: 0.5347, Test Linf Norm: 0.6871\n",
            "Epoch 26: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0086, Test L1 Norm: 0.0049, Train Linf Norm: 0.6096, Test Linf Norm: 0.3333\n",
            "Epoch 27: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0097, Test L1 Norm: 0.0053, Train Linf Norm: 0.6898, Test Linf Norm: 0.4286\n",
            "Epoch 28: Train Loss: 0.0003, Test Loss: 0.0004, Train L1 Norm: 0.0142, Test L1 Norm: 0.0466, Train Linf Norm: 1.1022, Test Linf Norm: 5.6725\n",
            "Epoch 29: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0051, Train Linf Norm: 0.6268, Test Linf Norm: 0.3407\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0079, Test L1 Norm: 0.0051, Train Linf Norm: 0.6223, Test Linf Norm: 0.4139\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0051, Train Linf Norm: 0.5060, Test Linf Norm: 0.3845\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0044, Train Linf Norm: 0.5575, Test Linf Norm: 0.3256\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0043, Train Linf Norm: 0.5125, Test Linf Norm: 0.3097\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0041, Train Linf Norm: 0.4786, Test Linf Norm: 0.2937\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0041, Train Linf Norm: 0.4828, Test Linf Norm: 0.2918\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0040, Train Linf Norm: 0.4318, Test Linf Norm: 0.2847\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0040, Train Linf Norm: 0.4134, Test Linf Norm: 0.3038\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0041, Train Linf Norm: 0.4259, Test Linf Norm: 0.2942\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0039, Train Linf Norm: 0.4378, Test Linf Norm: 0.2778\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0040, Train Linf Norm: 0.4177, Test Linf Norm: 0.2871\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0038, Train Linf Norm: 0.4133, Test Linf Norm: 0.2708\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0039, Train Linf Norm: 0.4113, Test Linf Norm: 0.2759\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0039, Train Linf Norm: 0.4185, Test Linf Norm: 0.2759\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0039, Train Linf Norm: 0.4094, Test Linf Norm: 0.2798\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:29:13,440]\u001b[0m Trial 177 finished with value: 0.003884400693711359 and parameters: {'n_layers': 3, 'n_units_0': 970, 'n_units_1': 651, 'n_units_2': 860, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'LogCosh', 'optimizer': 'Adagrad', 'lr': 0.00077263292425796, 'batch_size': 171, 'n_epochs': 45, 'scheduler': 'CosineAnnealingLR', 'T_max': 14}. Best is trial 120 with value: 0.0022177403999259697.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0039, Train Linf Norm: 0.4087, Test Linf Norm: 0.2780\n",
            "Epoch 1: Train Loss: 0.0127, Test Loss: 0.0007, Train L1 Norm: 0.0854, Test L1 Norm: 0.0238, Train Linf Norm: 2.5573, Test Linf Norm: 0.5308\n",
            "Epoch 2: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0255, Test L1 Norm: 0.0139, Train Linf Norm: 0.8662, Test Linf Norm: 0.3916\n",
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0170, Test L1 Norm: 0.0245, Train Linf Norm: 0.5531, Test Linf Norm: 0.9381\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0151, Test L1 Norm: 0.0197, Train Linf Norm: 0.5076, Test Linf Norm: 0.7485\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0118, Test L1 Norm: 0.0172, Train Linf Norm: 0.3841, Test Linf Norm: 0.6464\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0105, Train Linf Norm: 0.3362, Test Linf Norm: 0.3568\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0086, Train Linf Norm: 0.2952, Test Linf Norm: 0.2716\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0089, Test L1 Norm: 0.0083, Train Linf Norm: 0.2798, Test Linf Norm: 0.2677\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0088, Test L1 Norm: 0.0092, Train Linf Norm: 0.2822, Test Linf Norm: 0.3120\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0086, Test L1 Norm: 0.0079, Train Linf Norm: 0.2757, Test Linf Norm: 0.2515\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0086, Test L1 Norm: 0.0084, Train Linf Norm: 0.2771, Test Linf Norm: 0.2764\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0084, Test L1 Norm: 0.0074, Train Linf Norm: 0.2699, Test Linf Norm: 0.2311\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0083, Test L1 Norm: 0.0073, Train Linf Norm: 0.2641, Test Linf Norm: 0.2282\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0076, Train Linf Norm: 0.2635, Test Linf Norm: 0.2407\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0076, Train Linf Norm: 0.2617, Test Linf Norm: 0.2407\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0075, Train Linf Norm: 0.2604, Test Linf Norm: 0.2367\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0083, Test L1 Norm: 0.0072, Train Linf Norm: 0.2649, Test Linf Norm: 0.2234\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0073, Train Linf Norm: 0.2608, Test Linf Norm: 0.2258\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0087, Train Linf Norm: 0.2568, Test Linf Norm: 0.2900\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0075, Train Linf Norm: 0.2593, Test Linf Norm: 0.2365\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0079, Test L1 Norm: 0.0073, Train Linf Norm: 0.2526, Test Linf Norm: 0.2303\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0069, Train Linf Norm: 0.2399, Test Linf Norm: 0.2165\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0079, Test L1 Norm: 0.0065, Train Linf Norm: 0.2537, Test Linf Norm: 0.1986\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0086, Train Linf Norm: 0.2378, Test Linf Norm: 0.2961\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0060, Train Linf Norm: 0.2345, Test Linf Norm: 0.1821\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0091, Train Linf Norm: 0.2170, Test Linf Norm: 0.3195\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0072, Test L1 Norm: 0.0088, Train Linf Norm: 0.2155, Test Linf Norm: 0.3062\n",
            "Epoch 28: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0058, Train Linf Norm: 0.2234, Test Linf Norm: 0.1755\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0058, Train Linf Norm: 0.2013, Test Linf Norm: 0.1724\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0104, Train Linf Norm: 0.1769, Test Linf Norm: 0.3849\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0049, Train Linf Norm: 0.1824, Test Linf Norm: 0.1368\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0071, Train Linf Norm: 0.1602, Test Linf Norm: 0.2415\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0049, Train Linf Norm: 0.1697, Test Linf Norm: 0.1471\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0059, Train Linf Norm: 0.1677, Test Linf Norm: 0.1919\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0068, Train Linf Norm: 0.1561, Test Linf Norm: 0.2294\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0045, Train Linf Norm: 0.1549, Test Linf Norm: 0.1281\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0054, Train Linf Norm: 0.1515, Test Linf Norm: 0.1752\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0049, Train Linf Norm: 0.1462, Test Linf Norm: 0.1497\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0053, Train Linf Norm: 0.1481, Test Linf Norm: 0.1665\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0046, Train Linf Norm: 0.1479, Test Linf Norm: 0.1363\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0051, Train Linf Norm: 0.1469, Test Linf Norm: 0.1583\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0052, Train Linf Norm: 0.1472, Test Linf Norm: 0.1641\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0052, Train Linf Norm: 0.1482, Test Linf Norm: 0.1641\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0051, Train Linf Norm: 0.1463, Test Linf Norm: 0.1592\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0053, Train Linf Norm: 0.1485, Test Linf Norm: 0.1662\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0051, Train Linf Norm: 0.1471, Test Linf Norm: 0.1596\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:33:18,338]\u001b[0m Trial 178 finished with value: 0.004544685333094094 and parameters: {'n_layers': 3, 'n_units_0': 973, 'n_units_1': 577, 'n_units_2': 225, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'LogCosh', 'optimizer': 'Adagrad', 'lr': 0.0007619844629046888, 'batch_size': 45, 'n_epochs': 47, 'scheduler': 'CosineAnnealingLR', 'T_max': 14}. Best is trial 120 with value: 0.0022177403999259697.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0045, Train Linf Norm: 0.1492, Test Linf Norm: 0.1322\n",
            "Epoch 1: Train Loss: 0.0258, Test Loss: 0.0013, Train L1 Norm: 0.1625, Test L1 Norm: 0.0450, Train Linf Norm: 10.0692, Test Linf Norm: 2.2750\n",
            "Epoch 2: Train Loss: 0.0003, Test Loss: 0.0003, Train L1 Norm: 0.0349, Test L1 Norm: 0.0340, Train Linf Norm: 2.1046, Test Linf Norm: 2.0591\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:33:27,303]\u001b[0m Trial 179 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0227, Test L1 Norm: 0.0360, Train Linf Norm: 1.3701, Test Linf Norm: 2.8208\n",
            "Epoch 1: Train Loss: 0.0154, Test Loss: 0.0001, Train L1 Norm: 0.1045, Test L1 Norm: 0.0351, Train Linf Norm: 4.3211, Test Linf Norm: 1.6144\n",
            "Epoch 2: Train Loss: 0.0001, Test Loss: 0.0003, Train L1 Norm: 0.0217, Test L1 Norm: 0.0185, Train Linf Norm: 0.8770, Test Linf Norm: 0.6044\n",
            "Epoch 3: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0158, Test L1 Norm: 0.0091, Train Linf Norm: 0.6310, Test Linf Norm: 0.2476\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0148, Test L1 Norm: 0.0092, Train Linf Norm: 0.6241, Test Linf Norm: 0.2085\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0149, Test L1 Norm: 0.0081, Train Linf Norm: 0.6650, Test Linf Norm: 0.2170\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0133, Test L1 Norm: 0.0069, Train Linf Norm: 0.5827, Test Linf Norm: 0.1987\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0126, Test L1 Norm: 0.0109, Train Linf Norm: 0.5545, Test Linf Norm: 0.4656\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0124, Test L1 Norm: 0.0058, Train Linf Norm: 0.5604, Test Linf Norm: 0.1636\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0109, Test L1 Norm: 0.0064, Train Linf Norm: 0.4832, Test Linf Norm: 0.2100\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0101, Test L1 Norm: 0.0068, Train Linf Norm: 0.4326, Test Linf Norm: 0.2356\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0097, Test L1 Norm: 0.0077, Train Linf Norm: 0.4104, Test Linf Norm: 0.2924\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0095, Test L1 Norm: 0.0055, Train Linf Norm: 0.4028, Test Linf Norm: 0.1617\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0075, Train Linf Norm: 0.3952, Test Linf Norm: 0.2910\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0092, Test L1 Norm: 0.0067, Train Linf Norm: 0.3895, Test Linf Norm: 0.2411\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0093, Test L1 Norm: 0.0070, Train Linf Norm: 0.3816, Test Linf Norm: 0.2593\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0092, Test L1 Norm: 0.0070, Train Linf Norm: 0.3888, Test Linf Norm: 0.2593\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0092, Test L1 Norm: 0.0070, Train Linf Norm: 0.3917, Test Linf Norm: 0.2597\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0092, Test L1 Norm: 0.0073, Train Linf Norm: 0.3923, Test Linf Norm: 0.2767\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0093, Test L1 Norm: 0.0072, Train Linf Norm: 0.3983, Test Linf Norm: 0.2724\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0091, Test L1 Norm: 0.0068, Train Linf Norm: 0.3818, Test Linf Norm: 0.2460\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0051, Train Linf Norm: 0.3984, Test Linf Norm: 0.1522\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0092, Test L1 Norm: 0.0074, Train Linf Norm: 0.3891, Test Linf Norm: 0.2897\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0088, Test L1 Norm: 0.0082, Train Linf Norm: 0.3748, Test Linf Norm: 0.3357\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0090, Test L1 Norm: 0.0074, Train Linf Norm: 0.3824, Test Linf Norm: 0.2987\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0057, Train Linf Norm: 0.3548, Test Linf Norm: 0.1880\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0084, Train Linf Norm: 0.3403, Test Linf Norm: 0.3576\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0089, Test L1 Norm: 0.0065, Train Linf Norm: 0.3856, Test Linf Norm: 0.2501\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0079, Test L1 Norm: 0.0068, Train Linf Norm: 0.3224, Test Linf Norm: 0.2614\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0086, Test L1 Norm: 0.0042, Train Linf Norm: 0.3589, Test Linf Norm: 0.1176\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0084, Test L1 Norm: 0.0112, Train Linf Norm: 0.3234, Test Linf Norm: 0.2291\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0050, Train Linf Norm: 0.3237, Test Linf Norm: 0.1665\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0169, Train Linf Norm: 0.2988, Test Linf Norm: 0.8255\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0048, Train Linf Norm: 0.3108, Test Linf Norm: 0.1673\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0063, Train Linf Norm: 0.2685, Test Linf Norm: 0.2564\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0039, Train Linf Norm: 0.2603, Test Linf Norm: 0.1062\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0058, Train Linf Norm: 0.2477, Test Linf Norm: 0.2325\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0043, Train Linf Norm: 0.2664, Test Linf Norm: 0.1305\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0036, Train Linf Norm: 0.2568, Test Linf Norm: 0.1024\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0037, Train Linf Norm: 0.2453, Test Linf Norm: 0.1083\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0044, Train Linf Norm: 0.2442, Test Linf Norm: 0.1532\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0037, Train Linf Norm: 0.2300, Test Linf Norm: 0.1073\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0038, Train Linf Norm: 0.2341, Test Linf Norm: 0.1152\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0040, Train Linf Norm: 0.2471, Test Linf Norm: 0.1300\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0036, Train Linf Norm: 0.2386, Test Linf Norm: 0.1019\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0036, Train Linf Norm: 0.2308, Test Linf Norm: 0.1026\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0036, Train Linf Norm: 0.2306, Test Linf Norm: 0.1026\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:36:37,832]\u001b[0m Trial 180 finished with value: 0.003594433604972437 and parameters: {'n_layers': 3, 'n_units_0': 1007, 'n_units_1': 499, 'n_units_2': 241, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'LogCosh', 'optimizer': 'Adagrad', 'lr': 0.0007026471213751687, 'batch_size': 60, 'n_epochs': 47, 'scheduler': 'CosineAnnealingLR', 'T_max': 15}. Best is trial 120 with value: 0.0022177403999259697.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0036, Train Linf Norm: 0.2331, Test Linf Norm: 0.1040\n",
            "Epoch 1: Train Loss: 0.0158, Test Loss: 0.0001, Train L1 Norm: 0.0825, Test L1 Norm: 0.0661, Train Linf Norm: 2.9904, Test Linf Norm: 3.4339\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:36:46,357]\u001b[0m Trial 181 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0214, Test L1 Norm: 0.0470, Train Linf Norm: 0.8730, Test Linf Norm: 2.4834\n",
            "Epoch 1: Train Loss: 0.0195, Test Loss: 0.0001, Train L1 Norm: 0.1246, Test L1 Norm: 0.0458, Train Linf Norm: 5.3933, Test Linf Norm: 2.3745\n",
            "Epoch 2: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0245, Test L1 Norm: 0.0151, Train Linf Norm: 1.0068, Test Linf Norm: 0.5370\n",
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0003, Train L1 Norm: 0.0172, Test L1 Norm: 0.0160, Train Linf Norm: 0.7165, Test Linf Norm: 0.4849\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0136, Test L1 Norm: 0.0162, Train Linf Norm: 0.5619, Test Linf Norm: 0.7755\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0118, Test L1 Norm: 0.0157, Train Linf Norm: 0.4949, Test Linf Norm: 0.7780\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:37:09,954]\u001b[0m Trial 182 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0113, Test L1 Norm: 0.0163, Train Linf Norm: 0.4843, Test Linf Norm: 0.8256\n",
            "Epoch 1: Train Loss: 0.0142, Test Loss: 0.0001, Train L1 Norm: 0.1243, Test L1 Norm: 0.0199, Train Linf Norm: 3.7810, Test Linf Norm: 0.4848\n",
            "Epoch 2: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0289, Test L1 Norm: 0.0110, Train Linf Norm: 0.8733, Test Linf Norm: 0.2624\n",
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0205, Test L1 Norm: 0.0092, Train Linf Norm: 0.6228, Test Linf Norm: 0.2249\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0159, Test L1 Norm: 0.0082, Train Linf Norm: 0.4899, Test Linf Norm: 0.2032\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0141, Test L1 Norm: 0.0069, Train Linf Norm: 0.4322, Test Linf Norm: 0.1583\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0127, Test L1 Norm: 0.0066, Train Linf Norm: 0.3910, Test Linf Norm: 0.1571\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0116, Test L1 Norm: 0.0060, Train Linf Norm: 0.3558, Test Linf Norm: 0.1380\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0110, Test L1 Norm: 0.0058, Train Linf Norm: 0.3346, Test Linf Norm: 0.1371\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0105, Test L1 Norm: 0.0058, Train Linf Norm: 0.3204, Test Linf Norm: 0.1338\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0102, Test L1 Norm: 0.0055, Train Linf Norm: 0.3082, Test Linf Norm: 0.1297\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0099, Test L1 Norm: 0.0059, Train Linf Norm: 0.3012, Test Linf Norm: 0.1418\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0095, Test L1 Norm: 0.0054, Train Linf Norm: 0.2825, Test Linf Norm: 0.1247\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0096, Test L1 Norm: 0.0054, Train Linf Norm: 0.2878, Test Linf Norm: 0.1231\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0095, Test L1 Norm: 0.0054, Train Linf Norm: 0.2831, Test Linf Norm: 0.1237\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0093, Test L1 Norm: 0.0053, Train Linf Norm: 0.2789, Test Linf Norm: 0.1216\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0053, Train Linf Norm: 0.2786, Test Linf Norm: 0.1216\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0053, Train Linf Norm: 0.2802, Test Linf Norm: 0.1213\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0053, Train Linf Norm: 0.2801, Test Linf Norm: 0.1219\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0054, Train Linf Norm: 0.2793, Test Linf Norm: 0.1236\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0093, Test L1 Norm: 0.0054, Train Linf Norm: 0.2770, Test Linf Norm: 0.1234\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0091, Test L1 Norm: 0.0054, Train Linf Norm: 0.2678, Test Linf Norm: 0.1248\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0089, Test L1 Norm: 0.0054, Train Linf Norm: 0.2617, Test Linf Norm: 0.1242\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0090, Test L1 Norm: 0.0053, Train Linf Norm: 0.2674, Test Linf Norm: 0.1241\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0053, Train Linf Norm: 0.2490, Test Linf Norm: 0.1247\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0053, Train Linf Norm: 0.2587, Test Linf Norm: 0.1234\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0083, Test L1 Norm: 0.0049, Train Linf Norm: 0.2440, Test Linf Norm: 0.1156\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0086, Test L1 Norm: 0.0051, Train Linf Norm: 0.2514, Test Linf Norm: 0.1171\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0052, Train Linf Norm: 0.2306, Test Linf Norm: 0.1226\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0087, Test L1 Norm: 0.0049, Train Linf Norm: 0.2550, Test Linf Norm: 0.1166\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0056, Train Linf Norm: 0.2208, Test Linf Norm: 0.1133\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0058, Train Linf Norm: 0.2276, Test Linf Norm: 0.1309\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0047, Train Linf Norm: 0.2185, Test Linf Norm: 0.1102\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0047, Train Linf Norm: 0.2165, Test Linf Norm: 0.1100\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0051, Train Linf Norm: 0.1942, Test Linf Norm: 0.1210\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0073, Test L1 Norm: 0.0044, Train Linf Norm: 0.2192, Test Linf Norm: 0.1034\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0043, Train Linf Norm: 0.2072, Test Linf Norm: 0.1010\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0042, Train Linf Norm: 0.1975, Test Linf Norm: 0.0984\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0042, Train Linf Norm: 0.1969, Test Linf Norm: 0.0976\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0043, Train Linf Norm: 0.1880, Test Linf Norm: 0.1029\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0042, Train Linf Norm: 0.1932, Test Linf Norm: 0.0983\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0041, Train Linf Norm: 0.1948, Test Linf Norm: 0.0966\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0042, Train Linf Norm: 0.1905, Test Linf Norm: 0.0984\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0042, Train Linf Norm: 0.1905, Test Linf Norm: 0.0984\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0042, Train Linf Norm: 0.1889, Test Linf Norm: 0.0984\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0042, Train Linf Norm: 0.1902, Test Linf Norm: 0.0986\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0042, Train Linf Norm: 0.1921, Test Linf Norm: 0.0986\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0041, Train Linf Norm: 0.1890, Test Linf Norm: 0.0980\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0042, Train Linf Norm: 0.1878, Test Linf Norm: 0.0984\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:41:48,184]\u001b[0m Trial 183 finished with value: 0.004155994103755802 and parameters: {'n_layers': 3, 'n_units_0': 996, 'n_units_1': 597, 'n_units_2': 279, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'LogCosh', 'optimizer': 'Adagrad', 'lr': 0.0006521404891972302, 'batch_size': 40, 'n_epochs': 49, 'scheduler': 'CosineAnnealingLR', 'T_max': 15}. Best is trial 120 with value: 0.0022177403999259697.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0042, Train Linf Norm: 0.1893, Test Linf Norm: 0.0983\n",
            "Epoch 1: Train Loss: 0.0135, Test Loss: 0.0000, Train L1 Norm: 0.0904, Test L1 Norm: 0.0440, Train Linf Norm: 2.3683, Test Linf Norm: 1.3893\n",
            "Epoch 2: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0182, Test L1 Norm: 0.0389, Train Linf Norm: 0.4448, Test Linf Norm: 1.2545\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:42:07,430]\u001b[0m Trial 184 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0132, Test L1 Norm: 0.0318, Train Linf Norm: 0.3259, Test Linf Norm: 1.0298\n",
            "Epoch 1: Train Loss: 0.0055, Test Loss: 0.0001, Train L1 Norm: 0.0522, Test L1 Norm: 0.0409, Train Linf Norm: 0.6298, Test Linf Norm: 0.5964\n",
            "Epoch 2: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0148, Test L1 Norm: 0.0223, Train Linf Norm: 0.1877, Test Linf Norm: 0.3206\n",
            "Epoch 3: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0133, Test L1 Norm: 0.0165, Train Linf Norm: 0.1730, Test Linf Norm: 0.2316\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0187, Train Linf Norm: 0.1169, Test Linf Norm: 0.2715\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0092, Test L1 Norm: 0.0148, Train Linf Norm: 0.1157, Test Linf Norm: 0.2073\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:43:26,777]\u001b[0m Trial 185 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0181, Train Linf Norm: 0.0958, Test Linf Norm: 0.2657\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:43:30,273]\u001b[0m Trial 186 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.0163, Test Loss: 0.0002, Train L1 Norm: 0.0900, Test L1 Norm: 0.0624, Train Linf Norm: 3.1521, Test Linf Norm: 2.8950\n",
            "Epoch 2: Train Loss: 0.0002, Test Loss: 0.0000, Train L1 Norm: 0.0222, Test L1 Norm: 0.0101, Train Linf Norm: 0.8277, Test Linf Norm: 0.2728\n",
            "Epoch 3: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0143, Test L1 Norm: 0.0100, Train Linf Norm: 0.5119, Test Linf Norm: 0.2667\n",
            "Epoch 4: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0126, Test L1 Norm: 0.0114, Train Linf Norm: 0.4484, Test Linf Norm: 0.3700\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0119, Test L1 Norm: 0.0092, Train Linf Norm: 0.4381, Test Linf Norm: 0.3030\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0121, Test L1 Norm: 0.0065, Train Linf Norm: 0.4696, Test Linf Norm: 0.1969\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0101, Test L1 Norm: 0.0076, Train Linf Norm: 0.3698, Test Linf Norm: 0.1841\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0066, Train Linf Norm: 0.2810, Test Linf Norm: 0.2151\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0003, Train L1 Norm: 0.0075, Test L1 Norm: 0.0221, Train Linf Norm: 0.2466, Test Linf Norm: 0.7912\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0052, Train Linf Norm: 0.2555, Test Linf Norm: 0.1587\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0053, Train Linf Norm: 0.2223, Test Linf Norm: 0.1600\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0045, Train Linf Norm: 0.2828, Test Linf Norm: 0.1276\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0054, Train Linf Norm: 0.2275, Test Linf Norm: 0.1578\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0046, Train Linf Norm: 0.2251, Test Linf Norm: 0.1359\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0041, Train Linf Norm: 0.2366, Test Linf Norm: 0.1181\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0038, Train Linf Norm: 0.1991, Test Linf Norm: 0.1044\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0038, Train Linf Norm: 0.1884, Test Linf Norm: 0.1098\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0054, Train Linf Norm: 0.1789, Test Linf Norm: 0.1809\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0046, Train Linf Norm: 0.1854, Test Linf Norm: 0.1496\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0043, Train Linf Norm: 0.2169, Test Linf Norm: 0.1340\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0045, Train Linf Norm: 0.1801, Test Linf Norm: 0.1399\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0037, Train Linf Norm: 0.1800, Test Linf Norm: 0.1063\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0037, Train Linf Norm: 0.1660, Test Linf Norm: 0.1107\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0041, Train Linf Norm: 0.1887, Test Linf Norm: 0.1284\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0043, Train Linf Norm: 0.1615, Test Linf Norm: 0.1006\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0040, Train Linf Norm: 0.1670, Test Linf Norm: 0.1258\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0041, Train Linf Norm: 0.1514, Test Linf Norm: 0.1158\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0048, Train Linf Norm: 0.1532, Test Linf Norm: 0.1663\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0036, Train Linf Norm: 0.1535, Test Linf Norm: 0.1033\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0044, Train Linf Norm: 0.1567, Test Linf Norm: 0.0974\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0038, Train Linf Norm: 0.1656, Test Linf Norm: 0.1176\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0048, Train Linf Norm: 0.1408, Test Linf Norm: 0.1706\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0037, Train Linf Norm: 0.1528, Test Linf Norm: 0.1163\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0038, Train Linf Norm: 0.1424, Test Linf Norm: 0.1199\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0043, Test L1 Norm: 0.0062, Train Linf Norm: 0.1417, Test Linf Norm: 0.1232\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0040, Train Linf Norm: 0.1469, Test Linf Norm: 0.1159\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0039, Train Linf Norm: 0.1594, Test Linf Norm: 0.1267\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0035, Train Linf Norm: 0.1486, Test Linf Norm: 0.1034\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0043, Train Linf Norm: 0.1484, Test Linf Norm: 0.1437\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0034, Train Linf Norm: 0.1325, Test Linf Norm: 0.1045\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0039, Train Linf Norm: 0.1326, Test Linf Norm: 0.1282\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0035, Train Linf Norm: 0.1246, Test Linf Norm: 0.1080\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0035, Train Linf Norm: 0.1241, Test Linf Norm: 0.1049\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0034, Train Linf Norm: 0.1494, Test Linf Norm: 0.1053\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0033, Train Linf Norm: 0.1165, Test Linf Norm: 0.1031\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0033, Train Linf Norm: 0.1256, Test Linf Norm: 0.0987\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0039, Test L1 Norm: 0.0034, Train Linf Norm: 0.1219, Test Linf Norm: 0.1007\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0036, Train Linf Norm: 0.1245, Test Linf Norm: 0.1153\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0034, Train Linf Norm: 0.1324, Test Linf Norm: 0.0904\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0034, Train Linf Norm: 0.1356, Test Linf Norm: 0.0928\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0036, Train Linf Norm: 0.1241, Test Linf Norm: 0.0988\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0035, Train Linf Norm: 0.1147, Test Linf Norm: 0.1101\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0035, Train Linf Norm: 0.1274, Test Linf Norm: 0.1143\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0086, Train Linf Norm: 0.1189, Test Linf Norm: 0.3341\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0032, Train Linf Norm: 0.1345, Test Linf Norm: 0.0903\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0064, Train Linf Norm: 0.1137, Test Linf Norm: 0.2605\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0035, Train Linf Norm: 0.1049, Test Linf Norm: 0.1119\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0033, Train Linf Norm: 0.1158, Test Linf Norm: 0.1028\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0034, Train Linf Norm: 0.1317, Test Linf Norm: 0.1088\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0031, Train Linf Norm: 0.1249, Test Linf Norm: 0.1010\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0033, Train Linf Norm: 0.1226, Test Linf Norm: 0.1111\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0031, Train Linf Norm: 0.1246, Test Linf Norm: 0.0975\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0033, Train Linf Norm: 0.1183, Test Linf Norm: 0.1069\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0036, Train Linf Norm: 0.1230, Test Linf Norm: 0.1248\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0034, Train Linf Norm: 0.1072, Test Linf Norm: 0.1091\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0042, Train Linf Norm: 0.1056, Test Linf Norm: 0.1546\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0059, Train Linf Norm: 0.1087, Test Linf Norm: 0.2385\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0036, Train Linf Norm: 0.1224, Test Linf Norm: 0.1212\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0038, Train Linf Norm: 0.1126, Test Linf Norm: 0.1056\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:48:43,577]\u001b[0m Trial 187 finished with value: 0.0028832548818551003 and parameters: {'n_layers': 3, 'n_units_0': 1047, 'n_units_1': 437, 'n_units_2': 715, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'LogCosh', 'optimizer': 'Adagrad', 'lr': 0.0006392640995120569, 'batch_size': 53, 'n_epochs': 70, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.6053966533084814, 'patience': 12, 'threshold': 0.008430610490808689}. Best is trial 120 with value: 0.0022177403999259697.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0029, Train Linf Norm: 0.1184, Test Linf Norm: 0.0830\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:48:49,195]\u001b[0m Trial 188 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:48:55,017]\u001b[0m Trial 189 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0115, Test Loss: 0.0002, Train L1 Norm: 0.7459, Test L1 Norm: 0.6403, Train Linf Norm: 25.8192, Test Linf Norm: 23.6968\n",
            "Epoch 1: Train Loss: 0.0122, Test Loss: 0.0002, Train L1 Norm: 0.0862, Test L1 Norm: 0.0149, Train Linf Norm: 3.0530, Test Linf Norm: 0.2862\n",
            "Epoch 2: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0213, Test L1 Norm: 0.0091, Train Linf Norm: 0.7630, Test Linf Norm: 0.2241\n",
            "Epoch 3: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0164, Test L1 Norm: 0.0086, Train Linf Norm: 0.5940, Test Linf Norm: 0.2170\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0131, Test L1 Norm: 0.0094, Train Linf Norm: 0.4625, Test Linf Norm: 0.2401\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0003, Train L1 Norm: 0.0113, Test L1 Norm: 0.0129, Train Linf Norm: 0.3858, Test Linf Norm: 0.2276\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0105, Test L1 Norm: 0.0060, Train Linf Norm: 0.3635, Test Linf Norm: 0.1572\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0058, Train Linf Norm: 0.3683, Test Linf Norm: 0.1528\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0055, Train Linf Norm: 0.3229, Test Linf Norm: 0.1431\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0090, Test L1 Norm: 0.0054, Train Linf Norm: 0.3144, Test Linf Norm: 0.1455\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0088, Test L1 Norm: 0.0058, Train Linf Norm: 0.3089, Test Linf Norm: 0.1546\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0084, Test L1 Norm: 0.0051, Train Linf Norm: 0.2966, Test Linf Norm: 0.1363\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0049, Train Linf Norm: 0.2702, Test Linf Norm: 0.1296\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0048, Train Linf Norm: 0.2680, Test Linf Norm: 0.1269\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0049, Train Linf Norm: 0.2621, Test Linf Norm: 0.1338\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0046, Train Linf Norm: 0.2682, Test Linf Norm: 0.1208\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0073, Test L1 Norm: 0.0046, Train Linf Norm: 0.2527, Test Linf Norm: 0.1247\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0044, Train Linf Norm: 0.2764, Test Linf Norm: 0.1176\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0048, Train Linf Norm: 0.2444, Test Linf Norm: 0.1260\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0043, Train Linf Norm: 0.2422, Test Linf Norm: 0.1162\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0041, Train Linf Norm: 0.2302, Test Linf Norm: 0.1146\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0042, Train Linf Norm: 0.2438, Test Linf Norm: 0.1136\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0051, Train Linf Norm: 0.2397, Test Linf Norm: 0.1257\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0043, Train Linf Norm: 0.2347, Test Linf Norm: 0.1173\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0042, Train Linf Norm: 0.2217, Test Linf Norm: 0.1134\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0041, Train Linf Norm: 0.2136, Test Linf Norm: 0.1099\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0039, Train Linf Norm: 0.2232, Test Linf Norm: 0.1063\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0040, Train Linf Norm: 0.2099, Test Linf Norm: 0.1093\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0037, Train Linf Norm: 0.2154, Test Linf Norm: 0.1024\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0038, Train Linf Norm: 0.1916, Test Linf Norm: 0.1063\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0038, Train Linf Norm: 0.2208, Test Linf Norm: 0.1021\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0039, Train Linf Norm: 0.1984, Test Linf Norm: 0.1078\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0039, Train Linf Norm: 0.1984, Test Linf Norm: 0.1057\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0036, Train Linf Norm: 0.2004, Test Linf Norm: 0.1012\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0036, Train Linf Norm: 0.1982, Test Linf Norm: 0.0998\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0035, Train Linf Norm: 0.2044, Test Linf Norm: 0.0960\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0038, Train Linf Norm: 0.1997, Test Linf Norm: 0.0991\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0038, Train Linf Norm: 0.1981, Test Linf Norm: 0.1028\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0035, Train Linf Norm: 0.2069, Test Linf Norm: 0.0982\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0036, Train Linf Norm: 0.1923, Test Linf Norm: 0.0997\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0035, Train Linf Norm: 0.1843, Test Linf Norm: 0.0981\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0041, Train Linf Norm: 0.1962, Test Linf Norm: 0.1047\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0034, Train Linf Norm: 0.1978, Test Linf Norm: 0.0960\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0033, Train Linf Norm: 0.1980, Test Linf Norm: 0.0946\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0036, Train Linf Norm: 0.1796, Test Linf Norm: 0.1018\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0034, Train Linf Norm: 0.1875, Test Linf Norm: 0.0974\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0035, Train Linf Norm: 0.1930, Test Linf Norm: 0.0963\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0035, Train Linf Norm: 0.1877, Test Linf Norm: 0.0988\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0036, Train Linf Norm: 0.1759, Test Linf Norm: 0.0983\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0035, Train Linf Norm: 0.1769, Test Linf Norm: 0.0972\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0034, Train Linf Norm: 0.1735, Test Linf Norm: 0.0955\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0033, Train Linf Norm: 0.1791, Test Linf Norm: 0.0942\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0035, Train Linf Norm: 0.1776, Test Linf Norm: 0.0943\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0032, Train Linf Norm: 0.1728, Test Linf Norm: 0.0919\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0033, Train Linf Norm: 0.1662, Test Linf Norm: 0.0944\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0032, Train Linf Norm: 0.1829, Test Linf Norm: 0.0898\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0035, Train Linf Norm: 0.1662, Test Linf Norm: 0.0949\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0034, Train Linf Norm: 0.1715, Test Linf Norm: 0.0950\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0032, Train Linf Norm: 0.1612, Test Linf Norm: 0.0899\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0033, Train Linf Norm: 0.1612, Test Linf Norm: 0.0941\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0033, Train Linf Norm: 0.1699, Test Linf Norm: 0.0902\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:53:35,084]\u001b[0m Trial 190 finished with value: 0.0029981144120218234 and parameters: {'n_layers': 3, 'n_units_0': 1007, 'n_units_1': 639, 'n_units_2': 670, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'LogCosh', 'optimizer': 'Adagrad', 'lr': 0.00022953044453348335, 'batch_size': 52, 'n_epochs': 61, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.7319458541167466, 'patience': 11, 'threshold': 0.006223297411926679}. Best is trial 120 with value: 0.0022177403999259697.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0030, Train Linf Norm: 0.1690, Test Linf Norm: 0.0860\n",
            "Epoch 1: Train Loss: 0.0137, Test Loss: 0.0004, Train L1 Norm: 0.1094, Test L1 Norm: 0.0459, Train Linf Norm: 4.0794, Test Linf Norm: 1.8121\n",
            "Epoch 2: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0218, Test L1 Norm: 0.0268, Train Linf Norm: 0.7142, Test Linf Norm: 1.0682\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:53:48,889]\u001b[0m Trial 191 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0154, Test L1 Norm: 0.0218, Train Linf Norm: 0.4774, Test Linf Norm: 0.8772\n",
            "Epoch 1: Train Loss: 0.0188, Test Loss: 0.0003, Train L1 Norm: 0.0792, Test L1 Norm: 0.0211, Train Linf Norm: 2.4339, Test Linf Norm: 0.5474\n",
            "Epoch 2: Train Loss: 0.0002, Test Loss: 0.0002, Train L1 Norm: 0.0218, Test L1 Norm: 0.0172, Train Linf Norm: 0.6873, Test Linf Norm: 0.3984\n",
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0172, Test L1 Norm: 0.0111, Train Linf Norm: 0.5828, Test Linf Norm: 0.3023\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0180, Test L1 Norm: 0.0109, Train Linf Norm: 0.6776, Test Linf Norm: 0.2760\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0131, Test L1 Norm: 0.0092, Train Linf Norm: 0.4470, Test Linf Norm: 0.2575\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0120, Test L1 Norm: 0.0100, Train Linf Norm: 0.4111, Test Linf Norm: 0.2681\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0110, Test L1 Norm: 0.0081, Train Linf Norm: 0.3727, Test Linf Norm: 0.2299\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0103, Test L1 Norm: 0.0086, Train Linf Norm: 0.3476, Test Linf Norm: 0.2683\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0156, Train Linf Norm: 0.3430, Test Linf Norm: 0.6789\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0097, Test L1 Norm: 0.0149, Train Linf Norm: 0.3363, Test Linf Norm: 0.6529\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0095, Test L1 Norm: 0.0105, Train Linf Norm: 0.3358, Test Linf Norm: 0.4062\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0090, Test L1 Norm: 0.0129, Train Linf Norm: 0.3181, Test Linf Norm: 0.5438\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0086, Test L1 Norm: 0.0121, Train Linf Norm: 0.2975, Test Linf Norm: 0.5062\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0086, Test L1 Norm: 0.0130, Train Linf Norm: 0.3018, Test Linf Norm: 0.5635\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0138, Train Linf Norm: 0.2829, Test Linf Norm: 0.6117\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0086, Test L1 Norm: 0.0139, Train Linf Norm: 0.3107, Test Linf Norm: 0.6322\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0112, Train Linf Norm: 0.2821, Test Linf Norm: 0.4792\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0142, Train Linf Norm: 0.2850, Test Linf Norm: 0.6570\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0124, Train Linf Norm: 0.2881, Test Linf Norm: 0.5526\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:55:11,887]\u001b[0m Trial 192 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0111, Train Linf Norm: 0.2749, Test Linf Norm: 0.4764\n",
            "Epoch 1: Train Loss: 0.0164, Test Loss: 0.0001, Train L1 Norm: 0.1029, Test L1 Norm: 0.0249, Train Linf Norm: 4.3321, Test Linf Norm: 1.0931\n",
            "Epoch 2: Train Loss: 0.0001, Test Loss: 0.0002, Train L1 Norm: 0.0227, Test L1 Norm: 0.0130, Train Linf Norm: 1.0202, Test Linf Norm: 0.3332\n",
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0188, Test L1 Norm: 0.0108, Train Linf Norm: 0.8487, Test Linf Norm: 0.2766\n",
            "Epoch 4: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0139, Test L1 Norm: 0.0085, Train Linf Norm: 0.5823, Test Linf Norm: 0.3160\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0007, Train L1 Norm: 0.0122, Test L1 Norm: 0.0339, Train Linf Norm: 0.5321, Test Linf Norm: 1.5302\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0002, Train L1 Norm: 0.0119, Test L1 Norm: 0.0402, Train Linf Norm: 0.5499, Test Linf Norm: 2.1448\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0065, Train Linf Norm: 0.4641, Test Linf Norm: 0.2492\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0094, Test L1 Norm: 0.0057, Train Linf Norm: 0.4148, Test Linf Norm: 0.1760\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0092, Test L1 Norm: 0.0077, Train Linf Norm: 0.4109, Test Linf Norm: 0.3195\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0095, Test L1 Norm: 0.0052, Train Linf Norm: 0.4377, Test Linf Norm: 0.1760\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0049, Train Linf Norm: 0.3469, Test Linf Norm: 0.1689\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0046, Train Linf Norm: 0.3492, Test Linf Norm: 0.1526\n",
            "Epoch 13: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0054, Train Linf Norm: 0.3190, Test Linf Norm: 0.2093\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0063, Train Linf Norm: 0.3372, Test Linf Norm: 0.2629\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0047, Train Linf Norm: 0.3095, Test Linf Norm: 0.1358\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0053, Train Linf Norm: 0.2821, Test Linf Norm: 0.1512\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0049, Train Linf Norm: 0.3011, Test Linf Norm: 0.1862\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0097, Train Linf Norm: 0.3024, Test Linf Norm: 0.4237\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0055, Train Linf Norm: 0.2801, Test Linf Norm: 0.1614\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0061, Train Linf Norm: 0.2718, Test Linf Norm: 0.2867\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0053, Train Linf Norm: 0.2937, Test Linf Norm: 0.1336\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0061, Test L1 Norm: 0.0072, Train Linf Norm: 0.2778, Test Linf Norm: 0.1727\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0052, Train Linf Norm: 0.2701, Test Linf Norm: 0.2352\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0061, Train Linf Norm: 0.2474, Test Linf Norm: 0.2911\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0077, Train Linf Norm: 0.2549, Test Linf Norm: 0.4078\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0046, Train Linf Norm: 0.2464, Test Linf Norm: 0.1415\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0052, Train Linf Norm: 0.2435, Test Linf Norm: 0.1800\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0039, Train Linf Norm: 0.2517, Test Linf Norm: 0.1273\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0071, Train Linf Norm: 0.2363, Test Linf Norm: 0.3546\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0099, Train Linf Norm: 0.2391, Test Linf Norm: 0.5510\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0002, Train L1 Norm: 0.0050, Test L1 Norm: 0.0090, Train Linf Norm: 0.2201, Test Linf Norm: 0.1855\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0034, Train Linf Norm: 0.2392, Test Linf Norm: 0.1202\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0069, Train Linf Norm: 0.2371, Test Linf Norm: 0.3560\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0062, Train Linf Norm: 0.2273, Test Linf Norm: 0.2806\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0002, Train L1 Norm: 0.0054, Test L1 Norm: 0.0239, Train Linf Norm: 0.2425, Test Linf Norm: 1.2694\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0074, Train Linf Norm: 0.3066, Test Linf Norm: 0.3826\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0066, Train Linf Norm: 0.2198, Test Linf Norm: 0.3426\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0068, Train Linf Norm: 0.2411, Test Linf Norm: 0.3557\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0062, Train Linf Norm: 0.2315, Test Linf Norm: 0.3203\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0077, Train Linf Norm: 0.2271, Test Linf Norm: 0.4124\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0063, Train Linf Norm: 0.2153, Test Linf Norm: 0.1610\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0079, Train Linf Norm: 0.2199, Test Linf Norm: 0.4322\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0099, Train Linf Norm: 0.2063, Test Linf Norm: 0.5373\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0053, Train Linf Norm: 0.2049, Test Linf Norm: 0.2685\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0041, Train Linf Norm: 0.2065, Test Linf Norm: 0.1778\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0045, Test L1 Norm: 0.0063, Train Linf Norm: 0.2045, Test Linf Norm: 0.3260\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 15:58:02,979]\u001b[0m Trial 193 finished with value: 0.006377004007343203 and parameters: {'n_layers': 3, 'n_units_0': 1020, 'n_units_1': 642, 'n_units_2': 696, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'LogCosh', 'optimizer': 'Adagrad', 'lr': 0.0004760285239296808, 'batch_size': 67, 'n_epochs': 47, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.484777227749283, 'patience': 10, 'threshold': 0.005119529997034171}. Best is trial 120 with value: 0.0022177403999259697.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0064, Train Linf Norm: 0.1975, Test Linf Norm: 0.3398\n",
            "Epoch 1: Train Loss: 0.0138, Test Loss: 0.0002, Train L1 Norm: 0.0626, Test L1 Norm: 0.0247, Train Linf Norm: 1.4686, Test Linf Norm: 0.6456\n",
            "Epoch 2: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0251, Test L1 Norm: 0.0129, Train Linf Norm: 0.7758, Test Linf Norm: 0.2976\n",
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0218, Test L1 Norm: 0.0124, Train Linf Norm: 0.7035, Test Linf Norm: 0.3033\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0157, Test L1 Norm: 0.0154, Train Linf Norm: 0.4695, Test Linf Norm: 0.4832\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0142, Test L1 Norm: 0.0084, Train Linf Norm: 0.4217, Test Linf Norm: 0.1885\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0121, Test L1 Norm: 0.0101, Train Linf Norm: 0.3536, Test Linf Norm: 0.2027\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0124, Test L1 Norm: 0.0077, Train Linf Norm: 0.3786, Test Linf Norm: 0.1815\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0101, Test L1 Norm: 0.0082, Train Linf Norm: 0.2881, Test Linf Norm: 0.1937\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0098, Test L1 Norm: 0.0076, Train Linf Norm: 0.2827, Test Linf Norm: 0.1835\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0102, Test L1 Norm: 0.0066, Train Linf Norm: 0.3009, Test Linf Norm: 0.1583\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0091, Test L1 Norm: 0.0064, Train Linf Norm: 0.2645, Test Linf Norm: 0.1585\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0086, Test L1 Norm: 0.0066, Train Linf Norm: 0.2486, Test Linf Norm: 0.1613\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0084, Test L1 Norm: 0.0063, Train Linf Norm: 0.2403, Test Linf Norm: 0.1525\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0095, Test L1 Norm: 0.0063, Train Linf Norm: 0.2979, Test Linf Norm: 0.1560\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0083, Test L1 Norm: 0.0058, Train Linf Norm: 0.2428, Test Linf Norm: 0.1415\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0077, Test L1 Norm: 0.0057, Train Linf Norm: 0.2195, Test Linf Norm: 0.1382\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0069, Train Linf Norm: 0.2389, Test Linf Norm: 0.1937\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0074, Test L1 Norm: 0.0055, Train Linf Norm: 0.2160, Test Linf Norm: 0.1341\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0073, Test L1 Norm: 0.0054, Train Linf Norm: 0.2092, Test Linf Norm: 0.1250\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0054, Train Linf Norm: 0.1975, Test Linf Norm: 0.1310\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0053, Train Linf Norm: 0.2015, Test Linf Norm: 0.1289\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0053, Train Linf Norm: 0.1996, Test Linf Norm: 0.1295\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0052, Train Linf Norm: 0.1924, Test Linf Norm: 0.1277\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0069, Train Linf Norm: 0.1870, Test Linf Norm: 0.1766\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0060, Train Linf Norm: 0.1840, Test Linf Norm: 0.1357\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0051, Train Linf Norm: 0.1918, Test Linf Norm: 0.1229\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0052, Train Linf Norm: 0.1912, Test Linf Norm: 0.1226\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0049, Train Linf Norm: 0.1857, Test Linf Norm: 0.1208\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0048, Train Linf Norm: 0.1869, Test Linf Norm: 0.1179\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0049, Train Linf Norm: 0.1875, Test Linf Norm: 0.1201\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0062, Test L1 Norm: 0.0052, Train Linf Norm: 0.1781, Test Linf Norm: 0.1231\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0048, Train Linf Norm: 0.1918, Test Linf Norm: 0.1164\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0048, Train Linf Norm: 0.1873, Test Linf Norm: 0.1167\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0055, Train Linf Norm: 0.1757, Test Linf Norm: 0.1194\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0053, Train Linf Norm: 0.1740, Test Linf Norm: 0.1166\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0060, Test L1 Norm: 0.0049, Train Linf Norm: 0.1726, Test Linf Norm: 0.1149\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0049, Train Linf Norm: 0.1644, Test Linf Norm: 0.1161\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0047, Train Linf Norm: 0.1732, Test Linf Norm: 0.1127\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0045, Train Linf Norm: 0.1702, Test Linf Norm: 0.1083\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0049, Train Linf Norm: 0.1602, Test Linf Norm: 0.1130\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0059, Test L1 Norm: 0.0044, Train Linf Norm: 0.1735, Test Linf Norm: 0.1063\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0056, Test L1 Norm: 0.0046, Train Linf Norm: 0.1598, Test Linf Norm: 0.1117\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0057, Test L1 Norm: 0.0044, Train Linf Norm: 0.1665, Test Linf Norm: 0.1070\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0042, Train Linf Norm: 0.1672, Test Linf Norm: 0.1012\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0058, Test L1 Norm: 0.0049, Train Linf Norm: 0.1677, Test Linf Norm: 0.1296\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0053, Train Linf Norm: 0.1575, Test Linf Norm: 0.1232\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0042, Train Linf Norm: 0.1571, Test Linf Norm: 0.1039\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0061, Train Linf Norm: 0.1621, Test Linf Norm: 0.1324\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0042, Train Linf Norm: 0.1565, Test Linf Norm: 0.1011\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0042, Train Linf Norm: 0.1508, Test Linf Norm: 0.0993\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0047, Train Linf Norm: 0.1552, Test Linf Norm: 0.1064\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0053, Train Linf Norm: 0.1455, Test Linf Norm: 0.1299\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0039, Train Linf Norm: 0.1523, Test Linf Norm: 0.0965\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0053, Test L1 Norm: 0.0073, Train Linf Norm: 0.1502, Test Linf Norm: 0.1250\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0053, Test L1 Norm: 0.0044, Train Linf Norm: 0.1532, Test Linf Norm: 0.1052\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0052, Test L1 Norm: 0.0043, Train Linf Norm: 0.1489, Test Linf Norm: 0.0983\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0043, Train Linf Norm: 0.1404, Test Linf Norm: 0.1034\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0055, Test L1 Norm: 0.0050, Train Linf Norm: 0.1654, Test Linf Norm: 0.1106\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0039, Train Linf Norm: 0.1378, Test Linf Norm: 0.0944\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0038, Train Linf Norm: 0.1501, Test Linf Norm: 0.0935\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0039, Train Linf Norm: 0.1451, Test Linf Norm: 0.0947\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0041, Train Linf Norm: 0.1351, Test Linf Norm: 0.0959\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0046, Train Linf Norm: 0.1413, Test Linf Norm: 0.0998\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0051, Test L1 Norm: 0.0039, Train Linf Norm: 0.1467, Test Linf Norm: 0.0932\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0049, Test L1 Norm: 0.0130, Train Linf Norm: 0.1426, Test Linf Norm: 0.3846\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0039, Train Linf Norm: 0.1409, Test Linf Norm: 0.0965\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0047, Test L1 Norm: 0.0039, Train Linf Norm: 0.1306, Test Linf Norm: 0.0927\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0048, Test L1 Norm: 0.0076, Train Linf Norm: 0.1378, Test Linf Norm: 0.1969\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0037, Train Linf Norm: 0.1392, Test Linf Norm: 0.0921\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0048, Test L1 Norm: 0.0041, Train Linf Norm: 0.1360, Test Linf Norm: 0.0989\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0050, Test L1 Norm: 0.0040, Train Linf Norm: 0.1465, Test Linf Norm: 0.0989\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 16:04:07,790]\u001b[0m Trial 194 finished with value: 0.003952949631982483 and parameters: {'n_layers': 3, 'n_units_0': 975, 'n_units_1': 426, 'n_units_2': 743, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'LogCosh', 'optimizer': 'Adagrad', 'lr': 0.00023087832281730027, 'batch_size': 46, 'n_epochs': 72, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.6223277513742445, 'patience': 11, 'threshold': 0.009785668184932468}. Best is trial 120 with value: 0.0022177403999259697.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0049, Test L1 Norm: 0.0040, Train Linf Norm: 0.1419, Test Linf Norm: 0.0957\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 16:04:12,232]\u001b[0m Trial 195 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 16:04:17,697]\u001b[0m Trial 196 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 2.8186, Test Loss: 2.8428, Train L1 Norm: 1.0000, Test L1 Norm: 1.0000, Train Linf Norm: 1.0000, Test Linf Norm: 1.0000\n",
            "Epoch 1: Train Loss: 0.0175, Test Loss: 0.0001, Train L1 Norm: 0.0995, Test L1 Norm: 0.0176, Train Linf Norm: 3.4087, Test Linf Norm: 0.4874\n",
            "Epoch 2: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0235, Test L1 Norm: 0.0126, Train Linf Norm: 0.8589, Test Linf Norm: 0.3394\n",
            "Epoch 3: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0196, Test L1 Norm: 0.0116, Train Linf Norm: 0.7442, Test Linf Norm: 0.3302\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0156, Test L1 Norm: 0.0104, Train Linf Norm: 0.5501, Test Linf Norm: 0.2917\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0153, Test L1 Norm: 0.0121, Train Linf Norm: 0.5622, Test Linf Norm: 0.3352\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0141, Test L1 Norm: 0.0094, Train Linf Norm: 0.5223, Test Linf Norm: 0.2691\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0134, Test L1 Norm: 0.0112, Train Linf Norm: 0.4919, Test Linf Norm: 0.3029\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0143, Test L1 Norm: 0.0089, Train Linf Norm: 0.5588, Test Linf Norm: 0.2564\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0118, Test L1 Norm: 0.0081, Train Linf Norm: 0.4271, Test Linf Norm: 0.2339\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0118, Test L1 Norm: 0.0090, Train Linf Norm: 0.4364, Test Linf Norm: 0.2290\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0119, Test L1 Norm: 0.0076, Train Linf Norm: 0.4524, Test Linf Norm: 0.2282\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0111, Test L1 Norm: 0.0076, Train Linf Norm: 0.4018, Test Linf Norm: 0.2142\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0116, Test L1 Norm: 0.0074, Train Linf Norm: 0.4462, Test Linf Norm: 0.2120\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0105, Test L1 Norm: 0.0076, Train Linf Norm: 0.3913, Test Linf Norm: 0.2367\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0104, Test L1 Norm: 0.0073, Train Linf Norm: 0.3940, Test Linf Norm: 0.2312\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0067, Train Linf Norm: 0.3709, Test Linf Norm: 0.1963\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0100, Test L1 Norm: 0.0069, Train Linf Norm: 0.3699, Test Linf Norm: 0.2104\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0103, Test L1 Norm: 0.0068, Train Linf Norm: 0.3984, Test Linf Norm: 0.2022\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0102, Test L1 Norm: 0.0069, Train Linf Norm: 0.3965, Test Linf Norm: 0.1944\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0096, Test L1 Norm: 0.0074, Train Linf Norm: 0.3621, Test Linf Norm: 0.2344\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0101, Test L1 Norm: 0.0064, Train Linf Norm: 0.3934, Test Linf Norm: 0.1894\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0091, Test L1 Norm: 0.0068, Train Linf Norm: 0.3368, Test Linf Norm: 0.2174\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0091, Test L1 Norm: 0.0059, Train Linf Norm: 0.3472, Test Linf Norm: 0.1744\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0091, Test L1 Norm: 0.0062, Train Linf Norm: 0.3448, Test Linf Norm: 0.1891\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0090, Test L1 Norm: 0.0060, Train Linf Norm: 0.3462, Test Linf Norm: 0.1834\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0088, Test L1 Norm: 0.0062, Train Linf Norm: 0.3299, Test Linf Norm: 0.1948\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0084, Test L1 Norm: 0.0063, Train Linf Norm: 0.3165, Test Linf Norm: 0.2004\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0057, Train Linf Norm: 0.3194, Test Linf Norm: 0.1746\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0057, Train Linf Norm: 0.3214, Test Linf Norm: 0.1703\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0056, Train Linf Norm: 0.3230, Test Linf Norm: 0.1736\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0083, Test L1 Norm: 0.0059, Train Linf Norm: 0.3142, Test Linf Norm: 0.1820\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0056, Train Linf Norm: 0.3133, Test Linf Norm: 0.1650\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0053, Train Linf Norm: 0.3174, Test Linf Norm: 0.1626\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0054, Train Linf Norm: 0.3072, Test Linf Norm: 0.1627\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0084, Test L1 Norm: 0.0057, Train Linf Norm: 0.3268, Test Linf Norm: 0.1621\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0080, Test L1 Norm: 0.0058, Train Linf Norm: 0.3104, Test Linf Norm: 0.1597\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 16:06:53,619]\u001b[0m Trial 197 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0079, Test L1 Norm: 0.0053, Train Linf Norm: 0.3013, Test Linf Norm: 0.1574\n",
            "Epoch 1: Train Loss: 0.0296, Test Loss: 0.0002, Train L1 Norm: 0.1672, Test L1 Norm: 0.0174, Train Linf Norm: 9.6825, Test Linf Norm: 0.5783\n",
            "Epoch 2: Train Loss: 0.0002, Test Loss: 0.0001, Train L1 Norm: 0.0254, Test L1 Norm: 0.0146, Train Linf Norm: 1.1644, Test Linf Norm: 0.5847\n",
            "Epoch 3: Train Loss: 0.0001, Test Loss: 0.0001, Train L1 Norm: 0.0199, Test L1 Norm: 0.0102, Train Linf Norm: 0.9205, Test Linf Norm: 0.3092\n",
            "Epoch 4: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0153, Test L1 Norm: 0.0095, Train Linf Norm: 0.6891, Test Linf Norm: 0.3075\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0001, Train L1 Norm: 0.0131, Test L1 Norm: 0.0102, Train Linf Norm: 0.5669, Test Linf Norm: 0.2924\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0123, Test L1 Norm: 0.0084, Train Linf Norm: 0.5436, Test Linf Norm: 0.2780\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0115, Test L1 Norm: 0.0081, Train Linf Norm: 0.5069, Test Linf Norm: 0.2843\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0111, Test L1 Norm: 0.0074, Train Linf Norm: 0.4972, Test Linf Norm: 0.2474\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0106, Test L1 Norm: 0.0076, Train Linf Norm: 0.4738, Test Linf Norm: 0.2693\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0099, Test L1 Norm: 0.0072, Train Linf Norm: 0.4340, Test Linf Norm: 0.2751\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0092, Test L1 Norm: 0.0072, Train Linf Norm: 0.3976, Test Linf Norm: 0.2398\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0091, Test L1 Norm: 0.0071, Train Linf Norm: 0.3995, Test Linf Norm: 0.2412\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0091, Test L1 Norm: 0.0072, Train Linf Norm: 0.3993, Test Linf Norm: 0.2879\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0089, Test L1 Norm: 0.0067, Train Linf Norm: 0.4000, Test Linf Norm: 0.2597\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0085, Test L1 Norm: 0.0066, Train Linf Norm: 0.3764, Test Linf Norm: 0.2373\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0069, Train Linf Norm: 0.3452, Test Linf Norm: 0.2847\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0082, Test L1 Norm: 0.0063, Train Linf Norm: 0.3527, Test Linf Norm: 0.2293\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0081, Test L1 Norm: 0.0064, Train Linf Norm: 0.3570, Test Linf Norm: 0.2394\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0078, Test L1 Norm: 0.0062, Train Linf Norm: 0.3394, Test Linf Norm: 0.2414\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0060, Train Linf Norm: 0.3271, Test Linf Norm: 0.2336\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0060, Train Linf Norm: 0.3308, Test Linf Norm: 0.2275\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0076, Test L1 Norm: 0.0062, Train Linf Norm: 0.3438, Test Linf Norm: 0.2291\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0075, Test L1 Norm: 0.0057, Train Linf Norm: 0.3359, Test Linf Norm: 0.2156\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0057, Train Linf Norm: 0.3187, Test Linf Norm: 0.2158\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0055, Train Linf Norm: 0.3094, Test Linf Norm: 0.2088\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0057, Train Linf Norm: 0.2862, Test Linf Norm: 0.2333\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0054, Train Linf Norm: 0.3116, Test Linf Norm: 0.2020\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0071, Test L1 Norm: 0.0055, Train Linf Norm: 0.3200, Test Linf Norm: 0.1983\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0068, Test L1 Norm: 0.0053, Train Linf Norm: 0.3035, Test Linf Norm: 0.1998\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0070, Test L1 Norm: 0.0053, Train Linf Norm: 0.3215, Test Linf Norm: 0.2007\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0069, Test L1 Norm: 0.0051, Train Linf Norm: 0.3132, Test Linf Norm: 0.1965\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0051, Train Linf Norm: 0.3067, Test Linf Norm: 0.1930\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0067, Test L1 Norm: 0.0054, Train Linf Norm: 0.3000, Test Linf Norm: 0.1959\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0052, Train Linf Norm: 0.2838, Test Linf Norm: 0.1970\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0050, Train Linf Norm: 0.2837, Test Linf Norm: 0.1895\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0050, Train Linf Norm: 0.2992, Test Linf Norm: 0.1930\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0049, Train Linf Norm: 0.2803, Test Linf Norm: 0.1888\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0064, Test L1 Norm: 0.0048, Train Linf Norm: 0.2868, Test Linf Norm: 0.1939\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0049, Train Linf Norm: 0.2845, Test Linf Norm: 0.1910\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0065, Test L1 Norm: 0.0047, Train Linf Norm: 0.3027, Test Linf Norm: 0.1782\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0051, Train Linf Norm: 0.2819, Test Linf Norm: 0.1943\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0063, Test L1 Norm: 0.0047, Train Linf Norm: 0.2843, Test Linf Norm: 0.1812\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 16:09:14,904]\u001b[0m Trial 198 finished with value: 0.004795710662251804 and parameters: {'n_layers': 3, 'n_units_0': 985, 'n_units_1': 319, 'n_units_2': 790, 'hidden_activation': 'ReLU', 'output_activation': 'ReLU', 'loss': 'LogCosh', 'optimizer': 'Adagrad', 'lr': 0.00023620772355812636, 'batch_size': 77, 'n_epochs': 43, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.5193494658222158, 'patience': 9, 'threshold': 0.004287023270223693}. Best is trial 120 with value: 0.0022177403999259697.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0061, Test L1 Norm: 0.0048, Train Linf Norm: 0.2729, Test Linf Norm: 0.1844\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-05-14 16:09:20,334]\u001b[0m Trial 199 pruned. \u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.4468, Test Loss: 0.1733, Train L1 Norm: 0.6789, Test L1 Norm: 0.4199, Train Linf Norm: 8.5653, Test Linf Norm: 3.1458\n",
            "Best trial:\n",
            "  Value:  0.0022177403999259697\n",
            "  Params: \n",
            "    n_layers: 3\n",
            "    n_units_0: 1044\n",
            "    n_units_1: 914\n",
            "    n_units_2: 840\n",
            "    hidden_activation: ReLU\n",
            "    output_activation: ReLU\n",
            "    loss: Huber\n",
            "    optimizer: Adagrad\n",
            "    lr: 0.0004971113311461239\n",
            "    batch_size: 164\n",
            "    n_epochs: 167\n",
            "    scheduler: CosineAnnealingLR\n",
            "    T_max: 8\n"
          ]
        }
      ],
      "source": [
        "if OPTIMIZE:\n",
        "    # Creating a study object with Optuna with TPE sampler and median pruner \n",
        "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner())\n",
        "\n",
        "    # Running Optuna with 100 trials when we are optimizing.\n",
        "    study.optimize(objective, n_trials=N_TRIALS)\n",
        "\n",
        "    # Printing the best trial information\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "    print(\"  Value: \", trial.value)\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(f\"    {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBaEHofw_MQE"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzMGU3JE_MQE",
        "outputId": "7fdbfebf-4421-4c88-d95f-f561f61fa5b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-59-d4ca07a22bfc>:48: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.FrozenTrial.suggest_float` instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n"
          ]
        }
      ],
      "source": [
        "# Creating the best network and optimizer using the best hyperparameters\n",
        "if OPTIMIZE:\n",
        "    net, \\\n",
        "    loss_fn, \\\n",
        "    optimizer, \\\n",
        "    batch_size, \\\n",
        "    n_epochs, \\\n",
        "    scheduler, \\\n",
        "    loss_name, \\\n",
        "    optimizer_name, \\\n",
        "    scheduler_name, \\\n",
        "    n_units, \\\n",
        "    n_layers, \\\n",
        "    hidden_activation, \\\n",
        "    output_activation, \\\n",
        "    lr = create_model(trial, optimize=True)\n",
        "# Creating the network with predefined hyperparameters\n",
        "else:\n",
        "    net, \\\n",
        "    loss_fn, \\\n",
        "    optimizer, \\\n",
        "    batch_size, \\\n",
        "    n_epochs, \\\n",
        "    scheduler, \\\n",
        "    loss_name, \\\n",
        "    optimizer_name, \\\n",
        "    scheduler_name, \\\n",
        "    n_units, \\\n",
        "    n_layers, \\\n",
        "    hidden_activation, \\\n",
        "    output_activation, \\\n",
        "    lr = create_model(trial=None, optimize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJGrR-CL_MQF",
        "outputId": "220d5ac2-84d8-4b8b-9a98-0ee08ba1920d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss_fn: SmoothL1Loss()\n",
            "batch_size: 164\n",
            "n_epochs: 167\n",
            "scheduler: <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f5a30850e20>\n",
            "loss_name: Huber\n",
            "optimizer_name: Adagrad\n",
            "scheduler_name: CosineAnnealingLR\n",
            "n_units: [1044, 914, 840]\n",
            "n_layers: 3\n",
            "hidden_activation: ReLU()\n",
            "output_activation: ReLU()\n"
          ]
        }
      ],
      "source": [
        "print(\"loss_fn:\", loss_fn)\n",
        "print(\"batch_size:\", batch_size)\n",
        "print(\"n_epochs:\", n_epochs)\n",
        "print(\"scheduler:\", scheduler)\n",
        "print(\"loss_name:\", loss_name)\n",
        "print(\"optimizer_name:\", optimizer_name)\n",
        "print(\"scheduler_name:\", scheduler_name)\n",
        "print(\"n_units:\", n_units)\n",
        "print(\"n_layers:\", n_layers)\n",
        "print(\"hidden_activation:\", hidden_activation)\n",
        "print(\"output_activation:\", output_activation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FShB39bz_MQF",
        "outputId": "86d261fa-1a82-4802-d96f-5d9347bf54a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0077, Train Linf Norm: 0.3387, Test Linf Norm: 1.0245\n",
            "Epoch 2: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0077, Train Linf Norm: 0.3269, Test Linf Norm: 1.0245\n",
            "Epoch 3: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0078, Train Linf Norm: 0.3309, Test Linf Norm: 1.0339\n",
            "Epoch 4: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0079, Train Linf Norm: 0.3348, Test Linf Norm: 1.0491\n",
            "Epoch 5: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0072, Train Linf Norm: 0.3412, Test Linf Norm: 0.9503\n",
            "Epoch 6: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0080, Train Linf Norm: 0.3432, Test Linf Norm: 1.0661\n",
            "Epoch 7: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0074, Train Linf Norm: 0.3325, Test Linf Norm: 0.9874\n",
            "Epoch 8: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0070, Train Linf Norm: 0.3239, Test Linf Norm: 0.9177\n",
            "Epoch 9: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0069, Train Linf Norm: 0.3372, Test Linf Norm: 0.9009\n",
            "Epoch 10: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0046, Test L1 Norm: 0.0044, Train Linf Norm: 0.4687, Test Linf Norm: 0.3598\n",
            "Epoch 11: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0077, Train Linf Norm: 0.3558, Test Linf Norm: 1.0157\n",
            "Epoch 12: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0076, Train Linf Norm: 0.3384, Test Linf Norm: 1.0092\n",
            "Epoch 13: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0074, Train Linf Norm: 0.3285, Test Linf Norm: 0.9800\n",
            "Epoch 14: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0069, Train Linf Norm: 0.3412, Test Linf Norm: 0.8981\n",
            "Epoch 15: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0075, Train Linf Norm: 0.3257, Test Linf Norm: 0.9933\n",
            "Epoch 16: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0074, Train Linf Norm: 0.3314, Test Linf Norm: 0.9821\n",
            "Epoch 17: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0072, Train Linf Norm: 0.3309, Test Linf Norm: 0.9506\n",
            "Epoch 18: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0072, Train Linf Norm: 0.3301, Test Linf Norm: 0.9506\n",
            "Epoch 19: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0074, Train Linf Norm: 0.3340, Test Linf Norm: 0.9822\n",
            "Epoch 20: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0074, Train Linf Norm: 0.3376, Test Linf Norm: 0.9808\n",
            "Epoch 21: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0072, Train Linf Norm: 0.3400, Test Linf Norm: 0.9499\n",
            "Epoch 22: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0071, Train Linf Norm: 0.3350, Test Linf Norm: 0.9405\n",
            "Epoch 23: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0065, Train Linf Norm: 0.3438, Test Linf Norm: 0.8405\n",
            "Epoch 24: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0077, Train Linf Norm: 0.3369, Test Linf Norm: 1.0291\n",
            "Epoch 25: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0081, Train Linf Norm: 0.3423, Test Linf Norm: 1.0850\n",
            "Epoch 26: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0066, Test L1 Norm: 0.0078, Train Linf Norm: 0.7254, Test Linf Norm: 1.0392\n",
            "Epoch 27: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0050, Train Linf Norm: 0.3146, Test Linf Norm: 0.5157\n",
            "Epoch 28: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0072, Train Linf Norm: 0.3492, Test Linf Norm: 0.9524\n",
            "Epoch 29: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0069, Train Linf Norm: 0.3365, Test Linf Norm: 0.8971\n",
            "Epoch 30: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0067, Train Linf Norm: 0.3354, Test Linf Norm: 0.8678\n",
            "Epoch 31: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0071, Train Linf Norm: 0.3297, Test Linf Norm: 0.9367\n",
            "Epoch 32: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0072, Train Linf Norm: 0.3255, Test Linf Norm: 0.9545\n",
            "Epoch 33: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0069, Train Linf Norm: 0.3357, Test Linf Norm: 0.9126\n",
            "Epoch 34: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0069, Train Linf Norm: 0.3282, Test Linf Norm: 0.9126\n",
            "Epoch 35: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0070, Train Linf Norm: 0.3334, Test Linf Norm: 0.9169\n",
            "Epoch 36: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0071, Train Linf Norm: 0.3371, Test Linf Norm: 0.9345\n",
            "Epoch 37: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0069, Train Linf Norm: 0.3384, Test Linf Norm: 0.9167\n",
            "Epoch 38: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0076, Train Linf Norm: 0.3424, Test Linf Norm: 1.0129\n",
            "Epoch 39: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0075, Train Linf Norm: 0.3431, Test Linf Norm: 1.0040\n",
            "Epoch 40: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0069, Train Linf Norm: 0.3674, Test Linf Norm: 0.8998\n",
            "Epoch 41: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0071, Train Linf Norm: 0.3465, Test Linf Norm: 0.9400\n",
            "Epoch 42: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0044, Test L1 Norm: 0.0057, Train Linf Norm: 0.4153, Test Linf Norm: 0.7080\n",
            "Epoch 43: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0064, Train Linf Norm: 0.3385, Test Linf Norm: 0.8094\n",
            "Epoch 44: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0061, Train Linf Norm: 0.3576, Test Linf Norm: 0.7827\n",
            "Epoch 45: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0073, Train Linf Norm: 0.3356, Test Linf Norm: 0.9752\n",
            "Epoch 46: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0068, Train Linf Norm: 0.3422, Test Linf Norm: 0.9042\n",
            "Epoch 47: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0070, Train Linf Norm: 0.3453, Test Linf Norm: 0.9253\n",
            "Epoch 48: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0071, Train Linf Norm: 0.3413, Test Linf Norm: 0.9482\n",
            "Epoch 49: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0070, Train Linf Norm: 0.3372, Test Linf Norm: 0.9239\n",
            "Epoch 50: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0070, Train Linf Norm: 0.3306, Test Linf Norm: 0.9239\n",
            "Epoch 51: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0071, Train Linf Norm: 0.3391, Test Linf Norm: 0.9454\n",
            "Epoch 52: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0070, Train Linf Norm: 0.3423, Test Linf Norm: 0.9343\n",
            "Epoch 53: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0069, Train Linf Norm: 0.3420, Test Linf Norm: 0.9051\n",
            "Epoch 54: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0073, Train Linf Norm: 0.3322, Test Linf Norm: 0.9740\n",
            "Epoch 55: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0084, Train Linf Norm: 0.3393, Test Linf Norm: 1.1459\n",
            "Epoch 56: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0073, Train Linf Norm: 0.3421, Test Linf Norm: 0.9766\n",
            "Epoch 57: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0075, Train Linf Norm: 0.3640, Test Linf Norm: 1.0139\n",
            "Epoch 58: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0037, Test L1 Norm: 0.0054, Train Linf Norm: 0.3023, Test Linf Norm: 0.6066\n",
            "Epoch 59: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0073, Train Linf Norm: 0.3302, Test Linf Norm: 0.9794\n",
            "Epoch 60: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0068, Train Linf Norm: 0.3459, Test Linf Norm: 0.8976\n",
            "Epoch 61: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0072, Train Linf Norm: 0.3372, Test Linf Norm: 0.9694\n",
            "Epoch 62: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0072, Train Linf Norm: 0.3393, Test Linf Norm: 0.9695\n",
            "Epoch 63: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0068, Train Linf Norm: 0.3333, Test Linf Norm: 0.9088\n",
            "Epoch 64: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0068, Train Linf Norm: 0.3275, Test Linf Norm: 0.9035\n",
            "Epoch 65: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0069, Train Linf Norm: 0.3340, Test Linf Norm: 0.9184\n",
            "Epoch 66: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0069, Train Linf Norm: 0.3320, Test Linf Norm: 0.9184\n",
            "Epoch 67: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0070, Train Linf Norm: 0.3326, Test Linf Norm: 0.9298\n",
            "Epoch 68: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0070, Train Linf Norm: 0.3297, Test Linf Norm: 0.9276\n",
            "Epoch 69: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0067, Train Linf Norm: 0.3341, Test Linf Norm: 0.8849\n",
            "Epoch 70: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0075, Train Linf Norm: 0.3438, Test Linf Norm: 1.0124\n",
            "Epoch 71: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0067, Train Linf Norm: 0.3331, Test Linf Norm: 0.8781\n",
            "Epoch 72: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0073, Train Linf Norm: 0.3317, Test Linf Norm: 0.9911\n",
            "Epoch 73: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0065, Train Linf Norm: 0.3409, Test Linf Norm: 0.8590\n",
            "Epoch 74: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0043, Test L1 Norm: 0.0061, Train Linf Norm: 0.3996, Test Linf Norm: 0.7654\n",
            "Epoch 75: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0036, Train Linf Norm: 0.3108, Test Linf Norm: 0.2116\n",
            "Epoch 76: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0060, Train Linf Norm: 0.3125, Test Linf Norm: 0.7677\n",
            "Epoch 77: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0068, Train Linf Norm: 0.3212, Test Linf Norm: 0.9071\n",
            "Epoch 78: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0060, Train Linf Norm: 0.3318, Test Linf Norm: 0.7739\n",
            "Epoch 79: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0067, Train Linf Norm: 0.3198, Test Linf Norm: 0.8871\n",
            "Epoch 80: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0068, Train Linf Norm: 0.3200, Test Linf Norm: 0.9008\n",
            "Epoch 81: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0068, Train Linf Norm: 0.3192, Test Linf Norm: 0.9125\n",
            "Epoch 82: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0068, Train Linf Norm: 0.3251, Test Linf Norm: 0.9125\n",
            "Epoch 83: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0068, Train Linf Norm: 0.3212, Test Linf Norm: 0.9109\n",
            "Epoch 84: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0068, Train Linf Norm: 0.3123, Test Linf Norm: 0.9143\n",
            "Epoch 85: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0067, Train Linf Norm: 0.3215, Test Linf Norm: 0.8897\n",
            "Epoch 86: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0066, Train Linf Norm: 0.3267, Test Linf Norm: 0.8754\n",
            "Epoch 87: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0070, Train Linf Norm: 0.3310, Test Linf Norm: 0.9411\n",
            "Epoch 88: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0064, Train Linf Norm: 0.3196, Test Linf Norm: 0.8365\n",
            "Epoch 89: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0074, Train Linf Norm: 0.3823, Test Linf Norm: 0.9912\n",
            "Epoch 90: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0041, Test L1 Norm: 0.0086, Train Linf Norm: 0.3773, Test Linf Norm: 1.1380\n",
            "Epoch 91: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0060, Train Linf Norm: 0.3309, Test Linf Norm: 0.7623\n",
            "Epoch 92: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0069, Train Linf Norm: 0.3001, Test Linf Norm: 0.9309\n",
            "Epoch 93: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0064, Train Linf Norm: 0.3200, Test Linf Norm: 0.8459\n",
            "Epoch 94: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0068, Train Linf Norm: 0.3190, Test Linf Norm: 0.9087\n",
            "Epoch 95: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0069, Train Linf Norm: 0.3119, Test Linf Norm: 0.9239\n",
            "Epoch 96: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0068, Train Linf Norm: 0.3154, Test Linf Norm: 0.9048\n",
            "Epoch 97: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0067, Train Linf Norm: 0.3220, Test Linf Norm: 0.8990\n",
            "Epoch 98: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0067, Train Linf Norm: 0.3172, Test Linf Norm: 0.8990\n",
            "Epoch 99: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0067, Train Linf Norm: 0.3085, Test Linf Norm: 0.8987\n",
            "Epoch 100: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0066, Train Linf Norm: 0.3126, Test Linf Norm: 0.8744\n",
            "Epoch 101: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0063, Train Linf Norm: 0.3261, Test Linf Norm: 0.8281\n",
            "Epoch 102: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0068, Train Linf Norm: 0.3074, Test Linf Norm: 0.9007\n",
            "Epoch 103: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0070, Train Linf Norm: 0.3255, Test Linf Norm: 0.9348\n",
            "Epoch 104: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0072, Train Linf Norm: 0.3085, Test Linf Norm: 0.9660\n",
            "Epoch 105: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0036, Test L1 Norm: 0.0057, Train Linf Norm: 0.3533, Test Linf Norm: 0.7452\n",
            "Epoch 106: Train Loss: 0.0001, Test Loss: 0.0000, Train L1 Norm: 0.0054, Test L1 Norm: 0.0030, Train Linf Norm: 0.4114, Test Linf Norm: 0.1899\n",
            "Epoch 107: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0068, Train Linf Norm: 0.3127, Test Linf Norm: 0.8950\n",
            "Epoch 108: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0064, Train Linf Norm: 0.3089, Test Linf Norm: 0.8498\n",
            "Epoch 109: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0059, Train Linf Norm: 0.3000, Test Linf Norm: 0.7693\n",
            "Epoch 110: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0063, Train Linf Norm: 0.2941, Test Linf Norm: 0.8305\n",
            "Epoch 111: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0065, Train Linf Norm: 0.2874, Test Linf Norm: 0.8619\n",
            "Epoch 112: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0062, Train Linf Norm: 0.3043, Test Linf Norm: 0.8171\n",
            "Epoch 113: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0063, Train Linf Norm: 0.3005, Test Linf Norm: 0.8277\n",
            "Epoch 114: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0063, Train Linf Norm: 0.2922, Test Linf Norm: 0.8277\n",
            "Epoch 115: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0063, Train Linf Norm: 0.2907, Test Linf Norm: 0.8375\n",
            "Epoch 116: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0064, Train Linf Norm: 0.3017, Test Linf Norm: 0.8540\n",
            "Epoch 117: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0063, Train Linf Norm: 0.2873, Test Linf Norm: 0.8321\n",
            "Epoch 118: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0064, Train Linf Norm: 0.2981, Test Linf Norm: 0.8486\n",
            "Epoch 119: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0061, Train Linf Norm: 0.3125, Test Linf Norm: 0.8060\n",
            "Epoch 120: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0066, Train Linf Norm: 0.2981, Test Linf Norm: 0.8753\n",
            "Epoch 121: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0063, Train Linf Norm: 0.3290, Test Linf Norm: 0.8394\n",
            "Epoch 122: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0040, Test L1 Norm: 0.0067, Train Linf Norm: 0.3661, Test Linf Norm: 0.8983\n",
            "Epoch 123: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0034, Test L1 Norm: 0.0062, Train Linf Norm: 0.3430, Test Linf Norm: 0.8279\n",
            "Epoch 124: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0059, Train Linf Norm: 0.3181, Test Linf Norm: 0.7689\n",
            "Epoch 125: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0063, Train Linf Norm: 0.3219, Test Linf Norm: 0.8427\n",
            "Epoch 126: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0057, Train Linf Norm: 0.3044, Test Linf Norm: 0.7423\n",
            "Epoch 127: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0063, Train Linf Norm: 0.3043, Test Linf Norm: 0.8343\n",
            "Epoch 128: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0065, Train Linf Norm: 0.2997, Test Linf Norm: 0.8669\n",
            "Epoch 129: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0063, Train Linf Norm: 0.3034, Test Linf Norm: 0.8398\n",
            "Epoch 130: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0063, Train Linf Norm: 0.3011, Test Linf Norm: 0.8398\n",
            "Epoch 131: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0064, Train Linf Norm: 0.2924, Test Linf Norm: 0.8484\n",
            "Epoch 132: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0063, Train Linf Norm: 0.2959, Test Linf Norm: 0.8334\n",
            "Epoch 133: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0065, Train Linf Norm: 0.3111, Test Linf Norm: 0.8640\n",
            "Epoch 134: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0064, Train Linf Norm: 0.3016, Test Linf Norm: 0.8493\n",
            "Epoch 135: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0057, Train Linf Norm: 0.3157, Test Linf Norm: 0.7341\n",
            "Epoch 136: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0062, Train Linf Norm: 0.2889, Test Linf Norm: 0.8158\n",
            "Epoch 137: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0061, Train Linf Norm: 0.3256, Test Linf Norm: 0.8022\n",
            "Epoch 138: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0038, Test L1 Norm: 0.0059, Train Linf Norm: 0.3419, Test Linf Norm: 0.7695\n",
            "Epoch 139: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0033, Test L1 Norm: 0.0066, Train Linf Norm: 0.3166, Test Linf Norm: 0.8824\n",
            "Epoch 140: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0064, Train Linf Norm: 0.2929, Test Linf Norm: 0.8598\n",
            "Epoch 141: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0060, Train Linf Norm: 0.3035, Test Linf Norm: 0.7914\n",
            "Epoch 142: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0062, Train Linf Norm: 0.3048, Test Linf Norm: 0.8346\n",
            "Epoch 143: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0061, Train Linf Norm: 0.3071, Test Linf Norm: 0.8164\n",
            "Epoch 144: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0061, Train Linf Norm: 0.2953, Test Linf Norm: 0.8149\n",
            "Epoch 145: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0064, Train Linf Norm: 0.2946, Test Linf Norm: 0.8557\n",
            "Epoch 146: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0064, Train Linf Norm: 0.3001, Test Linf Norm: 0.8557\n",
            "Epoch 147: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0062, Train Linf Norm: 0.2976, Test Linf Norm: 0.8293\n",
            "Epoch 148: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0064, Train Linf Norm: 0.2985, Test Linf Norm: 0.8531\n",
            "Epoch 149: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0063, Train Linf Norm: 0.3031, Test Linf Norm: 0.8383\n",
            "Epoch 150: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0059, Train Linf Norm: 0.3138, Test Linf Norm: 0.7710\n",
            "Epoch 151: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0032, Test L1 Norm: 0.0065, Train Linf Norm: 0.3124, Test Linf Norm: 0.8820\n",
            "Epoch 152: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0072, Train Linf Norm: 0.3048, Test Linf Norm: 0.9880\n",
            "Epoch 153: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0028, Test L1 Norm: 0.0065, Train Linf Norm: 0.2445, Test Linf Norm: 0.8808\n",
            "Epoch 154: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0035, Test L1 Norm: 0.0050, Train Linf Norm: 0.3398, Test Linf Norm: 0.6009\n",
            "Epoch 155: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0068, Train Linf Norm: 0.3032, Test Linf Norm: 0.9221\n",
            "Epoch 156: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0058, Train Linf Norm: 0.3061, Test Linf Norm: 0.7547\n",
            "Epoch 157: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0055, Train Linf Norm: 0.2955, Test Linf Norm: 0.7172\n",
            "Epoch 158: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0059, Train Linf Norm: 0.3017, Test Linf Norm: 0.7771\n",
            "Epoch 159: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0060, Train Linf Norm: 0.2930, Test Linf Norm: 0.8063\n",
            "Epoch 160: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0062, Train Linf Norm: 0.2921, Test Linf Norm: 0.8361\n",
            "Epoch 161: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0061, Train Linf Norm: 0.2943, Test Linf Norm: 0.8160\n",
            "Epoch 162: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0061, Train Linf Norm: 0.3009, Test Linf Norm: 0.8160\n",
            "Epoch 163: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0061, Train Linf Norm: 0.2914, Test Linf Norm: 0.8094\n",
            "Epoch 164: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0060, Train Linf Norm: 0.2883, Test Linf Norm: 0.7978\n",
            "Epoch 165: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0062, Train Linf Norm: 0.2959, Test Linf Norm: 0.8325\n",
            "Epoch 166: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0030, Test L1 Norm: 0.0060, Train Linf Norm: 0.2965, Test Linf Norm: 0.7969\n",
            "Epoch 167: Train Loss: 0.0000, Test Loss: 0.0000, Train L1 Norm: 0.0031, Test L1 Norm: 0.0060, Train Linf Norm: 0.3133, Test Linf Norm: 0.7982\n"
          ]
        }
      ],
      "source": [
        "# Training and evaluating the network using the train_and_eval function\n",
        "train_losses, test_losses, train_metrics, test_metrics = train_and_eval(\n",
        "    net, loss_fn, optimizer, batch_size, n_epochs, scheduler\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWZG2gJf_MQG"
      },
      "source": [
        "## Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "6JGQSkxT_MQG"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# check if the drive is mounted\n",
        "drive_mounted = os.path.exists(\"/content/drive\")\n",
        "\n",
        "# define a function to save a file to the drive or the current directory\n",
        "def save_file(file_name):\n",
        "  if drive_mounted:\n",
        "    # save the file to the drive folder\n",
        "    drive_folder = \"/content/drive/My Drive/bsc/con2prim_restoration/Optuna_run_6\" # change this to your desired folder\n",
        "    file_path = os.path.join(drive_folder, file_name)\n",
        "    # copy the file from the current directory to the drive folder\n",
        "    shutil.copyfile(file_name, file_path)\n",
        "  else:\n",
        "    # do nothing as the file is already in the current directory\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "QloWWnEP_MQR"
      },
      "outputs": [],
      "source": [
        "\n",
        "# save the network to a .pth file\n",
        "torch.save(net.state_dict(), \"net.pth\")\n",
        "save_file(\"net.pth\")\n",
        "\n",
        "# save the optimizer to a .pth file\n",
        "torch.save(optimizer.state_dict(), \"optimizer.pth\")\n",
        "save_file(\"optimizer.pth\")\n",
        "\n",
        "# save the scheduler to a .pth file if it is not None\n",
        "if scheduler is not None:\n",
        "  torch.save(scheduler.state_dict(), \"scheduler.pth\")\n",
        "  save_file(\"scheduler.pth\")\n",
        "\n",
        "# create a dictionary to store the rest of the variables\n",
        "var_dict = {\n",
        "  \"batch_size\": batch_size,\n",
        "  \"n_epochs\": n_epochs,\n",
        "  \"loss_name\": loss_name,\n",
        "  \"optimizer_name\": optimizer_name,\n",
        "  \"scheduler_name\": scheduler_name,\n",
        "  \"n_units\": n_units,\n",
        "  \"n_layers\": n_layers,\n",
        "  \"hidden_activation_name\": hidden_activation.__class__.__name__,\n",
        "  \"output_activation_name\": output_activation.__class__.__name__,\n",
        "  \"lr\": lr,\n",
        "}\n",
        "\n",
        "# save the dictionary to a .json file\n",
        "with open(\"var_dict.json\", \"w\") as f:\n",
        "  json.dump(var_dict, f)\n",
        "save_file(\"var_dict.json\")\n",
        "\n",
        "# Saving the output of the training using pandas\n",
        "train_df = pd.DataFrame(\n",
        "    {\n",
        "        \"train_loss\": train_losses,\n",
        "        \"test_loss\": test_losses,\n",
        "        \"train_l1_norm\": [m[\"l1_norm\"] for m in train_metrics],\n",
        "        \"test_l1_norm\": [m[\"l1_norm\"] for m in test_metrics],\n",
        "        \"train_linf_norm\": [m[\"linf_norm\"] for m in train_metrics],\n",
        "        \"test_linf_norm\": [m[\"linf_norm\"] for m in test_metrics],\n",
        "    }\n",
        ")\n",
        "train_df.to_csv(\"train_output.csv\", index=False)\n",
        "save_file(\"train_output.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEdRvAtR_MQS"
      },
      "source": [
        "## Visualizing the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FaZWyrB-_MQS",
        "outputId": "1e442505-2aed-4214-8ab4-d4f44add3ce2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 0 Axes>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f593632eb30>]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f593632ee00>]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Epoch')"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'L1 Norm')"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.001, 100.0)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f59362cb820>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f593521ca60>]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f593521cd30>]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Epoch')"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Linf Norm')"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.001, 100.0)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f5938224730>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5934ed2b90>]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5934ed1a80>]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Epoch')"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'MSE')"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1e-07, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f5935095f30>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plotting the losses and metrics for the best network \n",
        "plt.figure(figsize=(12, 8))\n",
        "#plt.subplot(2, 2, 1)\n",
        "#plt.plot(train_losses, label=\"Train Loss\")\n",
        "#plt.plot(test_losses, label=\"Test Loss\")\n",
        "#plt.xlabel(\"Epoch\")\n",
        "#plt.ylabel(\"Loss\")\n",
        "#plt.legend()\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot([m[\"l1_norm\"] for m in train_metrics], label=\"Train L1 Norm\")\n",
        "plt.plot([m[\"l1_norm\"] for m in test_metrics], label=\"Test L1 Norm\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"L1 Norm\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-3, 1e2)\n",
        "plt.legend()\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot([m[\"linf_norm\"] for m in train_metrics], label=\"Train Linf Norm\")\n",
        "plt.plot([m[\"linf_norm\"] for m in test_metrics], label=\"Test Linf Norm\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Linf Norm\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-3, 1e2)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Added plotting MSE of training data and MSE of test data in one plot \n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(train_losses,label=\"training data\")\n",
        "plt.plot(test_losses,label=\"test data\")\n",
        "#if scheduler is not None:\n",
        "#    plt.plot([scheduler.get_last_lr()[0] for _ in range(n_epochs)], label=\"Learning rate\") \n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-7, 1e0)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxiIrhyb_MQT"
      },
      "source": [
        "## Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "-HGobQQA_MQT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d167437c-40e3-4be1-98a2-594deefcd44e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# load the dictionary from the .json file\n",
        "with open(\"var_dict.json\", \"r\") as f:\n",
        "  var_dict_loaded = json.load(f)\n",
        "\n",
        "# extract the variables from the dictionary\n",
        "batch_size_loaded = var_dict_loaded[\"batch_size\"]\n",
        "n_epochs_loaded = var_dict_loaded[\"n_epochs\"]\n",
        "loss_name_loaded = var_dict_loaded[\"loss_name\"]\n",
        "optimizer_name_loaded = var_dict_loaded[\"optimizer_name\"]\n",
        "scheduler_name_loaded = var_dict_loaded[\"scheduler_name\"]\n",
        "n_units_loaded = var_dict_loaded[\"n_units\"]\n",
        "n_layers_loaded = var_dict_loaded[\"n_layers\"]\n",
        "hidden_activation_name_loaded = var_dict_loaded[\"hidden_activation_name\"]\n",
        "output_activation_name_loaded = var_dict_loaded[\"output_activation_name\"]\n",
        "lr_loaded = var_dict_loaded[\"lr\"]\n",
        "\n",
        "# create the activation functions from their names\n",
        "if hidden_activation_name_loaded == \"ReLU\":\n",
        "  hidden_activation_loaded = nn.ReLU()\n",
        "elif hidden_activation_name_loaded == \"LeakyReLU\":\n",
        "  hidden_activation_loaded = nn.LeakyReLU() \n",
        "elif hidden_activation_name_loaded == \"ELU\":\n",
        "  hidden_activation_loaded = nn.ELU() \n",
        "elif hidden_activation_name_loaded == \"Tanh\":\n",
        "  hidden_activation_loaded = nn.Tanh()\n",
        "else:\n",
        "  hidden_activation_loaded = nn.Sigmoid()\n",
        "\n",
        "if output_activation_name_loaded == \"ReLU\":\n",
        "    output_activation_loaded = nn.ReLU()\n",
        "elif output_activation_name_loaded == \"Softplus\":\n",
        "    output_activation_loaded = nn.Softplus()\n",
        "else:\n",
        "    output_activation_loaded = nn.Identity()\n",
        "\n",
        "\n",
        "\n",
        "# load the network from the .pth file\n",
        "net_loaded = Net(n_layers_loaded, n_units_loaded, hidden_activation_loaded, output_activation_loaded).to(device)\n",
        "net_loaded.load_state_dict(torch.load(\"net.pth\"))\n",
        "\n",
        "# create the loss function from its name\n",
        "if loss_name_loaded == \"MSE\":\n",
        "  loss_fn_loaded = nn.MSELoss()\n",
        "elif loss_name_loaded == \"MAE\":\n",
        "  loss_fn_loaded = nn.L1Loss()\n",
        "elif loss_name_loaded == \"Huber\":\n",
        "  loss_fn_loaded = nn.SmoothL1Loss() \n",
        "else:\n",
        "  # create the log-cosh loss function\n",
        "  def log_cosh_loss_loaded(y_pred, y_true):\n",
        "    return torch.mean(torch.log(torch.cosh(y_pred - y_true)))\n",
        "  loss_fn_loaded = log_cosh_loss_loaded\n",
        "\n",
        "# load the optimizer from the .pth file\n",
        "optimizer_loaded_state_dict = torch.load(\"optimizer.pth\")\n",
        "if optimizer_name_loaded == \"SGD\":\n",
        "  # Added getting the weight decay and momentum parameters from the state dict\n",
        "  weight_decay_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"weight_decay\"]\n",
        "  momentum_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"momentum\"]\n",
        "  optimizer_loaded = optim.SGD(net_loaded.parameters(), lr=lr_loaded, weight_decay=weight_decay_loaded, momentum=momentum_loaded)\n",
        "elif optimizer_name_loaded == \"Adam\":\n",
        "  # Added getting the weight decay and beta parameters from the state dict\n",
        "  weight_decay_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"weight_decay\"]\n",
        "  beta1_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"betas\"][0]\n",
        "  beta2_loaded = optimizer_loaded_state_dict[\"param_groups\"][0][\"betas\"][1]\n",
        "  optimizer_loaded = optim.Adam(net_loaded.parameters(), lr=lr_loaded, weight_decay=weight_decay_loaded, betas=(beta1_loaded, beta2_loaded))\n",
        "elif optimizer_name_loaded == \"RMSprop\":\n",
        "  optimizer_loaded = optim.RMSprop(net_loaded.parameters(), lr=lr_loaded)\n",
        "else:\n",
        "  # Added loading the Adagrad optimizer\n",
        "  optimizer_loaded = optim.Adagrad(net_loaded.parameters(), lr=lr_loaded)\n",
        "optimizer_loaded.load_state_dict(optimizer_loaded_state_dict)\n",
        "\n",
        "# load the scheduler from the .pth file\n",
        "scheduler_loaded_state_dict = torch.load(\"scheduler.pth\")\n",
        "if scheduler_name_loaded == \"StepLR\":\n",
        "  # Added getting the step_size and gamma parameters from the state dict\n",
        "  step_size_loaded = scheduler_loaded_state_dict[\"step_size\"]\n",
        "  gamma_loaded = scheduler_loaded_state_dict[\"gamma\"]\n",
        "  scheduler_loaded = optim.lr_scheduler.StepLR(optimizer_loaded, step_size=step_size_loaded, gamma=gamma_loaded)\n",
        "elif scheduler_name_loaded == \"ExponentialLR\":\n",
        "  # Added getting the gamma parameter from the state dict\n",
        "  gamma_loaded = scheduler_loaded_state_dict[\"gamma\"]\n",
        "  scheduler_loaded = optim.lr_scheduler.ExponentialLR(optimizer_loaded, gamma=gamma_loaded)\n",
        "elif scheduler_name_loaded == \"CosineAnnealingLR\":\n",
        "  # Added getting the T_max parameter from the state dict\n",
        "  T_max_loaded = scheduler_loaded_state_dict[\"T_max\"]\n",
        "  scheduler_loaded = optim.lr_scheduler.CosineAnnealingLR(optimizer_loaded, T_max=T_max_loaded)\n",
        "elif scheduler_name_loaded == \"ReduceLROnPlateau\":\n",
        "  # Added getting the mode, factor, patience, threshold and min_lr parameters from the state dict\n",
        "  mode_loaded = scheduler_loaded_state_dict[\"mode\"]\n",
        "  factor_loaded = scheduler_loaded_state_dict[\"factor\"]\n",
        "  patience_loaded = scheduler_loaded_state_dict[\"patience\"]\n",
        "  threshold_loaded = scheduler_loaded_state_dict[\"threshold\"]\n",
        "  min_lr_loaded = scheduler_loaded_state_dict[\"min_lrs\"][0]\n",
        "  scheduler_loaded = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                    optimizer_loaded, mode=mode_loaded, factor=factor_loaded, patience=patience_loaded, threshold=threshold_loaded, min_lr=min_lr_loaded\n",
        "                )\n",
        "# elif scheduler_name_loaded == \"OneCycleLR\":\n",
        "#   max_lr_loaded = scheduler_loaded_state_dict[\"max_lr\"]\n",
        "#   epochs_loaded = scheduler_loaded_state_dict[\"epochs\"]\n",
        "#   steps_per_epoch_loaded = scheduler_loaded_state_dict[\"steps_per_epoch\"]\n",
        "#   pct_start_loaded = scheduler_loaded_state_dict[\"pct_start\"]\n",
        "#   max_lr_loaded = scheduler_loaded_state_dict[\"max_lr\"]\n",
        "#   scheduler_loaded = optim.lr_scheduler.OneCycleLR(\n",
        "#                     optimizer_loaded, max_lr=max_lr_loaded, epochs=epochs_loaded, steps_per_epoch=steps_per_epoch_loaded, pct_start=pct_start_loaded\n",
        "#                 )\n",
        "else:\n",
        "  scheduler_loaded = None\n",
        "\n",
        "if scheduler_loaded is not None:\n",
        "  # Added loading the state dict to the scheduler_loaded\n",
        "  scheduler_loaded.load_state_dict(scheduler_loaded_state_dict)\n",
        "\n",
        "# Loading the output of the training using pandas\n",
        "train_df_loaded = pd.read_csv(\"train_output.csv\")\n",
        "train_losses_loaded = train_df_loaded[\"train_loss\"].tolist()\n",
        "test_losses_loaded = train_df_loaded[\"test_loss\"].tolist()\n",
        "train_metrics_loaded = [\n",
        "    {\n",
        "        \"l1_norm\": train_df_loaded[\"train_l1_norm\"][i],\n",
        "        \"linf_norm\": train_df_loaded[\"train_linf_norm\"][i],\n",
        "    }\n",
        "    for i in range(len(train_df_loaded))\n",
        "]\n",
        "test_metrics_loaded = [\n",
        "    {\n",
        "        \"l1_norm\": train_df_loaded[\"test_l1_norm\"][i],\n",
        "        \"linf_norm\": train_df_loaded[\"test_linf_norm\"][i],\n",
        "    }\n",
        "    for i in range(len(train_df_loaded))\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "4IBfrQJq_MQU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "384ddace-3aba-4c9a-c46f-84ee90c010ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "164"
            ]
          },
          "metadata": {},
          "execution_count": 89
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "167"
            ]
          },
          "metadata": {},
          "execution_count": 89
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Huber'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Adagrad'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'CosineAnnealingLR'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1044, 914, 840]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 89
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ReLU'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ReLU'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0004971113311461239"
            ]
          },
          "metadata": {},
          "execution_count": 89
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ReLU()"
            ]
          },
          "metadata": {},
          "execution_count": 89
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ReLU()"
            ]
          },
          "metadata": {},
          "execution_count": 89
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (hidden_activation): ReLU()\n",
              "  (output_activation): ReLU()\n",
              "  (layers): ModuleList(\n",
              "    (0): Linear(in_features=3, out_features=1044, bias=True)\n",
              "    (1): Linear(in_features=1044, out_features=914, bias=True)\n",
              "    (2): Linear(in_features=914, out_features=840, bias=True)\n",
              "    (3): Linear(in_features=840, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 89
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'training': True,\n",
              " '_parameters': OrderedDict(),\n",
              " '_buffers': OrderedDict(),\n",
              " '_non_persistent_buffers_set': set(),\n",
              " '_backward_pre_hooks': OrderedDict(),\n",
              " '_backward_hooks': OrderedDict(),\n",
              " '_is_full_backward_hook': None,\n",
              " '_forward_hooks': OrderedDict(),\n",
              " '_forward_hooks_with_kwargs': OrderedDict(),\n",
              " '_forward_pre_hooks': OrderedDict(),\n",
              " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
              " '_state_dict_hooks': OrderedDict(),\n",
              " '_state_dict_pre_hooks': OrderedDict(),\n",
              " '_load_state_dict_pre_hooks': OrderedDict(),\n",
              " '_load_state_dict_post_hooks': OrderedDict(),\n",
              " '_modules': OrderedDict([('hidden_activation', ReLU()),\n",
              "              ('output_activation', ReLU()),\n",
              "              ('layers',\n",
              "               ModuleList(\n",
              "                 (0): Linear(in_features=3, out_features=1044, bias=True)\n",
              "                 (1): Linear(in_features=1044, out_features=914, bias=True)\n",
              "                 (2): Linear(in_features=914, out_features=840, bias=True)\n",
              "                 (3): Linear(in_features=840, out_features=1, bias=True)\n",
              "               ))]),\n",
              " 'n_layers': 3,\n",
              " 'n_units': [1044, 914, 840]}"
            ]
          },
          "metadata": {},
          "execution_count": 89
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SmoothL1Loss()"
            ]
          },
          "metadata": {},
          "execution_count": 89
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Adagrad (\n",
              "Parameter Group 0\n",
              "    differentiable: False\n",
              "    eps: 1e-10\n",
              "    foreach: None\n",
              "    initial_accumulator_value: 0\n",
              "    initial_lr: 0.0004971113311461239\n",
              "    lr: 0.0004243110622021168\n",
              "    lr_decay: 0\n",
              "    maximize: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 89
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'defaults': {'lr': 0.0004971113311461239,\n",
              "  'lr_decay': 0,\n",
              "  'eps': 1e-10,\n",
              "  'weight_decay': 0,\n",
              "  'initial_accumulator_value': 0,\n",
              "  'foreach': None,\n",
              "  'maximize': False,\n",
              "  'differentiable': False},\n",
              " '_optimizer_step_pre_hooks': OrderedDict(),\n",
              " '_optimizer_step_post_hooks': OrderedDict(),\n",
              " '_zero_grad_profile_name': 'Optimizer.zero_grad#Adagrad.zero_grad',\n",
              " 'state': defaultdict(dict,\n",
              "             {Parameter containing:\n",
              "              tensor([[ 0.2491, -0.0154, -0.5158],\n",
              "                      [-0.2211, -0.5437,  0.2490],\n",
              "                      [ 0.3927,  0.5422, -0.3364],\n",
              "                      ...,\n",
              "                      [-0.5098,  0.0773, -0.4235],\n",
              "                      [-0.1741,  0.1158, -0.2311],\n",
              "                      [-0.4212,  0.0641,  0.4889]], device='cuda:0', requires_grad=True): {'step': tensor(162992.),\n",
              "               'sum': tensor([[8.6594e-06, 1.1861e-06, 3.5638e-07],\n",
              "                       [7.9705e-05, 3.0766e-06, 1.9596e-04],\n",
              "                       [2.9655e-03, 1.1533e-02, 1.1469e-02],\n",
              "                       ...,\n",
              "                       [1.1669e-11, 2.3281e-11, 3.2442e-11],\n",
              "                       [8.8054e-07, 8.7520e-07, 7.5623e-07],\n",
              "                       [6.6338e-04, 1.3117e-03, 1.6853e-03]], device='cuda:0')},\n",
              "              Parameter containing:\n",
              "              tensor([-0.5349,  0.2851,  0.0322,  ...,  0.1765,  0.5665,  0.2583],\n",
              "                     device='cuda:0', requires_grad=True): {'step': tensor(162992.),\n",
              "               'sum': tensor([1.4020e-07, 2.3776e-06, 5.2954e-05,  ..., 1.9458e-09, 5.8587e-07,\n",
              "                       1.9002e-05], device='cuda:0')},\n",
              "              Parameter containing:\n",
              "              tensor([[ 0.0084, -0.0205,  0.0311,  ...,  0.0193,  0.0006,  0.0167],\n",
              "                      [ 0.0142, -0.0110,  0.0175,  ..., -0.0236, -0.0192, -0.0082],\n",
              "                      [ 0.0194, -0.0241,  0.0152,  ...,  0.0226, -0.0299, -0.0003],\n",
              "                      ...,\n",
              "                      [ 0.0283,  0.0114, -0.0209,  ..., -0.0059, -0.0151,  0.0089],\n",
              "                      [ 0.0130,  0.0149, -0.0033,  ..., -0.0183, -0.0262, -0.0195],\n",
              "                      [-0.0159,  0.0195,  0.0096,  ...,  0.0212,  0.0006, -0.0147]],\n",
              "                     device='cuda:0', requires_grad=True): {'step': tensor(162992.),\n",
              "               'sum': tensor([[2.2868e-06, 3.8248e-08, 8.2803e-02,  ..., 2.3901e-10, 4.7431e-07,\n",
              "                        5.7434e-02],\n",
              "                       [3.6098e-07, 3.7843e-09, 3.1614e-04,  ..., 2.5521e-10, 6.3654e-08,\n",
              "                        1.9081e-04],\n",
              "                       [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
              "                        0.0000e+00],\n",
              "                       ...,\n",
              "                       [0.0000e+00, 0.0000e+00, 6.6632e-09,  ..., 0.0000e+00, 0.0000e+00,\n",
              "                        1.2953e-08],\n",
              "                       [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
              "                        0.0000e+00],\n",
              "                       [8.1508e-12, 9.7509e-07, 2.9782e-03,  ..., 3.3430e-12, 1.0760e-08,\n",
              "                        3.2494e-03]], device='cuda:0')},\n",
              "              Parameter containing:\n",
              "              tensor([-1.8118e-02,  2.7817e-02, -1.9838e-02,  2.3841e-02,  1.5238e-02,\n",
              "                       1.8506e-02, -2.6745e-02, -3.1433e-02, -3.0839e-02,  8.7426e-03,\n",
              "                       1.7750e-02, -1.9132e-02,  4.8521e-03, -1.7422e-02,  1.7771e-02,\n",
              "                       5.9097e-03,  2.3067e-02, -3.8717e-03, -1.6859e-02, -1.0940e-02,\n",
              "                      -6.0716e-03,  9.5921e-03,  8.6659e-03,  2.3914e-02, -8.2484e-03,\n",
              "                      -1.1126e-02, -1.3545e-02, -6.9706e-03, -1.4684e-02,  1.0366e-02,\n",
              "                      -3.0602e-02, -2.5311e-04, -2.9148e-02,  1.3749e-02,  2.6279e-02,\n",
              "                       2.3415e-02,  3.0909e-02,  2.2240e-02, -2.0534e-04, -3.0910e-02,\n",
              "                      -2.9967e-02, -1.6821e-02, -1.6823e-02, -6.6773e-03, -1.9761e-02,\n",
              "                      -2.6331e-02,  7.7959e-03,  2.1958e-02, -5.8001e-03,  1.4815e-02,\n",
              "                      -9.5447e-03,  5.1434e-03,  1.6097e-02,  2.0478e-02, -8.1313e-03,\n",
              "                       1.6197e-02,  9.1496e-03,  2.5342e-02,  1.6137e-02, -2.5656e-02,\n",
              "                      -1.8861e-02, -3.8307e-03, -5.4298e-03,  8.8476e-03,  6.4336e-03,\n",
              "                      -1.3226e-02,  2.3352e-02,  1.9205e-02,  2.7015e-02, -6.4431e-03,\n",
              "                      -2.0183e-02, -1.7816e-02, -7.1554e-03, -1.2285e-02, -1.8457e-02,\n",
              "                      -2.5042e-02,  2.7155e-02,  2.2412e-02,  2.0798e-02, -1.9102e-02,\n",
              "                      -3.1425e-02,  1.6581e-02,  1.9337e-02,  1.2459e-02,  5.6943e-03,\n",
              "                       9.0794e-03,  2.9483e-02,  2.5612e-02, -4.5737e-03, -1.3002e-03,\n",
              "                      -1.9898e-02, -1.7501e-02, -2.8132e-02,  7.5364e-03, -1.9936e-03,\n",
              "                      -2.0954e-02, -7.1631e-04,  1.2868e-02, -2.4564e-02, -2.1866e-02,\n",
              "                       2.3896e-02, -1.3259e-02,  1.7876e-02,  2.8075e-02, -1.4995e-02,\n",
              "                      -2.7850e-03, -3.0166e-03,  2.8716e-02, -1.0577e-02, -7.1062e-03,\n",
              "                       4.9014e-03,  3.2766e-02, -2.0347e-02, -1.8240e-02,  1.7218e-02,\n",
              "                       1.1607e-03,  6.5541e-04, -2.8179e-02,  3.2313e-04,  1.8283e-02,\n",
              "                      -1.8563e-03, -1.6919e-02,  2.4340e-02,  2.7390e-02,  2.6575e-02,\n",
              "                       1.0088e-02, -2.0847e-04,  1.2015e-02,  2.4438e-02,  2.9886e-02,\n",
              "                      -1.9280e-02,  5.4056e-03,  2.5907e-02,  6.7308e-03,  2.9087e-03,\n",
              "                       8.9887e-03, -1.9327e-02, -2.1997e-02, -3.9602e-03, -1.5551e-02,\n",
              "                      -3.0551e-02,  2.5063e-02,  1.5483e-02,  7.8820e-03,  8.6799e-04,\n",
              "                      -5.1095e-03,  2.9765e-02, -6.1604e-03,  2.3014e-02,  3.6488e-03,\n",
              "                       2.2484e-02,  2.0513e-02, -2.4284e-02, -1.6262e-02, -1.4485e-02,\n",
              "                      -1.1174e-02,  1.9979e-02, -1.7574e-02,  2.5884e-02, -1.0794e-02,\n",
              "                       9.1818e-03,  1.9142e-02,  1.5617e-02,  1.8375e-02, -1.3344e-02,\n",
              "                      -2.0693e-02,  2.1482e-03, -2.1523e-02, -1.3912e-02, -2.3021e-02,\n",
              "                      -1.9401e-02, -8.3076e-03,  6.6115e-03,  2.0910e-02,  2.6615e-02,\n",
              "                       5.4168e-03, -2.9698e-02,  2.2153e-02, -1.3466e-04,  9.7449e-03,\n",
              "                       2.3699e-02, -2.3275e-02, -3.1955e-02,  1.9719e-02, -1.0907e-02,\n",
              "                       2.8429e-02,  1.9254e-03, -9.3987e-03, -1.0479e-02, -1.9665e-02,\n",
              "                      -1.1272e-02, -2.3966e-02,  2.9274e-02, -2.9660e-02,  1.2494e-02,\n",
              "                       8.3643e-03, -4.7802e-03,  2.0754e-02, -6.4386e-03, -1.0606e-03,\n",
              "                      -2.8425e-03, -3.5221e-03, -7.8872e-03, -1.7450e-02,  9.3602e-05,\n",
              "                       1.6344e-02,  8.8699e-03,  2.6997e-03, -1.0894e-02,  2.2183e-02,\n",
              "                      -7.5934e-03, -1.5766e-02,  1.3197e-02, -2.8102e-02,  1.8722e-02,\n",
              "                       2.9627e-02, -1.9444e-02, -3.0675e-02, -4.5557e-03,  1.6862e-02,\n",
              "                       2.9614e-02,  1.3836e-02, -1.2307e-04,  2.3630e-02,  1.9079e-02,\n",
              "                      -2.8872e-02, -2.7364e-02,  2.2552e-02,  9.0262e-03,  1.6525e-02,\n",
              "                       1.9304e-02, -1.2763e-02,  1.2107e-02,  1.2066e-02, -2.6160e-02,\n",
              "                       1.6968e-02, -1.9698e-02,  1.7990e-02,  6.7113e-03, -2.0761e-02,\n",
              "                       9.6888e-03,  1.6073e-03,  3.0049e-02,  8.8395e-03, -1.3277e-02,\n",
              "                      -3.0571e-03, -1.3384e-02, -1.6273e-02,  8.0172e-03, -1.5162e-02,\n",
              "                       2.5000e-02,  2.3237e-02,  2.6496e-02, -1.6572e-02,  2.7605e-02,\n",
              "                       6.9329e-03,  1.2048e-02,  3.6213e-03,  2.1627e-02, -2.1237e-02,\n",
              "                       1.9211e-02,  3.8263e-03, -3.1893e-03,  2.5782e-03,  2.7857e-02,\n",
              "                      -1.1668e-02,  4.7829e-03,  9.7774e-03, -1.1603e-02, -1.1157e-04,\n",
              "                       1.4537e-02, -1.6710e-02,  1.6391e-02,  1.5101e-02, -2.4145e-02,\n",
              "                      -1.3569e-02,  1.8777e-02,  8.7727e-03,  1.2942e-02, -1.6551e-02,\n",
              "                       2.5341e-02,  2.3251e-02,  3.4124e-03, -1.9081e-02,  6.7153e-03,\n",
              "                      -2.1702e-02, -3.2667e-03,  1.2888e-02, -1.8995e-02,  1.3751e-02,\n",
              "                      -1.1347e-02,  2.5117e-02, -2.2513e-04,  7.6802e-03,  2.2909e-02,\n",
              "                       4.1527e-03,  2.4187e-02, -2.3635e-02, -2.2223e-02, -7.9086e-03,\n",
              "                      -8.8565e-03,  1.5338e-02,  2.0760e-02,  1.8024e-02, -1.1566e-02,\n",
              "                      -9.7946e-03, -6.1387e-03, -9.8751e-03,  2.0857e-02, -2.5879e-02,\n",
              "                      -2.4259e-02, -2.1507e-02, -2.2012e-02,  1.6896e-02,  2.0769e-02,\n",
              "                       1.6404e-02,  1.5029e-02,  8.0733e-04, -7.0626e-03,  2.6044e-02,\n",
              "                       1.0507e-02,  8.0682e-03, -4.9415e-03,  7.9126e-03,  1.5225e-02,\n",
              "                       1.4061e-02,  2.2191e-02,  3.0241e-02,  3.1632e-03,  2.0591e-02,\n",
              "                      -2.5114e-02, -5.2946e-03,  2.5521e-02,  3.0078e-02, -9.1289e-03,\n",
              "                      -7.5353e-03, -1.3472e-02,  1.0194e-03, -2.5093e-02, -1.1598e-02,\n",
              "                       3.9075e-03,  2.8188e-02, -1.6021e-02,  5.4272e-03,  5.4096e-03,\n",
              "                      -1.1444e-02, -2.3850e-02,  3.0925e-03, -5.6242e-03, -2.3519e-02,\n",
              "                       1.9755e-02,  1.3664e-02,  7.7505e-03,  1.4545e-02, -2.4284e-02,\n",
              "                       2.9570e-03,  2.5232e-02,  8.7355e-03, -1.9063e-02, -2.7562e-02,\n",
              "                       3.0592e-02,  1.5665e-02, -1.3394e-02, -1.3365e-02, -3.6689e-03,\n",
              "                       1.4641e-02, -7.6485e-03, -5.6154e-04,  1.8096e-02, -2.5056e-02,\n",
              "                      -2.8741e-03,  8.2832e-03,  1.3541e-02,  1.4654e-02, -2.7058e-02,\n",
              "                      -1.1220e-02,  2.3834e-03,  2.9731e-02,  4.0272e-03,  2.3995e-02,\n",
              "                      -2.9039e-03, -6.5095e-03,  1.0198e-02, -1.8073e-02,  2.2547e-02,\n",
              "                       2.7533e-02, -3.0975e-02, -1.9335e-02, -2.8631e-02,  1.2945e-02,\n",
              "                      -2.4050e-03,  2.8984e-02, -2.3017e-02, -2.3147e-03, -1.1843e-02,\n",
              "                       2.4194e-02,  5.1528e-03,  6.1314e-03,  4.7550e-03,  1.4418e-02,\n",
              "                      -2.1870e-02,  2.3858e-03, -2.2174e-02, -1.7394e-02, -5.2301e-03,\n",
              "                      -1.0067e-02, -2.7866e-02, -9.3755e-03,  1.5629e-02,  2.4071e-02,\n",
              "                       2.0701e-03,  6.5893e-03,  2.2720e-02,  2.9220e-02, -1.3871e-02,\n",
              "                       6.8237e-03,  1.1073e-02, -2.9574e-02,  1.6362e-02, -1.0868e-02,\n",
              "                      -7.2435e-03, -2.7287e-02,  1.8707e-02,  2.0534e-02,  2.2766e-02,\n",
              "                      -2.7630e-02, -3.0816e-02, -3.9331e-03, -1.7676e-02,  5.8166e-03,\n",
              "                      -2.7955e-02,  4.7158e-03, -2.8129e-02,  1.0950e-02,  2.0204e-02,\n",
              "                       3.1067e-04,  2.6103e-02, -2.9586e-02,  2.6510e-02,  1.1562e-02,\n",
              "                       4.5301e-04,  2.5527e-02,  1.5000e-02, -1.0185e-02, -3.0959e-02,\n",
              "                       1.5899e-02,  1.2843e-03, -2.3301e-02, -2.4410e-02, -7.9867e-03,\n",
              "                       2.3622e-02, -2.5409e-02, -3.4703e-03,  2.0630e-02,  2.6553e-02,\n",
              "                      -6.0895e-03, -2.6139e-02,  7.4240e-03,  1.3288e-02, -1.9744e-02,\n",
              "                       6.1175e-03, -8.7067e-03, -8.9110e-04, -1.4531e-02,  2.5095e-02,\n",
              "                       7.0041e-03, -5.6488e-03,  2.2695e-02,  1.3583e-02,  1.5536e-02,\n",
              "                      -3.6452e-03,  2.5018e-02,  2.3305e-02,  2.6635e-02,  2.4868e-02,\n",
              "                       2.6701e-02, -1.3020e-03,  1.8816e-02,  1.4462e-02, -1.9436e-02,\n",
              "                      -8.5722e-03,  1.6436e-02, -1.8730e-02, -1.5612e-02,  1.0505e-02,\n",
              "                      -7.3045e-03, -9.2654e-03,  4.0530e-03,  2.0449e-02, -2.1521e-02,\n",
              "                       2.3009e-02,  4.5904e-03,  1.1671e-02, -2.6559e-02, -6.6724e-03,\n",
              "                       1.4720e-02, -1.3155e-03,  1.0224e-02, -4.8472e-03,  1.0899e-02,\n",
              "                      -8.0005e-03,  2.7024e-02, -2.3387e-02, -1.5996e-02,  4.2850e-03,\n",
              "                       1.7434e-02, -2.6449e-02,  1.9437e-02, -2.9781e-02, -3.2994e-04,\n",
              "                       2.5557e-02, -8.4039e-03, -2.3315e-04,  1.4014e-03, -2.9112e-02,\n",
              "                      -1.8403e-02, -2.4355e-02, -2.5911e-02, -1.8838e-02,  2.5219e-02,\n",
              "                       2.1696e-02, -1.4893e-02, -2.2284e-02,  2.9499e-02,  1.5011e-03,\n",
              "                       1.7513e-03, -6.8941e-03,  2.2283e-02,  2.7758e-02, -1.8171e-02,\n",
              "                       5.4178e-03, -7.2049e-03,  1.2354e-02,  2.1812e-02,  1.7427e-02,\n",
              "                       2.4205e-02,  1.4981e-02, -8.2413e-03,  2.2317e-02,  1.4127e-02,\n",
              "                      -1.2634e-02,  2.5160e-02, -2.4898e-03, -4.7699e-03,  1.5621e-02,\n",
              "                       3.3414e-03,  1.4538e-03, -2.3740e-02,  1.9958e-02,  1.7023e-02,\n",
              "                      -3.3535e-04, -2.1802e-03, -2.3860e-02, -1.5098e-02, -2.0221e-02,\n",
              "                       2.5401e-02, -9.3477e-03,  2.0335e-02,  7.3290e-03, -1.6413e-02,\n",
              "                      -2.9674e-02, -1.7253e-02,  1.4320e-02,  7.6620e-03,  2.7278e-02,\n",
              "                       2.3301e-02, -2.3178e-02, -2.4300e-02,  2.4014e-02,  3.4079e-03,\n",
              "                       4.7119e-03,  2.5677e-02,  8.8305e-03,  1.0131e-02,  2.0829e-02,\n",
              "                       2.0087e-02, -1.5111e-02,  1.7677e-03,  2.2972e-02,  1.2750e-02,\n",
              "                      -1.3178e-02,  3.4779e-03, -2.7756e-02,  3.8141e-03, -1.2839e-02,\n",
              "                       8.6578e-03, -7.6490e-03,  1.2918e-03,  2.6574e-02,  2.1223e-02,\n",
              "                      -2.6139e-02,  1.0425e-02, -1.1013e-02, -1.7062e-02, -1.2136e-02,\n",
              "                      -1.7372e-02,  1.8214e-02,  2.2429e-02,  7.1559e-03,  1.2937e-02,\n",
              "                       2.4048e-02,  4.0526e-03,  1.3215e-03,  2.3807e-02,  1.6253e-02,\n",
              "                       2.7084e-02,  2.2869e-02, -1.0692e-02,  2.6442e-02, -2.7687e-02,\n",
              "                      -1.8129e-03, -1.2693e-02,  2.9193e-02, -2.2183e-02, -1.2941e-02,\n",
              "                      -2.6905e-02, -1.0474e-02, -1.1379e-02, -2.8667e-02,  1.5728e-02,\n",
              "                      -1.1269e-04,  2.4486e-02,  1.5575e-02,  6.3477e-03, -3.8496e-03,\n",
              "                       2.7682e-02, -1.4838e-02,  4.0568e-03, -3.0306e-02,  1.0135e-02,\n",
              "                       1.4527e-02,  2.4086e-02, -1.3246e-03, -5.5541e-03, -3.0655e-03,\n",
              "                      -7.8492e-03,  7.9160e-03, -5.7563e-03, -9.3403e-03,  2.6658e-02,\n",
              "                       2.7359e-02,  1.7629e-02,  6.9327e-03, -2.7225e-02, -1.3017e-02,\n",
              "                      -2.7790e-02, -1.8800e-02,  2.6362e-02,  6.4411e-03, -2.3820e-02,\n",
              "                      -3.1337e-02, -1.7480e-02, -1.4079e-03,  1.2087e-02, -7.3949e-03,\n",
              "                       1.5092e-02,  1.1141e-02, -2.0506e-02, -9.5732e-03, -4.9644e-03,\n",
              "                      -2.8180e-02,  6.0945e-04,  1.5499e-02, -1.3375e-03, -1.7667e-02,\n",
              "                      -4.7949e-03,  1.5820e-02, -2.7158e-02, -1.8429e-02, -2.6913e-02,\n",
              "                       9.4847e-03,  1.2014e-02,  2.3916e-02, -1.9053e-02, -1.4085e-02,\n",
              "                       5.2371e-03, -4.7322e-03, -2.7768e-02,  2.3044e-02,  1.7862e-02,\n",
              "                       5.1652e-03,  5.7866e-03,  1.5405e-02,  3.0862e-02,  7.4468e-03,\n",
              "                      -2.8660e-02,  2.1098e-02, -1.2801e-02, -9.1947e-03, -1.4526e-02,\n",
              "                       3.5940e-03,  3.4609e-03, -1.5262e-03,  6.1190e-03,  1.3020e-02,\n",
              "                       1.8133e-02, -3.0893e-03,  9.3023e-03, -4.3035e-03, -5.8621e-03,\n",
              "                      -4.9882e-03,  1.9467e-02,  5.0334e-03,  1.7634e-02, -2.4214e-03,\n",
              "                       1.5824e-02,  1.1113e-02,  2.7371e-02,  1.7201e-03,  7.7691e-04,\n",
              "                       1.0473e-02,  7.1966e-03, -3.5944e-03,  2.9299e-02,  1.6452e-03,\n",
              "                      -1.4841e-02, -5.2132e-03,  1.7415e-02, -1.2846e-02,  1.4125e-02,\n",
              "                       1.7135e-02, -1.5827e-02,  1.8330e-02, -1.5705e-02, -4.1303e-03,\n",
              "                       2.9470e-02,  1.3214e-03, -3.0864e-02,  2.2606e-02, -1.4998e-02,\n",
              "                       1.7664e-02, -2.5116e-02,  5.5415e-03,  1.9734e-02, -2.0948e-02,\n",
              "                      -1.0499e-02, -4.4634e-03, -2.3454e-03, -2.5811e-02, -1.7683e-02,\n",
              "                      -2.5238e-02,  2.9903e-02, -1.3415e-02, -2.4076e-02,  8.0738e-03,\n",
              "                      -6.4178e-03,  2.6694e-02,  2.4435e-02, -2.0672e-02,  2.8067e-03,\n",
              "                      -1.1542e-02,  2.3122e-02, -2.7174e-02,  7.0581e-03,  4.4595e-03,\n",
              "                      -2.8221e-02, -1.0331e-02, -1.1479e-02, -2.8966e-02, -2.0666e-02,\n",
              "                       1.1246e-03,  2.5485e-02,  1.9940e-02, -2.4241e-03, -2.7133e-02,\n",
              "                      -6.4601e-05, -1.4391e-02,  3.9006e-03,  1.9059e-02,  1.4498e-02,\n",
              "                       1.6925e-02, -2.5129e-02,  6.7160e-03,  1.6520e-02, -1.1597e-02,\n",
              "                      -2.5776e-03, -1.0697e-02, -1.8838e-02, -1.8677e-03,  1.9182e-04,\n",
              "                       1.1723e-02, -1.2418e-03, -1.3915e-02, -1.5141e-02,  2.1583e-02,\n",
              "                      -3.6220e-03,  4.7592e-03,  6.0094e-03,  1.5773e-02,  1.8182e-02,\n",
              "                       2.8263e-02, -3.0791e-02, -2.1032e-02,  3.9093e-03, -2.7280e-02,\n",
              "                       2.0330e-02, -2.2542e-02, -2.1506e-02,  1.1711e-02, -1.7368e-02,\n",
              "                      -1.5672e-02, -9.6132e-03, -2.1427e-02, -1.0420e-02,  2.0561e-02,\n",
              "                       2.0470e-02, -8.8239e-03, -5.3848e-04, -1.3559e-02,  1.4518e-02,\n",
              "                       1.5428e-02,  2.5589e-02,  8.3284e-03,  1.7063e-02,  1.7706e-02,\n",
              "                      -1.9177e-02, -5.2429e-03,  8.2786e-03, -1.4832e-02, -1.7999e-02,\n",
              "                      -2.3194e-02,  4.3262e-03, -5.3708e-04,  5.0061e-03, -9.5460e-03,\n",
              "                       2.6492e-02,  2.0044e-02, -1.9577e-02, -2.6207e-02, -1.7423e-02,\n",
              "                      -2.8654e-02, -2.3089e-02,  2.0345e-02, -1.6634e-02,  1.5337e-02,\n",
              "                      -1.7777e-02, -2.9554e-02,  1.7709e-02, -1.8214e-02, -2.0106e-03,\n",
              "                      -2.8342e-02,  1.1914e-02, -2.4641e-02, -1.3474e-02, -3.6725e-03,\n",
              "                       3.8342e-03,  4.7166e-03,  2.7441e-02,  2.7444e-02,  3.5137e-03,\n",
              "                       2.6906e-02, -2.0687e-02,  2.6347e-02, -2.4380e-02, -2.8713e-03,\n",
              "                       2.2155e-02, -2.1435e-02, -8.5123e-03,  3.0116e-02,  1.0453e-02,\n",
              "                      -2.2873e-02, -7.3061e-03,  1.3505e-02, -1.0680e-02,  1.5951e-02,\n",
              "                      -3.5714e-03, -1.0233e-02,  1.5326e-02, -5.9924e-03,  1.9178e-02,\n",
              "                      -1.8455e-02, -1.6403e-02, -7.0622e-03,  2.1060e-02, -2.7514e-02,\n",
              "                      -1.5754e-02, -1.6234e-02,  2.9609e-02,  1.9457e-02, -1.6782e-02,\n",
              "                       2.5768e-02, -2.2089e-02,  1.7510e-02, -1.5594e-02,  1.6076e-02,\n",
              "                       3.8558e-03,  9.0380e-03, -2.5038e-02, -2.2602e-02, -1.9117e-02,\n",
              "                      -8.2552e-03, -5.6334e-03, -2.2476e-02,  2.0380e-02,  1.2791e-02,\n",
              "                      -2.8848e-02,  2.6836e-02,  1.6533e-02, -2.6037e-02,  3.0777e-03,\n",
              "                      -2.7272e-02, -2.8729e-02,  1.1994e-02, -2.3057e-02,  2.3210e-02,\n",
              "                      -2.6339e-02,  6.6061e-03,  1.9914e-02, -2.7028e-02,  2.1034e-03,\n",
              "                      -2.1373e-02,  6.3881e-04,  2.5240e-02, -3.3166e-03], device='cuda:0',\n",
              "                     requires_grad=True): {'step': tensor(162992.),\n",
              "               'sum': tensor([1.8178e-03, 8.8419e-06, 0.0000e+00, 1.4713e-05, 4.1363e-03, 2.3102e-03,\n",
              "                       3.7828e-08, 5.7241e-05, 0.0000e+00, 1.0672e-04, 0.0000e+00, 1.3188e-04,\n",
              "                       9.4690e-04, 3.4056e-04, 4.9437e-03, 2.3293e-05, 1.9509e-05, 0.0000e+00,\n",
              "                       0.0000e+00, 5.2333e-04, 5.5463e-04, 5.2893e-06, 6.5303e-06, 3.7043e-05,\n",
              "                       5.5802e-08, 0.0000e+00, 2.0201e-03, 1.7917e-03, 4.4481e-06, 1.2738e-07,\n",
              "                       2.3544e-05, 0.0000e+00, 2.8093e-09, 1.7909e-03, 2.3043e-04, 1.1906e-04,\n",
              "                       0.0000e+00, 9.1122e-06, 2.8932e-03, 3.2605e-06, 3.2378e-04, 3.5413e-05,\n",
              "                       2.0610e-04, 2.1229e-04, 8.9221e-05, 9.6064e-04, 0.0000e+00, 6.3053e-08,\n",
              "                       3.0481e-08, 3.3462e-05, 1.1397e-05, 2.1180e-05, 4.5748e-05, 0.0000e+00,\n",
              "                       0.0000e+00, 8.5413e-04, 3.9653e-03, 2.6040e-03, 8.0275e-06, 1.5020e-03,\n",
              "                       2.8603e-03, 3.2713e-06, 0.0000e+00, 1.2886e-03, 8.6722e-05, 8.0053e-05,\n",
              "                       6.1440e-04, 2.6785e-04, 4.9918e-06, 7.2947e-04, 2.4232e-05, 1.8366e-06,\n",
              "                       0.0000e+00, 1.7950e-03, 4.6178e-03, 3.3534e-04, 5.4991e-08, 1.9219e-03,\n",
              "                       9.0815e-04, 1.4646e-04, 1.4303e-05, 0.0000e+00, 8.8030e-05, 1.2855e-04,\n",
              "                       5.6149e-06, 4.3457e-12, 4.0869e-07, 6.3740e-05, 1.6735e-03, 9.3538e-06,\n",
              "                       3.2633e-05, 8.7070e-04, 1.1749e-03, 5.1439e-03, 8.0134e-06, 1.2795e-03,\n",
              "                       2.7341e-03, 0.0000e+00, 1.3178e-08, 0.0000e+00, 2.2557e-03, 2.2762e-04,\n",
              "                       6.9107e-05, 6.9905e-08, 0.0000e+00, 5.6098e-06, 3.8264e-06, 3.3114e-07,\n",
              "                       9.2938e-07, 0.0000e+00, 3.8667e-04, 1.3182e-04, 8.6293e-04, 2.4915e-04,\n",
              "                       3.9608e-12, 1.1707e-03, 9.3784e-06, 1.5970e-03, 4.1784e-04, 0.0000e+00,\n",
              "                       8.7399e-13, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.6960e-04,\n",
              "                       1.6823e-04, 1.1886e-04, 8.6633e-07, 4.3513e-04, 0.0000e+00, 1.2958e-04,\n",
              "                       0.0000e+00, 4.3869e-04, 8.7422e-04, 7.0369e-04, 9.2601e-05, 1.7172e-12,\n",
              "                       1.2189e-03, 1.0717e-06, 1.6318e-04, 0.0000e+00, 1.0707e-04, 7.7873e-04,\n",
              "                       3.3878e-05, 9.9165e-09, 9.2018e-05, 9.9309e-05, 4.0792e-04, 0.0000e+00,\n",
              "                       0.0000e+00, 2.4184e-05, 2.4206e-04, 2.3432e-04, 0.0000e+00, 2.9637e-04,\n",
              "                       4.8276e-05, 3.4652e-06, 1.6622e-03, 5.4601e-09, 5.1232e-05, 1.8219e-04,\n",
              "                       1.1178e-04, 2.5780e-03, 1.0120e-03, 1.3210e-04, 3.6287e-04, 0.0000e+00,\n",
              "                       3.5230e-08, 4.9495e-04, 6.0981e-05, 1.8753e-03, 1.3581e-03, 1.0863e-04,\n",
              "                       4.5461e-05, 5.7212e-05, 2.8478e-07, 1.5043e-04, 2.0721e-04, 8.3958e-04,\n",
              "                       3.7637e-04, 0.0000e+00, 4.8141e-05, 2.7997e-04, 6.3379e-06, 7.7339e-08,\n",
              "                       2.3950e-04, 5.5438e-05, 2.9546e-06, 2.1191e-04, 0.0000e+00, 2.7345e-07,\n",
              "                       1.4564e-04, 3.5029e-05, 2.3818e-04, 1.3155e-08, 2.3901e-03, 2.6351e-07,\n",
              "                       1.9339e-04, 0.0000e+00, 3.6826e-07, 1.0029e-05, 1.0675e-03, 1.3719e-06,\n",
              "                       1.4466e-03, 9.6529e-04, 0.0000e+00, 1.0067e-06, 0.0000e+00, 7.1739e-03,\n",
              "                       1.0996e-03, 1.2652e-03, 8.7846e-13, 3.8670e-04, 5.6041e-03, 2.6006e-05,\n",
              "                       1.0874e-04, 4.4075e-04, 6.1344e-03, 2.1399e-04, 7.7282e-06, 3.0037e-03,\n",
              "                       1.5108e-04, 5.5770e-04, 7.9713e-03, 2.1050e-03, 0.0000e+00, 2.7435e-07,\n",
              "                       9.5638e-04, 2.5797e-06, 2.4990e-03, 5.5720e-04, 0.0000e+00, 4.3270e-05,\n",
              "                       1.3632e-04, 2.2100e-04, 1.7904e-04, 8.6211e-05, 6.7222e-07, 0.0000e+00,\n",
              "                       0.0000e+00, 2.0633e-04, 2.1265e-06, 0.0000e+00, 7.8196e-03, 2.6247e-08,\n",
              "                       8.6987e-04, 3.3481e-05, 1.0058e-04, 4.2652e-05, 1.1517e-04, 9.0923e-05,\n",
              "                       1.3974e-04, 5.0837e-10, 5.2684e-04, 0.0000e+00, 0.0000e+00, 3.3561e-05,\n",
              "                       3.2505e-07, 3.5018e-07, 1.0155e-05, 7.3772e-04, 7.1837e-03, 2.8465e-04,\n",
              "                       3.0904e-05, 6.5590e-05, 1.0348e-04, 7.9774e-05, 3.6550e-05, 3.3209e-05,\n",
              "                       2.0752e-07, 5.9207e-03, 6.0036e-03, 5.0395e-04, 7.2728e-05, 1.6918e-05,\n",
              "                       3.4434e-08, 9.4686e-04, 4.9799e-04, 6.1786e-04, 1.5904e-03, 6.5162e-04,\n",
              "                       5.1374e-04, 9.7643e-07, 1.1683e-04, 2.6670e-06, 1.6813e-08, 6.3807e-05,\n",
              "                       9.8702e-05, 8.3345e-05, 1.4945e-03, 2.4639e-03, 5.7901e-05, 0.0000e+00,\n",
              "                       1.9644e-04, 5.4867e-03, 2.8138e-03, 1.4502e-04, 1.7008e-03, 1.0669e-03,\n",
              "                       8.2897e-04, 1.8248e-05, 1.4104e-03, 1.0957e-03, 1.0317e-05, 3.2038e-04,\n",
              "                       8.7963e-11, 6.1302e-05, 5.3411e-05, 0.0000e+00, 5.4554e-03, 1.7714e-04,\n",
              "                       2.9561e-03, 1.8787e-04, 1.2943e-07, 1.7913e-03, 1.0416e-04, 6.6177e-06,\n",
              "                       9.2525e-12, 0.0000e+00, 2.9168e-03, 3.5228e-06, 2.1761e-04, 9.4617e-05,\n",
              "                       8.9599e-06, 4.2370e-03, 9.5982e-05, 9.6232e-03, 6.1404e-04, 3.3488e-08,\n",
              "                       6.9221e-04, 4.8393e-05, 5.6399e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "                       1.6148e-05, 1.2440e-04, 1.0531e-04, 8.7926e-07, 1.3247e-04, 0.0000e+00,\n",
              "                       0.0000e+00, 7.1361e-04, 0.0000e+00, 0.0000e+00, 7.3980e-04, 2.9874e-04,\n",
              "                       3.9520e-03, 0.0000e+00, 7.9557e-05, 4.1799e-03, 6.5955e-03, 1.1543e-04,\n",
              "                       1.3819e-09, 2.9266e-04, 4.7076e-04, 5.4426e-04, 0.0000e+00, 6.6976e-04,\n",
              "                       9.3501e-04, 1.6154e-05, 0.0000e+00, 3.3875e-04, 5.5114e-04, 1.7160e-05,\n",
              "                       1.6596e-05, 4.0499e-05, 6.1104e-07, 1.2470e-03, 3.5060e-03, 1.3067e-05,\n",
              "                       2.2946e-03, 3.2189e-05, 2.3367e-04, 0.0000e+00, 1.7335e-04, 2.2981e-03,\n",
              "                       1.1861e-04, 2.7024e-03, 1.6597e-04, 1.6013e-04, 7.4004e-04, 6.9577e-04,\n",
              "                       0.0000e+00, 6.4554e-04, 1.4124e-07, 9.9347e-04, 2.9613e-03, 1.3063e-03,\n",
              "                       0.0000e+00, 0.0000e+00, 3.0821e-04, 3.9479e-04, 1.5526e-03, 1.7951e-05,\n",
              "                       0.0000e+00, 3.1672e-05, 1.1693e-04, 2.1946e-03, 2.9620e-04, 6.8978e-04,\n",
              "                       0.0000e+00, 0.0000e+00, 1.0141e-06, 7.3252e-04, 1.6139e-05, 1.4390e-04,\n",
              "                       7.1995e-05, 7.0265e-04, 1.2979e-04, 2.4285e-04, 6.3793e-05, 4.1131e-06,\n",
              "                       0.0000e+00, 5.1600e-05, 1.1408e-04, 0.0000e+00, 1.7924e-04, 1.3634e-03,\n",
              "                       4.3140e-04, 0.0000e+00, 3.7330e-07, 3.5409e-06, 4.1928e-04, 6.5249e-04,\n",
              "                       1.2099e-04, 5.7279e-06, 6.6098e-05, 1.5701e-04, 2.0068e-03, 0.0000e+00,\n",
              "                       5.4722e-06, 0.0000e+00, 1.4790e-03, 9.5612e-04, 1.5115e-05, 5.6443e-05,\n",
              "                       1.5541e-04, 6.7879e-04, 4.6793e-04, 1.5139e-03, 3.7100e-13, 1.5815e-04,\n",
              "                       2.3327e-07, 0.0000e+00, 3.6397e-04, 7.5191e-04, 1.8130e-04, 1.3100e-09,\n",
              "                       1.3047e-09, 2.0108e-05, 1.5396e-06, 4.4667e-04, 1.4004e-05, 2.7480e-05,\n",
              "                       0.0000e+00, 2.8302e-05, 0.0000e+00, 1.7830e-06, 1.6318e-05, 9.6891e-04,\n",
              "                       9.7745e-04, 4.7909e-05, 8.6215e-04, 9.8932e-07, 8.6373e-05, 4.4413e-12,\n",
              "                       1.5014e-04, 1.1533e-03, 1.0237e-03, 5.3248e-04, 1.8156e-03, 1.1929e-06,\n",
              "                       3.2741e-05, 0.0000e+00, 1.0182e-03, 2.4378e-04, 4.5750e-11, 0.0000e+00,\n",
              "                       6.9979e-05, 3.5394e-05, 1.3866e-05, 2.6431e-06, 1.9674e-04, 2.8228e-06,\n",
              "                       0.0000e+00, 2.6921e-05, 1.9110e-05, 1.5656e-05, 1.7652e-06, 6.9035e-14,\n",
              "                       1.4411e-03, 2.5373e-03, 2.1696e-04, 1.5000e-06, 0.0000e+00, 1.3172e-07,\n",
              "                       6.1196e-07, 0.0000e+00, 1.4053e-04, 4.8160e-04, 1.8673e-04, 5.0333e-04,\n",
              "                       0.0000e+00, 5.4435e-05, 4.7243e-03, 1.7146e-04, 3.7323e-11, 6.3268e-04,\n",
              "                       4.1341e-04, 1.9633e-04, 8.2519e-06, 1.9042e-04, 3.3496e-06, 1.3135e-04,\n",
              "                       2.7770e-04, 2.8978e-09, 0.0000e+00, 0.0000e+00, 9.6722e-10, 1.3740e-05,\n",
              "                       0.0000e+00, 2.8451e-03, 6.1022e-06, 5.0089e-06, 3.9918e-05, 5.5177e-05,\n",
              "                       1.7429e-05, 5.2871e-04, 4.5508e-04, 4.2993e-03, 1.0259e-03, 1.0707e-05,\n",
              "                       2.3420e-03, 1.6058e-03, 3.6381e-05, 2.3416e-06, 5.4767e-05, 1.3699e-03,\n",
              "                       0.0000e+00, 1.1143e-04, 1.9589e-05, 1.3769e-04, 2.4388e-04, 3.3882e-04,\n",
              "                       7.9592e-05, 2.9273e-05, 0.0000e+00, 2.1581e-05, 9.1396e-09, 4.0573e-05,\n",
              "                       9.3980e-05, 0.0000e+00, 3.0558e-04, 7.9765e-04, 2.9318e-03, 9.9732e-05,\n",
              "                       6.2645e-08, 1.2628e-05, 1.8080e-03, 2.8732e-10, 2.3433e-04, 2.6885e-04,\n",
              "                       8.1698e-06, 3.7233e-04, 2.1872e-09, 1.0398e-03, 6.2576e-04, 1.9722e-04,\n",
              "                       8.5000e-05, 0.0000e+00, 3.2990e-04, 2.3645e-05, 3.2957e-04, 2.0985e-08,\n",
              "                       1.9440e-04, 0.0000e+00, 1.3586e-03, 0.0000e+00, 3.2565e-08, 7.2663e-04,\n",
              "                       3.6273e-05, 1.9099e-05, 2.4048e-07, 0.0000e+00, 3.6012e-05, 3.6205e-04,\n",
              "                       2.5303e-09, 2.4455e-03, 2.8177e-05, 0.0000e+00, 2.5350e-03, 8.6275e-04,\n",
              "                       0.0000e+00, 1.6285e-03, 3.7482e-05, 7.2426e-04, 2.8834e-04, 0.0000e+00,\n",
              "                       2.0302e-04, 1.1556e-03, 4.3006e-05, 1.5188e-03, 0.0000e+00, 2.4467e-05,\n",
              "                       8.4526e-06, 1.5723e-03, 0.0000e+00, 1.3458e-03, 4.9009e-05, 1.3651e-03,\n",
              "                       3.1818e-03, 6.5907e-13, 7.2552e-08, 1.3898e-11, 0.0000e+00, 4.9426e-07,\n",
              "                       4.4435e-03, 2.2086e-04, 0.0000e+00, 3.6256e-04, 3.6735e-03, 2.4304e-05,\n",
              "                       2.1968e-04, 1.4533e-05, 3.8160e-03, 0.0000e+00, 5.1240e-05, 7.5322e-15,\n",
              "                       2.8178e-04, 1.0892e-04, 5.7151e-05, 0.0000e+00, 3.3585e-08, 1.2326e-05,\n",
              "                       2.4207e-04, 6.5718e-13, 3.2672e-05, 2.2977e-03, 1.4244e-07, 2.9218e-06,\n",
              "                       0.0000e+00, 2.3164e-06, 0.0000e+00, 4.4051e-05, 9.3146e-04, 3.1909e-08,\n",
              "                       2.2418e-10, 7.7099e-04, 6.3908e-05, 0.0000e+00, 2.7747e-05, 7.7253e-05,\n",
              "                       4.9083e-04, 5.1957e-05, 6.2455e-04, 7.0273e-04, 7.2636e-06, 8.3178e-06,\n",
              "                       2.7844e-04, 1.7998e-04, 0.0000e+00, 3.2715e-04, 3.6083e-05, 2.1135e-03,\n",
              "                       1.7316e-04, 1.3531e-04, 3.8667e-05, 2.2077e-03, 8.6450e-03, 1.4267e-04,\n",
              "                       5.5851e-05, 6.7854e-04, 1.3725e-03, 2.6057e-05, 1.3889e-06, 9.6433e-05,\n",
              "                       6.2828e-05, 3.4995e-04, 2.1720e-06, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "                       6.7847e-06, 3.2589e-05, 3.7428e-06, 4.6815e-07, 0.0000e+00, 8.9346e-05,\n",
              "                       0.0000e+00, 2.7728e-05, 2.9203e-06, 1.2159e-04, 7.1997e-04, 4.5983e-05,\n",
              "                       0.0000e+00, 4.6232e-05, 3.4197e-04, 2.5088e-05, 1.3357e-05, 1.5459e-04,\n",
              "                       8.5722e-06, 6.6653e-06, 2.7658e-05, 0.0000e+00, 4.7446e-03, 0.0000e+00,\n",
              "                       9.0066e-05, 2.5503e-04, 8.1965e-05, 2.8534e-05, 0.0000e+00, 2.3306e-06,\n",
              "                       1.7078e-04, 5.9173e-05, 2.2587e-05, 5.5886e-08, 3.9620e-04, 0.0000e+00,\n",
              "                       9.5628e-05, 0.0000e+00, 1.6697e-03, 2.5437e-07, 5.6937e-03, 5.2606e-07,\n",
              "                       3.9591e-03, 0.0000e+00, 3.7456e-06, 0.0000e+00, 1.7162e-03, 7.6250e-05,\n",
              "                       4.8377e-07, 2.4118e-04, 9.5065e-05, 1.5709e-03, 4.1235e-05, 2.9447e-09,\n",
              "                       1.4498e-04, 1.4710e-03, 6.5833e-05, 8.2601e-05, 0.0000e+00, 0.0000e+00,\n",
              "                       0.0000e+00, 2.3208e-05, 6.4536e-06, 2.9088e-03, 1.7784e-04, 1.3706e-04,\n",
              "                       4.7886e-05, 2.1293e-08, 0.0000e+00, 2.7513e-03, 1.6295e-08, 6.6660e-04,\n",
              "                       4.7617e-12, 3.0116e-05, 1.8064e-04, 4.5224e-04, 0.0000e+00, 2.5625e-05,\n",
              "                       2.1846e-07, 0.0000e+00, 7.9720e-04, 6.6112e-04, 2.4672e-04, 5.3585e-05,\n",
              "                       2.0108e-03, 7.2895e-07, 3.7522e-06, 9.5506e-05, 2.7983e-04, 1.5394e-07,\n",
              "                       0.0000e+00, 2.3516e-04, 0.0000e+00, 5.8053e-04, 1.1139e-04, 1.5747e-04,\n",
              "                       3.4509e-07, 1.0486e-04, 0.0000e+00, 3.1199e-05, 0.0000e+00, 1.7738e-03,\n",
              "                       8.5031e-04, 9.1827e-05, 2.4312e-12, 1.8071e-03, 2.1597e-03, 1.4596e-03,\n",
              "                       0.0000e+00, 2.4476e-03, 7.3213e-03, 8.4620e-05, 1.5275e-03, 1.7119e-03,\n",
              "                       3.9017e-04, 4.8051e-04, 1.3437e-03, 3.9011e-05, 0.0000e+00, 2.0466e-03,\n",
              "                       5.0309e-05, 2.7215e-06, 3.5377e-04, 2.2830e-03, 1.0259e-04, 0.0000e+00,\n",
              "                       2.5129e-06, 4.9285e-04, 1.4903e-04, 2.0009e-05, 2.7837e-05, 1.4380e-04,\n",
              "                       8.3923e-06, 6.2599e-05, 1.2365e-05, 4.5300e-03, 0.0000e+00, 2.0559e-04,\n",
              "                       2.0096e-07, 5.9236e-05, 7.5345e-08, 2.4044e-04, 0.0000e+00, 1.4498e-04,\n",
              "                       6.2194e-10, 3.7518e-04, 5.4151e-09, 1.5767e-05, 5.9699e-04, 0.0000e+00,\n",
              "                       5.0161e-05, 3.1908e-05, 5.2235e-06, 4.2107e-05, 2.5071e-09, 3.4301e-05,\n",
              "                       0.0000e+00, 7.6960e-05, 2.0595e-04, 0.0000e+00, 3.3602e-04, 3.1358e-04,\n",
              "                       8.3321e-06, 4.3193e-07, 1.3644e-08, 2.6082e-05, 1.9417e-03, 2.6046e-04,\n",
              "                       5.9844e-10, 1.6362e-10, 3.8424e-07, 1.9940e-04, 2.9268e-03, 2.2111e-08,\n",
              "                       2.5450e-05, 1.4658e-03, 1.3502e-04, 0.0000e+00, 2.3919e-03, 3.0953e-05,\n",
              "                       1.3435e-05, 4.2713e-05, 0.0000e+00, 4.7714e-04, 5.2251e-05, 4.0711e-04,\n",
              "                       3.4316e-03, 2.7699e-05, 1.6434e-03, 6.8070e-05, 2.3816e-05, 0.0000e+00,\n",
              "                       1.9953e-09, 3.4524e-08, 1.2325e-06, 4.7465e-05, 2.0181e-03, 0.0000e+00,\n",
              "                       0.0000e+00, 2.0547e-08, 3.0466e-04, 0.0000e+00, 5.8402e-05, 1.4179e-06,\n",
              "                       1.9820e-04, 7.2399e-07, 7.9666e-06, 1.6571e-03, 6.1282e-05, 1.2121e-06,\n",
              "                       2.7634e-05, 1.3732e-03, 0.0000e+00, 4.0589e-05, 6.3103e-04, 7.0776e-08,\n",
              "                       3.7233e-04, 4.1292e-09, 3.0147e-05, 4.7207e-04, 3.0295e-03, 0.0000e+00,\n",
              "                       1.5913e-04, 4.2377e-03, 3.4639e-04, 4.9632e-09, 4.6798e-03, 1.4531e-10,\n",
              "                       0.0000e+00, 1.5009e-04], device='cuda:0')},\n",
              "              Parameter containing:\n",
              "              tensor([[-0.0011,  0.0150, -0.0290,  ...,  0.0266, -0.0234, -0.0158],\n",
              "                      [ 0.0112, -0.0044,  0.0054,  ..., -0.0279, -0.0017, -0.0185],\n",
              "                      [-0.0184, -0.0007, -0.0313,  ..., -0.0153, -0.0048,  0.0041],\n",
              "                      ...,\n",
              "                      [ 0.0244, -0.0298,  0.0153,  ...,  0.0161, -0.0074,  0.0194],\n",
              "                      [-0.0193,  0.0254,  0.0143,  ..., -0.0110,  0.0213, -0.0299],\n",
              "                      [-0.0099, -0.0057,  0.0059,  ..., -0.0267,  0.0158,  0.0241]],\n",
              "                     device='cuda:0', requires_grad=True): {'step': tensor(162992.),\n",
              "               'sum': tensor([[4.3978e-04, 6.9996e-07, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
              "                        1.9122e-03],\n",
              "                       [1.1485e-03, 1.5676e-06, 0.0000e+00,  ..., 1.9688e-10, 0.0000e+00,\n",
              "                        8.0975e-03],\n",
              "                       [8.7574e-04, 1.0642e-06, 0.0000e+00,  ..., 1.7078e-10, 0.0000e+00,\n",
              "                        5.6916e-03],\n",
              "                       ...,\n",
              "                       [5.1190e-04, 6.9791e-07, 0.0000e+00,  ..., 8.5406e-11, 0.0000e+00,\n",
              "                        3.6175e-03],\n",
              "                       [1.0005e-03, 1.3651e-06, 0.0000e+00,  ..., 1.6892e-10, 0.0000e+00,\n",
              "                        7.0024e-03],\n",
              "                       [2.4698e-04, 3.0998e-07, 0.0000e+00,  ..., 3.9767e-11, 0.0000e+00,\n",
              "                        1.7336e-03]], device='cuda:0')},\n",
              "              Parameter containing:\n",
              "              tensor([ 1.8927e-02,  5.1221e-03, -6.3455e-03,  6.3523e-03, -1.8603e-03,\n",
              "                      -2.5010e-02,  3.1347e-02,  1.4582e-03, -9.5578e-03, -1.6248e-03,\n",
              "                      -7.8893e-03,  2.3840e-02,  2.9724e-02,  5.5908e-03,  9.4720e-04,\n",
              "                       2.1751e-02,  8.3106e-04,  1.5326e-02,  5.0339e-03, -2.9501e-02,\n",
              "                      -3.2940e-02,  1.7357e-02, -3.2578e-02,  8.6837e-03, -3.0859e-02,\n",
              "                      -1.6332e-02, -6.6037e-03,  2.7375e-04, -1.1204e-02, -1.3180e-02,\n",
              "                       2.9279e-03,  3.3989e-03,  2.4262e-02, -3.1681e-02, -1.0913e-02,\n",
              "                      -1.0458e-02,  1.6959e-02, -5.0488e-03, -3.0351e-02, -2.1643e-03,\n",
              "                      -1.4353e-02, -2.6382e-02,  3.0768e-02, -2.1259e-02,  1.1666e-02,\n",
              "                       1.9044e-02,  1.0232e-02,  2.8427e-02, -2.2469e-02, -2.1643e-02,\n",
              "                       2.2096e-02,  7.8680e-03,  1.8666e-02,  2.9698e-02,  2.2635e-02,\n",
              "                       1.7691e-02, -2.9671e-02,  4.9964e-03, -2.9822e-02,  7.9353e-03,\n",
              "                       9.7695e-03,  2.3912e-02, -3.2366e-02, -1.5210e-02,  6.0818e-03,\n",
              "                       1.8792e-02,  2.0241e-02,  5.5982e-03,  2.9330e-04, -2.4334e-02,\n",
              "                       6.4341e-03,  4.3976e-03, -1.1488e-02,  1.1159e-02,  1.0546e-02,\n",
              "                       1.3750e-02, -3.1680e-02,  1.7578e-02, -1.5640e-02, -2.1870e-02,\n",
              "                      -3.0378e-02,  2.7670e-02, -5.1787e-03, -1.8056e-03,  2.0900e-02,\n",
              "                       4.4609e-03, -1.4667e-02,  6.0511e-03, -1.8227e-02,  3.0482e-02,\n",
              "                       2.5518e-02,  1.4948e-02, -9.2702e-03,  1.9513e-02, -1.9906e-02,\n",
              "                       3.1905e-02, -2.5397e-02,  5.7974e-03,  5.7382e-04,  2.9320e-02,\n",
              "                       3.2204e-02,  1.2893e-02,  2.1015e-02,  2.2990e-02, -6.4969e-05,\n",
              "                       5.8146e-03, -2.2263e-02, -1.9236e-02,  2.1229e-02, -2.8150e-02,\n",
              "                       1.1730e-02,  2.3219e-02, -2.1322e-02, -1.6256e-02, -1.2405e-02,\n",
              "                      -2.9742e-02,  9.3833e-03, -2.8342e-02, -1.6681e-02,  2.5492e-02,\n",
              "                       2.2188e-02,  4.1535e-04, -3.1688e-02,  1.0771e-02, -2.0049e-02,\n",
              "                       5.1351e-03, -1.1425e-02, -3.0886e-02,  2.9006e-02,  1.8110e-02,\n",
              "                       2.2185e-02,  7.8402e-03,  1.2789e-02,  2.4833e-02,  2.8787e-02,\n",
              "                      -4.6257e-03,  1.6405e-02,  1.2906e-02,  2.3993e-02,  1.5517e-02,\n",
              "                      -2.6469e-02, -2.5906e-02,  7.7334e-03, -2.6463e-02,  1.0474e-03,\n",
              "                      -1.9411e-02,  1.7606e-02,  1.4320e-02, -7.9357e-03, -2.2951e-03,\n",
              "                       6.3760e-03, -2.6427e-02,  1.7020e-02, -1.0167e-02, -3.2292e-02,\n",
              "                       7.6268e-03, -1.0041e-02,  1.6680e-02, -2.9899e-02, -2.0065e-02,\n",
              "                      -3.2987e-02, -1.9525e-02, -9.3159e-03, -9.7834e-03, -1.9171e-02,\n",
              "                       1.8495e-02,  9.2168e-03,  5.5167e-03, -1.7018e-02,  2.4381e-02,\n",
              "                       7.0030e-03,  3.2638e-02, -2.6448e-02,  1.4497e-02, -9.7048e-03,\n",
              "                       2.5006e-02, -1.1184e-02, -1.4239e-03,  1.3675e-02,  3.1494e-02,\n",
              "                       1.3383e-02,  2.9359e-02,  3.0570e-02,  2.7051e-02, -2.2908e-04,\n",
              "                      -2.0143e-02, -6.7779e-03,  2.5610e-02,  5.8868e-03,  1.5429e-02,\n",
              "                      -7.6164e-03,  2.4390e-02,  2.7092e-02,  9.2754e-03,  2.9862e-02,\n",
              "                       3.0693e-02,  1.4166e-02, -1.7550e-02,  2.6814e-02,  1.4938e-02,\n",
              "                      -3.0595e-02,  2.6268e-03, -5.6717e-04, -1.0284e-02,  2.9573e-03,\n",
              "                       2.4493e-02, -1.2793e-02, -8.2947e-03,  8.6051e-03,  1.5251e-02,\n",
              "                      -1.2551e-03,  1.8370e-02,  2.0194e-02,  3.1704e-02,  1.6271e-02,\n",
              "                       5.7648e-03, -1.1754e-02, -2.8936e-02,  1.3602e-02, -2.0230e-02,\n",
              "                       2.3500e-02, -3.4909e-03,  6.8151e-03, -3.9713e-03, -1.9410e-02,\n",
              "                      -2.2285e-02, -8.1350e-03, -5.3547e-03, -3.1504e-02, -1.6675e-03,\n",
              "                       2.8465e-02,  1.1023e-02, -1.4967e-02,  3.3372e-02, -1.2001e-02,\n",
              "                       2.3860e-02, -3.2786e-02,  6.5018e-03,  4.8840e-03, -2.9866e-02,\n",
              "                      -3.1997e-02,  4.7691e-03, -5.3828e-03, -1.9831e-02, -1.1306e-03,\n",
              "                       7.1287e-04, -2.1314e-02,  2.2658e-02,  1.2472e-03, -1.7829e-02,\n",
              "                      -2.4073e-02,  1.2982e-04,  1.8906e-02,  2.6881e-02, -1.9781e-02,\n",
              "                       2.9587e-02,  2.0501e-02, -3.0865e-02, -2.2421e-02, -1.5331e-02,\n",
              "                      -5.9236e-03, -2.9065e-02,  1.8332e-02, -1.1057e-02,  3.2437e-03,\n",
              "                       3.0179e-02,  1.3030e-02,  2.8039e-03, -1.1443e-02, -1.6171e-02,\n",
              "                      -7.3875e-03,  2.9498e-02, -2.8567e-02, -1.7148e-02,  7.7138e-03,\n",
              "                      -9.3358e-03,  6.1467e-03,  2.0958e-02, -5.4689e-03,  2.9116e-02,\n",
              "                      -1.0117e-02,  1.8468e-02,  1.9251e-02,  8.7718e-04,  8.4298e-03,\n",
              "                       2.4617e-02,  2.0735e-02,  3.2348e-02,  1.5459e-02, -3.0858e-02,\n",
              "                       2.2610e-02,  1.2571e-02,  1.3680e-02,  7.0614e-03,  1.1473e-02,\n",
              "                       6.4096e-03, -2.4560e-02,  2.9356e-02, -5.1656e-03,  8.0530e-03,\n",
              "                      -2.3912e-02,  1.1003e-02,  2.2139e-04, -2.6259e-02, -3.0709e-02,\n",
              "                      -1.8203e-02,  1.7659e-02, -5.8809e-03, -1.2096e-02, -2.5559e-02,\n",
              "                      -2.1161e-03, -2.4080e-03, -1.4081e-02,  2.3075e-02, -1.3532e-02,\n",
              "                       1.4881e-02, -2.5931e-02, -1.9693e-02,  1.1995e-02,  1.7738e-02,\n",
              "                       1.7832e-02,  1.5712e-02, -1.7824e-02,  1.2335e-02,  2.7940e-02,\n",
              "                       2.9430e-02, -6.0981e-03, -3.9327e-03,  1.8676e-04, -1.6828e-02,\n",
              "                       9.6875e-03, -1.0320e-02, -1.5209e-02,  1.7820e-02,  3.7072e-03,\n",
              "                      -6.9283e-03,  2.4926e-02, -2.9640e-02,  1.6759e-02, -2.8035e-02,\n",
              "                      -2.9126e-02,  4.4247e-04,  2.5255e-02,  3.2039e-02, -7.6639e-04,\n",
              "                       1.8959e-02,  1.8422e-02,  1.0468e-02, -1.5606e-02, -1.7895e-02,\n",
              "                       2.0422e-04,  2.6132e-02,  1.9343e-02, -2.0902e-02, -3.1470e-02,\n",
              "                      -2.9382e-02,  5.9990e-03,  5.1359e-03,  2.8704e-02, -3.3020e-02,\n",
              "                      -8.3787e-04,  1.9185e-02,  1.3709e-02,  2.6157e-03,  2.7711e-02,\n",
              "                       4.4097e-03, -2.6750e-02,  1.2574e-02, -2.2876e-02,  4.3015e-03,\n",
              "                      -1.5464e-02, -3.0670e-02, -6.9692e-03,  6.6724e-03,  9.0484e-03,\n",
              "                       7.7590e-03, -2.2830e-02,  3.1140e-02, -2.9158e-02, -2.5565e-02,\n",
              "                       1.3649e-02,  2.1585e-02, -2.5726e-02,  7.5124e-03,  2.3015e-02,\n",
              "                       2.2905e-02, -1.2637e-02, -3.0784e-02, -2.9450e-02, -8.2776e-03,\n",
              "                       1.9575e-02, -1.1717e-03,  1.4080e-02, -3.2793e-02,  1.0969e-02,\n",
              "                      -5.7936e-03,  1.6334e-02,  2.6089e-02,  1.5582e-02, -1.8675e-02,\n",
              "                      -1.1825e-02, -3.9736e-04,  2.4679e-02, -2.8159e-02,  6.3136e-03,\n",
              "                      -1.2345e-02,  7.7562e-03,  1.1998e-02,  1.5554e-03, -9.4891e-03,\n",
              "                      -2.0778e-02, -4.1658e-03, -1.0279e-02, -1.5105e-02,  1.2113e-02,\n",
              "                       2.0310e-02, -7.7104e-03, -9.6433e-03,  2.1543e-02, -9.0736e-03,\n",
              "                      -1.2576e-02,  2.6052e-02,  2.8745e-02,  4.8979e-03,  1.9407e-02,\n",
              "                       2.3075e-02,  2.6245e-02,  1.4625e-03,  2.1202e-02, -2.5316e-02,\n",
              "                      -3.2156e-02, -2.1280e-02,  8.5685e-03, -1.0954e-02,  3.2335e-02,\n",
              "                      -3.0746e-02,  1.8819e-04, -2.8861e-02, -1.3456e-02, -5.0047e-03,\n",
              "                      -2.7469e-02,  3.0849e-02,  2.4916e-02, -9.5976e-03, -2.8835e-02,\n",
              "                       3.0585e-02,  2.0288e-02,  3.1159e-02, -3.2167e-02,  1.6660e-02,\n",
              "                      -8.6249e-03,  1.7406e-02, -3.0004e-02,  2.6126e-03, -3.0445e-02,\n",
              "                       1.4691e-02,  2.7817e-02,  2.0409e-02, -1.1569e-02,  1.8775e-02,\n",
              "                       3.3508e-03,  6.9867e-04, -9.1174e-03, -5.3175e-03,  2.7795e-02,\n",
              "                       7.8157e-03,  3.0626e-02, -2.3580e-03,  3.1309e-03,  1.0328e-02,\n",
              "                      -6.8944e-03,  2.8782e-02, -2.2018e-02,  2.8706e-02, -9.1219e-03,\n",
              "                      -1.1259e-02,  3.0469e-02,  2.2541e-02, -8.4406e-03, -1.6104e-02,\n",
              "                      -2.9304e-03, -4.8423e-03,  1.8177e-02,  1.6771e-03,  2.2772e-02,\n",
              "                      -1.3932e-02,  1.4532e-02,  3.2265e-02, -1.6483e-02,  1.0695e-02,\n",
              "                       2.1712e-02,  5.8775e-03,  2.9231e-02, -1.5308e-02, -3.6297e-03,\n",
              "                      -3.9448e-03, -9.0778e-03,  2.7032e-02, -4.2240e-04,  1.0461e-02,\n",
              "                      -5.2272e-03,  2.4845e-02,  3.2272e-02, -2.5079e-02, -9.7401e-03,\n",
              "                       3.3530e-02,  2.9799e-02, -1.8699e-02, -2.1961e-02, -9.2221e-03,\n",
              "                       2.1172e-03,  3.3247e-04,  3.0518e-02, -1.3045e-02, -1.3759e-02,\n",
              "                       2.7634e-02,  4.0073e-03, -7.2399e-03, -4.2636e-03, -3.4953e-02,\n",
              "                       1.8751e-02,  2.5191e-02, -2.7228e-03,  2.9692e-02,  3.1514e-02,\n",
              "                       1.6832e-02, -2.5429e-02, -1.2584e-02, -1.3050e-02, -2.6537e-02,\n",
              "                       2.7773e-02, -3.2396e-03,  2.4540e-02, -2.1761e-02, -8.4514e-03,\n",
              "                       7.5165e-03,  2.1400e-02, -1.6529e-02,  2.8550e-02, -1.8732e-02,\n",
              "                       2.0587e-02, -1.6782e-02, -7.5184e-04,  1.0302e-02,  1.7501e-02,\n",
              "                       1.6918e-02, -2.1261e-02, -6.4355e-03,  1.7775e-02, -1.1328e-02,\n",
              "                      -3.1278e-02,  2.0045e-02, -2.1787e-02, -2.2428e-03,  2.9343e-02,\n",
              "                      -9.3957e-03,  8.0241e-03,  2.8974e-02, -1.5669e-02, -7.3951e-03,\n",
              "                       2.7528e-02,  1.1322e-02,  1.6284e-02,  2.8990e-02, -3.0958e-03,\n",
              "                      -2.4017e-02, -9.3226e-03,  1.8039e-02, -1.0251e-02,  8.9491e-03,\n",
              "                       2.3754e-03, -3.9807e-03, -4.4466e-03,  1.3898e-02,  1.5712e-03,\n",
              "                      -2.0080e-02, -3.2700e-02,  2.7509e-02, -3.1064e-02, -1.8094e-02,\n",
              "                      -2.4836e-03,  1.7167e-02,  2.7292e-02,  2.3941e-02,  4.8119e-03,\n",
              "                      -1.6045e-02, -5.9072e-03, -2.7343e-02,  2.8222e-04,  9.8365e-03,\n",
              "                       1.7415e-02,  1.8912e-02, -1.3324e-02,  1.6349e-02, -2.8214e-02,\n",
              "                       8.4565e-03,  1.1052e-02, -5.7405e-03,  1.1273e-02, -2.1949e-03,\n",
              "                      -1.7726e-02,  1.8747e-02,  2.4196e-02, -1.0137e-02, -2.3812e-02,\n",
              "                      -1.2278e-02, -1.4339e-02, -6.2420e-03, -2.5390e-02, -2.3117e-03,\n",
              "                       1.2715e-02,  2.0863e-02,  1.3462e-02, -4.2703e-04, -3.9571e-03,\n",
              "                      -5.0714e-03,  1.9301e-02, -9.4622e-04,  1.5198e-02, -2.4975e-02,\n",
              "                       1.0043e-02,  8.1847e-03, -2.6837e-02, -1.2704e-02, -2.5560e-02,\n",
              "                       2.3369e-02, -9.2318e-03,  2.1868e-02,  2.1909e-02, -3.5216e-03,\n",
              "                       7.0945e-03, -1.0589e-02,  1.1031e-02,  7.1564e-03,  1.2713e-02,\n",
              "                      -1.3993e-02, -3.6386e-03,  1.5999e-02, -1.3954e-02, -1.5165e-02,\n",
              "                      -1.9999e-02,  3.3830e-03, -2.6046e-02,  5.2853e-03, -1.1573e-02,\n",
              "                       2.3597e-02, -4.6195e-03, -2.7684e-02, -2.7404e-02,  1.2742e-02,\n",
              "                       1.6137e-02, -9.1372e-03,  2.7146e-02,  2.2029e-02,  2.6347e-02,\n",
              "                      -2.5641e-02,  2.2225e-02,  4.2598e-03,  2.7160e-02,  1.3085e-02,\n",
              "                      -1.1619e-02,  3.2142e-03,  2.4655e-02,  1.5217e-02, -1.3173e-02,\n",
              "                       1.9245e-02,  1.2351e-03, -7.5416e-03, -7.0327e-03,  3.0604e-02,\n",
              "                      -1.1170e-02,  1.2449e-02, -3.2878e-02,  3.4741e-02,  1.1468e-02,\n",
              "                      -3.6181e-03,  2.9950e-02, -2.9271e-02, -1.0523e-02,  3.9430e-03,\n",
              "                      -3.2741e-02,  5.2178e-03,  2.3637e-02, -4.1829e-03,  2.5731e-02,\n",
              "                       2.7388e-02,  1.0659e-02,  9.0068e-03,  1.9524e-02,  2.6013e-02,\n",
              "                      -3.1591e-02,  3.1329e-02,  2.3098e-02, -2.4066e-02, -2.0023e-02,\n",
              "                       8.8087e-03, -1.4433e-03, -2.3289e-02,  2.8438e-02,  3.1747e-02,\n",
              "                      -1.3471e-02,  3.1723e-02, -4.8878e-04,  1.8693e-02,  2.4573e-02,\n",
              "                      -6.0951e-04, -2.8182e-02,  6.6607e-04, -2.1510e-02, -2.9051e-02,\n",
              "                      -1.4550e-02,  1.4416e-02, -2.5855e-02,  1.1109e-02, -1.3412e-02,\n",
              "                      -1.8272e-02, -6.6001e-03,  7.7085e-03,  2.9565e-02,  5.0854e-04,\n",
              "                      -1.5329e-02,  1.9919e-02,  9.7481e-04, -2.7359e-02,  6.5234e-03,\n",
              "                      -1.8656e-02,  3.1190e-02, -3.2748e-02, -1.9377e-02,  1.2248e-02,\n",
              "                       3.0604e-02,  3.0263e-02, -1.4544e-02, -4.6499e-03,  4.8474e-03,\n",
              "                      -1.9061e-02, -1.1660e-02,  1.2246e-02,  2.6904e-02, -1.2875e-02,\n",
              "                      -3.2081e-02, -3.1931e-02,  2.0139e-03,  1.3254e-02,  1.8403e-02,\n",
              "                      -4.1436e-03,  6.0135e-03,  2.5694e-02,  2.0937e-05,  1.2095e-02,\n",
              "                       9.4952e-04,  2.0200e-02, -1.9913e-02,  7.6790e-03,  2.6354e-02,\n",
              "                       2.7277e-02, -3.3127e-02, -1.6153e-03, -9.6690e-03, -5.7313e-03,\n",
              "                      -2.1667e-02,  2.5687e-02,  2.3512e-02, -1.5623e-02,  5.3760e-03,\n",
              "                      -2.1109e-02, -1.6786e-02, -1.8331e-02,  2.5909e-02, -1.7662e-02,\n",
              "                      -2.8497e-02, -3.4930e-03,  4.2250e-04,  6.5064e-04,  2.0336e-03,\n",
              "                      -7.1874e-03,  2.3845e-02, -1.9640e-02, -2.6436e-02,  2.9493e-02,\n",
              "                       1.3693e-02,  1.6125e-02, -1.0303e-02,  7.4460e-03, -1.3200e-02,\n",
              "                      -2.2474e-02, -2.1977e-02, -9.9615e-03,  5.5570e-03,  1.7066e-02,\n",
              "                       2.4319e-02,  1.6411e-02, -3.0866e-02,  2.7912e-02,  2.3383e-02,\n",
              "                      -7.7102e-03,  2.4872e-02, -2.1051e-02,  1.3724e-02,  1.7152e-02,\n",
              "                      -7.3422e-03,  2.6152e-02, -3.0895e-02,  6.2165e-03, -2.0641e-02,\n",
              "                      -1.6931e-03,  7.7407e-03,  2.2044e-03,  2.1164e-04,  7.5273e-04,\n",
              "                      -2.1238e-02,  1.7640e-02, -1.3325e-02,  1.6020e-02,  5.1311e-03,\n",
              "                       1.6632e-02,  2.4890e-02,  2.0036e-02,  1.3413e-02,  1.3770e-02,\n",
              "                       2.1385e-02, -3.6739e-03,  3.9765e-03,  2.8669e-02, -1.7709e-02,\n",
              "                      -4.3481e-03,  2.7301e-02, -3.0285e-02,  2.4981e-02,  2.4199e-02,\n",
              "                       1.2630e-02,  1.4794e-02,  5.6664e-03,  1.3399e-02,  1.5550e-02,\n",
              "                       2.5306e-02, -2.5831e-02, -7.6578e-03,  1.7840e-02, -1.5370e-02],\n",
              "                     device='cuda:0', requires_grad=True): {'step': tensor(162992.),\n",
              "               'sum': tensor([3.0551e-03, 8.7659e-03, 4.1526e-03, 5.6205e-03, 1.5546e-03, 1.5415e-03,\n",
              "                       7.9985e-04, 6.1858e-09, 7.0961e-04, 1.3380e-04, 1.0757e-04, 2.2458e-04,\n",
              "                       4.7378e-05, 0.0000e+00, 3.9525e-03, 0.0000e+00, 2.7456e-06, 1.4638e-03,\n",
              "                       0.0000e+00, 8.9440e-05, 0.0000e+00, 6.4276e-05, 0.0000e+00, 9.9169e-05,\n",
              "                       4.6580e-03, 3.9782e-03, 1.1951e-02, 4.6025e-05, 2.3884e-09, 2.2266e-03,\n",
              "                       8.7271e-05, 1.8168e-03, 1.4763e-03, 4.0890e-05, 3.9509e-03, 6.7695e-04,\n",
              "                       1.1902e-02, 1.1754e-02, 1.7458e-03, 1.7968e-03, 9.2229e-03, 5.8503e-05,\n",
              "                       9.1887e-04, 8.4040e-05, 5.9834e-07, 3.9367e-03, 1.7477e-05, 2.0845e-05,\n",
              "                       4.3750e-03, 4.5062e-03, 5.2115e-04, 0.0000e+00, 5.5810e-07, 4.3912e-07,\n",
              "                       1.1761e-02, 4.0482e-04, 8.0187e-10, 5.5158e-03, 2.2349e-07, 1.2279e-03,\n",
              "                       1.8335e-04, 1.7094e-03, 3.3419e-04, 3.5425e-04, 1.7460e-06, 1.2009e-03,\n",
              "                       1.0647e-04, 1.0958e-03, 1.7325e-03, 2.1930e-13, 6.6666e-04, 5.9891e-06,\n",
              "                       1.1495e-03, 3.8921e-04, 4.4986e-03, 2.1872e-05, 2.2617e-04, 0.0000e+00,\n",
              "                       1.3992e-02, 1.9759e-04, 1.3532e-04, 9.6458e-05, 3.6515e-05, 2.6498e-08,\n",
              "                       8.5901e-04, 3.8270e-07, 2.9316e-03, 9.4305e-03, 2.6079e-03, 2.7813e-03,\n",
              "                       2.8713e-04, 6.6121e-10, 8.2106e-04, 1.9314e-03, 7.2950e-08, 3.5287e-04,\n",
              "                       1.3101e-02, 2.5481e-03, 1.3327e-04, 8.9948e-03, 2.5087e-03, 1.0838e-02,\n",
              "                       0.0000e+00, 1.1074e-04, 6.5949e-04, 4.4922e-07, 8.8070e-03, 0.0000e+00,\n",
              "                       1.4822e-04, 3.2507e-04, 3.2857e-08, 3.9637e-03, 1.1969e-02, 1.3284e-02,\n",
              "                       1.1545e-03, 4.2310e-08, 2.8365e-13, 4.9381e-03, 8.6603e-06, 9.9029e-03,\n",
              "                       7.3908e-03, 2.3688e-04, 1.1805e-05, 2.8047e-03, 9.1555e-05, 4.1039e-03,\n",
              "                       2.3931e-04, 4.0265e-05, 3.3763e-07, 2.0462e-03, 7.0422e-05, 5.2475e-03,\n",
              "                       2.4431e-09, 3.6257e-05, 3.7264e-03, 7.2015e-03, 1.1043e-03, 7.7323e-05,\n",
              "                       4.5401e-03, 3.3839e-03, 2.6531e-04, 1.3836e-02, 4.4551e-03, 2.6561e-11,\n",
              "                       3.9951e-03, 1.8609e-04, 1.0500e-02, 3.6057e-05, 6.7229e-10, 2.8457e-05,\n",
              "                       5.3092e-03, 1.5599e-04, 7.9105e-03, 1.0990e-03, 1.5247e-05, 9.2998e-03,\n",
              "                       5.8867e-06, 1.4884e-04, 2.3201e-04, 2.8248e-05, 2.2065e-04, 1.2359e-03,\n",
              "                       1.0459e-03, 2.4358e-07, 9.2976e-04, 7.1682e-04, 1.8385e-03, 2.6853e-04,\n",
              "                       5.1461e-05, 1.7010e-11, 1.7375e-04, 5.9722e-03, 6.4810e-06, 1.1317e-05,\n",
              "                       1.5007e-04, 2.5070e-03, 6.5270e-04, 6.0765e-07, 8.7249e-04, 4.5312e-10,\n",
              "                       3.3550e-14, 3.8944e-05, 1.1000e-07, 1.0932e-02, 9.5532e-03, 1.5206e-04,\n",
              "                       5.9602e-05, 2.6213e-03, 3.7931e-05, 1.6600e-04, 5.1968e-06, 5.1768e-06,\n",
              "                       4.5139e-03, 0.0000e+00, 2.6326e-08, 6.1911e-03, 3.5473e-03, 1.0475e-05,\n",
              "                       3.3985e-07, 4.7959e-05, 1.4386e-02, 4.6841e-05, 3.9127e-04, 3.8240e-03,\n",
              "                       1.6434e-04, 7.5633e-03, 8.7687e-03, 2.1317e-04, 1.0785e-04, 5.1720e-03,\n",
              "                       6.7898e-06, 3.4600e-05, 0.0000e+00, 2.8750e-03, 3.5309e-05, 1.4157e-10,\n",
              "                       3.3643e-03, 1.1309e-02, 2.1903e-05, 7.2750e-03, 3.1140e-06, 1.2103e-04,\n",
              "                       4.6499e-04, 1.4594e-09, 2.7416e-03, 6.0242e-04, 9.7110e-05, 8.1218e-03,\n",
              "                       1.1602e-02, 1.8920e-04, 5.4721e-03, 8.0788e-10, 7.2210e-03, 1.6126e-03,\n",
              "                       0.0000e+00, 5.8668e-07, 4.0470e-04, 9.9216e-05, 2.0347e-03, 2.7224e-06,\n",
              "                       0.0000e+00, 6.0298e-03, 0.0000e+00, 1.3812e-04, 2.0618e-08, 2.5332e-05,\n",
              "                       1.8448e-04, 1.4913e-06, 1.2132e-02, 5.4580e-09, 2.6802e-03, 6.7662e-04,\n",
              "                       1.9941e-05, 3.4993e-04, 2.5174e-05, 3.7606e-03, 8.8500e-04, 4.5088e-03,\n",
              "                       1.4020e-05, 3.3614e-03, 1.0097e-03, 3.0518e-04, 6.4256e-04, 4.9520e-03,\n",
              "                       4.6536e-04, 1.5051e-04, 8.2350e-04, 2.8311e-03, 1.2124e-02, 8.3650e-05,\n",
              "                       1.3090e-04, 2.7986e-03, 1.3460e-02, 3.7799e-04, 1.3907e-02, 1.2288e-05,\n",
              "                       0.0000e+00, 2.9150e-05, 7.7204e-04, 3.1102e-04, 4.6170e-09, 9.7675e-03,\n",
              "                       1.9906e-04, 1.2529e-04, 3.5747e-07, 1.0634e-03, 3.4578e-03, 2.0395e-06,\n",
              "                       1.3936e-03, 0.0000e+00, 2.7318e-04, 6.3933e-04, 5.1151e-05, 2.5028e-05,\n",
              "                       0.0000e+00, 4.6610e-04, 1.5179e-09, 2.1189e-05, 4.3905e-11, 1.3164e-04,\n",
              "                       3.8516e-03, 6.2673e-03, 1.9933e-03, 3.1465e-06, 2.2437e-04, 2.3354e-04,\n",
              "                       2.8856e-03, 1.2556e-04, 1.9283e-03, 8.0599e-05, 2.0160e-06, 4.7527e-05,\n",
              "                       4.1368e-07, 3.9791e-03, 2.1418e-04, 0.0000e+00, 2.9000e-05, 2.3097e-03,\n",
              "                       1.4875e-04, 3.9179e-04, 7.6968e-12, 8.3173e-03, 4.6006e-04, 6.7912e-04,\n",
              "                       3.7259e-05, 1.2713e-04, 0.0000e+00, 3.7309e-03, 0.0000e+00, 4.1353e-04,\n",
              "                       4.3458e-04, 3.5546e-03, 8.5586e-05, 1.5399e-03, 8.6546e-09, 0.0000e+00,\n",
              "                       5.6824e-07, 5.6625e-04, 8.9713e-03, 7.0846e-05, 0.0000e+00, 1.7139e-05,\n",
              "                       5.5400e-03, 8.9517e-04, 1.0375e-02, 3.4876e-05, 1.5620e-06, 7.1570e-05,\n",
              "                       5.7749e-05, 3.6105e-03, 1.2859e-08, 3.9210e-04, 8.5671e-08, 7.4371e-03,\n",
              "                       6.2792e-03, 6.0200e-04, 4.7977e-03, 1.4163e-03, 2.9675e-04, 1.2867e-03,\n",
              "                       1.7917e-03, 1.2675e-03, 0.0000e+00, 1.0607e-02, 1.6802e-04, 0.0000e+00,\n",
              "                       1.6804e-04, 1.9964e-04, 7.2390e-03, 5.2439e-04, 2.4138e-03, 1.1498e-02,\n",
              "                       3.9103e-03, 3.9957e-03, 8.9320e-05, 1.4986e-03, 1.6220e-10, 1.2181e-02,\n",
              "                       0.0000e+00, 2.2690e-03, 1.4703e-04, 5.8400e-04, 2.2584e-04, 8.6558e-04,\n",
              "                       1.6012e-05, 5.2667e-04, 9.6933e-05, 8.9585e-04, 3.9045e-04, 4.1112e-05,\n",
              "                       4.8630e-08, 2.9135e-03, 5.0859e-03, 1.6317e-07, 1.5063e-04, 3.4858e-07,\n",
              "                       1.9112e-03, 2.4666e-03, 1.6436e-04, 2.1848e-04, 8.7278e-10, 8.6750e-03,\n",
              "                       2.5974e-03, 3.3470e-03, 1.1667e-04, 4.3583e-03, 1.1958e-03, 1.0276e-02,\n",
              "                       2.9459e-04, 5.1255e-06, 2.5170e-04, 1.4952e-04, 3.4720e-04, 1.2518e-03,\n",
              "                       5.0695e-03, 4.5571e-08, 2.3176e-03, 2.5710e-08, 7.6818e-04, 5.2044e-04,\n",
              "                       8.9604e-03, 2.0054e-05, 0.0000e+00, 1.1479e-09, 1.2136e-04, 4.7247e-07,\n",
              "                       2.8038e-03, 6.5588e-04, 9.2613e-05, 1.3746e-02, 0.0000e+00, 2.4586e-06,\n",
              "                       7.8180e-04, 1.3842e-02, 1.0367e-03, 5.4298e-03, 2.5372e-08, 1.0807e-03,\n",
              "                       2.1429e-05, 5.9584e-03, 2.8392e-03, 1.4713e-03, 1.8898e-05, 1.1901e-03,\n",
              "                       4.5888e-06, 1.4563e-04, 2.2733e-03, 6.5454e-16, 1.3949e-02, 1.7366e-08,\n",
              "                       1.8213e-04, 2.2524e-05, 4.8234e-04, 0.0000e+00, 1.0913e-02, 9.1228e-04,\n",
              "                       1.3215e-04, 1.0290e-02, 2.0953e-04, 1.8324e-04, 5.3684e-09, 5.7960e-04,\n",
              "                       1.2613e-02, 4.8656e-06, 2.3456e-03, 1.2114e-03, 1.1778e-03, 4.0733e-03,\n",
              "                       2.6117e-05, 5.5424e-04, 2.8724e-06, 9.1693e-06, 9.9891e-03, 3.0122e-03,\n",
              "                       9.9088e-09, 0.0000e+00, 7.0732e-03, 1.9507e-06, 0.0000e+00, 7.9848e-04,\n",
              "                       0.0000e+00, 8.7340e-04, 3.1667e-03, 2.9456e-04, 2.5771e-04, 7.0470e-03,\n",
              "                       1.3747e-03, 6.6786e-07, 7.7035e-05, 9.6045e-04, 4.8201e-04, 4.0022e-03,\n",
              "                       1.7286e-06, 1.2834e-06, 4.5211e-04, 6.1648e-03, 5.3615e-04, 1.4144e-02,\n",
              "                       2.6363e-03, 2.1058e-12, 1.6448e-06, 0.0000e+00, 2.0427e-03, 6.5324e-04,\n",
              "                       1.4748e-09, 3.0620e-03, 2.5554e-05, 2.9892e-04, 4.9810e-03, 5.9781e-04,\n",
              "                       5.0313e-04, 1.2522e-04, 1.6368e-03, 3.5304e-04, 1.2899e-02, 1.2520e-03,\n",
              "                       2.9911e-04, 1.5766e-03, 7.3670e-03, 6.3832e-07, 3.1454e-04, 4.7919e-04,\n",
              "                       2.8755e-03, 0.0000e+00, 3.6736e-03, 3.3946e-03, 1.2123e-03, 1.0442e-02,\n",
              "                       2.9178e-04, 2.1827e-03, 4.3787e-03, 2.0178e-03, 9.0031e-03, 1.1669e-03,\n",
              "                       2.5460e-03, 5.9128e-04, 3.2639e-05, 4.8655e-04, 1.2559e-03, 1.2114e-04,\n",
              "                       5.3249e-04, 0.0000e+00, 5.0832e-08, 0.0000e+00, 1.3648e-02, 1.2956e-02,\n",
              "                       0.0000e+00, 1.9661e-03, 1.0406e-03, 8.6761e-05, 1.9728e-03, 2.2719e-05,\n",
              "                       1.8526e-03, 7.7707e-04, 1.1276e-04, 8.8958e-11, 2.5570e-03, 4.4899e-04,\n",
              "                       1.0979e-06, 6.6204e-06, 1.1269e-02, 1.3149e-04, 1.0905e-06, 4.9520e-03,\n",
              "                       8.2168e-04, 4.3081e-03, 1.7271e-03, 3.6867e-03, 2.9106e-07, 1.9213e-05,\n",
              "                       1.9915e-05, 0.0000e+00, 1.8684e-07, 5.6102e-04, 5.2883e-04, 2.0426e-03,\n",
              "                       7.4678e-05, 3.3882e-03, 1.2420e-03, 3.7073e-05, 2.0809e-04, 0.0000e+00,\n",
              "                       2.4291e-04, 1.1150e-06, 1.1584e-02, 1.2526e-05, 5.2865e-03, 2.3394e-05,\n",
              "                       5.9881e-03, 1.8295e-03, 6.4266e-06, 0.0000e+00, 9.8596e-03, 3.1175e-10,\n",
              "                       7.0831e-08, 2.8464e-03, 2.5626e-05, 2.2826e-03, 6.6064e-04, 8.8277e-03,\n",
              "                       1.0369e-03, 3.5941e-06, 1.0129e-02, 7.5006e-03, 6.7111e-03, 1.2009e-02,\n",
              "                       2.9626e-05, 1.1624e-03, 3.1310e-03, 1.2614e-03, 9.7977e-03, 8.9065e-06,\n",
              "                       1.5387e-05, 6.3964e-04, 2.0857e-06, 5.9831e-03, 5.9975e-03, 0.0000e+00,\n",
              "                       3.0239e-04, 3.3770e-03, 4.7865e-03, 5.8751e-04, 2.8663e-03, 0.0000e+00,\n",
              "                       3.0450e-04, 8.9058e-04, 4.2202e-06, 1.4960e-06, 1.2985e-15, 1.7031e-03,\n",
              "                       1.3776e-02, 5.3255e-09, 1.3562e-02, 3.4356e-05, 3.6055e-04, 6.9092e-04,\n",
              "                       2.1849e-03, 1.9473e-03, 3.1533e-06, 2.7518e-03, 9.0094e-05, 8.2517e-03,\n",
              "                       1.4321e-04, 6.8932e-07, 2.2309e-03, 8.4630e-03, 0.0000e+00, 8.2833e-03,\n",
              "                       1.1658e-02, 5.4814e-04, 1.0512e-05, 1.4061e-02, 4.3468e-04, 3.5435e-03,\n",
              "                       7.6899e-03, 6.9384e-05, 6.3300e-04, 3.2030e-10, 1.4684e-03, 3.4492e-04,\n",
              "                       3.3425e-05, 1.1575e-02, 3.5480e-03, 5.6886e-10, 0.0000e+00, 4.2620e-05,\n",
              "                       3.8312e-04, 3.2608e-04, 7.2101e-03, 3.0681e-04, 1.4645e-03, 1.1111e-04,\n",
              "                       2.1133e-05, 1.6804e-05, 2.2339e-06, 5.2768e-03, 3.0763e-03, 3.2255e-06,\n",
              "                       9.1063e-03, 4.9472e-04, 3.0715e-07, 9.8079e-04, 1.3715e-02, 4.8660e-04,\n",
              "                       1.6067e-04, 2.2113e-14, 9.4436e-04, 2.7751e-04, 2.5839e-04, 2.6490e-03,\n",
              "                       4.2800e-03, 2.5619e-06, 6.0236e-05, 0.0000e+00, 3.7011e-03, 8.2691e-09,\n",
              "                       7.4905e-03, 1.3366e-05, 5.3312e-04, 1.1727e-03, 2.2219e-03, 1.6983e-06,\n",
              "                       2.5199e-03, 0.0000e+00, 9.0042e-06, 6.2167e-03, 5.0076e-04, 6.7656e-04,\n",
              "                       0.0000e+00, 1.7156e-04, 2.1037e-04, 4.9249e-04, 5.1610e-04, 8.0392e-04,\n",
              "                       0.0000e+00, 8.6305e-08, 2.4714e-04, 1.4239e-05, 1.1526e-04, 8.5157e-04,\n",
              "                       0.0000e+00, 2.4966e-04, 6.9229e-03, 1.3164e-07, 1.2003e-02, 0.0000e+00,\n",
              "                       1.6092e-04, 6.1578e-03, 0.0000e+00, 3.3420e-06, 4.4659e-03, 2.4887e-06,\n",
              "                       3.4486e-05, 8.2247e-12, 0.0000e+00, 9.8431e-03, 7.8032e-12, 3.6018e-04,\n",
              "                       4.5556e-03, 5.0030e-03, 2.5611e-08, 0.0000e+00, 3.0569e-03, 1.7730e-03,\n",
              "                       2.6466e-05, 0.0000e+00, 9.3442e-05, 2.1435e-07, 4.6084e-03, 1.2258e-06,\n",
              "                       7.8526e-06, 6.0321e-04, 1.0901e-04, 7.3211e-05, 5.7577e-04, 2.4057e-03,\n",
              "                       6.6295e-04, 5.9162e-03, 1.1495e-03, 1.3945e-02, 2.6699e-12, 3.3684e-03,\n",
              "                       8.4714e-04, 5.3013e-06, 7.3078e-05, 2.2843e-04, 2.0998e-03, 2.6513e-06,\n",
              "                       6.9071e-04, 3.1966e-05, 2.3662e-03, 4.8321e-05, 3.1865e-04, 3.1029e-03,\n",
              "                       1.7280e-03, 0.0000e+00, 4.8617e-03, 0.0000e+00, 9.0562e-03, 5.4239e-04,\n",
              "                       6.8437e-07, 7.7802e-04, 0.0000e+00, 1.1521e-13, 6.8950e-03, 8.3691e-05,\n",
              "                       4.4780e-03, 9.8482e-03, 1.7651e-07, 2.1422e-03, 2.5613e-06, 2.1786e-03,\n",
              "                       6.3222e-05, 4.1526e-03, 1.3703e-07, 7.3479e-07, 5.2720e-04, 5.0253e-09,\n",
              "                       1.5663e-03, 5.4250e-03, 1.2894e-04, 1.0775e-04, 9.5948e-05, 3.7154e-03,\n",
              "                       2.1523e-06, 6.0229e-04, 2.5563e-03, 1.8196e-03, 5.5506e-08, 1.0162e-03,\n",
              "                       7.9525e-05, 6.0988e-03, 1.5853e-03, 5.1124e-10, 1.9568e-07, 4.0440e-04,\n",
              "                       5.4979e-05, 1.6118e-04, 4.4534e-04, 1.8510e-03, 6.4498e-03, 5.6550e-05,\n",
              "                       4.1656e-12, 6.9268e-03, 5.7856e-05, 2.2623e-07, 3.1795e-07, 2.4430e-06,\n",
              "                       4.1999e-04, 6.1744e-03, 1.3869e-06, 2.5948e-03, 6.7083e-04, 2.4029e-03,\n",
              "                       2.3132e-04, 1.5914e-03, 4.0947e-03, 3.9330e-03, 7.5912e-03, 1.8831e-03],\n",
              "                      device='cuda:0')},\n",
              "              Parameter containing:\n",
              "              tensor([[ 1.9164e-02, -2.6892e-02, -2.5727e-02,  2.1801e-02,  1.2254e-02,\n",
              "                       -1.7262e-02, -1.2724e-02,  1.0954e-02, -2.5102e-02, -1.4763e-02,\n",
              "                        1.8938e-02, -1.0669e-02, -2.6465e-02,  3.5014e-03,  1.7949e-02,\n",
              "                        7.8463e-04,  1.0639e-02,  3.1837e-02, -2.9686e-02, -3.3847e-02,\n",
              "                        6.3855e-03, -1.0299e-02, -2.0043e-02, -9.9410e-03, -2.8669e-02,\n",
              "                        1.8348e-02,  3.2789e-02, -1.0123e-02,  2.8397e-02,  1.3679e-02,\n",
              "                       -1.4750e-02,  1.2294e-02,  1.3386e-02, -3.4751e-03, -1.8019e-02,\n",
              "                        2.7829e-02,  3.4167e-02,  3.1186e-02,  3.3943e-02, -1.4837e-02,\n",
              "                        2.8412e-02, -9.5281e-03, -3.3718e-02,  1.0498e-02, -3.6541e-02,\n",
              "                        1.8140e-02, -3.9761e-03,  7.0445e-03,  2.0761e-02,  2.1547e-02,\n",
              "                       -3.0524e-02, -3.1369e-02, -6.6387e-04, -3.5795e-02,  3.2768e-02,\n",
              "                       -7.8146e-03,  2.1132e-02, -2.8955e-02,  1.4915e-02,  1.0655e-02,\n",
              "                       -5.3673e-03, -1.1869e-02, -1.5724e-02, -1.0130e-02,  1.3521e-02,\n",
              "                       -2.4001e-02,  2.9218e-03,  1.0160e-02, -1.2710e-02,  3.6275e-03,\n",
              "                        7.5566e-03, -4.7611e-03, -1.9959e-02, -1.3781e-02,  2.5636e-02,\n",
              "                        1.5119e-03,  3.1328e-02,  7.7680e-03,  3.4220e-02, -1.3156e-02,\n",
              "                       -3.8684e-03,  2.2372e-02,  3.2006e-02,  2.6199e-02, -1.7136e-02,\n",
              "                       -2.6559e-02,  1.9828e-02,  2.8017e-02, -1.4699e-02,  2.9622e-02,\n",
              "                        2.8106e-02,  2.0219e-02,  8.1820e-03, -1.5066e-02, -1.5592e-03,\n",
              "                       -2.4202e-02,  3.3043e-02, -2.7951e-02, -1.7070e-02,  2.7692e-02,\n",
              "                       -1.4364e-02,  3.0371e-02, -1.0146e-02, -1.7735e-02, -3.2489e-02,\n",
              "                       -2.1748e-02, -2.7093e-02, -2.6706e-02, -1.5301e-02,  6.3175e-03,\n",
              "                       -4.2294e-03,  1.8102e-02,  3.1684e-02, -3.3429e-02,  1.1052e-02,\n",
              "                        1.1153e-02, -1.1135e-02,  2.0547e-02,  9.4565e-03,  2.8659e-02,\n",
              "                       -3.2304e-02, -8.5253e-03,  9.8740e-04, -3.2515e-02,  5.7744e-03,\n",
              "                        1.8454e-02,  2.7991e-02,  1.6542e-02, -5.5110e-03,  1.2966e-02,\n",
              "                       -2.4903e-03,  2.0756e-02,  2.6872e-02,  2.0719e-03,  2.6698e-02,\n",
              "                        2.6937e-02, -1.0350e-02, -2.9594e-02,  1.9505e-02, -2.4811e-02,\n",
              "                       -2.1372e-02,  3.3810e-02,  2.1292e-02, -1.8156e-02,  1.9320e-02,\n",
              "                        2.0476e-02,  2.9422e-02, -4.8722e-03, -2.0351e-02,  2.1648e-03,\n",
              "                        2.1023e-02,  2.5992e-02,  2.9900e-02, -3.5773e-02,  1.4984e-02,\n",
              "                        2.7783e-02, -8.6234e-03,  3.8275e-03, -7.8582e-03, -1.3038e-02,\n",
              "                       -6.6945e-03, -1.2494e-02, -2.9399e-02, -4.0356e-04,  2.7304e-02,\n",
              "                       -2.5248e-02,  1.2978e-02, -2.8332e-02,  2.6237e-03,  3.0991e-02,\n",
              "                       -1.5593e-02, -3.0953e-02,  3.7444e-03, -8.8383e-03,  3.6671e-03,\n",
              "                        1.4608e-02,  7.7422e-03, -3.5206e-02,  8.3963e-03, -2.9149e-05,\n",
              "                        5.3355e-04, -1.6557e-02, -2.4932e-04,  3.0136e-02,  2.9218e-02,\n",
              "                       -1.9063e-02, -7.4019e-03,  1.9924e-02,  2.6476e-03,  1.7579e-02,\n",
              "                        3.5641e-02, -2.3627e-03,  1.9354e-02,  7.3279e-03,  6.3396e-03,\n",
              "                        2.2755e-02, -1.9922e-02, -4.0839e-03, -2.8102e-03,  1.8032e-03,\n",
              "                        3.4641e-02, -1.6968e-02, -2.0948e-02, -1.7952e-02, -1.5762e-02,\n",
              "                        2.5184e-02,  2.7009e-02,  4.1368e-03, -2.9937e-03, -3.3989e-02,\n",
              "                       -1.0791e-02,  1.0778e-02,  1.2258e-02,  1.5530e-02,  1.8753e-02,\n",
              "                       -2.5601e-02,  2.0388e-02,  3.0720e-02,  7.7695e-03,  2.4577e-02,\n",
              "                       -1.4049e-02, -5.9786e-03,  3.1868e-02,  2.8414e-02, -1.6118e-02,\n",
              "                        7.2309e-03, -1.3870e-02,  2.6207e-02,  3.2498e-02,  3.8844e-03,\n",
              "                        2.1468e-02,  5.2975e-03,  2.9293e-02,  1.2179e-02, -2.3820e-02,\n",
              "                        2.9919e-02,  2.1188e-02,  3.5891e-03, -3.1657e-02, -3.5290e-03,\n",
              "                        1.5458e-02, -3.3222e-02, -2.0949e-02, -1.6199e-02,  5.6103e-03,\n",
              "                        1.5774e-03,  7.4854e-03, -1.7007e-03,  3.1759e-02,  2.0259e-03,\n",
              "                       -1.7563e-02, -1.0711e-02, -6.3695e-03, -5.3778e-03, -6.7225e-03,\n",
              "                        1.8048e-02,  1.0755e-02,  2.3554e-02, -1.1749e-02, -2.2812e-02,\n",
              "                       -3.3209e-02,  5.0157e-03,  9.0059e-03,  2.0166e-02, -2.9905e-02,\n",
              "                       -1.0361e-02, -1.6952e-02,  1.5587e-02,  3.1821e-02, -1.3162e-02,\n",
              "                       -2.5820e-02,  1.5213e-02,  3.3480e-02, -2.9889e-02,  3.4109e-02,\n",
              "                        2.5910e-02,  4.7473e-04, -4.1064e-03, -2.2389e-02, -1.4442e-02,\n",
              "                       -1.0090e-02,  3.1058e-02,  1.9477e-02, -1.4116e-02, -9.1546e-04,\n",
              "                       -2.3294e-02, -3.1001e-02, -3.1080e-03, -2.5900e-02, -2.5833e-03,\n",
              "                        2.2948e-02, -2.7404e-02,  2.0938e-03, -1.6314e-02,  7.2268e-03,\n",
              "                        2.6303e-02,  5.0551e-03,  4.2268e-02,  9.5783e-03, -2.8062e-02,\n",
              "                       -2.3176e-02,  2.9036e-02,  1.8238e-02, -1.1576e-02,  1.0560e-02,\n",
              "                       -4.7455e-03,  1.5481e-02, -9.6963e-03,  1.3572e-02, -2.4814e-03,\n",
              "                       -9.0755e-03, -1.3248e-02, -1.5156e-02,  2.1175e-02, -4.8873e-03,\n",
              "                       -5.7145e-03, -1.7799e-03, -1.4527e-02,  1.5180e-02, -1.5102e-02,\n",
              "                       -1.3385e-03,  2.6260e-02, -2.0462e-02,  7.6336e-03, -7.5990e-03,\n",
              "                       -1.9020e-02,  1.6421e-02,  1.8357e-02,  2.0197e-02,  2.3549e-02,\n",
              "                        2.5910e-02,  2.0020e-02, -9.6136e-03, -3.1384e-02, -1.5160e-02,\n",
              "                       -1.5759e-02, -3.2620e-03,  6.7547e-03,  2.7307e-02,  2.8187e-03,\n",
              "                        2.3175e-02,  1.0533e-03,  2.4636e-02,  2.4147e-02,  3.2294e-02,\n",
              "                       -7.9744e-03,  2.9523e-04, -1.1069e-02, -1.1315e-02,  2.4483e-02,\n",
              "                       -1.2246e-03,  2.6948e-02, -3.0309e-02,  2.8522e-02,  2.3758e-02,\n",
              "                       -1.0362e-02, -2.2526e-02,  1.2462e-02, -2.3512e-02,  1.0746e-02,\n",
              "                        1.2388e-02,  1.1245e-02,  3.4421e-02, -2.9722e-02,  5.8853e-03,\n",
              "                        2.0164e-02, -1.1090e-02,  2.0598e-02,  2.9892e-02,  2.7861e-02,\n",
              "                        1.8214e-02, -3.3168e-02,  2.1349e-02,  2.5317e-02,  7.7080e-03,\n",
              "                       -1.3940e-02,  1.0932e-02,  3.2169e-02,  1.4731e-02,  1.3920e-02,\n",
              "                        1.8216e-02, -2.9760e-02, -6.5155e-03, -2.3267e-02, -2.1574e-02,\n",
              "                       -2.9390e-02, -1.3546e-02, -1.8404e-02, -6.7295e-03, -3.0841e-03,\n",
              "                       -1.2020e-02,  1.5565e-02,  2.4015e-02, -1.5731e-03, -1.4448e-02,\n",
              "                        2.4683e-03,  1.2724e-02,  1.4196e-02, -2.6454e-02,  4.2488e-03,\n",
              "                        1.9821e-02,  3.3461e-02, -3.3788e-02,  1.6643e-02, -1.8105e-02,\n",
              "                       -2.5016e-02, -2.6505e-02,  2.9217e-02, -2.5498e-02, -1.2316e-03,\n",
              "                       -2.8791e-02, -5.7491e-03,  5.3344e-03, -2.7318e-02,  2.0590e-02,\n",
              "                       -1.7374e-02, -1.4138e-02, -2.3924e-02, -1.1758e-02, -2.9732e-02,\n",
              "                        2.7415e-02,  1.3914e-02, -1.1100e-02, -3.3782e-02,  3.4014e-03,\n",
              "                        6.6157e-03, -2.6489e-02,  7.3456e-03, -2.3915e-02,  3.3789e-02,\n",
              "                        1.3524e-02, -3.9328e-03, -1.1748e-02,  3.4188e-02,  1.0439e-02,\n",
              "                        2.5560e-02, -1.7626e-02,  9.7497e-03, -2.9297e-02, -2.2205e-02,\n",
              "                       -3.1745e-02,  1.0881e-02,  1.3792e-03,  1.0024e-02, -5.2818e-03,\n",
              "                       -2.9460e-02,  1.3753e-02, -2.3731e-02,  3.4108e-02,  9.6065e-03,\n",
              "                       -3.6565e-03, -1.3102e-02,  3.2685e-02, -5.8352e-03,  3.0224e-02,\n",
              "                       -1.8754e-02, -1.3580e-02,  3.3541e-02, -1.8403e-02,  4.5067e-03,\n",
              "                        1.4659e-02,  2.7977e-02,  3.2902e-02, -3.1192e-02, -2.2964e-02,\n",
              "                       -2.7153e-02, -1.3610e-02,  2.7034e-02,  2.1715e-02, -3.2977e-02,\n",
              "                       -2.3016e-02,  2.9279e-02,  3.2642e-02, -2.6722e-02,  3.3798e-02,\n",
              "                       -8.5045e-03,  2.4184e-02,  1.2652e-02,  2.6147e-02, -3.2880e-02,\n",
              "                       -1.7590e-02,  8.5500e-03, -3.5172e-02, -2.5039e-02,  2.2207e-02,\n",
              "                        2.4922e-02,  1.0636e-02,  2.0075e-02, -3.4757e-03,  8.7404e-03,\n",
              "                        7.3545e-03, -2.8163e-02, -1.5295e-03,  6.3591e-03,  6.7782e-03,\n",
              "                        2.8061e-02, -2.2219e-02,  3.4350e-02, -3.2700e-02, -1.8234e-02,\n",
              "                       -7.4928e-03,  2.1571e-02,  1.2925e-02,  8.5642e-03, -2.5933e-02,\n",
              "                       -3.3531e-02,  1.4061e-03, -4.7934e-03, -2.5764e-02,  7.0540e-03,\n",
              "                       -2.8791e-02, -1.2600e-02,  1.1983e-02, -2.0331e-02,  3.4518e-02,\n",
              "                        1.0508e-02, -2.7421e-02, -3.0243e-02,  2.5270e-02, -1.4965e-02,\n",
              "                       -9.4040e-03,  6.1235e-03, -2.2583e-02,  1.1981e-02,  2.5460e-02,\n",
              "                       -2.0848e-02, -2.2311e-02,  2.9260e-02, -5.6622e-03,  1.4002e-02,\n",
              "                        2.0423e-02,  1.2861e-02,  2.8849e-02, -2.2931e-02,  1.4480e-02,\n",
              "                        1.3572e-02, -1.8032e-03, -2.6999e-02, -2.8585e-02,  7.0263e-03,\n",
              "                       -2.3643e-02, -5.3279e-03,  1.0095e-03, -1.2842e-02,  3.3901e-02,\n",
              "                        3.2881e-02, -1.7047e-02, -2.2871e-02,  9.3174e-03,  3.7522e-03,\n",
              "                        1.2902e-02, -1.0092e-02,  1.4570e-02, -2.3742e-02, -1.4597e-02,\n",
              "                       -1.8822e-02,  1.4621e-02,  3.2151e-02, -1.3802e-03, -4.8693e-03,\n",
              "                        3.0635e-02,  3.0403e-02,  1.9586e-04,  2.0930e-02, -1.6199e-02,\n",
              "                        1.8892e-02,  1.2989e-02,  1.7495e-02,  4.1322e-04,  2.0814e-02,\n",
              "                        2.1609e-02, -2.1072e-02,  1.9286e-03, -3.3472e-02, -1.5531e-02,\n",
              "                       -1.9214e-02, -5.0458e-03,  1.8462e-02, -2.9279e-02, -1.4867e-02,\n",
              "                       -9.2819e-03,  2.4885e-02, -1.1206e-02, -1.5413e-02,  3.2363e-02,\n",
              "                       -2.4961e-03, -2.0746e-02, -5.5059e-03,  2.2634e-02, -1.6131e-02,\n",
              "                        1.1825e-02,  2.9179e-02,  2.8729e-02,  4.8585e-03,  3.2816e-03,\n",
              "                        1.5459e-02, -6.1939e-03,  1.3664e-02, -2.5449e-02,  2.6952e-02,\n",
              "                        9.3390e-03, -2.0754e-02,  2.9078e-02,  2.4905e-02,  2.3769e-02,\n",
              "                        3.1750e-02,  1.5873e-02, -9.7720e-03,  1.8655e-02, -1.0264e-02,\n",
              "                        3.0003e-02, -2.5272e-03, -9.0198e-04, -1.9007e-02, -2.9515e-02,\n",
              "                       -3.5120e-02,  2.3259e-02,  2.6186e-02, -1.0633e-02, -2.5941e-02,\n",
              "                        2.1150e-02, -6.9411e-03,  1.6475e-02,  3.4133e-02, -1.0504e-02,\n",
              "                        8.5321e-03, -2.0560e-02,  6.9351e-04, -1.2917e-02, -3.2478e-02,\n",
              "                        3.3995e-02, -1.7761e-02,  3.3528e-02, -2.6437e-03,  5.5147e-03,\n",
              "                       -8.4137e-03,  1.5894e-02,  1.2610e-02,  3.6185e-02,  1.5313e-02,\n",
              "                       -7.2398e-03,  2.6704e-02, -1.9375e-02, -1.4233e-02, -2.3934e-02,\n",
              "                        2.6576e-02,  9.2874e-03,  2.6184e-02,  3.1857e-02,  8.4899e-03,\n",
              "                       -1.6207e-02,  3.4101e-02, -1.3658e-02,  1.8561e-02,  2.5305e-02,\n",
              "                       -2.9620e-02, -3.3935e-02,  1.3013e-02, -2.6737e-02, -3.0735e-02,\n",
              "                        8.2631e-03,  3.3142e-02, -2.9789e-02,  2.0920e-02,  2.3857e-02,\n",
              "                        1.8649e-03, -1.6386e-02, -1.8337e-02,  3.2168e-02,  6.4672e-03,\n",
              "                       -3.3945e-02,  3.2537e-02,  7.6973e-03,  3.2516e-02, -3.6020e-02,\n",
              "                        2.0930e-02,  1.5923e-02,  2.8751e-03,  2.7549e-02, -2.7557e-02,\n",
              "                       -8.8161e-03,  9.0645e-03, -3.3748e-02,  6.4138e-03, -1.5500e-02,\n",
              "                        1.9655e-02,  1.0007e-02,  1.1725e-02,  2.7740e-02,  1.7093e-02,\n",
              "                        1.9172e-02,  3.4225e-04,  1.0893e-02, -2.9095e-02,  1.7732e-02,\n",
              "                        1.1599e-03,  3.0690e-02, -4.3111e-03, -6.6411e-03, -1.3046e-02,\n",
              "                       -2.1193e-02, -3.0463e-02, -3.0362e-02, -2.7848e-02, -1.5176e-02,\n",
              "                        2.2938e-02, -2.9290e-02, -1.8476e-02, -3.1795e-02, -2.6471e-02,\n",
              "                        2.4947e-02, -9.2098e-03, -2.6129e-02,  8.1122e-03, -2.4157e-02,\n",
              "                        5.2439e-03, -1.3634e-02,  3.2073e-02, -2.6019e-02, -2.1678e-02,\n",
              "                        1.4371e-02,  1.5681e-02,  2.8017e-02,  2.2392e-02,  3.3008e-02,\n",
              "                       -1.5937e-02,  3.5453e-03,  2.2567e-02, -3.4662e-03, -3.0684e-02,\n",
              "                        2.3462e-02, -1.2869e-02, -2.3260e-03,  1.5414e-02, -2.1458e-02,\n",
              "                        2.8654e-02,  1.1282e-02, -1.9175e-02,  2.3434e-02,  2.0393e-02,\n",
              "                        4.2986e-03,  1.5589e-02, -2.7554e-02,  1.3399e-02,  1.3465e-03,\n",
              "                       -3.1509e-02, -2.0694e-02,  2.8056e-02,  1.9604e-02, -3.3393e-02,\n",
              "                       -7.4919e-03,  7.8771e-03, -9.1133e-03,  2.9048e-02, -8.3608e-03,\n",
              "                       -2.2866e-02,  1.1144e-02,  2.2105e-02, -1.7883e-02,  3.4183e-02,\n",
              "                        6.4353e-03,  1.7170e-02, -2.4722e-02,  1.0961e-02, -6.4935e-03,\n",
              "                       -3.5629e-02,  1.5998e-02,  2.7223e-02,  8.0873e-03, -3.0720e-03,\n",
              "                       -2.3196e-02, -2.9841e-02,  5.0699e-03,  1.6152e-02, -2.5272e-02,\n",
              "                       -9.9407e-03,  2.0193e-02,  2.7020e-02,  3.1129e-02, -1.1053e-02,\n",
              "                        3.3421e-02,  8.0295e-03,  9.1394e-03, -3.1960e-02,  2.3972e-02,\n",
              "                       -2.4523e-02, -2.8134e-02,  2.8634e-02,  8.3792e-05,  1.4576e-02,\n",
              "                       -4.0129e-03, -1.9215e-02,  5.9058e-03, -3.3595e-02,  1.2683e-02,\n",
              "                        1.9479e-03, -1.2704e-02, -9.5612e-04, -2.0384e-02,  2.1143e-02,\n",
              "                        3.2302e-03,  2.8852e-03,  2.0751e-02,  1.8086e-02, -3.2168e-02,\n",
              "                       -3.2538e-02,  1.4629e-02,  1.2262e-02,  1.8872e-02, -1.2123e-02,\n",
              "                       -1.1140e-02, -2.6065e-02,  1.3678e-02, -3.1001e-02,  3.3887e-02,\n",
              "                       -6.7562e-03,  1.9986e-02, -1.9770e-02, -1.3885e-02,  2.6495e-02,\n",
              "                        2.2948e-02, -2.2302e-03,  1.8649e-02,  2.4000e-02,  2.9374e-03,\n",
              "                       -2.9864e-02, -3.2267e-03,  1.5808e-03, -2.8460e-02,  2.2464e-02,\n",
              "                       -2.3714e-02,  1.4644e-02, -1.5501e-02, -3.2308e-02, -1.5641e-02,\n",
              "                       -1.9569e-02,  1.8521e-02,  1.7954e-02, -2.5147e-02,  1.2594e-02]],\n",
              "                     device='cuda:0', requires_grad=True): {'step': tensor(162992.),\n",
              "               'sum': tensor([[3.5601e+00, 2.4655e+01, 7.2159e+00, 2.5430e+01, 7.2202e-01, 7.4560e-01,\n",
              "                        8.0639e-01, 2.0296e-08, 1.1074e-01, 1.0931e-01, 4.5626e-02, 2.2895e-01,\n",
              "                        1.3315e-03, 0.0000e+00, 4.2717e+00, 0.0000e+00, 2.2078e-04, 2.3080e-01,\n",
              "                        0.0000e+00, 1.9488e-03, 0.0000e+00, 2.2381e-01, 0.0000e+00, 1.3340e-01,\n",
              "                        1.8412e+00, 7.9423e+00, 8.0682e+00, 1.8412e-02, 6.2428e-10, 1.5704e+01,\n",
              "                        2.4669e-02, 1.6755e+02, 3.8738e+00, 1.1650e+00, 1.4160e+01, 7.9035e-02,\n",
              "                        2.1683e+01, 1.9037e+01, 1.8532e+00, 1.3030e+01, 9.8268e+00, 2.8385e-01,\n",
              "                        1.0460e-01, 1.6540e-02, 1.8198e-06, 8.8645e+00, 2.9541e-02, 6.6963e-03,\n",
              "                        2.4637e+00, 2.3201e+00, 6.7415e-02, 0.0000e+00, 1.0822e+00, 6.6328e-07,\n",
              "                        1.2932e+01, 2.2343e+00, 9.0547e-11, 5.1866e+00, 4.1888e-06, 2.5118e+00,\n",
              "                        5.5147e+00, 9.2978e+00, 1.9595e-01, 2.5552e-01, 7.1915e-06, 6.3151e-02,\n",
              "                        6.1677e+00, 6.4617e+00, 9.0787e+00, 1.0363e-12, 7.7384e+00, 1.1210e-02,\n",
              "                        4.1058e-01, 3.7916e-01, 4.0140e+00, 3.6320e+00, 2.3399e-02, 0.0000e+00,\n",
              "                        3.2343e+01, 1.5414e-01, 2.0001e+00, 3.3633e-03, 6.8045e-03, 6.1503e-10,\n",
              "                        1.4677e-01, 2.0502e-07, 3.7148e+00, 4.5209e+01, 1.6401e+01, 9.2872e-01,\n",
              "                        3.2664e-03, 1.1065e-09, 2.4629e+01, 2.5595e+00, 9.1271e-04, 2.9309e-02,\n",
              "                        2.4117e+01, 5.4505e-01, 1.9262e-01, 2.5103e+00, 1.3434e+01, 2.2021e+01,\n",
              "                        0.0000e+00, 5.0159e-03, 2.0717e-02, 3.4132e-06, 9.6276e+00, 0.0000e+00,\n",
              "                        2.1830e-02, 1.9401e+00, 8.4688e-06, 1.7795e+01, 4.3542e+01, 1.5875e+01,\n",
              "                        2.1680e+00, 6.9053e-07, 5.8582e-11, 1.4590e+01, 1.8672e-03, 6.4125e+01,\n",
              "                        4.4214e+00, 2.6746e-01, 1.3725e+01, 2.3180e-01, 1.2066e-01, 2.3947e+01,\n",
              "                        1.3904e-02, 1.1104e-02, 8.6136e-04, 2.0282e+01, 8.2694e+00, 3.1898e+01,\n",
              "                        8.4385e-11, 7.7724e-01, 2.3476e+00, 1.0517e+01, 7.7896e+00, 6.4027e-03,\n",
              "                        4.2377e+01, 8.0047e-01, 4.5682e-02, 3.7932e+01, 1.0198e+01, 9.1747e-14,\n",
              "                        3.7501e+00, 2.5144e-02, 1.1339e+01, 4.7529e-02, 7.9075e-11, 3.5770e-01,\n",
              "                        2.1286e+01, 4.6659e-03, 6.3804e+00, 1.6564e-01, 1.4321e-03, 2.4251e+01,\n",
              "                        5.3728e-03, 6.8409e+00, 2.6421e-01, 8.4461e-03, 8.6446e-01, 1.6946e+00,\n",
              "                        9.1441e-02, 1.7380e+00, 3.1067e-01, 6.2750e-02, 2.5749e+00, 1.8325e-02,\n",
              "                        4.9815e+00, 4.7941e-13, 3.3559e-02, 1.1392e+00, 1.7380e-02, 9.2577e-03,\n",
              "                        1.0364e+01, 2.3668e+00, 8.3892e+00, 2.0588e-06, 1.4920e+01, 4.7040e-06,\n",
              "                        7.2125e-13, 1.6124e-03, 1.0829e-01, 7.4565e+01, 4.8252e+00, 3.7888e-02,\n",
              "                        9.8580e-02, 1.7573e+00, 1.7971e+00, 7.0839e-02, 2.5376e-05, 1.6436e-01,\n",
              "                        1.9359e+01, 0.0000e+00, 5.2381e-07, 1.8702e+01, 6.4751e+00, 9.3633e-02,\n",
              "                        2.4949e-03, 1.6835e+00, 6.1827e+01, 5.3098e-04, 9.2575e-02, 1.5885e+01,\n",
              "                        8.3483e-02, 4.0361e+01, 4.3490e+01, 5.9279e+00, 3.5053e+00, 2.8162e+00,\n",
              "                        8.2448e-04, 7.9722e-03, 0.0000e+00, 4.5565e+01, 2.2269e-03, 2.4036e-11,\n",
              "                        8.6150e+00, 1.0701e+01, 3.7333e-03, 6.4359e+01, 2.8653e-04, 2.6672e-01,\n",
              "                        9.2193e-03, 4.6454e-12, 1.0025e+00, 1.5515e+01, 9.5486e-02, 1.0140e+01,\n",
              "                        9.9059e+00, 9.1357e+00, 2.1202e+01, 1.6748e-09, 1.6548e+00, 1.2408e+01,\n",
              "                        0.0000e+00, 1.5238e-06, 1.5282e-01, 7.1667e+00, 2.5514e-01, 2.3448e-02,\n",
              "                        0.0000e+00, 2.7043e-01, 0.0000e+00, 1.7380e-02, 6.7695e-07, 6.0664e+00,\n",
              "                        4.5231e-01, 1.2649e-02, 7.5746e+00, 1.0612e-06, 4.3481e+00, 6.7580e+00,\n",
              "                        2.1952e-02, 3.7286e+01, 2.1309e-02, 3.4110e+00, 1.7876e+00, 2.8941e+00,\n",
              "                        2.4936e-03, 3.2805e+00, 1.0864e-01, 2.4863e+01, 2.3869e+00, 3.2669e+01,\n",
              "                        1.8091e-02, 3.1278e-01, 9.9543e-01, 2.7456e+01, 3.7849e+01, 6.9637e-03,\n",
              "                        4.5676e-02, 2.1215e+01, 1.7542e+01, 1.1471e-02, 3.3419e+01, 3.3526e-03,\n",
              "                        0.0000e+00, 1.2932e-01, 2.9202e-01, 1.8184e-01, 1.0310e-06, 1.3350e+01,\n",
              "                        2.9835e-01, 2.4009e-01, 3.0804e-03, 1.4872e-01, 7.1616e-01, 4.8222e-02,\n",
              "                        3.2620e-01, 0.0000e+00, 4.8671e-02, 8.5396e-02, 2.7003e+01, 3.3501e-03,\n",
              "                        0.0000e+00, 2.7519e-01, 5.3829e-08, 5.4579e-05, 3.8800e-12, 3.1603e-03,\n",
              "                        1.5842e+00, 2.6212e+00, 8.1064e-01, 6.2756e-05, 4.9950e-01, 1.4387e+01,\n",
              "                        2.6725e+01, 4.0419e-01, 6.4608e+00, 6.0214e+00, 5.1094e-05, 2.6051e-02,\n",
              "                        4.4333e-06, 5.3368e+00, 1.6999e+00, 0.0000e+00, 1.7554e+00, 1.4417e+00,\n",
              "                        2.9420e-01, 1.2534e-01, 1.3540e-09, 5.9714e+01, 7.8963e-02, 1.6203e+01,\n",
              "                        2.4174e-02, 8.3424e-03, 0.0000e+00, 1.0033e+01, 0.0000e+00, 1.1799e-01,\n",
              "                        2.6249e-01, 2.9539e+00, 1.3116e-01, 7.5129e-02, 1.7885e-07, 0.0000e+00,\n",
              "                        2.8967e-03, 5.6903e+00, 1.8953e+01, 2.4187e+00, 0.0000e+00, 1.3591e+01,\n",
              "                        4.0586e+00, 1.2229e-01, 7.8388e+00, 1.5706e-02, 3.8097e+01, 2.3035e-02,\n",
              "                        3.4524e-02, 1.9644e+00, 1.1900e-06, 1.2384e-02, 1.2671e-07, 5.3267e+00,\n",
              "                        8.1611e+00, 5.1476e-01, 1.4721e+01, 3.4026e+00, 1.3357e-02, 2.2414e+01,\n",
              "                        2.1224e+01, 5.0553e+00, 0.0000e+00, 5.0169e+00, 8.9027e-01, 0.0000e+00,\n",
              "                        2.6405e-01, 2.5554e-02, 4.5413e+00, 3.2070e-01, 2.1944e+00, 4.2843e+00,\n",
              "                        1.9579e+00, 2.3589e+00, 1.3908e-01, 3.8932e+00, 1.4044e-10, 9.1844e+00,\n",
              "                        0.0000e+00, 9.0087e+00, 6.4931e-02, 7.7100e-02, 7.7737e-01, 9.9881e-02,\n",
              "                        2.8164e-03, 2.3017e-01, 7.4038e-03, 5.1445e-01, 1.5151e+00, 9.7666e-01,\n",
              "                        6.5653e-07, 6.0643e+01, 9.7839e+00, 6.2127e-04, 9.6525e-02, 6.1760e-04,\n",
              "                        4.2627e+01, 3.2308e+01, 2.0549e-02, 4.9462e+01, 8.1180e-11, 1.7083e+00,\n",
              "                        3.6223e-01, 3.9737e+01, 9.2642e-03, 7.0358e-01, 3.3013e-01, 3.9325e+01,\n",
              "                        1.7366e-02, 2.3307e+00, 5.0258e-02, 4.1794e+00, 4.7563e+01, 1.5824e-01,\n",
              "                        5.4641e+00, 5.8434e-07, 2.3211e+00, 1.8362e-07, 5.5736e-01, 2.5748e-01,\n",
              "                        1.2727e+01, 1.0033e-03, 0.0000e+00, 1.7436e-11, 9.8306e+00, 2.2181e-05,\n",
              "                        5.1631e-01, 2.0376e+00, 6.5818e-03, 1.0794e+02, 0.0000e+00, 1.1924e-03,\n",
              "                        1.5781e+00, 2.2833e+01, 2.8493e+00, 1.3213e+00, 2.1073e-08, 4.9076e+00,\n",
              "                        1.4331e-04, 8.4512e+00, 1.3201e+00, 4.3115e+00, 9.4972e+00, 1.1839e+01,\n",
              "                        1.4703e-03, 2.1962e-02, 4.5852e+01, 5.6379e-20, 2.6496e+01, 6.3433e-08,\n",
              "                        4.5740e+00, 6.5162e-04, 1.3837e-01, 0.0000e+00, 4.3692e+01, 6.2750e-02,\n",
              "                        2.2352e-01, 3.2774e+00, 4.9361e-02, 3.9177e+00, 1.3653e-09, 3.4834e-02,\n",
              "                        2.7318e+01, 9.7855e-05, 5.8246e+00, 2.4480e-01, 2.8631e+00, 5.6836e-01,\n",
              "                        2.9544e-03, 4.4394e-02, 1.2052e-05, 5.0957e-05, 2.3712e+00, 3.5975e+00,\n",
              "                        1.8765e-09, 0.0000e+00, 1.1674e+01, 7.1599e-04, 0.0000e+00, 9.4915e-02,\n",
              "                        0.0000e+00, 2.6346e+01, 1.4139e-01, 3.6556e-02, 1.7709e-01, 2.2524e+01,\n",
              "                        5.8351e+00, 2.6591e-06, 7.7939e-01, 5.1525e+01, 6.0767e+00, 1.7494e+00,\n",
              "                        1.7423e-02, 3.9073e-04, 1.6082e+00, 5.9783e+00, 1.7044e-01, 2.8228e+01,\n",
              "                        6.2086e-01, 7.9185e-13, 2.1206e-03, 0.0000e+00, 9.4773e+00, 4.2028e+00,\n",
              "                        2.1016e-10, 3.6217e-01, 3.5537e+00, 5.5097e+00, 2.4554e+00, 2.5523e+01,\n",
              "                        1.0978e-01, 5.2910e-02, 1.0898e+01, 4.5792e-02, 6.8919e+00, 2.7029e+01,\n",
              "                        6.0814e-02, 3.2737e-01, 2.5927e+01, 5.6245e-05, 7.4810e-01, 1.2111e+01,\n",
              "                        4.3525e-01, 0.0000e+00, 3.1427e+00, 7.5217e-01, 4.3470e-01, 6.4804e+00,\n",
              "                        2.3033e+00, 2.7698e+01, 3.4344e+00, 3.0810e+01, 1.1223e+01, 7.5611e-01,\n",
              "                        2.6917e+01, 3.6052e-01, 5.1547e+00, 1.9621e-01, 1.3632e-01, 2.9191e-01,\n",
              "                        1.3762e-01, 0.0000e+00, 2.4710e-03, 0.0000e+00, 3.9822e+01, 5.0608e+01,\n",
              "                        0.0000e+00, 2.9000e+00, 1.1587e+01, 4.3563e-01, 1.3250e+01, 1.3844e-02,\n",
              "                        2.1031e+00, 1.6615e-01, 3.1511e-02, 2.6209e-11, 8.2915e+00, 5.0758e-02,\n",
              "                        1.7399e-01, 6.8144e-03, 5.8755e+01, 3.3527e-03, 1.1801e+00, 6.7247e+00,\n",
              "                        2.8488e-01, 4.0160e+01, 3.3097e+00, 7.4207e+00, 1.4404e-01, 5.1080e-03,\n",
              "                        1.0350e-03, 0.0000e+00, 1.3272e-03, 1.7409e-01, 3.4322e-01, 1.3233e+00,\n",
              "                        1.9094e+00, 1.7200e+00, 4.9411e-02, 3.0252e-03, 3.3037e-01, 0.0000e+00,\n",
              "                        2.5775e-01, 1.5997e-06, 4.1926e+01, 1.3414e-01, 9.3137e+00, 4.6899e-02,\n",
              "                        2.4941e+01, 1.3203e+00, 1.5373e-04, 0.0000e+00, 2.3126e+01, 2.0935e-09,\n",
              "                        1.1073e-05, 4.4647e+01, 1.2770e-01, 1.2640e+01, 2.2109e-02, 2.0285e+01,\n",
              "                        1.3474e+01, 4.6568e-05, 6.1039e+00, 2.2205e+01, 1.7769e+01, 5.2525e+01,\n",
              "                        2.6880e-03, 9.4407e+00, 7.3577e-01, 2.9796e+01, 1.1674e+01, 1.0579e-01,\n",
              "                        4.8539e+00, 1.1243e-01, 5.9385e-06, 3.3341e+00, 2.0028e+00, 0.0000e+00,\n",
              "                        4.2901e-01, 3.2372e+00, 4.0183e+00, 1.7556e+01, 9.9278e+00, 0.0000e+00,\n",
              "                        1.5441e-01, 4.0789e+01, 8.1221e-05, 5.0609e-02, 2.0959e-19, 3.5148e-01,\n",
              "                        1.2675e+01, 1.8839e-10, 8.0674e+01, 1.0138e+00, 3.4638e+01, 5.8933e+00,\n",
              "                        3.9883e+00, 1.4535e+01, 3.8860e-06, 1.7977e+01, 1.2582e-01, 2.6838e+01,\n",
              "                        3.3598e-03, 1.1920e-06, 5.6671e-01, 4.6324e+01, 0.0000e+00, 3.3881e+01,\n",
              "                        1.2659e+01, 5.6540e+00, 3.6977e-04, 7.5340e+01, 1.0232e-01, 5.1975e+00,\n",
              "                        5.8650e+01, 2.0431e-03, 1.4171e-02, 1.9411e-12, 2.1820e-01, 7.3743e-03,\n",
              "                        2.4604e-02, 6.6963e+00, 1.8576e+00, 3.1781e-11, 0.0000e+00, 3.5761e+00,\n",
              "                        8.7603e-02, 3.1373e-02, 1.1208e+00, 2.1757e+00, 1.4275e-01, 1.3667e-03,\n",
              "                        8.5971e-03, 5.1207e-05, 2.2729e-06, 7.8301e+01, 4.6937e+01, 8.8644e-03,\n",
              "                        1.1118e+02, 8.1953e-02, 3.1487e-04, 1.8015e+01, 1.3762e+01, 1.4788e+01,\n",
              "                        2.0615e-02, 2.7880e-22, 5.7718e+00, 2.1629e-01, 2.0803e-01, 1.9427e+00,\n",
              "                        1.2031e+01, 1.1385e+01, 1.4642e-02, 0.0000e+00, 7.2413e+00, 1.1734e-05,\n",
              "                        2.2588e+00, 1.1211e-02, 6.4728e+00, 5.1735e+00, 3.6077e+00, 6.1640e-06,\n",
              "                        1.5879e+00, 0.0000e+00, 3.8270e-05, 1.3387e+01, 2.8169e-01, 2.1839e-01,\n",
              "                        0.0000e+00, 3.0219e-02, 3.8111e-03, 2.6906e+00, 4.8899e-01, 4.5746e+01,\n",
              "                        0.0000e+00, 2.8348e-06, 3.0714e-02, 2.9703e-05, 2.1301e-02, 1.1568e-01,\n",
              "                        0.0000e+00, 3.2036e-01, 1.8032e+00, 3.7294e-08, 1.0900e+01, 0.0000e+00,\n",
              "                        2.4968e+01, 1.5168e+01, 0.0000e+00, 4.6021e-05, 3.1287e+00, 2.5866e-05,\n",
              "                        3.5393e+00, 2.1497e-12, 0.0000e+00, 3.5690e+01, 1.0059e-11, 8.6943e-02,\n",
              "                        5.8748e+00, 4.3374e+01, 5.5955e-06, 0.0000e+00, 1.8261e+00, 6.5841e+00,\n",
              "                        2.7256e+00, 0.0000e+00, 9.0257e-04, 2.6742e-06, 2.5154e+01, 5.1463e-05,\n",
              "                        9.1632e-04, 9.0226e+00, 1.6671e-01, 5.3909e-03, 3.6899e+00, 1.5393e+00,\n",
              "                        1.8964e+00, 5.2974e+01, 6.3737e-01, 1.9321e+01, 7.7718e-13, 1.4711e+01,\n",
              "                        6.9467e-02, 2.0306e-03, 1.1442e-01, 6.4749e-03, 8.6942e+00, 2.0918e-05,\n",
              "                        6.9399e+00, 8.5283e-01, 2.2265e+00, 2.9152e-03, 1.8741e+01, 1.7107e+01,\n",
              "                        5.1898e-01, 0.0000e+00, 2.4161e+01, 0.0000e+00, 6.1521e+00, 2.4566e-01,\n",
              "                        7.7324e-07, 9.9272e+00, 0.0000e+00, 1.0856e-17, 1.6146e+01, 2.4479e-03,\n",
              "                        1.0998e+00, 1.6324e+01, 1.0356e-03, 1.2497e+01, 2.1900e-03, 2.4399e+00,\n",
              "                        8.4194e-02, 1.9264e+00, 7.3051e-07, 2.3581e-02, 1.5736e+00, 6.9309e-06,\n",
              "                        1.0826e+00, 3.8522e+01, 7.8612e+01, 9.5468e+00, 5.1526e-03, 3.0051e+01,\n",
              "                        5.4577e-06, 1.5262e-01, 2.2606e+01, 1.0396e+01, 7.1397e-07, 6.8526e-01,\n",
              "                        1.2287e-01, 7.5065e+00, 1.7759e+00, 7.6465e-11, 1.0479e-07, 1.9849e+00,\n",
              "                        2.8398e-03, 1.1804e-02, 3.4127e-01, 2.9948e-01, 1.7248e+01, 1.1404e+01,\n",
              "                        2.6231e-15, 4.6009e+01, 1.4970e+00, 9.7813e-08, 2.5191e-03, 4.5596e-02,\n",
              "                        5.1069e-02, 2.0750e+00, 1.2679e-06, 1.4035e+01, 8.5439e-02, 2.2572e-01,\n",
              "                        1.7432e-02, 6.9426e-01, 3.2987e+01, 1.5554e+01, 9.9721e+00, 1.6871e+01]],\n",
              "                      device='cuda:0')},\n",
              "              Parameter containing:\n",
              "              tensor([-0.0002], device='cuda:0', requires_grad=True): {'step': tensor(162992.),\n",
              "               'sum': tensor([12.0176], device='cuda:0')}}),\n",
              " 'param_groups': [{'lr': 0.0004243110622021168,\n",
              "   'lr_decay': 0,\n",
              "   'eps': 1e-10,\n",
              "   'weight_decay': 0,\n",
              "   'initial_accumulator_value': 0,\n",
              "   'foreach': None,\n",
              "   'maximize': False,\n",
              "   'differentiable': False,\n",
              "   'initial_lr': 0.0004971113311461239,\n",
              "   'params': [Parameter containing:\n",
              "    tensor([[ 0.2491, -0.0154, -0.5158],\n",
              "            [-0.2211, -0.5437,  0.2490],\n",
              "            [ 0.3927,  0.5422, -0.3364],\n",
              "            ...,\n",
              "            [-0.5098,  0.0773, -0.4235],\n",
              "            [-0.1741,  0.1158, -0.2311],\n",
              "            [-0.4212,  0.0641,  0.4889]], device='cuda:0', requires_grad=True),\n",
              "    Parameter containing:\n",
              "    tensor([-0.5349,  0.2851,  0.0322,  ...,  0.1765,  0.5665,  0.2583],\n",
              "           device='cuda:0', requires_grad=True),\n",
              "    Parameter containing:\n",
              "    tensor([[ 0.0084, -0.0205,  0.0311,  ...,  0.0193,  0.0006,  0.0167],\n",
              "            [ 0.0142, -0.0110,  0.0175,  ..., -0.0236, -0.0192, -0.0082],\n",
              "            [ 0.0194, -0.0241,  0.0152,  ...,  0.0226, -0.0299, -0.0003],\n",
              "            ...,\n",
              "            [ 0.0283,  0.0114, -0.0209,  ..., -0.0059, -0.0151,  0.0089],\n",
              "            [ 0.0130,  0.0149, -0.0033,  ..., -0.0183, -0.0262, -0.0195],\n",
              "            [-0.0159,  0.0195,  0.0096,  ...,  0.0212,  0.0006, -0.0147]],\n",
              "           device='cuda:0', requires_grad=True),\n",
              "    Parameter containing:\n",
              "    tensor([-1.8118e-02,  2.7817e-02, -1.9838e-02,  2.3841e-02,  1.5238e-02,\n",
              "             1.8506e-02, -2.6745e-02, -3.1433e-02, -3.0839e-02,  8.7426e-03,\n",
              "             1.7750e-02, -1.9132e-02,  4.8521e-03, -1.7422e-02,  1.7771e-02,\n",
              "             5.9097e-03,  2.3067e-02, -3.8717e-03, -1.6859e-02, -1.0940e-02,\n",
              "            -6.0716e-03,  9.5921e-03,  8.6659e-03,  2.3914e-02, -8.2484e-03,\n",
              "            -1.1126e-02, -1.3545e-02, -6.9706e-03, -1.4684e-02,  1.0366e-02,\n",
              "            -3.0602e-02, -2.5311e-04, -2.9148e-02,  1.3749e-02,  2.6279e-02,\n",
              "             2.3415e-02,  3.0909e-02,  2.2240e-02, -2.0534e-04, -3.0910e-02,\n",
              "            -2.9967e-02, -1.6821e-02, -1.6823e-02, -6.6773e-03, -1.9761e-02,\n",
              "            -2.6331e-02,  7.7959e-03,  2.1958e-02, -5.8001e-03,  1.4815e-02,\n",
              "            -9.5447e-03,  5.1434e-03,  1.6097e-02,  2.0478e-02, -8.1313e-03,\n",
              "             1.6197e-02,  9.1496e-03,  2.5342e-02,  1.6137e-02, -2.5656e-02,\n",
              "            -1.8861e-02, -3.8307e-03, -5.4298e-03,  8.8476e-03,  6.4336e-03,\n",
              "            -1.3226e-02,  2.3352e-02,  1.9205e-02,  2.7015e-02, -6.4431e-03,\n",
              "            -2.0183e-02, -1.7816e-02, -7.1554e-03, -1.2285e-02, -1.8457e-02,\n",
              "            -2.5042e-02,  2.7155e-02,  2.2412e-02,  2.0798e-02, -1.9102e-02,\n",
              "            -3.1425e-02,  1.6581e-02,  1.9337e-02,  1.2459e-02,  5.6943e-03,\n",
              "             9.0794e-03,  2.9483e-02,  2.5612e-02, -4.5737e-03, -1.3002e-03,\n",
              "            -1.9898e-02, -1.7501e-02, -2.8132e-02,  7.5364e-03, -1.9936e-03,\n",
              "            -2.0954e-02, -7.1631e-04,  1.2868e-02, -2.4564e-02, -2.1866e-02,\n",
              "             2.3896e-02, -1.3259e-02,  1.7876e-02,  2.8075e-02, -1.4995e-02,\n",
              "            -2.7850e-03, -3.0166e-03,  2.8716e-02, -1.0577e-02, -7.1062e-03,\n",
              "             4.9014e-03,  3.2766e-02, -2.0347e-02, -1.8240e-02,  1.7218e-02,\n",
              "             1.1607e-03,  6.5541e-04, -2.8179e-02,  3.2313e-04,  1.8283e-02,\n",
              "            -1.8563e-03, -1.6919e-02,  2.4340e-02,  2.7390e-02,  2.6575e-02,\n",
              "             1.0088e-02, -2.0847e-04,  1.2015e-02,  2.4438e-02,  2.9886e-02,\n",
              "            -1.9280e-02,  5.4056e-03,  2.5907e-02,  6.7308e-03,  2.9087e-03,\n",
              "             8.9887e-03, -1.9327e-02, -2.1997e-02, -3.9602e-03, -1.5551e-02,\n",
              "            -3.0551e-02,  2.5063e-02,  1.5483e-02,  7.8820e-03,  8.6799e-04,\n",
              "            -5.1095e-03,  2.9765e-02, -6.1604e-03,  2.3014e-02,  3.6488e-03,\n",
              "             2.2484e-02,  2.0513e-02, -2.4284e-02, -1.6262e-02, -1.4485e-02,\n",
              "            -1.1174e-02,  1.9979e-02, -1.7574e-02,  2.5884e-02, -1.0794e-02,\n",
              "             9.1818e-03,  1.9142e-02,  1.5617e-02,  1.8375e-02, -1.3344e-02,\n",
              "            -2.0693e-02,  2.1482e-03, -2.1523e-02, -1.3912e-02, -2.3021e-02,\n",
              "            -1.9401e-02, -8.3076e-03,  6.6115e-03,  2.0910e-02,  2.6615e-02,\n",
              "             5.4168e-03, -2.9698e-02,  2.2153e-02, -1.3466e-04,  9.7449e-03,\n",
              "             2.3699e-02, -2.3275e-02, -3.1955e-02,  1.9719e-02, -1.0907e-02,\n",
              "             2.8429e-02,  1.9254e-03, -9.3987e-03, -1.0479e-02, -1.9665e-02,\n",
              "            -1.1272e-02, -2.3966e-02,  2.9274e-02, -2.9660e-02,  1.2494e-02,\n",
              "             8.3643e-03, -4.7802e-03,  2.0754e-02, -6.4386e-03, -1.0606e-03,\n",
              "            -2.8425e-03, -3.5221e-03, -7.8872e-03, -1.7450e-02,  9.3602e-05,\n",
              "             1.6344e-02,  8.8699e-03,  2.6997e-03, -1.0894e-02,  2.2183e-02,\n",
              "            -7.5934e-03, -1.5766e-02,  1.3197e-02, -2.8102e-02,  1.8722e-02,\n",
              "             2.9627e-02, -1.9444e-02, -3.0675e-02, -4.5557e-03,  1.6862e-02,\n",
              "             2.9614e-02,  1.3836e-02, -1.2307e-04,  2.3630e-02,  1.9079e-02,\n",
              "            -2.8872e-02, -2.7364e-02,  2.2552e-02,  9.0262e-03,  1.6525e-02,\n",
              "             1.9304e-02, -1.2763e-02,  1.2107e-02,  1.2066e-02, -2.6160e-02,\n",
              "             1.6968e-02, -1.9698e-02,  1.7990e-02,  6.7113e-03, -2.0761e-02,\n",
              "             9.6888e-03,  1.6073e-03,  3.0049e-02,  8.8395e-03, -1.3277e-02,\n",
              "            -3.0571e-03, -1.3384e-02, -1.6273e-02,  8.0172e-03, -1.5162e-02,\n",
              "             2.5000e-02,  2.3237e-02,  2.6496e-02, -1.6572e-02,  2.7605e-02,\n",
              "             6.9329e-03,  1.2048e-02,  3.6213e-03,  2.1627e-02, -2.1237e-02,\n",
              "             1.9211e-02,  3.8263e-03, -3.1893e-03,  2.5782e-03,  2.7857e-02,\n",
              "            -1.1668e-02,  4.7829e-03,  9.7774e-03, -1.1603e-02, -1.1157e-04,\n",
              "             1.4537e-02, -1.6710e-02,  1.6391e-02,  1.5101e-02, -2.4145e-02,\n",
              "            -1.3569e-02,  1.8777e-02,  8.7727e-03,  1.2942e-02, -1.6551e-02,\n",
              "             2.5341e-02,  2.3251e-02,  3.4124e-03, -1.9081e-02,  6.7153e-03,\n",
              "            -2.1702e-02, -3.2667e-03,  1.2888e-02, -1.8995e-02,  1.3751e-02,\n",
              "            -1.1347e-02,  2.5117e-02, -2.2513e-04,  7.6802e-03,  2.2909e-02,\n",
              "             4.1527e-03,  2.4187e-02, -2.3635e-02, -2.2223e-02, -7.9086e-03,\n",
              "            -8.8565e-03,  1.5338e-02,  2.0760e-02,  1.8024e-02, -1.1566e-02,\n",
              "            -9.7946e-03, -6.1387e-03, -9.8751e-03,  2.0857e-02, -2.5879e-02,\n",
              "            -2.4259e-02, -2.1507e-02, -2.2012e-02,  1.6896e-02,  2.0769e-02,\n",
              "             1.6404e-02,  1.5029e-02,  8.0733e-04, -7.0626e-03,  2.6044e-02,\n",
              "             1.0507e-02,  8.0682e-03, -4.9415e-03,  7.9126e-03,  1.5225e-02,\n",
              "             1.4061e-02,  2.2191e-02,  3.0241e-02,  3.1632e-03,  2.0591e-02,\n",
              "            -2.5114e-02, -5.2946e-03,  2.5521e-02,  3.0078e-02, -9.1289e-03,\n",
              "            -7.5353e-03, -1.3472e-02,  1.0194e-03, -2.5093e-02, -1.1598e-02,\n",
              "             3.9075e-03,  2.8188e-02, -1.6021e-02,  5.4272e-03,  5.4096e-03,\n",
              "            -1.1444e-02, -2.3850e-02,  3.0925e-03, -5.6242e-03, -2.3519e-02,\n",
              "             1.9755e-02,  1.3664e-02,  7.7505e-03,  1.4545e-02, -2.4284e-02,\n",
              "             2.9570e-03,  2.5232e-02,  8.7355e-03, -1.9063e-02, -2.7562e-02,\n",
              "             3.0592e-02,  1.5665e-02, -1.3394e-02, -1.3365e-02, -3.6689e-03,\n",
              "             1.4641e-02, -7.6485e-03, -5.6154e-04,  1.8096e-02, -2.5056e-02,\n",
              "            -2.8741e-03,  8.2832e-03,  1.3541e-02,  1.4654e-02, -2.7058e-02,\n",
              "            -1.1220e-02,  2.3834e-03,  2.9731e-02,  4.0272e-03,  2.3995e-02,\n",
              "            -2.9039e-03, -6.5095e-03,  1.0198e-02, -1.8073e-02,  2.2547e-02,\n",
              "             2.7533e-02, -3.0975e-02, -1.9335e-02, -2.8631e-02,  1.2945e-02,\n",
              "            -2.4050e-03,  2.8984e-02, -2.3017e-02, -2.3147e-03, -1.1843e-02,\n",
              "             2.4194e-02,  5.1528e-03,  6.1314e-03,  4.7550e-03,  1.4418e-02,\n",
              "            -2.1870e-02,  2.3858e-03, -2.2174e-02, -1.7394e-02, -5.2301e-03,\n",
              "            -1.0067e-02, -2.7866e-02, -9.3755e-03,  1.5629e-02,  2.4071e-02,\n",
              "             2.0701e-03,  6.5893e-03,  2.2720e-02,  2.9220e-02, -1.3871e-02,\n",
              "             6.8237e-03,  1.1073e-02, -2.9574e-02,  1.6362e-02, -1.0868e-02,\n",
              "            -7.2435e-03, -2.7287e-02,  1.8707e-02,  2.0534e-02,  2.2766e-02,\n",
              "            -2.7630e-02, -3.0816e-02, -3.9331e-03, -1.7676e-02,  5.8166e-03,\n",
              "            -2.7955e-02,  4.7158e-03, -2.8129e-02,  1.0950e-02,  2.0204e-02,\n",
              "             3.1067e-04,  2.6103e-02, -2.9586e-02,  2.6510e-02,  1.1562e-02,\n",
              "             4.5301e-04,  2.5527e-02,  1.5000e-02, -1.0185e-02, -3.0959e-02,\n",
              "             1.5899e-02,  1.2843e-03, -2.3301e-02, -2.4410e-02, -7.9867e-03,\n",
              "             2.3622e-02, -2.5409e-02, -3.4703e-03,  2.0630e-02,  2.6553e-02,\n",
              "            -6.0895e-03, -2.6139e-02,  7.4240e-03,  1.3288e-02, -1.9744e-02,\n",
              "             6.1175e-03, -8.7067e-03, -8.9110e-04, -1.4531e-02,  2.5095e-02,\n",
              "             7.0041e-03, -5.6488e-03,  2.2695e-02,  1.3583e-02,  1.5536e-02,\n",
              "            -3.6452e-03,  2.5018e-02,  2.3305e-02,  2.6635e-02,  2.4868e-02,\n",
              "             2.6701e-02, -1.3020e-03,  1.8816e-02,  1.4462e-02, -1.9436e-02,\n",
              "            -8.5722e-03,  1.6436e-02, -1.8730e-02, -1.5612e-02,  1.0505e-02,\n",
              "            -7.3045e-03, -9.2654e-03,  4.0530e-03,  2.0449e-02, -2.1521e-02,\n",
              "             2.3009e-02,  4.5904e-03,  1.1671e-02, -2.6559e-02, -6.6724e-03,\n",
              "             1.4720e-02, -1.3155e-03,  1.0224e-02, -4.8472e-03,  1.0899e-02,\n",
              "            -8.0005e-03,  2.7024e-02, -2.3387e-02, -1.5996e-02,  4.2850e-03,\n",
              "             1.7434e-02, -2.6449e-02,  1.9437e-02, -2.9781e-02, -3.2994e-04,\n",
              "             2.5557e-02, -8.4039e-03, -2.3315e-04,  1.4014e-03, -2.9112e-02,\n",
              "            -1.8403e-02, -2.4355e-02, -2.5911e-02, -1.8838e-02,  2.5219e-02,\n",
              "             2.1696e-02, -1.4893e-02, -2.2284e-02,  2.9499e-02,  1.5011e-03,\n",
              "             1.7513e-03, -6.8941e-03,  2.2283e-02,  2.7758e-02, -1.8171e-02,\n",
              "             5.4178e-03, -7.2049e-03,  1.2354e-02,  2.1812e-02,  1.7427e-02,\n",
              "             2.4205e-02,  1.4981e-02, -8.2413e-03,  2.2317e-02,  1.4127e-02,\n",
              "            -1.2634e-02,  2.5160e-02, -2.4898e-03, -4.7699e-03,  1.5621e-02,\n",
              "             3.3414e-03,  1.4538e-03, -2.3740e-02,  1.9958e-02,  1.7023e-02,\n",
              "            -3.3535e-04, -2.1802e-03, -2.3860e-02, -1.5098e-02, -2.0221e-02,\n",
              "             2.5401e-02, -9.3477e-03,  2.0335e-02,  7.3290e-03, -1.6413e-02,\n",
              "            -2.9674e-02, -1.7253e-02,  1.4320e-02,  7.6620e-03,  2.7278e-02,\n",
              "             2.3301e-02, -2.3178e-02, -2.4300e-02,  2.4014e-02,  3.4079e-03,\n",
              "             4.7119e-03,  2.5677e-02,  8.8305e-03,  1.0131e-02,  2.0829e-02,\n",
              "             2.0087e-02, -1.5111e-02,  1.7677e-03,  2.2972e-02,  1.2750e-02,\n",
              "            -1.3178e-02,  3.4779e-03, -2.7756e-02,  3.8141e-03, -1.2839e-02,\n",
              "             8.6578e-03, -7.6490e-03,  1.2918e-03,  2.6574e-02,  2.1223e-02,\n",
              "            -2.6139e-02,  1.0425e-02, -1.1013e-02, -1.7062e-02, -1.2136e-02,\n",
              "            -1.7372e-02,  1.8214e-02,  2.2429e-02,  7.1559e-03,  1.2937e-02,\n",
              "             2.4048e-02,  4.0526e-03,  1.3215e-03,  2.3807e-02,  1.6253e-02,\n",
              "             2.7084e-02,  2.2869e-02, -1.0692e-02,  2.6442e-02, -2.7687e-02,\n",
              "            -1.8129e-03, -1.2693e-02,  2.9193e-02, -2.2183e-02, -1.2941e-02,\n",
              "            -2.6905e-02, -1.0474e-02, -1.1379e-02, -2.8667e-02,  1.5728e-02,\n",
              "            -1.1269e-04,  2.4486e-02,  1.5575e-02,  6.3477e-03, -3.8496e-03,\n",
              "             2.7682e-02, -1.4838e-02,  4.0568e-03, -3.0306e-02,  1.0135e-02,\n",
              "             1.4527e-02,  2.4086e-02, -1.3246e-03, -5.5541e-03, -3.0655e-03,\n",
              "            -7.8492e-03,  7.9160e-03, -5.7563e-03, -9.3403e-03,  2.6658e-02,\n",
              "             2.7359e-02,  1.7629e-02,  6.9327e-03, -2.7225e-02, -1.3017e-02,\n",
              "            -2.7790e-02, -1.8800e-02,  2.6362e-02,  6.4411e-03, -2.3820e-02,\n",
              "            -3.1337e-02, -1.7480e-02, -1.4079e-03,  1.2087e-02, -7.3949e-03,\n",
              "             1.5092e-02,  1.1141e-02, -2.0506e-02, -9.5732e-03, -4.9644e-03,\n",
              "            -2.8180e-02,  6.0945e-04,  1.5499e-02, -1.3375e-03, -1.7667e-02,\n",
              "            -4.7949e-03,  1.5820e-02, -2.7158e-02, -1.8429e-02, -2.6913e-02,\n",
              "             9.4847e-03,  1.2014e-02,  2.3916e-02, -1.9053e-02, -1.4085e-02,\n",
              "             5.2371e-03, -4.7322e-03, -2.7768e-02,  2.3044e-02,  1.7862e-02,\n",
              "             5.1652e-03,  5.7866e-03,  1.5405e-02,  3.0862e-02,  7.4468e-03,\n",
              "            -2.8660e-02,  2.1098e-02, -1.2801e-02, -9.1947e-03, -1.4526e-02,\n",
              "             3.5940e-03,  3.4609e-03, -1.5262e-03,  6.1190e-03,  1.3020e-02,\n",
              "             1.8133e-02, -3.0893e-03,  9.3023e-03, -4.3035e-03, -5.8621e-03,\n",
              "            -4.9882e-03,  1.9467e-02,  5.0334e-03,  1.7634e-02, -2.4214e-03,\n",
              "             1.5824e-02,  1.1113e-02,  2.7371e-02,  1.7201e-03,  7.7691e-04,\n",
              "             1.0473e-02,  7.1966e-03, -3.5944e-03,  2.9299e-02,  1.6452e-03,\n",
              "            -1.4841e-02, -5.2132e-03,  1.7415e-02, -1.2846e-02,  1.4125e-02,\n",
              "             1.7135e-02, -1.5827e-02,  1.8330e-02, -1.5705e-02, -4.1303e-03,\n",
              "             2.9470e-02,  1.3214e-03, -3.0864e-02,  2.2606e-02, -1.4998e-02,\n",
              "             1.7664e-02, -2.5116e-02,  5.5415e-03,  1.9734e-02, -2.0948e-02,\n",
              "            -1.0499e-02, -4.4634e-03, -2.3454e-03, -2.5811e-02, -1.7683e-02,\n",
              "            -2.5238e-02,  2.9903e-02, -1.3415e-02, -2.4076e-02,  8.0738e-03,\n",
              "            -6.4178e-03,  2.6694e-02,  2.4435e-02, -2.0672e-02,  2.8067e-03,\n",
              "            -1.1542e-02,  2.3122e-02, -2.7174e-02,  7.0581e-03,  4.4595e-03,\n",
              "            -2.8221e-02, -1.0331e-02, -1.1479e-02, -2.8966e-02, -2.0666e-02,\n",
              "             1.1246e-03,  2.5485e-02,  1.9940e-02, -2.4241e-03, -2.7133e-02,\n",
              "            -6.4601e-05, -1.4391e-02,  3.9006e-03,  1.9059e-02,  1.4498e-02,\n",
              "             1.6925e-02, -2.5129e-02,  6.7160e-03,  1.6520e-02, -1.1597e-02,\n",
              "            -2.5776e-03, -1.0697e-02, -1.8838e-02, -1.8677e-03,  1.9182e-04,\n",
              "             1.1723e-02, -1.2418e-03, -1.3915e-02, -1.5141e-02,  2.1583e-02,\n",
              "            -3.6220e-03,  4.7592e-03,  6.0094e-03,  1.5773e-02,  1.8182e-02,\n",
              "             2.8263e-02, -3.0791e-02, -2.1032e-02,  3.9093e-03, -2.7280e-02,\n",
              "             2.0330e-02, -2.2542e-02, -2.1506e-02,  1.1711e-02, -1.7368e-02,\n",
              "            -1.5672e-02, -9.6132e-03, -2.1427e-02, -1.0420e-02,  2.0561e-02,\n",
              "             2.0470e-02, -8.8239e-03, -5.3848e-04, -1.3559e-02,  1.4518e-02,\n",
              "             1.5428e-02,  2.5589e-02,  8.3284e-03,  1.7063e-02,  1.7706e-02,\n",
              "            -1.9177e-02, -5.2429e-03,  8.2786e-03, -1.4832e-02, -1.7999e-02,\n",
              "            -2.3194e-02,  4.3262e-03, -5.3708e-04,  5.0061e-03, -9.5460e-03,\n",
              "             2.6492e-02,  2.0044e-02, -1.9577e-02, -2.6207e-02, -1.7423e-02,\n",
              "            -2.8654e-02, -2.3089e-02,  2.0345e-02, -1.6634e-02,  1.5337e-02,\n",
              "            -1.7777e-02, -2.9554e-02,  1.7709e-02, -1.8214e-02, -2.0106e-03,\n",
              "            -2.8342e-02,  1.1914e-02, -2.4641e-02, -1.3474e-02, -3.6725e-03,\n",
              "             3.8342e-03,  4.7166e-03,  2.7441e-02,  2.7444e-02,  3.5137e-03,\n",
              "             2.6906e-02, -2.0687e-02,  2.6347e-02, -2.4380e-02, -2.8713e-03,\n",
              "             2.2155e-02, -2.1435e-02, -8.5123e-03,  3.0116e-02,  1.0453e-02,\n",
              "            -2.2873e-02, -7.3061e-03,  1.3505e-02, -1.0680e-02,  1.5951e-02,\n",
              "            -3.5714e-03, -1.0233e-02,  1.5326e-02, -5.9924e-03,  1.9178e-02,\n",
              "            -1.8455e-02, -1.6403e-02, -7.0622e-03,  2.1060e-02, -2.7514e-02,\n",
              "            -1.5754e-02, -1.6234e-02,  2.9609e-02,  1.9457e-02, -1.6782e-02,\n",
              "             2.5768e-02, -2.2089e-02,  1.7510e-02, -1.5594e-02,  1.6076e-02,\n",
              "             3.8558e-03,  9.0380e-03, -2.5038e-02, -2.2602e-02, -1.9117e-02,\n",
              "            -8.2552e-03, -5.6334e-03, -2.2476e-02,  2.0380e-02,  1.2791e-02,\n",
              "            -2.8848e-02,  2.6836e-02,  1.6533e-02, -2.6037e-02,  3.0777e-03,\n",
              "            -2.7272e-02, -2.8729e-02,  1.1994e-02, -2.3057e-02,  2.3210e-02,\n",
              "            -2.6339e-02,  6.6061e-03,  1.9914e-02, -2.7028e-02,  2.1034e-03,\n",
              "            -2.1373e-02,  6.3881e-04,  2.5240e-02, -3.3166e-03], device='cuda:0',\n",
              "           requires_grad=True),\n",
              "    Parameter containing:\n",
              "    tensor([[-0.0011,  0.0150, -0.0290,  ...,  0.0266, -0.0234, -0.0158],\n",
              "            [ 0.0112, -0.0044,  0.0054,  ..., -0.0279, -0.0017, -0.0185],\n",
              "            [-0.0184, -0.0007, -0.0313,  ..., -0.0153, -0.0048,  0.0041],\n",
              "            ...,\n",
              "            [ 0.0244, -0.0298,  0.0153,  ...,  0.0161, -0.0074,  0.0194],\n",
              "            [-0.0193,  0.0254,  0.0143,  ..., -0.0110,  0.0213, -0.0299],\n",
              "            [-0.0099, -0.0057,  0.0059,  ..., -0.0267,  0.0158,  0.0241]],\n",
              "           device='cuda:0', requires_grad=True),\n",
              "    Parameter containing:\n",
              "    tensor([ 1.8927e-02,  5.1221e-03, -6.3455e-03,  6.3523e-03, -1.8603e-03,\n",
              "            -2.5010e-02,  3.1347e-02,  1.4582e-03, -9.5578e-03, -1.6248e-03,\n",
              "            -7.8893e-03,  2.3840e-02,  2.9724e-02,  5.5908e-03,  9.4720e-04,\n",
              "             2.1751e-02,  8.3106e-04,  1.5326e-02,  5.0339e-03, -2.9501e-02,\n",
              "            -3.2940e-02,  1.7357e-02, -3.2578e-02,  8.6837e-03, -3.0859e-02,\n",
              "            -1.6332e-02, -6.6037e-03,  2.7375e-04, -1.1204e-02, -1.3180e-02,\n",
              "             2.9279e-03,  3.3989e-03,  2.4262e-02, -3.1681e-02, -1.0913e-02,\n",
              "            -1.0458e-02,  1.6959e-02, -5.0488e-03, -3.0351e-02, -2.1643e-03,\n",
              "            -1.4353e-02, -2.6382e-02,  3.0768e-02, -2.1259e-02,  1.1666e-02,\n",
              "             1.9044e-02,  1.0232e-02,  2.8427e-02, -2.2469e-02, -2.1643e-02,\n",
              "             2.2096e-02,  7.8680e-03,  1.8666e-02,  2.9698e-02,  2.2635e-02,\n",
              "             1.7691e-02, -2.9671e-02,  4.9964e-03, -2.9822e-02,  7.9353e-03,\n",
              "             9.7695e-03,  2.3912e-02, -3.2366e-02, -1.5210e-02,  6.0818e-03,\n",
              "             1.8792e-02,  2.0241e-02,  5.5982e-03,  2.9330e-04, -2.4334e-02,\n",
              "             6.4341e-03,  4.3976e-03, -1.1488e-02,  1.1159e-02,  1.0546e-02,\n",
              "             1.3750e-02, -3.1680e-02,  1.7578e-02, -1.5640e-02, -2.1870e-02,\n",
              "            -3.0378e-02,  2.7670e-02, -5.1787e-03, -1.8056e-03,  2.0900e-02,\n",
              "             4.4609e-03, -1.4667e-02,  6.0511e-03, -1.8227e-02,  3.0482e-02,\n",
              "             2.5518e-02,  1.4948e-02, -9.2702e-03,  1.9513e-02, -1.9906e-02,\n",
              "             3.1905e-02, -2.5397e-02,  5.7974e-03,  5.7382e-04,  2.9320e-02,\n",
              "             3.2204e-02,  1.2893e-02,  2.1015e-02,  2.2990e-02, -6.4969e-05,\n",
              "             5.8146e-03, -2.2263e-02, -1.9236e-02,  2.1229e-02, -2.8150e-02,\n",
              "             1.1730e-02,  2.3219e-02, -2.1322e-02, -1.6256e-02, -1.2405e-02,\n",
              "            -2.9742e-02,  9.3833e-03, -2.8342e-02, -1.6681e-02,  2.5492e-02,\n",
              "             2.2188e-02,  4.1535e-04, -3.1688e-02,  1.0771e-02, -2.0049e-02,\n",
              "             5.1351e-03, -1.1425e-02, -3.0886e-02,  2.9006e-02,  1.8110e-02,\n",
              "             2.2185e-02,  7.8402e-03,  1.2789e-02,  2.4833e-02,  2.8787e-02,\n",
              "            -4.6257e-03,  1.6405e-02,  1.2906e-02,  2.3993e-02,  1.5517e-02,\n",
              "            -2.6469e-02, -2.5906e-02,  7.7334e-03, -2.6463e-02,  1.0474e-03,\n",
              "            -1.9411e-02,  1.7606e-02,  1.4320e-02, -7.9357e-03, -2.2951e-03,\n",
              "             6.3760e-03, -2.6427e-02,  1.7020e-02, -1.0167e-02, -3.2292e-02,\n",
              "             7.6268e-03, -1.0041e-02,  1.6680e-02, -2.9899e-02, -2.0065e-02,\n",
              "            -3.2987e-02, -1.9525e-02, -9.3159e-03, -9.7834e-03, -1.9171e-02,\n",
              "             1.8495e-02,  9.2168e-03,  5.5167e-03, -1.7018e-02,  2.4381e-02,\n",
              "             7.0030e-03,  3.2638e-02, -2.6448e-02,  1.4497e-02, -9.7048e-03,\n",
              "             2.5006e-02, -1.1184e-02, -1.4239e-03,  1.3675e-02,  3.1494e-02,\n",
              "             1.3383e-02,  2.9359e-02,  3.0570e-02,  2.7051e-02, -2.2908e-04,\n",
              "            -2.0143e-02, -6.7779e-03,  2.5610e-02,  5.8868e-03,  1.5429e-02,\n",
              "            -7.6164e-03,  2.4390e-02,  2.7092e-02,  9.2754e-03,  2.9862e-02,\n",
              "             3.0693e-02,  1.4166e-02, -1.7550e-02,  2.6814e-02,  1.4938e-02,\n",
              "            -3.0595e-02,  2.6268e-03, -5.6717e-04, -1.0284e-02,  2.9573e-03,\n",
              "             2.4493e-02, -1.2793e-02, -8.2947e-03,  8.6051e-03,  1.5251e-02,\n",
              "            -1.2551e-03,  1.8370e-02,  2.0194e-02,  3.1704e-02,  1.6271e-02,\n",
              "             5.7648e-03, -1.1754e-02, -2.8936e-02,  1.3602e-02, -2.0230e-02,\n",
              "             2.3500e-02, -3.4909e-03,  6.8151e-03, -3.9713e-03, -1.9410e-02,\n",
              "            -2.2285e-02, -8.1350e-03, -5.3547e-03, -3.1504e-02, -1.6675e-03,\n",
              "             2.8465e-02,  1.1023e-02, -1.4967e-02,  3.3372e-02, -1.2001e-02,\n",
              "             2.3860e-02, -3.2786e-02,  6.5018e-03,  4.8840e-03, -2.9866e-02,\n",
              "            -3.1997e-02,  4.7691e-03, -5.3828e-03, -1.9831e-02, -1.1306e-03,\n",
              "             7.1287e-04, -2.1314e-02,  2.2658e-02,  1.2472e-03, -1.7829e-02,\n",
              "            -2.4073e-02,  1.2982e-04,  1.8906e-02,  2.6881e-02, -1.9781e-02,\n",
              "             2.9587e-02,  2.0501e-02, -3.0865e-02, -2.2421e-02, -1.5331e-02,\n",
              "            -5.9236e-03, -2.9065e-02,  1.8332e-02, -1.1057e-02,  3.2437e-03,\n",
              "             3.0179e-02,  1.3030e-02,  2.8039e-03, -1.1443e-02, -1.6171e-02,\n",
              "            -7.3875e-03,  2.9498e-02, -2.8567e-02, -1.7148e-02,  7.7138e-03,\n",
              "            -9.3358e-03,  6.1467e-03,  2.0958e-02, -5.4689e-03,  2.9116e-02,\n",
              "            -1.0117e-02,  1.8468e-02,  1.9251e-02,  8.7718e-04,  8.4298e-03,\n",
              "             2.4617e-02,  2.0735e-02,  3.2348e-02,  1.5459e-02, -3.0858e-02,\n",
              "             2.2610e-02,  1.2571e-02,  1.3680e-02,  7.0614e-03,  1.1473e-02,\n",
              "             6.4096e-03, -2.4560e-02,  2.9356e-02, -5.1656e-03,  8.0530e-03,\n",
              "            -2.3912e-02,  1.1003e-02,  2.2139e-04, -2.6259e-02, -3.0709e-02,\n",
              "            -1.8203e-02,  1.7659e-02, -5.8809e-03, -1.2096e-02, -2.5559e-02,\n",
              "            -2.1161e-03, -2.4080e-03, -1.4081e-02,  2.3075e-02, -1.3532e-02,\n",
              "             1.4881e-02, -2.5931e-02, -1.9693e-02,  1.1995e-02,  1.7738e-02,\n",
              "             1.7832e-02,  1.5712e-02, -1.7824e-02,  1.2335e-02,  2.7940e-02,\n",
              "             2.9430e-02, -6.0981e-03, -3.9327e-03,  1.8676e-04, -1.6828e-02,\n",
              "             9.6875e-03, -1.0320e-02, -1.5209e-02,  1.7820e-02,  3.7072e-03,\n",
              "            -6.9283e-03,  2.4926e-02, -2.9640e-02,  1.6759e-02, -2.8035e-02,\n",
              "            -2.9126e-02,  4.4247e-04,  2.5255e-02,  3.2039e-02, -7.6639e-04,\n",
              "             1.8959e-02,  1.8422e-02,  1.0468e-02, -1.5606e-02, -1.7895e-02,\n",
              "             2.0422e-04,  2.6132e-02,  1.9343e-02, -2.0902e-02, -3.1470e-02,\n",
              "            -2.9382e-02,  5.9990e-03,  5.1359e-03,  2.8704e-02, -3.3020e-02,\n",
              "            -8.3787e-04,  1.9185e-02,  1.3709e-02,  2.6157e-03,  2.7711e-02,\n",
              "             4.4097e-03, -2.6750e-02,  1.2574e-02, -2.2876e-02,  4.3015e-03,\n",
              "            -1.5464e-02, -3.0670e-02, -6.9692e-03,  6.6724e-03,  9.0484e-03,\n",
              "             7.7590e-03, -2.2830e-02,  3.1140e-02, -2.9158e-02, -2.5565e-02,\n",
              "             1.3649e-02,  2.1585e-02, -2.5726e-02,  7.5124e-03,  2.3015e-02,\n",
              "             2.2905e-02, -1.2637e-02, -3.0784e-02, -2.9450e-02, -8.2776e-03,\n",
              "             1.9575e-02, -1.1717e-03,  1.4080e-02, -3.2793e-02,  1.0969e-02,\n",
              "            -5.7936e-03,  1.6334e-02,  2.6089e-02,  1.5582e-02, -1.8675e-02,\n",
              "            -1.1825e-02, -3.9736e-04,  2.4679e-02, -2.8159e-02,  6.3136e-03,\n",
              "            -1.2345e-02,  7.7562e-03,  1.1998e-02,  1.5554e-03, -9.4891e-03,\n",
              "            -2.0778e-02, -4.1658e-03, -1.0279e-02, -1.5105e-02,  1.2113e-02,\n",
              "             2.0310e-02, -7.7104e-03, -9.6433e-03,  2.1543e-02, -9.0736e-03,\n",
              "            -1.2576e-02,  2.6052e-02,  2.8745e-02,  4.8979e-03,  1.9407e-02,\n",
              "             2.3075e-02,  2.6245e-02,  1.4625e-03,  2.1202e-02, -2.5316e-02,\n",
              "            -3.2156e-02, -2.1280e-02,  8.5685e-03, -1.0954e-02,  3.2335e-02,\n",
              "            -3.0746e-02,  1.8819e-04, -2.8861e-02, -1.3456e-02, -5.0047e-03,\n",
              "            -2.7469e-02,  3.0849e-02,  2.4916e-02, -9.5976e-03, -2.8835e-02,\n",
              "             3.0585e-02,  2.0288e-02,  3.1159e-02, -3.2167e-02,  1.6660e-02,\n",
              "            -8.6249e-03,  1.7406e-02, -3.0004e-02,  2.6126e-03, -3.0445e-02,\n",
              "             1.4691e-02,  2.7817e-02,  2.0409e-02, -1.1569e-02,  1.8775e-02,\n",
              "             3.3508e-03,  6.9867e-04, -9.1174e-03, -5.3175e-03,  2.7795e-02,\n",
              "             7.8157e-03,  3.0626e-02, -2.3580e-03,  3.1309e-03,  1.0328e-02,\n",
              "            -6.8944e-03,  2.8782e-02, -2.2018e-02,  2.8706e-02, -9.1219e-03,\n",
              "            -1.1259e-02,  3.0469e-02,  2.2541e-02, -8.4406e-03, -1.6104e-02,\n",
              "            -2.9304e-03, -4.8423e-03,  1.8177e-02,  1.6771e-03,  2.2772e-02,\n",
              "            -1.3932e-02,  1.4532e-02,  3.2265e-02, -1.6483e-02,  1.0695e-02,\n",
              "             2.1712e-02,  5.8775e-03,  2.9231e-02, -1.5308e-02, -3.6297e-03,\n",
              "            -3.9448e-03, -9.0778e-03,  2.7032e-02, -4.2240e-04,  1.0461e-02,\n",
              "            -5.2272e-03,  2.4845e-02,  3.2272e-02, -2.5079e-02, -9.7401e-03,\n",
              "             3.3530e-02,  2.9799e-02, -1.8699e-02, -2.1961e-02, -9.2221e-03,\n",
              "             2.1172e-03,  3.3247e-04,  3.0518e-02, -1.3045e-02, -1.3759e-02,\n",
              "             2.7634e-02,  4.0073e-03, -7.2399e-03, -4.2636e-03, -3.4953e-02,\n",
              "             1.8751e-02,  2.5191e-02, -2.7228e-03,  2.9692e-02,  3.1514e-02,\n",
              "             1.6832e-02, -2.5429e-02, -1.2584e-02, -1.3050e-02, -2.6537e-02,\n",
              "             2.7773e-02, -3.2396e-03,  2.4540e-02, -2.1761e-02, -8.4514e-03,\n",
              "             7.5165e-03,  2.1400e-02, -1.6529e-02,  2.8550e-02, -1.8732e-02,\n",
              "             2.0587e-02, -1.6782e-02, -7.5184e-04,  1.0302e-02,  1.7501e-02,\n",
              "             1.6918e-02, -2.1261e-02, -6.4355e-03,  1.7775e-02, -1.1328e-02,\n",
              "            -3.1278e-02,  2.0045e-02, -2.1787e-02, -2.2428e-03,  2.9343e-02,\n",
              "            -9.3957e-03,  8.0241e-03,  2.8974e-02, -1.5669e-02, -7.3951e-03,\n",
              "             2.7528e-02,  1.1322e-02,  1.6284e-02,  2.8990e-02, -3.0958e-03,\n",
              "            -2.4017e-02, -9.3226e-03,  1.8039e-02, -1.0251e-02,  8.9491e-03,\n",
              "             2.3754e-03, -3.9807e-03, -4.4466e-03,  1.3898e-02,  1.5712e-03,\n",
              "            -2.0080e-02, -3.2700e-02,  2.7509e-02, -3.1064e-02, -1.8094e-02,\n",
              "            -2.4836e-03,  1.7167e-02,  2.7292e-02,  2.3941e-02,  4.8119e-03,\n",
              "            -1.6045e-02, -5.9072e-03, -2.7343e-02,  2.8222e-04,  9.8365e-03,\n",
              "             1.7415e-02,  1.8912e-02, -1.3324e-02,  1.6349e-02, -2.8214e-02,\n",
              "             8.4565e-03,  1.1052e-02, -5.7405e-03,  1.1273e-02, -2.1949e-03,\n",
              "            -1.7726e-02,  1.8747e-02,  2.4196e-02, -1.0137e-02, -2.3812e-02,\n",
              "            -1.2278e-02, -1.4339e-02, -6.2420e-03, -2.5390e-02, -2.3117e-03,\n",
              "             1.2715e-02,  2.0863e-02,  1.3462e-02, -4.2703e-04, -3.9571e-03,\n",
              "            -5.0714e-03,  1.9301e-02, -9.4622e-04,  1.5198e-02, -2.4975e-02,\n",
              "             1.0043e-02,  8.1847e-03, -2.6837e-02, -1.2704e-02, -2.5560e-02,\n",
              "             2.3369e-02, -9.2318e-03,  2.1868e-02,  2.1909e-02, -3.5216e-03,\n",
              "             7.0945e-03, -1.0589e-02,  1.1031e-02,  7.1564e-03,  1.2713e-02,\n",
              "            -1.3993e-02, -3.6386e-03,  1.5999e-02, -1.3954e-02, -1.5165e-02,\n",
              "            -1.9999e-02,  3.3830e-03, -2.6046e-02,  5.2853e-03, -1.1573e-02,\n",
              "             2.3597e-02, -4.6195e-03, -2.7684e-02, -2.7404e-02,  1.2742e-02,\n",
              "             1.6137e-02, -9.1372e-03,  2.7146e-02,  2.2029e-02,  2.6347e-02,\n",
              "            -2.5641e-02,  2.2225e-02,  4.2598e-03,  2.7160e-02,  1.3085e-02,\n",
              "            -1.1619e-02,  3.2142e-03,  2.4655e-02,  1.5217e-02, -1.3173e-02,\n",
              "             1.9245e-02,  1.2351e-03, -7.5416e-03, -7.0327e-03,  3.0604e-02,\n",
              "            -1.1170e-02,  1.2449e-02, -3.2878e-02,  3.4741e-02,  1.1468e-02,\n",
              "            -3.6181e-03,  2.9950e-02, -2.9271e-02, -1.0523e-02,  3.9430e-03,\n",
              "            -3.2741e-02,  5.2178e-03,  2.3637e-02, -4.1829e-03,  2.5731e-02,\n",
              "             2.7388e-02,  1.0659e-02,  9.0068e-03,  1.9524e-02,  2.6013e-02,\n",
              "            -3.1591e-02,  3.1329e-02,  2.3098e-02, -2.4066e-02, -2.0023e-02,\n",
              "             8.8087e-03, -1.4433e-03, -2.3289e-02,  2.8438e-02,  3.1747e-02,\n",
              "            -1.3471e-02,  3.1723e-02, -4.8878e-04,  1.8693e-02,  2.4573e-02,\n",
              "            -6.0951e-04, -2.8182e-02,  6.6607e-04, -2.1510e-02, -2.9051e-02,\n",
              "            -1.4550e-02,  1.4416e-02, -2.5855e-02,  1.1109e-02, -1.3412e-02,\n",
              "            -1.8272e-02, -6.6001e-03,  7.7085e-03,  2.9565e-02,  5.0854e-04,\n",
              "            -1.5329e-02,  1.9919e-02,  9.7481e-04, -2.7359e-02,  6.5234e-03,\n",
              "            -1.8656e-02,  3.1190e-02, -3.2748e-02, -1.9377e-02,  1.2248e-02,\n",
              "             3.0604e-02,  3.0263e-02, -1.4544e-02, -4.6499e-03,  4.8474e-03,\n",
              "            -1.9061e-02, -1.1660e-02,  1.2246e-02,  2.6904e-02, -1.2875e-02,\n",
              "            -3.2081e-02, -3.1931e-02,  2.0139e-03,  1.3254e-02,  1.8403e-02,\n",
              "            -4.1436e-03,  6.0135e-03,  2.5694e-02,  2.0937e-05,  1.2095e-02,\n",
              "             9.4952e-04,  2.0200e-02, -1.9913e-02,  7.6790e-03,  2.6354e-02,\n",
              "             2.7277e-02, -3.3127e-02, -1.6153e-03, -9.6690e-03, -5.7313e-03,\n",
              "            -2.1667e-02,  2.5687e-02,  2.3512e-02, -1.5623e-02,  5.3760e-03,\n",
              "            -2.1109e-02, -1.6786e-02, -1.8331e-02,  2.5909e-02, -1.7662e-02,\n",
              "            -2.8497e-02, -3.4930e-03,  4.2250e-04,  6.5064e-04,  2.0336e-03,\n",
              "            -7.1874e-03,  2.3845e-02, -1.9640e-02, -2.6436e-02,  2.9493e-02,\n",
              "             1.3693e-02,  1.6125e-02, -1.0303e-02,  7.4460e-03, -1.3200e-02,\n",
              "            -2.2474e-02, -2.1977e-02, -9.9615e-03,  5.5570e-03,  1.7066e-02,\n",
              "             2.4319e-02,  1.6411e-02, -3.0866e-02,  2.7912e-02,  2.3383e-02,\n",
              "            -7.7102e-03,  2.4872e-02, -2.1051e-02,  1.3724e-02,  1.7152e-02,\n",
              "            -7.3422e-03,  2.6152e-02, -3.0895e-02,  6.2165e-03, -2.0641e-02,\n",
              "            -1.6931e-03,  7.7407e-03,  2.2044e-03,  2.1164e-04,  7.5273e-04,\n",
              "            -2.1238e-02,  1.7640e-02, -1.3325e-02,  1.6020e-02,  5.1311e-03,\n",
              "             1.6632e-02,  2.4890e-02,  2.0036e-02,  1.3413e-02,  1.3770e-02,\n",
              "             2.1385e-02, -3.6739e-03,  3.9765e-03,  2.8669e-02, -1.7709e-02,\n",
              "            -4.3481e-03,  2.7301e-02, -3.0285e-02,  2.4981e-02,  2.4199e-02,\n",
              "             1.2630e-02,  1.4794e-02,  5.6664e-03,  1.3399e-02,  1.5550e-02,\n",
              "             2.5306e-02, -2.5831e-02, -7.6578e-03,  1.7840e-02, -1.5370e-02],\n",
              "           device='cuda:0', requires_grad=True),\n",
              "    Parameter containing:\n",
              "    tensor([[ 1.9164e-02, -2.6892e-02, -2.5727e-02,  2.1801e-02,  1.2254e-02,\n",
              "             -1.7262e-02, -1.2724e-02,  1.0954e-02, -2.5102e-02, -1.4763e-02,\n",
              "              1.8938e-02, -1.0669e-02, -2.6465e-02,  3.5014e-03,  1.7949e-02,\n",
              "              7.8463e-04,  1.0639e-02,  3.1837e-02, -2.9686e-02, -3.3847e-02,\n",
              "              6.3855e-03, -1.0299e-02, -2.0043e-02, -9.9410e-03, -2.8669e-02,\n",
              "              1.8348e-02,  3.2789e-02, -1.0123e-02,  2.8397e-02,  1.3679e-02,\n",
              "             -1.4750e-02,  1.2294e-02,  1.3386e-02, -3.4751e-03, -1.8019e-02,\n",
              "              2.7829e-02,  3.4167e-02,  3.1186e-02,  3.3943e-02, -1.4837e-02,\n",
              "              2.8412e-02, -9.5281e-03, -3.3718e-02,  1.0498e-02, -3.6541e-02,\n",
              "              1.8140e-02, -3.9761e-03,  7.0445e-03,  2.0761e-02,  2.1547e-02,\n",
              "             -3.0524e-02, -3.1369e-02, -6.6387e-04, -3.5795e-02,  3.2768e-02,\n",
              "             -7.8146e-03,  2.1132e-02, -2.8955e-02,  1.4915e-02,  1.0655e-02,\n",
              "             -5.3673e-03, -1.1869e-02, -1.5724e-02, -1.0130e-02,  1.3521e-02,\n",
              "             -2.4001e-02,  2.9218e-03,  1.0160e-02, -1.2710e-02,  3.6275e-03,\n",
              "              7.5566e-03, -4.7611e-03, -1.9959e-02, -1.3781e-02,  2.5636e-02,\n",
              "              1.5119e-03,  3.1328e-02,  7.7680e-03,  3.4220e-02, -1.3156e-02,\n",
              "             -3.8684e-03,  2.2372e-02,  3.2006e-02,  2.6199e-02, -1.7136e-02,\n",
              "             -2.6559e-02,  1.9828e-02,  2.8017e-02, -1.4699e-02,  2.9622e-02,\n",
              "              2.8106e-02,  2.0219e-02,  8.1820e-03, -1.5066e-02, -1.5592e-03,\n",
              "             -2.4202e-02,  3.3043e-02, -2.7951e-02, -1.7070e-02,  2.7692e-02,\n",
              "             -1.4364e-02,  3.0371e-02, -1.0146e-02, -1.7735e-02, -3.2489e-02,\n",
              "             -2.1748e-02, -2.7093e-02, -2.6706e-02, -1.5301e-02,  6.3175e-03,\n",
              "             -4.2294e-03,  1.8102e-02,  3.1684e-02, -3.3429e-02,  1.1052e-02,\n",
              "              1.1153e-02, -1.1135e-02,  2.0547e-02,  9.4565e-03,  2.8659e-02,\n",
              "             -3.2304e-02, -8.5253e-03,  9.8740e-04, -3.2515e-02,  5.7744e-03,\n",
              "              1.8454e-02,  2.7991e-02,  1.6542e-02, -5.5110e-03,  1.2966e-02,\n",
              "             -2.4903e-03,  2.0756e-02,  2.6872e-02,  2.0719e-03,  2.6698e-02,\n",
              "              2.6937e-02, -1.0350e-02, -2.9594e-02,  1.9505e-02, -2.4811e-02,\n",
              "             -2.1372e-02,  3.3810e-02,  2.1292e-02, -1.8156e-02,  1.9320e-02,\n",
              "              2.0476e-02,  2.9422e-02, -4.8722e-03, -2.0351e-02,  2.1648e-03,\n",
              "              2.1023e-02,  2.5992e-02,  2.9900e-02, -3.5773e-02,  1.4984e-02,\n",
              "              2.7783e-02, -8.6234e-03,  3.8275e-03, -7.8582e-03, -1.3038e-02,\n",
              "             -6.6945e-03, -1.2494e-02, -2.9399e-02, -4.0356e-04,  2.7304e-02,\n",
              "             -2.5248e-02,  1.2978e-02, -2.8332e-02,  2.6237e-03,  3.0991e-02,\n",
              "             -1.5593e-02, -3.0953e-02,  3.7444e-03, -8.8383e-03,  3.6671e-03,\n",
              "              1.4608e-02,  7.7422e-03, -3.5206e-02,  8.3963e-03, -2.9149e-05,\n",
              "              5.3355e-04, -1.6557e-02, -2.4932e-04,  3.0136e-02,  2.9218e-02,\n",
              "             -1.9063e-02, -7.4019e-03,  1.9924e-02,  2.6476e-03,  1.7579e-02,\n",
              "              3.5641e-02, -2.3627e-03,  1.9354e-02,  7.3279e-03,  6.3396e-03,\n",
              "              2.2755e-02, -1.9922e-02, -4.0839e-03, -2.8102e-03,  1.8032e-03,\n",
              "              3.4641e-02, -1.6968e-02, -2.0948e-02, -1.7952e-02, -1.5762e-02,\n",
              "              2.5184e-02,  2.7009e-02,  4.1368e-03, -2.9937e-03, -3.3989e-02,\n",
              "             -1.0791e-02,  1.0778e-02,  1.2258e-02,  1.5530e-02,  1.8753e-02,\n",
              "             -2.5601e-02,  2.0388e-02,  3.0720e-02,  7.7695e-03,  2.4577e-02,\n",
              "             -1.4049e-02, -5.9786e-03,  3.1868e-02,  2.8414e-02, -1.6118e-02,\n",
              "              7.2309e-03, -1.3870e-02,  2.6207e-02,  3.2498e-02,  3.8844e-03,\n",
              "              2.1468e-02,  5.2975e-03,  2.9293e-02,  1.2179e-02, -2.3820e-02,\n",
              "              2.9919e-02,  2.1188e-02,  3.5891e-03, -3.1657e-02, -3.5290e-03,\n",
              "              1.5458e-02, -3.3222e-02, -2.0949e-02, -1.6199e-02,  5.6103e-03,\n",
              "              1.5774e-03,  7.4854e-03, -1.7007e-03,  3.1759e-02,  2.0259e-03,\n",
              "             -1.7563e-02, -1.0711e-02, -6.3695e-03, -5.3778e-03, -6.7225e-03,\n",
              "              1.8048e-02,  1.0755e-02,  2.3554e-02, -1.1749e-02, -2.2812e-02,\n",
              "             -3.3209e-02,  5.0157e-03,  9.0059e-03,  2.0166e-02, -2.9905e-02,\n",
              "             -1.0361e-02, -1.6952e-02,  1.5587e-02,  3.1821e-02, -1.3162e-02,\n",
              "             -2.5820e-02,  1.5213e-02,  3.3480e-02, -2.9889e-02,  3.4109e-02,\n",
              "              2.5910e-02,  4.7473e-04, -4.1064e-03, -2.2389e-02, -1.4442e-02,\n",
              "             -1.0090e-02,  3.1058e-02,  1.9477e-02, -1.4116e-02, -9.1546e-04,\n",
              "             -2.3294e-02, -3.1001e-02, -3.1080e-03, -2.5900e-02, -2.5833e-03,\n",
              "              2.2948e-02, -2.7404e-02,  2.0938e-03, -1.6314e-02,  7.2268e-03,\n",
              "              2.6303e-02,  5.0551e-03,  4.2268e-02,  9.5783e-03, -2.8062e-02,\n",
              "             -2.3176e-02,  2.9036e-02,  1.8238e-02, -1.1576e-02,  1.0560e-02,\n",
              "             -4.7455e-03,  1.5481e-02, -9.6963e-03,  1.3572e-02, -2.4814e-03,\n",
              "             -9.0755e-03, -1.3248e-02, -1.5156e-02,  2.1175e-02, -4.8873e-03,\n",
              "             -5.7145e-03, -1.7799e-03, -1.4527e-02,  1.5180e-02, -1.5102e-02,\n",
              "             -1.3385e-03,  2.6260e-02, -2.0462e-02,  7.6336e-03, -7.5990e-03,\n",
              "             -1.9020e-02,  1.6421e-02,  1.8357e-02,  2.0197e-02,  2.3549e-02,\n",
              "              2.5910e-02,  2.0020e-02, -9.6136e-03, -3.1384e-02, -1.5160e-02,\n",
              "             -1.5759e-02, -3.2620e-03,  6.7547e-03,  2.7307e-02,  2.8187e-03,\n",
              "              2.3175e-02,  1.0533e-03,  2.4636e-02,  2.4147e-02,  3.2294e-02,\n",
              "             -7.9744e-03,  2.9523e-04, -1.1069e-02, -1.1315e-02,  2.4483e-02,\n",
              "             -1.2246e-03,  2.6948e-02, -3.0309e-02,  2.8522e-02,  2.3758e-02,\n",
              "             -1.0362e-02, -2.2526e-02,  1.2462e-02, -2.3512e-02,  1.0746e-02,\n",
              "              1.2388e-02,  1.1245e-02,  3.4421e-02, -2.9722e-02,  5.8853e-03,\n",
              "              2.0164e-02, -1.1090e-02,  2.0598e-02,  2.9892e-02,  2.7861e-02,\n",
              "              1.8214e-02, -3.3168e-02,  2.1349e-02,  2.5317e-02,  7.7080e-03,\n",
              "             -1.3940e-02,  1.0932e-02,  3.2169e-02,  1.4731e-02,  1.3920e-02,\n",
              "              1.8216e-02, -2.9760e-02, -6.5155e-03, -2.3267e-02, -2.1574e-02,\n",
              "             -2.9390e-02, -1.3546e-02, -1.8404e-02, -6.7295e-03, -3.0841e-03,\n",
              "             -1.2020e-02,  1.5565e-02,  2.4015e-02, -1.5731e-03, -1.4448e-02,\n",
              "              2.4683e-03,  1.2724e-02,  1.4196e-02, -2.6454e-02,  4.2488e-03,\n",
              "              1.9821e-02,  3.3461e-02, -3.3788e-02,  1.6643e-02, -1.8105e-02,\n",
              "             -2.5016e-02, -2.6505e-02,  2.9217e-02, -2.5498e-02, -1.2316e-03,\n",
              "             -2.8791e-02, -5.7491e-03,  5.3344e-03, -2.7318e-02,  2.0590e-02,\n",
              "             -1.7374e-02, -1.4138e-02, -2.3924e-02, -1.1758e-02, -2.9732e-02,\n",
              "              2.7415e-02,  1.3914e-02, -1.1100e-02, -3.3782e-02,  3.4014e-03,\n",
              "              6.6157e-03, -2.6489e-02,  7.3456e-03, -2.3915e-02,  3.3789e-02,\n",
              "              1.3524e-02, -3.9328e-03, -1.1748e-02,  3.4188e-02,  1.0439e-02,\n",
              "              2.5560e-02, -1.7626e-02,  9.7497e-03, -2.9297e-02, -2.2205e-02,\n",
              "             -3.1745e-02,  1.0881e-02,  1.3792e-03,  1.0024e-02, -5.2818e-03,\n",
              "             -2.9460e-02,  1.3753e-02, -2.3731e-02,  3.4108e-02,  9.6065e-03,\n",
              "             -3.6565e-03, -1.3102e-02,  3.2685e-02, -5.8352e-03,  3.0224e-02,\n",
              "             -1.8754e-02, -1.3580e-02,  3.3541e-02, -1.8403e-02,  4.5067e-03,\n",
              "              1.4659e-02,  2.7977e-02,  3.2902e-02, -3.1192e-02, -2.2964e-02,\n",
              "             -2.7153e-02, -1.3610e-02,  2.7034e-02,  2.1715e-02, -3.2977e-02,\n",
              "             -2.3016e-02,  2.9279e-02,  3.2642e-02, -2.6722e-02,  3.3798e-02,\n",
              "             -8.5045e-03,  2.4184e-02,  1.2652e-02,  2.6147e-02, -3.2880e-02,\n",
              "             -1.7590e-02,  8.5500e-03, -3.5172e-02, -2.5039e-02,  2.2207e-02,\n",
              "              2.4922e-02,  1.0636e-02,  2.0075e-02, -3.4757e-03,  8.7404e-03,\n",
              "              7.3545e-03, -2.8163e-02, -1.5295e-03,  6.3591e-03,  6.7782e-03,\n",
              "              2.8061e-02, -2.2219e-02,  3.4350e-02, -3.2700e-02, -1.8234e-02,\n",
              "             -7.4928e-03,  2.1571e-02,  1.2925e-02,  8.5642e-03, -2.5933e-02,\n",
              "             -3.3531e-02,  1.4061e-03, -4.7934e-03, -2.5764e-02,  7.0540e-03,\n",
              "             -2.8791e-02, -1.2600e-02,  1.1983e-02, -2.0331e-02,  3.4518e-02,\n",
              "              1.0508e-02, -2.7421e-02, -3.0243e-02,  2.5270e-02, -1.4965e-02,\n",
              "             -9.4040e-03,  6.1235e-03, -2.2583e-02,  1.1981e-02,  2.5460e-02,\n",
              "             -2.0848e-02, -2.2311e-02,  2.9260e-02, -5.6622e-03,  1.4002e-02,\n",
              "              2.0423e-02,  1.2861e-02,  2.8849e-02, -2.2931e-02,  1.4480e-02,\n",
              "              1.3572e-02, -1.8032e-03, -2.6999e-02, -2.8585e-02,  7.0263e-03,\n",
              "             -2.3643e-02, -5.3279e-03,  1.0095e-03, -1.2842e-02,  3.3901e-02,\n",
              "              3.2881e-02, -1.7047e-02, -2.2871e-02,  9.3174e-03,  3.7522e-03,\n",
              "              1.2902e-02, -1.0092e-02,  1.4570e-02, -2.3742e-02, -1.4597e-02,\n",
              "             -1.8822e-02,  1.4621e-02,  3.2151e-02, -1.3802e-03, -4.8693e-03,\n",
              "              3.0635e-02,  3.0403e-02,  1.9586e-04,  2.0930e-02, -1.6199e-02,\n",
              "              1.8892e-02,  1.2989e-02,  1.7495e-02,  4.1322e-04,  2.0814e-02,\n",
              "              2.1609e-02, -2.1072e-02,  1.9286e-03, -3.3472e-02, -1.5531e-02,\n",
              "             -1.9214e-02, -5.0458e-03,  1.8462e-02, -2.9279e-02, -1.4867e-02,\n",
              "             -9.2819e-03,  2.4885e-02, -1.1206e-02, -1.5413e-02,  3.2363e-02,\n",
              "             -2.4961e-03, -2.0746e-02, -5.5059e-03,  2.2634e-02, -1.6131e-02,\n",
              "              1.1825e-02,  2.9179e-02,  2.8729e-02,  4.8585e-03,  3.2816e-03,\n",
              "              1.5459e-02, -6.1939e-03,  1.3664e-02, -2.5449e-02,  2.6952e-02,\n",
              "              9.3390e-03, -2.0754e-02,  2.9078e-02,  2.4905e-02,  2.3769e-02,\n",
              "              3.1750e-02,  1.5873e-02, -9.7720e-03,  1.8655e-02, -1.0264e-02,\n",
              "              3.0003e-02, -2.5272e-03, -9.0198e-04, -1.9007e-02, -2.9515e-02,\n",
              "             -3.5120e-02,  2.3259e-02,  2.6186e-02, -1.0633e-02, -2.5941e-02,\n",
              "              2.1150e-02, -6.9411e-03,  1.6475e-02,  3.4133e-02, -1.0504e-02,\n",
              "              8.5321e-03, -2.0560e-02,  6.9351e-04, -1.2917e-02, -3.2478e-02,\n",
              "              3.3995e-02, -1.7761e-02,  3.3528e-02, -2.6437e-03,  5.5147e-03,\n",
              "             -8.4137e-03,  1.5894e-02,  1.2610e-02,  3.6185e-02,  1.5313e-02,\n",
              "             -7.2398e-03,  2.6704e-02, -1.9375e-02, -1.4233e-02, -2.3934e-02,\n",
              "              2.6576e-02,  9.2874e-03,  2.6184e-02,  3.1857e-02,  8.4899e-03,\n",
              "             -1.6207e-02,  3.4101e-02, -1.3658e-02,  1.8561e-02,  2.5305e-02,\n",
              "             -2.9620e-02, -3.3935e-02,  1.3013e-02, -2.6737e-02, -3.0735e-02,\n",
              "              8.2631e-03,  3.3142e-02, -2.9789e-02,  2.0920e-02,  2.3857e-02,\n",
              "              1.8649e-03, -1.6386e-02, -1.8337e-02,  3.2168e-02,  6.4672e-03,\n",
              "             -3.3945e-02,  3.2537e-02,  7.6973e-03,  3.2516e-02, -3.6020e-02,\n",
              "              2.0930e-02,  1.5923e-02,  2.8751e-03,  2.7549e-02, -2.7557e-02,\n",
              "             -8.8161e-03,  9.0645e-03, -3.3748e-02,  6.4138e-03, -1.5500e-02,\n",
              "              1.9655e-02,  1.0007e-02,  1.1725e-02,  2.7740e-02,  1.7093e-02,\n",
              "              1.9172e-02,  3.4225e-04,  1.0893e-02, -2.9095e-02,  1.7732e-02,\n",
              "              1.1599e-03,  3.0690e-02, -4.3111e-03, -6.6411e-03, -1.3046e-02,\n",
              "             -2.1193e-02, -3.0463e-02, -3.0362e-02, -2.7848e-02, -1.5176e-02,\n",
              "              2.2938e-02, -2.9290e-02, -1.8476e-02, -3.1795e-02, -2.6471e-02,\n",
              "              2.4947e-02, -9.2098e-03, -2.6129e-02,  8.1122e-03, -2.4157e-02,\n",
              "              5.2439e-03, -1.3634e-02,  3.2073e-02, -2.6019e-02, -2.1678e-02,\n",
              "              1.4371e-02,  1.5681e-02,  2.8017e-02,  2.2392e-02,  3.3008e-02,\n",
              "             -1.5937e-02,  3.5453e-03,  2.2567e-02, -3.4662e-03, -3.0684e-02,\n",
              "              2.3462e-02, -1.2869e-02, -2.3260e-03,  1.5414e-02, -2.1458e-02,\n",
              "              2.8654e-02,  1.1282e-02, -1.9175e-02,  2.3434e-02,  2.0393e-02,\n",
              "              4.2986e-03,  1.5589e-02, -2.7554e-02,  1.3399e-02,  1.3465e-03,\n",
              "             -3.1509e-02, -2.0694e-02,  2.8056e-02,  1.9604e-02, -3.3393e-02,\n",
              "             -7.4919e-03,  7.8771e-03, -9.1133e-03,  2.9048e-02, -8.3608e-03,\n",
              "             -2.2866e-02,  1.1144e-02,  2.2105e-02, -1.7883e-02,  3.4183e-02,\n",
              "              6.4353e-03,  1.7170e-02, -2.4722e-02,  1.0961e-02, -6.4935e-03,\n",
              "             -3.5629e-02,  1.5998e-02,  2.7223e-02,  8.0873e-03, -3.0720e-03,\n",
              "             -2.3196e-02, -2.9841e-02,  5.0699e-03,  1.6152e-02, -2.5272e-02,\n",
              "             -9.9407e-03,  2.0193e-02,  2.7020e-02,  3.1129e-02, -1.1053e-02,\n",
              "              3.3421e-02,  8.0295e-03,  9.1394e-03, -3.1960e-02,  2.3972e-02,\n",
              "             -2.4523e-02, -2.8134e-02,  2.8634e-02,  8.3792e-05,  1.4576e-02,\n",
              "             -4.0129e-03, -1.9215e-02,  5.9058e-03, -3.3595e-02,  1.2683e-02,\n",
              "              1.9479e-03, -1.2704e-02, -9.5612e-04, -2.0384e-02,  2.1143e-02,\n",
              "              3.2302e-03,  2.8852e-03,  2.0751e-02,  1.8086e-02, -3.2168e-02,\n",
              "             -3.2538e-02,  1.4629e-02,  1.2262e-02,  1.8872e-02, -1.2123e-02,\n",
              "             -1.1140e-02, -2.6065e-02,  1.3678e-02, -3.1001e-02,  3.3887e-02,\n",
              "             -6.7562e-03,  1.9986e-02, -1.9770e-02, -1.3885e-02,  2.6495e-02,\n",
              "              2.2948e-02, -2.2302e-03,  1.8649e-02,  2.4000e-02,  2.9374e-03,\n",
              "             -2.9864e-02, -3.2267e-03,  1.5808e-03, -2.8460e-02,  2.2464e-02,\n",
              "             -2.3714e-02,  1.4644e-02, -1.5501e-02, -3.2308e-02, -1.5641e-02,\n",
              "             -1.9569e-02,  1.8521e-02,  1.7954e-02, -2.5147e-02,  1.2594e-02]],\n",
              "           device='cuda:0', requires_grad=True),\n",
              "    Parameter containing:\n",
              "    tensor([-0.0002], device='cuda:0', requires_grad=True)]}],\n",
              " '_warned_capturable_if_run_uncaptured': True,\n",
              " 'step': <function torch.optim.optimizer._use_grad_for_differentiable.<locals>._use_grad(self, *args, **kwargs)>,\n",
              " '_step_count': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 89
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.optim.lr_scheduler.CosineAnnealingLR at 0x7f5a30851f30>"
            ]
          },
          "metadata": {},
          "execution_count": 89
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'T_max': 8,\n",
              " 'eta_min': 0,\n",
              " 'optimizer': Adagrad (\n",
              " Parameter Group 0\n",
              "     differentiable: False\n",
              "     eps: 1e-10\n",
              "     foreach: None\n",
              "     initial_accumulator_value: 0\n",
              "     initial_lr: 0.0004971113311461239\n",
              "     lr: 0.0004243110622021168\n",
              "     lr_decay: 0\n",
              "     maximize: False\n",
              "     weight_decay: 0\n",
              " ),\n",
              " 'base_lrs': [0.0004971113311461239],\n",
              " 'last_epoch': 334,\n",
              " 'verbose': False,\n",
              " '_step_count': 335,\n",
              " '_get_lr_called_within_step': False,\n",
              " '_last_lr': [0.0004243110622021168]}"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "batch_size_loaded\n",
        "n_epochs_loaded\n",
        "loss_name_loaded\n",
        "optimizer_name_loaded\n",
        "scheduler_name_loaded\n",
        "n_units_loaded\n",
        "n_layers_loaded\n",
        "hidden_activation_name_loaded\n",
        "output_activation_name_loaded\n",
        "lr_loaded\n",
        "hidden_activation_loaded\n",
        "output_activation_loaded\n",
        "net_loaded\n",
        "net_loaded.__dict__ # print the subparameters of the network\n",
        "loss_fn_loaded\n",
        "optimizer_loaded\n",
        "optimizer_loaded.__dict__ # print the subparameters of the optimizer\n",
        "scheduler_loaded\n",
        "scheduler_loaded.__dict__ # print the subparameters of the scheduler\n",
        "#train_losses_loaded\n",
        "#test_losses_loaded\n",
        "#train_metrics_loaded\n",
        "#test_metrics_loaded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0ZYeIHU_MQU"
      },
      "source": [
        "Let us verify correct loading of the train and test metrics by visualizing them again but now through the loaded values. Likewise for the train and test losses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "C-nVC2gM_MQU"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"last_expr_or_assign\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "1kTagB7I_MQV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fff9321d-86f9-41ad-9c96-85e685a4cc3c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "# Plotting the losses and metrics for the best network plt.figure(figsize=(12, \n",
        "#plt.subplot(2, 2, 1)\n",
        "#plt.plot(train_losses_loaded, label=\"Train Loss\")\n",
        "#plt.plot(test_losses_loaded, label=\"Test Loss\")\n",
        "#plt.xlabel(\"Epoch\")\n",
        "#plt.ylabel(\"Loss\")\n",
        "#plt.legend()\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot([m[\"l1_norm\"] for m in train_metrics_loaded], label=\"Train L1 Norm\")\n",
        "plt.plot([m[\"l1_norm\"] for m in test_metrics_loaded], label=\"Test L1 Norm\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"L1 Norm\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-3, 1e2)\n",
        "plt.legend()\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot([m[\"linf_norm\"] for m in train_metrics_loaded], label=\"Train Linf Norm\")\n",
        "plt.plot([m[\"linf_norm\"] for m in test_metrics_loaded], label=\"Test Linf Norm\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Linf Norm\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-3, 1e2)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Added plotting MSE of training data and MSE of test data in one plot \n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(train_losses_loaded,label=\"training data\")\n",
        "plt.plot(test_losses_loaded,label=\"test data\")\n",
        "#if scheduler is not None:\n",
        "#    plt.plot([scheduler.get_last_lr()[0] for _ in range(n_epochs)], label=\"Learning rate\") \n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE\")\n",
        "# Added setting the vertical axis to be in powers of 10\n",
        "plt.yscale(\"log\")\n",
        "# Added setting the vertical axis limits to be from 10^-7 to 10^0\n",
        "plt.ylim(1e-7, 1e0)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "uSI39oez_MQV"
      },
      "outputs": [],
      "source": [
        "%config InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nldTtejZ_MQV"
      },
      "source": [
        "## Evaluating the network on arbirary input\n",
        "### Comparing `net` and `net_loaded`\n",
        "\n",
        "We compare `net` and `net_loaded` to confirm correct loading of the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "Me2T3PJs_MQW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95823c2d-b3f4-4a50-b755-2e4f70654f3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor([[ 0.2491, -0.0154, -0.5158],\n",
            "        [-0.2211, -0.5437,  0.2490],\n",
            "        [ 0.3927,  0.5422, -0.3364],\n",
            "        ...,\n",
            "        [-0.5098,  0.0773, -0.4235],\n",
            "        [-0.1741,  0.1158, -0.2311],\n",
            "        [-0.4212,  0.0641,  0.4889]], device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([-0.5349,  0.2851,  0.0322,  ...,  0.1765,  0.5665,  0.2583],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.0084, -0.0205,  0.0311,  ...,  0.0193,  0.0006,  0.0167],\n",
            "        [ 0.0142, -0.0110,  0.0175,  ..., -0.0236, -0.0192, -0.0082],\n",
            "        [ 0.0194, -0.0241,  0.0152,  ...,  0.0226, -0.0299, -0.0003],\n",
            "        ...,\n",
            "        [ 0.0283,  0.0114, -0.0209,  ..., -0.0059, -0.0151,  0.0089],\n",
            "        [ 0.0130,  0.0149, -0.0033,  ..., -0.0183, -0.0262, -0.0195],\n",
            "        [-0.0159,  0.0195,  0.0096,  ...,  0.0212,  0.0006, -0.0147]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([-1.8118e-02,  2.7817e-02, -1.9838e-02,  2.3841e-02,  1.5238e-02,\n",
            "         1.8506e-02, -2.6745e-02, -3.1433e-02, -3.0839e-02,  8.7426e-03,\n",
            "         1.7750e-02, -1.9132e-02,  4.8521e-03, -1.7422e-02,  1.7771e-02,\n",
            "         5.9097e-03,  2.3067e-02, -3.8717e-03, -1.6859e-02, -1.0940e-02,\n",
            "        -6.0716e-03,  9.5921e-03,  8.6659e-03,  2.3914e-02, -8.2484e-03,\n",
            "        -1.1126e-02, -1.3545e-02, -6.9706e-03, -1.4684e-02,  1.0366e-02,\n",
            "        -3.0602e-02, -2.5311e-04, -2.9148e-02,  1.3749e-02,  2.6279e-02,\n",
            "         2.3415e-02,  3.0909e-02,  2.2240e-02, -2.0534e-04, -3.0910e-02,\n",
            "        -2.9967e-02, -1.6821e-02, -1.6823e-02, -6.6773e-03, -1.9761e-02,\n",
            "        -2.6331e-02,  7.7959e-03,  2.1958e-02, -5.8001e-03,  1.4815e-02,\n",
            "        -9.5447e-03,  5.1434e-03,  1.6097e-02,  2.0478e-02, -8.1313e-03,\n",
            "         1.6197e-02,  9.1496e-03,  2.5342e-02,  1.6137e-02, -2.5656e-02,\n",
            "        -1.8861e-02, -3.8307e-03, -5.4298e-03,  8.8476e-03,  6.4336e-03,\n",
            "        -1.3226e-02,  2.3352e-02,  1.9205e-02,  2.7015e-02, -6.4431e-03,\n",
            "        -2.0183e-02, -1.7816e-02, -7.1554e-03, -1.2285e-02, -1.8457e-02,\n",
            "        -2.5042e-02,  2.7155e-02,  2.2412e-02,  2.0798e-02, -1.9102e-02,\n",
            "        -3.1425e-02,  1.6581e-02,  1.9337e-02,  1.2459e-02,  5.6943e-03,\n",
            "         9.0794e-03,  2.9483e-02,  2.5612e-02, -4.5737e-03, -1.3002e-03,\n",
            "        -1.9898e-02, -1.7501e-02, -2.8132e-02,  7.5364e-03, -1.9936e-03,\n",
            "        -2.0954e-02, -7.1631e-04,  1.2868e-02, -2.4564e-02, -2.1866e-02,\n",
            "         2.3896e-02, -1.3259e-02,  1.7876e-02,  2.8075e-02, -1.4995e-02,\n",
            "        -2.7850e-03, -3.0166e-03,  2.8716e-02, -1.0577e-02, -7.1062e-03,\n",
            "         4.9014e-03,  3.2766e-02, -2.0347e-02, -1.8240e-02,  1.7218e-02,\n",
            "         1.1607e-03,  6.5541e-04, -2.8179e-02,  3.2313e-04,  1.8283e-02,\n",
            "        -1.8563e-03, -1.6919e-02,  2.4340e-02,  2.7390e-02,  2.6575e-02,\n",
            "         1.0088e-02, -2.0847e-04,  1.2015e-02,  2.4438e-02,  2.9886e-02,\n",
            "        -1.9280e-02,  5.4056e-03,  2.5907e-02,  6.7308e-03,  2.9087e-03,\n",
            "         8.9887e-03, -1.9327e-02, -2.1997e-02, -3.9602e-03, -1.5551e-02,\n",
            "        -3.0551e-02,  2.5063e-02,  1.5483e-02,  7.8820e-03,  8.6799e-04,\n",
            "        -5.1095e-03,  2.9765e-02, -6.1604e-03,  2.3014e-02,  3.6488e-03,\n",
            "         2.2484e-02,  2.0513e-02, -2.4284e-02, -1.6262e-02, -1.4485e-02,\n",
            "        -1.1174e-02,  1.9979e-02, -1.7574e-02,  2.5884e-02, -1.0794e-02,\n",
            "         9.1818e-03,  1.9142e-02,  1.5617e-02,  1.8375e-02, -1.3344e-02,\n",
            "        -2.0693e-02,  2.1482e-03, -2.1523e-02, -1.3912e-02, -2.3021e-02,\n",
            "        -1.9401e-02, -8.3076e-03,  6.6115e-03,  2.0910e-02,  2.6615e-02,\n",
            "         5.4168e-03, -2.9698e-02,  2.2153e-02, -1.3466e-04,  9.7449e-03,\n",
            "         2.3699e-02, -2.3275e-02, -3.1955e-02,  1.9719e-02, -1.0907e-02,\n",
            "         2.8429e-02,  1.9254e-03, -9.3987e-03, -1.0479e-02, -1.9665e-02,\n",
            "        -1.1272e-02, -2.3966e-02,  2.9274e-02, -2.9660e-02,  1.2494e-02,\n",
            "         8.3643e-03, -4.7802e-03,  2.0754e-02, -6.4386e-03, -1.0606e-03,\n",
            "        -2.8425e-03, -3.5221e-03, -7.8872e-03, -1.7450e-02,  9.3602e-05,\n",
            "         1.6344e-02,  8.8699e-03,  2.6997e-03, -1.0894e-02,  2.2183e-02,\n",
            "        -7.5934e-03, -1.5766e-02,  1.3197e-02, -2.8102e-02,  1.8722e-02,\n",
            "         2.9627e-02, -1.9444e-02, -3.0675e-02, -4.5557e-03,  1.6862e-02,\n",
            "         2.9614e-02,  1.3836e-02, -1.2307e-04,  2.3630e-02,  1.9079e-02,\n",
            "        -2.8872e-02, -2.7364e-02,  2.2552e-02,  9.0262e-03,  1.6525e-02,\n",
            "         1.9304e-02, -1.2763e-02,  1.2107e-02,  1.2066e-02, -2.6160e-02,\n",
            "         1.6968e-02, -1.9698e-02,  1.7990e-02,  6.7113e-03, -2.0761e-02,\n",
            "         9.6888e-03,  1.6073e-03,  3.0049e-02,  8.8395e-03, -1.3277e-02,\n",
            "        -3.0571e-03, -1.3384e-02, -1.6273e-02,  8.0172e-03, -1.5162e-02,\n",
            "         2.5000e-02,  2.3237e-02,  2.6496e-02, -1.6572e-02,  2.7605e-02,\n",
            "         6.9329e-03,  1.2048e-02,  3.6213e-03,  2.1627e-02, -2.1237e-02,\n",
            "         1.9211e-02,  3.8263e-03, -3.1893e-03,  2.5782e-03,  2.7857e-02,\n",
            "        -1.1668e-02,  4.7829e-03,  9.7774e-03, -1.1603e-02, -1.1157e-04,\n",
            "         1.4537e-02, -1.6710e-02,  1.6391e-02,  1.5101e-02, -2.4145e-02,\n",
            "        -1.3569e-02,  1.8777e-02,  8.7727e-03,  1.2942e-02, -1.6551e-02,\n",
            "         2.5341e-02,  2.3251e-02,  3.4124e-03, -1.9081e-02,  6.7153e-03,\n",
            "        -2.1702e-02, -3.2667e-03,  1.2888e-02, -1.8995e-02,  1.3751e-02,\n",
            "        -1.1347e-02,  2.5117e-02, -2.2513e-04,  7.6802e-03,  2.2909e-02,\n",
            "         4.1527e-03,  2.4187e-02, -2.3635e-02, -2.2223e-02, -7.9086e-03,\n",
            "        -8.8565e-03,  1.5338e-02,  2.0760e-02,  1.8024e-02, -1.1566e-02,\n",
            "        -9.7946e-03, -6.1387e-03, -9.8751e-03,  2.0857e-02, -2.5879e-02,\n",
            "        -2.4259e-02, -2.1507e-02, -2.2012e-02,  1.6896e-02,  2.0769e-02,\n",
            "         1.6404e-02,  1.5029e-02,  8.0733e-04, -7.0626e-03,  2.6044e-02,\n",
            "         1.0507e-02,  8.0682e-03, -4.9415e-03,  7.9126e-03,  1.5225e-02,\n",
            "         1.4061e-02,  2.2191e-02,  3.0241e-02,  3.1632e-03,  2.0591e-02,\n",
            "        -2.5114e-02, -5.2946e-03,  2.5521e-02,  3.0078e-02, -9.1289e-03,\n",
            "        -7.5353e-03, -1.3472e-02,  1.0194e-03, -2.5093e-02, -1.1598e-02,\n",
            "         3.9075e-03,  2.8188e-02, -1.6021e-02,  5.4272e-03,  5.4096e-03,\n",
            "        -1.1444e-02, -2.3850e-02,  3.0925e-03, -5.6242e-03, -2.3519e-02,\n",
            "         1.9755e-02,  1.3664e-02,  7.7505e-03,  1.4545e-02, -2.4284e-02,\n",
            "         2.9570e-03,  2.5232e-02,  8.7355e-03, -1.9063e-02, -2.7562e-02,\n",
            "         3.0592e-02,  1.5665e-02, -1.3394e-02, -1.3365e-02, -3.6689e-03,\n",
            "         1.4641e-02, -7.6485e-03, -5.6154e-04,  1.8096e-02, -2.5056e-02,\n",
            "        -2.8741e-03,  8.2832e-03,  1.3541e-02,  1.4654e-02, -2.7058e-02,\n",
            "        -1.1220e-02,  2.3834e-03,  2.9731e-02,  4.0272e-03,  2.3995e-02,\n",
            "        -2.9039e-03, -6.5095e-03,  1.0198e-02, -1.8073e-02,  2.2547e-02,\n",
            "         2.7533e-02, -3.0975e-02, -1.9335e-02, -2.8631e-02,  1.2945e-02,\n",
            "        -2.4050e-03,  2.8984e-02, -2.3017e-02, -2.3147e-03, -1.1843e-02,\n",
            "         2.4194e-02,  5.1528e-03,  6.1314e-03,  4.7550e-03,  1.4418e-02,\n",
            "        -2.1870e-02,  2.3858e-03, -2.2174e-02, -1.7394e-02, -5.2301e-03,\n",
            "        -1.0067e-02, -2.7866e-02, -9.3755e-03,  1.5629e-02,  2.4071e-02,\n",
            "         2.0701e-03,  6.5893e-03,  2.2720e-02,  2.9220e-02, -1.3871e-02,\n",
            "         6.8237e-03,  1.1073e-02, -2.9574e-02,  1.6362e-02, -1.0868e-02,\n",
            "        -7.2435e-03, -2.7287e-02,  1.8707e-02,  2.0534e-02,  2.2766e-02,\n",
            "        -2.7630e-02, -3.0816e-02, -3.9331e-03, -1.7676e-02,  5.8166e-03,\n",
            "        -2.7955e-02,  4.7158e-03, -2.8129e-02,  1.0950e-02,  2.0204e-02,\n",
            "         3.1067e-04,  2.6103e-02, -2.9586e-02,  2.6510e-02,  1.1562e-02,\n",
            "         4.5301e-04,  2.5527e-02,  1.5000e-02, -1.0185e-02, -3.0959e-02,\n",
            "         1.5899e-02,  1.2843e-03, -2.3301e-02, -2.4410e-02, -7.9867e-03,\n",
            "         2.3622e-02, -2.5409e-02, -3.4703e-03,  2.0630e-02,  2.6553e-02,\n",
            "        -6.0895e-03, -2.6139e-02,  7.4240e-03,  1.3288e-02, -1.9744e-02,\n",
            "         6.1175e-03, -8.7067e-03, -8.9110e-04, -1.4531e-02,  2.5095e-02,\n",
            "         7.0041e-03, -5.6488e-03,  2.2695e-02,  1.3583e-02,  1.5536e-02,\n",
            "        -3.6452e-03,  2.5018e-02,  2.3305e-02,  2.6635e-02,  2.4868e-02,\n",
            "         2.6701e-02, -1.3020e-03,  1.8816e-02,  1.4462e-02, -1.9436e-02,\n",
            "        -8.5722e-03,  1.6436e-02, -1.8730e-02, -1.5612e-02,  1.0505e-02,\n",
            "        -7.3045e-03, -9.2654e-03,  4.0530e-03,  2.0449e-02, -2.1521e-02,\n",
            "         2.3009e-02,  4.5904e-03,  1.1671e-02, -2.6559e-02, -6.6724e-03,\n",
            "         1.4720e-02, -1.3155e-03,  1.0224e-02, -4.8472e-03,  1.0899e-02,\n",
            "        -8.0005e-03,  2.7024e-02, -2.3387e-02, -1.5996e-02,  4.2850e-03,\n",
            "         1.7434e-02, -2.6449e-02,  1.9437e-02, -2.9781e-02, -3.2994e-04,\n",
            "         2.5557e-02, -8.4039e-03, -2.3315e-04,  1.4014e-03, -2.9112e-02,\n",
            "        -1.8403e-02, -2.4355e-02, -2.5911e-02, -1.8838e-02,  2.5219e-02,\n",
            "         2.1696e-02, -1.4893e-02, -2.2284e-02,  2.9499e-02,  1.5011e-03,\n",
            "         1.7513e-03, -6.8941e-03,  2.2283e-02,  2.7758e-02, -1.8171e-02,\n",
            "         5.4178e-03, -7.2049e-03,  1.2354e-02,  2.1812e-02,  1.7427e-02,\n",
            "         2.4205e-02,  1.4981e-02, -8.2413e-03,  2.2317e-02,  1.4127e-02,\n",
            "        -1.2634e-02,  2.5160e-02, -2.4898e-03, -4.7699e-03,  1.5621e-02,\n",
            "         3.3414e-03,  1.4538e-03, -2.3740e-02,  1.9958e-02,  1.7023e-02,\n",
            "        -3.3535e-04, -2.1802e-03, -2.3860e-02, -1.5098e-02, -2.0221e-02,\n",
            "         2.5401e-02, -9.3477e-03,  2.0335e-02,  7.3290e-03, -1.6413e-02,\n",
            "        -2.9674e-02, -1.7253e-02,  1.4320e-02,  7.6620e-03,  2.7278e-02,\n",
            "         2.3301e-02, -2.3178e-02, -2.4300e-02,  2.4014e-02,  3.4079e-03,\n",
            "         4.7119e-03,  2.5677e-02,  8.8305e-03,  1.0131e-02,  2.0829e-02,\n",
            "         2.0087e-02, -1.5111e-02,  1.7677e-03,  2.2972e-02,  1.2750e-02,\n",
            "        -1.3178e-02,  3.4779e-03, -2.7756e-02,  3.8141e-03, -1.2839e-02,\n",
            "         8.6578e-03, -7.6490e-03,  1.2918e-03,  2.6574e-02,  2.1223e-02,\n",
            "        -2.6139e-02,  1.0425e-02, -1.1013e-02, -1.7062e-02, -1.2136e-02,\n",
            "        -1.7372e-02,  1.8214e-02,  2.2429e-02,  7.1559e-03,  1.2937e-02,\n",
            "         2.4048e-02,  4.0526e-03,  1.3215e-03,  2.3807e-02,  1.6253e-02,\n",
            "         2.7084e-02,  2.2869e-02, -1.0692e-02,  2.6442e-02, -2.7687e-02,\n",
            "        -1.8129e-03, -1.2693e-02,  2.9193e-02, -2.2183e-02, -1.2941e-02,\n",
            "        -2.6905e-02, -1.0474e-02, -1.1379e-02, -2.8667e-02,  1.5728e-02,\n",
            "        -1.1269e-04,  2.4486e-02,  1.5575e-02,  6.3477e-03, -3.8496e-03,\n",
            "         2.7682e-02, -1.4838e-02,  4.0568e-03, -3.0306e-02,  1.0135e-02,\n",
            "         1.4527e-02,  2.4086e-02, -1.3246e-03, -5.5541e-03, -3.0655e-03,\n",
            "        -7.8492e-03,  7.9160e-03, -5.7563e-03, -9.3403e-03,  2.6658e-02,\n",
            "         2.7359e-02,  1.7629e-02,  6.9327e-03, -2.7225e-02, -1.3017e-02,\n",
            "        -2.7790e-02, -1.8800e-02,  2.6362e-02,  6.4411e-03, -2.3820e-02,\n",
            "        -3.1337e-02, -1.7480e-02, -1.4079e-03,  1.2087e-02, -7.3949e-03,\n",
            "         1.5092e-02,  1.1141e-02, -2.0506e-02, -9.5732e-03, -4.9644e-03,\n",
            "        -2.8180e-02,  6.0945e-04,  1.5499e-02, -1.3375e-03, -1.7667e-02,\n",
            "        -4.7949e-03,  1.5820e-02, -2.7158e-02, -1.8429e-02, -2.6913e-02,\n",
            "         9.4847e-03,  1.2014e-02,  2.3916e-02, -1.9053e-02, -1.4085e-02,\n",
            "         5.2371e-03, -4.7322e-03, -2.7768e-02,  2.3044e-02,  1.7862e-02,\n",
            "         5.1652e-03,  5.7866e-03,  1.5405e-02,  3.0862e-02,  7.4468e-03,\n",
            "        -2.8660e-02,  2.1098e-02, -1.2801e-02, -9.1947e-03, -1.4526e-02,\n",
            "         3.5940e-03,  3.4609e-03, -1.5262e-03,  6.1190e-03,  1.3020e-02,\n",
            "         1.8133e-02, -3.0893e-03,  9.3023e-03, -4.3035e-03, -5.8621e-03,\n",
            "        -4.9882e-03,  1.9467e-02,  5.0334e-03,  1.7634e-02, -2.4214e-03,\n",
            "         1.5824e-02,  1.1113e-02,  2.7371e-02,  1.7201e-03,  7.7691e-04,\n",
            "         1.0473e-02,  7.1966e-03, -3.5944e-03,  2.9299e-02,  1.6452e-03,\n",
            "        -1.4841e-02, -5.2132e-03,  1.7415e-02, -1.2846e-02,  1.4125e-02,\n",
            "         1.7135e-02, -1.5827e-02,  1.8330e-02, -1.5705e-02, -4.1303e-03,\n",
            "         2.9470e-02,  1.3214e-03, -3.0864e-02,  2.2606e-02, -1.4998e-02,\n",
            "         1.7664e-02, -2.5116e-02,  5.5415e-03,  1.9734e-02, -2.0948e-02,\n",
            "        -1.0499e-02, -4.4634e-03, -2.3454e-03, -2.5811e-02, -1.7683e-02,\n",
            "        -2.5238e-02,  2.9903e-02, -1.3415e-02, -2.4076e-02,  8.0738e-03,\n",
            "        -6.4178e-03,  2.6694e-02,  2.4435e-02, -2.0672e-02,  2.8067e-03,\n",
            "        -1.1542e-02,  2.3122e-02, -2.7174e-02,  7.0581e-03,  4.4595e-03,\n",
            "        -2.8221e-02, -1.0331e-02, -1.1479e-02, -2.8966e-02, -2.0666e-02,\n",
            "         1.1246e-03,  2.5485e-02,  1.9940e-02, -2.4241e-03, -2.7133e-02,\n",
            "        -6.4601e-05, -1.4391e-02,  3.9006e-03,  1.9059e-02,  1.4498e-02,\n",
            "         1.6925e-02, -2.5129e-02,  6.7160e-03,  1.6520e-02, -1.1597e-02,\n",
            "        -2.5776e-03, -1.0697e-02, -1.8838e-02, -1.8677e-03,  1.9182e-04,\n",
            "         1.1723e-02, -1.2418e-03, -1.3915e-02, -1.5141e-02,  2.1583e-02,\n",
            "        -3.6220e-03,  4.7592e-03,  6.0094e-03,  1.5773e-02,  1.8182e-02,\n",
            "         2.8263e-02, -3.0791e-02, -2.1032e-02,  3.9093e-03, -2.7280e-02,\n",
            "         2.0330e-02, -2.2542e-02, -2.1506e-02,  1.1711e-02, -1.7368e-02,\n",
            "        -1.5672e-02, -9.6132e-03, -2.1427e-02, -1.0420e-02,  2.0561e-02,\n",
            "         2.0470e-02, -8.8239e-03, -5.3848e-04, -1.3559e-02,  1.4518e-02,\n",
            "         1.5428e-02,  2.5589e-02,  8.3284e-03,  1.7063e-02,  1.7706e-02,\n",
            "        -1.9177e-02, -5.2429e-03,  8.2786e-03, -1.4832e-02, -1.7999e-02,\n",
            "        -2.3194e-02,  4.3262e-03, -5.3708e-04,  5.0061e-03, -9.5460e-03,\n",
            "         2.6492e-02,  2.0044e-02, -1.9577e-02, -2.6207e-02, -1.7423e-02,\n",
            "        -2.8654e-02, -2.3089e-02,  2.0345e-02, -1.6634e-02,  1.5337e-02,\n",
            "        -1.7777e-02, -2.9554e-02,  1.7709e-02, -1.8214e-02, -2.0106e-03,\n",
            "        -2.8342e-02,  1.1914e-02, -2.4641e-02, -1.3474e-02, -3.6725e-03,\n",
            "         3.8342e-03,  4.7166e-03,  2.7441e-02,  2.7444e-02,  3.5137e-03,\n",
            "         2.6906e-02, -2.0687e-02,  2.6347e-02, -2.4380e-02, -2.8713e-03,\n",
            "         2.2155e-02, -2.1435e-02, -8.5123e-03,  3.0116e-02,  1.0453e-02,\n",
            "        -2.2873e-02, -7.3061e-03,  1.3505e-02, -1.0680e-02,  1.5951e-02,\n",
            "        -3.5714e-03, -1.0233e-02,  1.5326e-02, -5.9924e-03,  1.9178e-02,\n",
            "        -1.8455e-02, -1.6403e-02, -7.0622e-03,  2.1060e-02, -2.7514e-02,\n",
            "        -1.5754e-02, -1.6234e-02,  2.9609e-02,  1.9457e-02, -1.6782e-02,\n",
            "         2.5768e-02, -2.2089e-02,  1.7510e-02, -1.5594e-02,  1.6076e-02,\n",
            "         3.8558e-03,  9.0380e-03, -2.5038e-02, -2.2602e-02, -1.9117e-02,\n",
            "        -8.2552e-03, -5.6334e-03, -2.2476e-02,  2.0380e-02,  1.2791e-02,\n",
            "        -2.8848e-02,  2.6836e-02,  1.6533e-02, -2.6037e-02,  3.0777e-03,\n",
            "        -2.7272e-02, -2.8729e-02,  1.1994e-02, -2.3057e-02,  2.3210e-02,\n",
            "        -2.6339e-02,  6.6061e-03,  1.9914e-02, -2.7028e-02,  2.1034e-03,\n",
            "        -2.1373e-02,  6.3881e-04,  2.5240e-02, -3.3166e-03], device='cuda:0',\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[-0.0011,  0.0150, -0.0290,  ...,  0.0266, -0.0234, -0.0158],\n",
            "        [ 0.0112, -0.0044,  0.0054,  ..., -0.0279, -0.0017, -0.0185],\n",
            "        [-0.0184, -0.0007, -0.0313,  ..., -0.0153, -0.0048,  0.0041],\n",
            "        ...,\n",
            "        [ 0.0244, -0.0298,  0.0153,  ...,  0.0161, -0.0074,  0.0194],\n",
            "        [-0.0193,  0.0254,  0.0143,  ..., -0.0110,  0.0213, -0.0299],\n",
            "        [-0.0099, -0.0057,  0.0059,  ..., -0.0267,  0.0158,  0.0241]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([ 1.8927e-02,  5.1221e-03, -6.3455e-03,  6.3523e-03, -1.8603e-03,\n",
            "        -2.5010e-02,  3.1347e-02,  1.4582e-03, -9.5578e-03, -1.6248e-03,\n",
            "        -7.8893e-03,  2.3840e-02,  2.9724e-02,  5.5908e-03,  9.4720e-04,\n",
            "         2.1751e-02,  8.3106e-04,  1.5326e-02,  5.0339e-03, -2.9501e-02,\n",
            "        -3.2940e-02,  1.7357e-02, -3.2578e-02,  8.6837e-03, -3.0859e-02,\n",
            "        -1.6332e-02, -6.6037e-03,  2.7375e-04, -1.1204e-02, -1.3180e-02,\n",
            "         2.9279e-03,  3.3989e-03,  2.4262e-02, -3.1681e-02, -1.0913e-02,\n",
            "        -1.0458e-02,  1.6959e-02, -5.0488e-03, -3.0351e-02, -2.1643e-03,\n",
            "        -1.4353e-02, -2.6382e-02,  3.0768e-02, -2.1259e-02,  1.1666e-02,\n",
            "         1.9044e-02,  1.0232e-02,  2.8427e-02, -2.2469e-02, -2.1643e-02,\n",
            "         2.2096e-02,  7.8680e-03,  1.8666e-02,  2.9698e-02,  2.2635e-02,\n",
            "         1.7691e-02, -2.9671e-02,  4.9964e-03, -2.9822e-02,  7.9353e-03,\n",
            "         9.7695e-03,  2.3912e-02, -3.2366e-02, -1.5210e-02,  6.0818e-03,\n",
            "         1.8792e-02,  2.0241e-02,  5.5982e-03,  2.9330e-04, -2.4334e-02,\n",
            "         6.4341e-03,  4.3976e-03, -1.1488e-02,  1.1159e-02,  1.0546e-02,\n",
            "         1.3750e-02, -3.1680e-02,  1.7578e-02, -1.5640e-02, -2.1870e-02,\n",
            "        -3.0378e-02,  2.7670e-02, -5.1787e-03, -1.8056e-03,  2.0900e-02,\n",
            "         4.4609e-03, -1.4667e-02,  6.0511e-03, -1.8227e-02,  3.0482e-02,\n",
            "         2.5518e-02,  1.4948e-02, -9.2702e-03,  1.9513e-02, -1.9906e-02,\n",
            "         3.1905e-02, -2.5397e-02,  5.7974e-03,  5.7382e-04,  2.9320e-02,\n",
            "         3.2204e-02,  1.2893e-02,  2.1015e-02,  2.2990e-02, -6.4969e-05,\n",
            "         5.8146e-03, -2.2263e-02, -1.9236e-02,  2.1229e-02, -2.8150e-02,\n",
            "         1.1730e-02,  2.3219e-02, -2.1322e-02, -1.6256e-02, -1.2405e-02,\n",
            "        -2.9742e-02,  9.3833e-03, -2.8342e-02, -1.6681e-02,  2.5492e-02,\n",
            "         2.2188e-02,  4.1535e-04, -3.1688e-02,  1.0771e-02, -2.0049e-02,\n",
            "         5.1351e-03, -1.1425e-02, -3.0886e-02,  2.9006e-02,  1.8110e-02,\n",
            "         2.2185e-02,  7.8402e-03,  1.2789e-02,  2.4833e-02,  2.8787e-02,\n",
            "        -4.6257e-03,  1.6405e-02,  1.2906e-02,  2.3993e-02,  1.5517e-02,\n",
            "        -2.6469e-02, -2.5906e-02,  7.7334e-03, -2.6463e-02,  1.0474e-03,\n",
            "        -1.9411e-02,  1.7606e-02,  1.4320e-02, -7.9357e-03, -2.2951e-03,\n",
            "         6.3760e-03, -2.6427e-02,  1.7020e-02, -1.0167e-02, -3.2292e-02,\n",
            "         7.6268e-03, -1.0041e-02,  1.6680e-02, -2.9899e-02, -2.0065e-02,\n",
            "        -3.2987e-02, -1.9525e-02, -9.3159e-03, -9.7834e-03, -1.9171e-02,\n",
            "         1.8495e-02,  9.2168e-03,  5.5167e-03, -1.7018e-02,  2.4381e-02,\n",
            "         7.0030e-03,  3.2638e-02, -2.6448e-02,  1.4497e-02, -9.7048e-03,\n",
            "         2.5006e-02, -1.1184e-02, -1.4239e-03,  1.3675e-02,  3.1494e-02,\n",
            "         1.3383e-02,  2.9359e-02,  3.0570e-02,  2.7051e-02, -2.2908e-04,\n",
            "        -2.0143e-02, -6.7779e-03,  2.5610e-02,  5.8868e-03,  1.5429e-02,\n",
            "        -7.6164e-03,  2.4390e-02,  2.7092e-02,  9.2754e-03,  2.9862e-02,\n",
            "         3.0693e-02,  1.4166e-02, -1.7550e-02,  2.6814e-02,  1.4938e-02,\n",
            "        -3.0595e-02,  2.6268e-03, -5.6717e-04, -1.0284e-02,  2.9573e-03,\n",
            "         2.4493e-02, -1.2793e-02, -8.2947e-03,  8.6051e-03,  1.5251e-02,\n",
            "        -1.2551e-03,  1.8370e-02,  2.0194e-02,  3.1704e-02,  1.6271e-02,\n",
            "         5.7648e-03, -1.1754e-02, -2.8936e-02,  1.3602e-02, -2.0230e-02,\n",
            "         2.3500e-02, -3.4909e-03,  6.8151e-03, -3.9713e-03, -1.9410e-02,\n",
            "        -2.2285e-02, -8.1350e-03, -5.3547e-03, -3.1504e-02, -1.6675e-03,\n",
            "         2.8465e-02,  1.1023e-02, -1.4967e-02,  3.3372e-02, -1.2001e-02,\n",
            "         2.3860e-02, -3.2786e-02,  6.5018e-03,  4.8840e-03, -2.9866e-02,\n",
            "        -3.1997e-02,  4.7691e-03, -5.3828e-03, -1.9831e-02, -1.1306e-03,\n",
            "         7.1287e-04, -2.1314e-02,  2.2658e-02,  1.2472e-03, -1.7829e-02,\n",
            "        -2.4073e-02,  1.2982e-04,  1.8906e-02,  2.6881e-02, -1.9781e-02,\n",
            "         2.9587e-02,  2.0501e-02, -3.0865e-02, -2.2421e-02, -1.5331e-02,\n",
            "        -5.9236e-03, -2.9065e-02,  1.8332e-02, -1.1057e-02,  3.2437e-03,\n",
            "         3.0179e-02,  1.3030e-02,  2.8039e-03, -1.1443e-02, -1.6171e-02,\n",
            "        -7.3875e-03,  2.9498e-02, -2.8567e-02, -1.7148e-02,  7.7138e-03,\n",
            "        -9.3358e-03,  6.1467e-03,  2.0958e-02, -5.4689e-03,  2.9116e-02,\n",
            "        -1.0117e-02,  1.8468e-02,  1.9251e-02,  8.7718e-04,  8.4298e-03,\n",
            "         2.4617e-02,  2.0735e-02,  3.2348e-02,  1.5459e-02, -3.0858e-02,\n",
            "         2.2610e-02,  1.2571e-02,  1.3680e-02,  7.0614e-03,  1.1473e-02,\n",
            "         6.4096e-03, -2.4560e-02,  2.9356e-02, -5.1656e-03,  8.0530e-03,\n",
            "        -2.3912e-02,  1.1003e-02,  2.2139e-04, -2.6259e-02, -3.0709e-02,\n",
            "        -1.8203e-02,  1.7659e-02, -5.8809e-03, -1.2096e-02, -2.5559e-02,\n",
            "        -2.1161e-03, -2.4080e-03, -1.4081e-02,  2.3075e-02, -1.3532e-02,\n",
            "         1.4881e-02, -2.5931e-02, -1.9693e-02,  1.1995e-02,  1.7738e-02,\n",
            "         1.7832e-02,  1.5712e-02, -1.7824e-02,  1.2335e-02,  2.7940e-02,\n",
            "         2.9430e-02, -6.0981e-03, -3.9327e-03,  1.8676e-04, -1.6828e-02,\n",
            "         9.6875e-03, -1.0320e-02, -1.5209e-02,  1.7820e-02,  3.7072e-03,\n",
            "        -6.9283e-03,  2.4926e-02, -2.9640e-02,  1.6759e-02, -2.8035e-02,\n",
            "        -2.9126e-02,  4.4247e-04,  2.5255e-02,  3.2039e-02, -7.6639e-04,\n",
            "         1.8959e-02,  1.8422e-02,  1.0468e-02, -1.5606e-02, -1.7895e-02,\n",
            "         2.0422e-04,  2.6132e-02,  1.9343e-02, -2.0902e-02, -3.1470e-02,\n",
            "        -2.9382e-02,  5.9990e-03,  5.1359e-03,  2.8704e-02, -3.3020e-02,\n",
            "        -8.3787e-04,  1.9185e-02,  1.3709e-02,  2.6157e-03,  2.7711e-02,\n",
            "         4.4097e-03, -2.6750e-02,  1.2574e-02, -2.2876e-02,  4.3015e-03,\n",
            "        -1.5464e-02, -3.0670e-02, -6.9692e-03,  6.6724e-03,  9.0484e-03,\n",
            "         7.7590e-03, -2.2830e-02,  3.1140e-02, -2.9158e-02, -2.5565e-02,\n",
            "         1.3649e-02,  2.1585e-02, -2.5726e-02,  7.5124e-03,  2.3015e-02,\n",
            "         2.2905e-02, -1.2637e-02, -3.0784e-02, -2.9450e-02, -8.2776e-03,\n",
            "         1.9575e-02, -1.1717e-03,  1.4080e-02, -3.2793e-02,  1.0969e-02,\n",
            "        -5.7936e-03,  1.6334e-02,  2.6089e-02,  1.5582e-02, -1.8675e-02,\n",
            "        -1.1825e-02, -3.9736e-04,  2.4679e-02, -2.8159e-02,  6.3136e-03,\n",
            "        -1.2345e-02,  7.7562e-03,  1.1998e-02,  1.5554e-03, -9.4891e-03,\n",
            "        -2.0778e-02, -4.1658e-03, -1.0279e-02, -1.5105e-02,  1.2113e-02,\n",
            "         2.0310e-02, -7.7104e-03, -9.6433e-03,  2.1543e-02, -9.0736e-03,\n",
            "        -1.2576e-02,  2.6052e-02,  2.8745e-02,  4.8979e-03,  1.9407e-02,\n",
            "         2.3075e-02,  2.6245e-02,  1.4625e-03,  2.1202e-02, -2.5316e-02,\n",
            "        -3.2156e-02, -2.1280e-02,  8.5685e-03, -1.0954e-02,  3.2335e-02,\n",
            "        -3.0746e-02,  1.8819e-04, -2.8861e-02, -1.3456e-02, -5.0047e-03,\n",
            "        -2.7469e-02,  3.0849e-02,  2.4916e-02, -9.5976e-03, -2.8835e-02,\n",
            "         3.0585e-02,  2.0288e-02,  3.1159e-02, -3.2167e-02,  1.6660e-02,\n",
            "        -8.6249e-03,  1.7406e-02, -3.0004e-02,  2.6126e-03, -3.0445e-02,\n",
            "         1.4691e-02,  2.7817e-02,  2.0409e-02, -1.1569e-02,  1.8775e-02,\n",
            "         3.3508e-03,  6.9867e-04, -9.1174e-03, -5.3175e-03,  2.7795e-02,\n",
            "         7.8157e-03,  3.0626e-02, -2.3580e-03,  3.1309e-03,  1.0328e-02,\n",
            "        -6.8944e-03,  2.8782e-02, -2.2018e-02,  2.8706e-02, -9.1219e-03,\n",
            "        -1.1259e-02,  3.0469e-02,  2.2541e-02, -8.4406e-03, -1.6104e-02,\n",
            "        -2.9304e-03, -4.8423e-03,  1.8177e-02,  1.6771e-03,  2.2772e-02,\n",
            "        -1.3932e-02,  1.4532e-02,  3.2265e-02, -1.6483e-02,  1.0695e-02,\n",
            "         2.1712e-02,  5.8775e-03,  2.9231e-02, -1.5308e-02, -3.6297e-03,\n",
            "        -3.9448e-03, -9.0778e-03,  2.7032e-02, -4.2240e-04,  1.0461e-02,\n",
            "        -5.2272e-03,  2.4845e-02,  3.2272e-02, -2.5079e-02, -9.7401e-03,\n",
            "         3.3530e-02,  2.9799e-02, -1.8699e-02, -2.1961e-02, -9.2221e-03,\n",
            "         2.1172e-03,  3.3247e-04,  3.0518e-02, -1.3045e-02, -1.3759e-02,\n",
            "         2.7634e-02,  4.0073e-03, -7.2399e-03, -4.2636e-03, -3.4953e-02,\n",
            "         1.8751e-02,  2.5191e-02, -2.7228e-03,  2.9692e-02,  3.1514e-02,\n",
            "         1.6832e-02, -2.5429e-02, -1.2584e-02, -1.3050e-02, -2.6537e-02,\n",
            "         2.7773e-02, -3.2396e-03,  2.4540e-02, -2.1761e-02, -8.4514e-03,\n",
            "         7.5165e-03,  2.1400e-02, -1.6529e-02,  2.8550e-02, -1.8732e-02,\n",
            "         2.0587e-02, -1.6782e-02, -7.5184e-04,  1.0302e-02,  1.7501e-02,\n",
            "         1.6918e-02, -2.1261e-02, -6.4355e-03,  1.7775e-02, -1.1328e-02,\n",
            "        -3.1278e-02,  2.0045e-02, -2.1787e-02, -2.2428e-03,  2.9343e-02,\n",
            "        -9.3957e-03,  8.0241e-03,  2.8974e-02, -1.5669e-02, -7.3951e-03,\n",
            "         2.7528e-02,  1.1322e-02,  1.6284e-02,  2.8990e-02, -3.0958e-03,\n",
            "        -2.4017e-02, -9.3226e-03,  1.8039e-02, -1.0251e-02,  8.9491e-03,\n",
            "         2.3754e-03, -3.9807e-03, -4.4466e-03,  1.3898e-02,  1.5712e-03,\n",
            "        -2.0080e-02, -3.2700e-02,  2.7509e-02, -3.1064e-02, -1.8094e-02,\n",
            "        -2.4836e-03,  1.7167e-02,  2.7292e-02,  2.3941e-02,  4.8119e-03,\n",
            "        -1.6045e-02, -5.9072e-03, -2.7343e-02,  2.8222e-04,  9.8365e-03,\n",
            "         1.7415e-02,  1.8912e-02, -1.3324e-02,  1.6349e-02, -2.8214e-02,\n",
            "         8.4565e-03,  1.1052e-02, -5.7405e-03,  1.1273e-02, -2.1949e-03,\n",
            "        -1.7726e-02,  1.8747e-02,  2.4196e-02, -1.0137e-02, -2.3812e-02,\n",
            "        -1.2278e-02, -1.4339e-02, -6.2420e-03, -2.5390e-02, -2.3117e-03,\n",
            "         1.2715e-02,  2.0863e-02,  1.3462e-02, -4.2703e-04, -3.9571e-03,\n",
            "        -5.0714e-03,  1.9301e-02, -9.4622e-04,  1.5198e-02, -2.4975e-02,\n",
            "         1.0043e-02,  8.1847e-03, -2.6837e-02, -1.2704e-02, -2.5560e-02,\n",
            "         2.3369e-02, -9.2318e-03,  2.1868e-02,  2.1909e-02, -3.5216e-03,\n",
            "         7.0945e-03, -1.0589e-02,  1.1031e-02,  7.1564e-03,  1.2713e-02,\n",
            "        -1.3993e-02, -3.6386e-03,  1.5999e-02, -1.3954e-02, -1.5165e-02,\n",
            "        -1.9999e-02,  3.3830e-03, -2.6046e-02,  5.2853e-03, -1.1573e-02,\n",
            "         2.3597e-02, -4.6195e-03, -2.7684e-02, -2.7404e-02,  1.2742e-02,\n",
            "         1.6137e-02, -9.1372e-03,  2.7146e-02,  2.2029e-02,  2.6347e-02,\n",
            "        -2.5641e-02,  2.2225e-02,  4.2598e-03,  2.7160e-02,  1.3085e-02,\n",
            "        -1.1619e-02,  3.2142e-03,  2.4655e-02,  1.5217e-02, -1.3173e-02,\n",
            "         1.9245e-02,  1.2351e-03, -7.5416e-03, -7.0327e-03,  3.0604e-02,\n",
            "        -1.1170e-02,  1.2449e-02, -3.2878e-02,  3.4741e-02,  1.1468e-02,\n",
            "        -3.6181e-03,  2.9950e-02, -2.9271e-02, -1.0523e-02,  3.9430e-03,\n",
            "        -3.2741e-02,  5.2178e-03,  2.3637e-02, -4.1829e-03,  2.5731e-02,\n",
            "         2.7388e-02,  1.0659e-02,  9.0068e-03,  1.9524e-02,  2.6013e-02,\n",
            "        -3.1591e-02,  3.1329e-02,  2.3098e-02, -2.4066e-02, -2.0023e-02,\n",
            "         8.8087e-03, -1.4433e-03, -2.3289e-02,  2.8438e-02,  3.1747e-02,\n",
            "        -1.3471e-02,  3.1723e-02, -4.8878e-04,  1.8693e-02,  2.4573e-02,\n",
            "        -6.0951e-04, -2.8182e-02,  6.6607e-04, -2.1510e-02, -2.9051e-02,\n",
            "        -1.4550e-02,  1.4416e-02, -2.5855e-02,  1.1109e-02, -1.3412e-02,\n",
            "        -1.8272e-02, -6.6001e-03,  7.7085e-03,  2.9565e-02,  5.0854e-04,\n",
            "        -1.5329e-02,  1.9919e-02,  9.7481e-04, -2.7359e-02,  6.5234e-03,\n",
            "        -1.8656e-02,  3.1190e-02, -3.2748e-02, -1.9377e-02,  1.2248e-02,\n",
            "         3.0604e-02,  3.0263e-02, -1.4544e-02, -4.6499e-03,  4.8474e-03,\n",
            "        -1.9061e-02, -1.1660e-02,  1.2246e-02,  2.6904e-02, -1.2875e-02,\n",
            "        -3.2081e-02, -3.1931e-02,  2.0139e-03,  1.3254e-02,  1.8403e-02,\n",
            "        -4.1436e-03,  6.0135e-03,  2.5694e-02,  2.0937e-05,  1.2095e-02,\n",
            "         9.4952e-04,  2.0200e-02, -1.9913e-02,  7.6790e-03,  2.6354e-02,\n",
            "         2.7277e-02, -3.3127e-02, -1.6153e-03, -9.6690e-03, -5.7313e-03,\n",
            "        -2.1667e-02,  2.5687e-02,  2.3512e-02, -1.5623e-02,  5.3760e-03,\n",
            "        -2.1109e-02, -1.6786e-02, -1.8331e-02,  2.5909e-02, -1.7662e-02,\n",
            "        -2.8497e-02, -3.4930e-03,  4.2250e-04,  6.5064e-04,  2.0336e-03,\n",
            "        -7.1874e-03,  2.3845e-02, -1.9640e-02, -2.6436e-02,  2.9493e-02,\n",
            "         1.3693e-02,  1.6125e-02, -1.0303e-02,  7.4460e-03, -1.3200e-02,\n",
            "        -2.2474e-02, -2.1977e-02, -9.9615e-03,  5.5570e-03,  1.7066e-02,\n",
            "         2.4319e-02,  1.6411e-02, -3.0866e-02,  2.7912e-02,  2.3383e-02,\n",
            "        -7.7102e-03,  2.4872e-02, -2.1051e-02,  1.3724e-02,  1.7152e-02,\n",
            "        -7.3422e-03,  2.6152e-02, -3.0895e-02,  6.2165e-03, -2.0641e-02,\n",
            "        -1.6931e-03,  7.7407e-03,  2.2044e-03,  2.1164e-04,  7.5273e-04,\n",
            "        -2.1238e-02,  1.7640e-02, -1.3325e-02,  1.6020e-02,  5.1311e-03,\n",
            "         1.6632e-02,  2.4890e-02,  2.0036e-02,  1.3413e-02,  1.3770e-02,\n",
            "         2.1385e-02, -3.6739e-03,  3.9765e-03,  2.8669e-02, -1.7709e-02,\n",
            "        -4.3481e-03,  2.7301e-02, -3.0285e-02,  2.4981e-02,  2.4199e-02,\n",
            "         1.2630e-02,  1.4794e-02,  5.6664e-03,  1.3399e-02,  1.5550e-02,\n",
            "         2.5306e-02, -2.5831e-02, -7.6578e-03,  1.7840e-02, -1.5370e-02],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([[ 1.9164e-02, -2.6892e-02, -2.5727e-02,  2.1801e-02,  1.2254e-02,\n",
            "         -1.7262e-02, -1.2724e-02,  1.0954e-02, -2.5102e-02, -1.4763e-02,\n",
            "          1.8938e-02, -1.0669e-02, -2.6465e-02,  3.5014e-03,  1.7949e-02,\n",
            "          7.8463e-04,  1.0639e-02,  3.1837e-02, -2.9686e-02, -3.3847e-02,\n",
            "          6.3855e-03, -1.0299e-02, -2.0043e-02, -9.9410e-03, -2.8669e-02,\n",
            "          1.8348e-02,  3.2789e-02, -1.0123e-02,  2.8397e-02,  1.3679e-02,\n",
            "         -1.4750e-02,  1.2294e-02,  1.3386e-02, -3.4751e-03, -1.8019e-02,\n",
            "          2.7829e-02,  3.4167e-02,  3.1186e-02,  3.3943e-02, -1.4837e-02,\n",
            "          2.8412e-02, -9.5281e-03, -3.3718e-02,  1.0498e-02, -3.6541e-02,\n",
            "          1.8140e-02, -3.9761e-03,  7.0445e-03,  2.0761e-02,  2.1547e-02,\n",
            "         -3.0524e-02, -3.1369e-02, -6.6387e-04, -3.5795e-02,  3.2768e-02,\n",
            "         -7.8146e-03,  2.1132e-02, -2.8955e-02,  1.4915e-02,  1.0655e-02,\n",
            "         -5.3673e-03, -1.1869e-02, -1.5724e-02, -1.0130e-02,  1.3521e-02,\n",
            "         -2.4001e-02,  2.9218e-03,  1.0160e-02, -1.2710e-02,  3.6275e-03,\n",
            "          7.5566e-03, -4.7611e-03, -1.9959e-02, -1.3781e-02,  2.5636e-02,\n",
            "          1.5119e-03,  3.1328e-02,  7.7680e-03,  3.4220e-02, -1.3156e-02,\n",
            "         -3.8684e-03,  2.2372e-02,  3.2006e-02,  2.6199e-02, -1.7136e-02,\n",
            "         -2.6559e-02,  1.9828e-02,  2.8017e-02, -1.4699e-02,  2.9622e-02,\n",
            "          2.8106e-02,  2.0219e-02,  8.1820e-03, -1.5066e-02, -1.5592e-03,\n",
            "         -2.4202e-02,  3.3043e-02, -2.7951e-02, -1.7070e-02,  2.7692e-02,\n",
            "         -1.4364e-02,  3.0371e-02, -1.0146e-02, -1.7735e-02, -3.2489e-02,\n",
            "         -2.1748e-02, -2.7093e-02, -2.6706e-02, -1.5301e-02,  6.3175e-03,\n",
            "         -4.2294e-03,  1.8102e-02,  3.1684e-02, -3.3429e-02,  1.1052e-02,\n",
            "          1.1153e-02, -1.1135e-02,  2.0547e-02,  9.4565e-03,  2.8659e-02,\n",
            "         -3.2304e-02, -8.5253e-03,  9.8740e-04, -3.2515e-02,  5.7744e-03,\n",
            "          1.8454e-02,  2.7991e-02,  1.6542e-02, -5.5110e-03,  1.2966e-02,\n",
            "         -2.4903e-03,  2.0756e-02,  2.6872e-02,  2.0719e-03,  2.6698e-02,\n",
            "          2.6937e-02, -1.0350e-02, -2.9594e-02,  1.9505e-02, -2.4811e-02,\n",
            "         -2.1372e-02,  3.3810e-02,  2.1292e-02, -1.8156e-02,  1.9320e-02,\n",
            "          2.0476e-02,  2.9422e-02, -4.8722e-03, -2.0351e-02,  2.1648e-03,\n",
            "          2.1023e-02,  2.5992e-02,  2.9900e-02, -3.5773e-02,  1.4984e-02,\n",
            "          2.7783e-02, -8.6234e-03,  3.8275e-03, -7.8582e-03, -1.3038e-02,\n",
            "         -6.6945e-03, -1.2494e-02, -2.9399e-02, -4.0356e-04,  2.7304e-02,\n",
            "         -2.5248e-02,  1.2978e-02, -2.8332e-02,  2.6237e-03,  3.0991e-02,\n",
            "         -1.5593e-02, -3.0953e-02,  3.7444e-03, -8.8383e-03,  3.6671e-03,\n",
            "          1.4608e-02,  7.7422e-03, -3.5206e-02,  8.3963e-03, -2.9149e-05,\n",
            "          5.3355e-04, -1.6557e-02, -2.4932e-04,  3.0136e-02,  2.9218e-02,\n",
            "         -1.9063e-02, -7.4019e-03,  1.9924e-02,  2.6476e-03,  1.7579e-02,\n",
            "          3.5641e-02, -2.3627e-03,  1.9354e-02,  7.3279e-03,  6.3396e-03,\n",
            "          2.2755e-02, -1.9922e-02, -4.0839e-03, -2.8102e-03,  1.8032e-03,\n",
            "          3.4641e-02, -1.6968e-02, -2.0948e-02, -1.7952e-02, -1.5762e-02,\n",
            "          2.5184e-02,  2.7009e-02,  4.1368e-03, -2.9937e-03, -3.3989e-02,\n",
            "         -1.0791e-02,  1.0778e-02,  1.2258e-02,  1.5530e-02,  1.8753e-02,\n",
            "         -2.5601e-02,  2.0388e-02,  3.0720e-02,  7.7695e-03,  2.4577e-02,\n",
            "         -1.4049e-02, -5.9786e-03,  3.1868e-02,  2.8414e-02, -1.6118e-02,\n",
            "          7.2309e-03, -1.3870e-02,  2.6207e-02,  3.2498e-02,  3.8844e-03,\n",
            "          2.1468e-02,  5.2975e-03,  2.9293e-02,  1.2179e-02, -2.3820e-02,\n",
            "          2.9919e-02,  2.1188e-02,  3.5891e-03, -3.1657e-02, -3.5290e-03,\n",
            "          1.5458e-02, -3.3222e-02, -2.0949e-02, -1.6199e-02,  5.6103e-03,\n",
            "          1.5774e-03,  7.4854e-03, -1.7007e-03,  3.1759e-02,  2.0259e-03,\n",
            "         -1.7563e-02, -1.0711e-02, -6.3695e-03, -5.3778e-03, -6.7225e-03,\n",
            "          1.8048e-02,  1.0755e-02,  2.3554e-02, -1.1749e-02, -2.2812e-02,\n",
            "         -3.3209e-02,  5.0157e-03,  9.0059e-03,  2.0166e-02, -2.9905e-02,\n",
            "         -1.0361e-02, -1.6952e-02,  1.5587e-02,  3.1821e-02, -1.3162e-02,\n",
            "         -2.5820e-02,  1.5213e-02,  3.3480e-02, -2.9889e-02,  3.4109e-02,\n",
            "          2.5910e-02,  4.7473e-04, -4.1064e-03, -2.2389e-02, -1.4442e-02,\n",
            "         -1.0090e-02,  3.1058e-02,  1.9477e-02, -1.4116e-02, -9.1546e-04,\n",
            "         -2.3294e-02, -3.1001e-02, -3.1080e-03, -2.5900e-02, -2.5833e-03,\n",
            "          2.2948e-02, -2.7404e-02,  2.0938e-03, -1.6314e-02,  7.2268e-03,\n",
            "          2.6303e-02,  5.0551e-03,  4.2268e-02,  9.5783e-03, -2.8062e-02,\n",
            "         -2.3176e-02,  2.9036e-02,  1.8238e-02, -1.1576e-02,  1.0560e-02,\n",
            "         -4.7455e-03,  1.5481e-02, -9.6963e-03,  1.3572e-02, -2.4814e-03,\n",
            "         -9.0755e-03, -1.3248e-02, -1.5156e-02,  2.1175e-02, -4.8873e-03,\n",
            "         -5.7145e-03, -1.7799e-03, -1.4527e-02,  1.5180e-02, -1.5102e-02,\n",
            "         -1.3385e-03,  2.6260e-02, -2.0462e-02,  7.6336e-03, -7.5990e-03,\n",
            "         -1.9020e-02,  1.6421e-02,  1.8357e-02,  2.0197e-02,  2.3549e-02,\n",
            "          2.5910e-02,  2.0020e-02, -9.6136e-03, -3.1384e-02, -1.5160e-02,\n",
            "         -1.5759e-02, -3.2620e-03,  6.7547e-03,  2.7307e-02,  2.8187e-03,\n",
            "          2.3175e-02,  1.0533e-03,  2.4636e-02,  2.4147e-02,  3.2294e-02,\n",
            "         -7.9744e-03,  2.9523e-04, -1.1069e-02, -1.1315e-02,  2.4483e-02,\n",
            "         -1.2246e-03,  2.6948e-02, -3.0309e-02,  2.8522e-02,  2.3758e-02,\n",
            "         -1.0362e-02, -2.2526e-02,  1.2462e-02, -2.3512e-02,  1.0746e-02,\n",
            "          1.2388e-02,  1.1245e-02,  3.4421e-02, -2.9722e-02,  5.8853e-03,\n",
            "          2.0164e-02, -1.1090e-02,  2.0598e-02,  2.9892e-02,  2.7861e-02,\n",
            "          1.8214e-02, -3.3168e-02,  2.1349e-02,  2.5317e-02,  7.7080e-03,\n",
            "         -1.3940e-02,  1.0932e-02,  3.2169e-02,  1.4731e-02,  1.3920e-02,\n",
            "          1.8216e-02, -2.9760e-02, -6.5155e-03, -2.3267e-02, -2.1574e-02,\n",
            "         -2.9390e-02, -1.3546e-02, -1.8404e-02, -6.7295e-03, -3.0841e-03,\n",
            "         -1.2020e-02,  1.5565e-02,  2.4015e-02, -1.5731e-03, -1.4448e-02,\n",
            "          2.4683e-03,  1.2724e-02,  1.4196e-02, -2.6454e-02,  4.2488e-03,\n",
            "          1.9821e-02,  3.3461e-02, -3.3788e-02,  1.6643e-02, -1.8105e-02,\n",
            "         -2.5016e-02, -2.6505e-02,  2.9217e-02, -2.5498e-02, -1.2316e-03,\n",
            "         -2.8791e-02, -5.7491e-03,  5.3344e-03, -2.7318e-02,  2.0590e-02,\n",
            "         -1.7374e-02, -1.4138e-02, -2.3924e-02, -1.1758e-02, -2.9732e-02,\n",
            "          2.7415e-02,  1.3914e-02, -1.1100e-02, -3.3782e-02,  3.4014e-03,\n",
            "          6.6157e-03, -2.6489e-02,  7.3456e-03, -2.3915e-02,  3.3789e-02,\n",
            "          1.3524e-02, -3.9328e-03, -1.1748e-02,  3.4188e-02,  1.0439e-02,\n",
            "          2.5560e-02, -1.7626e-02,  9.7497e-03, -2.9297e-02, -2.2205e-02,\n",
            "         -3.1745e-02,  1.0881e-02,  1.3792e-03,  1.0024e-02, -5.2818e-03,\n",
            "         -2.9460e-02,  1.3753e-02, -2.3731e-02,  3.4108e-02,  9.6065e-03,\n",
            "         -3.6565e-03, -1.3102e-02,  3.2685e-02, -5.8352e-03,  3.0224e-02,\n",
            "         -1.8754e-02, -1.3580e-02,  3.3541e-02, -1.8403e-02,  4.5067e-03,\n",
            "          1.4659e-02,  2.7977e-02,  3.2902e-02, -3.1192e-02, -2.2964e-02,\n",
            "         -2.7153e-02, -1.3610e-02,  2.7034e-02,  2.1715e-02, -3.2977e-02,\n",
            "         -2.3016e-02,  2.9279e-02,  3.2642e-02, -2.6722e-02,  3.3798e-02,\n",
            "         -8.5045e-03,  2.4184e-02,  1.2652e-02,  2.6147e-02, -3.2880e-02,\n",
            "         -1.7590e-02,  8.5500e-03, -3.5172e-02, -2.5039e-02,  2.2207e-02,\n",
            "          2.4922e-02,  1.0636e-02,  2.0075e-02, -3.4757e-03,  8.7404e-03,\n",
            "          7.3545e-03, -2.8163e-02, -1.5295e-03,  6.3591e-03,  6.7782e-03,\n",
            "          2.8061e-02, -2.2219e-02,  3.4350e-02, -3.2700e-02, -1.8234e-02,\n",
            "         -7.4928e-03,  2.1571e-02,  1.2925e-02,  8.5642e-03, -2.5933e-02,\n",
            "         -3.3531e-02,  1.4061e-03, -4.7934e-03, -2.5764e-02,  7.0540e-03,\n",
            "         -2.8791e-02, -1.2600e-02,  1.1983e-02, -2.0331e-02,  3.4518e-02,\n",
            "          1.0508e-02, -2.7421e-02, -3.0243e-02,  2.5270e-02, -1.4965e-02,\n",
            "         -9.4040e-03,  6.1235e-03, -2.2583e-02,  1.1981e-02,  2.5460e-02,\n",
            "         -2.0848e-02, -2.2311e-02,  2.9260e-02, -5.6622e-03,  1.4002e-02,\n",
            "          2.0423e-02,  1.2861e-02,  2.8849e-02, -2.2931e-02,  1.4480e-02,\n",
            "          1.3572e-02, -1.8032e-03, -2.6999e-02, -2.8585e-02,  7.0263e-03,\n",
            "         -2.3643e-02, -5.3279e-03,  1.0095e-03, -1.2842e-02,  3.3901e-02,\n",
            "          3.2881e-02, -1.7047e-02, -2.2871e-02,  9.3174e-03,  3.7522e-03,\n",
            "          1.2902e-02, -1.0092e-02,  1.4570e-02, -2.3742e-02, -1.4597e-02,\n",
            "         -1.8822e-02,  1.4621e-02,  3.2151e-02, -1.3802e-03, -4.8693e-03,\n",
            "          3.0635e-02,  3.0403e-02,  1.9586e-04,  2.0930e-02, -1.6199e-02,\n",
            "          1.8892e-02,  1.2989e-02,  1.7495e-02,  4.1322e-04,  2.0814e-02,\n",
            "          2.1609e-02, -2.1072e-02,  1.9286e-03, -3.3472e-02, -1.5531e-02,\n",
            "         -1.9214e-02, -5.0458e-03,  1.8462e-02, -2.9279e-02, -1.4867e-02,\n",
            "         -9.2819e-03,  2.4885e-02, -1.1206e-02, -1.5413e-02,  3.2363e-02,\n",
            "         -2.4961e-03, -2.0746e-02, -5.5059e-03,  2.2634e-02, -1.6131e-02,\n",
            "          1.1825e-02,  2.9179e-02,  2.8729e-02,  4.8585e-03,  3.2816e-03,\n",
            "          1.5459e-02, -6.1939e-03,  1.3664e-02, -2.5449e-02,  2.6952e-02,\n",
            "          9.3390e-03, -2.0754e-02,  2.9078e-02,  2.4905e-02,  2.3769e-02,\n",
            "          3.1750e-02,  1.5873e-02, -9.7720e-03,  1.8655e-02, -1.0264e-02,\n",
            "          3.0003e-02, -2.5272e-03, -9.0198e-04, -1.9007e-02, -2.9515e-02,\n",
            "         -3.5120e-02,  2.3259e-02,  2.6186e-02, -1.0633e-02, -2.5941e-02,\n",
            "          2.1150e-02, -6.9411e-03,  1.6475e-02,  3.4133e-02, -1.0504e-02,\n",
            "          8.5321e-03, -2.0560e-02,  6.9351e-04, -1.2917e-02, -3.2478e-02,\n",
            "          3.3995e-02, -1.7761e-02,  3.3528e-02, -2.6437e-03,  5.5147e-03,\n",
            "         -8.4137e-03,  1.5894e-02,  1.2610e-02,  3.6185e-02,  1.5313e-02,\n",
            "         -7.2398e-03,  2.6704e-02, -1.9375e-02, -1.4233e-02, -2.3934e-02,\n",
            "          2.6576e-02,  9.2874e-03,  2.6184e-02,  3.1857e-02,  8.4899e-03,\n",
            "         -1.6207e-02,  3.4101e-02, -1.3658e-02,  1.8561e-02,  2.5305e-02,\n",
            "         -2.9620e-02, -3.3935e-02,  1.3013e-02, -2.6737e-02, -3.0735e-02,\n",
            "          8.2631e-03,  3.3142e-02, -2.9789e-02,  2.0920e-02,  2.3857e-02,\n",
            "          1.8649e-03, -1.6386e-02, -1.8337e-02,  3.2168e-02,  6.4672e-03,\n",
            "         -3.3945e-02,  3.2537e-02,  7.6973e-03,  3.2516e-02, -3.6020e-02,\n",
            "          2.0930e-02,  1.5923e-02,  2.8751e-03,  2.7549e-02, -2.7557e-02,\n",
            "         -8.8161e-03,  9.0645e-03, -3.3748e-02,  6.4138e-03, -1.5500e-02,\n",
            "          1.9655e-02,  1.0007e-02,  1.1725e-02,  2.7740e-02,  1.7093e-02,\n",
            "          1.9172e-02,  3.4225e-04,  1.0893e-02, -2.9095e-02,  1.7732e-02,\n",
            "          1.1599e-03,  3.0690e-02, -4.3111e-03, -6.6411e-03, -1.3046e-02,\n",
            "         -2.1193e-02, -3.0463e-02, -3.0362e-02, -2.7848e-02, -1.5176e-02,\n",
            "          2.2938e-02, -2.9290e-02, -1.8476e-02, -3.1795e-02, -2.6471e-02,\n",
            "          2.4947e-02, -9.2098e-03, -2.6129e-02,  8.1122e-03, -2.4157e-02,\n",
            "          5.2439e-03, -1.3634e-02,  3.2073e-02, -2.6019e-02, -2.1678e-02,\n",
            "          1.4371e-02,  1.5681e-02,  2.8017e-02,  2.2392e-02,  3.3008e-02,\n",
            "         -1.5937e-02,  3.5453e-03,  2.2567e-02, -3.4662e-03, -3.0684e-02,\n",
            "          2.3462e-02, -1.2869e-02, -2.3260e-03,  1.5414e-02, -2.1458e-02,\n",
            "          2.8654e-02,  1.1282e-02, -1.9175e-02,  2.3434e-02,  2.0393e-02,\n",
            "          4.2986e-03,  1.5589e-02, -2.7554e-02,  1.3399e-02,  1.3465e-03,\n",
            "         -3.1509e-02, -2.0694e-02,  2.8056e-02,  1.9604e-02, -3.3393e-02,\n",
            "         -7.4919e-03,  7.8771e-03, -9.1133e-03,  2.9048e-02, -8.3608e-03,\n",
            "         -2.2866e-02,  1.1144e-02,  2.2105e-02, -1.7883e-02,  3.4183e-02,\n",
            "          6.4353e-03,  1.7170e-02, -2.4722e-02,  1.0961e-02, -6.4935e-03,\n",
            "         -3.5629e-02,  1.5998e-02,  2.7223e-02,  8.0873e-03, -3.0720e-03,\n",
            "         -2.3196e-02, -2.9841e-02,  5.0699e-03,  1.6152e-02, -2.5272e-02,\n",
            "         -9.9407e-03,  2.0193e-02,  2.7020e-02,  3.1129e-02, -1.1053e-02,\n",
            "          3.3421e-02,  8.0295e-03,  9.1394e-03, -3.1960e-02,  2.3972e-02,\n",
            "         -2.4523e-02, -2.8134e-02,  2.8634e-02,  8.3792e-05,  1.4576e-02,\n",
            "         -4.0129e-03, -1.9215e-02,  5.9058e-03, -3.3595e-02,  1.2683e-02,\n",
            "          1.9479e-03, -1.2704e-02, -9.5612e-04, -2.0384e-02,  2.1143e-02,\n",
            "          3.2302e-03,  2.8852e-03,  2.0751e-02,  1.8086e-02, -3.2168e-02,\n",
            "         -3.2538e-02,  1.4629e-02,  1.2262e-02,  1.8872e-02, -1.2123e-02,\n",
            "         -1.1140e-02, -2.6065e-02,  1.3678e-02, -3.1001e-02,  3.3887e-02,\n",
            "         -6.7562e-03,  1.9986e-02, -1.9770e-02, -1.3885e-02,  2.6495e-02,\n",
            "          2.2948e-02, -2.2302e-03,  1.8649e-02,  2.4000e-02,  2.9374e-03,\n",
            "         -2.9864e-02, -3.2267e-03,  1.5808e-03, -2.8460e-02,  2.2464e-02,\n",
            "         -2.3714e-02,  1.4644e-02, -1.5501e-02, -3.2308e-02, -1.5641e-02,\n",
            "         -1.9569e-02,  1.8521e-02,  1.7954e-02, -2.5147e-02,  1.2594e-02]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([-0.0002], device='cuda:0', requires_grad=True)]\n"
          ]
        }
      ],
      "source": [
        "print(list(net.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "ozUDGrjj_MQW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "157c76ca-d62d-44bc-c678-f90a38d3ed1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor([[ 0.2491, -0.0154, -0.5158],\n",
            "        [-0.2211, -0.5437,  0.2490],\n",
            "        [ 0.3927,  0.5422, -0.3364],\n",
            "        ...,\n",
            "        [-0.5098,  0.0773, -0.4235],\n",
            "        [-0.1741,  0.1158, -0.2311],\n",
            "        [-0.4212,  0.0641,  0.4889]], device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([-0.5349,  0.2851,  0.0322,  ...,  0.1765,  0.5665,  0.2583],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.0084, -0.0205,  0.0311,  ...,  0.0193,  0.0006,  0.0167],\n",
            "        [ 0.0142, -0.0110,  0.0175,  ..., -0.0236, -0.0192, -0.0082],\n",
            "        [ 0.0194, -0.0241,  0.0152,  ...,  0.0226, -0.0299, -0.0003],\n",
            "        ...,\n",
            "        [ 0.0283,  0.0114, -0.0209,  ..., -0.0059, -0.0151,  0.0089],\n",
            "        [ 0.0130,  0.0149, -0.0033,  ..., -0.0183, -0.0262, -0.0195],\n",
            "        [-0.0159,  0.0195,  0.0096,  ...,  0.0212,  0.0006, -0.0147]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([-1.8118e-02,  2.7817e-02, -1.9838e-02,  2.3841e-02,  1.5238e-02,\n",
            "         1.8506e-02, -2.6745e-02, -3.1433e-02, -3.0839e-02,  8.7426e-03,\n",
            "         1.7750e-02, -1.9132e-02,  4.8521e-03, -1.7422e-02,  1.7771e-02,\n",
            "         5.9097e-03,  2.3067e-02, -3.8717e-03, -1.6859e-02, -1.0940e-02,\n",
            "        -6.0716e-03,  9.5921e-03,  8.6659e-03,  2.3914e-02, -8.2484e-03,\n",
            "        -1.1126e-02, -1.3545e-02, -6.9706e-03, -1.4684e-02,  1.0366e-02,\n",
            "        -3.0602e-02, -2.5311e-04, -2.9148e-02,  1.3749e-02,  2.6279e-02,\n",
            "         2.3415e-02,  3.0909e-02,  2.2240e-02, -2.0534e-04, -3.0910e-02,\n",
            "        -2.9967e-02, -1.6821e-02, -1.6823e-02, -6.6773e-03, -1.9761e-02,\n",
            "        -2.6331e-02,  7.7959e-03,  2.1958e-02, -5.8001e-03,  1.4815e-02,\n",
            "        -9.5447e-03,  5.1434e-03,  1.6097e-02,  2.0478e-02, -8.1313e-03,\n",
            "         1.6197e-02,  9.1496e-03,  2.5342e-02,  1.6137e-02, -2.5656e-02,\n",
            "        -1.8861e-02, -3.8307e-03, -5.4298e-03,  8.8476e-03,  6.4336e-03,\n",
            "        -1.3226e-02,  2.3352e-02,  1.9205e-02,  2.7015e-02, -6.4431e-03,\n",
            "        -2.0183e-02, -1.7816e-02, -7.1554e-03, -1.2285e-02, -1.8457e-02,\n",
            "        -2.5042e-02,  2.7155e-02,  2.2412e-02,  2.0798e-02, -1.9102e-02,\n",
            "        -3.1425e-02,  1.6581e-02,  1.9337e-02,  1.2459e-02,  5.6943e-03,\n",
            "         9.0794e-03,  2.9483e-02,  2.5612e-02, -4.5737e-03, -1.3002e-03,\n",
            "        -1.9898e-02, -1.7501e-02, -2.8132e-02,  7.5364e-03, -1.9936e-03,\n",
            "        -2.0954e-02, -7.1631e-04,  1.2868e-02, -2.4564e-02, -2.1866e-02,\n",
            "         2.3896e-02, -1.3259e-02,  1.7876e-02,  2.8075e-02, -1.4995e-02,\n",
            "        -2.7850e-03, -3.0166e-03,  2.8716e-02, -1.0577e-02, -7.1062e-03,\n",
            "         4.9014e-03,  3.2766e-02, -2.0347e-02, -1.8240e-02,  1.7218e-02,\n",
            "         1.1607e-03,  6.5541e-04, -2.8179e-02,  3.2313e-04,  1.8283e-02,\n",
            "        -1.8563e-03, -1.6919e-02,  2.4340e-02,  2.7390e-02,  2.6575e-02,\n",
            "         1.0088e-02, -2.0847e-04,  1.2015e-02,  2.4438e-02,  2.9886e-02,\n",
            "        -1.9280e-02,  5.4056e-03,  2.5907e-02,  6.7308e-03,  2.9087e-03,\n",
            "         8.9887e-03, -1.9327e-02, -2.1997e-02, -3.9602e-03, -1.5551e-02,\n",
            "        -3.0551e-02,  2.5063e-02,  1.5483e-02,  7.8820e-03,  8.6799e-04,\n",
            "        -5.1095e-03,  2.9765e-02, -6.1604e-03,  2.3014e-02,  3.6488e-03,\n",
            "         2.2484e-02,  2.0513e-02, -2.4284e-02, -1.6262e-02, -1.4485e-02,\n",
            "        -1.1174e-02,  1.9979e-02, -1.7574e-02,  2.5884e-02, -1.0794e-02,\n",
            "         9.1818e-03,  1.9142e-02,  1.5617e-02,  1.8375e-02, -1.3344e-02,\n",
            "        -2.0693e-02,  2.1482e-03, -2.1523e-02, -1.3912e-02, -2.3021e-02,\n",
            "        -1.9401e-02, -8.3076e-03,  6.6115e-03,  2.0910e-02,  2.6615e-02,\n",
            "         5.4168e-03, -2.9698e-02,  2.2153e-02, -1.3466e-04,  9.7449e-03,\n",
            "         2.3699e-02, -2.3275e-02, -3.1955e-02,  1.9719e-02, -1.0907e-02,\n",
            "         2.8429e-02,  1.9254e-03, -9.3987e-03, -1.0479e-02, -1.9665e-02,\n",
            "        -1.1272e-02, -2.3966e-02,  2.9274e-02, -2.9660e-02,  1.2494e-02,\n",
            "         8.3643e-03, -4.7802e-03,  2.0754e-02, -6.4386e-03, -1.0606e-03,\n",
            "        -2.8425e-03, -3.5221e-03, -7.8872e-03, -1.7450e-02,  9.3602e-05,\n",
            "         1.6344e-02,  8.8699e-03,  2.6997e-03, -1.0894e-02,  2.2183e-02,\n",
            "        -7.5934e-03, -1.5766e-02,  1.3197e-02, -2.8102e-02,  1.8722e-02,\n",
            "         2.9627e-02, -1.9444e-02, -3.0675e-02, -4.5557e-03,  1.6862e-02,\n",
            "         2.9614e-02,  1.3836e-02, -1.2307e-04,  2.3630e-02,  1.9079e-02,\n",
            "        -2.8872e-02, -2.7364e-02,  2.2552e-02,  9.0262e-03,  1.6525e-02,\n",
            "         1.9304e-02, -1.2763e-02,  1.2107e-02,  1.2066e-02, -2.6160e-02,\n",
            "         1.6968e-02, -1.9698e-02,  1.7990e-02,  6.7113e-03, -2.0761e-02,\n",
            "         9.6888e-03,  1.6073e-03,  3.0049e-02,  8.8395e-03, -1.3277e-02,\n",
            "        -3.0571e-03, -1.3384e-02, -1.6273e-02,  8.0172e-03, -1.5162e-02,\n",
            "         2.5000e-02,  2.3237e-02,  2.6496e-02, -1.6572e-02,  2.7605e-02,\n",
            "         6.9329e-03,  1.2048e-02,  3.6213e-03,  2.1627e-02, -2.1237e-02,\n",
            "         1.9211e-02,  3.8263e-03, -3.1893e-03,  2.5782e-03,  2.7857e-02,\n",
            "        -1.1668e-02,  4.7829e-03,  9.7774e-03, -1.1603e-02, -1.1157e-04,\n",
            "         1.4537e-02, -1.6710e-02,  1.6391e-02,  1.5101e-02, -2.4145e-02,\n",
            "        -1.3569e-02,  1.8777e-02,  8.7727e-03,  1.2942e-02, -1.6551e-02,\n",
            "         2.5341e-02,  2.3251e-02,  3.4124e-03, -1.9081e-02,  6.7153e-03,\n",
            "        -2.1702e-02, -3.2667e-03,  1.2888e-02, -1.8995e-02,  1.3751e-02,\n",
            "        -1.1347e-02,  2.5117e-02, -2.2513e-04,  7.6802e-03,  2.2909e-02,\n",
            "         4.1527e-03,  2.4187e-02, -2.3635e-02, -2.2223e-02, -7.9086e-03,\n",
            "        -8.8565e-03,  1.5338e-02,  2.0760e-02,  1.8024e-02, -1.1566e-02,\n",
            "        -9.7946e-03, -6.1387e-03, -9.8751e-03,  2.0857e-02, -2.5879e-02,\n",
            "        -2.4259e-02, -2.1507e-02, -2.2012e-02,  1.6896e-02,  2.0769e-02,\n",
            "         1.6404e-02,  1.5029e-02,  8.0733e-04, -7.0626e-03,  2.6044e-02,\n",
            "         1.0507e-02,  8.0682e-03, -4.9415e-03,  7.9126e-03,  1.5225e-02,\n",
            "         1.4061e-02,  2.2191e-02,  3.0241e-02,  3.1632e-03,  2.0591e-02,\n",
            "        -2.5114e-02, -5.2946e-03,  2.5521e-02,  3.0078e-02, -9.1289e-03,\n",
            "        -7.5353e-03, -1.3472e-02,  1.0194e-03, -2.5093e-02, -1.1598e-02,\n",
            "         3.9075e-03,  2.8188e-02, -1.6021e-02,  5.4272e-03,  5.4096e-03,\n",
            "        -1.1444e-02, -2.3850e-02,  3.0925e-03, -5.6242e-03, -2.3519e-02,\n",
            "         1.9755e-02,  1.3664e-02,  7.7505e-03,  1.4545e-02, -2.4284e-02,\n",
            "         2.9570e-03,  2.5232e-02,  8.7355e-03, -1.9063e-02, -2.7562e-02,\n",
            "         3.0592e-02,  1.5665e-02, -1.3394e-02, -1.3365e-02, -3.6689e-03,\n",
            "         1.4641e-02, -7.6485e-03, -5.6154e-04,  1.8096e-02, -2.5056e-02,\n",
            "        -2.8741e-03,  8.2832e-03,  1.3541e-02,  1.4654e-02, -2.7058e-02,\n",
            "        -1.1220e-02,  2.3834e-03,  2.9731e-02,  4.0272e-03,  2.3995e-02,\n",
            "        -2.9039e-03, -6.5095e-03,  1.0198e-02, -1.8073e-02,  2.2547e-02,\n",
            "         2.7533e-02, -3.0975e-02, -1.9335e-02, -2.8631e-02,  1.2945e-02,\n",
            "        -2.4050e-03,  2.8984e-02, -2.3017e-02, -2.3147e-03, -1.1843e-02,\n",
            "         2.4194e-02,  5.1528e-03,  6.1314e-03,  4.7550e-03,  1.4418e-02,\n",
            "        -2.1870e-02,  2.3858e-03, -2.2174e-02, -1.7394e-02, -5.2301e-03,\n",
            "        -1.0067e-02, -2.7866e-02, -9.3755e-03,  1.5629e-02,  2.4071e-02,\n",
            "         2.0701e-03,  6.5893e-03,  2.2720e-02,  2.9220e-02, -1.3871e-02,\n",
            "         6.8237e-03,  1.1073e-02, -2.9574e-02,  1.6362e-02, -1.0868e-02,\n",
            "        -7.2435e-03, -2.7287e-02,  1.8707e-02,  2.0534e-02,  2.2766e-02,\n",
            "        -2.7630e-02, -3.0816e-02, -3.9331e-03, -1.7676e-02,  5.8166e-03,\n",
            "        -2.7955e-02,  4.7158e-03, -2.8129e-02,  1.0950e-02,  2.0204e-02,\n",
            "         3.1067e-04,  2.6103e-02, -2.9586e-02,  2.6510e-02,  1.1562e-02,\n",
            "         4.5301e-04,  2.5527e-02,  1.5000e-02, -1.0185e-02, -3.0959e-02,\n",
            "         1.5899e-02,  1.2843e-03, -2.3301e-02, -2.4410e-02, -7.9867e-03,\n",
            "         2.3622e-02, -2.5409e-02, -3.4703e-03,  2.0630e-02,  2.6553e-02,\n",
            "        -6.0895e-03, -2.6139e-02,  7.4240e-03,  1.3288e-02, -1.9744e-02,\n",
            "         6.1175e-03, -8.7067e-03, -8.9110e-04, -1.4531e-02,  2.5095e-02,\n",
            "         7.0041e-03, -5.6488e-03,  2.2695e-02,  1.3583e-02,  1.5536e-02,\n",
            "        -3.6452e-03,  2.5018e-02,  2.3305e-02,  2.6635e-02,  2.4868e-02,\n",
            "         2.6701e-02, -1.3020e-03,  1.8816e-02,  1.4462e-02, -1.9436e-02,\n",
            "        -8.5722e-03,  1.6436e-02, -1.8730e-02, -1.5612e-02,  1.0505e-02,\n",
            "        -7.3045e-03, -9.2654e-03,  4.0530e-03,  2.0449e-02, -2.1521e-02,\n",
            "         2.3009e-02,  4.5904e-03,  1.1671e-02, -2.6559e-02, -6.6724e-03,\n",
            "         1.4720e-02, -1.3155e-03,  1.0224e-02, -4.8472e-03,  1.0899e-02,\n",
            "        -8.0005e-03,  2.7024e-02, -2.3387e-02, -1.5996e-02,  4.2850e-03,\n",
            "         1.7434e-02, -2.6449e-02,  1.9437e-02, -2.9781e-02, -3.2994e-04,\n",
            "         2.5557e-02, -8.4039e-03, -2.3315e-04,  1.4014e-03, -2.9112e-02,\n",
            "        -1.8403e-02, -2.4355e-02, -2.5911e-02, -1.8838e-02,  2.5219e-02,\n",
            "         2.1696e-02, -1.4893e-02, -2.2284e-02,  2.9499e-02,  1.5011e-03,\n",
            "         1.7513e-03, -6.8941e-03,  2.2283e-02,  2.7758e-02, -1.8171e-02,\n",
            "         5.4178e-03, -7.2049e-03,  1.2354e-02,  2.1812e-02,  1.7427e-02,\n",
            "         2.4205e-02,  1.4981e-02, -8.2413e-03,  2.2317e-02,  1.4127e-02,\n",
            "        -1.2634e-02,  2.5160e-02, -2.4898e-03, -4.7699e-03,  1.5621e-02,\n",
            "         3.3414e-03,  1.4538e-03, -2.3740e-02,  1.9958e-02,  1.7023e-02,\n",
            "        -3.3535e-04, -2.1802e-03, -2.3860e-02, -1.5098e-02, -2.0221e-02,\n",
            "         2.5401e-02, -9.3477e-03,  2.0335e-02,  7.3290e-03, -1.6413e-02,\n",
            "        -2.9674e-02, -1.7253e-02,  1.4320e-02,  7.6620e-03,  2.7278e-02,\n",
            "         2.3301e-02, -2.3178e-02, -2.4300e-02,  2.4014e-02,  3.4079e-03,\n",
            "         4.7119e-03,  2.5677e-02,  8.8305e-03,  1.0131e-02,  2.0829e-02,\n",
            "         2.0087e-02, -1.5111e-02,  1.7677e-03,  2.2972e-02,  1.2750e-02,\n",
            "        -1.3178e-02,  3.4779e-03, -2.7756e-02,  3.8141e-03, -1.2839e-02,\n",
            "         8.6578e-03, -7.6490e-03,  1.2918e-03,  2.6574e-02,  2.1223e-02,\n",
            "        -2.6139e-02,  1.0425e-02, -1.1013e-02, -1.7062e-02, -1.2136e-02,\n",
            "        -1.7372e-02,  1.8214e-02,  2.2429e-02,  7.1559e-03,  1.2937e-02,\n",
            "         2.4048e-02,  4.0526e-03,  1.3215e-03,  2.3807e-02,  1.6253e-02,\n",
            "         2.7084e-02,  2.2869e-02, -1.0692e-02,  2.6442e-02, -2.7687e-02,\n",
            "        -1.8129e-03, -1.2693e-02,  2.9193e-02, -2.2183e-02, -1.2941e-02,\n",
            "        -2.6905e-02, -1.0474e-02, -1.1379e-02, -2.8667e-02,  1.5728e-02,\n",
            "        -1.1269e-04,  2.4486e-02,  1.5575e-02,  6.3477e-03, -3.8496e-03,\n",
            "         2.7682e-02, -1.4838e-02,  4.0568e-03, -3.0306e-02,  1.0135e-02,\n",
            "         1.4527e-02,  2.4086e-02, -1.3246e-03, -5.5541e-03, -3.0655e-03,\n",
            "        -7.8492e-03,  7.9160e-03, -5.7563e-03, -9.3403e-03,  2.6658e-02,\n",
            "         2.7359e-02,  1.7629e-02,  6.9327e-03, -2.7225e-02, -1.3017e-02,\n",
            "        -2.7790e-02, -1.8800e-02,  2.6362e-02,  6.4411e-03, -2.3820e-02,\n",
            "        -3.1337e-02, -1.7480e-02, -1.4079e-03,  1.2087e-02, -7.3949e-03,\n",
            "         1.5092e-02,  1.1141e-02, -2.0506e-02, -9.5732e-03, -4.9644e-03,\n",
            "        -2.8180e-02,  6.0945e-04,  1.5499e-02, -1.3375e-03, -1.7667e-02,\n",
            "        -4.7949e-03,  1.5820e-02, -2.7158e-02, -1.8429e-02, -2.6913e-02,\n",
            "         9.4847e-03,  1.2014e-02,  2.3916e-02, -1.9053e-02, -1.4085e-02,\n",
            "         5.2371e-03, -4.7322e-03, -2.7768e-02,  2.3044e-02,  1.7862e-02,\n",
            "         5.1652e-03,  5.7866e-03,  1.5405e-02,  3.0862e-02,  7.4468e-03,\n",
            "        -2.8660e-02,  2.1098e-02, -1.2801e-02, -9.1947e-03, -1.4526e-02,\n",
            "         3.5940e-03,  3.4609e-03, -1.5262e-03,  6.1190e-03,  1.3020e-02,\n",
            "         1.8133e-02, -3.0893e-03,  9.3023e-03, -4.3035e-03, -5.8621e-03,\n",
            "        -4.9882e-03,  1.9467e-02,  5.0334e-03,  1.7634e-02, -2.4214e-03,\n",
            "         1.5824e-02,  1.1113e-02,  2.7371e-02,  1.7201e-03,  7.7691e-04,\n",
            "         1.0473e-02,  7.1966e-03, -3.5944e-03,  2.9299e-02,  1.6452e-03,\n",
            "        -1.4841e-02, -5.2132e-03,  1.7415e-02, -1.2846e-02,  1.4125e-02,\n",
            "         1.7135e-02, -1.5827e-02,  1.8330e-02, -1.5705e-02, -4.1303e-03,\n",
            "         2.9470e-02,  1.3214e-03, -3.0864e-02,  2.2606e-02, -1.4998e-02,\n",
            "         1.7664e-02, -2.5116e-02,  5.5415e-03,  1.9734e-02, -2.0948e-02,\n",
            "        -1.0499e-02, -4.4634e-03, -2.3454e-03, -2.5811e-02, -1.7683e-02,\n",
            "        -2.5238e-02,  2.9903e-02, -1.3415e-02, -2.4076e-02,  8.0738e-03,\n",
            "        -6.4178e-03,  2.6694e-02,  2.4435e-02, -2.0672e-02,  2.8067e-03,\n",
            "        -1.1542e-02,  2.3122e-02, -2.7174e-02,  7.0581e-03,  4.4595e-03,\n",
            "        -2.8221e-02, -1.0331e-02, -1.1479e-02, -2.8966e-02, -2.0666e-02,\n",
            "         1.1246e-03,  2.5485e-02,  1.9940e-02, -2.4241e-03, -2.7133e-02,\n",
            "        -6.4601e-05, -1.4391e-02,  3.9006e-03,  1.9059e-02,  1.4498e-02,\n",
            "         1.6925e-02, -2.5129e-02,  6.7160e-03,  1.6520e-02, -1.1597e-02,\n",
            "        -2.5776e-03, -1.0697e-02, -1.8838e-02, -1.8677e-03,  1.9182e-04,\n",
            "         1.1723e-02, -1.2418e-03, -1.3915e-02, -1.5141e-02,  2.1583e-02,\n",
            "        -3.6220e-03,  4.7592e-03,  6.0094e-03,  1.5773e-02,  1.8182e-02,\n",
            "         2.8263e-02, -3.0791e-02, -2.1032e-02,  3.9093e-03, -2.7280e-02,\n",
            "         2.0330e-02, -2.2542e-02, -2.1506e-02,  1.1711e-02, -1.7368e-02,\n",
            "        -1.5672e-02, -9.6132e-03, -2.1427e-02, -1.0420e-02,  2.0561e-02,\n",
            "         2.0470e-02, -8.8239e-03, -5.3848e-04, -1.3559e-02,  1.4518e-02,\n",
            "         1.5428e-02,  2.5589e-02,  8.3284e-03,  1.7063e-02,  1.7706e-02,\n",
            "        -1.9177e-02, -5.2429e-03,  8.2786e-03, -1.4832e-02, -1.7999e-02,\n",
            "        -2.3194e-02,  4.3262e-03, -5.3708e-04,  5.0061e-03, -9.5460e-03,\n",
            "         2.6492e-02,  2.0044e-02, -1.9577e-02, -2.6207e-02, -1.7423e-02,\n",
            "        -2.8654e-02, -2.3089e-02,  2.0345e-02, -1.6634e-02,  1.5337e-02,\n",
            "        -1.7777e-02, -2.9554e-02,  1.7709e-02, -1.8214e-02, -2.0106e-03,\n",
            "        -2.8342e-02,  1.1914e-02, -2.4641e-02, -1.3474e-02, -3.6725e-03,\n",
            "         3.8342e-03,  4.7166e-03,  2.7441e-02,  2.7444e-02,  3.5137e-03,\n",
            "         2.6906e-02, -2.0687e-02,  2.6347e-02, -2.4380e-02, -2.8713e-03,\n",
            "         2.2155e-02, -2.1435e-02, -8.5123e-03,  3.0116e-02,  1.0453e-02,\n",
            "        -2.2873e-02, -7.3061e-03,  1.3505e-02, -1.0680e-02,  1.5951e-02,\n",
            "        -3.5714e-03, -1.0233e-02,  1.5326e-02, -5.9924e-03,  1.9178e-02,\n",
            "        -1.8455e-02, -1.6403e-02, -7.0622e-03,  2.1060e-02, -2.7514e-02,\n",
            "        -1.5754e-02, -1.6234e-02,  2.9609e-02,  1.9457e-02, -1.6782e-02,\n",
            "         2.5768e-02, -2.2089e-02,  1.7510e-02, -1.5594e-02,  1.6076e-02,\n",
            "         3.8558e-03,  9.0380e-03, -2.5038e-02, -2.2602e-02, -1.9117e-02,\n",
            "        -8.2552e-03, -5.6334e-03, -2.2476e-02,  2.0380e-02,  1.2791e-02,\n",
            "        -2.8848e-02,  2.6836e-02,  1.6533e-02, -2.6037e-02,  3.0777e-03,\n",
            "        -2.7272e-02, -2.8729e-02,  1.1994e-02, -2.3057e-02,  2.3210e-02,\n",
            "        -2.6339e-02,  6.6061e-03,  1.9914e-02, -2.7028e-02,  2.1034e-03,\n",
            "        -2.1373e-02,  6.3881e-04,  2.5240e-02, -3.3166e-03], device='cuda:0',\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[-0.0011,  0.0150, -0.0290,  ...,  0.0266, -0.0234, -0.0158],\n",
            "        [ 0.0112, -0.0044,  0.0054,  ..., -0.0279, -0.0017, -0.0185],\n",
            "        [-0.0184, -0.0007, -0.0313,  ..., -0.0153, -0.0048,  0.0041],\n",
            "        ...,\n",
            "        [ 0.0244, -0.0298,  0.0153,  ...,  0.0161, -0.0074,  0.0194],\n",
            "        [-0.0193,  0.0254,  0.0143,  ..., -0.0110,  0.0213, -0.0299],\n",
            "        [-0.0099, -0.0057,  0.0059,  ..., -0.0267,  0.0158,  0.0241]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([ 1.8927e-02,  5.1221e-03, -6.3455e-03,  6.3523e-03, -1.8603e-03,\n",
            "        -2.5010e-02,  3.1347e-02,  1.4582e-03, -9.5578e-03, -1.6248e-03,\n",
            "        -7.8893e-03,  2.3840e-02,  2.9724e-02,  5.5908e-03,  9.4720e-04,\n",
            "         2.1751e-02,  8.3106e-04,  1.5326e-02,  5.0339e-03, -2.9501e-02,\n",
            "        -3.2940e-02,  1.7357e-02, -3.2578e-02,  8.6837e-03, -3.0859e-02,\n",
            "        -1.6332e-02, -6.6037e-03,  2.7375e-04, -1.1204e-02, -1.3180e-02,\n",
            "         2.9279e-03,  3.3989e-03,  2.4262e-02, -3.1681e-02, -1.0913e-02,\n",
            "        -1.0458e-02,  1.6959e-02, -5.0488e-03, -3.0351e-02, -2.1643e-03,\n",
            "        -1.4353e-02, -2.6382e-02,  3.0768e-02, -2.1259e-02,  1.1666e-02,\n",
            "         1.9044e-02,  1.0232e-02,  2.8427e-02, -2.2469e-02, -2.1643e-02,\n",
            "         2.2096e-02,  7.8680e-03,  1.8666e-02,  2.9698e-02,  2.2635e-02,\n",
            "         1.7691e-02, -2.9671e-02,  4.9964e-03, -2.9822e-02,  7.9353e-03,\n",
            "         9.7695e-03,  2.3912e-02, -3.2366e-02, -1.5210e-02,  6.0818e-03,\n",
            "         1.8792e-02,  2.0241e-02,  5.5982e-03,  2.9330e-04, -2.4334e-02,\n",
            "         6.4341e-03,  4.3976e-03, -1.1488e-02,  1.1159e-02,  1.0546e-02,\n",
            "         1.3750e-02, -3.1680e-02,  1.7578e-02, -1.5640e-02, -2.1870e-02,\n",
            "        -3.0378e-02,  2.7670e-02, -5.1787e-03, -1.8056e-03,  2.0900e-02,\n",
            "         4.4609e-03, -1.4667e-02,  6.0511e-03, -1.8227e-02,  3.0482e-02,\n",
            "         2.5518e-02,  1.4948e-02, -9.2702e-03,  1.9513e-02, -1.9906e-02,\n",
            "         3.1905e-02, -2.5397e-02,  5.7974e-03,  5.7382e-04,  2.9320e-02,\n",
            "         3.2204e-02,  1.2893e-02,  2.1015e-02,  2.2990e-02, -6.4969e-05,\n",
            "         5.8146e-03, -2.2263e-02, -1.9236e-02,  2.1229e-02, -2.8150e-02,\n",
            "         1.1730e-02,  2.3219e-02, -2.1322e-02, -1.6256e-02, -1.2405e-02,\n",
            "        -2.9742e-02,  9.3833e-03, -2.8342e-02, -1.6681e-02,  2.5492e-02,\n",
            "         2.2188e-02,  4.1535e-04, -3.1688e-02,  1.0771e-02, -2.0049e-02,\n",
            "         5.1351e-03, -1.1425e-02, -3.0886e-02,  2.9006e-02,  1.8110e-02,\n",
            "         2.2185e-02,  7.8402e-03,  1.2789e-02,  2.4833e-02,  2.8787e-02,\n",
            "        -4.6257e-03,  1.6405e-02,  1.2906e-02,  2.3993e-02,  1.5517e-02,\n",
            "        -2.6469e-02, -2.5906e-02,  7.7334e-03, -2.6463e-02,  1.0474e-03,\n",
            "        -1.9411e-02,  1.7606e-02,  1.4320e-02, -7.9357e-03, -2.2951e-03,\n",
            "         6.3760e-03, -2.6427e-02,  1.7020e-02, -1.0167e-02, -3.2292e-02,\n",
            "         7.6268e-03, -1.0041e-02,  1.6680e-02, -2.9899e-02, -2.0065e-02,\n",
            "        -3.2987e-02, -1.9525e-02, -9.3159e-03, -9.7834e-03, -1.9171e-02,\n",
            "         1.8495e-02,  9.2168e-03,  5.5167e-03, -1.7018e-02,  2.4381e-02,\n",
            "         7.0030e-03,  3.2638e-02, -2.6448e-02,  1.4497e-02, -9.7048e-03,\n",
            "         2.5006e-02, -1.1184e-02, -1.4239e-03,  1.3675e-02,  3.1494e-02,\n",
            "         1.3383e-02,  2.9359e-02,  3.0570e-02,  2.7051e-02, -2.2908e-04,\n",
            "        -2.0143e-02, -6.7779e-03,  2.5610e-02,  5.8868e-03,  1.5429e-02,\n",
            "        -7.6164e-03,  2.4390e-02,  2.7092e-02,  9.2754e-03,  2.9862e-02,\n",
            "         3.0693e-02,  1.4166e-02, -1.7550e-02,  2.6814e-02,  1.4938e-02,\n",
            "        -3.0595e-02,  2.6268e-03, -5.6717e-04, -1.0284e-02,  2.9573e-03,\n",
            "         2.4493e-02, -1.2793e-02, -8.2947e-03,  8.6051e-03,  1.5251e-02,\n",
            "        -1.2551e-03,  1.8370e-02,  2.0194e-02,  3.1704e-02,  1.6271e-02,\n",
            "         5.7648e-03, -1.1754e-02, -2.8936e-02,  1.3602e-02, -2.0230e-02,\n",
            "         2.3500e-02, -3.4909e-03,  6.8151e-03, -3.9713e-03, -1.9410e-02,\n",
            "        -2.2285e-02, -8.1350e-03, -5.3547e-03, -3.1504e-02, -1.6675e-03,\n",
            "         2.8465e-02,  1.1023e-02, -1.4967e-02,  3.3372e-02, -1.2001e-02,\n",
            "         2.3860e-02, -3.2786e-02,  6.5018e-03,  4.8840e-03, -2.9866e-02,\n",
            "        -3.1997e-02,  4.7691e-03, -5.3828e-03, -1.9831e-02, -1.1306e-03,\n",
            "         7.1287e-04, -2.1314e-02,  2.2658e-02,  1.2472e-03, -1.7829e-02,\n",
            "        -2.4073e-02,  1.2982e-04,  1.8906e-02,  2.6881e-02, -1.9781e-02,\n",
            "         2.9587e-02,  2.0501e-02, -3.0865e-02, -2.2421e-02, -1.5331e-02,\n",
            "        -5.9236e-03, -2.9065e-02,  1.8332e-02, -1.1057e-02,  3.2437e-03,\n",
            "         3.0179e-02,  1.3030e-02,  2.8039e-03, -1.1443e-02, -1.6171e-02,\n",
            "        -7.3875e-03,  2.9498e-02, -2.8567e-02, -1.7148e-02,  7.7138e-03,\n",
            "        -9.3358e-03,  6.1467e-03,  2.0958e-02, -5.4689e-03,  2.9116e-02,\n",
            "        -1.0117e-02,  1.8468e-02,  1.9251e-02,  8.7718e-04,  8.4298e-03,\n",
            "         2.4617e-02,  2.0735e-02,  3.2348e-02,  1.5459e-02, -3.0858e-02,\n",
            "         2.2610e-02,  1.2571e-02,  1.3680e-02,  7.0614e-03,  1.1473e-02,\n",
            "         6.4096e-03, -2.4560e-02,  2.9356e-02, -5.1656e-03,  8.0530e-03,\n",
            "        -2.3912e-02,  1.1003e-02,  2.2139e-04, -2.6259e-02, -3.0709e-02,\n",
            "        -1.8203e-02,  1.7659e-02, -5.8809e-03, -1.2096e-02, -2.5559e-02,\n",
            "        -2.1161e-03, -2.4080e-03, -1.4081e-02,  2.3075e-02, -1.3532e-02,\n",
            "         1.4881e-02, -2.5931e-02, -1.9693e-02,  1.1995e-02,  1.7738e-02,\n",
            "         1.7832e-02,  1.5712e-02, -1.7824e-02,  1.2335e-02,  2.7940e-02,\n",
            "         2.9430e-02, -6.0981e-03, -3.9327e-03,  1.8676e-04, -1.6828e-02,\n",
            "         9.6875e-03, -1.0320e-02, -1.5209e-02,  1.7820e-02,  3.7072e-03,\n",
            "        -6.9283e-03,  2.4926e-02, -2.9640e-02,  1.6759e-02, -2.8035e-02,\n",
            "        -2.9126e-02,  4.4247e-04,  2.5255e-02,  3.2039e-02, -7.6639e-04,\n",
            "         1.8959e-02,  1.8422e-02,  1.0468e-02, -1.5606e-02, -1.7895e-02,\n",
            "         2.0422e-04,  2.6132e-02,  1.9343e-02, -2.0902e-02, -3.1470e-02,\n",
            "        -2.9382e-02,  5.9990e-03,  5.1359e-03,  2.8704e-02, -3.3020e-02,\n",
            "        -8.3787e-04,  1.9185e-02,  1.3709e-02,  2.6157e-03,  2.7711e-02,\n",
            "         4.4097e-03, -2.6750e-02,  1.2574e-02, -2.2876e-02,  4.3015e-03,\n",
            "        -1.5464e-02, -3.0670e-02, -6.9692e-03,  6.6724e-03,  9.0484e-03,\n",
            "         7.7590e-03, -2.2830e-02,  3.1140e-02, -2.9158e-02, -2.5565e-02,\n",
            "         1.3649e-02,  2.1585e-02, -2.5726e-02,  7.5124e-03,  2.3015e-02,\n",
            "         2.2905e-02, -1.2637e-02, -3.0784e-02, -2.9450e-02, -8.2776e-03,\n",
            "         1.9575e-02, -1.1717e-03,  1.4080e-02, -3.2793e-02,  1.0969e-02,\n",
            "        -5.7936e-03,  1.6334e-02,  2.6089e-02,  1.5582e-02, -1.8675e-02,\n",
            "        -1.1825e-02, -3.9736e-04,  2.4679e-02, -2.8159e-02,  6.3136e-03,\n",
            "        -1.2345e-02,  7.7562e-03,  1.1998e-02,  1.5554e-03, -9.4891e-03,\n",
            "        -2.0778e-02, -4.1658e-03, -1.0279e-02, -1.5105e-02,  1.2113e-02,\n",
            "         2.0310e-02, -7.7104e-03, -9.6433e-03,  2.1543e-02, -9.0736e-03,\n",
            "        -1.2576e-02,  2.6052e-02,  2.8745e-02,  4.8979e-03,  1.9407e-02,\n",
            "         2.3075e-02,  2.6245e-02,  1.4625e-03,  2.1202e-02, -2.5316e-02,\n",
            "        -3.2156e-02, -2.1280e-02,  8.5685e-03, -1.0954e-02,  3.2335e-02,\n",
            "        -3.0746e-02,  1.8819e-04, -2.8861e-02, -1.3456e-02, -5.0047e-03,\n",
            "        -2.7469e-02,  3.0849e-02,  2.4916e-02, -9.5976e-03, -2.8835e-02,\n",
            "         3.0585e-02,  2.0288e-02,  3.1159e-02, -3.2167e-02,  1.6660e-02,\n",
            "        -8.6249e-03,  1.7406e-02, -3.0004e-02,  2.6126e-03, -3.0445e-02,\n",
            "         1.4691e-02,  2.7817e-02,  2.0409e-02, -1.1569e-02,  1.8775e-02,\n",
            "         3.3508e-03,  6.9867e-04, -9.1174e-03, -5.3175e-03,  2.7795e-02,\n",
            "         7.8157e-03,  3.0626e-02, -2.3580e-03,  3.1309e-03,  1.0328e-02,\n",
            "        -6.8944e-03,  2.8782e-02, -2.2018e-02,  2.8706e-02, -9.1219e-03,\n",
            "        -1.1259e-02,  3.0469e-02,  2.2541e-02, -8.4406e-03, -1.6104e-02,\n",
            "        -2.9304e-03, -4.8423e-03,  1.8177e-02,  1.6771e-03,  2.2772e-02,\n",
            "        -1.3932e-02,  1.4532e-02,  3.2265e-02, -1.6483e-02,  1.0695e-02,\n",
            "         2.1712e-02,  5.8775e-03,  2.9231e-02, -1.5308e-02, -3.6297e-03,\n",
            "        -3.9448e-03, -9.0778e-03,  2.7032e-02, -4.2240e-04,  1.0461e-02,\n",
            "        -5.2272e-03,  2.4845e-02,  3.2272e-02, -2.5079e-02, -9.7401e-03,\n",
            "         3.3530e-02,  2.9799e-02, -1.8699e-02, -2.1961e-02, -9.2221e-03,\n",
            "         2.1172e-03,  3.3247e-04,  3.0518e-02, -1.3045e-02, -1.3759e-02,\n",
            "         2.7634e-02,  4.0073e-03, -7.2399e-03, -4.2636e-03, -3.4953e-02,\n",
            "         1.8751e-02,  2.5191e-02, -2.7228e-03,  2.9692e-02,  3.1514e-02,\n",
            "         1.6832e-02, -2.5429e-02, -1.2584e-02, -1.3050e-02, -2.6537e-02,\n",
            "         2.7773e-02, -3.2396e-03,  2.4540e-02, -2.1761e-02, -8.4514e-03,\n",
            "         7.5165e-03,  2.1400e-02, -1.6529e-02,  2.8550e-02, -1.8732e-02,\n",
            "         2.0587e-02, -1.6782e-02, -7.5184e-04,  1.0302e-02,  1.7501e-02,\n",
            "         1.6918e-02, -2.1261e-02, -6.4355e-03,  1.7775e-02, -1.1328e-02,\n",
            "        -3.1278e-02,  2.0045e-02, -2.1787e-02, -2.2428e-03,  2.9343e-02,\n",
            "        -9.3957e-03,  8.0241e-03,  2.8974e-02, -1.5669e-02, -7.3951e-03,\n",
            "         2.7528e-02,  1.1322e-02,  1.6284e-02,  2.8990e-02, -3.0958e-03,\n",
            "        -2.4017e-02, -9.3226e-03,  1.8039e-02, -1.0251e-02,  8.9491e-03,\n",
            "         2.3754e-03, -3.9807e-03, -4.4466e-03,  1.3898e-02,  1.5712e-03,\n",
            "        -2.0080e-02, -3.2700e-02,  2.7509e-02, -3.1064e-02, -1.8094e-02,\n",
            "        -2.4836e-03,  1.7167e-02,  2.7292e-02,  2.3941e-02,  4.8119e-03,\n",
            "        -1.6045e-02, -5.9072e-03, -2.7343e-02,  2.8222e-04,  9.8365e-03,\n",
            "         1.7415e-02,  1.8912e-02, -1.3324e-02,  1.6349e-02, -2.8214e-02,\n",
            "         8.4565e-03,  1.1052e-02, -5.7405e-03,  1.1273e-02, -2.1949e-03,\n",
            "        -1.7726e-02,  1.8747e-02,  2.4196e-02, -1.0137e-02, -2.3812e-02,\n",
            "        -1.2278e-02, -1.4339e-02, -6.2420e-03, -2.5390e-02, -2.3117e-03,\n",
            "         1.2715e-02,  2.0863e-02,  1.3462e-02, -4.2703e-04, -3.9571e-03,\n",
            "        -5.0714e-03,  1.9301e-02, -9.4622e-04,  1.5198e-02, -2.4975e-02,\n",
            "         1.0043e-02,  8.1847e-03, -2.6837e-02, -1.2704e-02, -2.5560e-02,\n",
            "         2.3369e-02, -9.2318e-03,  2.1868e-02,  2.1909e-02, -3.5216e-03,\n",
            "         7.0945e-03, -1.0589e-02,  1.1031e-02,  7.1564e-03,  1.2713e-02,\n",
            "        -1.3993e-02, -3.6386e-03,  1.5999e-02, -1.3954e-02, -1.5165e-02,\n",
            "        -1.9999e-02,  3.3830e-03, -2.6046e-02,  5.2853e-03, -1.1573e-02,\n",
            "         2.3597e-02, -4.6195e-03, -2.7684e-02, -2.7404e-02,  1.2742e-02,\n",
            "         1.6137e-02, -9.1372e-03,  2.7146e-02,  2.2029e-02,  2.6347e-02,\n",
            "        -2.5641e-02,  2.2225e-02,  4.2598e-03,  2.7160e-02,  1.3085e-02,\n",
            "        -1.1619e-02,  3.2142e-03,  2.4655e-02,  1.5217e-02, -1.3173e-02,\n",
            "         1.9245e-02,  1.2351e-03, -7.5416e-03, -7.0327e-03,  3.0604e-02,\n",
            "        -1.1170e-02,  1.2449e-02, -3.2878e-02,  3.4741e-02,  1.1468e-02,\n",
            "        -3.6181e-03,  2.9950e-02, -2.9271e-02, -1.0523e-02,  3.9430e-03,\n",
            "        -3.2741e-02,  5.2178e-03,  2.3637e-02, -4.1829e-03,  2.5731e-02,\n",
            "         2.7388e-02,  1.0659e-02,  9.0068e-03,  1.9524e-02,  2.6013e-02,\n",
            "        -3.1591e-02,  3.1329e-02,  2.3098e-02, -2.4066e-02, -2.0023e-02,\n",
            "         8.8087e-03, -1.4433e-03, -2.3289e-02,  2.8438e-02,  3.1747e-02,\n",
            "        -1.3471e-02,  3.1723e-02, -4.8878e-04,  1.8693e-02,  2.4573e-02,\n",
            "        -6.0951e-04, -2.8182e-02,  6.6607e-04, -2.1510e-02, -2.9051e-02,\n",
            "        -1.4550e-02,  1.4416e-02, -2.5855e-02,  1.1109e-02, -1.3412e-02,\n",
            "        -1.8272e-02, -6.6001e-03,  7.7085e-03,  2.9565e-02,  5.0854e-04,\n",
            "        -1.5329e-02,  1.9919e-02,  9.7481e-04, -2.7359e-02,  6.5234e-03,\n",
            "        -1.8656e-02,  3.1190e-02, -3.2748e-02, -1.9377e-02,  1.2248e-02,\n",
            "         3.0604e-02,  3.0263e-02, -1.4544e-02, -4.6499e-03,  4.8474e-03,\n",
            "        -1.9061e-02, -1.1660e-02,  1.2246e-02,  2.6904e-02, -1.2875e-02,\n",
            "        -3.2081e-02, -3.1931e-02,  2.0139e-03,  1.3254e-02,  1.8403e-02,\n",
            "        -4.1436e-03,  6.0135e-03,  2.5694e-02,  2.0937e-05,  1.2095e-02,\n",
            "         9.4952e-04,  2.0200e-02, -1.9913e-02,  7.6790e-03,  2.6354e-02,\n",
            "         2.7277e-02, -3.3127e-02, -1.6153e-03, -9.6690e-03, -5.7313e-03,\n",
            "        -2.1667e-02,  2.5687e-02,  2.3512e-02, -1.5623e-02,  5.3760e-03,\n",
            "        -2.1109e-02, -1.6786e-02, -1.8331e-02,  2.5909e-02, -1.7662e-02,\n",
            "        -2.8497e-02, -3.4930e-03,  4.2250e-04,  6.5064e-04,  2.0336e-03,\n",
            "        -7.1874e-03,  2.3845e-02, -1.9640e-02, -2.6436e-02,  2.9493e-02,\n",
            "         1.3693e-02,  1.6125e-02, -1.0303e-02,  7.4460e-03, -1.3200e-02,\n",
            "        -2.2474e-02, -2.1977e-02, -9.9615e-03,  5.5570e-03,  1.7066e-02,\n",
            "         2.4319e-02,  1.6411e-02, -3.0866e-02,  2.7912e-02,  2.3383e-02,\n",
            "        -7.7102e-03,  2.4872e-02, -2.1051e-02,  1.3724e-02,  1.7152e-02,\n",
            "        -7.3422e-03,  2.6152e-02, -3.0895e-02,  6.2165e-03, -2.0641e-02,\n",
            "        -1.6931e-03,  7.7407e-03,  2.2044e-03,  2.1164e-04,  7.5273e-04,\n",
            "        -2.1238e-02,  1.7640e-02, -1.3325e-02,  1.6020e-02,  5.1311e-03,\n",
            "         1.6632e-02,  2.4890e-02,  2.0036e-02,  1.3413e-02,  1.3770e-02,\n",
            "         2.1385e-02, -3.6739e-03,  3.9765e-03,  2.8669e-02, -1.7709e-02,\n",
            "        -4.3481e-03,  2.7301e-02, -3.0285e-02,  2.4981e-02,  2.4199e-02,\n",
            "         1.2630e-02,  1.4794e-02,  5.6664e-03,  1.3399e-02,  1.5550e-02,\n",
            "         2.5306e-02, -2.5831e-02, -7.6578e-03,  1.7840e-02, -1.5370e-02],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([[ 1.9164e-02, -2.6892e-02, -2.5727e-02,  2.1801e-02,  1.2254e-02,\n",
            "         -1.7262e-02, -1.2724e-02,  1.0954e-02, -2.5102e-02, -1.4763e-02,\n",
            "          1.8938e-02, -1.0669e-02, -2.6465e-02,  3.5014e-03,  1.7949e-02,\n",
            "          7.8463e-04,  1.0639e-02,  3.1837e-02, -2.9686e-02, -3.3847e-02,\n",
            "          6.3855e-03, -1.0299e-02, -2.0043e-02, -9.9410e-03, -2.8669e-02,\n",
            "          1.8348e-02,  3.2789e-02, -1.0123e-02,  2.8397e-02,  1.3679e-02,\n",
            "         -1.4750e-02,  1.2294e-02,  1.3386e-02, -3.4751e-03, -1.8019e-02,\n",
            "          2.7829e-02,  3.4167e-02,  3.1186e-02,  3.3943e-02, -1.4837e-02,\n",
            "          2.8412e-02, -9.5281e-03, -3.3718e-02,  1.0498e-02, -3.6541e-02,\n",
            "          1.8140e-02, -3.9761e-03,  7.0445e-03,  2.0761e-02,  2.1547e-02,\n",
            "         -3.0524e-02, -3.1369e-02, -6.6387e-04, -3.5795e-02,  3.2768e-02,\n",
            "         -7.8146e-03,  2.1132e-02, -2.8955e-02,  1.4915e-02,  1.0655e-02,\n",
            "         -5.3673e-03, -1.1869e-02, -1.5724e-02, -1.0130e-02,  1.3521e-02,\n",
            "         -2.4001e-02,  2.9218e-03,  1.0160e-02, -1.2710e-02,  3.6275e-03,\n",
            "          7.5566e-03, -4.7611e-03, -1.9959e-02, -1.3781e-02,  2.5636e-02,\n",
            "          1.5119e-03,  3.1328e-02,  7.7680e-03,  3.4220e-02, -1.3156e-02,\n",
            "         -3.8684e-03,  2.2372e-02,  3.2006e-02,  2.6199e-02, -1.7136e-02,\n",
            "         -2.6559e-02,  1.9828e-02,  2.8017e-02, -1.4699e-02,  2.9622e-02,\n",
            "          2.8106e-02,  2.0219e-02,  8.1820e-03, -1.5066e-02, -1.5592e-03,\n",
            "         -2.4202e-02,  3.3043e-02, -2.7951e-02, -1.7070e-02,  2.7692e-02,\n",
            "         -1.4364e-02,  3.0371e-02, -1.0146e-02, -1.7735e-02, -3.2489e-02,\n",
            "         -2.1748e-02, -2.7093e-02, -2.6706e-02, -1.5301e-02,  6.3175e-03,\n",
            "         -4.2294e-03,  1.8102e-02,  3.1684e-02, -3.3429e-02,  1.1052e-02,\n",
            "          1.1153e-02, -1.1135e-02,  2.0547e-02,  9.4565e-03,  2.8659e-02,\n",
            "         -3.2304e-02, -8.5253e-03,  9.8740e-04, -3.2515e-02,  5.7744e-03,\n",
            "          1.8454e-02,  2.7991e-02,  1.6542e-02, -5.5110e-03,  1.2966e-02,\n",
            "         -2.4903e-03,  2.0756e-02,  2.6872e-02,  2.0719e-03,  2.6698e-02,\n",
            "          2.6937e-02, -1.0350e-02, -2.9594e-02,  1.9505e-02, -2.4811e-02,\n",
            "         -2.1372e-02,  3.3810e-02,  2.1292e-02, -1.8156e-02,  1.9320e-02,\n",
            "          2.0476e-02,  2.9422e-02, -4.8722e-03, -2.0351e-02,  2.1648e-03,\n",
            "          2.1023e-02,  2.5992e-02,  2.9900e-02, -3.5773e-02,  1.4984e-02,\n",
            "          2.7783e-02, -8.6234e-03,  3.8275e-03, -7.8582e-03, -1.3038e-02,\n",
            "         -6.6945e-03, -1.2494e-02, -2.9399e-02, -4.0356e-04,  2.7304e-02,\n",
            "         -2.5248e-02,  1.2978e-02, -2.8332e-02,  2.6237e-03,  3.0991e-02,\n",
            "         -1.5593e-02, -3.0953e-02,  3.7444e-03, -8.8383e-03,  3.6671e-03,\n",
            "          1.4608e-02,  7.7422e-03, -3.5206e-02,  8.3963e-03, -2.9149e-05,\n",
            "          5.3355e-04, -1.6557e-02, -2.4932e-04,  3.0136e-02,  2.9218e-02,\n",
            "         -1.9063e-02, -7.4019e-03,  1.9924e-02,  2.6476e-03,  1.7579e-02,\n",
            "          3.5641e-02, -2.3627e-03,  1.9354e-02,  7.3279e-03,  6.3396e-03,\n",
            "          2.2755e-02, -1.9922e-02, -4.0839e-03, -2.8102e-03,  1.8032e-03,\n",
            "          3.4641e-02, -1.6968e-02, -2.0948e-02, -1.7952e-02, -1.5762e-02,\n",
            "          2.5184e-02,  2.7009e-02,  4.1368e-03, -2.9937e-03, -3.3989e-02,\n",
            "         -1.0791e-02,  1.0778e-02,  1.2258e-02,  1.5530e-02,  1.8753e-02,\n",
            "         -2.5601e-02,  2.0388e-02,  3.0720e-02,  7.7695e-03,  2.4577e-02,\n",
            "         -1.4049e-02, -5.9786e-03,  3.1868e-02,  2.8414e-02, -1.6118e-02,\n",
            "          7.2309e-03, -1.3870e-02,  2.6207e-02,  3.2498e-02,  3.8844e-03,\n",
            "          2.1468e-02,  5.2975e-03,  2.9293e-02,  1.2179e-02, -2.3820e-02,\n",
            "          2.9919e-02,  2.1188e-02,  3.5891e-03, -3.1657e-02, -3.5290e-03,\n",
            "          1.5458e-02, -3.3222e-02, -2.0949e-02, -1.6199e-02,  5.6103e-03,\n",
            "          1.5774e-03,  7.4854e-03, -1.7007e-03,  3.1759e-02,  2.0259e-03,\n",
            "         -1.7563e-02, -1.0711e-02, -6.3695e-03, -5.3778e-03, -6.7225e-03,\n",
            "          1.8048e-02,  1.0755e-02,  2.3554e-02, -1.1749e-02, -2.2812e-02,\n",
            "         -3.3209e-02,  5.0157e-03,  9.0059e-03,  2.0166e-02, -2.9905e-02,\n",
            "         -1.0361e-02, -1.6952e-02,  1.5587e-02,  3.1821e-02, -1.3162e-02,\n",
            "         -2.5820e-02,  1.5213e-02,  3.3480e-02, -2.9889e-02,  3.4109e-02,\n",
            "          2.5910e-02,  4.7473e-04, -4.1064e-03, -2.2389e-02, -1.4442e-02,\n",
            "         -1.0090e-02,  3.1058e-02,  1.9477e-02, -1.4116e-02, -9.1546e-04,\n",
            "         -2.3294e-02, -3.1001e-02, -3.1080e-03, -2.5900e-02, -2.5833e-03,\n",
            "          2.2948e-02, -2.7404e-02,  2.0938e-03, -1.6314e-02,  7.2268e-03,\n",
            "          2.6303e-02,  5.0551e-03,  4.2268e-02,  9.5783e-03, -2.8062e-02,\n",
            "         -2.3176e-02,  2.9036e-02,  1.8238e-02, -1.1576e-02,  1.0560e-02,\n",
            "         -4.7455e-03,  1.5481e-02, -9.6963e-03,  1.3572e-02, -2.4814e-03,\n",
            "         -9.0755e-03, -1.3248e-02, -1.5156e-02,  2.1175e-02, -4.8873e-03,\n",
            "         -5.7145e-03, -1.7799e-03, -1.4527e-02,  1.5180e-02, -1.5102e-02,\n",
            "         -1.3385e-03,  2.6260e-02, -2.0462e-02,  7.6336e-03, -7.5990e-03,\n",
            "         -1.9020e-02,  1.6421e-02,  1.8357e-02,  2.0197e-02,  2.3549e-02,\n",
            "          2.5910e-02,  2.0020e-02, -9.6136e-03, -3.1384e-02, -1.5160e-02,\n",
            "         -1.5759e-02, -3.2620e-03,  6.7547e-03,  2.7307e-02,  2.8187e-03,\n",
            "          2.3175e-02,  1.0533e-03,  2.4636e-02,  2.4147e-02,  3.2294e-02,\n",
            "         -7.9744e-03,  2.9523e-04, -1.1069e-02, -1.1315e-02,  2.4483e-02,\n",
            "         -1.2246e-03,  2.6948e-02, -3.0309e-02,  2.8522e-02,  2.3758e-02,\n",
            "         -1.0362e-02, -2.2526e-02,  1.2462e-02, -2.3512e-02,  1.0746e-02,\n",
            "          1.2388e-02,  1.1245e-02,  3.4421e-02, -2.9722e-02,  5.8853e-03,\n",
            "          2.0164e-02, -1.1090e-02,  2.0598e-02,  2.9892e-02,  2.7861e-02,\n",
            "          1.8214e-02, -3.3168e-02,  2.1349e-02,  2.5317e-02,  7.7080e-03,\n",
            "         -1.3940e-02,  1.0932e-02,  3.2169e-02,  1.4731e-02,  1.3920e-02,\n",
            "          1.8216e-02, -2.9760e-02, -6.5155e-03, -2.3267e-02, -2.1574e-02,\n",
            "         -2.9390e-02, -1.3546e-02, -1.8404e-02, -6.7295e-03, -3.0841e-03,\n",
            "         -1.2020e-02,  1.5565e-02,  2.4015e-02, -1.5731e-03, -1.4448e-02,\n",
            "          2.4683e-03,  1.2724e-02,  1.4196e-02, -2.6454e-02,  4.2488e-03,\n",
            "          1.9821e-02,  3.3461e-02, -3.3788e-02,  1.6643e-02, -1.8105e-02,\n",
            "         -2.5016e-02, -2.6505e-02,  2.9217e-02, -2.5498e-02, -1.2316e-03,\n",
            "         -2.8791e-02, -5.7491e-03,  5.3344e-03, -2.7318e-02,  2.0590e-02,\n",
            "         -1.7374e-02, -1.4138e-02, -2.3924e-02, -1.1758e-02, -2.9732e-02,\n",
            "          2.7415e-02,  1.3914e-02, -1.1100e-02, -3.3782e-02,  3.4014e-03,\n",
            "          6.6157e-03, -2.6489e-02,  7.3456e-03, -2.3915e-02,  3.3789e-02,\n",
            "          1.3524e-02, -3.9328e-03, -1.1748e-02,  3.4188e-02,  1.0439e-02,\n",
            "          2.5560e-02, -1.7626e-02,  9.7497e-03, -2.9297e-02, -2.2205e-02,\n",
            "         -3.1745e-02,  1.0881e-02,  1.3792e-03,  1.0024e-02, -5.2818e-03,\n",
            "         -2.9460e-02,  1.3753e-02, -2.3731e-02,  3.4108e-02,  9.6065e-03,\n",
            "         -3.6565e-03, -1.3102e-02,  3.2685e-02, -5.8352e-03,  3.0224e-02,\n",
            "         -1.8754e-02, -1.3580e-02,  3.3541e-02, -1.8403e-02,  4.5067e-03,\n",
            "          1.4659e-02,  2.7977e-02,  3.2902e-02, -3.1192e-02, -2.2964e-02,\n",
            "         -2.7153e-02, -1.3610e-02,  2.7034e-02,  2.1715e-02, -3.2977e-02,\n",
            "         -2.3016e-02,  2.9279e-02,  3.2642e-02, -2.6722e-02,  3.3798e-02,\n",
            "         -8.5045e-03,  2.4184e-02,  1.2652e-02,  2.6147e-02, -3.2880e-02,\n",
            "         -1.7590e-02,  8.5500e-03, -3.5172e-02, -2.5039e-02,  2.2207e-02,\n",
            "          2.4922e-02,  1.0636e-02,  2.0075e-02, -3.4757e-03,  8.7404e-03,\n",
            "          7.3545e-03, -2.8163e-02, -1.5295e-03,  6.3591e-03,  6.7782e-03,\n",
            "          2.8061e-02, -2.2219e-02,  3.4350e-02, -3.2700e-02, -1.8234e-02,\n",
            "         -7.4928e-03,  2.1571e-02,  1.2925e-02,  8.5642e-03, -2.5933e-02,\n",
            "         -3.3531e-02,  1.4061e-03, -4.7934e-03, -2.5764e-02,  7.0540e-03,\n",
            "         -2.8791e-02, -1.2600e-02,  1.1983e-02, -2.0331e-02,  3.4518e-02,\n",
            "          1.0508e-02, -2.7421e-02, -3.0243e-02,  2.5270e-02, -1.4965e-02,\n",
            "         -9.4040e-03,  6.1235e-03, -2.2583e-02,  1.1981e-02,  2.5460e-02,\n",
            "         -2.0848e-02, -2.2311e-02,  2.9260e-02, -5.6622e-03,  1.4002e-02,\n",
            "          2.0423e-02,  1.2861e-02,  2.8849e-02, -2.2931e-02,  1.4480e-02,\n",
            "          1.3572e-02, -1.8032e-03, -2.6999e-02, -2.8585e-02,  7.0263e-03,\n",
            "         -2.3643e-02, -5.3279e-03,  1.0095e-03, -1.2842e-02,  3.3901e-02,\n",
            "          3.2881e-02, -1.7047e-02, -2.2871e-02,  9.3174e-03,  3.7522e-03,\n",
            "          1.2902e-02, -1.0092e-02,  1.4570e-02, -2.3742e-02, -1.4597e-02,\n",
            "         -1.8822e-02,  1.4621e-02,  3.2151e-02, -1.3802e-03, -4.8693e-03,\n",
            "          3.0635e-02,  3.0403e-02,  1.9586e-04,  2.0930e-02, -1.6199e-02,\n",
            "          1.8892e-02,  1.2989e-02,  1.7495e-02,  4.1322e-04,  2.0814e-02,\n",
            "          2.1609e-02, -2.1072e-02,  1.9286e-03, -3.3472e-02, -1.5531e-02,\n",
            "         -1.9214e-02, -5.0458e-03,  1.8462e-02, -2.9279e-02, -1.4867e-02,\n",
            "         -9.2819e-03,  2.4885e-02, -1.1206e-02, -1.5413e-02,  3.2363e-02,\n",
            "         -2.4961e-03, -2.0746e-02, -5.5059e-03,  2.2634e-02, -1.6131e-02,\n",
            "          1.1825e-02,  2.9179e-02,  2.8729e-02,  4.8585e-03,  3.2816e-03,\n",
            "          1.5459e-02, -6.1939e-03,  1.3664e-02, -2.5449e-02,  2.6952e-02,\n",
            "          9.3390e-03, -2.0754e-02,  2.9078e-02,  2.4905e-02,  2.3769e-02,\n",
            "          3.1750e-02,  1.5873e-02, -9.7720e-03,  1.8655e-02, -1.0264e-02,\n",
            "          3.0003e-02, -2.5272e-03, -9.0198e-04, -1.9007e-02, -2.9515e-02,\n",
            "         -3.5120e-02,  2.3259e-02,  2.6186e-02, -1.0633e-02, -2.5941e-02,\n",
            "          2.1150e-02, -6.9411e-03,  1.6475e-02,  3.4133e-02, -1.0504e-02,\n",
            "          8.5321e-03, -2.0560e-02,  6.9351e-04, -1.2917e-02, -3.2478e-02,\n",
            "          3.3995e-02, -1.7761e-02,  3.3528e-02, -2.6437e-03,  5.5147e-03,\n",
            "         -8.4137e-03,  1.5894e-02,  1.2610e-02,  3.6185e-02,  1.5313e-02,\n",
            "         -7.2398e-03,  2.6704e-02, -1.9375e-02, -1.4233e-02, -2.3934e-02,\n",
            "          2.6576e-02,  9.2874e-03,  2.6184e-02,  3.1857e-02,  8.4899e-03,\n",
            "         -1.6207e-02,  3.4101e-02, -1.3658e-02,  1.8561e-02,  2.5305e-02,\n",
            "         -2.9620e-02, -3.3935e-02,  1.3013e-02, -2.6737e-02, -3.0735e-02,\n",
            "          8.2631e-03,  3.3142e-02, -2.9789e-02,  2.0920e-02,  2.3857e-02,\n",
            "          1.8649e-03, -1.6386e-02, -1.8337e-02,  3.2168e-02,  6.4672e-03,\n",
            "         -3.3945e-02,  3.2537e-02,  7.6973e-03,  3.2516e-02, -3.6020e-02,\n",
            "          2.0930e-02,  1.5923e-02,  2.8751e-03,  2.7549e-02, -2.7557e-02,\n",
            "         -8.8161e-03,  9.0645e-03, -3.3748e-02,  6.4138e-03, -1.5500e-02,\n",
            "          1.9655e-02,  1.0007e-02,  1.1725e-02,  2.7740e-02,  1.7093e-02,\n",
            "          1.9172e-02,  3.4225e-04,  1.0893e-02, -2.9095e-02,  1.7732e-02,\n",
            "          1.1599e-03,  3.0690e-02, -4.3111e-03, -6.6411e-03, -1.3046e-02,\n",
            "         -2.1193e-02, -3.0463e-02, -3.0362e-02, -2.7848e-02, -1.5176e-02,\n",
            "          2.2938e-02, -2.9290e-02, -1.8476e-02, -3.1795e-02, -2.6471e-02,\n",
            "          2.4947e-02, -9.2098e-03, -2.6129e-02,  8.1122e-03, -2.4157e-02,\n",
            "          5.2439e-03, -1.3634e-02,  3.2073e-02, -2.6019e-02, -2.1678e-02,\n",
            "          1.4371e-02,  1.5681e-02,  2.8017e-02,  2.2392e-02,  3.3008e-02,\n",
            "         -1.5937e-02,  3.5453e-03,  2.2567e-02, -3.4662e-03, -3.0684e-02,\n",
            "          2.3462e-02, -1.2869e-02, -2.3260e-03,  1.5414e-02, -2.1458e-02,\n",
            "          2.8654e-02,  1.1282e-02, -1.9175e-02,  2.3434e-02,  2.0393e-02,\n",
            "          4.2986e-03,  1.5589e-02, -2.7554e-02,  1.3399e-02,  1.3465e-03,\n",
            "         -3.1509e-02, -2.0694e-02,  2.8056e-02,  1.9604e-02, -3.3393e-02,\n",
            "         -7.4919e-03,  7.8771e-03, -9.1133e-03,  2.9048e-02, -8.3608e-03,\n",
            "         -2.2866e-02,  1.1144e-02,  2.2105e-02, -1.7883e-02,  3.4183e-02,\n",
            "          6.4353e-03,  1.7170e-02, -2.4722e-02,  1.0961e-02, -6.4935e-03,\n",
            "         -3.5629e-02,  1.5998e-02,  2.7223e-02,  8.0873e-03, -3.0720e-03,\n",
            "         -2.3196e-02, -2.9841e-02,  5.0699e-03,  1.6152e-02, -2.5272e-02,\n",
            "         -9.9407e-03,  2.0193e-02,  2.7020e-02,  3.1129e-02, -1.1053e-02,\n",
            "          3.3421e-02,  8.0295e-03,  9.1394e-03, -3.1960e-02,  2.3972e-02,\n",
            "         -2.4523e-02, -2.8134e-02,  2.8634e-02,  8.3792e-05,  1.4576e-02,\n",
            "         -4.0129e-03, -1.9215e-02,  5.9058e-03, -3.3595e-02,  1.2683e-02,\n",
            "          1.9479e-03, -1.2704e-02, -9.5612e-04, -2.0384e-02,  2.1143e-02,\n",
            "          3.2302e-03,  2.8852e-03,  2.0751e-02,  1.8086e-02, -3.2168e-02,\n",
            "         -3.2538e-02,  1.4629e-02,  1.2262e-02,  1.8872e-02, -1.2123e-02,\n",
            "         -1.1140e-02, -2.6065e-02,  1.3678e-02, -3.1001e-02,  3.3887e-02,\n",
            "         -6.7562e-03,  1.9986e-02, -1.9770e-02, -1.3885e-02,  2.6495e-02,\n",
            "          2.2948e-02, -2.2302e-03,  1.8649e-02,  2.4000e-02,  2.9374e-03,\n",
            "         -2.9864e-02, -3.2267e-03,  1.5808e-03, -2.8460e-02,  2.2464e-02,\n",
            "         -2.3714e-02,  1.4644e-02, -1.5501e-02, -3.2308e-02, -1.5641e-02,\n",
            "         -1.9569e-02,  1.8521e-02,  1.7954e-02, -2.5147e-02,  1.2594e-02]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([-0.0002], device='cuda:0', requires_grad=True)]\n"
          ]
        }
      ],
      "source": [
        "print(list(net_loaded.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "oOoey1gY_MQW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba927692-4d76-4950-eacb-ee5e64027f88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (hidden_activation): ReLU()\n",
              "  (output_activation): ReLU()\n",
              "  (layers): ModuleList(\n",
              "    (0): Linear(in_features=3, out_features=1044, bias=True)\n",
              "    (1): Linear(in_features=1044, out_features=914, bias=True)\n",
              "    (2): Linear(in_features=914, out_features=840, bias=True)\n",
              "    (3): Linear(in_features=840, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "# Set the network to evaluation mode\n",
        "net.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "75-J4HyG_MQX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22955062-4dea-479a-91d3-0e40d9197c4c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[9.2391e+00, 7.6261e+00, 9.0068e+00],\n",
              "        [8.7729e+00, 6.5701e+00, 1.5107e+01],\n",
              "        [9.0244e+00, 2.6344e+00, 1.1228e+01],\n",
              "        [6.9064e+00, 4.5420e+00, 3.4896e+00],\n",
              "        [6.8061e+00, 1.0049e+01, 1.5727e+01],\n",
              "        [4.0151e+00, 5.3607e+00, 4.4291e+00],\n",
              "        [7.2645e+00, 3.3973e+00, 1.0916e+01],\n",
              "        [5.1509e+00, 8.1000e+00, 9.8301e+00],\n",
              "        [3.7517e+00, 7.6702e-01, 3.8852e+00],\n",
              "        [3.4741e+00, 3.1925e-01, 2.6293e+00],\n",
              "        [3.5881e-02, 6.5245e-03, 1.4835e-02],\n",
              "        [7.5507e-01, 2.8364e-01, 6.7603e-01],\n",
              "        [1.0444e+00, 1.7094e+00, 2.1702e+00],\n",
              "        [4.7779e-01, 9.1731e-02, 1.2585e-01],\n",
              "        [8.0554e-01, 4.4223e-01, 1.3920e+00],\n",
              "        [1.8538e+00, 1.5641e-01, 2.9077e+00],\n",
              "        [2.8811e+00, 4.0518e+00, 3.6530e+00],\n",
              "        [5.4723e+00, 6.7788e+00, 9.1475e+00],\n",
              "        [9.2268e+00, 1.5631e+01, 1.9276e+01],\n",
              "        [5.8094e+00, 1.0021e+01, 7.2681e+00]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "rho_example, vx_example, epsilon_example = sample_primitive_variables(20)\n",
        "\n",
        "# Create arbitrary input\n",
        "inputs =  generate_input_data(rho_example, vx_example, epsilon_example)\n",
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "UUM5sUJG_MQX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e988496e-c8c5-4ee2-fe9f-8366cbff9f91"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([4.6638], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([9.3209], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([7.3443], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([1.5199], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([8.5634], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([1.4428], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([7.0164], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([4.6313], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([2.5610], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([1.7481], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([0.0095], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([0.4282], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([1.0494], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([0.0789], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([0.8919], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([1.9362], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([1.3174], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([4.7464], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([9.0737], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([1.2833], device='cuda:0', grad_fn=<ReluBackward0>)]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "# Pass the inputs to the network and get the outputs\n",
        "outputs = [net(input) for input in inputs]\n",
        "# Print the outputs\n",
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "KiWO79Zm_MQX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbe829cf-68a1-4a23-de8a-53b4f7a5607a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (hidden_activation): ReLU()\n",
              "  (output_activation): ReLU()\n",
              "  (layers): ModuleList(\n",
              "    (0): Linear(in_features=3, out_features=1044, bias=True)\n",
              "    (1): Linear(in_features=1044, out_features=914, bias=True)\n",
              "    (2): Linear(in_features=914, out_features=840, bias=True)\n",
              "    (3): Linear(in_features=840, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "# Set the network to evaluation mode\n",
        "net_loaded.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "6TrwpwgD_MQY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "697c2e3a-faec-4921-ff50-3cd5ce30e1d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([4.6638], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([9.3209], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([7.3443], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([1.5199], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([8.5634], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([1.4428], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([7.0164], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([4.6313], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([2.5610], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([1.7481], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([0.0095], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([0.4282], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([1.0494], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([0.0789], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([0.8919], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([1.9362], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([1.3174], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([4.7464], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([9.0737], device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([1.2833], device='cuda:0', grad_fn=<ReluBackward0>)]"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "\n",
        "# Pass the inputs to the network and get the outputs\n",
        "outputs = [net_loaded(input) for input in inputs]\n",
        "# Print the outputs\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I74u2Eaw_MQY"
      },
      "source": [
        "## Porting the model to C++"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "hwKZgaot_MQY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b82b587-f169-47e7-a688-c758d5bfb93c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1797, -0.4762,  1.2668]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 100
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.8595, 6.0535, 5.5984]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 100
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.7028]], device='cuda:0', grad_fn=<ReluBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "import torch.jit\n",
        "\n",
        "# Creating a dummy input tensor of shape (1, 3) to trace the model\n",
        "dummy_input = torch.randn(1, 3).to(device)\n",
        "dummy_input\n",
        "\n",
        "# Tracing the model using the torch.jit.trace function\n",
        "traced_model = torch.jit.trace(net_loaded, dummy_input)\n",
        "\n",
        "# Saving the traced model to a file named \"net.pt\"\n",
        "traced_model.save(\"net.pt\")\n",
        "save_file(\"net.pt\")\n",
        "\n",
        "example_input_to_validate_correct_export_and_import = generate_input_data(*sample_primitive_variables(1))\n",
        "example_input_to_validate_correct_export_and_import\n",
        "net_loaded(example_input_to_validate_correct_export_and_import)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_2rWi86Qn9S"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "bsc",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
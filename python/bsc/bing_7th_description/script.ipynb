{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bing 7th description\n",
    "\n",
    "I haven't run the code yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setting the random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Checking if GPU is available and setting the device accordingly\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Defining the constants for the equation of state and the intervals for sampling\n",
    "Gamma = 1.4 # Adiabatic index\n",
    "rho_min = 0 # Lower bound for density\n",
    "rho_max = 10.1 # Upper bound for density\n",
    "eps_min = 0 # Lower bound for specific internal energy\n",
    "eps_max = 2.02 # Upper bound for specific internal energy\n",
    "vx_min = 0 # Lower bound for velocity in x direction\n",
    "vx_max = 0.721 # Upper bound for velocity in x direction\n",
    "\n",
    "# Defining the function to calculate the pressure from the equation of state\n",
    "def p(rho, eps):\n",
    "    return (Gamma - 1) * rho * eps\n",
    "\n",
    "# Defining the function to calculate the conservative variables from the primitive variables\n",
    "def cons(rho, eps, vx):\n",
    "    W = 1 / np.sqrt(1 - vx**2) # Lorentz factor\n",
    "    h = 1 + eps + p(rho, eps) / rho # Specific enthalpy\n",
    "    D = rho * W # Conserved density\n",
    "    Sx = rho * h * W**2 * vx # Conserved momentum in x direction\n",
    "    tau = rho * h * W**2 - p(rho, eps) - D # Conserved energy density\n",
    "    return D, Sx, tau\n",
    "\n",
    "# Defining the function to generate the training and test datasets by sampling the primitive variables and calculating the corresponding conservative variables and pressure\n",
    "def generate_data(n_train, n_test):\n",
    "    # Sampling the primitive variables uniformly over the intervals\n",
    "    rho_train = np.random.uniform(rho_min, rho_max, n_train)\n",
    "    eps_train = np.random.uniform(eps_min, eps_max, n_train)\n",
    "    vx_train = np.random.uniform(vx_min, vx_max, n_train)\n",
    "    rho_test = np.random.uniform(rho_min, rho_max, n_test)\n",
    "    eps_test = np.random.uniform(eps_min, eps_max, n_test)\n",
    "    vx_test = np.random.uniform(vx_min, vx_max, n_test)\n",
    "\n",
    "    # Calculating the conservative variables and pressure for each sample\n",
    "    D_train, Sx_train, tau_train = cons(rho_train, eps_train, vx_train)\n",
    "    D_test, Sx_test, tau_test = cons(rho_test, eps_test, vx_test)\n",
    "    p_train = p(rho_train, eps_train)\n",
    "    p_test = p(rho_test, eps_test)\n",
    "\n",
    "    # Stacking the conservative variables as inputs and pressure as output\n",
    "    X_train = np.stack((D_train, Sx_train, tau_train), axis=1)\n",
    "    y_train = p_train.reshape(-1, 1)\n",
    "    X_test = np.stack((D_test, Sx_test, tau_test), axis=1)\n",
    "    y_test = p_test.reshape(-1, 1)\n",
    "\n",
    "    # Converting the numpy arrays to torch tensors and moving them to device\n",
    "    X_train = torch.from_numpy(X_train).float().to(device)\n",
    "    y_train = torch.from_numpy(y_train).float().to(device)\n",
    "    X_test = torch.from_numpy(X_test).float().to(device)\n",
    "    y_test = torch.from_numpy(y_test).float().to(device)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Defining the neural network class with two hidden layers and sigmoid activation functions\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 600) # First hidden layer with 600 neurons\n",
    "        self.fc2 = nn.Linear(600, 200) # Second hidden layer with 200 neurons\n",
    "        self.fc3 = nn.Linear(200, 1) # Output layer with 1 neuron\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid activation function\n",
    "        self.relu = nn.ReLU() # ReLU nonlinearity for the output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x)) # Applying the first hidden layer and sigmoid\n",
    "        x = self.sigmoid(self.fc2(x)) # Applying the second hidden layer and sigmoid\n",
    "        x = self.relu(self.fc3(x)) # Applying the output layer and ReLU\n",
    "        return x\n",
    "\n",
    "# Creating an instance of the neural network and moving it to device\n",
    "net = Net().to(device)\n",
    "\n",
    "# Defining the loss function as mean squared error\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Defining the optimizer as Adam with initial learning rate\n",
    "optimizer = optim.Adam(net.parameters(), lr=6e-4)\n",
    "\n",
    "# Defining the scheduler to reduce the learning rate on plateau\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, threshold=0.0005, min_lr=1e-6)\n",
    "\n",
    "# Defining the number of epochs, batch size, and data size\n",
    "epochs = 400\n",
    "batch_size = 32\n",
    "n_train = 80000\n",
    "n_test = 10000\n",
    "\n",
    "# Generating the training and test datasets\n",
    "X_train, y_train, X_test, y_test = generate_data(n_train, n_test)\n",
    "\n",
    "# Creating lists to store the losses and learning rate for each epoch\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "lrs = []\n",
    "\n",
    "# Looping over the epochs\n",
    "for epoch in range(epochs):\n",
    "    # Shuffling the training data and splitting it into batches\n",
    "    perm = torch.randperm(n_train)\n",
    "    X_train = X_train[perm]\n",
    "    y_train = y_train[perm]\n",
    "    batches = torch.split(X_train, batch_size)\n",
    "    targets = torch.split(y_train, batch_size)\n",
    "\n",
    "    # Initializing the running loss for the training data\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Looping over the batches\n",
    "    for i, (batch, target) in enumerate(zip(batches, targets)):\n",
    "        # Zeroing the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = net(batch)\n",
    "\n",
    "        # Calculating the loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Updating the running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculating the average loss for the training data\n",
    "    train_loss = running_loss / len(batches)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Evaluating the network on the test data\n",
    "    with torch.no_grad():\n",
    "        output_test = net(X_test)\n",
    "        test_loss = criterion(output_test, y_test).item()\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "    # Updating the scheduler based on the test loss\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "    # Getting the current learning rate\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    lrs.append(lr)\n",
    "\n",
    "    # Printing the progress\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, LR: {lr:.6f}')\n",
    "\n",
    "# Defining the function to calculate the L1 and L_inf norms of the error on a data series\n",
    "def error_norms(y_true, y_pred):\n",
    "    error = y_true - y_pred # Element-wise error\n",
    "    L1_norm = torch.mean(torch.abs(error)) # Mean absolute error\n",
    "    L_inf_norm = torch.max(torch.abs(error)) # Maximum absolute error\n",
    "    return L1_norm.item(), L_inf_norm.item()\n",
    "\n",
    "# Calculating the error norms for the training and test data\n",
    "train_L1_norm, train_L_inf_norm = error_norms(y_train, net(X_train))\n",
    "test_L1_norm, test_L_inf_norm = error_norms(y_test, net(X_test))\n",
    "\n",
    "# Printing the error norms\n",
    "print(f'Train L1 norm: {train_L1_norm:.4f}, Train L_inf norm: {train_L_inf_norm:.4f}')\n",
    "print(f'Test L1 norm: {test_L1_norm:.4f}, Test L_inf norm: {test_L_inf_norm:.4f}')\n",
    "\n",
    "# Defining the function to calculate the other primitive variables from pressure and conservative variables using equations A2-A5 from Dieseldorst et al.\n",
    "def prim(p, D, Sx, tau):\n",
    "    vx = Sx / (tau + D + p) # Primitive velocity in x direction\n",
    "    W = 1 / torch.sqrt(1 - vx**2) # Lorentz factor\n",
    "    eps = (tau + D * (1 - W) + p * (1 - W**2)) / (D * W) # Specific internal energy\n",
    "    rho = D / W # Density\n",
    "    return rho, eps, vx\n",
    "\n",
    "# Calculating the other primitive variables for the training and test data\n",
    "rho_train_pred, eps_train_pred, vx_train_pred = prim(net(X_train), X_train[:,0], X_train[:,1], X_train[:,2])\n",
    "rho_test_pred, eps_test_pred, vx_test_pred = prim(net(X_test), X_test[:,0], X_test[:,1], X_test[:,2])\n",
    "\n",
    "# Calculating the true primitive variables for the training and test data\n",
    "rho_train_true = X_train[:,0] / torch.sqrt(1 - (X_train[:,1] / (X_train[:,2] + X_train[:,0] + y_train))**2)\n",
    "eps_train_true = y_train / ((Gamma - 1) * rho_train_true)\n",
    "vx_train_true = X_train[:,1] / (X_train[:,2] + X_train[:,0] + y_train)\n",
    "rho_test_true = X_test[:,0] / torch.sqrt(1 - (X_test[:,1] / (X_test[:,2] + X_test[:,0] + y_test))**2)\n",
    "eps_test_true = y_test / ((Gamma - 1) * rho_test_true)\n",
    "vx_test_true = X_test[:,1] / (X_test[:,2] + X_test[:,0] + y_test)\n",
    "\n",
    "# Plotting the mean squared error against epochs for the training and test data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(epochs), train_losses, label='Train')\n",
    "plt.plot(range(epochs), test_losses, label='Test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Mean Squared Error vs Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the learning rate against epochs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(epochs), lrs)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate vs Epochs')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the pressure against density for the training and test data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(rho_train_true.cpu(), y_train.cpu(), s=1, label='Train True')\n",
    "plt.scatter(rho_train_pred.cpu(), net(X_train).cpu(), s=1, label='Train Pred')\n",
    "plt.scatter(rho_test_true.cpu(), y_test.cpu(), s=1, label='Test True')\n",
    "plt.scatter(rho_test_pred.cpu(), net(X_test).cpu(), s=1, label='Test Pred')\n",
    "plt.xlabel('Density')\n",
    "plt.ylabel('Pressure')\n",
    "plt.title('Pressure vs Density')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Saving the results and plots\n",
    "torch.save(net.state_dict(), 'net.pth') # Saving the network parameters\n",
    "np.savez('results.npz', train_losses=train_losses, test_losses=test_losses, lrs=lrs,\n",
    "         train_L1_norm=train_L1_norm, train_L_inf_norm=train_L_inf_norm,\n",
    "         test_L1_norm=test_L1_norm, test_L_inf_norm=test_L_inf_norm,\n",
    "         rho_train_true=rho_train_true.cpu().numpy(), eps_train_true=eps_train_true.cpu().numpy(), vx_train_true=vx_train_true.cpu().numpy(),\n",
    "         rho_train_pred=rho_train_pred.cpu().numpy(), eps_train_pred=eps_train_pred.cpu().numpy(), vx_train_pred=vx_train_pred.cpu().numpy(),\n",
    "         rho_test_true=rho_test_true.cpu().numpy(), eps_test_true=eps_test_true.cpu().numpy(), vx_test_true=vx_test_true.cpu().numpy(),\n",
    "         rho_test_pred=rho_test_pred.cpu().numpy(), eps_test_pred=eps_test_pred.cpu().numpy(), vx_test_pred=vx_test_pred.cpu().numpy())\n",
    "plt.savefig('mse.png') # Saving the MSE plot\n",
    "plt.savefig('lr.png') # Saving the learning rate plot\n",
    "plt.savefig('p_rho.png') # Saving the pressure-density plot\n",
    "\n",
    "# Printing a message to indicate that the code is done\n",
    "print('Code completed.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BScPhysics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
